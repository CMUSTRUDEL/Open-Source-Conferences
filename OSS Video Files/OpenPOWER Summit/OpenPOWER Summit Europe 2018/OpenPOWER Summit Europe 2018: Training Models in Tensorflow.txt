Title: OpenPOWER Summit Europe 2018: Training Models in Tensorflow
Publication date: 2019-02-07
Playlist: OpenPOWER Summit Europe 2018
Description: 
	Jason Furmanek and Kris Murphy, IBM, speak at OpenPOWER Summit Europe 2018.

For more information, please visit: https://openpowerfoundation.org/summit-2018-10-eu/
Captions: 
	00:00:00,210 --> 00:00:04,110
okay so here's we're gonna be talking

00:00:02,250 --> 00:00:05,910
about interesting technologies that we

00:00:04,110 --> 00:00:07,290
have in in IBM's power III and here's

00:00:05,910 --> 00:00:10,980
that here's the hook for the technology

00:00:07,290 --> 00:00:13,559
imagine imagine you you've curated a big

00:00:10,980 --> 00:00:14,580
data set that you know we spend a lot of

00:00:13,559 --> 00:00:19,109
time clicking on this data

00:00:14,580 --> 00:00:20,970
you've built a deep learning model that

00:00:19,109 --> 00:00:24,900
will pull all the information out of a

00:00:20,970 --> 00:00:27,359
train on it that kind of thing and and

00:00:24,900 --> 00:00:30,090
you you've gone through the training you

00:00:27,359 --> 00:00:31,590
you you think it's gonna work all sudden

00:00:30,090 --> 00:00:33,690
you blow up without a memory error and

00:00:31,590 --> 00:00:35,160
what do you do you've already kind of

00:00:33,690 --> 00:00:36,840
compromised maybe you've scaled down

00:00:35,160 --> 00:00:38,610
your data sets these kinds of things

00:00:36,840 --> 00:00:42,360
what do you do well we're gonna be

00:00:38,610 --> 00:00:45,180
talking about that so we're gonna be

00:00:42,360 --> 00:00:49,559
talking about as essentially a way of

00:00:45,180 --> 00:00:51,270
utilizing host memory to you know

00:00:49,559 --> 00:00:53,640
overcome some of these problems where

00:00:51,270 --> 00:00:56,550
you have kind of a limited memory set on

00:00:53,640 --> 00:01:00,420
your GPU so one of the main kind of

00:00:56,550 --> 00:01:03,989
problems GPUs have a small amount of

00:01:00,420 --> 00:01:05,670
memory they beckon may Nvidia release

00:01:03,989 --> 00:01:08,189
the new version of the volta has 32

00:01:05,670 --> 00:01:09,960
gigabytes of memory on it which is a lot

00:01:08,189 --> 00:01:12,869
they did a good job with that they

00:01:09,960 --> 00:01:15,869
doubled the amount of memory from 16 to

00:01:12,869 --> 00:01:18,420
30 which is fantastic except for 32 is

00:01:15,869 --> 00:01:19,700
still way smaller than how much do you

00:01:18,420 --> 00:01:22,110
have on your on your system you can have

00:01:19,700 --> 00:01:24,479
one terabyte two terabyte system memory

00:01:22,110 --> 00:01:26,640
so this is still kind of quite a bit

00:01:24,479 --> 00:01:28,799
behind and you know a single cat scan

00:01:26,640 --> 00:01:31,530
can be up to a terabyte of memory right

00:01:28,799 --> 00:01:35,520
so 32 gigabytes is great it's still

00:01:31,530 --> 00:01:38,400
pretty small though our neural networks

00:01:35,520 --> 00:01:40,079
are getting wider getting bigger how

00:01:38,400 --> 00:01:41,369
they do a lot more stuff you know we

00:01:40,079 --> 00:01:43,500
still have just a few layers and we've

00:01:41,369 --> 00:01:46,020
got hundreds of layers and some of these

00:01:43,500 --> 00:01:51,240
models and that alone takes up a lot of

00:01:46,020 --> 00:01:55,470
space and we'll see why we also have the

00:01:51,240 --> 00:01:58,140
data we have itself is huge we have high

00:01:55,470 --> 00:02:00,990
resolution images we have a lot of them

00:01:58,140 --> 00:02:03,079
you know think of I mentioned a single

00:02:00,990 --> 00:02:05,969
cat scan could be up to terabyte and

00:02:03,079 --> 00:02:08,069
hospitals have a whole library of those

00:02:05,969 --> 00:02:09,899
right how do you use that we could scale

00:02:08,069 --> 00:02:11,940
it down really small and you know you

00:02:09,899 --> 00:02:13,910
can juggling with the GPU memory is what

00:02:11,940 --> 00:02:16,730
you have to do today

00:02:13,910 --> 00:02:19,490
so if you have if you have this limited

00:02:16,730 --> 00:02:22,640
amount of GPU memory you have to really

00:02:19,490 --> 00:02:24,350
know what it's being used for you manage

00:02:22,640 --> 00:02:27,680
it very closely it's very important to

00:02:24,350 --> 00:02:29,780
know you know you'd want to be wasting

00:02:27,680 --> 00:02:32,690
any of that memory so this is a

00:02:29,780 --> 00:02:35,360
representation a typical tensorflow

00:02:32,690 --> 00:02:40,250
model you know what what's the what's

00:02:35,360 --> 00:02:42,740
the memory being used for so we've got

00:02:40,250 --> 00:02:44,810
an input data obviously and this is the

00:02:42,740 --> 00:02:48,590
you know we're talking about images you

00:02:44,810 --> 00:02:50,240
give that as an example your image set

00:02:48,590 --> 00:02:54,070
that you working on in a particular

00:02:50,240 --> 00:02:54,070
operation maybe you've got a batch DUP

00:02:54,370 --> 00:02:58,100
maybe you got batches of them but you

00:02:56,960 --> 00:03:00,370
know still you know there's certain

00:02:58,100 --> 00:03:03,710
amount of do you remember using for that

00:03:00,370 --> 00:03:05,750
the kernels or the actual GPU code that

00:03:03,710 --> 00:03:08,000
you use to you know calculate your loss

00:03:05,750 --> 00:03:10,070
or or you you really lose or whatever

00:03:08,000 --> 00:03:12,350
you know the actual kind of code we

00:03:10,070 --> 00:03:14,450
can't get rid of that is important we

00:03:12,350 --> 00:03:17,300
also have the the neural network itself

00:03:14,450 --> 00:03:18,709
you know we need that has to be resident

00:03:17,300 --> 00:03:19,700
in the GPU memory we can't really get

00:03:18,709 --> 00:03:21,410
rid of that let's what kind of scaled

00:03:19,700 --> 00:03:23,510
down what our what we can actually do

00:03:21,410 --> 00:03:25,610
with it and shrink it down we're trying

00:03:23,510 --> 00:03:28,480
to not have to do that and then we've

00:03:25,610 --> 00:03:30,830
got our layer output so for each layer

00:03:28,480 --> 00:03:31,970
that you know does the calculations and

00:03:30,830 --> 00:03:34,160
then there's there's a certain amount of

00:03:31,970 --> 00:03:36,050
data that's passed on to the next layer

00:03:34,160 --> 00:03:37,610
we would call this like the tensors this

00:03:36,050 --> 00:03:40,490
is the output of each layer and these

00:03:37,610 --> 00:03:41,930
kind of accumulate as we go so it's

00:03:40,490 --> 00:03:43,459
foreign to kind of understand all that

00:03:41,930 --> 00:03:46,340
so we can kind of try and find ways to

00:03:43,459 --> 00:03:51,280
you know reduce what we're doing always

00:03:46,340 --> 00:03:53,270
manage it a little better so let's let's

00:03:51,280 --> 00:03:55,580
take a look at this a little bit closer

00:03:53,270 --> 00:03:57,650
maybe a solution will present itself on

00:03:55,580 --> 00:04:00,020
on on how wiki they're managed things

00:03:57,650 --> 00:04:04,040
better or reduce the amount of memory

00:04:00,020 --> 00:04:07,130
that we're using so so what we'll just

00:04:04,040 --> 00:04:09,650
kind of look through the way training

00:04:07,130 --> 00:04:13,600
works today on a GPU so we've got we've

00:04:09,650 --> 00:04:16,250
got our our neural network here and

00:04:13,600 --> 00:04:18,500
we're kind of going to go through the

00:04:16,250 --> 00:04:20,900
training step so you know first thing it

00:04:18,500 --> 00:04:23,630
does it goes through the first layer of

00:04:20,900 --> 00:04:25,010
the network generally does the

00:04:23,630 --> 00:04:26,090
calculation generates they're the first

00:04:25,010 --> 00:04:29,870
tensor

00:04:26,090 --> 00:04:31,130
puts that in memory and here we're not

00:04:29,870 --> 00:04:32,660
gonna compromise you and we're not gonna

00:04:31,130 --> 00:04:34,190
you know shrink down our data set we're

00:04:32,660 --> 00:04:35,630
not gonna do anything else so this is

00:04:34,190 --> 00:04:38,389
kind of a scenario where we're out of

00:04:35,630 --> 00:04:40,520
memory so what we're not yet so we have

00:04:38,389 --> 00:04:42,650
the tensor one is cashed in a GPU memory

00:04:40,520 --> 00:04:45,470
and I mean that's password of T to the

00:04:42,650 --> 00:04:47,630
second layer and there's a tensor to

00:04:45,470 --> 00:04:50,990
that's generated that's in the memory as

00:04:47,630 --> 00:04:54,710
well go to the next layer and then oh no

00:04:50,990 --> 00:04:57,169
testa three is outside of the the blue

00:04:54,710 --> 00:05:01,700
box there which emit represents the GPU

00:04:57,169 --> 00:05:04,100
memory now we're out of memory so reason

00:05:01,700 --> 00:05:06,530
I'm kind of showing this it's kind of a

00:05:04,100 --> 00:05:10,220
good visual representation to see maybe

00:05:06,530 --> 00:05:12,320
what we're doing here is a clue what a

00:05:10,220 --> 00:05:13,760
solution could be so we've got the

00:05:12,320 --> 00:05:15,020
tensors are kind of hanging around a

00:05:13,760 --> 00:05:16,850
little bit the reason they hang around

00:05:15,020 --> 00:05:19,070
is by the time we get to the loss

00:05:16,850 --> 00:05:21,889
function we'll leave these in memory for

00:05:19,070 --> 00:05:23,540
the back propagation stages which are

00:05:21,889 --> 00:05:26,600
back down this way we need these tensors

00:05:23,540 --> 00:05:28,550
to to calculate again so we can't just

00:05:26,600 --> 00:05:32,570
delete them after we go forward so we

00:05:28,550 --> 00:05:34,220
need them around but but maybe maybe we

00:05:32,570 --> 00:05:38,510
can do something different maybe we can

00:05:34,220 --> 00:05:40,220
cache those out to system memory until

00:05:38,510 --> 00:05:41,300
we need them because they're just kind

00:05:40,220 --> 00:05:43,220
of sitting around there and they're not

00:05:41,300 --> 00:05:46,100
there they're not needed until they're

00:05:43,220 --> 00:05:47,900
needed right so let's let's try to come

00:05:46,100 --> 00:05:49,430
through a solution here that we put

00:05:47,900 --> 00:05:52,340
those tensors back out to system memory

00:05:49,430 --> 00:05:55,070
and maybe that's a good solution so this

00:05:52,340 --> 00:05:57,260
is how maybe that could work and we'll

00:05:55,070 --> 00:05:59,720
walk through this so if we have the

00:05:57,260 --> 00:06:03,280
first layer here we've got the first

00:05:59,720 --> 00:06:05,840
tensor generated after the first layer

00:06:03,280 --> 00:06:08,270
let's try to swap that down to system

00:06:05,840 --> 00:06:09,889
memory and then we'll you know we still

00:06:08,270 --> 00:06:11,120
kind of have all of you remember it free

00:06:09,889 --> 00:06:13,880
we go to the we generate the second

00:06:11,120 --> 00:06:17,120
tensor base of the second layer there

00:06:13,880 --> 00:06:18,740
and then we'll move that one down and

00:06:17,120 --> 00:06:20,060
we'll kind of continue on with answer

00:06:18,740 --> 00:06:24,650
three and by the time we get to tensor

00:06:20,060 --> 00:06:26,139
for you know everything still fits right

00:06:24,650 --> 00:06:29,720
we didn't have to compromise on

00:06:26,139 --> 00:06:32,300
shrinking our data or ripping out layers

00:06:29,720 --> 00:06:35,060
of the the model there

00:06:32,300 --> 00:06:36,610
so what's so intensive or worth and

00:06:35,060 --> 00:06:40,689
we're kind of at the

00:06:36,610 --> 00:06:41,949
end of the the layers set here week

00:06:40,689 --> 00:06:44,199
actually kind of keep that one in GPU

00:06:41,949 --> 00:06:45,460
memory because you know after the loss

00:06:44,199 --> 00:06:46,509
function is calculated we're going to

00:06:45,460 --> 00:06:51,009
need it back anyway for the back

00:06:46,509 --> 00:06:53,469
propagation phase so so we get back to

00:06:51,009 --> 00:06:56,310
the propagation back propagation we can

00:06:53,469 --> 00:06:59,680
actually kind of get rid of ten so for

00:06:56,310 --> 00:07:04,800
now it's kind of garbage collected and

00:06:59,680 --> 00:07:07,509
then we swap back in tensor three two

00:07:04,800 --> 00:07:08,740
and then one and so you know that's kind

00:07:07,509 --> 00:07:12,960
of the idea maybe that'll work

00:07:08,740 --> 00:07:15,360
I think it solves a couple things we

00:07:12,960 --> 00:07:18,550
conserve our GPU memory

00:07:15,360 --> 00:07:21,520
we've not compromised on the size of the

00:07:18,550 --> 00:07:24,969
the data set or were the tensors and we

00:07:21,520 --> 00:07:29,259
have not compromised on the size of our

00:07:24,969 --> 00:07:31,840
neural network so we've actually

00:07:29,259 --> 00:07:34,210
implemented this into an extension for

00:07:31,840 --> 00:07:37,539
tensor flow we call it

00:07:34,210 --> 00:07:39,419
TfL MST F tends to flow LM s is large

00:07:37,539 --> 00:07:41,590
model support

00:07:39,419 --> 00:07:43,750
it's called large model support but it's

00:07:41,590 --> 00:07:46,449
also can be used for large datasets it's

00:07:43,750 --> 00:07:47,560
not only for larger models right so this

00:07:46,449 --> 00:07:51,430
is kind of this is another

00:07:47,560 --> 00:07:53,050
representation of the actual tensor flow

00:07:51,430 --> 00:07:55,360
graph so when you're when you're doing a

00:07:53,050 --> 00:07:56,979
tensor flow model you kind of build the

00:07:55,360 --> 00:07:58,060
graph first and then you execute the

00:07:56,979 --> 00:08:02,440
graph and that's then it goes through

00:07:58,060 --> 00:08:04,300
the training so how how this works you

00:08:02,440 --> 00:08:06,129
kind of define your operations and the

00:08:04,300 --> 00:08:09,699
neno so eight output the output of a

00:08:06,129 --> 00:08:11,889
goes into input a B you know you kind of

00:08:09,699 --> 00:08:13,810
define whether your operations are on

00:08:11,889 --> 00:08:15,789
the GPU or on the CPU we generally want

00:08:13,810 --> 00:08:16,960
them on the GPU is you know kind of the

00:08:15,789 --> 00:08:19,810
accelerator that's where you want your

00:08:16,960 --> 00:08:23,860
operations to happen what we've done

00:08:19,810 --> 00:08:27,339
with TF LMS is we've added swap out and

00:08:23,860 --> 00:08:29,289
swap in nodes primarily on the CPU so

00:08:27,339 --> 00:08:32,199
what happens is the output of operation

00:08:29,289 --> 00:08:35,050
a goes into a swap out node it swaps out

00:08:32,199 --> 00:08:36,909
those tensors like we just saw that goes

00:08:35,050 --> 00:08:39,159
into a swap in node and then that comes

00:08:36,909 --> 00:08:41,560
back out to the GPU swaps back in the

00:08:39,159 --> 00:08:43,240
tensor once when it's ready if you

00:08:41,560 --> 00:08:44,709
notice there's actually a dotted line

00:08:43,240 --> 00:08:47,170
here as well that's that's a control

00:08:44,709 --> 00:08:50,320
vector it's always avoid the point that

00:08:47,170 --> 00:08:52,240
you don't want to swap out and immediate

00:08:50,320 --> 00:08:53,740
swap back in you want to have some sort

00:08:52,240 --> 00:08:55,660
of control mechanism and say oh just

00:08:53,740 --> 00:08:58,180
just swap back in when I need you

00:08:55,660 --> 00:08:59,650
so what bike swap back in when when at

00:08:58,180 --> 00:09:01,390
the last possible moment I want to

00:08:59,650 --> 00:09:03,910
conserve my GP member for as long as I

00:09:01,390 --> 00:09:05,500
can and then swap updates well back in

00:09:03,910 --> 00:09:06,850
so that that control vector we believe

00:09:05,500 --> 00:09:08,440
enabled with a couple of different API

00:09:06,850 --> 00:09:10,360
is to be able to control that when that

00:09:08,440 --> 00:09:15,100
actually happens so I can stay in system

00:09:10,360 --> 00:09:17,650
memory till you need it okay so that's

00:09:15,100 --> 00:09:21,100
we mentioned the extension to test for

00:09:17,650 --> 00:09:22,780
that's kind of the former representation

00:09:21,100 --> 00:09:24,160
was kind of the graph over you this is

00:09:22,780 --> 00:09:25,630
kind of what would look like in the code

00:09:24,160 --> 00:09:28,030
it sounds like might be hard thing to

00:09:25,630 --> 00:09:29,730
implement but it's actually kind of

00:09:28,030 --> 00:09:32,530
hesitated to put code up here because

00:09:29,730 --> 00:09:34,570
people sleep but it's actually I wanted

00:09:32,530 --> 00:09:36,790
to show how easy it is actually

00:09:34,570 --> 00:09:41,410
implement so we intensive flow kind of

00:09:36,790 --> 00:09:44,380
the two main high level API sets that

00:09:41,410 --> 00:09:46,960
people use are either Kerris or the

00:09:44,380 --> 00:09:49,090
estimator api there's a couple different

00:09:46,960 --> 00:09:51,730
ways of using tensorflow but this is

00:09:49,090 --> 00:09:54,850
kind of ways of enabling it in these two

00:09:51,730 --> 00:09:58,630
models so in Karis you pretty much kind

00:09:54,850 --> 00:10:01,630
of just enable a LMS Chara's callback

00:09:58,630 --> 00:10:03,760
object and you pass that into the model

00:10:01,630 --> 00:10:05,350
to generate or API and that's it you're

00:10:03,760 --> 00:10:06,580
good to go there's a couple other

00:10:05,350 --> 00:10:08,710
optional things you can use for the

00:10:06,580 --> 00:10:11,200
control vectors but there's defaults in

00:10:08,710 --> 00:10:12,160
there that work pretty well it's a

00:10:11,200 --> 00:10:13,750
little bit more complicated for the

00:10:12,160 --> 00:10:18,400
estimator a pair but not much

00:10:13,750 --> 00:10:20,530
there's a name scope that you have to

00:10:18,400 --> 00:10:23,010
define in order for this all that kind

00:10:20,530 --> 00:10:25,570
of work in the estimator is also about

00:10:23,010 --> 00:10:27,970
exclusive name scoping on everything so

00:10:25,570 --> 00:10:31,060
you've got to set up the name scope and

00:10:27,970 --> 00:10:34,480
then you can you know pass along into

00:10:31,060 --> 00:10:35,890
the or there's an LMS hooked API I bet

00:10:34,480 --> 00:10:38,920
you that you use any pass it into the

00:10:35,890 --> 00:10:41,080
trained classifier there so that's

00:10:38,920 --> 00:10:43,540
that's it and that will enable the

00:10:41,080 --> 00:10:45,220
Python module to do what we were just

00:10:43,540 --> 00:10:48,040
showing in the representation pretty

00:10:45,220 --> 00:10:48,490
easily pretty some code change it's not

00:10:48,040 --> 00:10:50,380
nothing

00:10:48,490 --> 00:10:52,720
but it's it's pretty minimal we've kind

00:10:50,380 --> 00:10:57,820
of focused on the main usage models

00:10:52,720 --> 00:10:59,610
intensive flow so what's what's possible

00:10:57,820 --> 00:11:03,370
what's what's some of the benefits so

00:10:59,610 --> 00:11:06,940
with with TF LMS that the goal was

00:11:03,370 --> 00:11:10,390
was to enable larger datasets larger

00:11:06,940 --> 00:11:12,280
models so we took one of Google's

00:11:10,390 --> 00:11:16,240
benchmark codes that they've got that

00:11:12,280 --> 00:11:17,920
see if CNN benchmark we call it a high

00:11:16,240 --> 00:11:19,840
performance benchmark and turtling it's

00:11:17,920 --> 00:11:21,370
a couple different names for it but it's

00:11:19,840 --> 00:11:23,260
it's a benchmark code that has lots of

00:11:21,370 --> 00:11:24,880
different models built into it like

00:11:23,260 --> 00:11:27,040
there's a Google map resident a couple

00:11:24,880 --> 00:11:30,100
different flavors of ResNet in there

00:11:27,040 --> 00:11:31,360
we enabled that with TF LMS and we've

00:11:30,100 --> 00:11:34,300
you know kind of it has some of these

00:11:31,360 --> 00:11:36,340
numbers here with higher resolution so

00:11:34,300 --> 00:11:39,490
with Google map we've got two point five

00:11:36,340 --> 00:11:42,970
increase two point five times increase

00:11:39,490 --> 00:11:45,160
in the maximum image resolution during

00:11:42,970 --> 00:11:47,020
the training resident 50 we got a five

00:11:45,160 --> 00:11:49,120
time increase in in batch size so that

00:11:47,020 --> 00:11:53,080
you know a higher batch size will allow

00:11:49,120 --> 00:11:54,820
you to to converge quicker and then in

00:11:53,080 --> 00:11:56,230
president 152 you got four point six

00:11:54,820 --> 00:11:58,390
increase in batch sizes so similar

00:11:56,230 --> 00:12:00,460
similar results there and these these

00:11:58,390 --> 00:12:02,620
are the resident wants are much larger

00:12:00,460 --> 00:12:05,140
size models so having having that much

00:12:02,620 --> 00:12:07,000
larger if a batch size increase in

00:12:05,140 --> 00:12:12,460
already large models is pretty

00:12:07,000 --> 00:12:18,550
significant we've also enabled a 3d unit

00:12:12,460 --> 00:12:22,060
brain MRI classification model we saw

00:12:18,550 --> 00:12:23,380
the 2.4 times increase in an image

00:12:22,060 --> 00:12:24,580
resolution there I'm gonna jump into

00:12:23,380 --> 00:12:25,750
that one a little bit more because that

00:12:24,580 --> 00:12:35,080
one's kind of a really interesting use

00:12:25,750 --> 00:12:38,590
case so 3d image segmentation what is

00:12:35,080 --> 00:12:40,510
image segmentation so you're probably

00:12:38,590 --> 00:12:42,760
familiar with like the the dog

00:12:40,510 --> 00:12:44,410
classifier demos or just you know the

00:12:42,760 --> 00:12:45,820
basic you've seen a lot of those you

00:12:44,410 --> 00:12:47,230
know what kind of cat is this well you

00:12:45,820 --> 00:12:49,240
know what you know it's a classifying

00:12:47,230 --> 00:12:51,130
the whole image in the except image

00:12:49,240 --> 00:12:54,100
segmentation is used when when you need

00:12:51,130 --> 00:12:56,770
to find multiple things within a single

00:12:54,100 --> 00:12:58,150
image so you kind of have every pixel on

00:12:56,770 --> 00:13:00,850
your image and you just have a class

00:12:58,150 --> 00:13:04,210
right so a good example would be if

00:13:00,850 --> 00:13:05,650
you're you have a satellite image and

00:13:04,210 --> 00:13:08,110
you need to be able to pick out a river

00:13:05,650 --> 00:13:09,340
versus a road right so you don't want it

00:13:08,110 --> 00:13:11,170
to say oh this is a picture of river

00:13:09,340 --> 00:13:12,700
when it also has a route in it you need

00:13:11,170 --> 00:13:14,380
to be able to you know segment up the

00:13:12,700 --> 00:13:16,079
image and you know those type of models

00:13:14,380 --> 00:13:22,420
are used for

00:13:16,079 --> 00:13:23,410
so the 3d unit part of it is so 3d means

00:13:22,420 --> 00:13:25,690
that we're working with

00:13:23,410 --> 00:13:28,630
three-dimensional images so so in like

00:13:25,690 --> 00:13:31,510
entire scans for example and then the

00:13:28,630 --> 00:13:33,699
unit is just kind of the slang for kind

00:13:31,510 --> 00:13:36,820
of how the image or the model is built

00:13:33,699 --> 00:13:39,779
there's a interconnected up and back to

00:13:36,820 --> 00:13:44,260
it so it's kind of you so but but 3d

00:13:39,779 --> 00:13:46,779
unit models didn't really have very high

00:13:44,260 --> 00:13:48,639
requirements on data so you the 3d

00:13:46,779 --> 00:13:51,070
images or have a lot of data already if

00:13:48,639 --> 00:13:53,380
you think about a 2d image versus a 3d

00:13:51,070 --> 00:13:55,899
image there's much much more data there

00:13:53,380 --> 00:13:58,300
already you know especially if you know

00:13:55,899 --> 00:14:01,420
I'm talking resolutions just yet so and

00:13:58,300 --> 00:14:02,649
then also the interconnected aspects on

00:14:01,420 --> 00:14:05,290
it you have to keep a lot of the data

00:14:02,649 --> 00:14:07,870
around a lot longer as well so it's

00:14:05,290 --> 00:14:09,519
really hard to get these image these

00:14:07,870 --> 00:14:13,360
models the Train higher than a batch

00:14:09,519 --> 00:14:15,820
size of one if you have any type of data

00:14:13,360 --> 00:14:20,529
whatsoever that's not just shrunken down

00:14:15,820 --> 00:14:23,620
to unusable so so really good use case

00:14:20,529 --> 00:14:28,149
for our for our TF LMS to try and see

00:14:23,620 --> 00:14:30,220
what we can do here so there is a what

00:14:28,149 --> 00:14:32,199
is the international multimodal brain

00:14:30,220 --> 00:14:34,870
tumor segmentation challenge of Bratz

00:14:32,199 --> 00:14:39,610
for short it's it's an annual challenge

00:14:34,870 --> 00:14:42,120
that that they hold to try and push the

00:14:39,610 --> 00:14:44,769
stay of the art of these type of models

00:14:42,120 --> 00:14:46,899
there's you know so there's a contest

00:14:44,769 --> 00:14:48,399
and whoever can write the best model and

00:14:46,899 --> 00:14:49,019
have the best accuracy you have the best

00:14:48,399 --> 00:14:54,370
you know

00:14:49,019 --> 00:14:55,930
data density that kind of thing so yeah

00:14:54,370 --> 00:14:57,069
that's it's kind of pushed the stage out

00:14:55,930 --> 00:14:59,260
there so we there's a lot of submissions

00:14:57,069 --> 00:15:00,990
in there and we actually kind of found

00:14:59,260 --> 00:15:04,149
an interesting submission that had a

00:15:00,990 --> 00:15:06,370
curious model already and we finished

00:15:04,149 --> 00:15:08,529
third I think there's somebody from the

00:15:06,370 --> 00:15:10,899
University of Pennsylvania was doing the

00:15:08,529 --> 00:15:12,670
work so we we were like that's a good

00:15:10,899 --> 00:15:15,310
place to start so we took that we

00:15:12,670 --> 00:15:18,579
enabled TF LMS well actually first we I

00:15:15,310 --> 00:15:20,260
think the the first the initial model

00:15:18,579 --> 00:15:23,319
didn't have it tends to flow back ends

00:15:20,260 --> 00:15:25,899
we switch to the Charis model to use a

00:15:23,319 --> 00:15:27,310
tensile back-end and we added TF LMS to

00:15:25,899 --> 00:15:30,279
it

00:15:27,310 --> 00:15:33,640
and and kind of tune it up and see what

00:15:30,279 --> 00:15:37,620
we could get out of there so here's some

00:15:33,640 --> 00:15:41,590
of the results so the the the challenge

00:15:37,620 --> 00:15:44,230
uses these dice coefficients scores in

00:15:41,590 --> 00:15:47,050
order to kind of score the the how well

00:15:44,230 --> 00:15:49,170
the model is doing versus the ground

00:15:47,050 --> 00:15:51,700
truth with with the ground truth being

00:15:49,170 --> 00:15:54,900
the the images that have an actual

00:15:51,700 --> 00:15:57,520
doctor looked at and determined a tumor

00:15:54,900 --> 00:16:00,970
you know and the core of the tumor and

00:15:57,520 --> 00:16:03,490
the features of it so then we train the

00:16:00,970 --> 00:16:06,070
model against this data set and then we

00:16:03,490 --> 00:16:09,310
see you know how it did versus the truth

00:16:06,070 --> 00:16:13,839
what this is showing is the benefits of

00:16:09,310 --> 00:16:15,960
a two x resolution difference in the

00:16:13,839 --> 00:16:18,400
image itself that we're training against

00:16:15,960 --> 00:16:22,180
we saw from the earlier chart that with

00:16:18,400 --> 00:16:25,029
with our TF lms enabled we got about 2.5

00:16:22,180 --> 00:16:26,950
percent or 2.5 times the higher

00:16:25,029 --> 00:16:28,330
resolution so this is the kind of

00:16:26,950 --> 00:16:31,150
benefit that you'll see from up from a

00:16:28,330 --> 00:16:32,950
two times resolution jump in these type

00:16:31,150 --> 00:16:37,720
of training so you just have that much

00:16:32,950 --> 00:16:39,850
more information in your in your data

00:16:37,720 --> 00:16:41,080
set that you can pull from that's a huge

00:16:39,850 --> 00:16:42,670
jump so you know we're meant to be

00:16:41,080 --> 00:16:47,080
familiar with how the dice coefficient

00:16:42,670 --> 00:16:49,300
work but essentially higher is better

00:16:47,080 --> 00:16:52,570
and like these lines kind of mean

00:16:49,300 --> 00:16:54,490
specific things but but higher is better

00:16:52,570 --> 00:16:56,970
for all of them and across-the-board

00:16:54,490 --> 00:16:59,650
higher resolution which you hire hired

00:16:56,970 --> 00:17:02,950
results higher higher score results on

00:16:59,650 --> 00:17:04,600
those and the the challenge there's

00:17:02,950 --> 00:17:06,100
three of these because the the challenge

00:17:04,600 --> 00:17:08,050
actually kind of scores you know

00:17:06,100 --> 00:17:09,130
identifying the tumor and then the core

00:17:08,050 --> 00:17:10,510
of the tumor and and some of the

00:17:09,130 --> 00:17:13,270
features as well so there's actually

00:17:10,510 --> 00:17:15,280
three different scores but definitely

00:17:13,270 --> 00:17:16,540
higher scores throughout right so this

00:17:15,280 --> 00:17:21,250
is the kind of benefit that a higher

00:17:16,540 --> 00:17:27,939
resolution can give you in your training

00:17:21,250 --> 00:17:30,160
runs so one thing that we didn't talk

00:17:27,939 --> 00:17:32,350
about it's like we know what's the catch

00:17:30,160 --> 00:17:36,030
here early why isn't anybody doing this

00:17:32,350 --> 00:17:38,560
already or it's pretty fundamental that

00:17:36,030 --> 00:17:40,510
swapping can be slow though right think

00:17:38,560 --> 00:17:41,950
about in your laptop you

00:17:40,510 --> 00:17:44,110
we've all gotten to the scenario we've

00:17:41,950 --> 00:17:47,020
run out of memory on her laptop right

00:17:44,110 --> 00:17:50,040
and start swapping to disk even if a

00:17:47,020 --> 00:17:54,970
fast disk that's that's a recipe for

00:17:50,040 --> 00:17:58,780
blue screen right blade very shortly if

00:17:54,970 --> 00:18:00,460
you started swapping so I'm actually

00:17:58,780 --> 00:18:01,960
gonna toss it over to Chris here in a

00:18:00,460 --> 00:18:04,480
minute but but you know she's going to

00:18:01,960 --> 00:18:06,700
talk about some of the specific hardware

00:18:04,480 --> 00:18:10,270
features in the oven power boxes that

00:18:06,700 --> 00:18:21,700
actually enable this to happen in a good

00:18:10,270 --> 00:18:23,590
performance so all right so my name is

00:18:21,700 --> 00:18:25,240
Chris Murphy it was on the beginning

00:18:23,590 --> 00:18:30,400
side but I also work on the power AI

00:18:25,240 --> 00:18:32,440
team with Jason so yeah swapping is slow

00:18:30,400 --> 00:18:36,630
but this is where we're going to talk

00:18:32,440 --> 00:18:40,330
about why this matters or why power 9

00:18:36,630 --> 00:18:42,430
matters right so I'm sure you guys have

00:18:40,330 --> 00:18:44,170
heard a lot about power 9 and our unique

00:18:42,430 --> 00:18:48,540
hardware features that we have in the AC

00:18:44,170 --> 00:18:52,330
922 or the GPU box but this is what a

00:18:48,540 --> 00:18:54,490
typical non power system looks like for

00:18:52,330 --> 00:18:56,830
our connectivity there's a couple of

00:18:54,490 --> 00:18:59,740
things to note here if you look at the

00:18:56,830 --> 00:19:02,260
boss between the system memory and CPU

00:18:59,740 --> 00:19:06,450
and how fast that boss is about you know

00:19:02,260 --> 00:19:10,360
average or typical of 76 gigabits per

00:19:06,450 --> 00:19:13,570
second and then look at the link between

00:19:10,360 --> 00:19:16,390
the CPU and the GPU is right so here

00:19:13,570 --> 00:19:20,350
they have PCI and in most cases as PCI 3

00:19:16,390 --> 00:19:23,350
these days gen 3 not only is that boss

00:19:20,350 --> 00:19:26,830
only 32 gigabits per second but also in

00:19:23,350 --> 00:19:30,100
a lot of systems you typically have two

00:19:26,830 --> 00:19:32,650
GPUs per PCI link right so if you're

00:19:30,100 --> 00:19:35,560
doing things to both of those GPUs at

00:19:32,650 --> 00:19:38,890
the same time you're sharing that 32

00:19:35,560 --> 00:19:42,310
Gigabit link so that's a huge bottleneck

00:19:38,890 --> 00:19:44,350
so for this type of system if you were

00:19:42,310 --> 00:19:46,420
doing this swapping of tensors in and

00:19:44,350 --> 00:19:49,060
out you know and you have a bunch of

00:19:46,420 --> 00:19:52,330
GPUs you know sharing these PCI links

00:19:49,060 --> 00:19:55,150
that can be really really slow and so

00:19:52,330 --> 00:19:58,270
you can do LMS on this

00:19:55,150 --> 00:20:00,910
we've contributed tensorflow LMS to a PR

00:19:58,270 --> 00:20:05,880
upstream you could download it we've ran

00:20:00,910 --> 00:20:09,970
it on x86 boxes it works but it is slow

00:20:05,880 --> 00:20:13,390
so if we compare it to what we have in

00:20:09,970 --> 00:20:16,030
our open power power 9 that IBM builds

00:20:13,390 --> 00:20:19,180
if you look at the the bus between

00:20:16,030 --> 00:20:20,830
system memory and CPU we've got 170

00:20:19,180 --> 00:20:23,140
gigabits per second it's quite a bit

00:20:20,830 --> 00:20:26,200
faster there and then if you look at the

00:20:23,140 --> 00:20:30,100
links between the GPU and the CPU we

00:20:26,200 --> 00:20:31,990
have envy link 2.0 and we have one of

00:20:30,100 --> 00:20:35,110
these links per GPU so they're not

00:20:31,990 --> 00:20:37,660
sharing this right so you get 150

00:20:35,110 --> 00:20:41,260
gigabits per second and then they also

00:20:37,660 --> 00:20:44,050
have envy link 2.0 between the GPUs lots

00:20:41,260 --> 00:20:46,330
of other boxes also have that Envy link

00:20:44,050 --> 00:20:49,000
between the GPU so they might claim they

00:20:46,330 --> 00:20:51,810
have Envy link but they don't have and

00:20:49,000 --> 00:20:54,370
be link between the CPU and the GPU

00:20:51,810 --> 00:20:55,840
right so if they can do things where

00:20:54,370 --> 00:20:57,190
they only have to be in the GPU and

00:20:55,840 --> 00:20:59,950
they're not transferring things back and

00:20:57,190 --> 00:21:02,590
forth to the CPU it's all good but for a

00:20:59,950 --> 00:21:04,720
case like lms where you want to be

00:21:02,590 --> 00:21:06,670
swapping things between the CPU and the

00:21:04,720 --> 00:21:08,920
GPU in order to take advantage of your

00:21:06,670 --> 00:21:13,000
system memory that bus is really going

00:21:08,920 --> 00:21:15,730
to matter so here going to show some

00:21:13,000 --> 00:21:19,570
pictures and these are pictures from

00:21:15,730 --> 00:21:23,200
using Nvidia's Envy profiler where you

00:21:19,570 --> 00:21:26,710
can look at when you're doing a model

00:21:23,200 --> 00:21:29,410
run in training and see the brown spaces

00:21:26,710 --> 00:21:31,150
those are memory copies so you can see

00:21:29,410 --> 00:21:33,690
where memory copies are happening and

00:21:31,150 --> 00:21:37,210
then the bottom line here is your GPU

00:21:33,690 --> 00:21:40,630
utilization line so this this top image

00:21:37,210 --> 00:21:42,780
here is on that that PC I connected

00:21:40,630 --> 00:21:47,140
system where you have the PCI link to

00:21:42,780 --> 00:21:49,750
your GPUs and when you run that that 3d

00:21:47,140 --> 00:21:53,980
unit test that Jason was talking about

00:21:49,750 --> 00:21:57,550
if you run that on that x86 system you

00:21:53,980 --> 00:21:59,800
can see a lot of white space in your GPU

00:21:57,550 --> 00:22:01,300
utilization line that means all those

00:21:59,800 --> 00:22:03,760
white spots are times that you're

00:22:01,300 --> 00:22:06,550
waiting for your data to get transferred

00:22:03,760 --> 00:22:08,230
into your GPUs so you're not fully

00:22:06,550 --> 00:22:10,450
utilizing those GPUs and your

00:22:08,230 --> 00:22:13,269
and waiting so that's where swapping is

00:22:10,450 --> 00:22:17,769
slow in this case so let's compare this

00:22:13,269 --> 00:22:20,710
to what we have if we run this on our

00:22:17,769 --> 00:22:23,289
open power system so this is the exact

00:22:20,710 --> 00:22:25,870
same run so if you look at how long this

00:22:23,289 --> 00:22:28,809
is on our box here it's a lot shorter

00:22:25,870 --> 00:22:31,630
total time is a lot shorter right and

00:22:28,809 --> 00:22:34,149
that's because if you look at our GPU

00:22:31,630 --> 00:22:38,289
utilization here our GPU is busy almost

00:22:34,149 --> 00:22:41,230
the entire time right because we're not

00:22:38,289 --> 00:22:43,929
waiting so long for all that swapping to

00:22:41,230 --> 00:22:49,110
happen into our GPU right so that's

00:22:43,929 --> 00:22:51,700
where we get those speed ups right and

00:22:49,110 --> 00:22:55,169
here you can see the connection so you

00:22:51,700 --> 00:22:57,639
can see where this it's the exact same

00:22:55,169 --> 00:22:59,110
stuff going on in both the blue lines

00:22:57,639 --> 00:23:00,340
connect here you can see the different

00:22:59,110 --> 00:23:02,110
pieces of the model when they're running

00:23:00,340 --> 00:23:04,510
and you can see you know how they

00:23:02,110 --> 00:23:09,690
compare there and how much more spread

00:23:04,510 --> 00:23:11,950
out they are on the PCI connected system

00:23:09,690 --> 00:23:13,539
so it's a little bit more details we

00:23:11,950 --> 00:23:15,820
have a few more graphs here on the

00:23:13,539 --> 00:23:19,240
difference it makes so this is the

00:23:15,820 --> 00:23:22,179
impact on the the epoch time on these

00:23:19,240 --> 00:23:23,980
different systems so that so here lower

00:23:22,179 --> 00:23:25,269
is better you don't want it to take very

00:23:23,980 --> 00:23:27,279
long you want to take shorter amount of

00:23:25,269 --> 00:23:30,340
time to run an epoch right so the first

00:23:27,279 --> 00:23:34,240
bar here is on the AC 922 with our envy

00:23:30,340 --> 00:23:37,179
link 2.0 and then you compare it to a

00:23:34,240 --> 00:23:40,110
GPU server with PCI and in these cases

00:23:37,179 --> 00:23:44,529
we're only using one GPU at a time right

00:23:40,110 --> 00:23:47,139
but remember on that PC I connected box

00:23:44,529 --> 00:23:50,230
that sometimes the two GPUs might have

00:23:47,139 --> 00:23:53,139
to share that same PCI link so this last

00:23:50,230 --> 00:23:55,000
bar here is what would happen if you

00:23:53,139 --> 00:23:57,399
were using that other GPU for some other

00:23:55,000 --> 00:23:59,049
work that was also using that link so

00:23:57,399 --> 00:24:01,149
now that link isn't fully available for

00:23:59,049 --> 00:24:02,679
your GPU that you're training now your

00:24:01,149 --> 00:24:09,149
epoch time goes up even more because now

00:24:02,679 --> 00:24:13,299
you have that shared bus contention and

00:24:09,149 --> 00:24:15,340
this is Effects of GPU utilization so

00:24:13,299 --> 00:24:17,350
here we want higher bars because we want

00:24:15,340 --> 00:24:19,840
our GPUs to be fully utilized right

00:24:17,350 --> 00:24:21,670
they're usually the most expensive part

00:24:19,840 --> 00:24:23,170
of a GPU system is

00:24:21,670 --> 00:24:25,000
GPU so you want to be taking full

00:24:23,170 --> 00:24:27,940
advantage of your GPUs and using them as

00:24:25,000 --> 00:24:31,750
much as possible right so the green bar

00:24:27,940 --> 00:24:33,670
is our GPU utilization and the orangish

00:24:31,750 --> 00:24:37,420
colored bars that's the memory copy

00:24:33,670 --> 00:24:39,490
throughput so you can see you know

00:24:37,420 --> 00:24:41,710
running this on the AC 922 what we get

00:24:39,490 --> 00:24:44,980
there and then in the middle of us is

00:24:41,710 --> 00:24:47,920
you know the PCI connected box if you

00:24:44,980 --> 00:24:50,440
don't have PCI contention and then at

00:24:47,920 --> 00:24:52,540
the end when you do have the PCI

00:24:50,440 --> 00:24:55,330
contention you can see that the memory

00:24:52,540 --> 00:24:57,190
copy throughput goes down and hence so

00:24:55,330 --> 00:25:03,640
does your GPU utilization because you're

00:24:57,190 --> 00:25:05,170
sitting around and waiting so we did

00:25:03,640 --> 00:25:08,290
want to look at and try and figure out

00:25:05,170 --> 00:25:11,410
our overhead of doing swapping because

00:25:08,290 --> 00:25:14,260
yes it's fast around the AC 922 than it

00:25:11,410 --> 00:25:16,300
is on the you know PCI connected boxes

00:25:14,260 --> 00:25:19,330
right but there is still some overhead

00:25:16,300 --> 00:25:22,480
right so because we now have that was

00:25:19,330 --> 00:25:25,570
sixteen gigabit GPUs and the 32 gigabit

00:25:22,480 --> 00:25:28,030
GPUs that Nvidia mix so we run these

00:25:25,570 --> 00:25:30,760
tests on the 16 gigabit GPU with

00:25:28,030 --> 00:25:32,920
swapping with using lms right so using

00:25:30,760 --> 00:25:34,750
an image that's too big and we have to

00:25:32,920 --> 00:25:36,760
swap in and out right and then we

00:25:34,750 --> 00:25:38,040
compared to running that same image size

00:25:36,760 --> 00:25:41,170
oops

00:25:38,040 --> 00:25:43,420
running that same image size on a thirty

00:25:41,170 --> 00:25:46,120
two gigabit GPU where we didn't need to

00:25:43,420 --> 00:25:48,700
swap right so that we could tell exactly

00:25:46,120 --> 00:25:52,390
how much overhead we had for doing this

00:25:48,700 --> 00:25:54,040
swapping right and our overhead wasn't

00:25:52,390 --> 00:25:58,570
wasn't really that bad you know when we

00:25:54,040 --> 00:26:02,770
were doing 1.4 X the image size it was

00:25:58,570 --> 00:26:04,330
like 3% overhead you know and bigger as

00:26:02,770 --> 00:26:05,380
you go to bigger and bigger images

00:26:04,330 --> 00:26:07,990
you're gonna be doing more and more

00:26:05,380 --> 00:26:11,440
swapping or bigger swaps right so by the

00:26:07,990 --> 00:26:13,810
time you get up to the 2.4 X you know

00:26:11,440 --> 00:26:15,120
your your overheads out like you know

00:26:13,810 --> 00:26:17,770
twenty seven twenty eight percent

00:26:15,120 --> 00:26:21,100
overhead so it does get more but it's

00:26:17,770 --> 00:26:23,200
still if you consider that you could do

00:26:21,100 --> 00:26:27,360
an image that wouldn't be possible

00:26:23,200 --> 00:26:27,360
without large model support right

00:26:34,179 --> 00:26:40,209
yeah so this is explicit so you know

00:26:37,369 --> 00:26:43,219
with the LMS stuff you know we have some

00:26:40,209 --> 00:26:47,089
Python modules that are used for doing

00:26:43,219 --> 00:26:49,039
explicit swapping and you can with that

00:26:47,089 --> 00:26:51,369
control line that Jason was talking

00:26:49,039 --> 00:26:54,320
about earlier you can sort of have it

00:26:51,369 --> 00:26:56,179
swapping a couple layers before you need

00:26:54,320 --> 00:26:58,450
the images so you can do some you know

00:26:56,179 --> 00:27:00,769
so it swapped in so it's already there

00:26:58,450 --> 00:27:03,379
instead of waiting until you actually go

00:27:00,769 --> 00:27:05,509
try to use it you know basically it's

00:27:03,379 --> 00:27:06,919
like prefetching it into the GPU a

00:27:05,509 --> 00:27:08,269
little bit and those are some of the

00:27:06,919 --> 00:27:10,549
things that you can tune there are

00:27:08,269 --> 00:27:12,019
tunable zhan lms that you can do so you

00:27:10,549 --> 00:27:14,419
can tune how early you want to swap

00:27:12,019 --> 00:27:16,759
things in and out to balance being able

00:27:14,419 --> 00:27:19,459
to do the maximum amount versus you know

00:27:16,759 --> 00:27:21,589
having stuff pre fetched so that's what

00:27:19,459 --> 00:27:24,259
explicit swapping will do for you

00:27:21,589 --> 00:27:25,940
versus um you know using some of the

00:27:24,259 --> 00:27:30,070
CUDA stuff and waiting until you

00:27:25,940 --> 00:27:30,070
actually go touch it for it to come in

00:27:32,379 --> 00:27:39,589
so the future future direction of large

00:27:35,479 --> 00:27:42,109
models support so coming up pretty soon

00:27:39,589 --> 00:27:43,729
we're expecting tensorflow 2.0 and there

00:27:42,109 --> 00:27:45,709
are some changes that will be coming in

00:27:43,729 --> 00:27:48,919
tensorflow that will have some impacts

00:27:45,709 --> 00:27:53,839
on tensorflow eager mode will be on by

00:27:48,919 --> 00:27:56,149
default and then tensorflow dot contribu

00:27:53,839 --> 00:27:58,940
be sunsetted and that's right now

00:27:56,149 --> 00:28:00,469
so I said we put this up in a PR that's

00:27:58,940 --> 00:28:03,829
where it is right now we submitted it's

00:28:00,469 --> 00:28:05,899
a tentacle contribute ooh contribute to

00:28:03,829 --> 00:28:09,469
tensorflow in the past and they're

00:28:05,899 --> 00:28:10,609
getting rid of that so we're looking at

00:28:09,469 --> 00:28:12,379
other ways to integrate this into

00:28:10,609 --> 00:28:16,099
tensorflow and where it's going to

00:28:12,379 --> 00:28:18,379
belong in the meantime we may have like

00:28:16,099 --> 00:28:19,729
our own repo that we have out there for

00:28:18,379 --> 00:28:23,509
storing it in so you guys can all access

00:28:19,729 --> 00:28:26,570
it it also is built into our power AI

00:28:23,509 --> 00:28:29,119
tensorflow so if you have power AI we

00:28:26,570 --> 00:28:31,249
already have lms built in there you

00:28:29,119 --> 00:28:32,869
don't have to let go download it but if

00:28:31,249 --> 00:28:35,119
you are building your own tensorflow

00:28:32,869 --> 00:28:38,029
you can go download it and build it in

00:28:35,119 --> 00:28:41,929
as well but you know we might integrate

00:28:38,029 --> 00:28:43,250
it into tensor flows grappler and you

00:28:41,929 --> 00:28:46,070
know if people want to contribute

00:28:43,250 --> 00:28:47,960
to this and work with us on this we're

00:28:46,070 --> 00:28:52,790
open to contributions and people working

00:28:47,960 --> 00:28:55,430
with us on that portion of it and we are

00:28:52,790 --> 00:28:57,170
also continuing research in this we have

00:28:55,430 --> 00:28:59,690
a bunch of research teams that work on

00:28:57,170 --> 00:29:01,430
things for power I and then they give us

00:28:59,690 --> 00:29:03,820
input into our development team and we

00:29:01,430 --> 00:29:07,310
take in product how some of those things

00:29:03,820 --> 00:29:09,320
research has a new version of tentacle

00:29:07,310 --> 00:29:11,300
OLMS that is doing some different things

00:29:09,320 --> 00:29:13,310
with the graph modifications and we've

00:29:11,300 --> 00:29:15,230
done some early experiments with it like

00:29:13,310 --> 00:29:19,670
we showed earlier you can do currently

00:29:15,230 --> 00:29:20,960
2.4 max resolution size our early

00:29:19,670 --> 00:29:23,630
experiments show what the new version

00:29:20,960 --> 00:29:26,660
will be able to do 6.9 X so that's a big

00:29:23,630 --> 00:29:29,000
bump with a new version we're still

00:29:26,660 --> 00:29:31,790
early in that and figuring out when that

00:29:29,000 --> 00:29:37,000
will be part of power AI but look for

00:29:31,790 --> 00:29:39,920
definite improvements in that area also

00:29:37,000 --> 00:29:41,960
LMS is not just for tend to flow we went

00:29:39,920 --> 00:29:44,660
over specifically how it works in tensor

00:29:41,960 --> 00:29:48,230
flow and the 3d unit model we ran with

00:29:44,660 --> 00:29:50,030
tensor flow but we also have large model

00:29:48,230 --> 00:29:52,850
support for cafe that's already also

00:29:50,030 --> 00:29:54,920
built into power AI and coming soon we

00:29:52,850 --> 00:29:58,010
will have a large model support for pi

00:29:54,920 --> 00:30:02,270
torch is that targeted for our next

00:29:58,010 --> 00:30:04,360
release an early version a version of

00:30:02,270 --> 00:30:06,350
that and our next release of power ai

00:30:04,360 --> 00:30:09,850
you should be able to start playing with

00:30:06,350 --> 00:30:09,850
the PI torch large models support

00:30:14,560 --> 00:30:19,090
all right so this is basically the the

00:30:17,230 --> 00:30:22,210
summary I guess we're all ready yet

00:30:19,090 --> 00:30:24,220
maybe a little fast so sort of what we

00:30:22,210 --> 00:30:31,030
went over is you can use tensor swapping

00:30:24,220 --> 00:30:31,900
to overcome GPU memory limits right how

00:30:31,030 --> 00:30:34,900
many minutes do I have you're

00:30:31,900 --> 00:30:40,330
politically up signs oh I only have one

00:30:34,900 --> 00:30:43,240
minute okay we're not fast sweet it

00:30:40,330 --> 00:30:45,070
allows us to train deeper models if you

00:30:43,240 --> 00:30:48,550
wanted to do a model with more layers it

00:30:45,070 --> 00:30:52,570
enables that or also higher resolution

00:30:48,550 --> 00:30:55,030
data like brain scans other medical

00:30:52,570 --> 00:30:56,710
images a lot of them are too big to fit

00:30:55,030 --> 00:31:00,000
a single image doesn't fit in your GPU

00:30:56,710 --> 00:31:02,470
so this allows you to use bigger images

00:31:00,000 --> 00:31:04,480
without having to compress those images

00:31:02,470 --> 00:31:06,430
and lose parts of your data and then

00:31:04,480 --> 00:31:08,410
also you can potentially do larger batch

00:31:06,430 --> 00:31:13,180
sizes which might enable you to converge

00:31:08,410 --> 00:31:16,270
faster and envy link 2.0 between the CPU

00:31:13,180 --> 00:31:17,980
and the GPU allows this you to do this

00:31:16,270 --> 00:31:22,150
without it being as slow as it would be

00:31:17,980 --> 00:31:24,100
on a lot of our competitors boxes so we

00:31:22,150 --> 00:31:25,780
also have a bunch of links here for more

00:31:24,100 --> 00:31:27,640
information

00:31:25,780 --> 00:31:29,830
that's the PR that we talked about at

00:31:27,640 --> 00:31:31,690
the top for tensor flow there's a

00:31:29,830 --> 00:31:36,250
research paper that a research team did

00:31:31,690 --> 00:31:37,930
on a large model support the if you want

00:31:36,250 --> 00:31:39,960
to know more about that 3d u net case

00:31:37,930 --> 00:31:42,610
study we have a whole blog post on it

00:31:39,960 --> 00:31:44,220
and a lot of the information on how that

00:31:42,610 --> 00:31:49,230
was ran and the different results and

00:31:44,220 --> 00:31:49,230

YouTube URL: https://www.youtube.com/watch?v=FBuWMymOqlk


