Title: OpenPOWER Summit Europe 2018: Making Elephants Fly
Publication date: 2018-11-14
Playlist: OpenPOWER Summit Europe 2018
Description: 
	Jez Wain, Escala Competency Centre, Atos, speaks at OpenPOWER Foundation's OpenPOWER Summit Europe 2018.

For more information, please visit: https://openpowerfoundation.org/summit-2018-10-eu/
Captions: 
	00:00:00,830 --> 00:00:06,740
so good morning my name is Jezza Wayne I

00:00:02,750 --> 00:00:08,059
worked at Atos based out of Grenoble the

00:00:06,740 --> 00:00:10,610
good news is I'm the last thing between

00:00:08,059 --> 00:00:11,510
you and lunch the bad news is this is

00:00:10,610 --> 00:00:14,750
the first time I give him this

00:00:11,510 --> 00:00:16,070
presentation so absolutely no I along no

00:00:14,750 --> 00:00:17,869
idea how long it's actually going to

00:00:16,070 --> 00:00:19,609
take but we'll see how we get on

00:00:17,869 --> 00:00:23,929
there's a lot of material I do tend to

00:00:19,609 --> 00:00:28,810
talk quickly please please tell me if

00:00:23,929 --> 00:00:31,550
I'm going too fast so not so long ago

00:00:28,810 --> 00:00:34,250
power and x86 were like oil and vinegar

00:00:31,550 --> 00:00:36,019
they're never ever could mix and in

00:00:34,250 --> 00:00:37,519
truth if we look a little bit back at

00:00:36,019 --> 00:00:41,059
what we just heard this morning in the

00:00:37,519 --> 00:00:43,039
keynote this is the true face of the

00:00:41,059 --> 00:00:46,160
world were up against x86 very clearly

00:00:43,039 --> 00:00:49,219
in the x86 world today when people think

00:00:46,160 --> 00:00:51,559
of cloud machine learning open-source

00:00:49,219 --> 00:00:54,050
databases the first reaction is to is

00:00:51,559 --> 00:00:55,609
towards 886 and I think this is one of

00:00:54,050 --> 00:01:00,800
the key roles of the album power

00:00:55,609 --> 00:01:03,949
organization is to to promote the power

00:01:00,800 --> 00:01:07,430
as a viable platform as an alternative

00:01:03,949 --> 00:01:09,050
against the x86 today and I hope with

00:01:07,430 --> 00:01:12,080
what this I'm going to present to you I

00:01:09,050 --> 00:01:13,640
can make you come to this same

00:01:12,080 --> 00:01:15,650
conclusion and I am keep harping on ty

00:01:13,640 --> 00:01:17,870
BM about this and what we should be

00:01:15,650 --> 00:01:19,130
doing in this space so there a whole

00:01:17,870 --> 00:01:20,840
bunch of reasons why they never really

00:01:19,130 --> 00:01:22,760
makes it of course there was the first

00:01:20,840 --> 00:01:25,400
religious war the kiss risking kisk

00:01:22,760 --> 00:01:27,830
battles but much more importantly was

00:01:25,400 --> 00:01:30,550
the second religious war which is about

00:01:27,830 --> 00:01:32,960
the big end you know in little-endian

00:01:30,550 --> 00:01:37,610
which of course comes from Gulliver's

00:01:32,960 --> 00:01:39,790
Travels and Lilliput and this was the

00:01:37,610 --> 00:01:42,230
fescue and the Lilliput which was

00:01:39,790 --> 00:01:44,090
analogy for England and France I'm an

00:01:42,230 --> 00:01:46,130
Englishman living in France so I'm very

00:01:44,090 --> 00:01:47,570
familiar with this sort of war and then

00:01:46,130 --> 00:01:48,740
the big-endian little-endian was an hour

00:01:47,570 --> 00:01:53,210
algae on the Catholics and the

00:01:48,740 --> 00:01:56,120
Protestants are really was a religious

00:01:53,210 --> 00:01:59,810
war and the problem with the the data

00:01:56,120 --> 00:02:02,360
format is that if you went on to the big

00:01:59,810 --> 00:02:04,760
Indian model then you're in a data

00:02:02,360 --> 00:02:06,260
lock-in because you all your data your

00:02:04,760 --> 00:02:08,539
application data was in this format and

00:02:06,260 --> 00:02:09,890
it wouldn't go off you can get out you

00:02:08,539 --> 00:02:12,980
know your once you've got your database

00:02:09,890 --> 00:02:14,630
or your whatever applicate your ERP was

00:02:12,980 --> 00:02:18,410
running on our big endian system

00:02:14,630 --> 00:02:19,580
then getting it off was a big job yeah

00:02:18,410 --> 00:02:23,090
and it usually wasn't very compatible

00:02:19,580 --> 00:02:24,560
and so this is locking into the platform

00:02:23,090 --> 00:02:26,420
which of course customers which was

00:02:24,560 --> 00:02:28,850
great to start with and IBM

00:02:26,420 --> 00:02:30,350
excuse me IBM is but made a lot of money

00:02:28,850 --> 00:02:33,770
with this locking the story in the past

00:02:30,350 --> 00:02:35,210
but now it's no longer acceptable and we

00:02:33,770 --> 00:02:37,820
need to be open with Neal to move things

00:02:35,210 --> 00:02:40,070
but the bigger thing was ever since this

00:02:37,820 --> 00:02:42,410
outset power at the power architecture

00:02:40,070 --> 00:02:45,650
was ambidextrous it can do big endian

00:02:42,410 --> 00:02:48,020
and little endian and it can do

00:02:45,650 --> 00:02:50,390
big-endian little-endian at the same

00:02:48,020 --> 00:02:58,600
time this is what's really smart about

00:02:50,390 --> 00:02:58,600
this platform we can run oops we can run

00:02:58,690 --> 00:03:04,520
so we can run AIX which is a big onion

00:03:01,640 --> 00:03:08,050
operating system and little-endian linux

00:03:04,520 --> 00:03:11,000
on a power on the same power server and

00:03:08,050 --> 00:03:13,550
the hypervisor will switch the operating

00:03:11,000 --> 00:03:16,610
modes on the fly and it just manages

00:03:13,550 --> 00:03:18,950
that context change but because we're

00:03:16,610 --> 00:03:20,330
doing now run in little-endian there

00:03:18,950 --> 00:03:23,570
means that the little-endian linux is

00:03:20,330 --> 00:03:25,760
that we can use are exactly the same as

00:03:23,570 --> 00:03:29,270
the little Indian elixirs that running

00:03:25,760 --> 00:03:31,490
on on x86 ok so just as a little

00:03:29,270 --> 00:03:34,340
anecdote here you see that little Indian

00:03:31,490 --> 00:03:38,390
little like little Indian Linux's are

00:03:34,340 --> 00:03:40,390
always done e-l yeah which is el4 little

00:03:38,390 --> 00:03:43,790
Indian and this is the big Indians

00:03:40,390 --> 00:03:44,959
having a dig at the fact that the little

00:03:43,790 --> 00:03:47,990
engines have got everything backwards

00:03:44,959 --> 00:03:50,420
because the ordering of the numbers is

00:03:47,990 --> 00:03:56,030
back to front so you'll see ya when it

00:03:50,420 --> 00:03:58,160
should really be you Elise right so when

00:03:56,030 --> 00:04:01,070
we on a same engine this the course we

00:03:58,160 --> 00:04:03,680
have in the same data format and on the

00:04:01,070 --> 00:04:05,930
underlying system which means that we

00:04:03,680 --> 00:04:09,140
can have the same application data

00:04:05,930 --> 00:04:10,670
format it means that the applications

00:04:09,140 --> 00:04:12,800
that are running are Arabic our systems

00:04:10,670 --> 00:04:16,310
are the same using the same data format

00:04:12,800 --> 00:04:17,540
on x86 as they are on on power and the

00:04:16,310 --> 00:04:21,260
example I wish to talk to you about

00:04:17,540 --> 00:04:22,550
today is will be the Postgres ok so it

00:04:21,260 --> 00:04:25,280
means that we can have a Postgres

00:04:22,550 --> 00:04:27,980
application running on x86 right into

00:04:25,280 --> 00:04:29,810
the database files and that though

00:04:27,980 --> 00:04:32,810
those database files are perfectly

00:04:29,810 --> 00:04:36,440
readable by an Postgres application

00:04:32,810 --> 00:04:38,050
running on power and vice versa which

00:04:36,440 --> 00:04:40,520
brings us to some very nice

00:04:38,050 --> 00:04:43,610
possibilities that we can have because

00:04:40,520 --> 00:04:44,780
there is no longer a locking exactly

00:04:43,610 --> 00:04:47,150
there's no longer platform locking

00:04:44,780 --> 00:04:50,030
because customers can now take a chance

00:04:47,150 --> 00:04:51,500
on the power platform because if it the

00:04:50,030 --> 00:04:55,880
model doesn't work for them then they

00:04:51,500 --> 00:04:58,610
can always go back to x86 and our

00:04:55,880 --> 00:04:59,990
application will still work but one of

00:04:58,610 --> 00:05:06,380
the things we're trying to promote in

00:04:59,990 --> 00:05:08,720
Atos is the idea of using a x86 as a

00:05:06,380 --> 00:05:10,310
cult standby for a power application so

00:05:08,720 --> 00:05:14,300
we can have power of squares running on

00:05:10,310 --> 00:05:18,100
power right into the database if the

00:05:14,300 --> 00:05:20,680
system fails or goes down then we can

00:05:18,100 --> 00:05:24,650
have that copy of the database files

00:05:20,680 --> 00:05:27,920
across to an x86 server or or and then

00:05:24,650 --> 00:05:30,140
the application will be retaken by the

00:05:27,920 --> 00:05:32,150
application run x86 of course and then

00:05:30,140 --> 00:05:37,630
when the power server comes back up

00:05:32,150 --> 00:05:40,100
again we can fail back gracefully oops I

00:05:37,630 --> 00:05:41,900
forgot my clicker and I'm now using my

00:05:40,100 --> 00:05:46,220
telephone for this I'm just hoping

00:05:41,900 --> 00:05:48,830
nobody's going to give me a call but of

00:05:46,220 --> 00:05:50,180
course when you say x86 it said earlier

00:05:48,830 --> 00:05:55,040
now it could be the cloud so we can

00:05:50,180 --> 00:05:58,100
actually run databases on power and then

00:05:55,040 --> 00:06:00,590
have the backup server running on the

00:05:58,100 --> 00:06:03,200
cloud in on Amazon or on Google or any

00:06:00,590 --> 00:06:04,460
other third party server and and for the

00:06:03,200 --> 00:06:05,960
rest of the talk all the things I'm

00:06:04,460 --> 00:06:08,240
talking about when I talk about safety 6

00:06:05,960 --> 00:06:11,840
I sort of in the slides I'm talking

00:06:08,240 --> 00:06:14,060
about generic 8 6 platform in premises

00:06:11,840 --> 00:06:15,950
but that could be equally the case where

00:06:14,060 --> 00:06:18,740
it could be off premises ok and we have

00:06:15,950 --> 00:06:20,650
a demonstrator room I'm setting this one

00:06:18,740 --> 00:06:24,080
up there the models that we have

00:06:20,650 --> 00:06:25,970
in-house work at us being a big company

00:06:24,080 --> 00:06:27,350
and a security conscious company I'm

00:06:25,970 --> 00:06:29,510
having trouble getting the firewalls

00:06:27,350 --> 00:06:31,340
open to go and talk to between my

00:06:29,510 --> 00:06:32,540
database and Amazon but this is the plan

00:06:31,340 --> 00:06:33,710
we've got to have a demonstrated

00:06:32,540 --> 00:06:36,110
technology demonstrator showing the

00:06:33,710 --> 00:06:37,670
exactly this stuff but the problem with

00:06:36,110 --> 00:06:39,470
the cold standby of course is that the

00:06:37,670 --> 00:06:41,600
recovery time and the recovery point so

00:06:39,470 --> 00:06:44,900
the time it takes to restore the data

00:06:41,600 --> 00:06:47,950
and the point in time that we go back to

00:06:44,900 --> 00:06:50,000
and the database is restored is somewhat

00:06:47,950 --> 00:06:53,060
problematic and there's some better

00:06:50,000 --> 00:06:56,000
solutions for improving the recovery

00:06:53,060 --> 00:06:57,860
point and the recovery time but before

00:06:56,000 --> 00:06:59,300
we go into that place we need to

00:06:57,860 --> 00:07:04,070
understand a little bit about how

00:06:59,300 --> 00:07:07,160
progress works into the database so when

00:07:04,070 --> 00:07:10,040
Postgres is working it's very much very

00:07:07,160 --> 00:07:13,400
model very similar to part to Oracle the

00:07:10,040 --> 00:07:17,000
before any transaction takes place on to

00:07:13,400 --> 00:07:19,430
the database itself the actual operation

00:07:17,000 --> 00:07:21,380
how to do the operation and above all

00:07:19,430 --> 00:07:23,420
how to roll the operation bank undoing

00:07:21,380 --> 00:07:24,830
the roots are the redo and the undo they

00:07:23,420 --> 00:07:27,170
get written into what's called the

00:07:24,830 --> 00:07:28,430
writer head log or the wall for people

00:07:27,170 --> 00:07:30,110
fooling with progress which is exactly

00:07:28,430 --> 00:07:33,020
the same as the redo log in our Oracle

00:07:30,110 --> 00:07:35,930
once it's been saved in that space the

00:07:33,020 --> 00:07:37,340
transaction is is committed into the

00:07:35,930 --> 00:07:40,610
database files themselves and we use

00:07:37,340 --> 00:07:42,710
this right ahead log for recovery so if

00:07:40,610 --> 00:07:44,540
there's a crash and transactions which

00:07:42,710 --> 00:07:47,240
have been started and not completed

00:07:44,540 --> 00:07:48,980
committed or rolled back we can we can

00:07:47,240 --> 00:07:52,730
automatically roll but roll them back

00:07:48,980 --> 00:07:55,010
when the database is is recovered so

00:07:52,730 --> 00:07:57,140
when we using a hot or a warm standby

00:07:55,010 --> 00:07:59,390
that the system works like this the

00:07:57,140 --> 00:08:01,880
database is running and it's writing all

00:07:59,390 --> 00:08:05,630
its transactions to the writer head log

00:08:01,880 --> 00:08:08,660
and we have a mirror system which could

00:08:05,630 --> 00:08:10,220
be a bun - or Red Hat it does actually

00:08:08,660 --> 00:08:13,160
work I've actually done this between a

00:08:10,220 --> 00:08:15,230
Red Hat on power and on a bun - on x86

00:08:13,160 --> 00:08:16,910
and this model still works you don't

00:08:15,230 --> 00:08:20,260
actually need the same Linux operating

00:08:16,910 --> 00:08:22,850
system to get this to work so I have my

00:08:20,260 --> 00:08:27,200
x86 running on on a distant remote

00:08:22,850 --> 00:08:29,150
server and then using a typically SSL

00:08:27,200 --> 00:08:30,770
based transfer this is all done

00:08:29,150 --> 00:08:34,370
automatically by the database there are

00:08:30,770 --> 00:08:35,900
third-party solutions such as bar man

00:08:34,370 --> 00:08:39,289
which is a great name project so this

00:08:35,900 --> 00:08:40,789
bar manager is a Recovery Manager named

00:08:39,289 --> 00:08:45,040
after the Armen in the Oracle world

00:08:40,789 --> 00:08:47,360
which is done by second quadrant and

00:08:45,040 --> 00:08:49,880
automatically Postgres will ship the

00:08:47,360 --> 00:08:52,190
right head logs from the primary server

00:08:49,880 --> 00:08:54,510
to the backup server they get put in

00:08:52,190 --> 00:08:56,760
into an archive and then automatically

00:08:54,510 --> 00:08:58,860
get injected into the database so the

00:08:56,760 --> 00:09:01,440
databases is up-to-date so these right

00:08:58,860 --> 00:09:03,990
head logs they're typically between 16

00:09:01,440 --> 00:09:06,210
and 64 gigabytes in size so they take a

00:09:03,990 --> 00:09:07,560
certain amount of time to fill up so if

00:09:06,210 --> 00:09:10,230
the database was to crash

00:09:07,560 --> 00:09:11,700
we only recover to the latest the latest

00:09:10,230 --> 00:09:14,220
wall which means that the recovery point

00:09:11,700 --> 00:09:16,050
is quite a long time back the recovery

00:09:14,220 --> 00:09:19,410
time is quickly but the recovery point

00:09:16,050 --> 00:09:21,810
is a little bit old in history so

00:09:19,410 --> 00:09:25,290
there's a second technique where we use

00:09:21,810 --> 00:09:28,470
the transaction stream where whereby the

00:09:25,290 --> 00:09:32,100
transactions in progress are also sent

00:09:28,470 --> 00:09:34,200
across SSL to the distant the backup

00:09:32,100 --> 00:09:35,600
server and this operation can be done

00:09:34,200 --> 00:09:38,010
either asynchronously or synchronously

00:09:35,600 --> 00:09:40,980
so if it's asynchronously there is a

00:09:38,010 --> 00:09:42,510
risk when we recover that there will be

00:09:40,980 --> 00:09:45,120
a certain number of transactions lost

00:09:42,510 --> 00:09:48,150
but if it's synchronous then there is no

00:09:45,120 --> 00:09:51,840
zero data loss between the failure and

00:09:48,150 --> 00:09:53,640
the recovery and this is a demonstrator

00:09:51,840 --> 00:09:55,410
we have it working in the lab in

00:09:53,640 --> 00:09:58,590
Grenoble and this works really really

00:09:55,410 --> 00:10:03,960
great and we have a little demonstrator

00:09:58,590 --> 00:10:07,410
works like this so I have a power server

00:10:03,960 --> 00:10:08,760
running which is the primary and the x86

00:10:07,410 --> 00:10:11,160
server is the backup and I'm using the

00:10:08,760 --> 00:10:13,410
log ship and the transaction string as

00:10:11,160 --> 00:10:14,910
the backup method and have a little PHP

00:10:13,410 --> 00:10:20,850
application running on on a web server

00:10:14,910 --> 00:10:23,310
and every 5 seconds the the web server

00:10:20,850 --> 00:10:27,770
is going to make an insert into the

00:10:23,310 --> 00:10:30,690
database and on the primary side and

00:10:27,770 --> 00:10:32,670
it's going to do a read on the second on

00:10:30,690 --> 00:10:34,080
both of both of them ok so one of the

00:10:32,670 --> 00:10:37,020
advantages of this approach is that the

00:10:34,080 --> 00:10:39,720
backup server can be used as a read only

00:10:37,020 --> 00:10:41,640
database at the same time as the primary

00:10:39,720 --> 00:10:44,550
server is used for transactions of

00:10:41,640 --> 00:10:47,010
course you cannot commit transactions to

00:10:44,550 --> 00:10:47,850
the backup database otherwise we're

00:10:47,010 --> 00:10:49,800
really in trouble

00:10:47,850 --> 00:10:51,810
because this is the model and so I'm

00:10:49,800 --> 00:10:53,700
afraid this is a little bit technical so

00:10:51,810 --> 00:10:55,770
you have to make a note of the of the

00:10:53,700 --> 00:10:57,660
the IP addresses on the two servers

00:10:55,770 --> 00:10:59,460
because these come into play a little

00:10:57,660 --> 00:11:01,410
bit how it works so you can see that the

00:10:59,460 --> 00:11:04,530
power service on the 192 subnet merican

00:11:01,410 --> 00:11:05,560
and x86 on the tab tense on it because

00:11:04,530 --> 00:11:07,779
this is the

00:11:05,560 --> 00:11:09,999
this is the transaction that we we run

00:11:07,779 --> 00:11:13,899
okay so we run insert into the real-time

00:11:09,999 --> 00:11:14,680
data the timestamp the IP and value

00:11:13,899 --> 00:11:16,810
random value

00:11:14,680 --> 00:11:18,550
okay so and the values we put in at the

00:11:16,810 --> 00:11:21,790
timestamp is now this is an operating

00:11:18,550 --> 00:11:23,949
system function and the second operating

00:11:21,790 --> 00:11:25,660
database function is the inet server

00:11:23,949 --> 00:11:27,850
address so this takes this the IP

00:11:25,660 --> 00:11:30,059
address of the database server as

00:11:27,850 --> 00:11:32,470
resource and the 197 and the ten and

00:11:30,059 --> 00:11:34,149
insert it into the row every five

00:11:32,470 --> 00:11:38,050
seconds and then we stick a zero at the

00:11:34,149 --> 00:11:41,439
end and then the instruction and so on

00:11:38,050 --> 00:11:43,360
the web server this is what we see we

00:11:41,439 --> 00:11:45,699
get the type the transaction ID the

00:11:43,360 --> 00:11:47,769
timestamp the database IP which is

00:11:45,699 --> 00:11:49,360
injected by thing and the value of zero

00:11:47,769 --> 00:11:52,839
is just how you have just to complete

00:11:49,360 --> 00:11:55,360
the table so though so we said the the

00:11:52,839 --> 00:11:57,939
we write it on to the power the the

00:11:55,360 --> 00:11:59,860
requests are replicated on t x86 the

00:11:57,939 --> 00:12:03,189
color coding is blue is the power orange

00:11:59,860 --> 00:12:10,899
is the x86 and you can see very clearly

00:12:03,189 --> 00:12:15,579
that the two tables okay so when I kill

00:12:10,899 --> 00:12:16,870
the the database the Postgres server the

00:12:15,579 --> 00:12:19,389
data are still available in the backup

00:12:16,870 --> 00:12:23,379
and they still can still read but as you

00:12:19,389 --> 00:12:26,649
can see at 1944 I got a I killed a thing

00:12:23,379 --> 00:12:30,279
and the right insert failed okay the

00:12:26,649 --> 00:12:32,889
date so everything stopped working on on

00:12:30,279 --> 00:12:36,129
the read I can no longer write to the

00:12:32,889 --> 00:12:39,399
database but I can then promote the

00:12:36,129 --> 00:12:42,759
backup server to become the primary

00:12:39,399 --> 00:12:45,970
server and the web application which

00:12:42,759 --> 00:12:48,220
needs some smart then start writing to

00:12:45,970 --> 00:12:50,259
the backup server because now it becomes

00:12:48,220 --> 00:12:51,970
now the primary and as you can see that

00:12:50,259 --> 00:12:55,179
we've switched over to the 10 based

00:12:51,970 --> 00:12:56,170
subnet in the in the in so the whole

00:12:55,179 --> 00:12:59,079
thing works and is completely

00:12:56,170 --> 00:13:02,610
transparent to the it can be made

00:12:59,079 --> 00:13:05,350
transparent to the to the application in

00:13:02,610 --> 00:13:06,550
so we can this is one one possible way

00:13:05,350 --> 00:13:09,610
of working this work and there's a

00:13:06,550 --> 00:13:11,470
second slightly more advanced feature we

00:13:09,610 --> 00:13:14,339
can do and this uses a technology called

00:13:11,470 --> 00:13:17,259
post Chris excel which is a technology

00:13:14,339 --> 00:13:18,910
promoted by or developed by second

00:13:17,259 --> 00:13:19,300
quadrant and one of the major players in

00:13:18,910 --> 00:13:23,670
the

00:13:19,300 --> 00:13:25,620
Chris world and this is a sharded

00:13:23,670 --> 00:13:29,800
clustered shared-nothing

00:13:25,620 --> 00:13:31,980
extension for Postgres for Postgres so

00:13:29,800 --> 00:13:33,910
this is an extremely simple model okay

00:13:31,980 --> 00:13:35,470
the details are as you can imagine

00:13:33,910 --> 00:13:37,540
having a global transaction manager is

00:13:35,470 --> 00:13:38,529
quite involved but this is that

00:13:37,540 --> 00:13:40,690
essentially we have a whole bunch of

00:13:38,529 --> 00:13:43,870
data nosing this can go this can scale a

00:13:40,690 --> 00:13:50,160
long long way and you have this Postgres

00:13:43,870 --> 00:13:50,160
excel layer on top of it and we can

00:13:50,579 --> 00:13:54,970
which manages the coordination of the

00:13:53,380 --> 00:13:56,829
transaction and the coordination of the

00:13:54,970 --> 00:13:58,209
clients and the global transaction

00:13:56,829 --> 00:14:00,279
manager which manages the transactions

00:13:58,209 --> 00:14:02,589
into the into the database of course the

00:14:00,279 --> 00:14:07,300
front-end as a loud but load balancer

00:14:02,589 --> 00:14:10,959
and and a sort of H a proxy which will

00:14:07,300 --> 00:14:12,850
need to distribute the workload okay and

00:14:10,959 --> 00:14:15,399
so this is the way it works and what we

00:14:12,850 --> 00:14:19,990
can do here is because the data format

00:14:15,399 --> 00:14:23,560
is absolutely identical then we can just

00:14:19,990 --> 00:14:25,300
automatically extend the existing post

00:14:23,560 --> 00:14:28,300
Chris Excel cluster will by inserting

00:14:25,300 --> 00:14:30,010
power nodes into the data node and this

00:14:28,300 --> 00:14:33,240
just works you know this is something

00:14:30,010 --> 00:14:36,459
that really just worked and so we can

00:14:33,240 --> 00:14:38,740
bring a smooth transition to an increase

00:14:36,459 --> 00:14:40,810
in the power using power servers and of

00:14:38,740 --> 00:14:42,940
course because power we are everybody's

00:14:40,810 --> 00:14:44,440
pushing power here so this is where we

00:14:42,940 --> 00:14:47,050
want to get to you know it provides a

00:14:44,440 --> 00:14:49,630
really smooth transition to get people

00:14:47,050 --> 00:14:52,829
off of x86 and into power with a with a

00:14:49,630 --> 00:14:56,230
nonstop no disruption to the service

00:14:52,829 --> 00:15:02,550
whatsoever and this is clearly well with

00:14:56,230 --> 00:15:05,529
China so why now now that we we've we've

00:15:02,550 --> 00:15:08,079
said you know we can do all this stuff

00:15:05,529 --> 00:15:10,149
why would anybody go to power and this

00:15:08,079 --> 00:15:12,100
brings me the second part of the the

00:15:10,149 --> 00:15:15,459
pitch which is we've been doing a lot of

00:15:12,100 --> 00:15:17,920
work on Postgres benchmarks on on power

00:15:15,459 --> 00:15:20,230
9 and I'd like to share those results

00:15:17,920 --> 00:15:22,480
with you which are really hot off the

00:15:20,230 --> 00:15:26,620
press I got the last resource yesterday

00:15:22,480 --> 00:15:30,430
evening so this really is last-minute

00:15:26,620 --> 00:15:32,710
stuff so we're using here is a PV bench

00:15:30,430 --> 00:15:34,930
PG bench which is a

00:15:32,710 --> 00:15:36,610
a benchmarking tool a TP CB type

00:15:34,930 --> 00:15:38,860
benchmarking tool it's a relatively

00:15:36,610 --> 00:15:41,160
simple transactional workload which

00:15:38,860 --> 00:15:43,960
comes bundled with the Postgres database

00:15:41,160 --> 00:15:45,910
so it's a transactional client and

00:15:43,960 --> 00:15:49,060
everybody in the Postgres world uses

00:15:45,910 --> 00:15:51,220
this tool okay we're working on using a

00:15:49,060 --> 00:15:53,380
next campaign is going to be TPC cpc-h

00:15:51,220 --> 00:15:55,660
based which is a decisional workload and

00:15:53,380 --> 00:15:57,580
we're gonna hopefully have some nice

00:15:55,660 --> 00:15:59,170
numbers in that in the short-term but

00:15:57,580 --> 00:16:02,950
for the time being this is the one time

00:15:59,170 --> 00:16:04,000
have for you the transaction the two

00:16:02,950 --> 00:16:05,410
modes of running this thing there's a

00:16:04,000 --> 00:16:11,320
read-only and as a readwrite function

00:16:05,410 --> 00:16:13,690
and the read/write uses five five SQL

00:16:11,320 --> 00:16:15,520
queries with a select updating insert

00:16:13,690 --> 00:16:16,900
and then there's a there's a read-only

00:16:15,520 --> 00:16:20,290
option which just uses select on the

00:16:16,900 --> 00:16:22,480
tables and we can multiply those

00:16:20,290 --> 00:16:24,160
multiple clients I'm afraid I'd have to

00:16:22,480 --> 00:16:26,230
get into more of the details here a

00:16:24,160 --> 00:16:27,520
little bit but they have this thing

00:16:26,230 --> 00:16:29,620
called the scaling factor so this is the

00:16:27,520 --> 00:16:32,230
size of the this is the database tables

00:16:29,620 --> 00:16:34,330
there are just four tables the branches

00:16:32,230 --> 00:16:35,320
the tellers the accounts and the history

00:16:34,330 --> 00:16:38,950
so this is obviously a banking

00:16:35,320 --> 00:16:41,290
application there's what and by default

00:16:38,950 --> 00:16:43,630
this is the scale of one and so there is

00:16:41,290 --> 00:16:45,100
one branch there are ten tellers in the

00:16:43,630 --> 00:16:48,160
branch and there are a hundred thousand

00:16:45,100 --> 00:16:49,660
accounts and the history is just the log

00:16:48,160 --> 00:16:52,090
of the upper transactions that we've

00:16:49,660 --> 00:16:55,180
committed to the database but nobody

00:16:52,090 --> 00:16:58,360
runs this database on scale factor one

00:16:55,180 --> 00:17:00,220
because it's just too small today these

00:16:58,360 --> 00:17:02,800
are the more typical numbers so we use a

00:17:00,220 --> 00:17:06,370
scale factor of a hundred and this just

00:17:02,800 --> 00:17:10,000
multiplies the number of lines in in in

00:17:06,370 --> 00:17:13,089
the accounts table by the same scale so

00:17:10,000 --> 00:17:16,150
a scale of 100 is 10 million lines which

00:17:13,089 --> 00:17:18,400
is a 1.5 gigabyte database but more

00:17:16,150 --> 00:17:20,230
typically we find three thousand or two

00:17:18,400 --> 00:17:23,890
and a half thousand so we're looking at

00:17:20,230 --> 00:17:25,660
a 45 gigabyte database which is not huge

00:17:23,890 --> 00:17:29,980
but it's starting to get a reasonably

00:17:25,660 --> 00:17:32,650
reasonably sized database this is the

00:17:29,980 --> 00:17:38,170
set that we have this is using a power

00:17:32,650 --> 00:17:40,900
and power 9 using Red Hat 7.5 14.4 and

00:17:38,170 --> 00:17:45,280
our 14.4 is important if anybody's using

00:17:40,900 --> 00:17:46,410
deploying Red Hat we had 4.14 we used an

00:17:45,280 --> 00:17:49,590
earlier version before

00:17:46,410 --> 00:17:54,030
and there was a huge performance gain

00:17:49,590 --> 00:17:56,550
moving to 414 it was something like 30

00:17:54,030 --> 00:17:59,610
or 40% it was it was a big big

00:17:56,550 --> 00:18:00,810
difference and it wasn't the only change

00:17:59,610 --> 00:18:03,990
we made but it's the only one that we

00:18:00,810 --> 00:18:07,890
can sort of think that might explain the

00:18:03,990 --> 00:18:09,390
difference week we got so you have to be

00:18:07,890 --> 00:18:11,070
very careful about the version you

00:18:09,390 --> 00:18:13,650
download from Red Hat it's not obvious

00:18:11,070 --> 00:18:15,960
when you go to the Red Hat site to get

00:18:13,650 --> 00:18:18,750
the right version so we have the

00:18:15,960 --> 00:18:20,340
injectors coming in and then we have two

00:18:18,750 --> 00:18:24,530
systems due to storage systems I can use

00:18:20,340 --> 00:18:28,410
the have an EMC unity 300 and all the

00:18:24,530 --> 00:18:31,020
onboard nvme okay and in terms of the

00:18:28,410 --> 00:18:34,950
the operations they're very close in

00:18:31,020 --> 00:18:38,610
terms of performance so we're running

00:18:34,950 --> 00:18:41,070
the bench scale of 1,000 with well used

00:18:38,610 --> 00:18:42,690
different scales with the 15 and we're

00:18:41,070 --> 00:18:44,640
using the Select on me so here's the

00:18:42,690 --> 00:18:46,830
picture of on the EMC machine just to

00:18:44,640 --> 00:18:48,930
show how fast this stuff really runs so

00:18:46,830 --> 00:18:52,500
I won't go into details here but this is

00:18:48,930 --> 00:18:54,120
the AR the bandwidth and joined the

00:18:52,500 --> 00:18:56,000
database load we completely saturated

00:18:54,120 --> 00:18:58,740
anything a gigabit fiber channel

00:18:56,000 --> 00:19:01,440
connection and so we've now gone to dual

00:18:58,740 --> 00:19:04,950
16 gigabit connection to the back to the

00:19:01,440 --> 00:19:07,590
disk array because so here we are half

00:19:04,950 --> 00:19:11,550
the press other numbers we got so this

00:19:07,590 --> 00:19:15,330
is a 14 core with 40 Q bytes of RAM and

00:19:11,550 --> 00:19:20,370
we are around about 600 so for the scale

00:19:15,330 --> 00:19:23,730
of 1000 which we are at six hundred

00:19:20,370 --> 00:19:25,560
thousand six hundred ten thousand

00:19:23,730 --> 00:19:27,720
transactions per minute second six

00:19:25,560 --> 00:19:31,140
hundred and ten thousand transactions

00:19:27,720 --> 00:19:33,090
per second which is just a you know it's

00:19:31,140 --> 00:19:35,580
a huge huge number this is a read-only

00:19:33,090 --> 00:19:37,890
so we're not dependent on i/o here

00:19:35,580 --> 00:19:41,880
there's no I know everything is in

00:19:37,890 --> 00:19:44,490
memory which is to me that's a huge

00:19:41,880 --> 00:19:47,070
number but let's see how it really

00:19:44,490 --> 00:19:48,780
compares so I had a look around in the

00:19:47,070 --> 00:19:50,160
in the publications and the best and

00:19:48,780 --> 00:19:52,560
some of the best numbers I found this

00:19:50,160 --> 00:19:54,270
one came from Enterprise dB

00:19:52,560 --> 00:19:55,620
I'm afraid it's not strictly an apples

00:19:54,270 --> 00:19:57,240
to apples comparison because they're

00:19:55,620 --> 00:19:59,880
using a slightly older version of the

00:19:57,240 --> 00:20:03,890
database but this is

00:19:59,880 --> 00:20:08,340
running on 60 cause 60 xenon cause and

00:20:03,890 --> 00:20:10,559
with 512 gigabytes of RAM so way way way

00:20:08,340 --> 00:20:14,130
bigger configuration and they reached

00:20:10,559 --> 00:20:15,600
700,000 and so our ambition just to be

00:20:14,130 --> 00:20:18,179
clear here our ambition is to reach a

00:20:15,600 --> 00:20:20,370
million we're not there yet but we want

00:20:18,179 --> 00:20:22,289
to get to the million but this is the

00:20:20,370 --> 00:20:23,520
best number we got from on the x86 the

00:20:22,289 --> 00:20:24,929
best one I could find I'm not saying

00:20:23,520 --> 00:20:29,760
that is the best this is the best one I

00:20:24,929 --> 00:20:33,809
found in the publications on 60 core

00:20:29,760 --> 00:20:35,100
sixty cores with a Z on and then IBM in

00:20:33,809 --> 00:20:39,659
Montpellier in the South of France

00:20:35,100 --> 00:20:42,390
worked with splendid data to do a

00:20:39,659 --> 00:20:44,940
benchmark on power8 and this is the

00:20:42,390 --> 00:20:49,289
numbers they got so there they got four

00:20:44,940 --> 00:20:52,200
hundred and twenty thousand transactions

00:20:49,289 --> 00:20:53,820
per second so this is on power eight so

00:20:52,200 --> 00:20:54,809
this was Sebastian chaparral I don't

00:20:53,820 --> 00:20:57,929
know if he's actually quite well-known

00:20:54,809 --> 00:21:00,299
inside IBM running with Miss binding

00:20:57,929 --> 00:21:04,020
data and again they had way more way

00:21:00,299 --> 00:21:06,270
more cores and and they had much bigger

00:21:04,020 --> 00:21:07,590
Ram them we were put into our system but

00:21:06,270 --> 00:21:09,240
in truth they don't need all that Ram

00:21:07,590 --> 00:21:13,250
you know even even the first one they

00:21:09,240 --> 00:21:18,049
don't need 512 gigabytes of data to it

00:21:13,250 --> 00:21:23,549
so splendid data is a Belgian company

00:21:18,049 --> 00:21:25,289
who specialized in migrating Oracle

00:21:23,549 --> 00:21:27,510
databases to Postgres they have a whole

00:21:25,289 --> 00:21:31,760
tool chain and a methodology to do that

00:21:27,510 --> 00:21:35,900
and Atos partners with splendid data

00:21:31,760 --> 00:21:35,900
they have similar to

00:21:37,380 --> 00:21:43,360
now enterprise-d be enterprise-d be have

00:21:40,870 --> 00:21:46,840
this compatibility cochere saris idea of

00:21:43,360 --> 00:21:48,040
migrating from Oracle to Postgres so if

00:21:46,840 --> 00:21:49,450
you bring this so that went back to my

00:21:48,040 --> 00:21:50,920
original question is why do we need to

00:21:49,450 --> 00:21:54,610
go white we know when to run you always

00:21:50,920 --> 00:21:56,920
back up stuff and scale out why is power

00:21:54,610 --> 00:21:59,110
a good story here and this is this for

00:21:56,920 --> 00:22:01,390
me the really the case why it shows you

00:21:59,110 --> 00:22:07,450
so if we look at the transactions per

00:22:01,390 --> 00:22:11,140
second per core then Intel way around

00:22:07,450 --> 00:22:13,690
about eleven thousand eleven thousand

00:22:11,140 --> 00:22:15,700
transactions per second per core this is

00:22:13,690 --> 00:22:17,260
Sebastian shab role on power eight

00:22:15,700 --> 00:22:19,780
they reached around two hundred and

00:22:17,260 --> 00:22:23,020
fifty thousand transactions per second

00:22:19,780 --> 00:22:27,970
per core and on power nine

00:22:23,020 --> 00:22:31,570
we're almost at we're over 400,000 volts

00:22:27,970 --> 00:22:34,480
sorry 40,000 so we're a factor of four

00:22:31,570 --> 00:22:38,520
times better per core of this and the

00:22:34,480 --> 00:22:40,840
important thing here is that all of

00:22:38,520 --> 00:22:44,620
these sort of middleware and any

00:22:40,840 --> 00:22:46,570
application software suffer with

00:22:44,620 --> 00:22:49,230
scalability at high levels of

00:22:46,570 --> 00:22:52,900
parallelism yeah there's a there's the

00:22:49,230 --> 00:22:54,970
who can help me that the parallelization

00:22:52,900 --> 00:22:56,320
effect you can only paralyze a certain

00:22:54,970 --> 00:22:59,250
amount than the sequential part becomes

00:22:56,320 --> 00:23:01,540
dominant I've forgotten the guy's name

00:22:59,250 --> 00:23:04,270
but I am doubt thingy I thank you very

00:23:01,540 --> 00:23:07,270
much I'm dealt the UM dolly law which

00:23:04,270 --> 00:23:11,440
means that we can't you know when you

00:23:07,270 --> 00:23:12,310
get very high orders of threading or

00:23:11,440 --> 00:23:16,690
paralyzation

00:23:12,310 --> 00:23:18,640
then scalability starts to suffer and so

00:23:16,690 --> 00:23:21,280
when the importance of the power per

00:23:18,640 --> 00:23:23,230
core really does come into come into

00:23:21,280 --> 00:23:26,950
play here and it's a big big fact factor

00:23:23,230 --> 00:23:28,510
okay so and this isn't as I say an

00:23:26,950 --> 00:23:29,890
apples to apples comparison strictly

00:23:28,510 --> 00:23:33,010
speaking because the versions the

00:23:29,890 --> 00:23:35,290
database aren't quite the same but you

00:23:33,010 --> 00:23:37,150
can see that in order to get these

00:23:35,290 --> 00:23:39,130
numbers that you know very close numbers

00:23:37,150 --> 00:23:41,440
we use just fourteen cause we use just a

00:23:39,130 --> 00:23:42,970
quarter of the cause that they need to

00:23:41,440 --> 00:23:47,200
make 56 in order to get the stuff to

00:23:42,970 --> 00:23:49,150
work this was the read right just for

00:23:47,200 --> 00:23:50,740
completeness this is the read right

00:23:49,150 --> 00:23:53,160
scale so this is a

00:23:50,740 --> 00:23:55,720
read/write we obviously much harder and

00:23:53,160 --> 00:23:57,940
so that we divide roughly by a factor of

00:23:55,720 --> 00:23:59,590
ten in terms of the the transactions per

00:23:57,940 --> 00:24:01,210
second one of the things we cannot

00:23:59,590 --> 00:24:02,559
explain is we expect to see this curve

00:24:01,210 --> 00:24:04,660
come off of the back of hundred twenty

00:24:02,559 --> 00:24:05,950
eight I absolutely no idea why this goes

00:24:04,660 --> 00:24:07,780
back up one hundred and forty-four I

00:24:05,950 --> 00:24:10,270
don't there's a cash effect or something

00:24:07,780 --> 00:24:11,380
that something funky happens this is a

00:24:10,270 --> 00:24:13,809
bit strange that needs a bit more

00:24:11,380 --> 00:24:18,070
investigating and just to show you the

00:24:13,809 --> 00:24:22,690
effect of of SMT because we can switch

00:24:18,070 --> 00:24:24,190
SMT 8 and SMT 4 on this processor this

00:24:22,690 --> 00:24:25,960
is one of the bigger processes not the

00:24:24,190 --> 00:24:28,740
ones we tend to get on the loose on

00:24:25,960 --> 00:24:32,080
power servers is that we get a

00:24:28,740 --> 00:24:35,080
comfortable 25% performance increase

00:24:32,080 --> 00:24:37,390
moving to SMT 874 and and just as a side

00:24:35,080 --> 00:24:38,800
note on SMT compared to Intel

00:24:37,390 --> 00:24:40,630
hyper-threading there's a very clear

00:24:38,800 --> 00:24:42,179
difference between the SMT on power

00:24:40,630 --> 00:24:44,830
compared to high Intel hyper-threading

00:24:42,179 --> 00:24:46,870
Intel's hyper threading is simply a very

00:24:44,830 --> 00:24:48,640
rapid context which at any given time

00:24:46,870 --> 00:24:53,290
there is only one thread running on the

00:24:48,640 --> 00:24:55,090
processor on SMT on power The Dispatch

00:24:53,290 --> 00:24:57,670
site the dispatcher on the processor

00:24:55,090 --> 00:24:59,500
dispatches instructions from any of the

00:24:57,670 --> 00:25:01,530
active thread ok so and if it's got

00:24:59,500 --> 00:25:03,100
eight threads it can dispatch

00:25:01,530 --> 00:25:04,420
instructions from any of the eight

00:25:03,100 --> 00:25:08,020
threads during this patch and so it

00:25:04,420 --> 00:25:13,150
really are a up to eight threads running

00:25:08,020 --> 00:25:17,620
in power on the processor so the idea

00:25:13,150 --> 00:25:20,590
here is that Atos that's me

00:25:17,620 --> 00:25:23,070
with our scholar platform we rebrand IBM

00:25:20,590 --> 00:25:25,510
power systems as the Escala platform

00:25:23,070 --> 00:25:27,850
that's our name from and our partner

00:25:25,510 --> 00:25:30,160
with splendid data we really can make

00:25:27,850 --> 00:25:32,260
everything supply because they can fly

00:25:30,160 --> 00:25:34,630
in in the sense that we can they go

00:25:32,260 --> 00:25:38,530
extremely fast and we can make them fly

00:25:34,630 --> 00:25:42,850
between x86 and power we have I think we

00:25:38,530 --> 00:25:45,790
have a very convincing story here and so

00:25:42,850 --> 00:25:46,390
I think this is a sign we've all been

00:25:45,790 --> 00:25:49,809
waiting for

00:25:46,390 --> 00:25:52,120
you know we really have to get out there

00:25:49,809 --> 00:25:55,929
and start telling our customers that

00:25:52,120 --> 00:25:58,480
power is a viable alternative to x86 and

00:25:55,929 --> 00:26:00,970
there was a lot of advantages using this

00:25:58,480 --> 00:26:02,500
platform some of the things I didn't

00:26:00,970 --> 00:26:04,179
really have time to get into in the

00:26:02,500 --> 00:26:06,730
numbers is that under

00:26:04,179 --> 00:26:10,360
extremely high loads the the response

00:26:06,730 --> 00:26:12,940
times that we get on Power Platform a

00:26:10,360 --> 00:26:15,700
much less variable than the response

00:26:12,940 --> 00:26:18,279
times you get on x86 yeah though the the

00:26:15,700 --> 00:26:20,769
variability of the response times is

00:26:18,279 --> 00:26:22,629
extremely stable or on power compared to

00:26:20,769 --> 00:26:25,629
x86 you know there are a lot of

00:26:22,629 --> 00:26:28,149
advantages to go with this platform I'd

00:26:25,629 --> 00:26:30,369
be happy to take any questions talk

00:26:28,149 --> 00:26:32,559
about offline with anybody any anything

00:26:30,369 --> 00:26:35,309
in particular but that's it for me thank

00:26:32,559 --> 00:26:35,309

YouTube URL: https://www.youtube.com/watch?v=0RWpJIjQTms


