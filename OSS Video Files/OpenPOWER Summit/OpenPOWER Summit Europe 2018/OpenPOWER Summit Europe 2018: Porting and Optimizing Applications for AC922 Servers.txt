Title: OpenPOWER Summit Europe 2018: Porting and Optimizing Applications for AC922 Servers
Publication date: 2019-02-07
Playlist: OpenPOWER Summit Europe 2018
Description: 
	Leopold Grinberg, IBM Research, speaks at OpenPOWER Summit Europe 2018.

For more information, please visit: https://openpowerfoundation.org/summit-2018-10-eu/
Captions: 
	00:00:00,829 --> 00:00:09,929
so AC 922 we have multiple memory pools

00:00:07,350 --> 00:00:12,570
even if you have unified addressing the

00:00:09,929 --> 00:00:15,990
physical memory still distributed each

00:00:12,570 --> 00:00:17,400
GPU has its own HP a memory 16 gigabyte

00:00:15,990 --> 00:00:21,410
in this case

00:00:17,400 --> 00:00:28,349
so it's other 4 6 pools of HP a memory

00:00:21,410 --> 00:00:32,700
CPU has dependent system some it has 512

00:00:28,349 --> 00:00:35,730
gigabyte ciara 256 gigabytes split even

00:00:32,700 --> 00:00:38,520
between the two sockets so again there

00:00:35,730 --> 00:00:40,440
is some distribution of the memory what

00:00:38,520 --> 00:00:43,890
I don't show here don't mention is that

00:00:40,440 --> 00:00:45,480
there is additional memory pool mostly

00:00:43,890 --> 00:00:47,789
used for checkpointing but actually can

00:00:45,480 --> 00:00:50,820
also be used as extended memories 1.6

00:00:47,789 --> 00:00:52,800
terabyte of nvme we are not going to

00:00:50,820 --> 00:00:54,750
talk about this but it's another

00:00:52,800 --> 00:00:59,539
complication for those who really want

00:00:54,750 --> 00:01:01,620
to use all three levels of memory and

00:00:59,539 --> 00:01:04,530
overall they're about 1 million of

00:01:01,620 --> 00:01:08,369
threads that can run concurrently on a a

00:01:04,530 --> 00:01:11,130
c 922 i will show you how did they get

00:01:08,369 --> 00:01:12,689
this number but the point is that you

00:01:11,130 --> 00:01:14,909
have a number of challenges one is

00:01:12,689 --> 00:01:19,200
managing the data managing the memory

00:01:14,909 --> 00:01:22,200
and we heard several times during talks

00:01:19,200 --> 00:01:25,619
today that it's really important to move

00:01:22,200 --> 00:01:27,000
the data to the right compute devices at

00:01:25,619 --> 00:01:29,790
the right time to get the performance

00:01:27,000 --> 00:01:32,790
and you need to manage the execution

00:01:29,790 --> 00:01:35,159
policy who is computing CPU GPU which

00:01:32,790 --> 00:01:39,060
GPU which CPU which core how many cores

00:01:35,159 --> 00:01:40,530
and you really need to take care of the

00:01:39,060 --> 00:01:42,450
parallelism because there is a lot of

00:01:40,530 --> 00:01:45,119
parallelism at different levels to think

00:01:42,450 --> 00:01:47,220
about two sockets in each socket you

00:01:45,119 --> 00:01:49,170
have two or three GPUs on each GPU have

00:01:47,220 --> 00:01:52,320
number of SMS number of blocks number of

00:01:49,170 --> 00:01:53,729
threads on the CPU have coarse a lot of

00:01:52,320 --> 00:01:56,450
power lines that can be expressed as

00:01:53,729 --> 00:01:59,579
nested parallelism flat parallelism and

00:01:56,450 --> 00:02:02,840
depending how you take care of this you

00:01:59,579 --> 00:02:05,790
get either good or bad performance so

00:02:02,840 --> 00:02:08,069
when you think about data management and

00:02:05,790 --> 00:02:10,229
expression parallelism you also want you

00:02:08,069 --> 00:02:12,120
have your code that you develop for this

00:02:10,229 --> 00:02:12,970
system to be able to run on different

00:02:12,120 --> 00:02:15,430
systems

00:02:12,970 --> 00:02:19,270
because this is what most customers do

00:02:15,430 --> 00:02:21,400
they don't care just about AC 9:22 they

00:02:19,270 --> 00:02:24,400
also run on Intel system on system is MD

00:02:21,400 --> 00:02:27,040
CPUs GPUs and so on so I really want to

00:02:24,400 --> 00:02:30,730
design portable and performance portable

00:02:27,040 --> 00:02:32,200
codes not just code tailored for a

00:02:30,730 --> 00:02:36,370
particular system so we will talk about

00:02:32,200 --> 00:02:38,950
this the factors that help us quite a

00:02:36,370 --> 00:02:41,680
lot in design portable and performance

00:02:38,950 --> 00:02:45,220
portable code bases these two factors

00:02:41,680 --> 00:02:49,260
one is unified addressing and the second

00:02:45,220 --> 00:02:52,030
factor is the compiler directive and

00:02:49,260 --> 00:02:54,490
this will be most of my talk will be

00:02:52,030 --> 00:02:59,290
around the compiler directives openmp

00:02:54,490 --> 00:03:01,210
that we use to program CPUs and GPUs and

00:02:59,290 --> 00:03:04,660
I will show a number of examples

00:03:01,210 --> 00:03:07,980
actually pieces of course that will

00:03:04,660 --> 00:03:11,500
describe how we manage memory and data

00:03:07,980 --> 00:03:13,270
nested data structures nested

00:03:11,500 --> 00:03:16,959
parallelism some examples from

00:03:13,270 --> 00:03:20,380
application that we worked on for summit

00:03:16,959 --> 00:03:23,200
and sierra computers and so on and

00:03:20,380 --> 00:03:28,930
please do ask me questions whenever you

00:03:23,200 --> 00:03:33,400
have them so parallelism to power 9

00:03:28,930 --> 00:03:35,140
processors 22 cores for users each core

00:03:33,400 --> 00:03:36,880
has four Hardware threads so you can

00:03:35,140 --> 00:03:41,440
compute how many threads are there in

00:03:36,880 --> 00:03:46,110
total a Volta 100 GPUs are the four six

00:03:41,440 --> 00:03:49,950
GPUs 80 SMS in each 2048 threads per SM

00:03:46,110 --> 00:03:57,820
so here you see how capable these

00:03:49,950 --> 00:03:59,290
compute devices are so if you calculate

00:03:57,820 --> 00:04:01,180
the total number of threads you arrive

00:03:59,290 --> 00:04:03,190
to about 1 million of thread that you

00:04:01,180 --> 00:04:05,080
want to use on the system to squeeze

00:04:03,190 --> 00:04:07,660
every single flop and every single

00:04:05,080 --> 00:04:09,730
gigabyte per second of bandwidth from

00:04:07,660 --> 00:04:14,500
the system so your investment actually

00:04:09,730 --> 00:04:17,709
pays off memories this is the view of

00:04:14,500 --> 00:04:19,239
memory that I have as a developer

00:04:17,709 --> 00:04:22,030
application developer so we have two

00:04:19,239 --> 00:04:24,340
sockets we have this weak link between

00:04:22,030 --> 00:04:26,410
their sockets in one of the talks this

00:04:24,340 --> 00:04:28,570
morning this it was mentioned that

00:04:26,410 --> 00:04:31,870
if you use the GPU from a different

00:04:28,570 --> 00:04:33,910
socket your men copy or kudamon copy

00:04:31,870 --> 00:04:35,680
memory bandages notice what you would

00:04:33,910 --> 00:04:38,080
expect from any link yes of course

00:04:35,680 --> 00:04:39,640
because you go over the bus it doesn't

00:04:38,080 --> 00:04:44,470
give you a lot of bandits it's basically

00:04:39,640 --> 00:04:48,850
designed to to move some operating

00:04:44,470 --> 00:04:52,450
system traffic over the bus and some

00:04:48,850 --> 00:04:54,880
communication when you go from the node

00:04:52,450 --> 00:04:56,890
to another node so most of the

00:04:54,880 --> 00:05:00,070
connectivity between the memories

00:04:56,890 --> 00:05:03,340
happening inside each socket so all

00:05:00,070 --> 00:05:07,000
three GPUs or two GPUs in case of C are

00:05:03,340 --> 00:05:10,620
computer there are interconnected so in

00:05:07,000 --> 00:05:15,520
this configuration for example we have

00:05:10,620 --> 00:05:18,640
total of six bricks for NV link on the

00:05:15,520 --> 00:05:22,270
CPU side so we split those between three

00:05:18,640 --> 00:05:25,270
or two GPUs or either three bricks and

00:05:22,270 --> 00:05:27,730
is how you get 75 gigabyte per second

00:05:25,270 --> 00:05:32,650
unidirectional memory Bennett's OB

00:05:27,730 --> 00:05:34,750
Direction 150 per GPU or on a machine

00:05:32,650 --> 00:05:37,090
with three GPUs you have two breaks per

00:05:34,750 --> 00:05:40,320
GPU so it will be 50 gigabyte per second

00:05:37,090 --> 00:05:40,320
in each direction per GPU

00:05:40,530 --> 00:05:45,880
so when you design your application you

00:05:43,750 --> 00:05:48,040
also need to take care about moving data

00:05:45,880 --> 00:05:50,160
so you want to move it into the right

00:05:48,040 --> 00:05:55,360
place into the right in the right time

00:05:50,160 --> 00:05:57,790
and as effectively as you can there are

00:05:55,360 --> 00:06:00,370
a number of options to program this

00:05:57,790 --> 00:06:04,450
machine so obviously for CPU we can use

00:06:00,370 --> 00:06:08,230
a new compiler PGI compiler Excel

00:06:04,450 --> 00:06:09,760
compiler clang compiler for CPU we have

00:06:08,230 --> 00:06:13,810
a number of options as well we can use

00:06:09,760 --> 00:06:19,860
CUDA it coming from Nvidia we have some

00:06:13,810 --> 00:06:24,040
support for CUDA also in Excel Fortran a

00:06:19,860 --> 00:06:27,850
we can use open ACC supported by PGI

00:06:24,040 --> 00:06:30,400
compiler and we can use open MP so using

00:06:27,850 --> 00:06:34,720
opening P for GPUs this kind of new

00:06:30,400 --> 00:06:37,419
things it's here for about maybe three

00:06:34,720 --> 00:06:40,240
years starting from open MP 4.0 standard

00:06:37,419 --> 00:06:42,849
nowadays we have open MP 4 point

00:06:40,240 --> 00:06:47,770
five standard it advanced quite a lot

00:06:42,849 --> 00:06:51,400
and opening P 5.0 standard is something

00:06:47,770 --> 00:06:54,280
that coming soon actually a lot of

00:06:51,400 --> 00:06:57,669
people who are in this opening P

00:06:54,280 --> 00:07:00,699
standard committee work for IBM and work

00:06:57,669 --> 00:07:02,470
for National Labs or a lot of experience

00:07:00,699 --> 00:07:05,250
that we got from working on the

00:07:02,470 --> 00:07:09,220
applications ended up in proposing new

00:07:05,250 --> 00:07:12,940
standards for the OpenMP so I will talk

00:07:09,220 --> 00:07:15,250
about the OpenMP so why use opening P

00:07:12,940 --> 00:07:17,229
okay so let's take this example or we

00:07:15,250 --> 00:07:19,120
have a very simple for loop attacks per

00:07:17,229 --> 00:07:21,819
application this is sequential

00:07:19,120 --> 00:07:23,500
implementation just you know nothing

00:07:21,819 --> 00:07:28,240
sophisticated nothing fancy here just

00:07:23,500 --> 00:07:30,419
for loop y equal a times X plus y and I

00:07:28,240 --> 00:07:33,190
can take this loop and incrementally

00:07:30,419 --> 00:07:36,009
improve their performance or speed of

00:07:33,190 --> 00:07:38,800
calculation by first creating threads on

00:07:36,009 --> 00:07:41,889
the CPU so now I can use multiple cores

00:07:38,800 --> 00:07:45,430
maybe multiple threads per core to run

00:07:41,889 --> 00:07:48,159
this loop in parallel next step I can

00:07:45,430 --> 00:07:51,400
create nested parallelism using teams

00:07:48,159 --> 00:07:53,919
and parallel four and still run it on

00:07:51,400 --> 00:07:56,169
the CPU and next step I cannot flow the

00:07:53,919 --> 00:07:57,610
computation to the GPU so you see I

00:07:56,169 --> 00:08:00,430
increase the number of compiler

00:07:57,610 --> 00:08:03,310
directives here and I will explain a

00:08:00,430 --> 00:08:06,849
little bit about almost all of them what

00:08:03,310 --> 00:08:09,340
they exactly do and and then incremental

00:08:06,849 --> 00:08:12,849
I move my the execution of this loop

00:08:09,340 --> 00:08:16,599
from the CPU to the GPU and if I switch

00:08:12,849 --> 00:08:19,630
for example inside this if target one if

00:08:16,599 --> 00:08:22,900
I switch one to zero I will compute on

00:08:19,630 --> 00:08:24,880
the CPU so the runtime actually can

00:08:22,900 --> 00:08:27,190
switch the execution from the CPU to the

00:08:24,880 --> 00:08:29,770
GPU depending maybe on the size of the

00:08:27,190 --> 00:08:31,449
problem that going to solve so if I need

00:08:29,770 --> 00:08:34,419
to adjust two doubles I'm not going to

00:08:31,449 --> 00:08:36,339
lunch occurring on the GPU if I'm

00:08:34,419 --> 00:08:40,690
running DGM for example I better go to

00:08:36,339 --> 00:08:43,810
the GPU and so on so OpenMP gives a lot

00:08:40,690 --> 00:08:47,350
of flexibility and it hides a lot of

00:08:43,810 --> 00:08:50,560
complexity from the user in order to

00:08:47,350 --> 00:08:52,900
offload a some calculation from the CPU

00:08:50,560 --> 00:08:53,870
to the GPU for example in this one line

00:08:52,900 --> 00:08:56,900
of directives

00:08:53,870 --> 00:08:59,570
we do we allocate memory on the CPU I'm

00:08:56,900 --> 00:09:03,200
sorry on the GPU we move data to the GPU

00:08:59,570 --> 00:09:07,700
so at least two API calls could Malakand

00:09:03,200 --> 00:09:09,980
could mem copy we launch a kernel on the

00:09:07,700 --> 00:09:13,130
GPU we express parallelism for the GPU

00:09:09,980 --> 00:09:16,190
how many CUDA blocks how many FS per

00:09:13,130 --> 00:09:19,730
block not used and at the end we also

00:09:16,190 --> 00:09:24,350
move data back from the GPU to the CPU

00:09:19,730 --> 00:09:27,320
and delete the memory allocated on the

00:09:24,350 --> 00:09:30,730
GPU so it's a lot of operation that we

00:09:27,320 --> 00:09:33,050
express in a single line of directives

00:09:30,730 --> 00:09:35,779
now in terms of performance do we

00:09:33,050 --> 00:09:38,180
achieve good performance using

00:09:35,779 --> 00:09:41,600
directives as we would achieve typically

00:09:38,180 --> 00:09:43,520
using CUDA and working somewhat harder

00:09:41,600 --> 00:09:47,690
on the applications in general the

00:09:43,520 --> 00:09:49,400
answer is yes and here for gonna show

00:09:47,690 --> 00:09:53,390
results that we obtained on Gminski

00:09:49,400 --> 00:09:56,150
system so it's power 8 + 4 Pascal GPUs

00:09:53,390 --> 00:09:58,250
you see this factor of 12 in terms of

00:09:56,150 --> 00:10:00,440
the speed of so this small s the factor

00:09:58,250 --> 00:10:03,410
that we expect to get for memory

00:10:00,440 --> 00:10:06,830
bandwidth limited applications right so

00:10:03,410 --> 00:10:12,140
we compared the Bennis of 2 power 8 CPUs

00:10:06,830 --> 00:10:18,080
with bennett's of a 4 Pascal GPUs we get

00:10:12,140 --> 00:10:20,690
about factor of 12 so opening P 4 point

00:10:18,080 --> 00:10:25,010
5 open before point 5 can actually take

00:10:20,690 --> 00:10:27,250
care of a lot of challenges that we face

00:10:25,010 --> 00:10:30,050
when we program so memory management

00:10:27,250 --> 00:10:33,080
definition of execution space managing

00:10:30,050 --> 00:10:34,310
data nested power region coarse grained

00:10:33,080 --> 00:10:36,440
parallelism fine grained parallelism

00:10:34,310 --> 00:10:38,540
internal communication for example few

00:10:36,440 --> 00:10:42,680
meaning to move the data between the CPU

00:10:38,540 --> 00:10:44,660
and GPU or different GPUs it doesn't

00:10:42,680 --> 00:10:46,700
take care of internal communication so

00:10:44,660 --> 00:10:48,980
for this we typically use MPI or message

00:10:46,700 --> 00:10:54,709
passing interface so opening piece out

00:10:48,980 --> 00:10:56,630
of the question there right so I

00:10:54,709 --> 00:10:59,420
mentioned several times managing memory

00:10:56,630 --> 00:11:00,620
and managing data and in my view this is

00:10:59,420 --> 00:11:02,060
not the same thing

00:11:00,620 --> 00:11:04,730
right so why it's not the same thing

00:11:02,060 --> 00:11:07,070
because managing memory for me at least

00:11:04,730 --> 00:11:08,060
it means we need to allocate the

00:11:07,070 --> 00:11:10,580
allocate

00:11:08,060 --> 00:11:15,170
at this step we don't care what data is

00:11:10,580 --> 00:11:17,660
in this memory regions we may use memory

00:11:15,170 --> 00:11:19,880
pools so we can pre allocate memory and

00:11:17,660 --> 00:11:23,150
then place different data into these

00:11:19,880 --> 00:11:26,330
memory pools we can use unified

00:11:23,150 --> 00:11:28,850
addressing and in this case we'll have

00:11:26,330 --> 00:11:30,980
just single pointer that will point to

00:11:28,850 --> 00:11:33,620
some memory buffer that can reside on

00:11:30,980 --> 00:11:37,250
other CPU on the GPU and be visible from

00:11:33,620 --> 00:11:39,380
both sides and we can also provide some

00:11:37,250 --> 00:11:41,060
hints that we may prevent some page

00:11:39,380 --> 00:11:44,030
migration for example from one Numa

00:11:41,060 --> 00:11:47,780
domain to another or from one CPU to a

00:11:44,030 --> 00:11:51,830
GPU or from one GPU to another GPU now

00:11:47,780 --> 00:11:54,320
managing data so once we have memory for

00:11:51,830 --> 00:11:56,270
example we can place different data into

00:11:54,320 --> 00:11:59,900
this memory right so at one stage of the

00:11:56,270 --> 00:12:01,460
code I can place some variables in other

00:11:59,900 --> 00:12:03,860
stage of the code they will just reuse

00:12:01,460 --> 00:12:06,470
this memory place in a level right when

00:12:03,860 --> 00:12:10,820
we replicate the data on the CP on the

00:12:06,470 --> 00:12:12,440
GPU which we often do is open in p4 we

00:12:10,820 --> 00:12:16,010
need to synchronize data for exam we

00:12:12,440 --> 00:12:19,610
start with all the data on the CPU side

00:12:16,010 --> 00:12:21,230
and then we replicate on the GPU the

00:12:19,610 --> 00:12:23,180
replicate data structure we can put on

00:12:21,230 --> 00:12:25,190
the GPU and then we need to bring data

00:12:23,180 --> 00:12:27,080
back and so sometimes it's easy

00:12:25,190 --> 00:12:30,980
sometimes when you have nested

00:12:27,080 --> 00:12:34,010
parallelism synchronizing variables in

00:12:30,980 --> 00:12:36,920
like five six levels of nested data

00:12:34,010 --> 00:12:38,750
structure is quite complicated so in

00:12:36,920 --> 00:12:40,340
this case we really want to manage data

00:12:38,750 --> 00:12:41,750
because if we need to synchronize

00:12:40,340 --> 00:12:45,290
between variables nothing to do with

00:12:41,750 --> 00:12:47,870
managing memory here random switching

00:12:45,290 --> 00:12:50,540
between host and device owing in this

00:12:47,870 --> 00:12:54,440
case host is the CPU and devices the GPU

00:12:50,540 --> 00:12:57,260
for example there's an application

00:12:54,440 --> 00:12:59,270
called mg algebraic multigrid so the way

00:12:57,260 --> 00:13:01,370
it works it's all very large problem

00:12:59,270 --> 00:13:03,140
then reduce the problem size and

00:13:01,370 --> 00:13:04,820
continue solving smaller problem and

00:13:03,140 --> 00:13:06,770
then reduce problem size continue

00:13:04,820 --> 00:13:08,600
solving even smaller problem it's called

00:13:06,770 --> 00:13:10,580
the V cycle when you go from very high

00:13:08,600 --> 00:13:12,830
resolution to low resolution and then

00:13:10,580 --> 00:13:14,330
you come back to very high resolution so

00:13:12,830 --> 00:13:15,860
what happens there is when the

00:13:14,330 --> 00:13:18,440
resolution very high problem size very

00:13:15,860 --> 00:13:20,240
large is worth using a GPU when the

00:13:18,440 --> 00:13:21,470
resolution is very low it can be down to

00:13:20,240 --> 00:13:24,170
one point

00:13:21,470 --> 00:13:26,240
from few millions of points maybe

00:13:24,170 --> 00:13:27,860
billions of points then you don't want

00:13:26,240 --> 00:13:29,480
to launch kernels on the GPU because

00:13:27,860 --> 00:13:30,830
there is not enough work so at some

00:13:29,480 --> 00:13:33,320
point you want to switch from

00:13:30,830 --> 00:13:36,010
computation on the CPU and the GPU so if

00:13:33,320 --> 00:13:39,050
you replicate your data on both devices

00:13:36,010 --> 00:13:41,360
you need to make a decision when you

00:13:39,050 --> 00:13:44,840
switch and also you need to update the

00:13:41,360 --> 00:13:47,480
state of your solution on other CPU and

00:13:44,840 --> 00:13:50,270
a GPU so this sum that you need really

00:13:47,480 --> 00:13:52,220
to manage if we're using unified

00:13:50,270 --> 00:13:54,380
addressing it's not a problem the

00:13:52,220 --> 00:13:56,870
hardware or the system software will do

00:13:54,380 --> 00:14:00,160
it for you it will move a page or it

00:13:56,870 --> 00:14:03,350
will just compute using different device

00:14:00,160 --> 00:14:11,030
on the memory which can actually reside

00:14:03,350 --> 00:14:13,340
on either side of envy link well using

00:14:11,030 --> 00:14:15,650
concurrent execution for example host

00:14:13,340 --> 00:14:17,240
and device if you have memory

00:14:15,650 --> 00:14:19,340
replication for example on the host you

00:14:17,240 --> 00:14:21,440
want to update even numbers in the array

00:14:19,340 --> 00:14:24,470
and on device you want to update odd

00:14:21,440 --> 00:14:26,690
numbers in the erase just as wild

00:14:24,470 --> 00:14:28,100
example how do you synchronize the data

00:14:26,690 --> 00:14:29,780
later on right you want to make sure

00:14:28,100 --> 00:14:32,060
that you put everything into the same

00:14:29,780 --> 00:14:37,640
array and everything has the same state

00:14:32,060 --> 00:14:39,590
so this is quite a challenge so in

00:14:37,640 --> 00:14:42,710
general we have two options with OpenMP

00:14:39,590 --> 00:14:45,560
4.5 one option is memory data management

00:14:42,710 --> 00:14:49,700
using OpenMP directives that can

00:14:45,560 --> 00:14:52,760
allocate memory move data from CPU to

00:14:49,700 --> 00:14:54,950
the GPU from one GPU to another GPU we

00:14:52,760 --> 00:14:57,440
have memory data management using

00:14:54,950 --> 00:15:00,680
unified addressing it's typically much

00:14:57,440 --> 00:15:03,530
easier than the first case sometimes

00:15:00,680 --> 00:15:06,800
there are performance penalties and we

00:15:03,530 --> 00:15:09,860
can actually also mix these two probably

00:15:06,800 --> 00:15:12,050
the most efficient way and most

00:15:09,860 --> 00:15:16,060
productive way to write the applications

00:15:12,050 --> 00:15:16,060
and I will show some examples

00:15:18,240 --> 00:15:23,850
all right so we continue the topic of

00:15:20,610 --> 00:15:25,769
managing memory and data as I mentioned

00:15:23,850 --> 00:15:28,110
opening P we can use directives so those

00:15:25,769 --> 00:15:30,809
a map enter data release delete I will

00:15:28,110 --> 00:15:35,040
show a few of the examples we can use

00:15:30,809 --> 00:15:37,199
OpenMP api calls for example OMP target

00:15:35,040 --> 00:15:42,540
a lock which is basically a wrapper

00:15:37,199 --> 00:15:45,050
around cudamalloc okay on p target mem

00:15:42,540 --> 00:15:50,730
copy which is basically a wrapper over

00:15:45,050 --> 00:15:53,600
kudamon copy so on and we can obviously

00:15:50,730 --> 00:15:58,499
mix and match between directives and

00:15:53,600 --> 00:16:02,429
api's are present in openmp so here for

00:15:58,499 --> 00:16:07,499
example I show a few a closes a few

00:16:02,429 --> 00:16:11,819
directives so for example the two

00:16:07,499 --> 00:16:15,149
directives means I want to replicate

00:16:11,819 --> 00:16:17,939
data and I want you move it from either

00:16:15,149 --> 00:16:20,189
Scipio to the GPO so two directors what

00:16:17,939 --> 00:16:22,050
it made you it may allocate a new memory

00:16:20,189 --> 00:16:25,379
on the GPU for example if I move from

00:16:22,050 --> 00:16:29,429
the CPU to the GPU and move the data to

00:16:25,379 --> 00:16:31,730
that memory okay and if it's detects

00:16:29,429 --> 00:16:34,350
that the memory was already allocated

00:16:31,730 --> 00:16:37,170
because we have pairing of two pointers

00:16:34,350 --> 00:16:39,720
so if we don't get a pointer point to

00:16:37,170 --> 00:16:42,199
the CPU memory and that pointer doesn't

00:16:39,720 --> 00:16:44,399
have a pair point it to the GPU memory

00:16:42,199 --> 00:16:44,699
opening period time we'll figure out

00:16:44,399 --> 00:16:46,559
that

00:16:44,699 --> 00:16:49,559
oh now I need to allocate in your memory

00:16:46,559 --> 00:16:52,679
I need to pair pointers and those will

00:16:49,559 --> 00:16:55,139
point you some two structures data

00:16:52,679 --> 00:16:57,779
structures that I will maybe synchronize

00:16:55,139 --> 00:17:00,509
later on so to helps you to allocate

00:16:57,779 --> 00:17:02,549
memory and move data from the similar

00:17:00,509 --> 00:17:04,949
thing but in the opposite direction

00:17:02,549 --> 00:17:08,010
data goes from the GPU to the CPU to

00:17:04,949 --> 00:17:10,470
from data goes both ways a lock means

00:17:08,010 --> 00:17:11,250
you only allocate memory and you may use

00:17:10,470 --> 00:17:13,230
it later on

00:17:11,250 --> 00:17:16,409
not every time you need to move data

00:17:13,230 --> 00:17:21,929
it's actually expensive release means it

00:17:16,409 --> 00:17:23,730
may delete the buffer or it may skip

00:17:21,929 --> 00:17:25,679
deleting the buffer just decrement

00:17:23,730 --> 00:17:28,079
what's called the reference counter and

00:17:25,679 --> 00:17:31,070
I'm not going to dive too deep into this

00:17:28,079 --> 00:17:33,470
because it's another thirty minutes talk

00:17:31,070 --> 00:17:36,830
it means just delete the memory buffer

00:17:33,470 --> 00:17:41,410
so pretty much all the calls wrappers

00:17:36,830 --> 00:17:45,350
around a CUDA api's for kudamon copy a

00:17:41,410 --> 00:17:48,920
cudamalloc could free right but they

00:17:45,350 --> 00:17:51,410
quite hide a complexity of programming

00:17:48,920 --> 00:17:53,540
from the user and you can express a

00:17:51,410 --> 00:17:55,490
number of steps that you want you make

00:17:53,540 --> 00:18:04,370
in your application by using just few

00:17:55,490 --> 00:18:08,510
words if you close to the openmp okay so

00:18:04,370 --> 00:18:11,930
here we have one example just a very

00:18:08,510 --> 00:18:15,260
simple loop it does sine of X times

00:18:11,930 --> 00:18:20,660
cosine of X I just made it up a lot of

00:18:15,260 --> 00:18:23,140
flops required here and here show how we

00:18:20,660 --> 00:18:26,360
can offload the execution from the a

00:18:23,140 --> 00:18:29,600
from the CPU to the GPU so first note

00:18:26,360 --> 00:18:31,100
the pragma OMP target directives it

00:18:29,600 --> 00:18:33,110
means that now we are going to use

00:18:31,100 --> 00:18:36,380
target the target can be actually device

00:18:33,110 --> 00:18:38,600
or host if I don't specify anything in

00:18:36,380 --> 00:18:40,990
addition so by default target is the

00:18:38,600 --> 00:18:44,870
device in this case is the GPU

00:18:40,990 --> 00:18:49,550
distribute teams distribute means create

00:18:44,870 --> 00:18:53,000
some number of CUDA blocks a there is a

00:18:49,550 --> 00:18:55,460
default parameters a Excel compiler for

00:18:53,000 --> 00:18:57,530
example have some smart in it so we will

00:18:55,460 --> 00:19:01,720
try to figure out what will be the best

00:18:57,530 --> 00:19:05,750
number of CUDA blocks to request and

00:19:01,720 --> 00:19:07,340
parallel for means in each block I won't

00:19:05,750 --> 00:19:09,200
use threads so this is basically the

00:19:07,340 --> 00:19:10,910
size of each CUDA block how many threads

00:19:09,200 --> 00:19:13,970
per block they are going to use and

00:19:10,910 --> 00:19:15,860
again there are default parameters we

00:19:13,970 --> 00:19:18,830
can specify all these parameters also

00:19:15,860 --> 00:19:21,200
explicitly and there are some smarts in

00:19:18,830 --> 00:19:23,090
the compilers that can try to figure out

00:19:21,200 --> 00:19:26,840
how many threads per block will be the

00:19:23,090 --> 00:19:29,330
most effective for this particular

00:19:26,840 --> 00:19:31,730
kernel and then we have the map director

00:19:29,330 --> 00:19:35,000
so you see we we take X which resides

00:19:31,730 --> 00:19:39,200
first in the CPU memory so the X points

00:19:35,000 --> 00:19:41,750
to the CPU memory and we say - and what

00:19:39,200 --> 00:19:44,750
OpenMP runtime will do it we will see

00:19:41,750 --> 00:19:48,440
okay is this CPU pointer paired

00:19:44,750 --> 00:19:50,570
with the GPU pointer no if no then it

00:19:48,440 --> 00:19:55,550
will allocate data for allocate memory

00:19:50,570 --> 00:19:59,540
for X on the GPU and copy the data from

00:19:55,550 --> 00:20:02,810
the CPU buffer here I showed in blue

00:19:59,540 --> 00:20:04,280
color qu the buffer on the GPU so

00:20:02,810 --> 00:20:08,060
basically it will replicate the date on

00:20:04,280 --> 00:20:10,520
the GPU and map from will check if Y has

00:20:08,060 --> 00:20:14,210
a pair or if there is another pointer

00:20:10,520 --> 00:20:17,840
that points to the data on to the memory

00:20:14,210 --> 00:20:20,930
on the GPU if yes it will do nothing and

00:20:17,840 --> 00:20:23,990
if no it will allocate memory for Y and

00:20:20,930 --> 00:20:27,410
in the end of this execution will also

00:20:23,990 --> 00:20:32,030
copy the data back to the GPU it's very

00:20:27,410 --> 00:20:37,940
simple model to offload execution from

00:20:32,030 --> 00:20:39,740
CPU to GPU we can use this joint set of

00:20:37,940 --> 00:20:43,700
problems for example at the beginning of

00:20:39,740 --> 00:20:45,830
our execution we can map the data

00:20:43,700 --> 00:20:49,790
allocate memory move data that we need

00:20:45,830 --> 00:20:51,920
to the GPU and release it or deleted at

00:20:49,790 --> 00:20:55,630
the end of the execution and in between

00:20:51,920 --> 00:20:58,580
we can have numerous number of kernels

00:20:55,630 --> 00:21:00,380
functions that will reuse the already

00:20:58,580 --> 00:21:03,680
mapped data so we don't have to put all

00:21:00,380 --> 00:21:06,860
the instructions at every fragment pit

00:21:03,680 --> 00:21:10,040
target directives and by doing that you

00:21:06,860 --> 00:21:13,220
also minimize the impact on performance

00:21:10,040 --> 00:21:19,030
due to allocated memory moving data and

00:21:13,220 --> 00:21:19,030
moving everything back sure

00:21:22,050 --> 00:21:34,290
so yes it does matter first can be CP or

00:21:29,850 --> 00:21:39,720
the GPU so in open and P there is right

00:21:34,290 --> 00:21:44,490
so a a how do you control which GPU will

00:21:39,720 --> 00:21:48,630
be used so for example in ac-9 t2 we can

00:21:44,490 --> 00:21:51,720
see or each process can see up to six

00:21:48,630 --> 00:21:54,990
GPUs right so first you can make only

00:21:51,720 --> 00:21:58,320
one GPU visible or one or two or three

00:21:54,990 --> 00:22:00,120
or up to six visible to each process so

00:21:58,320 --> 00:22:04,320
you can make one and then there is the

00:22:00,120 --> 00:22:10,100
only one and second there is a OpenMP

00:22:04,320 --> 00:22:13,140
API called OMP set default device it's

00:22:10,100 --> 00:22:17,790
somewhat equivalent but not exactly

00:22:13,140 --> 00:22:19,560
equivalent to CUDA set device and you

00:22:17,790 --> 00:22:23,040
can choose the GPUs that you're going to

00:22:19,560 --> 00:22:24,540
use and you can switch later on to a

00:22:23,040 --> 00:22:26,850
different chip you'll have an example

00:22:24,540 --> 00:22:28,560
where we will use multiple threads

00:22:26,850 --> 00:22:31,860
running on different GPU so you can

00:22:28,560 --> 00:22:33,720
launch tasks on different GPUs just need

00:22:31,860 --> 00:22:37,470
to be careful if you allocated memory on

00:22:33,720 --> 00:22:46,140
GPUs 0 yeah don't don't try to use that

00:22:37,470 --> 00:22:48,600
pointer for gp1 right so as I said as I

00:22:46,140 --> 00:22:51,360
mentioned before we can enter data at

00:22:48,600 --> 00:22:52,350
the beginning of the some scope right

00:22:51,360 --> 00:22:54,510
can be in the beginning of the

00:22:52,350 --> 00:22:56,970
application and exit at the end of the

00:22:54,510 --> 00:22:59,820
application or at some scope and in

00:22:56,970 --> 00:23:02,850
between we can have a lot of kernels

00:22:59,820 --> 00:23:05,820
that we launched on the GPU and in this

00:23:02,850 --> 00:23:08,250
case we don't have to specify all the

00:23:05,820 --> 00:23:11,910
map directors because the runtime that

00:23:08,250 --> 00:23:16,680
we have will detect if pointer X has a

00:23:11,910 --> 00:23:19,140
pair CPU and GPU and pointer Y has a

00:23:16,680 --> 00:23:20,820
pair on the CPU and the GPU if it

00:23:19,140 --> 00:23:24,300
doesn't in this case your code will

00:23:20,820 --> 00:23:26,910
collapse and you will see some error if

00:23:24,300 --> 00:23:29,160
it does it will work properly now there

00:23:26,910 --> 00:23:31,410
is function a and what I want to show

00:23:29,160 --> 00:23:33,990
that inside this function a which can be

00:23:31,410 --> 00:23:35,880
called from anywhere in this code and I

00:23:33,990 --> 00:23:38,730
don't know if it's going to be called

00:23:35,880 --> 00:23:40,830
from the region we already enter the

00:23:38,730 --> 00:23:43,440
data or map the data from the CPU to the

00:23:40,830 --> 00:23:47,070
GPU or not so in this case it's called

00:23:43,440 --> 00:23:51,260
from within the region where x and y are

00:23:47,070 --> 00:23:54,390
already mapped or have a replica on the

00:23:51,260 --> 00:23:56,640
GPU so in this case what will happen the

00:23:54,390 --> 00:23:57,900
map directors will be simply ignored it

00:23:56,640 --> 00:24:00,510
will find that there is already a

00:23:57,900 --> 00:24:02,580
pointer point it to the GPU memory it

00:24:00,510 --> 00:24:05,160
will assume the data is synchronous is

00:24:02,580 --> 00:24:07,890
synchronized already so there will be no

00:24:05,160 --> 00:24:10,590
copy and will just launch a kernel and

00:24:07,890 --> 00:24:13,470
executed without any copy to off Rome of

00:24:10,590 --> 00:24:16,110
data but if I call this function from

00:24:13,470 --> 00:24:19,320
the region let's say I place it before

00:24:16,110 --> 00:24:23,400
the first pragma OMP target enter then

00:24:19,320 --> 00:24:26,880
what will happen the openmp runtime when

00:24:23,400 --> 00:24:29,880
it means map - we'll check if pointer to

00:24:26,880 --> 00:24:32,429
the CPU memory X has a pair on the GPU

00:24:29,880 --> 00:24:39,240
if not it will allocate memory and move

00:24:32,429 --> 00:24:40,740
data to the GPU and so on here I've

00:24:39,240 --> 00:24:45,150
shown an example where you can actually

00:24:40,740 --> 00:24:48,929
use OpenMP for managing data in memory

00:24:45,150 --> 00:24:51,090
and you can call functions are highly

00:24:48,929 --> 00:24:53,370
optimized for a particular device for

00:24:51,090 --> 00:24:56,690
example this function is for qu sparse

00:24:53,370 --> 00:24:59,730
before in sparse matrix vector multiply

00:24:56,690 --> 00:25:01,320
you can see I map the data using OpenMP

00:24:59,730 --> 00:25:03,539
directives and then I provide another

00:25:01,320 --> 00:25:05,520
director that vision this scope instead

00:25:03,539 --> 00:25:08,340
of using the CPU pointer I wanted to use

00:25:05,520 --> 00:25:12,990
the GPO pointer so to pass the paired

00:25:08,340 --> 00:25:16,380
GPU pointer to ku sparse and COO sparse

00:25:12,990 --> 00:25:19,049
will basically compute what it needs to

00:25:16,380 --> 00:25:21,450
compute and then it will exit okay so

00:25:19,049 --> 00:25:26,690
it's it's part of sort of cooperation

00:25:21,450 --> 00:25:26,690
between OpenMP and coding with CUDA

00:25:27,140 --> 00:25:32,490
deeply nested a data structure so deeply

00:25:30,299 --> 00:25:35,549
nested data structure can be quite a

00:25:32,490 --> 00:25:37,860
pain for example here I show a data

00:25:35,549 --> 00:25:41,600
structure from a code called Quicksilver

00:25:37,860 --> 00:25:46,740
it is a Monte Carlo code used by

00:25:41,600 --> 00:25:49,200
National Labs and here we see very

00:25:46,740 --> 00:25:49,680
unstructured data so there are objects

00:25:49,200 --> 00:25:52,020
that have

00:25:49,680 --> 00:25:54,570
pointers to other objects cubes vector

00:25:52,020 --> 00:25:56,840
it's kind of STD vector something

00:25:54,570 --> 00:25:59,490
similar so it has some size and and

00:25:56,840 --> 00:26:03,680
pointer and few other attributes there

00:25:59,490 --> 00:26:05,970
so we have vectors and vectors with

00:26:03,680 --> 00:26:08,310
vision vectors with injectors vision

00:26:05,970 --> 00:26:11,460
vectors typically have about five six

00:26:08,310 --> 00:26:15,510
levels of data nesting so when we have

00:26:11,460 --> 00:26:17,760
that complex in the code using mapping

00:26:15,510 --> 00:26:20,520
for each of them becomes quite

00:26:17,760 --> 00:26:23,430
complicated and there are some solution

00:26:20,520 --> 00:26:26,280
and I will go over these solutions and

00:26:23,430 --> 00:26:29,880
in this situation actually present in

00:26:26,280 --> 00:26:34,430
many of the real HPC applications where

00:26:29,880 --> 00:26:38,630
not everything looks like simple a

00:26:34,430 --> 00:26:38,630
duck's bill loop or something like this

00:26:38,810 --> 00:26:44,670
so I also have this very simple example

00:26:41,430 --> 00:26:48,030
we have a structure which has a pointer

00:26:44,670 --> 00:26:50,190
Y and it contains the size right so

00:26:48,030 --> 00:26:52,890
number of elements and very simple

00:26:50,190 --> 00:26:56,630
constructor that gets pointer Y and

00:26:52,890 --> 00:26:59,850
assigns to day a dot Y and get the size

00:26:56,630 --> 00:27:03,270
so online for example eight I create a

00:26:59,850 --> 00:27:08,010
memory on the CPU allocate memory of the

00:27:03,270 --> 00:27:13,170
CPU and on row a I create an object of

00:27:08,010 --> 00:27:15,210
this structure a and I provide the

00:27:13,170 --> 00:27:17,760
pointer to the CPU memory to this object

00:27:15,210 --> 00:27:20,340
and also the size the number of elements

00:27:17,760 --> 00:27:22,200
now I want to use this object on the GPU

00:27:20,340 --> 00:27:26,130
right so I need to replicate the data on

00:27:22,200 --> 00:27:29,370
the GPU say okay I will map a as a

00:27:26,130 --> 00:27:33,540
pointer so just one value to the GPU I

00:27:29,370 --> 00:27:37,080
will map Y to the GPU and I will try now

00:27:33,540 --> 00:27:40,710
to use Y eight pointer to Y on the GPU

00:27:37,080 --> 00:27:43,170
and my code will collapse and the reason

00:27:40,710 --> 00:27:47,430
why my code is collapse is when I do map

00:27:43,170 --> 00:27:51,120
to of a I allocate memory on the GPU and

00:27:47,430 --> 00:27:53,640
I perform bitwise copy from the CPU to

00:27:51,120 --> 00:27:56,160
the GPU memory so if you do bitwise copy

00:27:53,640 --> 00:27:59,940
it will copy the size fine but it also

00:27:56,160 --> 00:28:01,590
will copy the value inside the Y and the

00:27:59,940 --> 00:28:02,700
value inside Y will basically point to

00:28:01,590 --> 00:28:05,399
the CPU memory

00:28:02,700 --> 00:28:11,759
and if cheap you cannot read the CPU

00:28:05,399 --> 00:28:13,919
memory you'll just fail so there are

00:28:11,759 --> 00:28:16,590
different ways to deal with this for

00:28:13,919 --> 00:28:19,619
example we can map a and then separately

00:28:16,590 --> 00:28:22,980
we can map a pointer why right so in

00:28:19,619 --> 00:28:28,830
this case everything will work fine all

00:28:22,980 --> 00:28:31,470
we can assign the value of pointer Y

00:28:28,830 --> 00:28:34,470
point you to the GPU memory in some

00:28:31,470 --> 00:28:36,509
different ways but if you imagine that

00:28:34,470 --> 00:28:38,970
we have six or seven levels of nesting

00:28:36,509 --> 00:28:40,980
then going through all these data

00:28:38,970 --> 00:28:42,899
structures and trying to assign the

00:28:40,980 --> 00:28:45,539
right pointers to the right structures

00:28:42,899 --> 00:28:48,830
and right pointers within the structures

00:28:45,539 --> 00:28:51,119
it's very painful and I'll think if you

00:28:48,830 --> 00:28:52,919
have the referencing for exam one

00:28:51,119 --> 00:28:54,539
structure just assign the pointer which

00:28:52,919 --> 00:28:57,480
will be and entacle to the point there's

00:28:54,539 --> 00:29:00,509
another structure it quickly becomes

00:28:57,480 --> 00:29:02,609
very messy and that doesn't mean that

00:29:00,509 --> 00:29:04,169
you can often you cannot offload the

00:29:02,609 --> 00:29:07,429
code and you code you'll never run on

00:29:04,169 --> 00:29:11,399
the GPU it means that you just need to

00:29:07,429 --> 00:29:13,799
use alternative techniques to port your

00:29:11,399 --> 00:29:15,359
code from the CPU to the GPU and that

00:29:13,799 --> 00:29:20,600
problem is exactly the same if you

00:29:15,359 --> 00:29:20,600
program is open and PEO open ACC or CUDA

00:29:20,960 --> 00:29:31,009
so to alleviate this issue right you

00:29:28,409 --> 00:29:33,509
solve this problem of working with

00:29:31,009 --> 00:29:36,629
nested structure something very deeply

00:29:33,509 --> 00:29:39,299
nested structures we can use other

00:29:36,629 --> 00:29:42,359
couldn't manage memory or we can use a

00:29:39,299 --> 00:29:45,690
TSO in one sentence we can use global

00:29:42,359 --> 00:29:49,679
addressing and I will show a couple of a

00:29:45,690 --> 00:29:54,059
examples here so this piece of code is

00:29:49,679 --> 00:29:55,649
taken from well it can be taken from

00:29:54,059 --> 00:29:58,289
different application but in these

00:29:55,649 --> 00:30:01,830
particular cases from Lu leche so we

00:29:58,289 --> 00:30:04,350
deal here with STD vector and with STD

00:30:01,830 --> 00:30:06,450
vector we do not have control on the

00:30:04,350 --> 00:30:09,149
pointer that points actually to the data

00:30:06,450 --> 00:30:15,380
because this pointer is protected within

00:30:09,149 --> 00:30:17,330
the class so we can call we can

00:30:15,380 --> 00:30:19,310
have API that would allocate memory to

00:30:17,330 --> 00:30:20,780
this pointer but we cannot change the

00:30:19,310 --> 00:30:23,900
value of this pointer so now it will

00:30:20,780 --> 00:30:26,690
point to the GPU memory so in order to

00:30:23,900 --> 00:30:29,090
be able to use STD vectors on the CPU

00:30:26,690 --> 00:30:31,430
and a GPU in particular code what we do

00:30:29,090 --> 00:30:33,380
is specialized the memory allocator so

00:30:31,430 --> 00:30:35,240
it's a very simple trick that we do and

00:30:33,380 --> 00:30:37,730
it actually saved us alot in many

00:30:35,240 --> 00:30:41,420
different codes with nested structures

00:30:37,730 --> 00:30:44,120
so instead of calling malloc we'll call

00:30:41,420 --> 00:30:46,280
cudamalloc managed and this will give us

00:30:44,120 --> 00:30:49,310
a pointer which is valid on the CPU and

00:30:46,280 --> 00:30:50,960
on the GPU and moreover we don't need to

00:30:49,310 --> 00:30:54,500
replicate the data on the CPU and the

00:30:50,960 --> 00:30:56,960
GPU the data and the memory and the data

00:30:54,500 --> 00:31:00,200
with the memory will move on demand when

00:30:56,960 --> 00:31:04,100
we touch corresponding page of cache

00:31:00,200 --> 00:31:06,920
line on either of the devices right so

00:31:04,100 --> 00:31:09,410
in this case the modification was really

00:31:06,920 --> 00:31:11,180
simple just few lines of the code we use

00:31:09,410 --> 00:31:14,210
if the F because this was the most

00:31:11,180 --> 00:31:17,150
portable way to make the minimal amount

00:31:14,210 --> 00:31:20,060
of changes in the code and have this got

00:31:17,150 --> 00:31:27,020
ready for different for computational

00:31:20,060 --> 00:31:30,890
different hardware today we can use ATS

00:31:27,020 --> 00:31:32,840
which is not exactly the same as a CUDA

00:31:30,890 --> 00:31:37,250
managed memory it has some advantages

00:31:32,840 --> 00:31:40,130
and some disadvantages so 80s stands as

00:31:37,250 --> 00:31:43,190
was said in a previous talk for a others

00:31:40,130 --> 00:31:47,230
translation services we basically have

00:31:43,190 --> 00:31:50,000
malloc write on on a a C in 92 and

00:31:47,230 --> 00:31:53,630
memory allocated with malloc is visible

00:31:50,000 --> 00:31:56,870
today for the GPUs and for the CPUs this

00:31:53,630 --> 00:31:59,210
memory wouldn't page today by by the

00:31:56,870 --> 00:32:01,670
first statue whatever touch it will

00:31:59,210 --> 00:32:03,890
always stay in the CPU and this is

00:32:01,670 --> 00:32:07,610
something that we're still working on

00:32:03,890 --> 00:32:10,580
with a Linux community and vision video

00:32:07,610 --> 00:32:12,950
because the end goal is that memory

00:32:10,580 --> 00:32:15,470
allocated with malloc will be paid to

00:32:12,950 --> 00:32:18,170
the device that need this memory just as

00:32:15,470 --> 00:32:19,430
in normal Numa system but we can

00:32:18,170 --> 00:32:22,340
prefetch this memory so we can

00:32:19,430 --> 00:32:24,410
explicitly prefetch and move the pages

00:32:22,340 --> 00:32:27,230
for example from the CPU to the GPU or

00:32:24,410 --> 00:32:28,350
to the differential or back from the GPU

00:32:27,230 --> 00:32:33,210
to the CPU

00:32:28,350 --> 00:32:35,490
so we can modify the specialized the

00:32:33,210 --> 00:32:41,279
memory allocator for this STD vector

00:32:35,490 --> 00:32:44,250
using for example a 80s not necessary

00:32:41,279 --> 00:32:46,620
could malloc managed now one difference

00:32:44,250 --> 00:32:49,759
between eight years in CUDA managed

00:32:46,620 --> 00:32:53,750
memory is that we could manage memory

00:32:49,759 --> 00:32:59,279
only memory allocated dynamically can be

00:32:53,750 --> 00:33:02,850
seen by the GPU and the CPU with eight

00:32:59,279 --> 00:33:05,629
years all variables global variables

00:33:02,850 --> 00:33:07,500
everything that is on the CPU stack and

00:33:05,629 --> 00:33:12,259
everything that was allocated

00:33:07,500 --> 00:33:17,610
dynamically can be seen by both devices

00:33:12,259 --> 00:33:20,190
we screw the managed memory if you try

00:33:17,610 --> 00:33:23,279
to push more than sixty 16 gigabyte of

00:33:20,190 --> 00:33:25,080
data into the GPU what will happen some

00:33:23,279 --> 00:33:27,269
memory will be swapped back to the CPU

00:33:25,080 --> 00:33:29,190
automatically there is a performance

00:33:27,269 --> 00:33:32,370
penalty but the code will not crash it

00:33:29,190 --> 00:33:34,049
will run and it might be handy in some

00:33:32,370 --> 00:33:35,700
application we take this performance hit

00:33:34,049 --> 00:33:38,700
but everything is managed automatically

00:33:35,700 --> 00:33:40,379
with 80s today

00:33:38,700 --> 00:33:43,470
the memory will not be swapped so you

00:33:40,379 --> 00:33:45,779
get you 16 gigabyte and if you try to

00:33:43,470 --> 00:33:49,409
push one more bite into the GPU you

00:33:45,779 --> 00:33:53,039
could will crush so it's a subject for

00:33:49,409 --> 00:33:57,110
future improvement but today this how it

00:33:53,039 --> 00:33:57,110
works yes

00:33:58,090 --> 00:34:05,349
I have a slide good question yeah it we

00:34:03,909 --> 00:34:06,669
always concern about performance so

00:34:05,349 --> 00:34:08,559
whatever we do we always check the

00:34:06,669 --> 00:34:11,289
performance so I did heven functionality

00:34:08,559 --> 00:34:15,520
is great perform a heron performance is

00:34:11,289 --> 00:34:16,230
even better yeah so I will and there is

00:34:15,520 --> 00:34:18,849
this question

00:34:16,230 --> 00:34:20,379
okay so I'll talk a bit more about eight

00:34:18,849 --> 00:34:27,220
years there are others translation

00:34:20,379 --> 00:34:32,409
services so when opening p 4.0 became

00:34:27,220 --> 00:34:34,690
available it was not obvious for opening

00:34:32,409 --> 00:34:37,359
pika media that opening p at some point

00:34:34,690 --> 00:34:40,089
will have to support also unified other

00:34:37,359 --> 00:34:41,980
things the model for opening p was well

00:34:40,089 --> 00:34:45,460
you start with all the data on the cpu

00:34:41,980 --> 00:34:47,379
and you replicate on the GPU and we need

00:34:45,460 --> 00:34:49,960
you synchronize data meaning you copy

00:34:47,379 --> 00:34:53,139
something from the GPU to the cpu update

00:34:49,960 --> 00:34:57,940
some variable or the other way around it

00:34:53,139 --> 00:35:02,740
has been proven impractical for very

00:34:57,940 --> 00:35:06,579
complex applications and the standard

00:35:02,740 --> 00:35:13,750
changed a opening p 5.0 for example will

00:35:06,579 --> 00:35:15,339
allow users to prescribe how how data

00:35:13,750 --> 00:35:18,160
can be synchronized with nested

00:35:15,339 --> 00:35:22,410
parallelism how you map nested

00:35:18,160 --> 00:35:26,670
structures and it also allows to use a

00:35:22,410 --> 00:35:30,579
unified addressing okay so for example

00:35:26,670 --> 00:35:32,109
here we here we have on P set default

00:35:30,579 --> 00:35:35,160
device there was a question about how we

00:35:32,109 --> 00:35:39,240
choose the device here in example and

00:35:35,160 --> 00:35:42,339
here we have perfect valid OpenMP

00:35:39,240 --> 00:35:44,319
four-point-five cold where we create a

00:35:42,339 --> 00:35:48,220
target region we map the data that we

00:35:44,319 --> 00:35:51,150
need create memory and and move the data

00:35:48,220 --> 00:35:54,490
back from the GPU to the CPU and provide

00:35:51,150 --> 00:35:56,710
parallelism that will employ in the GPU

00:35:54,490 --> 00:35:58,150
kernel and we can take this code and

00:35:56,710 --> 00:36:01,029
modify it

00:35:58,150 --> 00:36:02,680
assuming that ATS is working and we can

00:36:01,029 --> 00:36:04,779
say well you know data is actually

00:36:02,680 --> 00:36:06,789
device pointer so it means it's valid

00:36:04,779 --> 00:36:08,380
device pointer you don't need to map

00:36:06,789 --> 00:36:10,270
anything you don't need to allocate new

00:36:08,380 --> 00:36:12,609
memory you don't need to

00:36:10,270 --> 00:36:14,950
copy any data from the CPU to the GPU

00:36:12,609 --> 00:36:17,800
just take this pointer and around with

00:36:14,950 --> 00:36:21,240
this so data was allocated on the CPU

00:36:17,800 --> 00:36:25,380
side currently there is no page fault

00:36:21,240 --> 00:36:28,030
there is no increased page fold and a

00:36:25,380 --> 00:36:30,369
what will happen the GPU will run this

00:36:28,030 --> 00:36:33,400
kernel but it will read or write data

00:36:30,369 --> 00:36:35,800
over MV link just very similar to zero

00:36:33,400 --> 00:36:38,140
copy functionality when you allocate

00:36:35,800 --> 00:36:42,190
data we could host a log and then you

00:36:38,140 --> 00:36:45,880
can access this data from the GPU the

00:36:42,190 --> 00:36:51,690
main difference between new and 80s and

00:36:45,880 --> 00:36:54,849
could host a lock is that new will take

00:36:51,690 --> 00:36:57,760
probably less than millisecond or so and

00:36:54,849 --> 00:36:59,560
could host a lock will take about three

00:36:57,760 --> 00:37:02,950
hundred four hundred microseconds so

00:36:59,560 --> 00:37:10,330
it's it's a big difference in time that

00:37:02,950 --> 00:37:13,450
spend on memory allocation so not how

00:37:10,330 --> 00:37:15,010
long the string is string of a openmp

00:37:13,450 --> 00:37:18,070
directive so it's much longer than this

00:37:15,010 --> 00:37:20,170
loop in in this example right so pragma

00:37:18,070 --> 00:37:23,140
OMP targeting the serial parallel for is

00:37:20,170 --> 00:37:26,680
device pointer and actually can be much

00:37:23,140 --> 00:37:27,250
longer than that if target if something

00:37:26,680 --> 00:37:30,369
else

00:37:27,250 --> 00:37:32,470
map to from and so on

00:37:30,369 --> 00:37:34,960
and we would like to reduce actually

00:37:32,470 --> 00:37:40,270
this burden and and have this even more

00:37:34,960 --> 00:37:44,740
compact so the current opening p runtime

00:37:40,270 --> 00:37:47,770
that IBM provides opening theme allows

00:37:44,740 --> 00:37:50,260
to export some environmental variables

00:37:47,770 --> 00:37:52,810
that will tell openmp runtime whatever

00:37:50,260 --> 00:37:54,880
pointer we get if it's not depth assume

00:37:52,810 --> 00:37:56,920
that it's a valid device pointer so

00:37:54,880 --> 00:38:01,270
developers don't have to write this is

00:37:56,920 --> 00:38:04,570
device pointer openmp will assume that

00:38:01,270 --> 00:38:06,099
it's a valid device pointer okay

00:38:04,570 --> 00:38:09,460
it will not even check it will check

00:38:06,099 --> 00:38:12,880
only if if the data was mapped to the

00:38:09,460 --> 00:38:15,460
GPU through a map directive and if not

00:38:12,880 --> 00:38:18,780
it just passed a pointer to the GPU

00:38:15,460 --> 00:38:18,780
kernel and you'll run with that

00:38:19,450 --> 00:38:26,350
you can also mix you can miss mix

00:38:22,390 --> 00:38:29,440
explicit directives and and assume that

00:38:26,350 --> 00:38:30,880
some pointers are valid device pointers

00:38:29,440 --> 00:38:34,990
for exam what will happen in this

00:38:30,880 --> 00:38:37,000
example I have a array of two doubles so

00:38:34,990 --> 00:38:39,700
it's really small array once I touch it

00:38:37,000 --> 00:38:41,710
on the GPU it becomes actually it sits

00:38:39,700 --> 00:38:44,860
in the register seem not indication sits

00:38:41,710 --> 00:38:47,890
in the registers so allocating memory

00:38:44,860 --> 00:38:50,650
for two doubles and deallocating and

00:38:47,890 --> 00:38:54,900
moving data from CPU the GPU will take

00:38:50,650 --> 00:38:57,490
way more time than about 400 a

00:38:54,900 --> 00:39:00,460
nanosecond latency that will be needed

00:38:57,490 --> 00:39:03,910
just to grab the student levels so in

00:39:00,460 --> 00:39:06,910
this case I will use a data array which

00:39:03,910 --> 00:39:09,100
will sit in the CPU memory compiler will

00:39:06,910 --> 00:39:10,570
understand that in open appear on time

00:39:09,100 --> 00:39:12,430
you'll understand that it's a valid

00:39:10,570 --> 00:39:13,930
device pointer because we use eight

00:39:12,430 --> 00:39:16,600
years and we also export some

00:39:13,930 --> 00:39:18,460
environment variables and on top of this

00:39:16,600 --> 00:39:20,560
I'll map data to because it's large

00:39:18,460 --> 00:39:23,140
array maybe won't you keep it on the GPU

00:39:20,560 --> 00:39:25,990
for later use I will make it explicitly

00:39:23,140 --> 00:39:32,230
so this already works and it's actually

00:39:25,990 --> 00:39:36,370
very handy for many codes so I showed

00:39:32,230 --> 00:39:38,980
examples for C C++ applications we have

00:39:36,370 --> 00:39:41,140
similar for Fortin application so c++

00:39:38,980 --> 00:39:43,930
typically is the language will be

00:39:41,140 --> 00:39:48,090
improvised first and try to figure out

00:39:43,930 --> 00:39:48,090
how things work and then we move a

00:39:48,720 --> 00:39:55,870
improvements into the Fortran language

00:39:51,180 --> 00:40:00,670
so here I show an example where first

00:39:55,870 --> 00:40:03,790
time when you run this little code where

00:40:00,670 --> 00:40:08,560
memories allocated on the CPU write and

00:40:03,790 --> 00:40:11,890
eight years is own what will happen is

00:40:08,560 --> 00:40:13,750
that further and Fortran the runtime

00:40:11,890 --> 00:40:17,950
will detect that pointer has not been

00:40:13,750 --> 00:40:21,160
mapped so it will map the data array

00:40:17,950 --> 00:40:22,960
explicitly it will actually allocate the

00:40:21,160 --> 00:40:24,700
memory for it and then it performed

00:40:22,960 --> 00:40:26,680
kudamon copy you can see this entry

00:40:24,700 --> 00:40:28,630
profile so you see what happens there

00:40:26,680 --> 00:40:30,910
there's a memory allocation which we

00:40:28,630 --> 00:40:33,010
don't see here but we do see could a mem

00:40:30,910 --> 00:40:36,160
copy then we see the kernel

00:40:33,010 --> 00:40:38,470
one point six microseconds and then

00:40:36,160 --> 00:40:40,750
there is a mem copy from the device to

00:40:38,470 --> 00:40:42,730
host and now we are going to run exactly

00:40:40,750 --> 00:40:45,100
the same executable the only difference

00:40:42,730 --> 00:40:48,550
is that we export this environmental

00:40:45,100 --> 00:40:51,010
variable that OpenMP runtime will detect

00:40:48,550 --> 00:40:53,650
any you'll act accordingly so now it

00:40:51,010 --> 00:40:55,680
runs exactly the same code what happens

00:40:53,650 --> 00:40:59,410
here is that we don't see any man copies

00:40:55,680 --> 00:41:01,720
because now it implicitly understood

00:40:59,410 --> 00:41:04,660
that every pointer that we pass to the

00:41:01,720 --> 00:41:06,850
GPU kernel is the valid device pointer

00:41:04,660 --> 00:41:08,410
and we don't need to map data we don't

00:41:06,850 --> 00:41:10,960
need to allocate new memory we don't

00:41:08,410 --> 00:41:12,460
need to copy data and you can also see

00:41:10,960 --> 00:41:14,620
that the kernel runtime actually

00:41:12,460 --> 00:41:18,040
increased by factor of six oh yeah about

00:41:14,620 --> 00:41:20,200
factor of six so why is it so this is so

00:41:18,040 --> 00:41:21,970
because now we run the kernel on the GPU

00:41:20,200 --> 00:41:25,930
but it reads data from the CPU memory

00:41:21,970 --> 00:41:27,550
over env link so this is small problem

00:41:25,930 --> 00:41:30,250
but if it would be bigger problem are

00:41:27,550 --> 00:41:32,620
this only 20 doubles bigger problem what

00:41:30,250 --> 00:41:34,660
you will see in memory bandits limited

00:41:32,620 --> 00:41:38,040
application the speed will be

00:41:34,660 --> 00:41:38,040
proportional to memory Bennett's

00:41:40,770 --> 00:41:47,110
performance there was a question about

00:41:42,550 --> 00:41:49,830
performance so this is a one of the

00:41:47,110 --> 00:41:52,510
coral one benchmark that was used for

00:41:49,830 --> 00:41:54,970
acceptance of Summit and Sierra it's

00:41:52,510 --> 00:41:56,740
called Lu leche it's what's called the

00:41:54,970 --> 00:41:58,300
base version the base version the

00:41:56,740 --> 00:42:01,240
definition of the best version is that

00:41:58,300 --> 00:42:03,520
we can use only compiler directives and

00:42:01,240 --> 00:42:06,910
minimal code changes in our support

00:42:03,520 --> 00:42:07,870
application from CP only application to

00:42:06,910 --> 00:42:10,090
application that can run on

00:42:07,870 --> 00:42:13,270
heterogeneous system we are not allowed

00:42:10,090 --> 00:42:15,550
to do some deeper changes for example

00:42:13,270 --> 00:42:17,860
you're not allowed to change data

00:42:15,550 --> 00:42:20,680
layouts we are not allowed to change

00:42:17,860 --> 00:42:25,720
merge split kernels and so on so really

00:42:20,680 --> 00:42:27,580
minimal may be like cosmetic changes in

00:42:25,720 --> 00:42:30,430
the code with directives in order to

00:42:27,580 --> 00:42:33,850
pour this application so Louis using STD

00:42:30,430 --> 00:42:37,000
vectors and what was done we specialized

00:42:33,850 --> 00:42:38,860
a the memory allocator for STD vectors

00:42:37,000 --> 00:42:40,690
either using eight years as I showed

00:42:38,860 --> 00:42:44,100
before or using CUDA

00:42:40,690 --> 00:42:46,190
managed memory and we decorated a

00:42:44,100 --> 00:42:49,340
parallel region

00:42:46,190 --> 00:42:52,010
the parallel four became fragmented

00:42:49,340 --> 00:42:54,560
target team distributed parallel four is

00:42:52,010 --> 00:42:57,160
maybe few directives of mapping and this

00:42:54,560 --> 00:43:00,350
was pretty much it for this code and

00:42:57,160 --> 00:43:02,360
what I compared here when we use one MP

00:43:00,350 --> 00:43:07,100
around pure GPUs only one process per

00:43:02,360 --> 00:43:09,860
GPU it's exactly the same code just the

00:43:07,100 --> 00:43:11,900
memory allocation is different we get

00:43:09,860 --> 00:43:14,510
fom is figure of Merit so it's like a

00:43:11,900 --> 00:43:17,240
speed right the higher the better we get

00:43:14,510 --> 00:43:19,280
comparable performance but a little bit

00:43:17,240 --> 00:43:20,960
better performance with 80s so what

00:43:19,280 --> 00:43:22,670
happens with ATS we allocate memory on

00:43:20,960 --> 00:43:23,960
the CPU and then we push it to the GPU

00:43:22,670 --> 00:43:26,660
we prefetch it so all the pages

00:43:23,960 --> 00:43:29,000
everything locate on the GPU now if CPU

00:43:26,660 --> 00:43:31,430
wants to use that memory on to address

00:43:29,000 --> 00:43:34,340
this memory and use data what happens it

00:43:31,430 --> 00:43:35,780
will address through the end villain so

00:43:34,340 --> 00:43:38,390
it will not go from the CPU to the GPU

00:43:35,780 --> 00:43:40,550
will go from the GPU to CP it will go

00:43:38,390 --> 00:43:43,340
from the CPU to the GPU right so kind of

00:43:40,550 --> 00:43:48,200
reverse zero copy operation and it will

00:43:43,340 --> 00:43:50,500
cache the data in the CPU l3 cache okay

00:43:48,200 --> 00:43:53,210
so there will be no page migration

00:43:50,500 --> 00:43:57,920
implicit page migration during the

00:43:53,210 --> 00:44:01,310
execution time here we run the same code

00:43:57,920 --> 00:44:04,850
and we put two processors per GPU using

00:44:01,310 --> 00:44:07,580
MPs and we have slightly better

00:44:04,850 --> 00:44:11,780
performance about 10% better performance

00:44:07,580 --> 00:44:17,110
and also with eight years we get better

00:44:11,780 --> 00:44:17,110
execution time so did answer

00:44:20,040 --> 00:44:24,790
yes it will be pretty much as writing

00:44:23,230 --> 00:44:26,230
the code on the CPU because if you'll go

00:44:24,790 --> 00:44:31,030
with the end willing bandits maybe a

00:44:26,230 --> 00:44:32,710
little bit better if it's a flop limited

00:44:31,030 --> 00:44:35,050
application like the gem for example you

00:44:32,710 --> 00:44:41,200
can read from the CPU you still will

00:44:35,050 --> 00:44:44,890
benefit from the GPU but yes nested

00:44:41,200 --> 00:44:47,500
parallelism so in this example what I'm

00:44:44,890 --> 00:44:50,290
doing here I say well ok I have two GPUs

00:44:47,500 --> 00:44:52,900
in my system in my socket and I have a

00:44:50,290 --> 00:44:54,760
lot of course so and I have some problem

00:44:52,900 --> 00:44:57,130
to solve so I will just take part of the

00:44:54,760 --> 00:44:59,800
problem or some kernel and a floatie to

00:44:57,130 --> 00:45:03,160
GPU 0 another kernel or another part of

00:44:59,800 --> 00:45:04,900
the program I will flow to GPU 1 and the

00:45:03,160 --> 00:45:06,730
rest I will solve on the CPUs and all

00:45:04,900 --> 00:45:08,290
the cores I'm trying to maximize the use

00:45:06,730 --> 00:45:10,690
of all the computer resources in the

00:45:08,290 --> 00:45:12,970
system some applications make sense for

00:45:10,690 --> 00:45:14,619
some application doesn't but the

00:45:12,970 --> 00:45:16,809
capability is there so what I do here I

00:45:14,619 --> 00:45:19,240
just create pragma OMP parallel region

00:45:16,809 --> 00:45:21,880
on the GPU so now i have a number of

00:45:19,240 --> 00:45:25,569
threads in this case 3 threads if it's

00:45:21,880 --> 00:45:28,390
thread number 0 i will offload work to

00:45:25,569 --> 00:45:30,700
device 0 if it's thread number 1

00:45:28,390 --> 00:45:32,319
so this more like a cartoon isn't real

00:45:30,700 --> 00:45:37,210
code it will show the code I will

00:45:32,319 --> 00:45:39,760
offload a task to gp1 and otherwise I

00:45:37,210 --> 00:45:42,609
will create a nested parallel region and

00:45:39,760 --> 00:45:45,160
I will use the rest of the CPU cores and

00:45:42,609 --> 00:45:48,369
threads in order to solve the remaining

00:45:45,160 --> 00:45:51,579
of the problem so here is a little code

00:45:48,369 --> 00:45:54,130
sure if there pick if the slides will be

00:45:51,579 --> 00:45:57,270
available right so for those who

00:45:54,130 --> 00:46:00,069
interest probably can just copy this and

00:45:57,270 --> 00:46:02,589
is the entire code so just compile and

00:46:00,069 --> 00:46:04,329
run it pretty much right so it shows how

00:46:02,589 --> 00:46:06,910
I create the region on the CPU power

00:46:04,329 --> 00:46:09,609
region on the CPU select the device that

00:46:06,910 --> 00:46:11,619
I want to use a launch the kernel and

00:46:09,609 --> 00:46:14,859
another thread will select a different

00:46:11,619 --> 00:46:17,349
device launch a different kernel and the

00:46:14,859 --> 00:46:20,700
remaining threads will perform

00:46:17,349 --> 00:46:20,700
calculations on the CPU

00:46:22,040 --> 00:46:25,730
I'll show another example also from

00:46:24,140 --> 00:46:29,210
Lelouch where we have nested parallelism

00:46:25,730 --> 00:46:30,680
so we have a couple of threads basic

00:46:29,210 --> 00:46:32,540
problem this section right a couple of

00:46:30,680 --> 00:46:36,760
threads running on CPU and each

00:46:32,540 --> 00:46:39,530
executing different tasks so each thread

00:46:36,760 --> 00:46:41,570
lunch a kernel on the GPU so now two

00:46:39,530 --> 00:46:43,820
kernels can run in parallel because they

00:46:41,570 --> 00:46:46,310
go on from different threads they will

00:46:43,820 --> 00:46:49,670
go so also over different streams

00:46:46,310 --> 00:46:52,310
because OpenMP runtime IBM's openmp

00:46:49,670 --> 00:46:55,940
runtime will pick a different stream for

00:46:52,310 --> 00:47:00,740
every consecutive Cardinal all mem copy

00:46:55,940 --> 00:47:04,010
and so on here we have another example

00:47:00,740 --> 00:47:06,590
of nested parallelism so again we create

00:47:04,010 --> 00:47:09,619
two threads on the CPU each thread

00:47:06,590 --> 00:47:13,130
launches a kernel but it calls a

00:47:09,619 --> 00:47:15,290
function that function can be run on

00:47:13,130 --> 00:47:17,300
either CPU on the GPU there is this

00:47:15,290 --> 00:47:19,310
little if target use device so use

00:47:17,300 --> 00:47:22,760
device can be 0 then it will run on the

00:47:19,310 --> 00:47:24,500
CPU using more threads the rest of the

00:47:22,760 --> 00:47:26,890
available threads or you can run on the

00:47:24,500 --> 00:47:29,000
device if use devices equal 1 and

00:47:26,890 --> 00:47:32,210
basically we can get either

00:47:29,000 --> 00:47:35,660
remaining of the CPU threads solving

00:47:32,210 --> 00:47:42,530
these problems or there are two kernels

00:47:35,660 --> 00:47:44,480
that will be launched on a device few

00:47:42,530 --> 00:47:48,200
words about a synchronous execution so

00:47:44,480 --> 00:47:51,380
there was some example before we saw

00:47:48,200 --> 00:47:53,030
that when GPU is computing so this is

00:47:51,380 --> 00:47:55,820
the utilization of GPU all these

00:47:53,030 --> 00:47:57,109
colorful boxes represent a kernels

00:47:55,820 --> 00:47:59,720
running on the GPU and you see this

00:47:57,109 --> 00:48:01,640
white spaces white spaces means GP is

00:47:59,720 --> 00:48:04,880
doing nothing so it's kind of wasted

00:48:01,640 --> 00:48:06,890
investment and we want to eliminate

00:48:04,880 --> 00:48:09,440
these white spaces so now while there

00:48:06,890 --> 00:48:12,230
are white spaces while there are gaps in

00:48:09,440 --> 00:48:14,240
GPU performance is because it takes some

00:48:12,230 --> 00:48:17,420
time to launch a kernel it takes some

00:48:14,240 --> 00:48:20,540
time to perform M copy so kernels cannot

00:48:17,420 --> 00:48:22,190
go back to back right if everything is

00:48:20,540 --> 00:48:24,800
synchronous synchronous in this case

00:48:22,190 --> 00:48:27,109
means when CPU launches the kernel it

00:48:24,800 --> 00:48:29,390
will do nothing until kernel is complete

00:48:27,109 --> 00:48:30,960
so it's synchronous between the CPU and

00:48:29,390 --> 00:48:35,500
the GPU

00:48:30,960 --> 00:48:38,200
openmp 4.5 and above allowed to use a

00:48:35,500 --> 00:48:40,990
synchronous execution through the no

00:48:38,200 --> 00:48:43,000
wait and depend clauses so it will the

00:48:40,990 --> 00:48:46,180
depend class will basically specify what

00:48:43,000 --> 00:48:49,630
is the dependency right I can have for

00:48:46,180 --> 00:48:52,180
example 500 kernels and kernel number 37

00:48:49,630 --> 00:48:54,370
will depend on a outcome of kernel

00:48:52,180 --> 00:48:57,850
number 2 so I can use the same variable

00:48:54,370 --> 00:49:00,580
for depend clause so the kernel number

00:48:57,850 --> 00:49:04,090
37 will start only after kernel number 2

00:49:00,580 --> 00:49:09,940
will finish right so now with a

00:49:04,090 --> 00:49:10,810
synchronous execution when first kernel

00:49:09,940 --> 00:49:14,230
has been launched

00:49:10,810 --> 00:49:17,260
then CPU becomes available for the next

00:49:14,230 --> 00:49:19,780
task and the next task for the CPU is to

00:49:17,260 --> 00:49:23,710
launch the next kernel so it launch the

00:49:19,780 --> 00:49:25,390
next kernel it goes to the zip GPU it

00:49:23,710 --> 00:49:27,010
can run in parallel if there are enough

00:49:25,390 --> 00:49:28,630
resources on the GPU if there are not

00:49:27,010 --> 00:49:31,840
enough resources will be queued and it

00:49:28,630 --> 00:49:33,700
will wait for its own turn but the CPU

00:49:31,840 --> 00:49:35,770
become available to launch the next

00:49:33,700 --> 00:49:39,670
kernel launch the next main copy so it

00:49:35,770 --> 00:49:41,170
pipeline tasks and as you can see the

00:49:39,670 --> 00:49:43,680
number of white spaces these gaps

00:49:41,170 --> 00:49:47,200
between the GPU kernels now narrow to

00:49:43,680 --> 00:49:50,380
minimal amount and some kernels also run

00:49:47,200 --> 00:49:52,830
in parallel on the GPU the performance

00:49:50,380 --> 00:49:59,800
impact is quite substantial in this case

00:49:52,830 --> 00:50:04,270
it's about things like 30% or so GPU

00:49:59,800 --> 00:50:06,040
shared memory does drink an ability you

00:50:04,270 --> 00:50:08,200
need to explain with GPU shared memories

00:50:06,040 --> 00:50:09,940
so chip your shared memory is a region

00:50:08,200 --> 00:50:12,190
and l1 cache that can be explicitly

00:50:09,940 --> 00:50:16,240
managed by the application so l1 cursed

00:50:12,190 --> 00:50:18,670
is very close to the compute devices the

00:50:16,240 --> 00:50:21,700
latency to l1 cache is very low the

00:50:18,670 --> 00:50:23,620
Bannister l1 cache is very high and in

00:50:21,700 --> 00:50:26,020
some plication it makes sense to use

00:50:23,620 --> 00:50:30,340
explicitly as a scratch pad and use and

00:50:26,020 --> 00:50:32,770
play some a data in the l1 cache instead

00:50:30,340 --> 00:50:33,510
of pulling it constantly from the main

00:50:32,770 --> 00:50:36,700
memory

00:50:33,510 --> 00:50:41,530
HBM memory so with CUDA

00:50:36,700 --> 00:50:44,110
it's quite easy to allocate a buffers in

00:50:41,530 --> 00:50:46,780
the l1 cache or in the main

00:50:44,110 --> 00:50:49,150
in shared memory shared between the

00:50:46,780 --> 00:50:53,460
threads of the block with open and P

00:50:49,150 --> 00:50:57,070
there is no API there is no way to

00:50:53,460 --> 00:50:59,350
explicitly allocate but there are some

00:50:57,070 --> 00:51:02,110
spots in the compiler that we put that

00:50:59,350 --> 00:51:05,230
may detect the static memory location as

00:51:02,110 --> 00:51:08,140
you see in line 29 we is known to the

00:51:05,230 --> 00:51:11,620
compiler the compile time size of the

00:51:08,140 --> 00:51:14,380
buffers and if the size of this buffer

00:51:11,620 --> 00:51:16,570
val is small enough compared to the size

00:51:14,380 --> 00:51:18,250
of the shared memory it will allocate

00:51:16,570 --> 00:51:20,470
this buffer in the shared memory and

00:51:18,250 --> 00:51:22,450
this will give us huge speed-up you can

00:51:20,470 --> 00:51:25,240
see that in this example if we are using

00:51:22,450 --> 00:51:27,730
shared memory we run this basically

00:51:25,240 --> 00:51:30,880
matrix transposition algorithm at about

00:51:27,730 --> 00:51:33,400
900 gigabyte per second so we maximize

00:51:30,880 --> 00:51:36,880
actually a little bit less will maximize

00:51:33,400 --> 00:51:38,680
the GPU bandwidth and if we are not

00:51:36,880 --> 00:51:40,390
using shared memory that memory

00:51:38,680 --> 00:51:42,780
allocation will take time because memory

00:51:40,390 --> 00:51:45,880
will be allocated in the HBM by each

00:51:42,780 --> 00:51:49,720
team or each CUDA block and the data

00:51:45,880 --> 00:51:50,260
will go from the GPU main memory to the

00:51:49,720 --> 00:51:53,500
sames

00:51:50,260 --> 00:51:59,020
and back at lower pace so it will run at

00:51:53,500 --> 00:52:01,010
about 40 gigabyte per second and I'm

00:51:59,020 --> 00:52:06,839
done

00:52:01,010 --> 00:52:06,839

YouTube URL: https://www.youtube.com/watch?v=JP0ISJGDODE


