Title: OpenPOWER Summit Europe 2018: Power Performance Porting
Publication date: 2018-11-26
Playlist: OpenPOWER Summit Europe 2018
Description: 
	Daniel Black, Linux Technology Center, speaks at OpenPOWER Summit Europe 2018.

For more information, please visit: https://openpowerfoundation.org/summit-2018-10-eu/
Captions: 
	00:00:01,590 --> 00:00:08,700
i'm daniel i'm from linux technology

00:00:04,859 --> 00:00:12,570
center also so what I'm actually talking

00:00:08,700 --> 00:00:14,580
about is a result of when fir onyx back

00:00:12,570 --> 00:00:17,160
in June to the benchmark on the talus to

00:00:14,580 --> 00:00:19,500
work station on the paranoid chips and

00:00:17,160 --> 00:00:22,080
while a lot of the benchmark results

00:00:19,500 --> 00:00:24,180
came across quite well

00:00:22,080 --> 00:00:27,539
we looked at some of the tests and go we

00:00:24,180 --> 00:00:30,420
shouldn't actually be that bad on it so

00:00:27,539 --> 00:00:33,480
what we found as we looked through some

00:00:30,420 --> 00:00:36,960
of the poorer performing tests there

00:00:33,480 --> 00:00:40,410
were a cover number of factors namely

00:00:36,960 --> 00:00:44,640
you know the application wasn't well

00:00:40,410 --> 00:00:48,300
tuned for the test and with a little bit

00:00:44,640 --> 00:00:50,700
of basic performance use of basic perf

00:00:48,300 --> 00:00:53,190
and compile time and build time options

00:00:50,700 --> 00:00:56,900
were able to improve the performance so

00:00:53,190 --> 00:00:56,900
that's what I'm going to cover today so

00:00:57,230 --> 00:01:03,480
I'm going to start really basic ie

00:01:00,300 --> 00:01:07,230
this is a CPU workload that isn't doing

00:01:03,480 --> 00:01:10,200
any CPU if you have this don't try to

00:01:07,230 --> 00:01:12,390
improve the performance on it wait till

00:01:10,200 --> 00:01:14,340
you've got something more like this

00:01:12,390 --> 00:01:17,370
which is all the threads actually in use

00:01:14,340 --> 00:01:20,580
and so once we've got that whether it's

00:01:17,370 --> 00:01:22,530
across all threads or or it's a single

00:01:20,580 --> 00:01:25,500
thread back stamp we've got some

00:01:22,530 --> 00:01:27,330
actually opportunity to know that it's

00:01:25,500 --> 00:01:31,110
going fast as possible and we can

00:01:27,330 --> 00:01:34,260
improve the performance on that occasion

00:01:31,110 --> 00:01:36,990
we'll get a result like this which was

00:01:34,260 --> 00:01:41,580
the blender test in the frantic test

00:01:36,990 --> 00:01:44,700
suite for this and this was apparently a

00:01:41,580 --> 00:01:47,250
CPU test but down on the the very fine

00:01:44,700 --> 00:01:50,610
print is the word like GPU right here

00:01:47,250 --> 00:01:52,860
which was the first indicator that maybe

00:01:50,610 --> 00:01:56,340
that's some reason why it's only running

00:01:52,860 --> 00:01:59,340
16 threads now after searching the

00:01:56,340 --> 00:02:04,370
codebase for things like Mac CPUs and

00:01:59,340 --> 00:02:06,299
all another few dead ends that way

00:02:04,370 --> 00:02:08,429
discuss someone who actually knew

00:02:06,299 --> 00:02:10,410
something about blender and how to use

00:02:08,429 --> 00:02:14,190
it and discover there's a thing called

00:02:10,410 --> 00:02:18,810
tile size and 256 by 2

00:02:14,190 --> 00:02:22,560
forty tile size when you're producing a

00:02:18,810 --> 00:02:25,350
1k by 1k image doesn't leave actually

00:02:22,560 --> 00:02:27,570
that many threads that can be used so

00:02:25,350 --> 00:02:30,090
what typically happens with CPU

00:02:27,570 --> 00:02:36,660
workloads so I found out is this figure

00:02:30,090 --> 00:02:39,270
is more like 32 by 32 so and that's why

00:02:36,660 --> 00:02:42,360
the FIR Onix test we'd had a blender

00:02:39,270 --> 00:02:45,030
fire a GPU and a cpu version

00:02:42,360 --> 00:02:47,210
it's just they weren't using the CPU so

00:02:45,030 --> 00:02:50,910
that was easy to fix

00:02:47,210 --> 00:02:53,220
so for harder problems we start by using

00:02:50,910 --> 00:02:56,820
perf to identify where in the

00:02:53,220 --> 00:02:59,100
application stack is the problem so the

00:02:56,820 --> 00:03:02,060
first thing to when you're actually

00:02:59,100 --> 00:03:04,830
using perfect good to actually resolve

00:03:02,060 --> 00:03:07,800
addresses to actual symbols and in

00:03:04,830 --> 00:03:10,830
function names that actually a lot more

00:03:07,800 --> 00:03:13,560
meaningful so every distribution will

00:03:10,830 --> 00:03:15,780
have a bunch of debug info that can be

00:03:13,560 --> 00:03:18,060
installed in various ways or if you're

00:03:15,780 --> 00:03:22,590
compiling your own you just use the dash

00:03:18,060 --> 00:03:27,870
G see if I exhaustive plus plus folks to

00:03:22,590 --> 00:03:30,330
get the bug symbols on those so using

00:03:27,870 --> 00:03:33,989
the perf top command this is what the

00:03:30,330 --> 00:03:35,820
blender work load was running and as we

00:03:33,989 --> 00:03:38,610
can see if we were interested in

00:03:35,820 --> 00:03:41,160
improving the performance of this the

00:03:38,610 --> 00:03:44,640
top two functions of that consuming like

00:03:41,160 --> 00:03:48,630
37 and 23% and those would be a good

00:03:44,640 --> 00:03:52,760
place to start and this is just like C++

00:03:48,630 --> 00:04:00,300
names so I mean team angles quite easily

00:03:52,760 --> 00:04:04,340
after that so for something a bit longer

00:04:00,300 --> 00:04:08,130
run we won't actually do a perfect chord

00:04:04,340 --> 00:04:10,920
- G to get the full stack we specify an

00:04:08,130 --> 00:04:13,620
output file name we especially like the

00:04:10,920 --> 00:04:16,410
process ID we're just going to put on

00:04:13,620 --> 00:04:19,650
Python and so this is for the

00:04:16,410 --> 00:04:22,890
scikit-learn application and I've just

00:04:19,650 --> 00:04:27,500
don't sleep 30 at the end and that's

00:04:22,890 --> 00:04:32,400
just telling the perf to

00:04:27,500 --> 00:04:34,979
record for 30 seconds so when I do a

00:04:32,400 --> 00:04:38,039
perf report based on the same input file

00:04:34,979 --> 00:04:40,770
adding a little percent limit so it

00:04:38,039 --> 00:04:42,840
doesn't go on indefinitely we see this

00:04:40,770 --> 00:04:45,630
is like a top-down view of the stack so

00:04:42,840 --> 00:04:47,520
up the top we see you know the second

00:04:45,630 --> 00:04:49,919
line most of its in like the main

00:04:47,520 --> 00:04:52,889
function go down a couple more lines

00:04:49,919 --> 00:04:55,789
that's all in the past and main and we

00:04:52,889 --> 00:05:00,840
go down and we really can't actually see

00:04:55,789 --> 00:05:04,169
it an obvious place to actually apply

00:05:00,840 --> 00:05:07,320
performance improvement so the key with

00:05:04,169 --> 00:05:10,440
actually using perf report is to use the

00:05:07,320 --> 00:05:12,330
no children option so when with the use

00:05:10,440 --> 00:05:14,759
the no children option here we can see

00:05:12,330 --> 00:05:18,199
most of the CPU time in this benchmark

00:05:14,759 --> 00:05:22,410
is in an underlying library called Blas

00:05:18,199 --> 00:05:24,240
in this one function called the GE mmm

00:05:22,410 --> 00:05:28,710
which is just a mouthful especially when

00:05:24,240 --> 00:05:30,810
it got cold so the no children helps to

00:05:28,710 --> 00:05:32,909
identify which functions need to be

00:05:30,810 --> 00:05:35,580
improved and in this case it's not

00:05:32,909 --> 00:05:38,099
actually in the the Python library or

00:05:35,580 --> 00:05:41,970
the the psyche test it's actually in an

00:05:38,099 --> 00:05:46,110
underlying library so the underlying

00:05:41,970 --> 00:05:50,010
library was pulled in by numpy which is

00:05:46,110 --> 00:05:54,000
a Python numeric library and we see with

00:05:50,010 --> 00:05:56,310
the distro tools that in this case of

00:05:54,000 --> 00:06:00,210
munchy that it depends on either lib

00:05:56,310 --> 00:06:05,820
blasts or anything the report provides

00:06:00,210 --> 00:06:08,090
the lib blast so3 library using an app

00:06:05,820 --> 00:06:09,419
file search we see there's another

00:06:08,090 --> 00:06:12,570
implementation called

00:06:09,419 --> 00:06:17,009
open Blas which is received some

00:06:12,570 --> 00:06:21,750
optimizations so by simply just doing an

00:06:17,009 --> 00:06:23,150
app install lib open blasts if base it

00:06:21,750 --> 00:06:26,430
will actually replace the existing

00:06:23,150 --> 00:06:29,419
implementations with the optimized one

00:06:26,430 --> 00:06:34,409
and it's a bi compatible so it'll and

00:06:29,419 --> 00:06:38,360
Anna and Bri running the test we go from

00:06:34,409 --> 00:06:42,180
the test running at

00:06:38,360 --> 00:06:44,940
328 seconds down to 26 seconds which is

00:06:42,180 --> 00:06:48,620
a pretty good in performance improvement

00:06:44,940 --> 00:06:51,020
for just installing a different package

00:06:48,620 --> 00:06:55,080
see I told you this was all basic stuff

00:06:51,020 --> 00:06:57,930
once you know here if we look at like

00:06:55,080 --> 00:07:00,450
the perfect report of it running under

00:06:57,930 --> 00:07:04,050
open PLAs we see that instead of like

00:07:00,450 --> 00:07:06,060
the DGE mm function there it's been

00:07:04,050 --> 00:07:07,940
replaced with a couple of

00:07:06,060 --> 00:07:10,530
similar-looking names which are

00:07:07,940 --> 00:07:17,340
obviously the power optimized versions

00:07:10,530 --> 00:07:22,050
of those another test was like the

00:07:17,340 --> 00:07:24,450
OpenSSL in the FIR onyx and the version

00:07:22,050 --> 00:07:29,340
tested was fished in one point one point

00:07:24,450 --> 00:07:31,140
zero and when we just like pulled down a

00:07:29,340 --> 00:07:33,600
latest version of one point one point

00:07:31,140 --> 00:07:36,120
two we discovered some work had actually

00:07:33,600 --> 00:07:39,210
gone on and we were getting an RSA

00:07:36,120 --> 00:07:42,480
signing speed of twice the rate in

00:07:39,210 --> 00:07:44,610
verifying about the same so just trying

00:07:42,480 --> 00:07:50,030
a later version can often give you a

00:07:44,610 --> 00:07:53,460
performance increase the third test

00:07:50,030 --> 00:07:56,430
third fourth what number I'm up to one

00:07:53,460 --> 00:08:00,660
of the other tests that Forex did was on

00:07:56,430 --> 00:08:07,290
an mp3 benchmark using the lame mp3

00:08:00,660 --> 00:08:09,330
encoder so in the profile report this is

00:08:07,290 --> 00:08:16,560
showing that both the count beats is at

00:08:09,330 --> 00:08:18,480
the top of the usage so this isn't a a

00:08:16,560 --> 00:08:21,080
kind of function name that sort of

00:08:18,480 --> 00:08:24,240
implies it's going to use a lot of CPU

00:08:21,080 --> 00:08:26,280
so we've looked a bit closer

00:08:24,240 --> 00:08:28,590
we installed you cyghfer onyx and and

00:08:26,280 --> 00:08:30,960
looked at the install log and found that

00:08:28,590 --> 00:08:36,330
it was actually compiled with like no

00:08:30,960 --> 00:08:40,320
optimization flags at all so to give you

00:08:36,330 --> 00:08:42,660
a quick write and demo on you know what

00:08:40,320 --> 00:08:49,830
different optimization flags we can do

00:08:42,660 --> 00:08:51,720
i've compiled the lame with - save temps

00:08:49,830 --> 00:08:53,610
so we get

00:08:51,720 --> 00:08:55,850
all the pre-processed headers are

00:08:53,610 --> 00:08:59,090
actually included into into one file

00:08:55,850 --> 00:09:03,810
which makes it easier to transplant in

00:08:59,090 --> 00:09:12,540
transplant copy into the compiler

00:09:03,810 --> 00:09:15,530
Explorer so what I've done here on this

00:09:12,540 --> 00:09:19,980
is basically cotton paste from the

00:09:15,530 --> 00:09:22,260
output of that file on so we've got this

00:09:19,980 --> 00:09:24,270
count bits function and what I've needed

00:09:22,260 --> 00:09:30,810
to do is just grab a number of other

00:09:24,270 --> 00:09:35,700
things from that file so while there are

00:09:30,810 --> 00:09:41,220
some power compilers up here I'll just

00:09:35,700 --> 00:09:47,640
do the easy option and do target equals

00:09:41,220 --> 00:09:49,590
PowerPC 64 le and this is like just in

00:09:47,640 --> 00:09:51,560
real time well it's sending the file to

00:09:49,590 --> 00:09:56,910
the server and getting it to compile

00:09:51,560 --> 00:10:02,250
it's a quick way of testing what changes

00:09:56,910 --> 00:10:12,000
are going to happen so to test this a

00:10:02,250 --> 00:10:17,970
bit more and then here's one I prepared

00:10:12,000 --> 00:10:20,340
earlier so what we got is like one

00:10:17,970 --> 00:10:24,300
window here is where we've got one

00:10:20,340 --> 00:10:27,480
compile option over here we've got the

00:10:24,300 --> 00:10:31,320
other and we've got a difference view

00:10:27,480 --> 00:10:33,990
down the bottom so as you probably saw

00:10:31,320 --> 00:10:37,080
partly there one thing you can do to

00:10:33,990 --> 00:10:41,060
improve the optimization when you can

00:10:37,080 --> 00:10:45,780
type correctly is do like - oh one and

00:10:41,060 --> 00:10:47,970
what this shows is that the output is

00:10:45,780 --> 00:10:50,130
actually different and if we look down

00:10:47,970 --> 00:10:53,160
through the right-hand side we can see

00:10:50,130 --> 00:10:58,140
that the output is like significantly

00:10:53,160 --> 00:11:03,450
smaller and it's used larger addresses

00:10:58,140 --> 00:11:05,839
on things and that's especially in like

00:11:03,450 --> 00:11:08,790
the main body of the loop

00:11:05,839 --> 00:11:11,880
and it's just some testing instructions

00:11:08,790 --> 00:11:15,000
there so that's just you know at the oh

00:11:11,880 --> 00:11:17,700
one one go a little bit further let's

00:11:15,000 --> 00:11:23,310
change the base 201 and see what

00:11:17,700 --> 00:11:25,470
difference I two makes so unlike oh one

00:11:23,310 --> 00:11:28,560
before where we got a significantly

00:11:25,470 --> 00:11:32,510
shorter one what we get with O 2 is the

00:11:28,560 --> 00:11:36,690
actual code is significantly longer and

00:11:32,510 --> 00:11:38,130
see up the top so in the beginning of

00:11:36,690 --> 00:11:41,370
the functions it's staving off all the

00:11:38,130 --> 00:11:43,170
variables to the stack so I think that

00:11:41,370 --> 00:11:46,170
it's saving off more variables this is

00:11:43,170 --> 00:11:49,170
actually using more variables and we

00:11:46,170 --> 00:11:53,310
could see that further down that we've

00:11:49,170 --> 00:11:55,860
got instead of a little of here we've

00:11:53,310 --> 00:11:59,640
got a bunch of vectorized operations

00:11:55,860 --> 00:12:03,540
down here and despite being a longer

00:11:59,640 --> 00:12:05,850
function in this case it actually power

00:12:03,540 --> 00:12:10,230
pipelines these kind of instructions

00:12:05,850 --> 00:12:14,910
quite well so this is one way of

00:12:10,230 --> 00:12:17,640
achieving a performance increase if we

00:12:14,910 --> 00:12:20,700
have a look at o2 and seen what

00:12:17,640 --> 00:12:22,770
difference are three makes we find it

00:12:20,700 --> 00:12:24,600
makes no difference at all oh two and A

00:12:22,770 --> 00:12:26,970
three are actually the same output for

00:12:24,600 --> 00:12:32,399
this particular function in the entire

00:12:26,970 --> 00:12:32,940
data set so questions see if you're

00:12:32,399 --> 00:12:34,980
still awake

00:12:32,940 --> 00:12:44,130
what other optimization could we

00:12:34,980 --> 00:12:49,110
actually try on this compiler yep yep so

00:12:44,130 --> 00:12:53,490
if we say oh - as a base ok so the

00:12:49,110 --> 00:12:57,930
option was - M tune actually I'm going

00:12:53,490 --> 00:13:02,010
to use the mCP you equals so let's just

00:12:57,930 --> 00:13:04,829
start with like power I to start at a

00:13:02,010 --> 00:13:08,250
baseline and it's made no difference

00:13:04,829 --> 00:13:13,110
it's actually treating power rate as the

00:13:08,250 --> 00:13:17,850
the basic change for this anyway but if

00:13:13,110 --> 00:13:18,960
we change it to power 9 we see that the

00:13:17,850 --> 00:13:23,430
output code

00:13:18,960 --> 00:13:25,700
has been reduced again and so whereas

00:13:23,430 --> 00:13:28,290
before impair rate we had these

00:13:25,700 --> 00:13:30,150
instructions I know an expression area

00:13:28,290 --> 00:13:34,200
assembly because I don't know them that

00:13:30,150 --> 00:13:38,130
well either we've got a bunch of

00:13:34,200 --> 00:13:40,110
operations here but once we've told us

00:13:38,130 --> 00:13:44,850
the compiler that we're going for power

00:13:40,110 --> 00:13:48,590
9 we've gone to the vectorized

00:13:44,850 --> 00:13:48,590
operations that are supported on power 9

00:13:48,740 --> 00:13:59,100
so it's no surprise from there that we

00:13:53,730 --> 00:14:02,850
try those oath 302 that's the same in mp

00:13:59,100 --> 00:14:05,910
cpu is it's like power 9 & reran the

00:14:02,850 --> 00:14:08,880
tests so removed the existing one and

00:14:05,910 --> 00:14:13,230
rerun it and what we ended up with is

00:14:08,880 --> 00:14:18,660
going from 98 seconds down to 21 seconds

00:14:13,230 --> 00:14:20,790
just by adding compiler flags Joel who

00:14:18,660 --> 00:14:23,190
did the test also looked through the

00:14:20,790 --> 00:14:26,370
code and there was a feature called fast

00:14:23,190 --> 00:14:27,450
log and that wasn't enabled by default

00:14:26,370 --> 00:14:30,140
on power

00:14:27,450 --> 00:14:33,750
she was just someone hadn't tested it

00:14:30,140 --> 00:14:36,480
worked fine and generated output and

00:14:33,750 --> 00:14:44,100
chose to another couple of seconds off

00:14:36,480 --> 00:14:46,230
the benchmark so I told you before the

00:14:44,100 --> 00:14:47,550
the FIR Onix what way of you know

00:14:46,230 --> 00:14:51,810
running benchmarks in the open

00:14:47,550 --> 00:14:54,410
benchmarks which bench suite a test

00:14:51,810 --> 00:14:57,000
suite has got a bunch of build logs

00:14:54,410 --> 00:14:58,950
destroys actually include build logs too

00:14:57,000 --> 00:15:04,800
they're just a little bit harder to find

00:14:58,950 --> 00:15:06,960
so I'll put these up and a bunch of ones

00:15:04,800 --> 00:15:10,230
so they can find them eventually through

00:15:06,960 --> 00:15:13,770
the launchpad packages fedora ones are a

00:15:10,230 --> 00:15:17,520
little bit easier to find on koji so

00:15:13,770 --> 00:15:20,720
there and we see that lame at least in

00:15:17,520 --> 00:15:23,820
the distros has got like a - OH - flag

00:15:20,720 --> 00:15:28,130
they've got to support parrots so you

00:15:23,820 --> 00:15:28,130
know it's chained for power right

00:15:30,959 --> 00:15:36,480
one other function that wasn't actually

00:15:33,370 --> 00:15:39,880
part of the frantic twist suite but was

00:15:36,480 --> 00:15:43,600
something was testing for a customer was

00:15:39,880 --> 00:15:47,440
just an OLTP test on Maury dB so this is

00:15:43,600 --> 00:15:51,610
Maury DB ten point three point nine on

00:15:47,440 --> 00:15:53,709
just a basic data based test and we see

00:15:51,610 --> 00:15:57,940
that like this mmm copy function right

00:15:53,709 --> 00:16:05,800
at the top using 4.8% effectively of the

00:15:57,940 --> 00:16:07,830
CPU so what happened in GCC 2.7 was

00:16:05,800 --> 00:16:10,540
there was a bunch of Chernobyl's added

00:16:07,830 --> 00:16:14,459
one of the Chernobyl's was an

00:16:10,540 --> 00:16:18,790
optimization where you could tell the

00:16:14,459 --> 00:16:21,850
GCC library within a program that was

00:16:18,790 --> 00:16:24,880
operating on cased memory so why this is

00:16:21,850 --> 00:16:26,589
important for man copy was there's a

00:16:24,880 --> 00:16:28,959
bunch of instructions that can actually

00:16:26,589 --> 00:16:31,330
copy quite quickly but they don't

00:16:28,959 --> 00:16:34,839
actually work if you're on like memory

00:16:31,330 --> 00:16:38,560
mapped i/o so those couldn't actually be

00:16:34,839 --> 00:16:40,390
enabled by default however knowing what

00:16:38,560 --> 00:16:44,110
murray DB does it doesn't actually

00:16:40,390 --> 00:16:46,839
operate on memory mapped i/o devices so

00:16:44,110 --> 00:16:50,310
it can turn this in the environment in

00:16:46,839 --> 00:16:53,980
this case of putting it in a assistant

00:16:50,310 --> 00:16:56,490
e-service Edition and what we see is

00:16:53,980 --> 00:17:00,399
that the previous four point eight

00:16:56,490 --> 00:17:04,020
percent of in the mem copy has gone down

00:17:00,399 --> 00:17:08,650
to number three on the list which is

00:17:04,020 --> 00:17:12,220
what is it three point three percent of

00:17:08,650 --> 00:17:15,370
CPUs and used a different function in in

00:17:12,220 --> 00:17:17,760
the process one especially mark focused

00:17:15,370 --> 00:17:17,760
memory

00:17:19,340 --> 00:17:26,870
this is in this is in G Lib C so yeah it

00:17:25,700 --> 00:17:32,090
doesn't matter actually what you can

00:17:26,870 --> 00:17:34,720
pile with it as long as it's GCC what

00:17:32,090 --> 00:17:40,400
that's a two point two seven we're above

00:17:34,720 --> 00:17:45,830
G Lib C did I stop them again definitely

00:17:40,400 --> 00:17:50,390
G live see yeah so yeah and that's

00:17:45,830 --> 00:17:57,110
what's in the latest Ubuntu LTS release

00:17:50,390 --> 00:17:59,270
and and le fedora and yeah those are

00:17:57,110 --> 00:18:05,840
probably the main two more bleeding edge

00:17:59,270 --> 00:18:10,640
ones on power so the other thing can do

00:18:05,840 --> 00:18:14,420
with perfe put is this is tool that

00:18:10,640 --> 00:18:18,910
Netflix called Brennan Greek right do

00:18:14,420 --> 00:18:23,330
does flake fine graphs right this flames

00:18:18,910 --> 00:18:28,250
let's go and so what I've done here is

00:18:23,330 --> 00:18:32,840
I've loaded up the recording that I did

00:18:28,250 --> 00:18:36,230
before and this is a representation of

00:18:32,840 --> 00:18:39,350
time actually going through so we can

00:18:36,230 --> 00:18:41,900
see here that the workload of the my

00:18:39,350 --> 00:18:45,650
skills test week where I recorded was

00:18:41,900 --> 00:18:51,050
fairly uniform by the color there was a

00:18:45,650 --> 00:18:52,820
glitch early on I suspect there was not

00:18:51,050 --> 00:18:54,200
related to actually what it was running

00:18:52,820 --> 00:18:58,460
but it was just the process of

00:18:54,200 --> 00:19:03,470
initializing the test suite and sorry

00:18:58,460 --> 00:19:07,940
their first recording so I can select

00:19:03,470 --> 00:19:11,240
like an interval here and this will

00:19:07,940 --> 00:19:15,050
display a top-down view of what's

00:19:11,240 --> 00:19:16,970
happening so it's my skill D process so

00:19:15,050 --> 00:19:19,550
I'm like before when I was recording a

00:19:16,970 --> 00:19:22,100
and just the process here of recording

00:19:19,550 --> 00:19:24,980
everything on the system which you can

00:19:22,100 --> 00:19:29,030
tell because I've run got the test

00:19:24,980 --> 00:19:31,170
program running on the same system which

00:19:29,030 --> 00:19:34,740
is the suspense

00:19:31,170 --> 00:19:38,250
and so this is you know a bottom down

00:19:34,740 --> 00:19:41,070
view of what's actually in the stack so

00:19:38,250 --> 00:19:44,130
where this might be useful is if say

00:19:41,070 --> 00:19:47,250
there was a background process in

00:19:44,130 --> 00:19:49,470
another area of the stack you could

00:19:47,250 --> 00:19:53,340
actually compare the two outputs and and

00:19:49,470 --> 00:19:58,680
see make sure that what you're seeing in

00:19:53,340 --> 00:20:00,410
your perf report results maps to

00:19:58,680 --> 00:20:03,000
something you actually want to optimize

00:20:00,410 --> 00:20:08,910
rather than maybe a background process

00:20:03,000 --> 00:20:14,490
which may not be as important okay so

00:20:08,910 --> 00:20:16,650
it's a time series grass so this is just

00:20:14,490 --> 00:20:21,360
sorry where the mouse is is the

00:20:16,650 --> 00:20:24,330
beginning of the samples and it just

00:20:21,360 --> 00:20:26,940
goes on and it's done for for some

00:20:24,330 --> 00:20:30,810
reason 30 oh that's right

00:20:26,940 --> 00:20:33,630
30 seconds down the bottom in the fine

00:20:30,810 --> 00:20:36,690
print which isn't very obvious and I to

00:20:33,630 --> 00:20:40,920
be honest I've never noticed before idea

00:20:36,690 --> 00:20:47,550
if the person does by default sampling

00:20:40,920 --> 00:20:53,850
at some sample right it's just a rolling

00:20:47,550 --> 00:20:58,530
down axis so so so we got one second and

00:20:53,850 --> 00:21:05,640
so many milliseconds going down yep no

00:20:58,530 --> 00:21:07,680
worse and so that's it so in short you

00:21:05,640 --> 00:21:12,240
know I haven't changed a single line of

00:21:07,680 --> 00:21:15,360
code here by using recent releases a bi

00:21:12,240 --> 00:21:19,950
compatible functions equivalent

00:21:15,360 --> 00:21:23,900
libraries optimization flags of like -

00:21:19,950 --> 00:21:30,420
OH - and the EM tuned

00:21:23,900 --> 00:21:32,700
m2m CPU to the right version enabled

00:21:30,420 --> 00:21:35,400
sometimes experimental features or

00:21:32,700 --> 00:21:38,220
features that haven't been tested was

00:21:35,400 --> 00:21:42,030
much on power but are usually fine and

00:21:38,220 --> 00:21:44,999
used GCC Chernobyl's we've actually got

00:21:42,030 --> 00:21:49,919
better results and I've showed you

00:21:44,999 --> 00:21:57,239
couple of tools in that can assist with

00:21:49,919 --> 00:22:00,179
the diagnosis and and as Jeremy said

00:21:57,239 --> 00:22:02,969
earlier there's a stack overflow a group

00:22:00,179 --> 00:22:06,479
if you trying to optimize a power

00:22:02,969 --> 00:22:09,419
workload and have a narrow narrowly

00:22:06,479 --> 00:22:13,729
scoped question that people may be able

00:22:09,419 --> 00:22:20,059
to answer the people use emails this

00:22:13,729 --> 00:22:22,559
first those gray bearded once and so I

00:22:20,059 --> 00:22:24,779
there's a bounty source program where

00:22:22,559 --> 00:22:28,049
IBM is that offered people in the

00:22:24,779 --> 00:22:29,849
community money to actually fix some of

00:22:28,049 --> 00:22:33,799
these problems in open source projects

00:22:29,849 --> 00:22:36,509
however if one is particularly affecting

00:22:33,799 --> 00:22:39,389
your company maybe you can either add

00:22:36,509 --> 00:22:43,619
your own problem or up the bounty for

00:22:39,389 --> 00:22:47,999
something that's already there if you

00:22:43,619 --> 00:22:52,579
come across like some - o'the Flags

00:22:47,999 --> 00:22:55,649
somehow missing from distros you can

00:22:52,579 --> 00:22:57,899
file a destroyed bug report and get it

00:22:55,649 --> 00:22:59,849
fixed that way especially if you can

00:22:57,899 --> 00:23:03,569
show this like a performance benefit in

00:22:59,849 --> 00:23:06,089
some way and the compiler Explorer is

00:23:03,569 --> 00:23:07,949
pretty good up there I've been meaning

00:23:06,089 --> 00:23:12,539
to have half written some code that

00:23:07,949 --> 00:23:16,199
improves the version numbers on some GC

00:23:12,539 --> 00:23:18,209
C compilers up there for power because

00:23:16,199 --> 00:23:20,519
they don't cross compile this easy is

00:23:18,209 --> 00:23:24,179
the ceiling ones by certifying target

00:23:20,519 --> 00:23:26,669
they've got to be their own entity and

00:23:24,179 --> 00:23:29,029
that's all the time for so is there any

00:23:26,669 --> 00:23:29,029
questions

00:23:29,600 --> 00:23:33,820

YouTube URL: https://www.youtube.com/watch?v=cZaUtdRPN6s


