Title: OpenPOWER Summit Europe 2018: Network Protocol Acceleration With CAPI SNAP
Publication date: 2018-11-14
Playlist: OpenPOWER Summit Europe 2018
Description: 
	Dr. Endric Schubert, MLE and Dr. Andrew McCormick, Alpha-Data speak at OpenPOWER Foundation's OpenPOWER Summit Europe 2018.

For more information, please visit: https://openpowerfoundation.org/summit-2018-10-eu/
Captions: 
	00:00:00,680 --> 00:00:05,790
okay good afternoon and as just

00:00:04,470 --> 00:00:08,460
mentioned I managed McCormick I'm the

00:00:05,790 --> 00:00:09,630
technical director at alpha data and I'm

00:00:08,460 --> 00:00:14,400
sort of cool presenting this work

00:00:09,630 --> 00:00:17,550
there's been largely done by email the

00:00:14,400 --> 00:00:20,160
missing link electronics and this is I

00:00:17,550 --> 00:00:24,930
think network protocol acceleration into

00:00:20,160 --> 00:00:27,000
the copy snap environment a snap for

00:00:24,930 --> 00:00:30,260
Lois who did all the first letters our

00:00:27,000 --> 00:00:32,879
storage network and acceleration and

00:00:30,260 --> 00:00:35,070
this work has really been focused on

00:00:32,879 --> 00:00:37,469
making sure we've got that in and that

00:00:35,070 --> 00:00:40,679
acronym the motivation for this

00:00:37,469 --> 00:00:42,600
obviously all your data has to come from

00:00:40,679 --> 00:00:46,550
somewhere so there's certainly

00:00:42,600 --> 00:00:50,780
increasing requirement for our yeah

00:00:46,550 --> 00:00:53,429
networking within datacenter systems a

00:00:50,780 --> 00:00:55,019
certainly increasing demand especially

00:00:53,429 --> 00:00:57,929
moving towards say technologies such as

00:00:55,019 --> 00:00:59,760
5g for low latency processing of

00:00:57,929 --> 00:01:04,309
responses especially coming from where

00:00:59,760 --> 00:01:08,250
your data center responding quickly to

00:01:04,309 --> 00:01:10,260
demand and then there's also a lot of

00:01:08,250 --> 00:01:12,270
applications especially 100 gigabit

00:01:10,260 --> 00:01:18,750
processing that basically did not want

00:01:12,270 --> 00:01:20,580
to the police all the processing

00:01:18,750 --> 00:01:24,870
requirements of their handling things

00:01:20,580 --> 00:01:26,790
like tcp/ip stacks within the mean CPU

00:01:24,870 --> 00:01:30,780
and therefore being able to afford these

00:01:26,790 --> 00:01:35,580
sort of fair tasks to the MGP it opens

00:01:30,780 --> 00:01:37,320
up all of applications and also opens up

00:01:35,580 --> 00:01:39,510
a lot of acceleration opportunities for

00:01:37,320 --> 00:01:43,140
you process the data as it's coming in

00:01:39,510 --> 00:01:46,950
and out of the processing system and I'm

00:01:43,140 --> 00:01:49,080
going to hand over to Hendrick a missing

00:01:46,950 --> 00:01:53,220
link who's going to go into detail about

00:01:49,080 --> 00:01:55,050
this implementation yes thank you Andrew

00:01:53,220 --> 00:01:57,840
so it's been less than a year that we

00:01:55,050 --> 00:02:01,920
get sucked in into open power and open

00:01:57,840 --> 00:02:04,890
copy started at SC 17 when we first

00:02:01,920 --> 00:02:07,759
heard of what I think Brunel called the

00:02:04,890 --> 00:02:13,140
well kept secret of the industry snap

00:02:07,759 --> 00:02:14,069
and being FPGA programmers one of the

00:02:13,140 --> 00:02:16,519
rare breeds of FV

00:02:14,069 --> 00:02:21,359
J programmers who accelerate software

00:02:16,519 --> 00:02:23,010
that open Cappy Cappy snap got quite our

00:02:21,359 --> 00:02:24,930
attention and what we're talking about

00:02:23,010 --> 00:02:27,599
here today is some idea some

00:02:24,930 --> 00:02:29,670
architecture ideas for putting some

00:02:27,599 --> 00:02:33,959
putting networking and in particular

00:02:29,670 --> 00:02:35,939
network acceleration into snap so that

00:02:33,959 --> 00:02:39,810
the quick background I just had a longer

00:02:35,939 --> 00:02:42,540
discussion with a guy from redhead FPGAs

00:02:39,810 --> 00:02:44,639
are not FPGAs anymore they're not poor

00:02:42,540 --> 00:02:46,950
men's gate arrays then they're not even

00:02:44,639 --> 00:02:50,219
just massively parallel compute beasts

00:02:46,950 --> 00:02:52,709
they're fully integrated heater genius

00:02:50,219 --> 00:02:55,919
systems you can say that on the right

00:02:52,709 --> 00:02:59,129
hand side this is one of the not even

00:02:55,919 --> 00:03:01,980
most modern FPGA set signings offers

00:02:59,129 --> 00:03:05,909
there's ink ultra scale MP SRC it has a

00:03:01,980 --> 00:03:08,489
quad arm it runs Linux it has other ARM

00:03:05,909 --> 00:03:10,769
processors which were using for security

00:03:08,489 --> 00:03:12,780
applications and or which can be used

00:03:10,769 --> 00:03:14,669
for other means like scrubbing and so

00:03:12,780 --> 00:03:17,010
forth and yes there's a lot of

00:03:14,669 --> 00:03:20,189
programmable logic involved as well but

00:03:17,010 --> 00:03:22,859
with these devices it opens building

00:03:20,189 --> 00:03:25,859
very heterogeneous very focused very

00:03:22,859 --> 00:03:28,769
optimized systems and that creates

00:03:25,859 --> 00:03:31,349
additional challenges so what we like

00:03:28,769 --> 00:03:33,629
about this entire projects that we can

00:03:31,349 --> 00:03:35,549
build all flavors of accelerators and

00:03:33,629 --> 00:03:38,849
that's Myron I had to borrow one of your

00:03:35,549 --> 00:03:41,459
slides and what we basically see is

00:03:38,849 --> 00:03:44,699
without going into details it allows us

00:03:41,459 --> 00:03:46,979
to do ingress and egress and actually

00:03:44,699 --> 00:03:51,389
bi-directional acceleration by hooking

00:03:46,979 --> 00:03:54,780
up FPGAs and what we wonder and what we

00:03:51,389 --> 00:03:58,290
propose here is why not use a hundred

00:03:54,780 --> 00:04:01,199
gig or faster networking on this side of

00:03:58,290 --> 00:04:03,180
the equation and put the compute burden

00:04:01,199 --> 00:04:05,639
where it belongs mainly in a packet

00:04:03,180 --> 00:04:08,159
processing system also called an FPGA

00:04:05,639 --> 00:04:11,930
instead of run having software legacy

00:04:08,159 --> 00:04:11,930
software with a legacy stacks running it

00:04:13,729 --> 00:04:19,859
one of the reasons that developed over

00:04:17,539 --> 00:04:23,250
when we when we kind of sketched out

00:04:19,859 --> 00:04:25,740
this work was that power has not only

00:04:23,250 --> 00:04:27,990
has these CAPA connects but also these

00:04:25,740 --> 00:04:31,230
PCI Express

00:04:27,990 --> 00:04:34,740
gen4 links and today most of the

00:04:31,230 --> 00:04:37,800
hardware he can buy today is PCI Express

00:04:34,740 --> 00:04:40,460
based and what users are apparently

00:04:37,800 --> 00:04:44,280
looking for is free up those precious

00:04:40,460 --> 00:04:47,760
PCI Express gen fallings and move

00:04:44,280 --> 00:04:50,310
certain functionality over to those kind

00:04:47,760 --> 00:04:52,470
of rarely used a kapa links and what we

00:04:50,310 --> 00:04:54,660
like about those kapa links is they're

00:04:52,470 --> 00:04:56,520
much faster they're much faster than PCI

00:04:54,660 --> 00:04:58,380
Express and when we're talking about

00:04:56,520 --> 00:05:00,000
acceleration we not only need tight

00:04:58,380 --> 00:05:06,540
coupling but we need sheer bandwidth

00:05:00,000 --> 00:05:09,330
between the CPU and the FPGA now we all

00:05:06,540 --> 00:05:11,130
know FPGA programming is difficult we

00:05:09,330 --> 00:05:13,890
had a longer session yesterday about

00:05:11,130 --> 00:05:16,320
snap so this is more of a reminder we're

00:05:13,890 --> 00:05:18,390
totally sold on the concept of snap it

00:05:16,320 --> 00:05:21,090
was amazing how quickly we got going

00:05:18,390 --> 00:05:23,880
with snap and it was amazing how quickly

00:05:21,090 --> 00:05:26,520
got first results with snap so it was a

00:05:23,880 --> 00:05:30,450
wonderful investment and I'm we're more

00:05:26,520 --> 00:05:33,330
than happy to help turn this into a very

00:05:30,450 --> 00:05:36,210
well-known secret in this industry okay

00:05:33,330 --> 00:05:40,170
so what we're presenting today is work

00:05:36,210 --> 00:05:43,140
based on snap where we use the concept

00:05:40,170 --> 00:05:46,560
of an action for putting certain

00:05:43,140 --> 00:05:49,380
functionality inside an FPGA and well

00:05:46,560 --> 00:05:52,620
we're using the infrastructure from snap

00:05:49,380 --> 00:05:56,160
and copy to bolt on the network

00:05:52,620 --> 00:05:58,470
processing part within the FPGA to the

00:05:56,160 --> 00:06:04,710
network processing part which shall

00:05:58,470 --> 00:06:06,570
reside on the power 9 system so let's

00:06:04,710 --> 00:06:08,550
have a quick look at what network

00:06:06,570 --> 00:06:10,680
processing really means and add a borrow

00:06:08,550 --> 00:06:15,270
to slide from other people number one is

00:06:10,680 --> 00:06:17,310
network processing is about envelopes of

00:06:15,270 --> 00:06:19,560
envelopes of envelopes packaging

00:06:17,310 --> 00:06:21,390
packaging packaging stuff right and

00:06:19,560 --> 00:06:24,600
that's a lot of compute burden and

00:06:21,390 --> 00:06:27,060
that's why we have dedicated NIC network

00:06:24,600 --> 00:06:29,910
interface cards with special hardware

00:06:27,060 --> 00:06:31,830
for processing on the right hand side I

00:06:29,910 --> 00:06:34,770
just border a slide from Brendan Gregg

00:06:31,830 --> 00:06:36,960
who has these flame graphs it's just one

00:06:34,770 --> 00:06:39,120
of the applications which indicate it's

00:06:36,960 --> 00:06:41,480
more of a stereotype it indicate the

00:06:39,120 --> 00:06:44,630
amount of computing that is need

00:06:41,480 --> 00:06:46,640
in the Linux kernel today in order to do

00:06:44,630 --> 00:06:48,530
certain flavors of network processing

00:06:46,640 --> 00:06:51,500
namely in this case it's a tcp/ip

00:06:48,530 --> 00:06:54,890
processing session right and what our

00:06:51,500 --> 00:06:58,010
motivation is to offload the CPUs from

00:06:54,890 --> 00:07:00,800
this burden and free them up free the

00:06:58,010 --> 00:07:03,110
CPUs up to do more sensical analytics

00:07:00,800 --> 00:07:09,140
for example rather than just what we

00:07:03,110 --> 00:07:12,200
would call plumbing so this is how we

00:07:09,140 --> 00:07:15,230
see systems today high-speed networking

00:07:12,200 --> 00:07:18,200
is connected towards the processor chip

00:07:15,230 --> 00:07:20,420
we do scatter care the DMA typically

00:07:18,200 --> 00:07:24,260
through PC Xpress network interface card

00:07:20,420 --> 00:07:26,650
into the software on the CPU side and we

00:07:24,260 --> 00:07:30,230
believe this is not the right approach

00:07:26,650 --> 00:07:32,780
comes with our vision that this side of

00:07:30,230 --> 00:07:35,090
the equation on the right hand side we

00:07:32,780 --> 00:07:37,790
see this as an FPGA as an FPGA with

00:07:35,090 --> 00:07:39,770
accelerator and Xilinx has networking in

00:07:37,790 --> 00:07:42,560
their genes so what we're thinking of

00:07:39,770 --> 00:07:44,480
basically is putting networking on the

00:07:42,560 --> 00:07:47,240
right-hand side of such a heterogeneous

00:07:44,480 --> 00:07:49,960
system and let the FPGA deal with a

00:07:47,240 --> 00:07:52,460
packet and with ingress and egress

00:07:49,960 --> 00:07:54,620
pre-processing or post-processing and

00:07:52,460 --> 00:07:56,530
then handover the payload data to the

00:07:54,620 --> 00:07:59,720
CPU to the software side of the thing

00:07:56,530 --> 00:08:01,490
and what I'm going to talk through is

00:07:59,720 --> 00:08:03,710
some architecture ideas that we're

00:08:01,490 --> 00:08:06,400
having and quite honestly this work the

00:08:03,710 --> 00:08:09,950
next steps of this work will be heavily

00:08:06,400 --> 00:08:12,350
steered by the community by the needs of

00:08:09,950 --> 00:08:14,930
the applications because we're platform

00:08:12,350 --> 00:08:17,480
guys we need to listen to people who

00:08:14,930 --> 00:08:19,550
have applications in mind and need to

00:08:17,480 --> 00:08:22,460
make sure we cater to those application

00:08:19,550 --> 00:08:25,340
the technology underneath the under

00:08:22,460 --> 00:08:29,000
lying technology that we're utilizing is

00:08:25,340 --> 00:08:31,460
a tcp/ip accelerator stack that we

00:08:29,000 --> 00:08:35,180
license from Fraunhofer Freneau first

00:08:31,460 --> 00:08:40,370
started I guess in 2010 putting FPGA

00:08:35,180 --> 00:08:42,260
using FPGA s4 TCP IP UDP IP network

00:08:40,370 --> 00:08:44,600
processing at that point of time it was

00:08:42,260 --> 00:08:48,880
a one gig implementation we're now

00:08:44,600 --> 00:08:51,170
facing a 10 25 gig implementation and

00:08:48,880 --> 00:08:54,650
within the internal delivery

00:08:51,170 --> 00:08:55,430
presentation the technology that they

00:08:54,650 --> 00:08:57,350
basically

00:08:55,430 --> 00:09:00,250
together which is shipping today as

00:08:57,350 --> 00:09:02,870
utilizing 128-bit wide data paths

00:09:00,250 --> 00:09:06,230
bi-directional data paths so given that

00:09:02,870 --> 00:09:10,580
today's clock speeds of an FPGA in the

00:09:06,230 --> 00:09:14,240
300 ish megahertz range we can easily

00:09:10,580 --> 00:09:16,370
deal with 25 gig line rates but in order

00:09:14,240 --> 00:09:18,290
to meet the next generation of link

00:09:16,370 --> 00:09:21,830
rates of hundred gig or multiple hundred

00:09:18,290 --> 00:09:23,839
gig lanes we know we have to gear up and

00:09:21,830 --> 00:09:26,290
basically move to what we're now

00:09:23,839 --> 00:09:31,550
consider a 512 bit data bus

00:09:26,290 --> 00:09:33,920
implementation the system they put

00:09:31,550 --> 00:09:38,260
together is basically very modular

00:09:33,920 --> 00:09:41,660
system and if you look at the OSI stack

00:09:38,260 --> 00:09:43,790
from left to right you can see that they

00:09:41,660 --> 00:09:46,730
have the lowest level processing which

00:09:43,790 --> 00:09:49,640
is in the physical defy layer processing

00:09:46,730 --> 00:09:52,550
for which we're using designing pcs PMA

00:09:49,640 --> 00:09:55,850
layer and then we have a media access

00:09:52,550 --> 00:09:58,310
controller a MEK block and then there is

00:09:55,850 --> 00:10:00,680
an Ethernet block there's an IP block

00:09:58,310 --> 00:10:04,580
and these things are connected through

00:10:00,680 --> 00:10:07,370
those 128-bit wide data buses streaming

00:10:04,580 --> 00:10:10,220
data buses or soon 512-bit wide data

00:10:07,370 --> 00:10:12,080
buses and at the end of the day you have

00:10:10,220 --> 00:10:15,230
your payload data and that's where the

00:10:12,080 --> 00:10:18,910
user logic sits and that's where the

00:10:15,230 --> 00:10:22,310
user logic deals with a payload data now

00:10:18,910 --> 00:10:25,610
when we started taking this Fraunhofer

00:10:22,310 --> 00:10:28,520
technology under our wing we thought of

00:10:25,610 --> 00:10:32,330
this for embedded devices which have an

00:10:28,520 --> 00:10:34,310
ACCI connect to embedded arm CPUs and

00:10:32,330 --> 00:10:36,320
that's why we basically have this

00:10:34,310 --> 00:10:39,950
hardware layer which is this stack

00:10:36,320 --> 00:10:42,170
control and an optional DMA and when we

00:10:39,950 --> 00:10:44,300
started to look at what capi brings to

00:10:42,170 --> 00:10:46,850
the party and in particular what snap

00:10:44,300 --> 00:10:49,130
brings to the party we see actually way

00:10:46,850 --> 00:10:51,350
more potential in using this technology

00:10:49,130 --> 00:10:56,480
for integrating with software systems

00:10:51,350 --> 00:10:58,520
than just for embedded systems so this

00:10:56,480 --> 00:11:01,279
is this is kind of the architecture that

00:10:58,520 --> 00:11:04,190
we anticipate where we have a software

00:11:01,279 --> 00:11:07,940
layer running Linux on open power in

00:11:04,190 --> 00:11:08,750
between the open copy layer part of it

00:11:07,940 --> 00:11:12,920
stretch

00:11:08,750 --> 00:11:15,800
into the FPGA side with a PSLV IBM power

00:11:12,920 --> 00:11:19,580
service layer and within the FPGA then

00:11:15,800 --> 00:11:22,400
we have this Fraunhofer stack processing

00:11:19,580 --> 00:11:24,800
the data from left to right for ingress

00:11:22,400 --> 00:11:28,340
and right to left back for egress

00:11:24,800 --> 00:11:31,130
traffic and eventually we have the user

00:11:28,340 --> 00:11:34,310
logic which talks to the counterpart

00:11:31,130 --> 00:11:36,620
through capi touch to the counterpart in

00:11:34,310 --> 00:11:38,920
in the software land and for testing

00:11:36,620 --> 00:11:41,060
purposes we use a hardware

00:11:38,920 --> 00:11:43,610
implementation of a well known

00:11:41,060 --> 00:11:47,210
networking tool which is net worth net

00:11:43,610 --> 00:11:49,880
surf the frame of a guy's implemented in

00:11:47,210 --> 00:11:52,700
programmable logic most of the

00:11:49,880 --> 00:11:55,370
functionality of net surf net perv so we

00:11:52,700 --> 00:11:58,850
can actually run the entire networking

00:11:55,370 --> 00:12:01,660
sessions for TCP and or UDP within the

00:11:58,850 --> 00:12:05,180
program of logic and just have raw data

00:12:01,660 --> 00:12:08,030
statistics fed to the software so we can

00:12:05,180 --> 00:12:10,310
do some testing on how fast we can go

00:12:08,030 --> 00:12:16,730
and how close to theoretical limits we

00:12:10,310 --> 00:12:20,330
can drive this technology so one of the

00:12:16,730 --> 00:12:25,370
many options which we envision is that

00:12:20,330 --> 00:12:28,520
we cut up this stack as part of in

00:12:25,370 --> 00:12:31,700
between the Mac and the Ethernet and

00:12:28,520 --> 00:12:35,060
then basically turn some portions become

00:12:31,700 --> 00:12:38,150
part of what we call the snap shell in

00:12:35,060 --> 00:12:40,070
some flavor open sourced and the rest of

00:12:38,150 --> 00:12:43,250
it can become a user or a snap action

00:12:40,070 --> 00:12:45,260
right that that's one of the many many

00:12:43,250 --> 00:12:49,100
ideas we're having you can go further

00:12:45,260 --> 00:12:53,180
and do the cut someplace else at the IP

00:12:49,100 --> 00:12:55,130
layer CF ICMP ARP and all this kind of

00:12:53,180 --> 00:12:58,820
management layers of networking also

00:12:55,130 --> 00:13:01,970
done in hardware and then basically do

00:12:58,820 --> 00:13:04,640
the TCP in the user logic okay it's

00:13:01,970 --> 00:13:06,950
basically the same form of acceleration

00:13:04,640 --> 00:13:08,720
what is different here is the licensing

00:13:06,950 --> 00:13:10,790
schemes which obviously we have to work

00:13:08,720 --> 00:13:13,660
out with the snap people and the

00:13:10,790 --> 00:13:13,660
Fraunhofer people

00:13:14,450 --> 00:13:20,149
and obviously you can go all the way and

00:13:17,269 --> 00:13:24,709
turn this entire full accelerator for

00:13:20,149 --> 00:13:28,190
networking traffic into a snap shell so

00:13:24,709 --> 00:13:31,070
users just have can bolt on to a

00:13:28,190 --> 00:13:33,829
streaming AXI bus they use a logic in

00:13:31,070 --> 00:13:39,470
form of a snap action like in this

00:13:33,829 --> 00:13:41,209
particular example net perf net surf so

00:13:39,470 --> 00:13:42,980
that's what we've implemented today and

00:13:41,209 --> 00:13:46,430
that's what we're running on one of the

00:13:42,980 --> 00:13:48,589
Alpha data cards for testing purposes so

00:13:46,430 --> 00:13:51,829
we did the logical cut which is free of

00:13:48,589 --> 00:13:54,829
any any difficult licensing handling

00:13:51,829 --> 00:13:56,870
with Fraunhofer because it just relies

00:13:54,829 --> 00:13:58,730
on the Sonics pcs PMA layer

00:13:56,870 --> 00:14:00,980
but at the same time it gives you the

00:13:58,730 --> 00:14:03,980
full user experience of what happens

00:14:00,980 --> 00:14:06,649
when you offload the entire network

00:14:03,980 --> 00:14:09,050
traffic and run it within the FPGA

00:14:06,649 --> 00:14:11,570
fabric and again everything is

00:14:09,050 --> 00:14:15,139
implemented as a snap action including

00:14:11,570 --> 00:14:19,550
the net perf and so you can hook it up

00:14:15,139 --> 00:14:22,640
connect it and drive it and I guess it

00:14:19,550 --> 00:14:24,260
runs 25 gig if your cards would run 25

00:14:22,640 --> 00:14:26,510
gigs or if we would finally port it to

00:14:24,260 --> 00:14:28,730
one of your cards which run for 25 gig

00:14:26,510 --> 00:14:31,910
right but that's that's basically the

00:14:28,730 --> 00:14:34,040
current implementation so here's some

00:14:31,910 --> 00:14:36,019
results of what we implemented based on

00:14:34,040 --> 00:14:39,019
based on snap so the first is it's

00:14:36,019 --> 00:14:42,709
basically the Linux command prompt from

00:14:39,019 --> 00:14:45,620
a terminal and the first step we do is

00:14:42,709 --> 00:14:48,199
basically start the snip maintenance

00:14:45,620 --> 00:14:51,350
tools and what you can see here is that

00:14:48,199 --> 00:14:54,920
you see the alpha data cart with nvme

00:14:51,350 --> 00:14:58,190
enabled and the ethernet 10 gig PCI pcs

00:14:54,920 --> 00:15:00,440
p.m. a layer enabled alright and what

00:14:58,190 --> 00:15:03,260
you also see is we we started to work

00:15:00,440 --> 00:15:06,170
with the snap people in burbling in and

00:15:03,260 --> 00:15:09,170
basically took our honestly PCI Express

00:15:06,170 --> 00:15:12,019
ID if we remember of PCI PCI C so we

00:15:09,170 --> 00:15:15,970
took our PCI Express idea ID and and

00:15:12,019 --> 00:15:17,930
turned into a snap ID and basically our

00:15:15,970 --> 00:15:21,260
registering here an ml e

00:15:17,930 --> 00:15:25,550
HDL 10 gig Ethernet TCP UDP IP

00:15:21,260 --> 00:15:27,890
accelerator what we're doing honestly

00:15:25,550 --> 00:15:29,870
here is we're not using this

00:15:27,890 --> 00:15:32,480
to the fullest extent of high-level

00:15:29,870 --> 00:15:35,540
synthesis we didn't implement major

00:15:32,480 --> 00:15:38,560
portions in C++ we're using the very log

00:15:35,540 --> 00:15:43,700
V HDL HDL design flow for snip for

00:15:38,560 --> 00:15:46,880
integrating this IP then last that's

00:15:43,700 --> 00:15:50,180
click you can basically start the snap

00:15:46,880 --> 00:15:52,610
action and what you see here is you end

00:15:50,180 --> 00:15:55,459
up having another network interface

00:15:52,610 --> 00:15:57,230
device under Linux if config you see and

00:15:55,459 --> 00:16:00,320
Pepsi row as the network interface

00:15:57,230 --> 00:16:02,269
device and in this case through the

00:16:00,320 --> 00:16:05,209
maintenance layer we assign an IP

00:16:02,269 --> 00:16:08,720
address to it so you can ping and pong

00:16:05,209 --> 00:16:13,269
and drive it from any other 10 key

00:16:08,720 --> 00:16:16,279
connected server some first test results

00:16:13,269 --> 00:16:18,670
basically showing how fast network

00:16:16,279 --> 00:16:23,060
processing can be i didn't bother

00:16:18,670 --> 00:16:25,160
showing the top for software performance

00:16:23,060 --> 00:16:28,190
here because no software really is

00:16:25,160 --> 00:16:33,170
involved on the system so what you see

00:16:28,190 --> 00:16:38,720
here is that line rates for TCP stream

00:16:33,170 --> 00:16:40,880
processing is about nine point four gigs

00:16:38,720 --> 00:16:44,000
which is close to the theoretical limit

00:16:40,880 --> 00:16:47,750
okay that's kind of expected good news

00:16:44,000 --> 00:16:50,180
is we expect the same high rates for 25

00:16:47,750 --> 00:16:53,720
gig close to the theoretical limit of 25

00:16:50,180 --> 00:16:55,940
gig and we believe that we can drive

00:16:53,720 --> 00:16:57,050
this up and scale to 100 gigs or

00:16:55,940 --> 00:17:00,890
multiple hundred gigs

00:16:57,050 --> 00:17:03,740
knowing how FPGA is process the bottom

00:17:00,890 --> 00:17:06,500
part of this slide we share some

00:17:03,740 --> 00:17:10,850
round-trip data on the latency again

00:17:06,500 --> 00:17:13,459
this is FPGA processing involved it's

00:17:10,850 --> 00:17:18,819
very predictable latency in here

00:17:13,459 --> 00:17:18,819
compared to running TCP stack in Linux

00:17:18,970 --> 00:17:24,530
alright and so if you're interested

00:17:21,589 --> 00:17:26,750
about how this work is going on where

00:17:24,530 --> 00:17:30,110
software guys so we heavily depend on

00:17:26,750 --> 00:17:32,000
hardware being available to us and in

00:17:30,110 --> 00:17:34,549
driving this current 10 gig

00:17:32,000 --> 00:17:37,580
implementation we're working with alpha

00:17:34,549 --> 00:17:39,800
data here to drive this with other alpha

00:17:37,580 --> 00:17:42,940
data cards towards 25k again towards

00:17:39,800 --> 00:17:42,940
multiple hundred gigs

00:17:43,370 --> 00:17:51,059
okay thank syndrich I'll just talk for a

00:17:48,299 --> 00:17:54,210
couple of minutes about the carts these

00:17:51,059 --> 00:17:58,640
two cars here are the ones that machine

00:17:54,210 --> 00:18:02,970
a missing link have been targeting a

00:17:58,640 --> 00:18:04,710
their free market a mostly on that the

00:18:02,970 --> 00:18:07,049
key or sixty is they've been able to get

00:18:04,710 --> 00:18:10,049
this and into the room I believe they've

00:18:07,049 --> 00:18:11,450
also drifted out a port for the the q11

00:18:10,049 --> 00:18:14,120
5 FPGA

00:18:11,450 --> 00:18:16,740
[Music]

00:18:14,120 --> 00:18:19,200
we were also able to take the bitstreams

00:18:16,740 --> 00:18:23,790
it generated and tested in facilities

00:18:19,200 --> 00:18:26,940
and I kept the keyboard these these

00:18:23,790 --> 00:18:31,799
boards are both a captive endorsed snap

00:18:26,940 --> 00:18:34,049
card say 4 / 8 systems so the framework

00:18:31,799 --> 00:18:36,090
can be used I guess widely and those

00:18:34,049 --> 00:18:38,130
systems are available out there but for

00:18:36,090 --> 00:18:43,110
the future developments we want to move

00:18:38,130 --> 00:18:46,370
beyond Kathy 1 we want to move beyond 10

00:18:43,110 --> 00:18:50,490
gigabit ethernet and so next late here

00:18:46,370 --> 00:18:53,910
she was possibly the cat is beginning to

00:18:50,490 --> 00:18:57,870
get it deployed a number of tests at

00:18:53,910 --> 00:19:01,040
servers hopefully across a few sites

00:18:57,870 --> 00:19:06,210
that may be available to voice of

00:19:01,040 --> 00:19:09,120
Hendrick and others and this card here

00:19:06,210 --> 00:19:12,840
is the ADM PCIe 9 v3 it's based on the

00:19:09,120 --> 00:19:17,840
button so it takes ultra skill plus vtp

00:19:12,840 --> 00:19:19,980
device this has got their 28 or 32 cake

00:19:17,840 --> 00:19:25,740
capable transceivers and therefore is

00:19:19,980 --> 00:19:30,510
capable of running either 25 gigabit per

00:19:25,740 --> 00:19:34,290
second on each single link coming out of

00:19:30,510 --> 00:19:38,730
the QSF piece or as a hundred kings

00:19:34,290 --> 00:19:42,000
tickling using all four wins in each

00:19:38,730 --> 00:19:46,400
case FP as coward also has the advantage

00:19:42,000 --> 00:19:50,040
that it is not only PCIe Jen for capable

00:19:46,400 --> 00:19:51,540
to an extent too much as much an extent

00:19:50,040 --> 00:19:54,180
as they thought take social skill plus

00:19:51,540 --> 00:19:59,720
is Jennifer cable I believe it does work

00:19:54,180 --> 00:20:02,610
with IBM power4 named PCIe gen for a

00:19:59,720 --> 00:20:04,380
it's also Kathy Kathy Tudor or cable and

00:20:02,610 --> 00:20:05,070
it's also got their twenty five gigabit

00:20:04,380 --> 00:20:07,560
per second

00:20:05,070 --> 00:20:09,660
open Kathy link as well and so therefore

00:20:07,560 --> 00:20:13,050
this is a definitely the the next

00:20:09,660 --> 00:20:15,870
platform we've talked to get this IP

00:20:13,050 --> 00:20:19,470
ported to as it certainly although a

00:20:15,870 --> 00:20:22,260
smooth transition up from snap wonder or

00:20:19,470 --> 00:20:24,570
I'm taking a second to snap to snap

00:20:22,260 --> 00:20:27,570
Kathy snap Kathy Tudor or snap and open

00:20:24,570 --> 00:20:29,940
Kathy snap and twenty five and hopefully

00:20:27,570 --> 00:20:34,980
in the future one hundred as well get

00:20:29,940 --> 00:20:38,220
your second links beyond this as to

00:20:34,980 --> 00:20:43,440
other boards at this IP Cuba target I'd

00:20:38,220 --> 00:20:47,370
say the EDM PC he named each seven but

00:20:43,440 --> 00:20:51,690
we have after birthday which is a fairly

00:20:47,370 --> 00:20:52,940
heavy weight the you thirty-seven phbm

00:20:51,690 --> 00:20:55,770
fpg

00:20:52,940 --> 00:20:57,870
this doesn't fit into too many parenting

00:20:55,770 --> 00:21:02,400
systems but we're certainly keen on

00:20:57,870 --> 00:21:07,820
seeing them the mihawk systems from West

00:21:02,400 --> 00:21:11,910
Tron mistress port is actually multiple

00:21:07,820 --> 00:21:14,580
open Kathy links a and it's possibly

00:21:11,910 --> 00:21:19,080
capable of forty or possibly in 1200

00:21:14,580 --> 00:21:21,020
gigabit per second links and the HBM may

00:21:19,080 --> 00:21:23,790
give a a good advantage in terms of they

00:21:21,020 --> 00:21:25,890
having a sort of buffer for retry data

00:21:23,790 --> 00:21:30,890
if not necessarily for any little agency

00:21:25,890 --> 00:21:35,370
data requirements of that of the

00:21:30,890 --> 00:21:36,720
Ethernet a stack another board that may

00:21:35,370 --> 00:21:39,000
be of interest in the futures as a

00:21:36,720 --> 00:21:42,900
little profile card this very similar

00:21:39,000 --> 00:21:46,320
but based on the via 33 phbm LPG and

00:21:42,900 --> 00:21:48,930
this has double density SFP so it's

00:21:46,320 --> 00:21:51,930
capable of either taking a standard QFP

00:21:48,930 --> 00:21:54,570
hundred gig link or double density one

00:21:51,930 --> 00:21:56,820
with two hundred qing-ge links on it and

00:21:54,570 --> 00:21:57,420
this has got often calculate coming up

00:21:56,820 --> 00:21:59,970
there

00:21:57,420 --> 00:22:07,220
your connector on this should physically

00:21:59,970 --> 00:22:07,220
fit into a lot more there remain servers

00:22:09,950 --> 00:22:18,140
so just hand back to end Richter to

00:22:14,180 --> 00:22:21,600
perform the call for participation and

00:22:18,140 --> 00:22:24,390
yes just just just to conclude Cappy

00:22:21,600 --> 00:22:27,090
snap very easy to use allowed us to

00:22:24,390 --> 00:22:30,240
integrate a network protocol processing

00:22:27,090 --> 00:22:33,120
within a power system within a few weeks

00:22:30,240 --> 00:22:35,190
honestly most of it was getting access

00:22:33,120 --> 00:22:38,520
to the hardware and learning about the

00:22:35,190 --> 00:22:40,260
details of the hardware what we're now

00:22:38,520 --> 00:22:42,900
looking for is participation from the

00:22:40,260 --> 00:22:44,970
ecosystem people who are interested in

00:22:42,900 --> 00:22:48,140
driving network protocol acceleration

00:22:44,970 --> 00:22:52,530
into open power and into open copy and

00:22:48,140 --> 00:22:54,480
who are providing us with use cases and

00:22:52,530 --> 00:22:56,310
applications so we can streamline this

00:22:54,480 --> 00:22:59,240
acceleration technology towards their

00:22:56,310 --> 00:22:59,240
needs thank you

00:22:59,680 --> 00:23:03,119

YouTube URL: https://www.youtube.com/watch?v=lJ_qZqzyY10


