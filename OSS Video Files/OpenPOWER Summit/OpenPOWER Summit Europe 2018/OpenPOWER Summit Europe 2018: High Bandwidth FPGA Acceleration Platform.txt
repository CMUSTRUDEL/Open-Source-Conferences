Title: OpenPOWER Summit Europe 2018: High Bandwidth FPGA Acceleration Platform
Publication date: 2019-02-07
Playlist: OpenPOWER Summit Europe 2018
Description: 
	High Bandwidth FPGA Acceleration is discussed at OpenPOWER Summit Europe 2018.

For more information, please visit: https://openpowerfoundation.org/summit-2018-10-eu/
Captions: 
	00:00:00,420 --> 00:00:06,959
yeah so I work in the IP solutions

00:00:03,810 --> 00:00:09,420
division Xilinx and I'm working on an

00:00:06,959 --> 00:00:11,910
FPGA acceleration platform a high

00:00:09,420 --> 00:00:13,350
bandwidth platform and so I'm just gonna

00:00:11,910 --> 00:00:16,170
going to give an overview of the

00:00:13,350 --> 00:00:18,390
platform I talked a little bit about the

00:00:16,170 --> 00:00:21,240
FPGA at the heart of the platform some

00:00:18,390 --> 00:00:24,480
detail on life talk about the build

00:00:21,240 --> 00:00:27,329
process and the user interface some

00:00:24,480 --> 00:00:28,980
quality of results and look at memory

00:00:27,329 --> 00:00:30,689
access performance that's I suppose

00:00:28,980 --> 00:00:32,850
that's the key metric for this platform

00:00:30,689 --> 00:00:34,230
and give a summary and talk about

00:00:32,850 --> 00:00:41,129
strategies to get the best performance

00:00:34,230 --> 00:00:43,620
from the platform so yesterday there was

00:00:41,129 --> 00:00:47,250
a family of accelerators announced the

00:00:43,620 --> 00:00:49,579
alveolar a turquoise i links they're

00:00:47,250 --> 00:00:53,460
based on vertex ultra scale plus device

00:00:49,579 --> 00:00:55,829
so this this platform the lvo you 280 is

00:00:53,460 --> 00:01:00,179
is a kind of a sister carat rose to

00:00:55,829 --> 00:01:02,219
those accelerators it's based on an

00:01:00,179 --> 00:01:06,329
extended member of that family the ultra

00:01:02,219 --> 00:01:08,880
scale plus HBM family so it's a vu 37 P

00:01:06,329 --> 00:01:11,430
it's the it's the device that's under

00:01:08,880 --> 00:01:13,680
carat so that's a v u9 p equivalent for

00:01:11,430 --> 00:01:16,770
anyone anyone familiar with ultra scale

00:01:13,680 --> 00:01:19,799
plus so you know it has a huge amount of

00:01:16,770 --> 00:01:22,140
processing so 1.3 make lots 9k DSPs

00:01:19,799 --> 00:01:24,479
large amount of embedded SRAM's

00:01:22,140 --> 00:01:27,479
with the addition of 8 gig of high

00:01:24,479 --> 00:01:29,969
bandwidth memory so as well as that it

00:01:27,479 --> 00:01:33,750
has dear I'm on board if you need depth

00:01:29,969 --> 00:01:37,950
so there's 32 gigs or two channels a

00:01:33,750 --> 00:01:40,979
host interface gen 3 by 16 Lane Gen 4 by

00:01:37,950 --> 00:01:42,149
2 it's kind of a standard standard form

00:01:40,979 --> 00:01:46,500
factor for these carrots

00:01:42,149 --> 00:01:50,219
it's available in Vivaro SDX in 2018 dot

00:01:46,500 --> 00:01:52,350
3 boards aren't available for general

00:01:50,219 --> 00:01:54,960
use at that time so their production

00:01:52,350 --> 00:01:59,609
release will most likely be 2019 that

00:01:54,960 --> 00:02:01,380
one time frame so in terms of what it

00:01:59,609 --> 00:02:04,439
what's in the fpga what's what's the

00:02:01,380 --> 00:02:06,840
fpga floorplan so the shell is what we

00:02:04,439 --> 00:02:09,569
term entirely a tall and skinny shell so

00:02:06,840 --> 00:02:12,730
it's a it's a narrow strip down the side

00:02:09,569 --> 00:02:15,100
of the tree SL arse of the device

00:02:12,730 --> 00:02:17,200
the advantage in doing that is that

00:02:15,100 --> 00:02:19,180
you're leaving you know a large amount

00:02:17,200 --> 00:02:21,760
of resources free in each SLR you're

00:02:19,180 --> 00:02:24,700
getting the best use of that advice and

00:02:21,760 --> 00:02:26,709
also you're not blocking use of DSPs and

00:02:24,700 --> 00:02:29,380
block ramp so by doing this you're

00:02:26,709 --> 00:02:34,750
actually only using lots and smaller

00:02:29,380 --> 00:02:38,080
amounts of block ramp so that the HBM so

00:02:34,750 --> 00:02:41,560
it takes 12 about 12.5% of the device

00:02:38,080 --> 00:02:44,230
area so the HBM are at the bottom at the

00:02:41,560 --> 00:02:48,850
bottom of SLR zero and and the access

00:02:44,230 --> 00:02:50,590
points to HB m and the SLR boundaries so

00:02:48,850 --> 00:02:52,540
it's important we want to be able to

00:02:50,590 --> 00:02:55,030
talk to hvm from every part of the

00:02:52,540 --> 00:02:57,760
device so you need a large amount of

00:02:55,030 --> 00:02:59,290
routing to do that so at the boundaries

00:02:57,760 --> 00:03:02,560
it's important that you have that so

00:02:59,290 --> 00:03:05,260
there are 20k signals 28 nets available

00:03:02,560 --> 00:03:07,510
between each SLR to route from a colonel

00:03:05,260 --> 00:03:11,380
let's say in the top s lower down to HB

00:03:07,510 --> 00:03:13,540
m and there are DDR memory controllers

00:03:11,380 --> 00:03:16,239
if necessary these will be optimized out

00:03:13,540 --> 00:03:20,860
if you don't use them and for successful

00:03:16,239 --> 00:03:23,170
SRAM so a little bit of detail about the

00:03:20,860 --> 00:03:24,489
device so this is a kind of a marketing

00:03:23,170 --> 00:03:28,110
slide but the main thing to take from

00:03:24,489 --> 00:03:30,100
this is that the DRM stacks are

00:03:28,110 --> 00:03:32,650
integrated using chip on wafer on

00:03:30,100 --> 00:03:35,860
substrate and there's a dedicated harden

00:03:32,650 --> 00:03:40,480
interface to the HBM to maximize

00:03:35,860 --> 00:03:43,290
bandwidth and flexibility so if we just

00:03:40,480 --> 00:03:45,549
talk about that that hardened interface

00:03:43,290 --> 00:03:48,880
so that the key thing is that it this

00:03:45,549 --> 00:03:51,700
isn't just memory so there's a hardened

00:03:48,880 --> 00:03:54,760
fight to talk to to the HBM stacks

00:03:51,700 --> 00:03:57,610
there are harden memory controllers and

00:03:54,760 --> 00:04:01,269
the harden switch network so the

00:03:57,610 --> 00:04:05,640
interface in the fabric is r32 axy

00:04:01,269 --> 00:04:09,459
masters connecting to the switch network

00:04:05,640 --> 00:04:11,680
so basically the the subsystem comes up

00:04:09,459 --> 00:04:13,720
by itself it's self initializing so a

00:04:11,680 --> 00:04:18,459
user just needs to connect to those to

00:04:13,720 --> 00:04:20,530
those acts a masters so the bandwidth

00:04:18,459 --> 00:04:23,140
and H of those masters is 14 point 4

00:04:20,530 --> 00:04:25,950
gigabytes per second there are 64

00:04:23,140 --> 00:04:29,250
outstanding transactions on each master

00:04:25,950 --> 00:04:30,870
that's important because you know more

00:04:29,250 --> 00:04:32,850
outstanding transactions you get better

00:04:30,870 --> 00:04:35,040
performance if there's latency in the

00:04:32,850 --> 00:04:37,680
switch memory controllers need more

00:04:35,040 --> 00:04:40,530
outstanding transactions to reorder for

00:04:37,680 --> 00:04:44,220
random access so the more outstanding

00:04:40,530 --> 00:04:46,320
transactions the better and this is a

00:04:44,220 --> 00:04:48,720
there's a unified memory map so each

00:04:46,320 --> 00:04:57,660
master has access to every every memory

00:04:48,720 --> 00:05:00,180
in the stack so physically mmm there are

00:04:57,660 --> 00:05:03,000
two hvm stacks there four gigs each with

00:05:00,180 --> 00:05:05,940
230 gigabytes per second of overall

00:05:03,000 --> 00:05:08,430
bandwidth there are eight hardened more

00:05:05,940 --> 00:05:10,950
memory controllers per stack each with

00:05:08,430 --> 00:05:12,570
64 deep reordering queues and that that

00:05:10,950 --> 00:05:17,910
that matches with the outstanding

00:05:12,570 --> 00:05:20,730
transactions each HBM stack has 16

00:05:17,910 --> 00:05:24,690
parallel pseudo channels so a pseudo

00:05:20,730 --> 00:05:27,510
channel is 256 megabytes there's two per

00:05:24,690 --> 00:05:28,890
memory controller and they each each

00:05:27,510 --> 00:05:31,410
pseudo channel has a bandwidth of

00:05:28,890 --> 00:05:35,630
fourteen point four gigabytes per second

00:05:31,410 --> 00:05:39,000
so that's the theoretical bandwidth and

00:05:35,630 --> 00:05:40,890
just the note uh none transactions or 32

00:05:39,000 --> 00:05:44,040
bytes it's the smallest transaction size

00:05:40,890 --> 00:05:48,750
possible and on a on a hvm PC that's the

00:05:44,040 --> 00:05:50,550
that's their jeddaks standard so some

00:05:48,750 --> 00:05:54,510
some notes on the switch so the first

00:05:50,550 --> 00:05:57,210
note is is a common technique used in

00:05:54,510 --> 00:06:01,260
switch design it's SSID blocking so this

00:05:57,210 --> 00:06:04,080
is a single slave per ID so what this

00:06:01,260 --> 00:06:06,210
means is that a master using a single ID

00:06:04,080 --> 00:06:09,600
can only have outstanding transactions

00:06:06,210 --> 00:06:12,960
at a single slave okay so for for random

00:06:09,600 --> 00:06:14,100
access cases that gives very negative

00:06:12,960 --> 00:06:16,020
performance so if you're switching

00:06:14,100 --> 00:06:17,820
between multiple pseudo channels and

00:06:16,020 --> 00:06:18,840
you're accessing multiple pseudo

00:06:17,820 --> 00:06:22,170
channels from a master

00:06:18,840 --> 00:06:24,420
that would give very poor performance so

00:06:22,170 --> 00:06:26,100
that that is the way the hardened switch

00:06:24,420 --> 00:06:28,890
is designed and in fact the way most

00:06:26,100 --> 00:06:30,930
access switches are designed so as the

00:06:28,890 --> 00:06:34,380
solution for that is to include a soft

00:06:30,930 --> 00:06:37,920
IP so there's a small soft IP a random

00:06:34,380 --> 00:06:39,660
access master attachment and this IP

00:06:37,920 --> 00:06:41,640
basically creates

00:06:39,660 --> 00:06:44,460
a multi-threaded stream from single

00:06:41,640 --> 00:06:47,430
threaded master so it breaks ax e

00:06:44,460 --> 00:06:49,710
reordering rules actually ordering rules

00:06:47,430 --> 00:06:53,250
and then reorders in order to get the

00:06:49,710 --> 00:06:55,290
best performance so this is a standalone

00:06:53,250 --> 00:06:56,970
IP it's available for you know

00:06:55,290 --> 00:07:00,570
standalone use with this with this

00:06:56,970 --> 00:07:03,480
platform with this device with a in

00:07:00,570 --> 00:07:06,120
acceleration platforms the compiler XO

00:07:03,480 --> 00:07:11,280
cc or SD X needs to be directed to

00:07:06,120 --> 00:07:14,160
include Rama so a second note on the

00:07:11,280 --> 00:07:17,870
switch the switch isn't a full 32

00:07:14,160 --> 00:07:21,090
Waycross points which it's actually

00:07:17,870 --> 00:07:24,090
composed of eight four-way cross point

00:07:21,090 --> 00:07:26,430
switches and between those there are

00:07:24,090 --> 00:07:28,980
lateral links so lateral links don't

00:07:26,430 --> 00:07:31,380
have as much bandwidth as they as the

00:07:28,980 --> 00:07:34,170
internal four-way cross phone switches

00:07:31,380 --> 00:07:38,910
so there are some lateral bandwidth

00:07:34,170 --> 00:07:40,680
limitations so if I take masters 4 to 7

00:07:38,910 --> 00:07:42,240
on the switch if they're talking to sudo

00:07:40,680 --> 00:07:44,520
challenge 4 to 7 you get perfect

00:07:42,240 --> 00:07:45,660
switching if I take masters there's

00:07:44,520 --> 00:07:47,490
three and they're talking to a pseudo

00:07:45,660 --> 00:07:50,550
channel 0 to 3 you get perfect switching

00:07:47,490 --> 00:07:52,950
if those sets of if those eight masters

00:07:50,550 --> 00:07:55,470
are talking to pseudo channel 21 to 23

00:07:52,950 --> 00:07:58,710
you will get you won't get the same

00:07:55,470 --> 00:08:00,120
performance as you would locally ok so

00:07:58,710 --> 00:08:03,120
the solution to that is that the

00:08:00,120 --> 00:08:04,980
platform user provides as part of the

00:08:03,120 --> 00:08:07,560
build process the platform user provides

00:08:04,980 --> 00:08:10,620
the memories that it requires or the sub

00:08:07,560 --> 00:08:13,050
set of PCs that it needs to access so

00:08:10,620 --> 00:08:15,300
the platform can then choose the switch

00:08:13,050 --> 00:08:18,270
port to minimize the use of these

00:08:15,300 --> 00:08:20,220
lateral links and as a result it

00:08:18,270 --> 00:08:25,700
minimizes the congestion in in the

00:08:20,220 --> 00:08:28,740
switch so a final note under switch so

00:08:25,700 --> 00:08:29,850
there are two hvm stacks and we've

00:08:28,740 --> 00:08:31,260
talked about swear I talked about

00:08:29,850 --> 00:08:33,870
switches one large switch but it's

00:08:31,260 --> 00:08:37,140
actually to switch components and the

00:08:33,870 --> 00:08:39,810
the there is a limited right transaction

00:08:37,140 --> 00:08:41,910
rate through the crossing between the

00:08:39,810 --> 00:08:45,180
two switch components so this is only

00:08:41,910 --> 00:08:49,200
for right transactions for 32 byte and

00:08:45,180 --> 00:08:51,690
64 byte transaction sizes so as to

00:08:49,200 --> 00:08:53,190
there's no solution to this so when

00:08:51,690 --> 00:08:54,930
coming up with a memory

00:08:53,190 --> 00:08:56,820
topology if you're using smaller

00:08:54,930 --> 00:09:03,030
transactions this need needs to be

00:08:56,820 --> 00:09:04,830
handled in any user application so in in

00:09:03,030 --> 00:09:06,840
terms of the build process so just

00:09:04,830 --> 00:09:09,600
looking at what happens under a hood

00:09:06,840 --> 00:09:12,660
when you're connecting a kernel master

00:09:09,600 --> 00:09:16,590
so each master requires AXI with and

00:09:12,660 --> 00:09:18,510
clock conversion a Rama IP for random

00:09:16,590 --> 00:09:20,550
access to improve random access

00:09:18,510 --> 00:09:24,690
performance if the master has random

00:09:20,550 --> 00:09:26,010
access ax Evo's pipelining so you want

00:09:24,690 --> 00:09:28,200
to be able to connect a kernel from

00:09:26,010 --> 00:09:29,850
anywhere in the device to HBM sort of

00:09:28,200 --> 00:09:33,900
there needs to be some kind of autumn

00:09:29,850 --> 00:09:35,490
automated pipelining and an optimal

00:09:33,900 --> 00:09:43,380
switchboard connection to minimize

00:09:35,490 --> 00:09:46,710
congestion so the acceleration tool SDX

00:09:43,380 --> 00:09:50,640
actually runs over Vivaro and this this

00:09:46,710 --> 00:09:53,730
Vivaro uses this HBM memory subsystem IP

00:09:50,640 --> 00:09:55,650
so it's a soft IP that that performs all

00:09:53,730 --> 00:09:58,860
of these functions so it wraps the

00:09:55,650 --> 00:10:02,910
necessary soft IP it exposes an api for

00:09:58,860 --> 00:10:04,770
the STX compiler and linker so the

00:10:02,910 --> 00:10:08,550
compiler and linker requests memory

00:10:04,770 --> 00:10:10,860
resources the soft IP then chooses the

00:10:08,550 --> 00:10:14,610
optimal port connection disables any

00:10:10,860 --> 00:10:23,120
unused HBM memories and and ports to

00:10:14,610 --> 00:10:26,010
save power so and above that X OCC

00:10:23,120 --> 00:10:28,050
accepts the user inputs or accepts the

00:10:26,010 --> 00:10:30,600
memory resources required by each master

00:10:28,050 --> 00:10:32,760
it stitches the kernels into the dynamic

00:10:30,600 --> 00:10:37,680
region and attaches the memory resources

00:10:32,760 --> 00:10:39,450
by this particular API so just to give

00:10:37,680 --> 00:10:42,210
it a quick example if you take a kernel

00:10:39,450 --> 00:10:46,560
in SL ours we want to curl in SL or 0 we

00:10:42,210 --> 00:10:48,630
want 16 AXI connections to HBM all of

00:10:46,560 --> 00:10:51,089
the kernel ports need full access to the

00:10:48,630 --> 00:10:53,670
32 gigabyte or 8 gigabytes of memory and

00:10:51,089 --> 00:10:59,330
all kernel masters have a random access

00:10:53,670 --> 00:10:59,330
traffic profile yeah

00:11:04,260 --> 00:11:08,350
it will influence latency exactly that

00:11:07,180 --> 00:11:09,670
and that's one of the reasons why you

00:11:08,350 --> 00:11:14,800
need a large amount of outstanding

00:11:09,670 --> 00:11:17,320
transactions yeah you will have

00:11:14,800 --> 00:11:20,320
transactions in flight but I mean the

00:11:17,320 --> 00:11:22,510
active protocol is split so you know you

00:11:20,320 --> 00:11:23,140
send transactions you don't wait for a

00:11:22,510 --> 00:11:25,960
response

00:11:23,140 --> 00:11:33,610
so latency is overcome by having a large

00:11:25,960 --> 00:11:35,020
number of outstanding transactions it

00:11:33,610 --> 00:11:36,970
will be different for different SLRs

00:11:35,020 --> 00:11:39,100
exactly yeah yeah it's not sudden

00:11:36,970 --> 00:11:42,850
significantly different it would be of

00:11:39,100 --> 00:11:46,180
the order of a couple of cycles you know

00:11:42,850 --> 00:11:53,110
a couple of pipeline cycles of the HBM

00:11:46,180 --> 00:11:59,980
block so just to look at the XO CC

00:11:53,110 --> 00:12:02,440
syntax to to execute the example I

00:11:59,980 --> 00:12:04,870
talked about so we have we have 16 ports

00:12:02,440 --> 00:12:07,060
each of those ports has a kernel

00:12:04,870 --> 00:12:09,160
argument so we need to associate that

00:12:07,060 --> 00:12:12,520
kernel argument with with memory

00:12:09,160 --> 00:12:14,530
resources so in this case we want to

00:12:12,520 --> 00:12:17,080
talk to all eight gigs on all memories

00:12:14,530 --> 00:12:20,980
so we need to specify that so HBM 0 to

00:12:17,080 --> 00:12:23,260
31 is are all the pseudo channels we

00:12:20,980 --> 00:12:25,870
need to place the kernel in SL are 0 so

00:12:23,260 --> 00:12:27,850
this this option places the kernel in SL

00:12:25,870 --> 00:12:30,610
are 0 for this case we don't need to

00:12:27,850 --> 00:12:33,880
this is the default placement anyway but

00:12:30,610 --> 00:12:36,810
as the device fills up floor planning

00:12:33,880 --> 00:12:41,010
will benefit the clock frequency and

00:12:36,810 --> 00:12:43,810
performance and we need to specify a

00:12:41,010 --> 00:12:47,080
user tickler file so what this user

00:12:43,810 --> 00:12:49,030
tickle file does is it's it allows you

00:12:47,080 --> 00:12:50,770
to include Rama so you basically in this

00:12:49,030 --> 00:12:53,680
it's a single line of tickle to identify

00:12:50,770 --> 00:12:56,020
the master ports that are random access

00:12:53,680 --> 00:12:58,900
or require random access so by default

00:12:56,020 --> 00:13:00,850
there is no Rama so we need to you need

00:12:58,900 --> 00:13:05,380
to include it if your if your master is

00:13:00,850 --> 00:13:08,380
random access so just a note on

00:13:05,380 --> 00:13:10,390
allocating device buffers so if you want

00:13:08,380 --> 00:13:11,449
to maximize access to a buffer you need

00:13:10,390 --> 00:13:14,329
to spread

00:13:11,449 --> 00:13:17,540
spread this buffer across multiple hpm

00:13:14,329 --> 00:13:19,730
sewer channels so the HBM search Alan

00:13:17,540 --> 00:13:22,910
Taylor ban was fourteen point four gigs

00:13:19,730 --> 00:13:24,920
if you've multiple engines trying to

00:13:22,910 --> 00:13:26,509
access this you will need more bandwidth

00:13:24,920 --> 00:13:29,240
so you want to split split the data

00:13:26,509 --> 00:13:31,759
across multiple buffers so that's a

00:13:29,240 --> 00:13:34,790
manual process so you need to specify

00:13:31,759 --> 00:13:37,339
built in the correct access and the X or

00:13:34,790 --> 00:13:39,259
CC command-line and also you need to

00:13:37,339 --> 00:13:42,019
split the buffer in your hosts code so

00:13:39,259 --> 00:13:44,869
actually create individual smaller

00:13:42,019 --> 00:13:49,309
buffers and allocate them to to the

00:13:44,869 --> 00:13:51,110
different HBM pseudo Charles and just

00:13:49,309 --> 00:13:54,259
one no runs eiling's runtime so the

00:13:51,110 --> 00:13:56,300
buffer can only be created up to the

00:13:54,259 --> 00:13:58,069
size of the HP and pseudo channel so as

00:13:56,300 --> 00:13:59,509
the maximum buffer size if you have

00:13:58,069 --> 00:14:04,970
larger buffer size you will have to

00:13:59,509 --> 00:14:06,829
split it anyway so some notes and

00:14:04,970 --> 00:14:10,490
constraints then are under bell process

00:14:06,829 --> 00:14:12,949
so at the moment everything is done via

00:14:10,490 --> 00:14:14,779
DMA there's no no streaming kernels in

00:14:12,949 --> 00:14:16,519
this platform or in any of the

00:14:14,779 --> 00:14:21,319
acceleration platters eiling's

00:14:16,519 --> 00:14:25,160
acceleration platforms up after 32

00:14:21,319 --> 00:14:28,629
kernel masters can access HBM so we

00:14:25,160 --> 00:14:31,730
recommend using only 31 up to 31 the

00:14:28,629 --> 00:14:34,730
reason is that one of those current

00:14:31,730 --> 00:14:36,799
ports will share with DMA so you know

00:14:34,730 --> 00:14:38,799
you may not get the performance you want

00:14:36,799 --> 00:14:40,879
either on DMA around that kernel port

00:14:38,799 --> 00:14:45,439
but as long as you're aware of that fact

00:14:40,879 --> 00:14:48,499
you can use 32 any kernels recording

00:14:45,439 --> 00:14:51,619
access to both HBM and DDR need separate

00:14:48,499 --> 00:14:54,829
masters so it's not a combined memory

00:14:51,619 --> 00:14:58,720
subsystem if you want to share access to

00:14:54,829 --> 00:15:01,519
both you may need to do mem copies etc

00:14:58,720 --> 00:15:04,160
and the current master has to can only

00:15:01,519 --> 00:15:07,040
access contiguous subsets of the of the

00:15:04,160 --> 00:15:10,790
HBM pseudo channels that's just to make

00:15:07,040 --> 00:15:14,149
the platform easier and by default all

00:15:10,790 --> 00:15:17,059
kernel masters will access one HP MPC so

00:15:14,149 --> 00:15:19,790
if you don't specify any memory

00:15:17,059 --> 00:15:21,199
resources for a particular arguments the

00:15:19,790 --> 00:15:24,350
platform will assume that you only want

00:15:21,199 --> 00:15:24,980
to use one one HP MPC the reason for

00:15:24,350 --> 00:15:28,339
that is just

00:15:24,980 --> 00:15:30,800
to minimize power for for to force users

00:15:28,339 --> 00:15:35,420
to to plan memory and also to minimize

00:15:30,800 --> 00:15:39,440
power and by default all kernels are

00:15:35,420 --> 00:15:42,199
placed in SLR zero so just to talk about

00:15:39,440 --> 00:15:43,940
about compile time so looking at the the

00:15:42,199 --> 00:15:46,639
trivial example we had earlier of one

00:15:43,940 --> 00:15:51,620
car with 16 masters so this is it's a

00:15:46,639 --> 00:15:53,570
real example of a kernel using kind of

00:15:51,620 --> 00:15:56,089
the standard clock frequencies of the

00:15:53,570 --> 00:15:58,880
platform so HP I'm running at 450

00:15:56,089 --> 00:16:02,750
megahertz the kernel running at 300 the

00:15:58,880 --> 00:16:04,459
compile time is three hours for larger

00:16:02,750 --> 00:16:07,100
kernels for stress test this is a

00:16:04,459 --> 00:16:09,139
customer design so twelve kernels each

00:16:07,100 --> 00:16:11,870
with a single master the kernels are

00:16:09,139 --> 00:16:15,769
equally spread across all SLRs again

00:16:11,870 --> 00:16:17,029
default frequencies and the device is

00:16:15,769 --> 00:16:19,279
heavily used greater than sixty percent

00:16:17,029 --> 00:16:22,339
and lots sort of compiled turbines nine

00:16:19,279 --> 00:16:24,380
hours so you should expect your design

00:16:22,339 --> 00:16:29,540
to fall somewhere in there between three

00:16:24,380 --> 00:16:31,940
and nine hours of comparison okay so

00:16:29,540 --> 00:16:34,670
memory access performance so that the

00:16:31,940 --> 00:16:38,660
baseline for memory access is point

00:16:34,670 --> 00:16:41,839
point so from a kernel master to two HBM

00:16:38,660 --> 00:16:45,019
sudo channel and if we look at that for

00:16:41,839 --> 00:16:47,449
linear accesses for any transaction size

00:16:45,019 --> 00:16:50,839
greater than 64 byte for read only and

00:16:47,449 --> 00:16:53,449
write only we get and 91 percent

00:16:50,839 --> 00:16:56,600
efficiency so that the bandwidth of one

00:16:53,449 --> 00:16:59,449
pc is tartly in gigabytes per second for

00:16:56,600 --> 00:17:03,230
and the total system bandwidth is 419

00:16:59,449 --> 00:17:05,360
gigabytes for a second for 32 bytes this

00:17:03,230 --> 00:17:08,510
drops so we only get 30 percent

00:17:05,360 --> 00:17:12,760
efficiency for a total system bandwidth

00:17:08,510 --> 00:17:15,770
of 144 gigabytes the reason for that is

00:17:12,760 --> 00:17:18,020
memory controller design and the HP n

00:17:15,770 --> 00:17:23,120
typical timing parameters so it's

00:17:18,020 --> 00:17:24,770
difficult in in HBM with with typical

00:17:23,120 --> 00:17:26,720
timing parameters to get above in fact

00:17:24,770 --> 00:17:29,000
the theoretical maximum is 50 percent

00:17:26,720 --> 00:17:31,309
so at refresh that brings it down to 40

00:17:29,000 --> 00:17:35,500
and the memory controller inefficiencies

00:17:31,309 --> 00:17:35,500
bring it down to around 30 percent

00:17:35,870 --> 00:17:40,700
so looking at random access which is of

00:17:38,419 --> 00:17:44,270
the more difficult case again this point

00:17:40,700 --> 00:17:48,350
point so if you look at transaction

00:17:44,270 --> 00:17:49,760
sizes of 128 bytes or greater again

00:17:48,350 --> 00:17:50,870
we're looking at write only and read

00:17:49,760 --> 00:17:53,480
only here we're not looking at

00:17:50,870 --> 00:17:56,570
turnaround time for read/write so we get

00:17:53,480 --> 00:17:59,360
an efficiency of of 60% unbought read

00:17:56,570 --> 00:18:04,760
only and write only total bandwidth is

00:17:59,360 --> 00:18:07,850
275 gigabytes per second so going down

00:18:04,760 --> 00:18:10,340
to 32 bytes we see our 30% performance

00:18:07,850 --> 00:18:11,690
level again so a total bandwidth of one

00:18:10,340 --> 00:18:18,039
hundred and forty four gigabytes per

00:18:11,690 --> 00:18:20,120
second so so we've looked at at

00:18:18,039 --> 00:18:22,850
standalone performance or point-to-point

00:18:20,120 --> 00:18:26,510
performance so really we we need to look

00:18:22,850 --> 00:18:28,700
at subsystem performance as well so what

00:18:26,510 --> 00:18:32,330
we've taken we've created a kernel that

00:18:28,700 --> 00:18:33,860
that can assess subsystem performance so

00:18:32,330 --> 00:18:36,620
this this kernel executes different

00:18:33,860 --> 00:18:39,049
access patterns and on different numbers

00:18:36,620 --> 00:18:41,149
of masters different transaction types

00:18:39,049 --> 00:18:43,669
or read and write different AXI

00:18:41,149 --> 00:18:45,529
transaction type different transaction

00:18:43,669 --> 00:18:48,980
sizes and different addressing patterns

00:18:45,529 --> 00:18:53,210
and and using different memory resources

00:18:48,980 --> 00:18:55,100
as well so for these experiments Rama

00:18:53,210 --> 00:18:57,049
was used in every master connection sort

00:18:55,100 --> 00:18:59,419
of random access IP is used in every

00:18:57,049 --> 00:19:01,730
master connection so there's no SSID

00:18:59,419 --> 00:19:04,399
blocking it's you know efficient for

00:19:01,730 --> 00:19:08,179
random access and RAM has no impact on

00:19:04,399 --> 00:19:10,970
linear linear performance so this this

00:19:08,179 --> 00:19:17,149
may come as a tool eventually in in in

00:19:10,970 --> 00:19:19,730
SDX or of Aveiro to ssh performance so

00:19:17,149 --> 00:19:21,529
I've taken one case so this is actually

00:19:19,730 --> 00:19:24,230
a kind of a worst case but I think it's

00:19:21,529 --> 00:19:27,289
interesting to look at and brings up a

00:19:24,230 --> 00:19:31,309
few interesting points so this is a 32

00:19:27,289 --> 00:19:33,260
byte random access case what we're doing

00:19:31,309 --> 00:19:35,029
is we're increasing the number of pseudo

00:19:33,260 --> 00:19:37,610
channels used and increasing the number

00:19:35,029 --> 00:19:41,870
of masters and looking at at system

00:19:37,610 --> 00:19:43,669
bandwidth okay so every the important

00:19:41,870 --> 00:19:45,140
point is that every master has access to

00:19:43,669 --> 00:19:48,230
every pseudo channel so on the switch

00:19:45,140 --> 00:19:49,250
every node is being used constantly so

00:19:48,230 --> 00:19:53,030
it's the worst case for

00:19:49,250 --> 00:19:55,370
switch okay so we have random reads on

00:19:53,030 --> 00:19:58,040
the left and random writes on the on the

00:19:55,370 --> 00:20:00,430
right so we're plotting bandwidth versus

00:19:58,040 --> 00:20:03,440
the pseudo Chowhound

00:20:00,430 --> 00:20:05,600
so the first point to note is that

00:20:03,440 --> 00:20:08,330
performance scales linearly with a

00:20:05,600 --> 00:20:10,280
smaller number of pcs so if you use up

00:20:08,330 --> 00:20:12,500
to eight pseudo channels and you're

00:20:10,280 --> 00:20:14,900
doing random access from four to eight

00:20:12,500 --> 00:20:16,790
masters you get seven to eight times

00:20:14,900 --> 00:20:19,400
better performance than one pseudo

00:20:16,790 --> 00:20:21,620
channel for right performance it

00:20:19,400 --> 00:20:24,200
actually scales well up to up to sixteen

00:20:21,620 --> 00:20:26,480
serie channels but once the number of

00:20:24,200 --> 00:20:27,980
students is greater than sixteen the

00:20:26,480 --> 00:20:29,690
performance is bottlenecked by the

00:20:27,980 --> 00:20:34,790
crossing between stacks so this is

00:20:29,690 --> 00:20:36,620
specific to 32 byte and 64 bytes for

00:20:34,790 --> 00:20:39,440
read performance it also scales in

00:20:36,620 --> 00:20:41,570
perfectly so for sixteen pseudo channels

00:20:39,440 --> 00:20:44,000
we only see eleven times better

00:20:41,570 --> 00:20:46,010
performance than one for thirty-two

00:20:44,000 --> 00:20:48,320
pseudo channels we only see thirteen

00:20:46,010 --> 00:20:50,780
times better performance than one okay

00:20:48,320 --> 00:20:54,590
so this is because of congestion and

00:20:50,780 --> 00:20:57,080
lateral links each switch port also has

00:20:54,590 --> 00:20:59,960
limited outstanding transactions so if

00:20:57,080 --> 00:21:01,880
you have large latency your there is a

00:20:59,960 --> 00:21:06,190
blocking effect if you've large latency

00:21:01,880 --> 00:21:09,980
due to congestion in the switch okay

00:21:06,190 --> 00:21:11,990
so just to summarize okay so I focused

00:21:09,980 --> 00:21:14,800
on some one a negative all right just

00:21:11,990 --> 00:21:18,290
just as a as a as a just to show that

00:21:14,800 --> 00:21:21,590
the American system is complex and needs

00:21:18,290 --> 00:21:23,720
to be experimented with to get the best

00:21:21,590 --> 00:21:25,280
performance but you know in general

00:21:23,720 --> 00:21:29,330
there's a huge amount of bandwidth for

00:21:25,280 --> 00:21:31,310
very little effort and and very little

00:21:29,330 --> 00:21:33,710
impact and the free platform resources

00:21:31,310 --> 00:21:36,560
so everything's hardened so you get this

00:21:33,710 --> 00:21:38,390
essentially for free there's a huge

00:21:36,560 --> 00:21:41,500
amount of flexibility in the X access

00:21:38,390 --> 00:21:44,120
patterns but because of that flexibility

00:21:41,500 --> 00:21:46,430
there is some maybe some work to be done

00:21:44,120 --> 00:21:49,160
depending on your depending on your

00:21:46,430 --> 00:21:52,040
memory access topology so you may need

00:21:49,160 --> 00:21:55,790
to spend some time optimizing to get the

00:21:52,040 --> 00:21:57,560
best performance you know a basic you

00:21:55,790 --> 00:22:00,380
know the most obvious thing is to use as

00:21:57,560 --> 00:22:03,470
many hvm pseudo channels as as necessary

00:22:00,380 --> 00:22:06,890
in parallel you that may need some

00:22:03,470 --> 00:22:08,870
experimentation to figure out any kernel

00:22:06,890 --> 00:22:11,780
master should have a large number of

00:22:08,870 --> 00:22:13,520
outstanding transactions so at least

00:22:11,780 --> 00:22:16,400
sixty four outstanding read and

00:22:13,520 --> 00:22:17,810
thirty-two outstanding rife more than

00:22:16,400 --> 00:22:20,240
this for random access so if you're

00:22:17,810 --> 00:22:21,170
using the Rama IP with reordering you

00:22:20,240 --> 00:22:25,940
may need more

00:22:21,170 --> 00:22:27,800
we used 128 in our test cases and you

00:22:25,940 --> 00:22:29,990
know be aware of the unexpected behavior

00:22:27,800 --> 00:22:31,700
so 32 by transactions have poor

00:22:29,990 --> 00:22:34,190
efficiency that's the nature of deer and

00:22:31,700 --> 00:22:36,410
smaller and smaller transactions so if

00:22:34,190 --> 00:22:39,260
possible in your application use larger

00:22:36,410 --> 00:22:41,030
and avoid routing small write

00:22:39,260 --> 00:22:45,230
transactions across the boundary that's

00:22:41,030 --> 00:22:47,890
that's a assigning specific thing so

00:22:45,230 --> 00:22:47,890

YouTube URL: https://www.youtube.com/watch?v=Ld1deFdCUys


