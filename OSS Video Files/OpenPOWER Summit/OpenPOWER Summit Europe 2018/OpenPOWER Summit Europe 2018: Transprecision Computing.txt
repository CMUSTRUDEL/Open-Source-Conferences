Title: OpenPOWER Summit Europe 2018: Transprecision Computing
Publication date: 2019-02-07
Playlist: OpenPOWER Summit Europe 2018
Description: 
	Dionysios Diamantopoulos, IBM Research, speaks at OpenPOWER Summit Europe 2018.

For more information, please visit: https://openpowerfoundation.org/summit-2018-10-eu/
Captions: 
	00:00:00,089 --> 00:00:04,980
good afternoon everybody I am Denise is

00:00:02,970 --> 00:00:07,589
the mandible is coming from from IBM

00:00:04,980 --> 00:00:10,559
research labs in Zurich and I'm gonna

00:00:07,589 --> 00:00:11,969
present our work regarding trans

00:00:10,559 --> 00:00:13,740
precision computing it is about a

00:00:11,969 --> 00:00:19,020
project that a lot of people are working

00:00:13,740 --> 00:00:21,330
on so probably a lot of people here who

00:00:19,020 --> 00:00:22,859
would you like the blue color but my

00:00:21,330 --> 00:00:26,400
story is start starting a little bit

00:00:22,859 --> 00:00:29,039
dark and this a lot of dark it is 91%

00:00:26,400 --> 00:00:31,679
dark and it is actually what is the dark

00:00:29,039 --> 00:00:34,559
silicone area of our tips today at 11

00:00:31,679 --> 00:00:36,329
nanometers technology node and this

00:00:34,559 --> 00:00:38,340
comes from the fact that we cannot power

00:00:36,329 --> 00:00:40,620
up all our transistors probably you know

00:00:38,340 --> 00:00:43,350
about that simultaneously because we're

00:00:40,620 --> 00:00:46,739
gonna overpass the thermal design power

00:00:43,350 --> 00:00:47,969
of our chips so one solution to fight

00:00:46,739 --> 00:00:49,680
this problem of course there are a lot

00:00:47,969 --> 00:00:51,600
of solutions out there but one of them

00:00:49,680 --> 00:00:54,329
is approximate computing and approximate

00:00:51,600 --> 00:00:56,430
computing is a technique that we are

00:00:54,329 --> 00:00:59,160
using and allows us to let's say if we

00:00:56,430 --> 00:01:03,000
have our computational progress and on

00:00:59,160 --> 00:01:05,850
the resultant axis and the Machine

00:01:03,000 --> 00:01:07,950
precision on the vertical one the

00:01:05,850 --> 00:01:09,570
transfer the approximate that this is

00:01:07,950 --> 00:01:13,700
the traditional computing as we know it

00:01:09,570 --> 00:01:17,040
and approximate computing is allows for

00:01:13,700 --> 00:01:18,960
decreasing the precision over the

00:01:17,040 --> 00:01:21,299
computational progress at a specific

00:01:18,960 --> 00:01:25,140
level so that we can gain energy from

00:01:21,299 --> 00:01:27,900
that fact one limitations on that is

00:01:25,140 --> 00:01:30,960
that it it is targeting only specific

00:01:27,900 --> 00:01:33,750
problems and approximation is introduced

00:01:30,960 --> 00:01:36,540
onto selected routines typically this

00:01:33,750 --> 00:01:39,360
leads to a lot of the final result of

00:01:36,540 --> 00:01:43,340
the quality loss and there is usually no

00:01:39,360 --> 00:01:46,829
error bound on this on this kind of

00:01:43,340 --> 00:01:48,860
approach so we are introducing a

00:01:46,829 --> 00:01:51,090
different computing paradigm named

00:01:48,860 --> 00:01:53,880
transportation computing with ease so

00:01:51,090 --> 00:01:56,159
I'm just showing here that the overview

00:01:53,880 --> 00:01:59,310
of our work and tomorrow I will get into

00:01:56,159 --> 00:02:01,560
more details the idea is that we keep

00:01:59,310 --> 00:02:03,479
the precision across the computational

00:02:01,560 --> 00:02:08,250
progress to the specific levels that we

00:02:03,479 --> 00:02:11,459
do and can tolerate this allows for more

00:02:08,250 --> 00:02:12,600
fine-grain precision adaptation over the

00:02:11,459 --> 00:02:15,570
progress

00:02:12,600 --> 00:02:17,430
and the differentiator from approximate

00:02:15,570 --> 00:02:22,050
computing is that we're using adapting

00:02:17,430 --> 00:02:25,410
adaptive precision calibration and we

00:02:22,050 --> 00:02:28,170
also use some stopping adaptive stopping

00:02:25,410 --> 00:02:30,360
criteria when specific precision is is

00:02:28,170 --> 00:02:32,670
being met and also we introduced

00:02:30,360 --> 00:02:38,400
recovery accuracy mechanisms by the end

00:02:32,670 --> 00:02:40,980
of the calculation now this is the whole

00:02:38,400 --> 00:02:43,200
idea of the project we do not create

00:02:40,980 --> 00:02:47,010
just one architecture or just one

00:02:43,200 --> 00:02:49,470
library but we are we're visiting the

00:02:47,010 --> 00:02:50,850
whole the complete computing stack from

00:02:49,470 --> 00:02:53,120
the physical foundations and the

00:02:50,850 --> 00:02:57,480
mathematical theory at the bottom level

00:02:53,120 --> 00:03:00,000
to the algorithms the upper level so we

00:02:57,480 --> 00:03:02,490
are introducing trust precision concepts

00:03:00,000 --> 00:03:05,520
libraries hardware software compiler

00:03:02,490 --> 00:03:08,400
support for all over the computing stack

00:03:05,520 --> 00:03:10,140
as we note and we prove that this is

00:03:08,400 --> 00:03:12,930
working we'll gonna show that this is

00:03:10,140 --> 00:03:15,480
working for a bunch of applications from

00:03:12,930 --> 00:03:18,840
HPC and simulations big data and deep

00:03:15,480 --> 00:03:22,220
learning the target of this project is

00:03:18,840 --> 00:03:24,600
to create two different let's say

00:03:22,220 --> 00:03:27,540
architectures one targeting the

00:03:24,600 --> 00:03:29,580
milliwatt space so why ot domain and

00:03:27,540 --> 00:03:33,240
other and one other the high-performance

00:03:29,580 --> 00:03:36,000
computing how we build this the basic

00:03:33,240 --> 00:03:39,060
architecture is our power eight systems

00:03:36,000 --> 00:03:42,300
coupled with processors specifically

00:03:39,060 --> 00:03:45,570
designed to accommodate for ant it

00:03:42,300 --> 00:03:48,920
reduce trans precision we are using for

00:03:45,570 --> 00:03:52,200
this the pulp the pulp system which is

00:03:48,920 --> 00:03:55,260
developed by ETH in Zurich and in

00:03:52,200 --> 00:03:57,270
University of Bologna and it is

00:03:55,260 --> 00:04:00,570
providing several features to add the

00:03:57,270 --> 00:04:02,700
trans precision concept we couple this

00:04:00,570 --> 00:04:05,550
thing together with our power eight

00:04:02,700 --> 00:04:10,830
systems over the coherent bus coupling

00:04:05,550 --> 00:04:12,570
and our goal is to build up on a

00:04:10,830 --> 00:04:14,690
high-performance computing system out of

00:04:12,570 --> 00:04:18,239
these architectures supporting

00:04:14,690 --> 00:04:22,020
transportation capabilities so spoiler

00:04:18,239 --> 00:04:25,730
alert here the pal project is has

00:04:22,020 --> 00:04:29,720
already sown and designed over 20

00:04:25,730 --> 00:04:34,340
bulk based Asics already and you can

00:04:29,720 --> 00:04:37,790
find more information here now how we

00:04:34,340 --> 00:04:40,370
build this infrastructure as a total up

00:04:37,790 --> 00:04:42,710
to now we have managed to build our

00:04:40,370 --> 00:04:46,160
first version of this of the system

00:04:42,710 --> 00:04:48,830
coupling together open power ecosystem

00:04:46,160 --> 00:04:50,960
which is providing our hyper performance

00:04:48,830 --> 00:04:53,480
computing nodes like power power 8 and

00:04:50,960 --> 00:04:55,820
power 9 platforms and also the services

00:04:53,480 --> 00:04:58,100
for cloud computing and we combine it

00:04:55,820 --> 00:05:01,270
with the technology developed or under

00:04:58,100 --> 00:05:03,800
the Oprah cone project and in

00:05:01,270 --> 00:05:07,360
introducing transposition capabilities

00:05:03,800 --> 00:05:11,240
now this is our first prototyping system

00:05:07,360 --> 00:05:15,200
composed of power 8 device and and and

00:05:11,240 --> 00:05:19,100
FPT FPGA card the FPGA cart is having

00:05:15,200 --> 00:05:20,510
this pulp processor over here and as you

00:05:19,100 --> 00:05:24,470
can see we are developing three

00:05:20,510 --> 00:05:26,810
different three environments for for

00:05:24,470 --> 00:05:28,490
beating up this this framework here the

00:05:26,810 --> 00:05:29,870
cloud development kit the software

00:05:28,490 --> 00:05:32,590
development kit and the hardware

00:05:29,870 --> 00:05:35,630
development kit a la that allows us to

00:05:32,590 --> 00:05:37,550
quickly deploy instances of Trance

00:05:35,630 --> 00:05:40,820
precision processors pulp based

00:05:37,550 --> 00:05:44,630
processors on the FDA couple them to our

00:05:40,820 --> 00:05:51,890
open powers system and create virtual

00:05:44,630 --> 00:05:54,350
images and documents is on on top of the

00:05:51,890 --> 00:05:58,340
super vessel environment so on the cloud

00:05:54,350 --> 00:06:00,920
and offer them on the cloud so the first

00:05:58,340 --> 00:06:03,590
version of our system is this one power

00:06:00,920 --> 00:06:06,890
8 Minsky node with we have tested both

00:06:03,590 --> 00:06:09,950
two devices k23 and 8k you and we've

00:06:06,890 --> 00:06:14,180
managed to load two clusters of pulp on

00:06:09,950 --> 00:06:15,920
the k 2 3 4 on the 8k you up to 50

00:06:14,180 --> 00:06:20,990
minutes right now but we're developing

00:06:15,920 --> 00:06:25,880
and we're targeting more more speed of

00:06:20,990 --> 00:06:28,820
our clusters and our second version is

00:06:25,880 --> 00:06:30,650
currently under development here is the

00:06:28,820 --> 00:06:35,030
power 9 machine that we have in zurich

00:06:30,650 --> 00:06:38,569
lab and 93

00:06:35,030 --> 00:06:40,460
card with copy and token copy support

00:06:38,569 --> 00:06:42,710
this is the image from actually previous

00:06:40,460 --> 00:06:45,680
week that we've managed to take the

00:06:42,710 --> 00:06:48,259
Akron cards and connect them with open

00:06:45,680 --> 00:06:52,569
copy open copy link so we're working on

00:06:48,259 --> 00:06:55,009
on developing this version as well so

00:06:52,569 --> 00:06:57,050
well we have we are testing a lot of

00:06:55,009 --> 00:07:00,860
different applications as I said from

00:06:57,050 --> 00:07:03,289
HPC big data and neural networks this is

00:07:00,860 --> 00:07:06,699
just one example that we've designed so

00:07:03,289 --> 00:07:09,469
far with with the use of snap tool and

00:07:06,699 --> 00:07:11,599
we've managed to map a specific neural

00:07:09,469 --> 00:07:13,789
network accelerator for the BLS TM which

00:07:11,599 --> 00:07:16,009
stunts it is a recurrent neural network

00:07:13,789 --> 00:07:18,530
it is a memory bound application and

00:07:16,009 --> 00:07:21,590
with copy one version you can see that

00:07:18,530 --> 00:07:24,110
the scales really well according you can

00:07:21,590 --> 00:07:25,879
add as many accelerator or FPGA ghen can

00:07:24,110 --> 00:07:30,080
host but with four accelerators we're

00:07:25,879 --> 00:07:32,900
reaching the 96% of the block Rams and

00:07:30,080 --> 00:07:36,110
we've managed to measure up to 22 fold

00:07:32,900 --> 00:07:38,569
energy efficiency compared to CPU using

00:07:36,110 --> 00:07:41,330
the threads of power eight and two point

00:07:38,569 --> 00:07:45,289
for energy efficiency compared to high

00:07:41,330 --> 00:07:50,300
end GPU so we've compared with P 100

00:07:45,289 --> 00:07:52,219
using the half data type so the first

00:07:50,300 --> 00:07:54,740
results are really promising and we're

00:07:52,219 --> 00:07:56,870
mapping other applications as well so

00:07:54,740 --> 00:08:00,710
just to take away method we're

00:07:56,870 --> 00:08:05,029
developing inside this project a first

00:08:00,710 --> 00:08:07,449
full transposition framework and it is

00:08:05,029 --> 00:08:10,039
we we we expect to be the open

00:08:07,449 --> 00:08:12,020
completely open every technology that

00:08:10,039 --> 00:08:14,449
we're developing is released that there

00:08:12,020 --> 00:08:18,050
are parts license more details can be

00:08:14,449 --> 00:08:20,659
provided tomorrow so we have a lot of

00:08:18,050 --> 00:08:22,729
tasks that we're currently working on I

00:08:20,659 --> 00:08:25,039
will get into more details tomorrow but

00:08:22,729 --> 00:08:26,930
of course there are other aspects that I

00:08:25,039 --> 00:08:29,270
didn't mention at all as the work that

00:08:26,930 --> 00:08:30,919
we're doing on compiler to support the

00:08:29,270 --> 00:08:33,289
transposition because the transposition

00:08:30,919 --> 00:08:37,219
and different or mixed precision is okay

00:08:33,289 --> 00:08:39,469
but how can you just developers to

00:08:37,219 --> 00:08:41,659
really easily adapt the precision at

00:08:39,469 --> 00:08:45,290
runtime so we're providing compiler

00:08:41,659 --> 00:08:48,340
support for that and other things such

00:08:45,290 --> 00:08:52,420
as trans precision DRAM controllers

00:08:48,340 --> 00:08:55,590
other other things so there is a lot of

00:08:52,420 --> 00:08:58,630
a room there so if you're working on

00:08:55,590 --> 00:09:00,790
mixed precision or you have ideas on

00:08:58,630 --> 00:09:04,270
that we will be really very happy to

00:09:00,790 --> 00:09:06,820
collaborate on this project and and

00:09:04,270 --> 00:09:09,580
visit this this site also we have post

00:09:06,820 --> 00:09:11,890
stable versions of our software like

00:09:09,580 --> 00:09:17,320
some libraries that are providing

00:09:11,890 --> 00:09:20,290
arbitrary-precision support for as well

00:09:17,320 --> 00:09:22,930
linear algebra on the github and you can

00:09:20,290 --> 00:09:24,940
also visit the site over there a lot of

00:09:22,930 --> 00:09:27,220
people are working on this project from

00:09:24,940 --> 00:09:30,400
from different expertise and different

00:09:27,220 --> 00:09:32,260
domains as I said we're revisiting the

00:09:30,400 --> 00:09:35,110
complete computing stack so this is the

00:09:32,260 --> 00:09:36,640
complete consortium and this is all from

00:09:35,110 --> 00:09:39,630
my side thanks a lot for your attention

00:09:36,640 --> 00:09:43,269
and this is all

00:09:39,630 --> 00:09:43,269

YouTube URL: https://www.youtube.com/watch?v=1m0bBgGehDI


