Title: OpenPOWER Summit Europe 2018: Feeding OpenCAPI Streaming-Based FPGA Accelerators
Publication date: 2019-02-07
Playlist: OpenPOWER Summit Europe 2018
Description: 
	Y.T.B. Mulder, TU Delft, speaks at OpenPOWER Summit Europe 2018.

For more information, please visit: https://openpowerfoundation.org/summit-2018-10-eu/
Captions: 
	00:00:00,030 --> 00:00:06,509
right my name is Evo I did my master

00:00:03,659 --> 00:00:08,670
thesis at University of Delft currently

00:00:06,509 --> 00:00:10,679
working at IBM Germany but this is from

00:00:08,670 --> 00:00:14,030
my time at the Adel that I'm gonna talk

00:00:10,679 --> 00:00:15,599
to you about how do we feed open Cappy

00:00:14,030 --> 00:00:20,400
accelerators and to be specific

00:00:15,599 --> 00:00:23,160
streaming based accelerators so the

00:00:20,400 --> 00:00:25,080
introduction is a bit bit more or

00:00:23,160 --> 00:00:28,080
roughly the same as many others today so

00:00:25,080 --> 00:00:30,269
I'll try to go quickly over it so as we

00:00:28,080 --> 00:00:32,790
all know CPU performance is reaching a

00:00:30,269 --> 00:00:35,399
plateau we need heterogeneous compute

00:00:32,790 --> 00:00:38,579
and accelerated are becoming commonplace

00:00:35,399 --> 00:00:40,739
such as GPUs but FBF PJ's are also

00:00:38,579 --> 00:00:42,059
getting into this space and what we're

00:00:40,739 --> 00:00:43,680
seeing is that network and storage

00:00:42,059 --> 00:00:46,950
bandwidth on a system level is

00:00:43,680 --> 00:00:50,969
increasing so rapidly as we see here

00:00:46,950 --> 00:00:53,460
from a 25 years roughly of data but

00:00:50,969 --> 00:00:56,699
actually DRAM and an interconnect

00:00:53,460 --> 00:01:00,750
dynamics is sort of not increasing the

00:00:56,699 --> 00:01:02,399
band is at the same rate so we need an

00:01:00,750 --> 00:01:04,710
increase in IO bandwidth in order to

00:01:02,399 --> 00:01:05,909
sustain our systems and and supply

00:01:04,710 --> 00:01:10,260
network and storage with enough

00:01:05,909 --> 00:01:12,299
bandwidth and besides the bandwidth

00:01:10,260 --> 00:01:15,030
bottleneck is there also that the

00:01:12,299 --> 00:01:17,610
traditional IO model is not helping here

00:01:15,030 --> 00:01:21,500
because it copies data through memory

00:01:17,610 --> 00:01:24,330
and open copy addresses this bottleneck

00:01:21,500 --> 00:01:25,830
and also the bandwidth bottleneck we've

00:01:24,330 --> 00:01:27,930
seen so there's an order of magnitude

00:01:25,830 --> 00:01:30,030
more bandwidth available and we have a

00:01:27,930 --> 00:01:32,159
shared memory model and we have shared

00:01:30,030 --> 00:01:36,750
virtual addressing which which should

00:01:32,159 --> 00:01:38,820
solve our problems so that's great so

00:01:36,750 --> 00:01:40,530
what kind of application would actually

00:01:38,820 --> 00:01:42,960
use these types of bandwidth we've seen

00:01:40,530 --> 00:01:45,750
of course a lot of examples today with

00:01:42,960 --> 00:01:49,500
adding storage and and whatnot but let's

00:01:45,750 --> 00:01:51,689
look at databases for change so in the

00:01:49,500 --> 00:01:54,649
past and Gyan also mentioned this there

00:01:51,689 --> 00:01:57,719
was something called net heads ax and

00:01:54,649 --> 00:02:00,540
database is relying on in on hard drives

00:01:57,719 --> 00:02:02,430
were relatively low bandwidth and there

00:02:00,540 --> 00:02:04,259
was a they were connected to a host of

00:02:02,430 --> 00:02:06,360
an FPGA which accelerated the query

00:02:04,259 --> 00:02:08,009
maybe did some filtering and then

00:02:06,360 --> 00:02:11,430
shipped it off to the to the host which

00:02:08,009 --> 00:02:14,310
this the the final processing but

00:02:11,430 --> 00:02:17,730
nowadays databases are moving

00:02:14,310 --> 00:02:21,959
two in memory and now FPGAs only have to

00:02:17,730 --> 00:02:24,390
catch up with the DRAM bandwidth instead

00:02:21,959 --> 00:02:26,940
of the bandwidth from hard drives so the

00:02:24,390 --> 00:02:29,489
interconnect between the FPGA and and

00:02:26,940 --> 00:02:33,630
the heart and the host is is a

00:02:29,489 --> 00:02:36,180
bottleneck so the challenge is to for

00:02:33,630 --> 00:02:37,890
FPGA is to be adopted is that an FPGA

00:02:36,180 --> 00:02:39,930
must keep up with the memory bandwidth

00:02:37,890 --> 00:02:41,910
instead of hard drive bandwidth for

00:02:39,930 --> 00:02:44,340
example and this is pretty difficult of

00:02:41,910 --> 00:02:45,989
PCI Express because they haven't been

00:02:44,340 --> 00:02:48,600
increasing their benefit at the same

00:02:45,989 --> 00:02:50,459
rate and as we've also heard today they

00:02:48,600 --> 00:02:53,370
have legacy support and stuff and that

00:02:50,459 --> 00:02:55,049
makes it all pretty pretty difficult so

00:02:53,370 --> 00:02:56,940
open Cappy starts off with this clean

00:02:55,049 --> 00:02:58,500
sheet of paper and it solves these

00:02:56,940 --> 00:02:58,950
traditional io bottlenecks at the same

00:02:58,500 --> 00:03:01,980
time

00:02:58,950 --> 00:03:03,959
this means that attached FPGAs can now

00:03:01,980 --> 00:03:06,000
match the memory bandwidth and that

00:03:03,959 --> 00:03:09,349
means that for example acceleration of

00:03:06,000 --> 00:03:12,000
database operators is possible again

00:03:09,349 --> 00:03:16,230
so this route was conducted in the same

00:03:12,000 --> 00:03:19,170
project as GM and what we were looking

00:03:16,230 --> 00:03:22,140
at in general is more this order of

00:03:19,170 --> 00:03:25,459
magnitude increase in bandwidth what

00:03:22,140 --> 00:03:27,959
what does that mean for FPGA design and

00:03:25,459 --> 00:03:33,239
just as a teaser it is it is pretty

00:03:27,959 --> 00:03:35,280
significant so from a high high level we

00:03:33,239 --> 00:03:37,920
have open capital on one side and then

00:03:35,280 --> 00:03:40,019
there is a local memory on the FPGA

00:03:37,920 --> 00:03:42,989
and the accelerator or the accelerated

00:03:40,019 --> 00:03:44,880
function itself so it this project was a

00:03:42,989 --> 00:03:46,739
group effort so other students looked at

00:03:44,880 --> 00:03:49,799
what kind of queries can be accelerated

00:03:46,739 --> 00:03:54,750
and that's um that's a real accelerator

00:03:49,799 --> 00:03:57,299
what I looked at is okay great we have

00:03:54,750 --> 00:03:59,280
open cap you have an accelerator but how

00:03:57,299 --> 00:04:01,680
do we actually use all of the bandwidth

00:03:59,280 --> 00:04:03,920
that open-carry supplies us and how can

00:04:01,680 --> 00:04:09,299
we actually sustain it and use it

00:04:03,920 --> 00:04:10,620
efficiently so this means that we we

00:04:09,299 --> 00:04:12,060
came up with a river a list of

00:04:10,620 --> 00:04:13,079
requirements so first of all we limited

00:04:12,060 --> 00:04:17,280
ourselves to streaming based

00:04:13,079 --> 00:04:19,079
applications to fully use to utilize the

00:04:17,280 --> 00:04:21,959
bandwidth we need multiple streams so

00:04:19,079 --> 00:04:25,110
let's let's say we have 64 streams of

00:04:21,959 --> 00:04:27,949
data in host memory that we that we want

00:04:25,110 --> 00:04:30,720
the accelerator to to have access to

00:04:27,949 --> 00:04:33,509
so typically you would send a cache line

00:04:30,720 --> 00:04:35,580
of data 2d to the FPGA and then only a

00:04:33,509 --> 00:04:37,620
little part might be interesting you

00:04:35,580 --> 00:04:40,740
have to pick that out and that's that's

00:04:37,620 --> 00:04:44,100
a lot of work so accelerators typically

00:04:40,740 --> 00:04:46,110
use a smaller data granularity so for

00:04:44,100 --> 00:04:47,570
this example let's say that in an entire

00:04:46,110 --> 00:04:50,340
cache line there are eight sort of

00:04:47,570 --> 00:04:54,030
elements or useful elements like a key

00:04:50,340 --> 00:04:57,229
value pair so in order then to keep up

00:04:54,030 --> 00:04:59,880
with the bandwidth you need to re

00:04:57,229 --> 00:05:02,310
consume an entire cache line of data or

00:04:59,880 --> 00:05:04,260
at least the same amount of data so if

00:05:02,310 --> 00:05:06,960
we would split it up in in eight useful

00:05:04,260 --> 00:05:08,760
elements we would need a treat ports to

00:05:06,960 --> 00:05:14,010
sort of a buffer structure that have

00:05:08,760 --> 00:05:15,750
access to any stream that means that you

00:05:14,010 --> 00:05:17,340
could get a read access pattern like

00:05:15,750 --> 00:05:18,960
this so that you have eight different

00:05:17,340 --> 00:05:20,430
streams where you want to read like the

00:05:18,960 --> 00:05:23,789
next element which could be at any

00:05:20,430 --> 00:05:25,530
offset and it reads also may cross a

00:05:23,789 --> 00:05:26,940
cache line boundary then you could have

00:05:25,530 --> 00:05:28,710
something like this where you read from

00:05:26,940 --> 00:05:31,220
four different streams but you actually

00:05:28,710 --> 00:05:35,490
need to read two successive cache lines

00:05:31,220 --> 00:05:36,870
from your from your buffer there are a

00:05:35,490 --> 00:05:38,430
couple of challenges we want to handle

00:05:36,870 --> 00:05:41,639
these various access patterns so it's

00:05:38,430 --> 00:05:44,010
not and at the same time we want to keep

00:05:41,639 --> 00:05:48,169
up with 120 bytes per cycle at open cap

00:05:44,010 --> 00:05:52,800
resupplies us then another problem is

00:05:48,169 --> 00:05:54,870
the the FPGA architecture so if you

00:05:52,800 --> 00:05:56,729
abstract how an FPGA physically looks

00:05:54,870 --> 00:05:58,889
like there are columns with with

00:05:56,729 --> 00:06:02,060
configurable logic blocks like here and

00:05:58,889 --> 00:06:05,010
we have for example columns of of memory

00:06:02,060 --> 00:06:07,229
so if we want to move or extract our

00:06:05,010 --> 00:06:09,419
data from a memory primitive to a core -

00:06:07,229 --> 00:06:12,510
- a lot for example it has to be routed

00:06:09,419 --> 00:06:15,780
somewhere else and and you know you

00:06:12,510 --> 00:06:17,849
don't have the freedom to put your your

00:06:15,780 --> 00:06:19,560
memory primitives like a be Ram as close

00:06:17,849 --> 00:06:25,229
as you as you want to to your

00:06:19,560 --> 00:06:26,760
accelerator and finally in order to keep

00:06:25,229 --> 00:06:28,860
over this bandwidth we have wide data

00:06:26,760 --> 00:06:35,039
paths much wider than then we had in the

00:06:28,860 --> 00:06:36,900
past so looking at this problem from a

00:06:35,039 --> 00:06:39,180
high point of view in this case you

00:06:36,900 --> 00:06:40,590
would have 64 streams of data in the

00:06:39,180 --> 00:06:42,180
host

00:06:40,590 --> 00:06:44,490
and then the FPGA is connected through

00:06:42,180 --> 00:06:46,200
open Cappy we would have a treat ports

00:06:44,490 --> 00:06:48,810
of sixteen bytes in order to get eight

00:06:46,200 --> 00:06:51,690
times 16 is 128 bytes of a cache line

00:06:48,810 --> 00:06:55,110
size of power we want to fetch from any

00:06:51,690 --> 00:06:57,570
stream at any point in time and there's

00:06:55,110 --> 00:06:59,010
a request prioritization in so that

00:06:57,570 --> 00:07:01,530
means that these eight read board's

00:06:59,010 --> 00:07:03,480
report zero till seven and report zero

00:07:01,530 --> 00:07:06,390
always has sort of like the first access

00:07:03,480 --> 00:07:08,670
so if you don't have that you actually

00:07:06,390 --> 00:07:11,310
can reliably say okay we if I read from

00:07:08,670 --> 00:07:12,960
the same stream eight times then I will

00:07:11,310 --> 00:07:17,880
get that in the same succession is my

00:07:12,960 --> 00:07:21,480
read ports are numbered so the proposal

00:07:17,880 --> 00:07:22,980
in order to do this and now I'm skipping

00:07:21,480 --> 00:07:26,220
over a couple of slides because there's

00:07:22,980 --> 00:07:28,080
a whole slide about this sort of naive

00:07:26,220 --> 00:07:29,820
solutions how would you just do just put

00:07:28,080 --> 00:07:33,060
a buffer there you read from it and then

00:07:29,820 --> 00:07:35,520
you're done it's not that simple

00:07:33,060 --> 00:07:38,370
so the point is basically to just cut to

00:07:35,520 --> 00:07:40,350
the chase data duplication is required

00:07:38,370 --> 00:07:42,540
we cannot have one buffer with a trade

00:07:40,350 --> 00:07:47,280
ports and and have the data only once in

00:07:42,540 --> 00:07:51,270
the buffer it but the observation here

00:07:47,280 --> 00:07:54,020
is that for streaming is that you don't

00:07:51,270 --> 00:07:56,460
need a copy of all of your buffer data

00:07:54,020 --> 00:07:58,470
so instead of having the host we're

00:07:56,460 --> 00:08:00,330
connected to a buffer and an A and in

00:07:58,470 --> 00:08:02,910
the accelerator or you accelerate a

00:08:00,330 --> 00:08:05,160
function we split this buffer in sort of

00:08:02,910 --> 00:08:08,310
two levels so there's a level one and an

00:08:05,160 --> 00:08:10,350
l1 and l2 and in each level different

00:08:08,310 --> 00:08:15,419
memory primitives inside of the FPGA are

00:08:10,350 --> 00:08:18,150
exploited so the results of this after

00:08:15,419 --> 00:08:20,880
after implementation is that you run

00:08:18,150 --> 00:08:23,880
synthesis and this table sort of

00:08:20,880 --> 00:08:25,560
summarizes various configurations where

00:08:23,880 --> 00:08:26,820
n stands for the number of streams that

00:08:25,560 --> 00:08:29,460
we have in host memory that we're trying

00:08:26,820 --> 00:08:31,140
to buffer so that would be for example

00:08:29,460 --> 00:08:33,169
for the first one 32 streams and P

00:08:31,140 --> 00:08:35,909
stands for the number of read ports

00:08:33,169 --> 00:08:38,159
target frequency is 200 megahertz and

00:08:35,909 --> 00:08:40,909
some parts are double pumped so we're

00:08:38,159 --> 00:08:43,530
also have parts at 400 megahertz and

00:08:40,909 --> 00:08:47,190
yeah this looks pretty good you know we

00:08:43,530 --> 00:08:49,620
we passed timing but this is synthesis

00:08:47,190 --> 00:08:53,850
and another point that I want to make is

00:08:49,620 --> 00:08:55,410
that with these new this new sort of

00:08:53,850 --> 00:09:00,420
of magnitude more bandwidth that we want

00:08:55,410 --> 00:09:02,190
to keep up with in our FPGA the results

00:09:00,420 --> 00:09:10,440
after implementation are very very

00:09:02,190 --> 00:09:14,610
different sorry sorry that is now

00:09:10,440 --> 00:09:16,560
suddenly - one nanosecond so before

00:09:14,610 --> 00:09:18,769
everything was roughly you know

00:09:16,560 --> 00:09:21,569
everything was passing in in synthesis

00:09:18,769 --> 00:09:23,930
and now in implementation we suddenly

00:09:21,569 --> 00:09:27,029
see that it barely passes timing and

00:09:23,930 --> 00:09:29,220
this is mostly due to all of the memory

00:09:27,029 --> 00:09:32,040
resources that were that we're trying to

00:09:29,220 --> 00:09:34,560
use and also getting the data out of the

00:09:32,040 --> 00:09:38,579
memory primitives at frequencies of 400

00:09:34,560 --> 00:09:40,380
megahertz so the conclusions of this

00:09:38,579 --> 00:09:43,529
work are that an increase in IO

00:09:40,380 --> 00:09:44,850
bandwidth impacts FPGA design and the

00:09:43,529 --> 00:09:46,050
traditional solutions which I will

00:09:44,850 --> 00:09:49,649
present to you tomorrow

00:09:46,050 --> 00:09:51,540
are insufficient so we need to think of

00:09:49,649 --> 00:09:53,579
new ways and in this case this was to

00:09:51,540 --> 00:09:57,269
split our design of a sort of a simple

00:09:53,579 --> 00:10:00,680
buffer and exploit different memory

00:09:57,269 --> 00:10:04,860
primitives and and use them maybe in an

00:10:00,680 --> 00:10:06,839
unconventional way so this this

00:10:04,860 --> 00:10:10,410
configurable architecture operates at

00:10:06,839 --> 00:10:13,170
200 megahertz some parts at 400 and the

00:10:10,410 --> 00:10:15,480
bottom line is feeding FPGAs is not as

00:10:13,170 --> 00:10:16,680
obvious as it seems thank you for your

00:10:15,480 --> 00:10:20,940
time

00:10:16,680 --> 00:10:20,940

YouTube URL: https://www.youtube.com/watch?v=R_uwgnm15WE


