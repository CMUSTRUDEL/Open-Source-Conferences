Title: OpenPOWER Summit US 2018: AI Demystified
Publication date: 2018-04-06
Playlist: OpenPOWER Summit US 2018
Description: 
	Ajay Arasanipalai and Sanjit Sudarsan discuss neural networks and deep learning with IBM Power and Tensorflow at the OpenPOWER Summit 2018. 

For more information please visit www.openpowerfoundation.org.
Captions: 
	00:00:00,000 --> 00:00:04,820
um good morning everyone can I start

00:00:05,000 --> 00:00:09,630
sure

00:00:06,150 --> 00:00:11,880
okay so good morning everyone I'm RJ and

00:00:09,630 --> 00:00:13,710
this is Sanjit and today we'd like to

00:00:11,880 --> 00:00:16,080
talk a little bit about artificial

00:00:13,710 --> 00:00:17,090
intelligence and deep learning it's a

00:00:16,080 --> 00:00:19,800
little bit of background information

00:00:17,090 --> 00:00:21,480
both of us the students of a Chelsea

00:00:19,800 --> 00:00:25,350
international school and we are

00:00:21,480 --> 00:00:27,750
currently in 11th grade we will mostly

00:00:25,350 --> 00:00:30,060
do deep learning in AI projects for fun

00:00:27,750 --> 00:00:32,070
and we'd like to share our experiences

00:00:30,060 --> 00:00:34,110
as well as a little bit of information

00:00:32,070 --> 00:00:37,230
on what exactly a AI is and the state of

00:00:34,110 --> 00:00:40,770
things today so artificial intelligence

00:00:37,230 --> 00:00:44,219
is a very hyped topic right now but

00:00:40,770 --> 00:00:46,980
honestly cutting through the hype there

00:00:44,219 --> 00:00:50,640
is pretty much a very specific set of

00:00:46,980 --> 00:00:52,649
tasks that AI is meant to solve so this

00:00:50,640 --> 00:00:54,989
is just a broad overview to make sure

00:00:52,649 --> 00:00:56,820
we're all on the same page artificial

00:00:54,989 --> 00:00:59,550
intelligence is sort of like this

00:00:56,820 --> 00:01:01,890
broader category or we'll slowly come to

00:00:59,550 --> 00:01:03,600
define these terms a little later on in

00:01:01,890 --> 00:01:05,100
the talk but within artificial

00:01:03,600 --> 00:01:07,580
intelligence we have this field called

00:01:05,100 --> 00:01:11,729
machine learning which is essentially

00:01:07,580 --> 00:01:14,400
using data and running albums to run

00:01:11,729 --> 00:01:17,159
inference and predictive models on this

00:01:14,400 --> 00:01:19,770
data and there's also a very specific

00:01:17,159 --> 00:01:21,420
set of algorithms within this field

00:01:19,770 --> 00:01:23,970
machine learning called the neural

00:01:21,420 --> 00:01:25,290
network and the neural network has seen

00:01:23,970 --> 00:01:27,450
tremendous improvements in performance

00:01:25,290 --> 00:01:29,520
in the recent years and this pretty much

00:01:27,450 --> 00:01:31,500
created this new field called deep

00:01:29,520 --> 00:01:32,880
learning and the named deep learning

00:01:31,500 --> 00:01:37,650
comes from the fact that you're using

00:01:32,880 --> 00:01:41,070
deep neural networks okay

00:01:37,650 --> 00:01:43,920
so most people think that they know AI

00:01:41,070 --> 00:01:45,479
but before we can really understand and

00:01:43,920 --> 00:01:48,509
appreciate AI we need know the

00:01:45,479 --> 00:01:50,399
difference between automation and AI so

00:01:48,509 --> 00:01:52,170
automation can be defined as the

00:01:50,399 --> 00:01:54,509
technology by which a process or

00:01:52,170 --> 00:01:57,750
procedure is performed without human

00:01:54,509 --> 00:02:01,979
assistance in the case of automation it

00:01:57,750 --> 00:02:04,049
is if X then Y but when it comes to AI

00:02:01,979 --> 00:02:05,340
it is intelligence demonstrated by

00:02:04,049 --> 00:02:07,380
machines

00:02:05,340 --> 00:02:12,310
the main its main ability is it can

00:02:07,380 --> 00:02:15,310
mimic what a human does or things and

00:02:12,310 --> 00:02:17,560
popular example would be the maze when

00:02:15,310 --> 00:02:18,940
you feed when he give a maze to an

00:02:17,560 --> 00:02:22,600
automated machine

00:02:18,940 --> 00:02:24,730
you usually instructor feed the solution

00:02:22,600 --> 00:02:27,100
to it and this will enable the automatic

00:02:24,730 --> 00:02:29,560
machine to solve the same is over and

00:02:27,100 --> 00:02:32,410
over again and number of times but when

00:02:29,560 --> 00:02:34,420
you give it a new Miss it will use the

00:02:32,410 --> 00:02:36,340
same solution given to it for the

00:02:34,420 --> 00:02:38,110
previous maze and it will not really

00:02:36,340 --> 00:02:41,050
solve the new maze it just follows the

00:02:38,110 --> 00:02:44,230
instructions given to it but when in an

00:02:41,050 --> 00:02:46,770
AI system if you feel it one solution it

00:02:44,230 --> 00:02:50,860
will think from then on and solve

00:02:46,770 --> 00:02:53,800
multiple different mazes and over and

00:02:50,860 --> 00:02:57,160
over again and again it can keep going

00:02:53,800 --> 00:02:59,560
it can keep doing it forever with the

00:02:57,160 --> 00:03:02,590
introduction of AI different types of

00:02:59,560 --> 00:03:05,500
learning have been made easier the most

00:03:02,590 --> 00:03:06,640
important or popular being supervised

00:03:05,500 --> 00:03:09,760
learning

00:03:06,640 --> 00:03:12,040
simply put for example you have a 5 year

00:03:09,760 --> 00:03:15,489
old kid who wants to know the difference

00:03:12,040 --> 00:03:17,500
between a cat and a dog so in order for

00:03:15,489 --> 00:03:21,100
you to train the kid you take 500 images

00:03:17,500 --> 00:03:23,560
of cats and dogs and for every image you

00:03:21,100 --> 00:03:26,170
go this is a cat and this is a dog

00:03:23,560 --> 00:03:29,769
simple as that so after you train the

00:03:26,170 --> 00:03:32,560
kid the kid will start to think on its

00:03:29,769 --> 00:03:36,670
own and you'll be able to identify what

00:03:32,560 --> 00:03:38,500
a cat is and what a dog is next is

00:03:36,670 --> 00:03:41,019
unsupervised learning unsupervised

00:03:38,500 --> 00:03:42,910
learning is if you take the same kid put

00:03:41,019 --> 00:03:45,910
him in a room all by himself and those

00:03:42,910 --> 00:03:48,220
500 images after a point of time once he

00:03:45,910 --> 00:03:50,470
goes through those 500 images he starts

00:03:48,220 --> 00:03:53,260
to figure out the pattern between those

00:03:50,470 --> 00:03:55,180
between between cats and dogs many

00:03:53,260 --> 00:03:57,610
starts to categorize and cluster them

00:03:55,180 --> 00:04:00,340
into Poulet and pulley he may not

00:03:57,610 --> 00:04:03,400
necessarily know that Poulet is cats and

00:04:00,340 --> 00:04:07,750
pull be his dogs but he will be able to

00:04:03,400 --> 00:04:10,450
identify similar trends in images so

00:04:07,750 --> 00:04:12,700
next is reinforcement learning this is

00:04:10,450 --> 00:04:14,620
when you take an AI agent and put it in

00:04:12,700 --> 00:04:17,709
an environment and you give it an

00:04:14,620 --> 00:04:21,940
objective in this case it's a lunar

00:04:17,709 --> 00:04:25,020
rover and its objective is to land the

00:04:21,940 --> 00:04:28,620
lunar rover between those two flag poles

00:04:25,020 --> 00:04:31,139
and it learns it learns from its

00:04:28,620 --> 00:04:33,180
failures basically what it does it tries

00:04:31,139 --> 00:04:36,509
over and over again and finally it

00:04:33,180 --> 00:04:38,490
figures out an optimal way to land the

00:04:36,509 --> 00:04:41,520
lunar rover between those two flag poles

00:04:38,490 --> 00:04:43,050
using minimum fluency so for the rest of

00:04:41,520 --> 00:04:46,139
the stock were mostly going to be

00:04:43,050 --> 00:04:48,090
focusing on supervised learning as this

00:04:46,139 --> 00:04:51,870
is where a majority of the economical

00:04:48,090 --> 00:04:54,360
value lies today essentially supervised

00:04:51,870 --> 00:04:58,440
learning is nothing more than learning a

00:04:54,360 --> 00:05:00,990
to be mappings so for example a very old

00:04:58,440 --> 00:05:05,430
machine learning algorithm is given an

00:05:00,990 --> 00:05:08,099
email a can you classify it as hot spam

00:05:05,430 --> 00:05:10,409
and a to be mapping another example

00:05:08,099 --> 00:05:12,389
would be when you're given a piece of

00:05:10,409 --> 00:05:13,409
audio and you're asked to transcribe it

00:05:12,389 --> 00:05:15,120
this is also known as a speech

00:05:13,409 --> 00:05:17,039
recognition but in the end it's still an

00:05:15,120 --> 00:05:20,370
A to B mapping you're mapping audio onto

00:05:17,039 --> 00:05:22,620
a text transcript another example would

00:05:20,370 --> 00:05:25,440
be machine translation where you're

00:05:22,620 --> 00:05:27,210
given a series of English words or

00:05:25,440 --> 00:05:29,759
English sentences and asked to translate

00:05:27,210 --> 00:05:32,009
them into french again an A to B mapping

00:05:29,759 --> 00:05:34,469
but this very simple act of map of

00:05:32,009 --> 00:05:38,849
learning the mapping between a and B has

00:05:34,469 --> 00:05:40,279
provided immense value so why do we need

00:05:38,849 --> 00:05:43,050
deep learning in the first place

00:05:40,279 --> 00:05:44,639
well these the algorithms which were

00:05:43,050 --> 00:05:46,319
going to be talking about today like

00:05:44,639 --> 00:05:49,259
neural networks aren't actually quite

00:05:46,319 --> 00:05:51,330
new they're over 20 years old however

00:05:49,259 --> 00:05:53,699
back in the day we did not have enough

00:05:51,330 --> 00:05:55,889
processing power and enough data to make

00:05:53,699 --> 00:05:58,440
these algorithms run efficiently and

00:05:55,889 --> 00:05:59,969
deliver high performance so if you take

00:05:58,440 --> 00:06:01,830
a look at some of the older machine

00:05:59,969 --> 00:06:04,199
learning the traditional machine

00:06:01,830 --> 00:06:07,050
learning algorithms there became a

00:06:04,199 --> 00:06:08,789
problem as the IT revolution began and

00:06:07,050 --> 00:06:11,250
we start getting more and more data from

00:06:08,789 --> 00:06:12,870
all across the globe the performance of

00:06:11,250 --> 00:06:16,349
these old machine learning algorithms

00:06:12,870 --> 00:06:17,430
did not scale as well with the amount of

00:06:16,349 --> 00:06:21,210
data

00:06:17,430 --> 00:06:23,909
however with deep learning we now have a

00:06:21,210 --> 00:06:26,370
greater performance with larger data

00:06:23,909 --> 00:06:28,110
sets which is allowed us to achieve

00:06:26,370 --> 00:06:33,000
incredible accuracy sometimes even

00:06:28,110 --> 00:06:36,210
surpassing human level performance ok so

00:06:33,000 --> 00:06:38,430
finally deep neutral networks this is

00:06:36,210 --> 00:06:40,410
the main algorithm used in

00:06:38,430 --> 00:06:42,060
deep learning today so we're not going

00:06:40,410 --> 00:06:44,850
to go too much into detail on how

00:06:42,060 --> 00:06:47,280
exactly a neural network works but the

00:06:44,850 --> 00:06:50,150
idea is that this algorithm which is

00:06:47,280 --> 00:06:53,130
represented by the diagram tear left

00:06:50,150 --> 00:06:55,340
learns the mapping and learns the

00:06:53,130 --> 00:06:58,170
representation of complex datasets and

00:06:55,340 --> 00:07:00,570
this algorithm the neural network has

00:06:58,170 --> 00:07:03,270
seen large improvements recent years

00:07:00,570 --> 00:07:07,230
thanks to the advent of big data and

00:07:03,270 --> 00:07:09,060
GPUs so the main differentiator between

00:07:07,230 --> 00:07:10,560
deep learning algorithm and a machine

00:07:09,060 --> 00:07:12,960
learning algorithm would be the number

00:07:10,560 --> 00:07:14,790
of hidden layers so hidden layers

00:07:12,960 --> 00:07:18,720
essentially is increasing the complexity

00:07:14,790 --> 00:07:22,770
of the algorithm to learn more complex

00:07:18,720 --> 00:07:25,170
features from data so now we're going to

00:07:22,770 --> 00:07:26,450
discuss about another improvement in the

00:07:25,170 --> 00:07:29,730
traditional neural network architecture

00:07:26,450 --> 00:07:30,870
called convolutional neural networks and

00:07:29,730 --> 00:07:32,490
we don't need to go too much into the

00:07:30,870 --> 00:07:36,090
theory of convolutional neural networks

00:07:32,490 --> 00:07:38,130
but the idea is that when we're trying

00:07:36,090 --> 00:07:40,740
to create a neural network that's able

00:07:38,130 --> 00:07:43,410
to analyze images we need a new

00:07:40,740 --> 00:07:46,500
technique so let's say we want to create

00:07:43,410 --> 00:07:49,170
a classifier to classify images as cats

00:07:46,500 --> 00:07:51,690
or dogs so an intuitive approach would

00:07:49,170 --> 00:07:54,330
be defeating the individual pixel values

00:07:51,690 --> 00:07:56,970
of an image into a neural network and to

00:07:54,330 --> 00:07:58,140
predict a class label however this turns

00:07:56,970 --> 00:08:00,720
out to be extremely computationally

00:07:58,140 --> 00:08:03,720
expensive and it's also not how our

00:08:00,720 --> 00:08:05,670
human visual cortex works when we see

00:08:03,720 --> 00:08:08,280
images we don't seen see them as

00:08:05,670 --> 00:08:12,360
individual pixel values we first see

00:08:08,280 --> 00:08:14,460
basic elementary data points like a

00:08:12,360 --> 00:08:16,860
vertical line or a horizontal line our

00:08:14,460 --> 00:08:19,410
brain then puts these together to form

00:08:16,860 --> 00:08:22,830
elementary shapes like squares circles

00:08:19,410 --> 00:08:24,780
and triangles later layers of neurons in

00:08:22,830 --> 00:08:26,850
our brain actually then puts these

00:08:24,780 --> 00:08:29,580
elementary shapes together to form

00:08:26,850 --> 00:08:32,790
elementary features like an eye or nose

00:08:29,580 --> 00:08:35,550
etc and finally we put these multiple

00:08:32,790 --> 00:08:38,850
eyes and noses together to identify that

00:08:35,550 --> 00:08:40,530
this is a person's face and this is what

00:08:38,850 --> 00:08:42,810
we are essentially trying to recreate

00:08:40,530 --> 00:08:44,250
with convolutional neural networks of

00:08:42,810 --> 00:08:46,260
course these convolutional neural

00:08:44,250 --> 00:08:48,800
networks need to be more formally

00:08:46,260 --> 00:08:51,370
defined so this is a quick overview of

00:08:48,800 --> 00:08:52,870
you know how a convolutional

00:08:51,370 --> 00:08:54,970
you'll never work in particular we're

00:08:52,870 --> 00:08:57,279
going to be focusing on the convolution

00:08:54,970 --> 00:08:59,200
operator so we have our image over here

00:08:57,279 --> 00:09:01,330
which is represented by this green

00:08:59,200 --> 00:09:03,279
matrix and yeah the represent in a

00:09:01,330 --> 00:09:06,160
representation of an image is a matrix

00:09:03,279 --> 00:09:08,200
so we take a smaller matrix that smaller

00:09:06,160 --> 00:09:10,870
than the image size this matrix is

00:09:08,200 --> 00:09:12,160
called a filter and we slide it over the

00:09:10,870 --> 00:09:14,380
larger image so it's kind of like taking

00:09:12,160 --> 00:09:16,510
a flashlight and sliding it over your

00:09:14,380 --> 00:09:17,620
entire image and at each step you're

00:09:16,510 --> 00:09:20,620
performing an element-wise

00:09:17,620 --> 00:09:22,630
multiplication of the entry in the

00:09:20,620 --> 00:09:24,850
filter matrix and in the image we then

00:09:22,630 --> 00:09:26,950
sum all these up together to create a

00:09:24,850 --> 00:09:30,150
new matrix called the convolved feature

00:09:26,950 --> 00:09:32,710
this matrix holds some information on

00:09:30,150 --> 00:09:35,200
more complex features than individual

00:09:32,710 --> 00:09:37,450
pixel values for example this particular

00:09:35,200 --> 00:09:39,400
convolved feature might correspond to a

00:09:37,450 --> 00:09:41,110
vertical line in the image so this sort

00:09:39,400 --> 00:09:44,730
of gives us a sense of how many vertical

00:09:41,110 --> 00:09:44,730
lines are present in the image

00:09:50,430 --> 00:09:54,400
tensorflow is an open source deep

00:09:52,660 --> 00:09:56,770
learning framework used for numerical

00:09:54,400 --> 00:09:58,779
computation it allows developers to

00:09:56,770 --> 00:10:04,270
quickly build and deploy deep learning

00:09:58,779 --> 00:10:07,029
models such as the neural networks so

00:10:04,270 --> 00:10:10,690
tensorflow was created by Google and it

00:10:07,029 --> 00:10:12,970
was open sourced in 2015 and since then

00:10:10,690 --> 00:10:14,680
it's had a rapid growing community

00:10:12,970 --> 00:10:17,820
there's also a lot of online support

00:10:14,680 --> 00:10:20,380
over now we'll now move on to actually

00:10:17,820 --> 00:10:22,930
implementing a convolutional neural

00:10:20,380 --> 00:10:24,670
network to identify handwritten digits

00:10:22,930 --> 00:10:26,890
and we're not we're going to do this not

00:10:24,670 --> 00:10:31,260
only using tensorflow but using another

00:10:26,890 --> 00:10:31,260
important software stack called power AI

00:10:31,290 --> 00:10:35,380
okay so a little bit of background so

00:10:33,640 --> 00:10:37,240
power AI is a software stack that

00:10:35,380 --> 00:10:38,830
contains all the popular or deep

00:10:37,240 --> 00:10:40,120
learning frameworks such as

00:10:38,830 --> 00:10:44,650
tensor flow and they're also a few other

00:10:40,120 --> 00:10:47,890
ones like cafe piano chain ER etc and

00:10:44,650 --> 00:10:50,410
Pao area essentially encapsulate all of

00:10:47,890 --> 00:10:53,709
these frameworks and optimizes them to

00:10:50,410 --> 00:10:56,080
run on IBM power systems so the example

00:10:53,709 --> 00:10:58,480
we have over here is it's a jupiter

00:10:56,080 --> 00:10:59,200
notebook and it's so it's a common

00:10:58,480 --> 00:11:01,209
tensorflow

00:10:59,200 --> 00:11:02,920
example that we pulled off of github and

00:11:01,209 --> 00:11:04,750
what we're doing over here is

00:11:02,920 --> 00:11:07,600
essentially training a convolution

00:11:04,750 --> 00:11:09,400
neural network to identify handwritten

00:11:07,600 --> 00:11:12,160
digits some you might be familiar with

00:11:09,400 --> 00:11:13,390
the amnesty data set yeah we're

00:11:12,160 --> 00:11:15,730
essentially training a convolutional

00:11:13,390 --> 00:11:17,710
neural network on that but this isn't

00:11:15,730 --> 00:11:18,850
any like you know groundbreaking new

00:11:17,710 --> 00:11:20,530
architecture what we wanted to

00:11:18,850 --> 00:11:24,460
demonstrate with this particular

00:11:20,530 --> 00:11:26,950
notebook is how efficient fast and easy

00:11:24,460 --> 00:11:29,590
it is to get started running a deep deep

00:11:26,950 --> 00:11:33,660
learning algorithms on IBM's power AI

00:11:29,590 --> 00:11:39,100
platform yeah and as you can see

00:11:33,660 --> 00:11:41,110
probably in the next vid over here we

00:11:39,100 --> 00:11:44,620
actually you can see how fast our

00:11:41,110 --> 00:11:45,880
algorithm is able to Train so the the

00:11:44,620 --> 00:11:49,120
you know the most computationally

00:11:45,880 --> 00:11:52,600
expensive and time demanding you know

00:11:49,120 --> 00:11:54,220
step of any deep learning process is the

00:11:52,600 --> 00:11:56,290
training step where you are iteratively

00:11:54,220 --> 00:11:58,630
going over your data set and trying to

00:11:56,290 --> 00:12:01,570
learn from it this step is done

00:11:58,630 --> 00:12:07,360
incredibly fast with IBM's power AI

00:12:01,570 --> 00:12:11,010
system and oh yeah if we wait for a

00:12:07,360 --> 00:12:11,010
minute we'll also see the outputs

00:12:20,339 --> 00:12:26,519
oh yeah and yeah just for those of you

00:12:23,490 --> 00:12:28,499
who are on a little bit more about this

00:12:26,519 --> 00:12:30,870
particular demonstration oh yeah anyways

00:12:28,499 --> 00:12:33,089
there we go so we have something called

00:12:30,870 --> 00:12:36,360
the loss function which measures the

00:12:33,089 --> 00:12:37,860
error on our particular neural network

00:12:36,360 --> 00:12:39,660
and over here you can see the loss is

00:12:37,860 --> 00:12:41,639
going down is this essentially this

00:12:39,660 --> 00:12:43,740
algorithm called the gradient descent

00:12:41,639 --> 00:12:45,959
which is used to reduce the error of

00:12:43,740 --> 00:12:47,939
your neural networks and finally we

00:12:45,959 --> 00:12:51,120
evaluate a new londo and see that we

00:12:47,939 --> 00:12:52,980
have a 98% accuracy this is quite common

00:12:51,120 --> 00:12:54,800
the MS data set it's a solved problem

00:12:52,980 --> 00:12:58,470
and it's quite easy

00:12:54,800 --> 00:13:00,959
yeah and over here this last bit over

00:12:58,470 --> 00:13:02,790
here is executing a little bit on the

00:13:00,959 --> 00:13:04,920
test data set and you can see we have

00:13:02,790 --> 00:13:06,930
the images over here and our model is

00:13:04,920 --> 00:13:09,600
able to correctly classify these

00:13:06,930 --> 00:13:11,459
particular images as 100 in digits for

00:13:09,600 --> 00:13:12,839
example you can see over here there's a

00:13:11,459 --> 00:13:18,720
picture of a zero and the model predicts

00:13:12,839 --> 00:13:21,449
zero so yeah that was a little bit on

00:13:18,720 --> 00:13:23,790
power AI and convolutional neural

00:13:21,449 --> 00:13:26,089
networks but deep learning is much more

00:13:23,790 --> 00:13:28,259
than that we have very interesting

00:13:26,089 --> 00:13:30,540
algorithms and research being done right

00:13:28,259 --> 00:13:31,829
now in deep learning particularly

00:13:30,540 --> 00:13:33,420
interesting example that I found

00:13:31,829 --> 00:13:35,249
recently was the deep fix

00:13:33,420 --> 00:13:39,120
I'll give them what this algorithm does

00:13:35,249 --> 00:13:41,939
is it swaps out a person's face from a

00:13:39,120 --> 00:13:44,179
video with another person's face so this

00:13:41,939 --> 00:13:47,189
is commonly used like right now in

00:13:44,179 --> 00:13:50,040
applications where let's say swapping

00:13:47,189 --> 00:13:51,870
out an actor a face of an actor in a

00:13:50,040 --> 00:13:53,610
movie with your favorite actor so people

00:13:51,870 --> 00:13:56,910
have done this algorithm and they've

00:13:53,610 --> 00:13:58,949
gotten quite reasonable results and you

00:13:56,910 --> 00:14:01,529
can find many more examples online of

00:13:58,949 --> 00:14:03,300
the deep fix algorithm but one

00:14:01,529 --> 00:14:05,279
particularly lucrative application that

00:14:03,300 --> 00:14:07,709
I found for this was in computer

00:14:05,279 --> 00:14:10,350
graphics so over here we have a few

00:14:07,709 --> 00:14:13,920
images from a game called FIFA 18 which

00:14:10,350 --> 00:14:15,420
is a soccer game and over here so so

00:14:13,920 --> 00:14:18,240
what we're doing over here is swapping

00:14:15,420 --> 00:14:20,279
between the two faces cycling before so

00:14:18,240 --> 00:14:22,769
the first the image that looks less

00:14:20,279 --> 00:14:25,259
professional is actually the

00:14:22,769 --> 00:14:27,899
computer-generated imagery that's used

00:14:25,259 --> 00:14:30,480
in this particular game but what the

00:14:27,899 --> 00:14:33,540
author of a blog post that I read

00:14:30,480 --> 00:14:34,000
recently what he did was he used the

00:14:33,540 --> 00:14:36,220
deep

00:14:34,000 --> 00:14:39,790
all exam and he trained it on real

00:14:36,220 --> 00:14:41,800
images of these particular soccer

00:14:39,790 --> 00:14:44,230
players and what he was able to do is

00:14:41,800 --> 00:14:46,120
actually considerably improve the

00:14:44,230 --> 00:14:48,400
quality of the computer-generated

00:14:46,120 --> 00:14:50,800
graphics I find this particularly

00:14:48,400 --> 00:14:53,500
interesting application of this

00:14:50,800 --> 00:14:56,380
particular algorithm as it can provide

00:14:53,500 --> 00:15:00,850
huge improvements in animation and game

00:14:56,380 --> 00:15:03,190
design industries another interesting

00:15:00,850 --> 00:15:06,850
research paper that was done recently

00:15:03,190 --> 00:15:08,740
was the dense force algorithm which was

00:15:06,850 --> 00:15:12,250
created by a team of researchers within

00:15:08,740 --> 00:15:13,840
the Facebook AI research team and it's

00:15:12,250 --> 00:15:15,910
quite self explanatory through the video

00:15:13,840 --> 00:15:18,370
but was what you're essentially doing is

00:15:15,910 --> 00:15:22,330
mapping out the individual pixel values

00:15:18,370 --> 00:15:24,850
of a human in a particular video and you

00:15:22,330 --> 00:15:27,430
create a UV surface coordinates of these

00:15:24,850 --> 00:15:28,900
people and what you can also do is once

00:15:27,430 --> 00:15:32,440
you've traded the mapping of the human

00:15:28,900 --> 00:15:34,120
body based on a pure video you can

00:15:32,440 --> 00:15:37,210
actually swap these out for interesting

00:15:34,120 --> 00:15:40,240
that you could probably add a you know a

00:15:37,210 --> 00:15:41,980
hat or something this is yeah and this

00:15:40,240 --> 00:15:44,320
is a little bit more the details on how

00:15:41,980 --> 00:15:47,460
exactly the dense power system works but

00:15:44,320 --> 00:15:47,460
we're not going to focus on that today

00:15:48,990 --> 00:15:55,510
so what's next for AI and deep learning

00:15:52,560 --> 00:15:58,390
so as of now many people believe that

00:15:55,510 --> 00:16:00,790
computer vision is a solved problem this

00:15:58,390 --> 00:16:03,250
is because computer vision involves

00:16:00,790 --> 00:16:05,380
static data the images are of a fixed

00:16:03,250 --> 00:16:07,180
size in most cases and even if the

00:16:05,380 --> 00:16:10,210
images are not of a fixed size we can

00:16:07,180 --> 00:16:12,580
pad the images and make every image out

00:16:10,210 --> 00:16:17,290
there of a standard format a standard

00:16:12,580 --> 00:16:19,660
size of a matrix and currently we have

00:16:17,290 --> 00:16:21,670
incredible as algorithms out there for

00:16:19,660 --> 00:16:23,950
computer vision and we've seen a greater

00:16:21,670 --> 00:16:26,860
than human level performance on some of

00:16:23,950 --> 00:16:29,740
these image data sets an example would

00:16:26,860 --> 00:16:33,100
be your image net where essentially is a

00:16:29,740 --> 00:16:34,690
group of a lot of images encapsulating a

00:16:33,100 --> 00:16:38,170
thousand different categories like

00:16:34,690 --> 00:16:39,790
airplanes buses ships etc and we've

00:16:38,170 --> 00:16:42,010
achieved a greater than human level

00:16:39,790 --> 00:16:44,290
performance using deep learning on this

00:16:42,010 --> 00:16:46,900
particular data set so this is all of

00:16:44,290 --> 00:16:47,710
research being done now on sequence

00:16:46,900 --> 00:16:49,870
models

00:16:47,710 --> 00:16:52,600
particularly sequence to sequence models

00:16:49,870 --> 00:16:54,640
and generative networks so so far we've

00:16:52,600 --> 00:16:59,080
talked about how deep learning can be

00:16:54,640 --> 00:17:00,910
used to you know scan images or you know

00:16:59,080 --> 00:17:02,590
go through data and perform regression

00:17:00,910 --> 00:17:04,480
and classification all that kind of

00:17:02,590 --> 00:17:06,010
stuff but deep learning is much more

00:17:04,480 --> 00:17:08,320
than that people have also created

00:17:06,010 --> 00:17:12,520
algorithms that's able to generate new

00:17:08,320 --> 00:17:15,339
data not only analyze it and the goal is

00:17:12,520 --> 00:17:17,170
eventually to come to a stage where all

00:17:15,339 --> 00:17:20,020
of these tasks speech recognition image

00:17:17,170 --> 00:17:23,170
classification machine translation all

00:17:20,020 --> 00:17:25,240
of these tasks should be using the help

00:17:23,170 --> 00:17:26,830
of deep learning should be able to be

00:17:25,240 --> 00:17:31,420
optimized to a state where they're able

00:17:26,830 --> 00:17:34,330
to surpass human level performance so in

00:17:31,420 --> 00:17:36,460
a recent experiment conducted a group of

00:17:34,330 --> 00:17:39,730
researchers wanted to train an AI

00:17:36,460 --> 00:17:43,120
machine system to classify or identify

00:17:39,730 --> 00:17:44,890
between dogs and wolves and as they did

00:17:43,120 --> 00:17:47,200
and they were when they were testing the

00:17:44,890 --> 00:17:49,750
machine when they gave the Machine a

00:17:47,200 --> 00:17:52,480
picture of a dog the system classified

00:17:49,750 --> 00:17:54,340
it as though as a wolf the researchers

00:17:52,480 --> 00:17:56,080
were baffled they didn't know what what

00:17:54,340 --> 00:17:59,650
happened what went wrong so they dug

00:17:56,080 --> 00:18:01,570
deep and when they went to really they

00:17:59,650 --> 00:18:04,480
went when they identified the problem it

00:18:01,570 --> 00:18:06,370
was nothing but there was biased when

00:18:04,480 --> 00:18:09,280
they when they fed the data said to the

00:18:06,370 --> 00:18:12,280
system every image of a wolf had snow in

00:18:09,280 --> 00:18:15,100
the background so the air system matched

00:18:12,280 --> 00:18:18,300
the presence of snow to the presence of

00:18:15,100 --> 00:18:21,730
the animal in the image being a wolf so

00:18:18,300 --> 00:18:23,020
what this essentially shows us while

00:18:21,730 --> 00:18:24,970
we're able to create interesting

00:18:23,020 --> 00:18:28,300
algorithms that are able to very

00:18:24,970 --> 00:18:30,220
efficiently scan through data in the end

00:18:28,300 --> 00:18:34,000
with deep learning we still don't know

00:18:30,220 --> 00:18:35,950
why exactly these algorithms work so

00:18:34,000 --> 00:18:37,360
there's also a lot of you know recent

00:18:35,950 --> 00:18:40,330
research being done on data

00:18:37,360 --> 00:18:42,790
visualization and to visualize somehow

00:18:40,330 --> 00:18:45,670
what deep neural networks are actually

00:18:42,790 --> 00:18:47,140
doing when we are training them so that

00:18:45,670 --> 00:18:48,820
we can better understand what makes

00:18:47,140 --> 00:18:52,510
these algorithms tick

00:18:48,820 --> 00:18:54,670
so another particularly an area of

00:18:52,510 --> 00:18:55,360
concern would be whether AI is safe

00:18:54,670 --> 00:18:58,390
right now

00:18:55,360 --> 00:18:59,610
so many people like Elon Musk have

00:18:58,390 --> 00:19:01,920
publicly said that

00:18:59,610 --> 00:19:04,679
is quite dangerous and minton we need to

00:19:01,920 --> 00:19:07,559
be selective in how we go forward in

00:19:04,679 --> 00:19:10,650
developing them an example of that would

00:19:07,559 --> 00:19:12,990
be for example this dog a wolf class by

00:19:10,650 --> 00:19:15,120
this may seem like a small or a trivial

00:19:12,990 --> 00:19:17,280
example but once deep learning gets

00:19:15,120 --> 00:19:19,380
implemented at a large scale and is

00:19:17,280 --> 00:19:21,210
increasingly used for more and more

00:19:19,380 --> 00:19:23,880
important tasks this would be an

00:19:21,210 --> 00:19:26,160
interesting thing to consider whether AI

00:19:23,880 --> 00:19:28,830
whether the AI we're developing is safe

00:19:26,160 --> 00:19:30,360
or not so anyways these are the few

00:19:28,830 --> 00:19:39,260
things we wanted to share with you today

00:19:30,360 --> 00:19:39,260
and thank you are there any questions

00:19:43,669 --> 00:19:47,669
either anything regarding regarding

00:19:46,620 --> 00:19:49,590
anything that we talked about or

00:19:47,669 --> 00:19:51,740
probably something else relate to deep

00:19:49,590 --> 00:19:51,740
learning

00:20:14,460 --> 00:20:20,440
and this is regarding the convolution of

00:20:17,380 --> 00:20:23,830
the amnesty example right okay so the

00:20:20,440 --> 00:20:26,590
analyst example that we ran we actually

00:20:23,830 --> 00:20:29,200
got access to the Nimbus cloud service

00:20:26,590 --> 00:20:31,150
and from there we were able to launch a

00:20:29,200 --> 00:20:34,870
service which essentially gives you a

00:20:31,150 --> 00:20:37,810
Duplin what book to work on the I don't

00:20:34,870 --> 00:20:39,670
remember the specifics of which instance

00:20:37,810 --> 00:20:43,810
we created if I remember correctly was

00:20:39,670 --> 00:20:45,250
in Nvidia k80 GPU and this the demo

00:20:43,810 --> 00:20:47,650
which we showed is actually running in

00:20:45,250 --> 00:20:49,330
real time so it was in a sped up demo or

00:20:47,650 --> 00:20:51,400
anything so we actually completed the

00:20:49,330 --> 00:20:55,840
entire Jupiter notebook in less than two

00:20:51,400 --> 00:20:58,060
minutes when running it and we got the

00:20:55,840 --> 00:20:59,770
Jupiter notebook out of github so it is

00:20:58,060 --> 00:21:03,640
part of the tensor flow examples

00:20:59,770 --> 00:21:05,170
repository and the data set is the

00:21:03,640 --> 00:21:06,850
traditional amoenus data set it's a

00:21:05,170 --> 00:21:08,860
publicly available data set of

00:21:06,850 --> 00:21:11,160
handwritten digits each with a

00:21:08,860 --> 00:21:14,440
particular label on what the number is

00:21:11,160 --> 00:21:17,880
this this particular data that we used i

00:21:14,440 --> 00:21:17,880
had 60,000 examples

00:21:18,000 --> 00:21:22,030
yeah powera I was already installed as

00:21:20,590 --> 00:21:23,710
part of the jupiter room so once you

00:21:22,030 --> 00:21:25,060
launch the instance the instance comes

00:21:23,710 --> 00:21:27,700
with tensorflow

00:21:25,060 --> 00:21:30,070
and powera I installed there are also

00:21:27,700 --> 00:21:32,560
some other customizable options where

00:21:30,070 --> 00:21:34,710
you have other frameworks installed as

00:21:32,560 --> 00:21:34,710
well

00:21:48,170 --> 00:21:52,820
so we everything that we learned is

00:21:50,870 --> 00:21:55,910
purely online so watching youtube videos

00:21:52,820 --> 00:21:58,460
and reading blog posts etc but two main

00:21:55,910 --> 00:22:00,470
the domain resources would be this

00:21:58,460 --> 00:22:02,750
YouTube channel Suraj rivalz

00:22:00,470 --> 00:22:04,700
YouTube channel so that's one of my

00:22:02,750 --> 00:22:07,160
favorite resources out there for deep

00:22:04,700 --> 00:22:10,550
learning so he publishes content on a

00:22:07,160 --> 00:22:13,700
weekly basis for pretty much the latest

00:22:10,550 --> 00:22:15,980
research papers and the math of AI all

00:22:13,700 --> 00:22:18,620
that stuff and there's also another the

00:22:15,980 --> 00:22:22,660
place where I got started was the

00:22:18,620 --> 00:22:22,660
Coursera deep learning specialization

00:22:24,610 --> 00:22:39,530
any other questions so course was a four

00:22:36,350 --> 00:22:42,170
month course but I think it's possible

00:22:39,530 --> 00:22:44,330
for most people to like go through the

00:22:42,170 --> 00:22:48,320
course a little bit faster so I II took

00:22:44,330 --> 00:22:50,630
about I think three weeks to learn the

00:22:48,320 --> 00:22:55,970
basics of deep learning and yeah ran a

00:22:50,630 --> 00:22:57,230
convolutional neural network okay yeah

00:22:55,970 --> 00:22:59,329
that should be thank you

00:22:57,230 --> 00:22:59,329

YouTube URL: https://www.youtube.com/watch?v=F9Sual7gRn4


