Title: OpenPOWER Summit US 2018: Powering Deep Learning for the Enterprise
Publication date: 2018-04-06
Playlist: OpenPOWER Summit US 2018
Description: 
	Sumit Sanyal of minds.ai discusses powering deep learning for the enterprise at the OpnPOWER Summit 2018. 

For more information please visit www.openpowerfoundation.org.
Captions: 
	00:00:01,220 --> 00:00:11,130
yeah we are a small company and we do

00:00:07,280 --> 00:00:14,880
consulting for enterprises and Chile we

00:00:11,130 --> 00:00:18,510
build neural networks and train them for

00:00:14,880 --> 00:00:21,630
various applications for various large

00:00:18,510 --> 00:00:26,400
enterprises and for this particular talk

00:00:21,630 --> 00:00:28,590
we picked NLP because it really pushes

00:00:26,400 --> 00:00:30,510
the limits of computation both on the

00:00:28,590 --> 00:00:37,260
training side and on the inference side

00:00:30,510 --> 00:00:39,360
and it's it's it's a very good Minsky's

00:00:37,260 --> 00:00:42,149
work really well IDM power systems work

00:00:39,360 --> 00:00:49,410
really well for this so that's the topic

00:00:42,149 --> 00:00:54,000
we have picked so classical NLP versus

00:00:49,410 --> 00:00:55,559
deep deep learning based NLP just to

00:00:54,000 --> 00:00:58,949
point out the distinction and to

00:00:55,559 --> 00:01:01,469
motivate the motivate the reason why

00:00:58,949 --> 00:01:10,650
deep learning based NLP is so compute

00:01:01,469 --> 00:01:13,560
compute intensive the the classical NLP

00:01:10,650 --> 00:01:15,420
is typically handcrafted features so you

00:01:13,560 --> 00:01:19,530
guys have probably heard of ontology z'

00:01:15,420 --> 00:01:21,509
and you know the various grammatical

00:01:19,530 --> 00:01:23,700
structures are kind of handcrafted into

00:01:21,509 --> 00:01:28,560
features and that is used to extract

00:01:23,700 --> 00:01:30,979
meaning from from text or speech also

00:01:28,560 --> 00:01:35,490
these are key word string or rule-based

00:01:30,979 --> 00:01:38,090
systems and they rely on large set of

00:01:35,490 --> 00:01:40,350
grammar based ontology z' and heuristics

00:01:38,090 --> 00:01:42,360
so what this means is that it is very

00:01:40,350 --> 00:01:44,880
difficult to go between languages and

00:01:42,360 --> 00:01:47,040
for each language or for each particular

00:01:44,880 --> 00:01:50,369
idiom or for anything you have to now

00:01:47,040 --> 00:01:53,700
start handcrafting these these things

00:01:50,369 --> 00:01:55,530
all over again neural networks as we

00:01:53,700 --> 00:01:57,149
know they learn the features so the

00:01:55,530 --> 00:01:59,909
features are auto-generated which is

00:01:57,149 --> 00:02:02,610
very cool you don't have to handcraft

00:01:59,909 --> 00:02:03,240
these features word and sentence

00:02:02,610 --> 00:02:07,820
embeddings

00:02:03,240 --> 00:02:10,940
in other word thought vectors are

00:02:07,820 --> 00:02:13,880
I learned in an unsupervised manner so

00:02:10,940 --> 00:02:17,560
you if you train your system on a

00:02:13,880 --> 00:02:22,130
particular corpus of texts or documents

00:02:17,560 --> 00:02:23,840
then then it will pick up the features

00:02:22,130 --> 00:02:26,660
and we'll pick up embeddings and

00:02:23,840 --> 00:02:34,550
concepts and jargon and acronyms from

00:02:26,660 --> 00:02:36,890
from from that corpus and finally if you

00:02:34,550 --> 00:02:39,950
do search or extraction or queries of

00:02:36,890 --> 00:02:43,250
any kind then because of these above

00:02:39,950 --> 00:02:47,330
features you can actually extract

00:02:43,250 --> 00:02:49,190
meaning based or you can extract you can

00:02:47,330 --> 00:02:52,880
extract information based on meaning

00:02:49,190 --> 00:02:56,739
rather than just a pure keyword based or

00:02:52,880 --> 00:03:04,760
string based search so this is just

00:02:56,739 --> 00:03:07,820
introduction the basic concept of the

00:03:04,760 --> 00:03:09,920
basic concept of word embeddings or

00:03:07,820 --> 00:03:13,430
thought embeddings is illustrated in

00:03:09,920 --> 00:03:16,820
this slide and it is quite spectacular

00:03:13,430 --> 00:03:19,450
as how well this works so a thought

00:03:16,820 --> 00:03:22,700
vector is essentially a way of

00:03:19,450 --> 00:03:28,489
representing a certain sentence or a set

00:03:22,700 --> 00:03:30,500
of words as a string of numbers and a

00:03:28,489 --> 00:03:32,209
string of and as a sequence of number of

00:03:30,500 --> 00:03:37,489
sequence of numbers is a vector in an

00:03:32,209 --> 00:03:39,680
n-dimensional space and the magic is the

00:03:37,489 --> 00:03:41,959
magic that happens is illustrated this

00:03:39,680 --> 00:03:44,570
picture over here illustrates how if you

00:03:41,959 --> 00:03:46,160
if you do your unsupervised learning and

00:03:44,570 --> 00:03:50,530
if you create your thought vectors and

00:03:46,160 --> 00:03:54,110
your sentence vectors in a proper way

00:03:50,530 --> 00:03:58,579
then the embedding in in in in the

00:03:54,110 --> 00:04:01,489
latent space actually contains contains

00:03:58,579 --> 00:04:04,130
meanings contains grammar and all of

00:04:01,489 --> 00:04:05,930
that stuff is embedded in there and this

00:04:04,130 --> 00:04:08,360
is the reason why deep learning based

00:04:05,930 --> 00:04:10,160
NLP you do not have to do kind of

00:04:08,360 --> 00:04:12,860
handcrafted features or you do not have

00:04:10,160 --> 00:04:16,910
to rely on rules of grammar or ontology

00:04:12,860 --> 00:04:19,639
Zoar such and yeah the illustration is

00:04:16,910 --> 00:04:21,450
that you can see that if you look at the

00:04:19,639 --> 00:04:23,520
this is not this is

00:04:21,450 --> 00:04:26,460
this is a cartoon this is just a

00:04:23,520 --> 00:04:28,020
two-dimensional representation the

00:04:26,460 --> 00:04:30,540
actual thought vectors could be you know

00:04:28,020 --> 00:04:32,880
512 dimensions or in a very high

00:04:30,540 --> 00:04:35,070
dimensional space typically we work with

00:04:32,880 --> 00:04:37,350
512 or thousand thousand dimensional

00:04:35,070 --> 00:04:40,890
vectors but this is a caricature it

00:04:37,350 --> 00:04:45,320
illustrates the point very well that if

00:04:40,890 --> 00:04:48,570
you you can do stuff like you can put

00:04:45,320 --> 00:04:52,590
semantic concepts into an equation

00:04:48,570 --> 00:04:54,450
because you can map the word King to its

00:04:52,590 --> 00:04:55,980
thought vector representation you can

00:04:54,450 --> 00:04:58,590
each of the words you can map it there

00:04:55,980 --> 00:05:03,900
and then you can do very simple math

00:04:58,590 --> 00:05:05,820
like add and subtract and look the the

00:05:03,900 --> 00:05:10,320
addition and the subtraction in this

00:05:05,820 --> 00:05:12,060
latent space when you do that

00:05:10,320 --> 00:05:14,070
calculation on the left side you come up

00:05:12,060 --> 00:05:15,840
with a number you do a reverse lookup

00:05:14,070 --> 00:05:19,770
and you see what word that translates to

00:05:15,840 --> 00:05:24,060
and it's quite fantastic that it would

00:05:19,770 --> 00:05:26,400
come up with that right so if you stare

00:05:24,060 --> 00:05:29,160
at that you can see that for an English

00:05:26,400 --> 00:05:33,120
speaker that seems to make some kind of

00:05:29,160 --> 00:05:36,030
sense that you take man away from Kings

00:05:33,120 --> 00:05:39,690
so you you still retain the notion of

00:05:36,030 --> 00:05:41,790
royalty maybe your aura or a ruler of a

00:05:39,690 --> 00:05:44,400
kingdom and then you add that woman into

00:05:41,790 --> 00:05:46,470
it and hallelujah that's queer queen and

00:05:44,400 --> 00:05:49,620
that's that's the sort of stuff so

00:05:46,470 --> 00:05:51,300
that's an illustration that these word

00:05:49,620 --> 00:05:54,390
embedding sentence embeddings are

00:05:51,300 --> 00:05:57,720
actually captured some notion of grammar

00:05:54,390 --> 00:06:07,470
some notion of semantics some notion of

00:05:57,720 --> 00:06:09,030
meaning right so just to go over the

00:06:07,470 --> 00:06:11,010
bullet points applicable at word

00:06:09,030 --> 00:06:13,140
sentence and even document level

00:06:11,010 --> 00:06:16,500
although the longer and longer your

00:06:13,140 --> 00:06:21,960
concept becomes the harder it is to to

00:06:16,500 --> 00:06:24,210
extract extract robust embeddings but

00:06:21,960 --> 00:06:26,640
word and a word and phrase level

00:06:24,210 --> 00:06:28,920
embeddings are quite popular quite

00:06:26,640 --> 00:06:33,210
robust and is generally the de facto way

00:06:28,920 --> 00:06:36,690
now to now to work with NLP systems

00:06:33,210 --> 00:06:38,910
you can do neural networks too kind of

00:06:36,690 --> 00:06:43,169
evens I'll bring it up to an document

00:06:38,910 --> 00:06:47,130
level using sliding convolutions or some

00:06:43,169 --> 00:06:48,750
other stuff like this it performs best

00:06:47,130 --> 00:06:51,360
when trained with an application

00:06:48,750 --> 00:06:54,449
vertical specific vocabulary because

00:06:51,360 --> 00:06:55,830
after all while there is a lot of while

00:06:54,449 --> 00:06:59,250
these neural networks have a lot of

00:06:55,830 --> 00:07:01,830
potential to extract the meaning but if

00:06:59,250 --> 00:07:04,139
it's but they work best if it's a

00:07:01,830 --> 00:07:06,240
limited domain so we have seen

00:07:04,139 --> 00:07:09,990
applications with financial document

00:07:06,240 --> 00:07:11,820
sets or legal etcetera and for a

00:07:09,990 --> 00:07:13,440
particular domain it really works very

00:07:11,820 --> 00:07:17,490
well because this kind of concept

00:07:13,440 --> 00:07:24,690
extraction it really it really works

00:07:17,490 --> 00:07:27,270
well right now of course this is nowhere

00:07:24,690 --> 00:07:30,030
near human level comprehension so this

00:07:27,270 --> 00:07:32,039
stuff is really good for tasks that are

00:07:30,030 --> 00:07:36,090
simple for humans you know I would say a

00:07:32,039 --> 00:07:38,039
five to six seven-year-old who is in

00:07:36,090 --> 00:07:39,449
their native language that kind of

00:07:38,039 --> 00:07:43,380
complexity you know small vocabulary

00:07:39,449 --> 00:07:45,889
simple concepts don't expect this to do

00:07:43,380 --> 00:07:48,900
too much but boring repetitive

00:07:45,889 --> 00:07:50,520
error-prone tasks like just reading

00:07:48,900 --> 00:07:52,349
through a document and checking for some

00:07:50,520 --> 00:07:58,889
certain things etc can be very well and

00:07:52,349 --> 00:08:01,590
very well automated now the the thing is

00:07:58,889 --> 00:08:03,449
that it is it is a big you usually use a

00:08:01,590 --> 00:08:06,240
mixture of supervised and unsupervised

00:08:03,449 --> 00:08:08,400
learning the unsupervised learning is

00:08:06,240 --> 00:08:14,400
used to create the word embeddings and

00:08:08,400 --> 00:08:16,050
the thought vectors so you don't need

00:08:14,400 --> 00:08:19,229
label data set right you can throw

00:08:16,050 --> 00:08:21,780
entire Wikipedia entire encyclopedias at

00:08:19,229 --> 00:08:23,909
this thing and it just cranks away and

00:08:21,780 --> 00:08:26,610
tries to learn the structure of the

00:08:23,909 --> 00:08:28,979
language tries to learn the grammar now

00:08:26,610 --> 00:08:30,659
of course like all things in deep

00:08:28,979 --> 00:08:33,000
learning and like the previous speaker

00:08:30,659 --> 00:08:37,820
was also showing you know these are

00:08:33,000 --> 00:08:40,320
computationally very heavy tasks and

00:08:37,820 --> 00:08:42,010
typically to create these word

00:08:40,320 --> 00:08:46,270
embeddings and thought embedding

00:08:42,010 --> 00:08:48,730
they take multiple weeks on a single GPU

00:08:46,270 --> 00:08:50,590
so you need to paralyze this and run it

00:08:48,730 --> 00:08:56,440
across and use some of the techniques

00:08:50,590 --> 00:08:58,630
that was described also previously once

00:08:56,440 --> 00:08:59,920
you have got your word embeddings and

00:08:58,630 --> 00:09:02,950
your thought embeddings for your

00:08:59,920 --> 00:09:05,740
particular application domain then you

00:09:02,950 --> 00:09:07,660
can then you can deploy some supervised

00:09:05,740 --> 00:09:13,510
learning to do specific tasks we can

00:09:07,660 --> 00:09:16,720
we'll come two examples of that yeah

00:09:13,510 --> 00:09:18,970
like I mentioned before don't expect

00:09:16,720 --> 00:09:20,560
this to write a book or to explain to

00:09:18,970 --> 00:09:22,270
you you know read a book and to explain

00:09:20,560 --> 00:09:25,390
to you what the book means that's way

00:09:22,270 --> 00:09:28,780
beyond it but simple mundane error-prone

00:09:25,390 --> 00:09:30,370
tasks can be very effective effectively

00:09:28,780 --> 00:09:35,530
automated examples are database

00:09:30,370 --> 00:09:38,350
normalization data set merging document

00:09:35,530 --> 00:09:45,580
comparison and retrieval document

00:09:38,350 --> 00:09:47,380
summarization data entry etc so you know

00:09:45,580 --> 00:09:50,410
for example you could you could you

00:09:47,380 --> 00:09:52,690
could do a semantic search and say find

00:09:50,410 --> 00:09:54,580
in in my corpus of documents inside an

00:09:52,690 --> 00:09:58,780
enterprise you know find me all the

00:09:54,580 --> 00:10:03,070
documents that would that are similar in

00:09:58,780 --> 00:10:04,810
meaning so you do not rely on keyword

00:10:03,070 --> 00:10:08,260
search you do not rely on your search

00:10:04,810 --> 00:10:10,030
query having the same terms as you would

00:10:08,260 --> 00:10:11,920
have in the document right so you could

00:10:10,030 --> 00:10:13,390
have a search query with no words in

00:10:11,920 --> 00:10:15,670
similar to what you're searching for

00:10:13,390 --> 00:10:17,290
because you know you could have synonyms

00:10:15,670 --> 00:10:18,780
you could use different jargon you could

00:10:17,290 --> 00:10:22,270
use different acronyms so you truly

00:10:18,780 --> 00:10:24,790
extract so so the basic idea is very

00:10:22,270 --> 00:10:26,410
simple you extract meaning from the

00:10:24,790 --> 00:10:28,270
question you go through all your

00:10:26,410 --> 00:10:30,940
documents and you extract the meaning

00:10:28,270 --> 00:10:33,130
and then you a comparison at the

00:10:30,940 --> 00:10:35,170
semantic level at the meaning level

00:10:33,130 --> 00:10:37,330
rather than doing a string compare or a

00:10:35,170 --> 00:10:40,180
string search that's the basic idea of

00:10:37,330 --> 00:10:42,420
kind of an NLP based search right first

00:10:40,180 --> 00:10:45,340
you turn your question into an embedding

00:10:42,420 --> 00:10:47,530
and you turn your documents into

00:10:45,340 --> 00:10:49,510
embeddings and then of course you cannot

00:10:47,530 --> 00:10:52,780
do a simple bit of a euclidean distance

00:10:49,510 --> 00:10:53,170
or a simple comparison so that is you

00:10:52,780 --> 00:10:55,929
have to

00:10:53,170 --> 00:10:58,209
for different notion of equality and use

00:10:55,929 --> 00:11:01,119
the neural network to implement that so

00:10:58,209 --> 00:11:03,009
the basic framework would be turn your

00:11:01,119 --> 00:11:05,259
sentence vector sentence questions

00:11:03,009 --> 00:11:07,600
question into a vector into an embedding

00:11:05,259 --> 00:11:10,269
turn your documents into a set of

00:11:07,600 --> 00:11:12,759
embeddings then use a neural network to

00:11:10,269 --> 00:11:19,269
compare those and that neural network

00:11:12,759 --> 00:11:22,329
essentially defines in a different

00:11:19,269 --> 00:11:24,850
notion of distance right rather than

00:11:22,329 --> 00:11:27,009
some simple Euclidean distance and that

00:11:24,850 --> 00:11:29,829
and and bite and on this basic framework

00:11:27,009 --> 00:11:32,199
you can tackle many problems like this

00:11:29,829 --> 00:11:37,929
use cases like this and you can see that

00:11:32,199 --> 00:11:40,269
in in modern enterprises banks or legal

00:11:37,929 --> 00:11:42,519
farms or even going through manuals and

00:11:40,269 --> 00:11:44,069
stuff like this wherever you have large

00:11:42,519 --> 00:11:47,589
corpus of

00:11:44,069 --> 00:11:50,439
private and secure documents you know

00:11:47,589 --> 00:11:54,579
these techniques really help to index to

00:11:50,439 --> 00:11:58,230
search to categorize to summarize etc

00:11:54,579 --> 00:12:01,329
itself basically anything that is text

00:11:58,230 --> 00:12:05,549
right let's come to the focus of this

00:12:01,329 --> 00:12:09,009
conference like I mentioned before

00:12:05,549 --> 00:12:10,929
training is the creation is essentially

00:12:09,009 --> 00:12:14,739
the process of creation of creating

00:12:10,929 --> 00:12:17,499
custom thought and word vectors as

00:12:14,739 --> 00:12:20,949
should be quite obvious compute workload

00:12:17,499 --> 00:12:22,869
scales with training data set size now

00:12:20,949 --> 00:12:23,829
when you're generating your word

00:12:22,869 --> 00:12:25,839
embeddings where you are doing your

00:12:23,829 --> 00:12:27,249
unsupervised learning the larger the

00:12:25,839 --> 00:12:33,850
data sets you use the better it is

00:12:27,249 --> 00:12:36,220
clearly and and for sensitive data

00:12:33,850 --> 00:12:39,360
enterprises prefer on-prem versus assure

00:12:36,220 --> 00:12:42,279
so typically you know they would set up

00:12:39,360 --> 00:12:45,999
clusters on pram or have like some

00:12:42,279 --> 00:12:48,369
co-located data center for this on the

00:12:45,999 --> 00:12:50,350
inference side compute scales with the

00:12:48,369 --> 00:12:53,739
number of users and the number of

00:12:50,350 --> 00:12:55,689
documents because you you know you have

00:12:53,739 --> 00:12:57,519
to kind of go through them and do the

00:12:55,689 --> 00:13:00,459
comparison across each document as I

00:12:57,519 --> 00:13:02,139
described before so you can see that

00:13:00,459 --> 00:13:04,010
that is a huge computer load right if

00:13:02,139 --> 00:13:06,590
you are getting you know

00:13:04,010 --> 00:13:08,450
doesn't queries in a typical enterprise

00:13:06,590 --> 00:13:12,110
you know dozen or a couple of dozen

00:13:08,450 --> 00:13:13,820
queries per second are coming in then

00:13:12,110 --> 00:13:15,770
you multiply it by the number of users

00:13:13,820 --> 00:13:19,160
by the number of simultaneous queries

00:13:15,770 --> 00:13:19,940
and by the court document what size that

00:13:19,160 --> 00:13:24,020
is

00:13:19,940 --> 00:13:26,870
so this is very unique because in a

00:13:24,020 --> 00:13:28,970
typical neural network application the

00:13:26,870 --> 00:13:31,310
training is orders of magnitude higher

00:13:28,970 --> 00:13:35,690
compute requirements than a single

00:13:31,310 --> 00:13:38,510
inference but in this case because of

00:13:35,690 --> 00:13:40,880
the nature of this problem your

00:13:38,510 --> 00:13:44,000
inference is could also be very compute

00:13:40,880 --> 00:13:45,830
intensive and if you have a lot of users

00:13:44,000 --> 00:13:49,160
or a very large corpus of their

00:13:45,830 --> 00:13:54,010
documents to search through then your

00:13:49,160 --> 00:13:56,960
training could could start equaling the

00:13:54,010 --> 00:13:59,120
super white part of the training the

00:13:56,960 --> 00:14:02,620
unsupervised part is done once and you

00:13:59,120 --> 00:14:02,620
can keep reusing the word embeddings

00:14:03,460 --> 00:14:10,250
inference computer I guess I just talked

00:14:05,960 --> 00:14:12,440
about this slide yeah so machine

00:14:10,250 --> 00:14:14,420
learning NLP you know it is you know we

00:14:12,440 --> 00:14:16,940
know we have all implemented kind of or

00:14:14,420 --> 00:14:19,250
in any one of us were programmers here

00:14:16,940 --> 00:14:22,070
we have all implemented you know spring

00:14:19,250 --> 00:14:28,420
search and stuff like this and you know

00:14:22,070 --> 00:14:34,250
it is a few 10 10 20 flops you know / /

00:14:28,420 --> 00:14:37,120
/ world of string compare on the other

00:14:34,250 --> 00:14:40,300
hand the deep learning NLP based search

00:14:37,120 --> 00:14:44,870
is of the order of 10 power 5 flops

00:14:40,300 --> 00:14:46,730
right so yes this deep learning is doing

00:14:44,870 --> 00:14:48,770
some very cool stuff and is able to

00:14:46,730 --> 00:14:53,690
extract a lot of meaning and stuff like

00:14:48,770 --> 00:14:56,810
this but it comes at a cost and that's

00:14:53,690 --> 00:14:59,090
the cost response time of course for

00:14:56,810 --> 00:15:02,060
search and stuff like this is is is is

00:14:59,090 --> 00:15:04,130
critical to user satisfaction and hence

00:15:02,060 --> 00:15:08,030
you have to have dedicated resources or

00:15:04,130 --> 00:15:09,560
spun up based on user activity deep

00:15:08,030 --> 00:15:11,300
learning jobs are compute limited the

00:15:09,560 --> 00:15:13,699
reason that bullet is there because we

00:15:11,300 --> 00:15:16,449
have run into customers who are used to

00:15:13,699 --> 00:15:20,509
you know old school kind of NLP

00:15:16,449 --> 00:15:23,419
classical NLP techniques and usually

00:15:20,509 --> 00:15:25,939
over there because the it is not compute

00:15:23,419 --> 00:15:27,889
limited it is typically IO limited in

00:15:25,939 --> 00:15:31,339
the previous world then when we run into

00:15:27,889 --> 00:15:33,259
customers who are used to that and then

00:15:31,339 --> 00:15:34,850
when we say hey you know this this job

00:15:33,259 --> 00:15:36,139
requires a lot of computer would say why

00:15:34,850 --> 00:15:38,509
don't you just spin up another thread

00:15:36,139 --> 00:15:42,919
because they are used to a world where

00:15:38,509 --> 00:15:46,220
the CPU is maybe at 5% or 10% percent

00:15:42,919 --> 00:15:46,999
loading because the thing is IO IO

00:15:46,220 --> 00:15:49,669
limited

00:15:46,999 --> 00:15:51,139
whereas deep learning is just you just

00:15:49,669 --> 00:15:53,809
flip it's just the other way around

00:15:51,139 --> 00:15:55,939
right this is severely compute limited

00:15:53,809 --> 00:15:58,790
job especially this kind of NLP jobs

00:15:55,939 --> 00:16:00,379
that we are describing and then you run

00:15:58,790 --> 00:16:02,449
into the problem where you have to

00:16:00,379 --> 00:16:04,429
explain to the customer hey I can't just

00:16:02,449 --> 00:16:08,629
spin up another thread on that CPU I

00:16:04,429 --> 00:16:10,009
need to add more hardware so it takes

00:16:08,629 --> 00:16:11,749
much longer because the computer

00:16:10,009 --> 00:16:13,850
requirements don't run by underestimate

00:16:11,749 --> 00:16:16,189
how much you need and this is what we

00:16:13,850 --> 00:16:17,929
tell our customers it's great news for

00:16:16,189 --> 00:16:20,749
IBM because they'll be buying lots and

00:16:17,929 --> 00:16:29,329
lots of IBM Minsky's hopefully to run

00:16:20,749 --> 00:16:33,049
these kind of jobs right so I apologize

00:16:29,329 --> 00:16:35,059
that this table is not yet filled in we

00:16:33,049 --> 00:16:41,179
did the best that we could in limited

00:16:35,059 --> 00:16:49,279
time but this is this is inference

00:16:41,179 --> 00:16:52,819
execution on about I believe it is on

00:16:49,279 --> 00:16:55,279
about 10 to 20,000 document corpus so

00:16:52,819 --> 00:16:57,889
not too large not too large but it's a

00:16:55,279 --> 00:17:01,879
typical small enterprise private corpus

00:16:57,889 --> 00:17:06,919
and we have profiled it on different

00:17:01,879 --> 00:17:08,839
different machines it's not quite apples

00:17:06,919 --> 00:17:11,240
to apples I'm afraid over there on the

00:17:08,839 --> 00:17:14,600
left side but it gives you a good

00:17:11,240 --> 00:17:17,510
example i right at the top is the is the

00:17:14,600 --> 00:17:20,329
K 20s you know you can see that that is

00:17:17,510 --> 00:17:22,579
you know 5.6 seconds just kind of not at

00:17:20,329 --> 00:17:24,709
all useful

00:17:22,579 --> 00:17:26,520
there's your K T's you know the K T's is

00:17:24,709 --> 00:17:28,740
still the workhorse of the data

00:17:26,520 --> 00:17:33,630
you know while we talk about exotic gpus

00:17:28,740 --> 00:17:35,610
like Pascal's and Tesla's and stuff like

00:17:33,630 --> 00:17:39,480
this the KT is still the vast majority

00:17:35,610 --> 00:17:41,670
workhorse of the of the data center and

00:17:39,480 --> 00:17:43,650
you know it is reasonably priced as well

00:17:41,670 --> 00:17:45,750
so those are the numbers if you can go

00:17:43,650 --> 00:17:49,470
to a 4k T machine you're down to about

00:17:45,750 --> 00:17:52,440
one second which is getting there but

00:17:49,470 --> 00:17:54,570
not quite good now here with the m-40

00:17:52,440 --> 00:17:58,590
and the Minsky's we are really getting

00:17:54,570 --> 00:17:59,820
into business but I would also but I

00:17:58,590 --> 00:18:02,820
would like to point out interestingly

00:17:59,820 --> 00:18:08,070
again a point in favor of Minsky's is

00:18:02,820 --> 00:18:12,179
that you get almost half in terms of the

00:18:08,070 --> 00:18:16,650
time and the cost is not that much more

00:18:12,179 --> 00:18:18,990
the cost numbers you know different

00:18:16,650 --> 00:18:21,900
vendor it depends on the source of where

00:18:18,990 --> 00:18:23,790
you're having this one is from a cloud

00:18:21,900 --> 00:18:28,679
vendor called zero scale their website

00:18:23,790 --> 00:18:30,600
and don't look at the absolute numbers

00:18:28,679 --> 00:18:32,340
because you know that will depend but

00:18:30,600 --> 00:18:45,990
the relative numbers are quite

00:18:32,340 --> 00:18:51,570
illustrative okay and I think that is my

00:18:45,990 --> 00:18:53,490
last slide yes a little bit about us we

00:18:51,570 --> 00:18:56,820
are existing kind of deep learning

00:18:53,490 --> 00:19:00,780
vendor to IBM we are vertical agnostic

00:18:56,820 --> 00:19:04,320
we work with automata motive finance and

00:19:00,780 --> 00:19:07,530
pharma NLP machine vision control

00:19:04,320 --> 00:19:09,720
systems NLP is the one I just talked

00:19:07,530 --> 00:19:13,559
about but we of course do a lot of work

00:19:09,720 --> 00:19:17,730
with machine vision as well very large

00:19:13,559 --> 00:19:21,120
neural let's not not 16,000 16,000

00:19:17,730 --> 00:19:22,559
layers but but we do work with vgg and

00:19:21,120 --> 00:19:24,630
ResNet which has kind of the state of

00:19:22,559 --> 00:19:27,870
the art large near let's and we scale

00:19:24,630 --> 00:19:29,700
them across multiple servers etc the

00:19:27,870 --> 00:19:32,070
other very interesting application which

00:19:29,700 --> 00:19:33,360
maybe sometime in the future we can give

00:19:32,070 --> 00:19:36,390
a talk on is using reinforcement

00:19:33,360 --> 00:19:38,149
learning for control systems that's also

00:19:36,390 --> 00:19:40,519
something we specialize in

00:19:38,149 --> 00:19:42,379
we are full-stack experts so we kind of

00:19:40,519 --> 00:19:44,059
understand hardware and all the way up

00:19:42,379 --> 00:19:49,789
to the kind of the mathematics of neural

00:19:44,059 --> 00:19:53,089
networks and we help people we have

00:19:49,789 --> 00:19:55,429
guide our customers for all the way from

00:19:53,089 --> 00:19:58,309
what are the problems that are good for

00:19:55,429 --> 00:20:00,199
deep learning to how do you create your

00:19:58,309 --> 00:20:03,259
data sets we all know that that is the

00:20:00,199 --> 00:20:06,169
very important factor for success of

00:20:03,259 --> 00:20:08,179
deep neural nets and all the way through

00:20:06,169 --> 00:20:11,529
designing training the neural nets

00:20:08,179 --> 00:20:14,329
deployment and then final production and

00:20:11,529 --> 00:20:16,129
the production step very interesting and

00:20:14,329 --> 00:20:18,289
very important for our deep neural nets

00:20:16,129 --> 00:20:21,379
because at that point active learning

00:20:18,289 --> 00:20:25,279
can kick in and as your users start

00:20:21,379 --> 00:20:27,379
using these systems you can point out

00:20:25,279 --> 00:20:29,989
the ones where the neural network got

00:20:27,379 --> 00:20:31,819
the answers wrong and you can save them

00:20:29,989 --> 00:20:34,699
all up batch them up during inference

00:20:31,819 --> 00:20:37,099
time stream it back into the cloud at

00:20:34,699 --> 00:20:39,079
night or when you have kind of dumb time

00:20:37,099 --> 00:20:41,749
and then accumulate all these

00:20:39,079 --> 00:20:44,119
error-prone examples and retrain the

00:20:41,749 --> 00:20:44,719
neural network so it rapidly keeps

00:20:44,119 --> 00:20:46,909
getting better

00:20:44,719 --> 00:20:48,829
so with conventional software deployment

00:20:46,909 --> 00:20:50,179
deployment is when you are done and you

00:20:48,829 --> 00:20:52,549
know at that point you're just doing bug

00:20:50,179 --> 00:20:54,979
fixes with deep neural network the very

00:20:52,549 --> 00:20:57,919
cool part is that deployment is where it

00:20:54,979 --> 00:21:00,439
starts and as that as more and more

00:20:57,919 --> 00:21:01,669
users start using these things you know

00:21:00,439 --> 00:21:06,289
the performance keep getting better

00:21:01,669 --> 00:21:11,539
because you can keep retraining right so

00:21:06,289 --> 00:21:15,259
that's the end of my talk thank you very

00:21:11,539 --> 00:21:18,559
much the last talk of the day and yeah

00:21:15,259 --> 00:21:24,549
if you have any questions I'm happy to

00:21:18,559 --> 00:21:24,549
answer them yes

00:21:28,649 --> 00:21:35,139
it's a good question I am not enough of

00:21:33,370 --> 00:21:37,299
an expert to kind of give you an

00:21:35,139 --> 00:21:42,269
in-depth answer on that but I can hint I

00:21:37,299 --> 00:21:42,269
can I can point to the general direction

00:21:43,259 --> 00:21:49,870
yeah the Markov models are very good for

00:21:46,960 --> 00:21:55,049
kind of sequence and multi-step kind of

00:21:49,870 --> 00:21:58,809
stuff which is indeed which is indeed

00:21:55,049 --> 00:22:03,669
what is happening here you use either

00:21:58,809 --> 00:22:06,519
Iranians or Ellis tiems or or sliding

00:22:03,669 --> 00:22:12,159
convolutions to kind of tackle the

00:22:06,519 --> 00:22:14,110
sequential nature of this yeah

00:22:12,159 --> 00:22:15,850
unfortunately Markov models you cannot

00:22:14,110 --> 00:22:21,549
back propagate through them so the

00:22:15,850 --> 00:22:29,470
credit assignment problem is is is is is

00:22:21,549 --> 00:22:35,529
not solvable in in in the hmm s yeah I

00:22:29,470 --> 00:22:37,389
think that's my best answer that see I

00:22:35,529 --> 00:22:40,179
started off by saying you don't have to

00:22:37,389 --> 00:22:45,009
enter our features because the process

00:22:40,179 --> 00:22:47,679
of back propagation you can learn you

00:22:45,009 --> 00:22:50,379
cannot back propagate through Markov

00:22:47,679 --> 00:22:51,610
model and I think I know why but we

00:22:50,379 --> 00:22:54,789
don't have to go that I think that's

00:22:51,610 --> 00:22:57,129
sufficient that you the credit

00:22:54,789 --> 00:23:00,100
assignment problem is is not easy to be

00:22:57,129 --> 00:23:02,259
solved on Markov models but it's a good

00:23:00,100 --> 00:23:04,059
question because it's very similar in

00:23:02,259 --> 00:23:06,700
structure to some of these Alice gems if

00:23:04,059 --> 00:23:08,230
you look at it right but the whole magic

00:23:06,700 --> 00:23:15,940
is about how do you assign those weights

00:23:08,230 --> 00:23:19,320
right anyone here in the audience who's

00:23:15,940 --> 00:23:19,320
worked with NLP and deep learning

00:23:24,000 --> 00:23:27,150

YouTube URL: https://www.youtube.com/watch?v=Zp1Y52sMDzg


