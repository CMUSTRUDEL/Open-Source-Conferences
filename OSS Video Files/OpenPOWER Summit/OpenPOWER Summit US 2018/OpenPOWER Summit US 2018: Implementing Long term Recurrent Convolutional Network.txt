Title: OpenPOWER Summit US 2018: Implementing Long term Recurrent Convolutional Network
Publication date: 2018-04-06
Playlist: OpenPOWER Summit US 2018
Description: 
	Xiaofan Zhang discusses implementing long-term recurrent convolutional network using HLS on POWER Systems at the OpenPOWER Summit 2018. 

For more information please visit www.openpowerfoundation.org.
Captions: 
	00:00:00,140 --> 00:00:07,440
so hello everyone my name is and PhD

00:00:05,339 --> 00:00:10,320
student from the University of Illinois

00:00:07,440 --> 00:00:12,719
urbana-champaign so today I could like

00:00:10,320 --> 00:00:17,100
to share some experience of my research

00:00:12,719 --> 00:00:21,500
relate to the FPGA is deep new and never

00:00:17,100 --> 00:00:24,359
decide on the power sector so mine is

00:00:21,500 --> 00:00:28,130
implementing non-recurring commercially

00:00:24,359 --> 00:00:32,340
elaborate using H OS on the power system

00:00:28,130 --> 00:00:34,230
so here why we need to accelerate our

00:00:32,340 --> 00:00:37,500
last year which is the long term

00:00:34,230 --> 00:00:40,020
recurrent convolutional network erosion

00:00:37,500 --> 00:00:42,629
is one of the most effective module

00:00:40,020 --> 00:00:46,140
available today which has a wide range

00:00:42,629 --> 00:00:48,390
of epic just the activity recognition

00:00:46,140 --> 00:00:51,059
the image or video classification

00:00:48,390 --> 00:00:54,000
captioning storytelling the most

00:00:51,059 --> 00:00:57,270
important feature of Ellison is that it

00:00:54,000 --> 00:01:00,690
come by the post CNN and Ironmen which

00:00:57,270 --> 00:01:04,439
means it can capture both features from

00:01:00,690 --> 00:01:06,409
the spatial and temporal from the issue

00:01:04,439 --> 00:01:11,220
and temporal features so it's very

00:01:06,409 --> 00:01:14,460
interesting and very useful so we need

00:01:11,220 --> 00:01:18,900
to make this network run fast so we try

00:01:14,460 --> 00:01:21,330
to make a saddle rater for it so here

00:01:18,900 --> 00:01:25,950
currently we have different choice for

00:01:21,330 --> 00:01:28,470
inference on the deep neural network GPU

00:01:25,950 --> 00:01:30,689
are very popular today people use GPO

00:01:28,470 --> 00:01:34,079
for training for inference because the

00:01:30,689 --> 00:01:36,240
high bandwidth the high throughput and

00:01:34,079 --> 00:01:38,310
the very high performance II and of

00:01:36,240 --> 00:01:43,070
course GPU take a lot of hard

00:01:38,310 --> 00:01:46,320
consumption also people can use CPU and

00:01:43,070 --> 00:01:49,530
defense to the multi thread technology

00:01:46,320 --> 00:01:52,950
and also the higher frequency GPU can be

00:01:49,530 --> 00:01:58,560
fast however you also take more than 16

00:01:52,950 --> 00:02:01,170
watts in terms of the dedicated side we

00:01:58,560 --> 00:02:04,110
can use the ASIC to accelerate it is

00:02:01,170 --> 00:02:07,290
very unhype for high performance and of

00:02:04,110 --> 00:02:10,860
course it's very high efficient however

00:02:07,290 --> 00:02:13,800
is it suffer from the longer time to

00:02:10,860 --> 00:02:14,370
keep and also is not flexible so in

00:02:13,800 --> 00:02:16,560
terms of

00:02:14,370 --> 00:02:20,760
of the tray of Amanda performanc

00:02:16,560 --> 00:02:25,140
flexibility as energy efficiency we can

00:02:20,760 --> 00:02:28,110
choose FPGA so here I gave a table

00:02:25,140 --> 00:02:32,900
listing the result of the Alex that

00:02:28,110 --> 00:02:36,299
inference latency here and I show the

00:02:32,900 --> 00:02:37,799
efficiency so you can see the FPGA based

00:02:36,299 --> 00:02:40,140
design actually achieved the highest

00:02:37,799 --> 00:02:44,190
energy efficiency peaking at twenty

00:02:40,140 --> 00:02:45,440
three point six image person what that

00:02:44,190 --> 00:02:49,670
we

00:02:45,440 --> 00:02:53,030
a very suitable when we try to

00:02:49,670 --> 00:02:56,810
accelerate a new in a very small device

00:02:53,030 --> 00:02:59,420
such as the HD one so however if you

00:02:56,810 --> 00:03:03,620
also have the the problem which is the

00:02:59,420 --> 00:03:07,130
memory limitation very a small amount of

00:03:03,620 --> 00:03:09,920
memory in the PJ no matter on to memory

00:03:07,130 --> 00:03:13,450
or the object memory and also a PJ is

00:03:09,920 --> 00:03:17,450
hard to program you need to use the RTO

00:03:13,450 --> 00:03:20,300
language to program a PJ so my solution

00:03:17,450 --> 00:03:22,610
is that to overcome the memory issue we

00:03:20,300 --> 00:03:25,880
use the Kathy interface which is the

00:03:22,610 --> 00:03:27,650
coherent servitor processed interface so

00:03:25,880 --> 00:03:30,680
by using --happy the accelerator can

00:03:27,650 --> 00:03:33,770
show a huge amount of the memory on the

00:03:30,680 --> 00:03:36,890
CPU side in the power system it can be

00:03:33,770 --> 00:03:39,950
up to 500 gigabyte on mode also the

00:03:36,890 --> 00:03:44,150
accellerated are the foop here to the

00:03:39,950 --> 00:03:46,670
host cpu which means the servitor can

00:03:44,150 --> 00:03:50,660
directly access the memory which is

00:03:46,670 --> 00:03:52,970
belong to the CPU side so the other

00:03:50,660 --> 00:03:55,040
problem is the time consuming and

00:03:52,970 --> 00:03:59,060
challenging at your programming also

00:03:55,040 --> 00:04:01,010
there is a hardware verification the

00:03:59,060 --> 00:04:03,920
resource allocation because we only have

00:04:01,010 --> 00:04:07,370
a very limited resources in the fpga so

00:04:03,920 --> 00:04:10,610
how to make this work easier

00:04:07,370 --> 00:04:13,040
we used the actual s which is the high

00:04:10,610 --> 00:04:16,340
level synthesis it's the design flow

00:04:13,040 --> 00:04:18,650
provider provided by xylenes here you

00:04:16,340 --> 00:04:21,980
can use a high-level language such as

00:04:18,650 --> 00:04:25,460
the c and c++ for a short description of

00:04:21,980 --> 00:04:29,780
the how weird how weird components and

00:04:25,460 --> 00:04:32,030
also is easier to debug and is easier to

00:04:29,780 --> 00:04:34,640
do the decides based of exploration

00:04:32,030 --> 00:04:40,520
because you can take a read bill in

00:04:34,640 --> 00:04:42,950
pragma like unrolling like partition so

00:04:40,520 --> 00:04:47,690
let me talk about the challenge when we

00:04:42,950 --> 00:04:49,760
try to decide this a seller question is

00:04:47,690 --> 00:04:56,110
very complicated here at least two

00:04:49,760 --> 00:04:58,370
figures if we take a 80 you it will take

00:04:56,110 --> 00:05:00,919
124 millisecond

00:04:58,370 --> 00:05:03,650
and if we try to use a generally five

00:05:00,919 --> 00:05:07,880
CPU it will take one night millisecond

00:05:03,650 --> 00:05:10,460
for process single image so in terms of

00:05:07,880 --> 00:05:12,260
FPGA which is very limited by the

00:05:10,460 --> 00:05:15,199
resources we need to come up with your

00:05:12,260 --> 00:05:17,060
very precise optimization schemes such

00:05:15,199 --> 00:05:19,610
as how many we saw should be I looked

00:05:17,060 --> 00:05:20,900
into the first layer how many we saw

00:05:19,610 --> 00:05:24,380
should be allocated to the other layers

00:05:20,900 --> 00:05:27,110
and also we need to face the memory

00:05:24,380 --> 00:05:29,330
issue is the memory access back memory

00:05:27,110 --> 00:05:32,240
access latency here because the origin

00:05:29,330 --> 00:05:34,310
memory is not enough for or common they

00:05:32,240 --> 00:05:37,729
all the parameters we need to use that

00:05:34,310 --> 00:05:40,639
external so we need to come up with some

00:05:37,729 --> 00:05:44,180
hierarchical memory decide to hide the

00:05:40,639 --> 00:05:47,960
memory access latency here so here is a

00:05:44,180 --> 00:05:50,930
diagram of the error I see on the front

00:05:47,960 --> 00:05:55,190
part is the CNN which is the annex net

00:05:50,930 --> 00:06:00,680
in most case and the back end is on here

00:05:55,190 --> 00:06:03,080
I list the STM which is a common use the

00:06:00,680 --> 00:06:08,240
image is input from the left side to the

00:06:03,080 --> 00:06:10,910
right side and CNN the other features

00:06:08,240 --> 00:06:12,680
are captured by CNN the feature will be

00:06:10,910 --> 00:06:15,050
positive the aresty and to generate a

00:06:12,680 --> 00:06:18,979
proper description such as a test

00:06:15,050 --> 00:06:20,539
message and the table here shows the

00:06:18,979 --> 00:06:24,020
different characteristics of different

00:06:20,539 --> 00:06:29,900
layer in the convolutional layer most of

00:06:24,020 --> 00:06:32,090
the most of the the computation if you

00:06:29,900 --> 00:06:33,889
spend a lot of computational resource

00:06:32,090 --> 00:06:39,169
which means the convolutional layer will

00:06:33,889 --> 00:06:41,690
be its memory is not memory bond is

00:06:39,169 --> 00:06:44,570
actually computational bond so in the

00:06:41,690 --> 00:06:46,700
conch in the fully connected layer is

00:06:44,570 --> 00:06:50,750
quite different it's actually a memory

00:06:46,700 --> 00:06:53,240
bond you can see is sixteen seven point

00:06:50,750 --> 00:06:55,880
seven sixteen seven percent of the

00:06:53,240 --> 00:06:58,190
resource are allocated to the memory so

00:06:55,880 --> 00:07:00,800
why here why in the last is the island

00:06:58,190 --> 00:07:02,660
quite in the middle of the convolutional

00:07:00,800 --> 00:07:04,010
FeliCa net layer so with different

00:07:02,660 --> 00:07:06,460
characteristics we need to have

00:07:04,010 --> 00:07:09,349
different skin to a cell radius network

00:07:06,460 --> 00:07:11,750
so here is we proposed a resource

00:07:09,349 --> 00:07:14,990
allocation scheme called web

00:07:11,750 --> 00:07:17,300
to generate the guy knife or for

00:07:14,990 --> 00:07:19,600
resource allocation so when we analyze

00:07:17,300 --> 00:07:24,020
the computation of demand of each layer

00:07:19,600 --> 00:07:26,540
and generate a free radical design such

00:07:24,020 --> 00:07:30,380
as how many TSP use used for a certain

00:07:26,540 --> 00:07:33,020
layer so the idea is to fight a lower

00:07:30,380 --> 00:07:37,460
bound of the latency so in the equation

00:07:33,020 --> 00:07:41,300
free we use a congee inequality to fight

00:07:37,460 --> 00:07:45,040
a lower bound and we can choose we can

00:07:41,300 --> 00:07:48,470
achieve this lower bound unless we which

00:07:45,040 --> 00:07:50,570
unless we reach a certain condition so

00:07:48,470 --> 00:07:56,330
the condition here are highlighted in

00:07:50,570 --> 00:08:01,669
red which means in the in the layer I

00:07:56,330 --> 00:08:03,740
and J is here in the middle the white

00:08:01,669 --> 00:08:06,800
highlighted in red here it means the

00:08:03,740 --> 00:08:10,700
convolutions that and the resource

00:08:06,800 --> 00:08:13,400
allocate to layer I and J should follow

00:08:10,700 --> 00:08:15,620
the ratio between the square root of the

00:08:13,400 --> 00:08:18,680
computation of Dammam I and the

00:08:15,620 --> 00:08:24,500
computation de montreuil so follow this

00:08:18,680 --> 00:08:28,840
guideline we can design the but resource

00:08:24,500 --> 00:08:32,000
allocation scheme so the other issue is

00:08:28,840 --> 00:08:35,120
aeration can tell large amount of loop

00:08:32,000 --> 00:08:38,990
iteration the efficient implementation

00:08:35,120 --> 00:08:43,210
look loop will directly improve overall

00:08:38,990 --> 00:08:46,820
performance so here we propose a H oSIP

00:08:43,210 --> 00:08:49,670
which is showing in the figure 8 it can

00:08:46,820 --> 00:08:51,830
be unrolled in two dimension the CI ty

00:08:49,670 --> 00:08:55,750
measure which it is the input channel

00:08:51,830 --> 00:08:59,900
and see the output

00:08:55,750 --> 00:09:03,339
so in this L actually as I PE contained

00:08:59,900 --> 00:09:06,560
the multiplier and she adder is ready

00:09:03,339 --> 00:09:09,580
come here to decide here and use this IP

00:09:06,560 --> 00:09:13,240
we can be a whole commercial network

00:09:09,580 --> 00:09:17,300
which is shown in the figure B so this

00:09:13,240 --> 00:09:19,400
convolutional layer is tell across the

00:09:17,300 --> 00:09:23,360
input Channel and the Alpha Channel and

00:09:19,400 --> 00:09:25,410
its motel is handled by one HQs IP which

00:09:23,360 --> 00:09:28,140
is which we showing in

00:09:25,410 --> 00:09:30,360
so we we can assemble output and to

00:09:28,140 --> 00:09:33,649
generate the whole apple of these layers

00:09:30,360 --> 00:09:37,110
so also we try the different

00:09:33,649 --> 00:09:40,080
optimization schemes such as opening we

00:09:37,110 --> 00:09:43,860
and the conversation we cut the network

00:09:40,080 --> 00:09:49,110
and at the end that network is reduced

00:09:43,860 --> 00:09:53,040
from 2.20 to G up to the 1.25 45 geodes

00:09:49,110 --> 00:09:55,019
so it's one and a half times reduced and

00:09:53,040 --> 00:09:57,540
we also have different conversation

00:09:55,019 --> 00:10:00,950
scheme we try to use 16-bit and half bid

00:09:57,540 --> 00:10:03,839
for the feature map and the parameters

00:10:00,950 --> 00:10:07,100
respectively and also we used Cafe to

00:10:03,839 --> 00:10:12,450
retrain the whole model to make sure

00:10:07,100 --> 00:10:15,570
suffer a pop and attack before the

00:10:12,450 --> 00:10:20,100
memory issue is pretty insane if acun so

00:10:15,570 --> 00:10:21,839
we try to decide a hierarchy memory so

00:10:20,100 --> 00:10:26,329
the first step is to turn the

00:10:21,839 --> 00:10:30,089
multi-dimensional weight into a linear

00:10:26,329 --> 00:10:32,640
sequences so here on the left side the

00:10:30,089 --> 00:10:34,950
multi-dimensional weight is turning into

00:10:32,640 --> 00:10:37,740
linear sequence here and we also insert

00:10:34,950 --> 00:10:41,070
a FIFO and the pimping part between the

00:10:37,740 --> 00:10:43,740
memory and commutation or engined so the

00:10:41,070 --> 00:10:46,290
FIFO is was for pre fraction a small

00:10:43,740 --> 00:10:48,570
amount of data and also here payment

00:10:46,290 --> 00:10:51,630
buffers were for height and memory

00:10:48,570 --> 00:10:54,630
access latency so in this decide we can

00:10:51,630 --> 00:10:57,300
use the external external memory but we

00:10:54,630 --> 00:11:00,810
will know in introduce the rotational

00:10:57,300 --> 00:11:03,420
latency also if we try to use traffic

00:11:00,810 --> 00:11:06,779
bit parameter just the other issue which

00:11:03,420 --> 00:11:09,390
means a bit here are not going to use

00:11:06,779 --> 00:11:13,860
because we are using 512 bit bus

00:11:09,390 --> 00:11:20,430
interface so to to make the to solve

00:11:13,860 --> 00:11:23,399
this problem we actually cut data into

00:11:20,430 --> 00:11:26,070
three pieces and we try to access the

00:11:23,399 --> 00:11:28,980
same data in three times and at the end

00:11:26,070 --> 00:11:31,230
we assemble the data and so in this case

00:11:28,980 --> 00:11:35,160
we can fully use the memory bandwidth

00:11:31,230 --> 00:11:37,010
and there's no West doesn't no waste

00:11:35,160 --> 00:11:40,020
here

00:11:37,010 --> 00:11:44,100
so here I am going to talk about the

00:11:40,020 --> 00:11:45,570
food system design so in the cap in the

00:11:44,100 --> 00:11:47,820
gravity side we also have different

00:11:45,570 --> 00:11:50,430
challenge here the first challenge is

00:11:47,820 --> 00:11:53,190
relate to the X scene interface because

00:11:50,430 --> 00:11:56,820
we are trying to use the XO as decipher

00:11:53,190 --> 00:11:59,670
low and accurate as we generate a clean

00:11:56,820 --> 00:12:03,110
interface which is work for communicate

00:11:59,670 --> 00:12:06,540
with the whole CPU and your accelerator

00:12:03,110 --> 00:12:08,520
also we need to remove the axiom pass

00:12:06,540 --> 00:12:10,680
because we already have the cabbie in

00:12:08,520 --> 00:12:13,170
the face when we remove the I seen bus

00:12:10,680 --> 00:12:16,770
there's some other issue which means we

00:12:13,170 --> 00:12:18,630
need to communicate with the host CPU so

00:12:16,770 --> 00:12:22,200
we need to design our communication skin

00:12:18,630 --> 00:12:25,400
and if you try to map the large design

00:12:22,200 --> 00:12:28,710
such as alcian we may need to manually

00:12:25,400 --> 00:12:32,250
do the present rod because we are facing

00:12:28,710 --> 00:12:34,800
a very tight timing constraint and also

00:12:32,250 --> 00:12:37,860
the cabbie interface will be serve a

00:12:34,800 --> 00:12:41,880
small amount of resources in the FPGA

00:12:37,860 --> 00:12:47,190
which we may need to have if we try to

00:12:41,880 --> 00:12:49,200
use that portion of the resource so the

00:12:47,190 --> 00:12:54,180
first solution to remove the axon

00:12:49,200 --> 00:12:57,960
interface what we are trying to do is we

00:12:54,180 --> 00:13:02,610
need to prepare a huge chunk of memory

00:12:57,960 --> 00:13:07,200
and we need to specify which offset is

00:13:02,610 --> 00:13:09,810
correct it will layer so it actually

00:13:07,200 --> 00:13:12,690
require a lot a little bit more effort

00:13:09,810 --> 00:13:14,700
when you try to program your accelerator

00:13:12,690 --> 00:13:19,080
because you need to figure out what the

00:13:14,700 --> 00:13:24,240
offset of a certain layer but the good

00:13:19,080 --> 00:13:27,020
in removing the axiom bus actually help

00:13:24,240 --> 00:13:30,480
us to reduce the resource utilization

00:13:27,020 --> 00:13:34,080
here we try the same design and the left

00:13:30,480 --> 00:13:36,270
side is using the the X ataxia interface

00:13:34,080 --> 00:13:41,280
and the right size in the cabby

00:13:36,270 --> 00:13:44,850
interface so we can see there is a 6%

00:13:41,280 --> 00:13:48,330
job of the lot and free path of a

00:13:44,850 --> 00:13:49,740
flip-flop and because we are using we

00:13:48,330 --> 00:13:51,960
try to build

00:13:49,740 --> 00:13:54,590
as more beer and as a buffer so we

00:13:51,960 --> 00:13:57,930
actually use more beer em in this case

00:13:54,590 --> 00:14:03,150
but in the end we can save the lot and

00:13:57,930 --> 00:14:06,450
flips lot so since we are try to decide

00:14:03,150 --> 00:14:11,270
the communication scheme we need to take

00:14:06,450 --> 00:14:14,270
a trench of the curvy interface has two

00:14:11,270 --> 00:14:14,270
scene

00:14:22,980 --> 00:14:32,160
oh yeah it's working it's looking okay

00:14:29,060 --> 00:14:35,370
yeah it provides five different lower

00:14:32,160 --> 00:14:37,980
layer component on top of the PCIe a

00:14:35,370 --> 00:14:41,310
physical channel but we are not directly

00:14:37,980 --> 00:14:46,680
in the red using this five module

00:14:41,310 --> 00:14:50,280
actually on top of them we have a free

00:14:46,680 --> 00:14:54,110
mod abstraction layer which is the

00:14:50,280 --> 00:14:57,300
control DMA and we use the control layer

00:14:54,110 --> 00:14:59,790
or the control model to transfer the

00:14:57,300 --> 00:15:03,300
control signal and DMA transfer the data

00:14:59,790 --> 00:15:07,170
between the host CPU and celebrated so

00:15:03,300 --> 00:15:11,190
on top of this layer is our customized

00:15:07,170 --> 00:15:15,150
accelerator so here is the detail on

00:15:11,190 --> 00:15:17,250
velocity machine design and the major

00:15:15,150 --> 00:15:21,810
finite state machine is on the bottom

00:15:17,250 --> 00:15:24,770
which has five different stage so the

00:15:21,810 --> 00:15:29,250
key component here is the copy and

00:15:24,770 --> 00:15:32,040
processing stage when we step into the

00:15:29,250 --> 00:15:35,130
cat a copy stage actually the data is

00:15:32,040 --> 00:15:39,090
copied from the memory to the local beer

00:15:35,130 --> 00:15:40,580
and here in the FPGA and the finite

00:15:39,090 --> 00:15:43,860
state machine will jump to the

00:15:40,580 --> 00:15:47,160
processing stage and get a signal to the

00:15:43,860 --> 00:15:50,040
ostian IP and arise I see an IP will

00:15:47,160 --> 00:15:56,310
start working and consume the data here

00:15:50,040 --> 00:15:59,070
and after it generate a result it will

00:15:56,310 --> 00:16:03,420
jump to the right stage which means we

00:15:59,070 --> 00:16:06,270
will get the data from the LAN IP and in

00:16:03,420 --> 00:16:09,930
usual to copy and there are several

00:16:06,270 --> 00:16:12,720
iteration here because one copy only

00:16:09,930 --> 00:16:15,630
copy a small portion of data from the

00:16:12,720 --> 00:16:19,140
memory to the local Biron it may take

00:16:15,630 --> 00:16:23,430
several hundreds of iteration and to all

00:16:19,140 --> 00:16:27,150
the data is copied to the VM and is

00:16:23,430 --> 00:16:29,630
processed by the LAN IP and the smaller

00:16:27,150 --> 00:16:32,880
finite state machine is what for

00:16:29,630 --> 00:16:33,750
coordinator input from the local beer

00:16:32,880 --> 00:16:38,610
and and

00:16:33,750 --> 00:16:41,610
septum from 1024-bit to 512-bit and then

00:16:38,610 --> 00:16:44,630
sent to the LRC an IP so here we have

00:16:41,610 --> 00:16:48,680
service depth of the memory

00:16:44,630 --> 00:16:52,020
transformation here and we intend to

00:16:48,680 --> 00:16:56,400
hide the latency because we have several

00:16:52,020 --> 00:16:59,880
power here so in our experiment this

00:16:56,400 --> 00:17:04,560
design actually helped us to UM to hide

00:16:59,880 --> 00:17:07,920
on memory access latency so in the

00:17:04,560 --> 00:17:12,150
future this might be improved but here

00:17:07,920 --> 00:17:14,699
we stick to this current result and in

00:17:12,150 --> 00:17:18,480
summary we have a sixth step to generate

00:17:14,699 --> 00:17:21,270
our design the first step is to use the

00:17:18,480 --> 00:17:24,150
very popular machine learning frameworks

00:17:21,270 --> 00:17:27,870
such as cafe or tensorflow to desire

00:17:24,150 --> 00:17:31,680
module to train your module and we start

00:17:27,870 --> 00:17:33,360
from step 2 and that were pruning which

00:17:31,680 --> 00:17:35,430
means we can cut the network because

00:17:33,360 --> 00:17:37,980
they are going to be done down here so

00:17:35,430 --> 00:17:40,740
we can use less parameters and less

00:17:37,980 --> 00:17:43,860
layer and we can still get the same

00:17:40,740 --> 00:17:46,290
accuracy so we also try to contact

00:17:43,860 --> 00:17:48,750
engine in this step so in the next step

00:17:46,290 --> 00:17:51,810
we reduce our RAM which is the resource

00:17:48,750 --> 00:17:54,900
allocation scheme to generate the

00:17:51,810 --> 00:17:57,480
different allocation scheme for

00:17:54,900 --> 00:18:00,840
different layers so follow this label

00:17:57,480 --> 00:18:03,630
for in this step we can start writing

00:18:00,840 --> 00:18:07,100
our HS code which is basically the C

00:18:03,630 --> 00:18:11,820
code and then we can go for the HQs and

00:18:07,100 --> 00:18:13,470
get the RTL design and once we get the

00:18:11,820 --> 00:18:17,040
RTO decide we can stitch this

00:18:13,470 --> 00:18:19,740
accelerator with the copy framework so

00:18:17,040 --> 00:18:24,150
we can launch the our design on the

00:18:19,740 --> 00:18:25,740
power system so we I'm to demonstrate

00:18:24,150 --> 00:18:29,370
our design we have three different

00:18:25,740 --> 00:18:31,590
strategy which is the base light which

00:18:29,370 --> 00:18:33,600
is very basic version we only generate

00:18:31,590 --> 00:18:37,530
one hour see an IP and try to run this

00:18:33,600 --> 00:18:42,030
IP and in the FPGA and the batch skin

00:18:37,530 --> 00:18:45,120
which is we use two identical errors in

00:18:42,030 --> 00:18:45,890
IP on the same a PJ which I'll use edge

00:18:45,120 --> 00:18:49,640
to

00:18:45,890 --> 00:18:52,480
to simultaneously run to image and

00:18:49,640 --> 00:18:55,570
generate to sentence and we have

00:18:52,480 --> 00:18:58,430
semi-batch in this case we only in

00:18:55,570 --> 00:19:02,000
generate one CNN which is the front end

00:18:58,430 --> 00:19:05,720
of a lion and we have to iron it so in

00:19:02,000 --> 00:19:09,260
this case to image will be processed by

00:19:05,720 --> 00:19:10,880
the same CNN in serial and all the

00:19:09,260 --> 00:19:14,380
feature of these two images will be

00:19:10,880 --> 00:19:18,530
passed to the iron and at the same time

00:19:14,380 --> 00:19:21,350
so here is the baseline we have four

00:19:18,530 --> 00:19:24,860
different skin we try to a cure

00:19:21,350 --> 00:19:27,530
increased resource utilization we try to

00:19:24,860 --> 00:19:31,580
see what's the best performance you can

00:19:27,530 --> 00:19:34,520
get in this particular FPGA so when we

00:19:31,580 --> 00:19:36,770
try to allocate I'm near 16 percent of

00:19:34,520 --> 00:19:40,670
the resource we can reach the highest

00:19:36,770 --> 00:19:44,000
performance and in this case is 17

00:19:40,670 --> 00:19:47,300
frames per second so in the batch mode

00:19:44,000 --> 00:19:49,400
we have two smaller hours an IP we share

00:19:47,300 --> 00:19:53,360
the weight we show the control signal

00:19:49,400 --> 00:19:57,320
but we have a different data path so in

00:19:53,360 --> 00:19:59,810
this case each version can reach 10

00:19:57,320 --> 00:20:04,550
frames per second so we have two so the

00:19:59,810 --> 00:20:07,880
final results 2023 in per second and in

00:20:04,550 --> 00:20:11,690
the semi batch we have a CVO portion

00:20:07,880 --> 00:20:14,720
here this Union we won twice and then

00:20:11,690 --> 00:20:18,380
all the feature will be passed to the

00:20:14,720 --> 00:20:20,960
iron met so that we saw is 15.4 frame

00:20:18,380 --> 00:20:25,480
per second so in summary we also

00:20:20,960 --> 00:20:30,050
compared to the CPU and GPU design or

00:20:25,480 --> 00:20:32,510
this to design our using batch one skin

00:20:30,050 --> 00:20:36,190
which means only one image coming from

00:20:32,510 --> 00:20:40,460
outside and generate one sentence and

00:20:36,190 --> 00:20:43,820
orange dog here is the for orange stall

00:20:40,460 --> 00:20:47,300
here are the baseline and the white dog

00:20:43,820 --> 00:20:51,170
here is the semi batch why the yellow

00:20:47,300 --> 00:20:54,320
dog here rather send the batch design so

00:20:51,170 --> 00:20:57,539
we compare the batch and I do a batch to

00:20:54,320 --> 00:21:01,700
design of the GPU we can see there is

00:20:57,539 --> 00:21:04,470
a huge improvement in terms of the

00:21:01,700 --> 00:21:05,820
efficiency here is the normalized

00:21:04,470 --> 00:21:09,690
throughput per watt

00:21:05,820 --> 00:21:14,850
so we also compared the skin 4 which is

00:21:09,690 --> 00:21:17,309
the batch one version of the GPUs batch

00:21:14,850 --> 00:21:19,710
one version here we have some

00:21:17,309 --> 00:21:22,320
interesting result so in terms of the

00:21:19,710 --> 00:21:28,679
efficiency the FPGA pasties I actually

00:21:22,320 --> 00:21:32,369
get 26.6 X and if we are try to compare

00:21:28,679 --> 00:21:37,109
with the performance here the lamanai

00:21:32,369 --> 00:21:39,659
throughput here we can get 2.1 x so it

00:21:37,109 --> 00:21:42,720
demonstrates the cpu pasties i can have

00:21:39,659 --> 00:21:44,820
a better power efficiency and of course

00:21:42,720 --> 00:21:49,019
in some case it can get better result in

00:21:44,820 --> 00:21:52,549
throughput so here I try to come from

00:21:49,019 --> 00:21:55,830
our work here is we try to implement an

00:21:52,549 --> 00:21:58,349
FPGA based accelerator in the power

00:21:55,830 --> 00:22:00,539
system using cabby interface to help us

00:21:58,349 --> 00:22:03,059
to get rid of the memory limitation

00:22:00,539 --> 00:22:05,549
memory implementation and we try to

00:22:03,059 --> 00:22:07,169
decide our own finite state machine

00:22:05,549 --> 00:22:09,809
because we don't have the axion

00:22:07,169 --> 00:22:14,580
interface we don't have any silence

00:22:09,809 --> 00:22:17,609
generate IP and we try to adapt the

00:22:14,580 --> 00:22:20,509
actual as decide to the kepi environment

00:22:17,609 --> 00:22:24,269
in this case we don't need to write the

00:22:20,509 --> 00:22:27,119
RTL program so it's saved a lot of time

00:22:24,269 --> 00:22:29,369
to using HS and in the end we try

00:22:27,119 --> 00:22:31,349
different strategy we have three

00:22:29,369 --> 00:22:34,669
different strategy to match the

00:22:31,349 --> 00:22:39,149
semi-batch end and the basic version and

00:22:34,669 --> 00:22:44,460
we generated a better result and the CPU

00:22:39,149 --> 00:22:48,320
and GPU solution so I'm testing for my

00:22:44,460 --> 00:22:48,320
talk and thank you for your attention

00:22:48,360 --> 00:22:52,820
[Applause]

00:22:53,250 --> 00:23:10,860
please excuse me is this to try to

00:23:08,460 --> 00:23:15,419
reduce the power consumption using

00:23:10,860 --> 00:23:18,240
cabbie actually using fpj can save your

00:23:15,419 --> 00:23:21,299
power because if you have higher power

00:23:18,240 --> 00:23:24,570
efficiency and happy if you have some

00:23:21,299 --> 00:23:28,260
other issue and power can solve one of

00:23:24,570 --> 00:23:31,470
them which is the memory limitation PJ

00:23:28,260 --> 00:23:34,380
the onboard memory on a high-end a

00:23:31,470 --> 00:23:39,780
pigeon is very small it's like 10

00:23:34,380 --> 00:23:43,049
megabyte so is you can fake not doing

00:23:39,780 --> 00:23:45,539
Network on chip on a PDA so you need to

00:23:43,049 --> 00:23:48,150
use the external memory and right now

00:23:45,539 --> 00:23:51,720
appears on most of the FPGA I see vertex

00:23:48,150 --> 00:23:54,990
design is vertex 7 Syria will use ddr3

00:23:51,720 --> 00:24:00,299
FPGA so you will get for gate of 8

00:23:54,990 --> 00:24:03,600
gigabyte very slow memory and here if

00:24:00,299 --> 00:24:06,990
you're using cabby you can get 500

00:24:03,600 --> 00:24:09,179
gigabyte memory on the CPU side so it's

00:24:06,990 --> 00:24:11,730
off you the limitation of the memory

00:24:09,179 --> 00:24:17,909
when you try to map a huge neural

00:24:11,730 --> 00:24:21,200
network a cake may not enough so we are

00:24:17,909 --> 00:24:24,570
using a PJ for small form for the reduce

00:24:21,200 --> 00:24:28,080
power efficient reduce power and we use

00:24:24,570 --> 00:24:31,530
Kathy to shove some assistant limitation

00:24:28,080 --> 00:24:39,450
in the PJ so in this stage is the memory

00:24:31,530 --> 00:24:43,039
issue yes here is our design for

00:24:39,450 --> 00:24:47,400
inference yes and the chaining path

00:24:43,039 --> 00:24:49,799
happened in the GPU and we try to do

00:24:47,400 --> 00:24:52,799
different infants in FPGA because I

00:24:49,799 --> 00:24:55,770
think it's not me true it's not good for

00:24:52,799 --> 00:24:59,460
people right now to do the inverse in a

00:24:55,770 --> 00:25:02,960
PJ because you can have support library

00:24:59,460 --> 00:25:06,630
you don't have the support programming

00:25:02,960 --> 00:25:09,990
light SDK the library

00:25:06,630 --> 00:25:12,870
so all the thing you need to do is right

00:25:09,990 --> 00:25:15,870
from scratch so no one are willing to do

00:25:12,870 --> 00:25:18,900
that so that's the one job and of

00:25:15,870 --> 00:25:20,970
choosing a PJ for training for up to

00:25:18,900 --> 00:25:24,030
finish or this problem you have a to

00:25:20,970 --> 00:25:25,919
chain to have people to use a PJ for

00:25:24,030 --> 00:25:40,110
training I think more people willing to

00:25:25,919 --> 00:25:42,630
using a PJ yes yeah but chaining can

00:25:40,110 --> 00:25:44,760
happen in the cloud site right you can

00:25:42,630 --> 00:25:47,220
use crowd for training but when you talk

00:25:44,760 --> 00:25:49,559
about inverse is more diverse you can

00:25:47,220 --> 00:25:52,350
use each device you can use crowd device

00:25:49,559 --> 00:25:53,419
when you talk about some IOT device you

00:25:52,350 --> 00:26:04,380
care about power

00:25:53,419 --> 00:26:06,890
so in that case FPGA can help you mean

00:26:04,380 --> 00:26:06,890
DSP

00:26:09,090 --> 00:26:24,840
okay um actually we don't involve TSP

00:26:13,830 --> 00:26:28,620
here and we try to use the actually the

00:26:24,840 --> 00:26:32,850
DSP are using inside a PJ you know the

00:26:28,620 --> 00:26:36,179
most critical result in a PJ actually is

00:26:32,850 --> 00:26:38,360
the DSP unit yes we need to do the

00:26:36,179 --> 00:26:38,360
multiplication

00:26:46,309 --> 00:26:54,960
yes that's true

00:26:48,289 --> 00:26:57,809
yeah but since I'm in the high brand

00:26:54,960 --> 00:27:00,269
through a lot of paper and actually ESP

00:26:57,809 --> 00:27:04,679
is not that popular in machine learning

00:27:00,269 --> 00:27:07,529
community yeah yeah I try to figure out

00:27:04,679 --> 00:27:16,499
I think a pitcher kind of involved the

00:27:07,529 --> 00:27:20,009
DSP yeah okay

00:27:16,499 --> 00:27:25,499
my mode of the LS Ian used more than 300

00:27:20,009 --> 00:27:29,940
and 300 and less than 400 mega boy

00:27:25,499 --> 00:27:34,440
so between free mega so it's just for

00:27:29,940 --> 00:27:37,110
one module so here is a like a test case

00:27:34,440 --> 00:27:38,940
your use of one aeration but what if you

00:27:37,110 --> 00:27:41,730
try to use different module and stuff

00:27:38,940 --> 00:27:45,119
into the same a PG you try to use four

00:27:41,730 --> 00:27:47,399
or eight and that we need to use more

00:27:45,119 --> 00:27:49,730
than one gigabyte memory and to get by

00:27:47,399 --> 00:27:49,730
memory

00:27:58,810 --> 00:28:09,490
yes yes that's true the GPO here

00:28:07,070 --> 00:28:16,040
actually using different data format and

00:28:09,490 --> 00:28:19,460
but we stick to the same patch size so

00:28:16,040 --> 00:28:22,160
here we compare mostly we compare the

00:28:19,460 --> 00:28:26,600
latency not a full put so Foucault is

00:28:22,160 --> 00:28:28,880
not good for a PJ because you don't have

00:28:26,600 --> 00:28:32,120
a high bandwidth memory first you don't

00:28:28,880 --> 00:28:36,620
have that a lot number of the the

00:28:32,120 --> 00:28:40,100
commutation kernel inside and but Nathan

00:28:36,620 --> 00:28:44,030
C can help maybe using the GPU let me

00:28:40,100 --> 00:28:47,120
using the FPGA I mean so in this table

00:28:44,030 --> 00:28:49,580
we use the same page size here's if you

00:28:47,120 --> 00:28:51,740
and GPU is using batch one so of course

00:28:49,580 --> 00:28:54,710
you can just more you can you jib you

00:28:51,740 --> 00:28:58,160
can use patch 100 or 1000 it depends on

00:28:54,710 --> 00:29:05,140
how many memory use but to be a fair

00:28:58,160 --> 00:29:05,140
compare you spoke of them using the h1

00:29:20,140 --> 00:29:28,160
no actually but the ball we use is the

00:29:23,150 --> 00:29:31,480
Alpha data 73 the Alpha data for we plot

00:29:28,160 --> 00:29:35,360
the Alpha data boy into the power 8

00:29:31,480 --> 00:29:38,150
machine so because our data already

00:29:35,360 --> 00:29:41,240
provided carry interface and the power

00:29:38,150 --> 00:29:44,240
ideas of cap interface embedded so we

00:29:41,240 --> 00:29:47,750
can use them so we don't diss our own

00:29:44,240 --> 00:29:49,310
ball we just use this a PJ on top of the

00:29:47,750 --> 00:29:52,550
power alpha data bone

00:29:49,310 --> 00:30:00,050
so we program that PJ and using the

00:29:52,550 --> 00:30:01,910
internal interface to communicate if you

00:30:00,050 --> 00:30:06,410
can write C code you can use actual s

00:30:01,910 --> 00:30:09,530
and basically us is a subset of the seco

00:30:06,410 --> 00:30:13,130
it's simple straightforward cycle but

00:30:09,530 --> 00:30:15,680
you need to think about how was the

00:30:13,130 --> 00:30:18,320
programming strategy we will look like

00:30:15,680 --> 00:30:20,770
in the hardware so you may need to pay

00:30:18,320 --> 00:30:24,170
attention to several memory access issue

00:30:20,770 --> 00:30:27,650
because you are actually programming the

00:30:24,170 --> 00:30:30,920
hardware so it's not straightforward in

00:30:27,650 --> 00:30:33,170
the software but it's it's not difficult

00:30:30,920 --> 00:30:37,100
I think spend one month is more enough

00:30:33,170 --> 00:30:40,910
time enough for software developer to

00:30:37,100 --> 00:30:44,810
learn how to program in the hardware and

00:30:40,910 --> 00:30:47,780
of course if you try to design very high

00:30:44,810 --> 00:30:50,840
efficient module you may need to spend

00:30:47,780 --> 00:30:58,450
more time so but isn't it very easy to

00:30:50,840 --> 00:30:58,450

YouTube URL: https://www.youtube.com/watch?v=xmGKwLtik_g


