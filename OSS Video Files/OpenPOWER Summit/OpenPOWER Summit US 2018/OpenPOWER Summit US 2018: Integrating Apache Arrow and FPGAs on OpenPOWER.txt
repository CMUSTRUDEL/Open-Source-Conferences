Title: OpenPOWER Summit US 2018: Integrating Apache Arrow and FPGAs on OpenPOWER
Publication date: 2018-04-03
Playlist: OpenPOWER Summit US 2018
Description: 
	Johan Peltenburg of Delft University of Technology, discusses Integrating Apache Arrow and FPGAs on OpenPOWER at OpenPOWER Summit 2018.

For more information, please visit: http://www.openpowerfoundation.org
Captions: 
	00:00:00,179 --> 00:00:05,009
welcome everybody thanks to the

00:00:02,310 --> 00:00:07,379
foundation for accepting the speech

00:00:05,009 --> 00:00:10,710
my name is Yan Pelt Murr I'm from the

00:00:07,379 --> 00:00:11,969
Delft University of Technology and we

00:00:10,710 --> 00:00:15,420
have actually been working with the

00:00:11,969 --> 00:00:17,670
framework just presented by Bruno I have

00:00:15,420 --> 00:00:20,279
to say I was quite impressed with the

00:00:17,670 --> 00:00:21,600
framework and we will get to see again

00:00:20,279 --> 00:00:26,340
some of the points that he mentioned in

00:00:21,600 --> 00:00:29,160
the end I want to talk about a thing

00:00:26,340 --> 00:00:32,579
that has been going on in the big data

00:00:29,160 --> 00:00:35,160
analytics cluster computing frameworks

00:00:32,579 --> 00:00:38,129
and and this is an Apache top-level

00:00:35,160 --> 00:00:39,600
project called arrow and it kind of

00:00:38,129 --> 00:00:43,710
solves a problem we were looking at for

00:00:39,600 --> 00:00:47,850
awhile to accelerate stuff in the big

00:00:43,710 --> 00:00:49,800
data analytics world with FPGAs so

00:00:47,850 --> 00:00:51,840
there's a an outline to the talk Google

00:00:49,800 --> 00:00:53,070
so this problem that we had is called

00:00:51,840 --> 00:00:56,760
serialization I will go into that a

00:00:53,070 --> 00:00:58,770
little bit more we will talk about that

00:00:56,760 --> 00:01:03,390
first then we will look at the Apache

00:00:58,770 --> 00:01:05,339
Apache arrow project it's an it

00:01:03,390 --> 00:01:08,220
specifies a formatting of the data in

00:01:05,339 --> 00:01:10,380
memory we built an fu G acceleration

00:01:08,220 --> 00:01:14,340
framework on top of that also on top of

00:01:10,380 --> 00:01:18,060
other technologies such as capi and snap

00:01:14,340 --> 00:01:20,130
and then we did an experiment to see

00:01:18,060 --> 00:01:23,580
what we can do with it and how fast it

00:01:20,130 --> 00:01:27,840
is and then I will conclude conclude the

00:01:23,580 --> 00:01:31,950
talk so let's look at the big data

00:01:27,840 --> 00:01:34,680
analytics landscape there are many many

00:01:31,950 --> 00:01:38,850
frameworks lately that allow you to very

00:01:34,680 --> 00:01:43,049
easily work with a cluster computing

00:01:38,850 --> 00:01:45,479
framework a cluster computing system so

00:01:43,049 --> 00:01:48,119
some well-known are of course Hadoop and

00:01:45,479 --> 00:01:51,270
spark but there are many others most of

00:01:48,119 --> 00:01:55,950
these you see here are Apache projects

00:01:51,270 --> 00:01:57,299
and big data analytics people and

00:01:55,950 --> 00:01:59,490
programmers they like to use these

00:01:57,299 --> 00:02:02,579
frameworks because they're written in

00:01:59,490 --> 00:02:06,990
very high-level languages so that allows

00:02:02,579 --> 00:02:10,020
them to very quickly set up programs

00:02:06,990 --> 00:02:11,970
that run on a big cluster they want to

00:02:10,020 --> 00:02:12,819
do transformations on collections of

00:02:11,970 --> 00:02:15,040
data

00:02:12,819 --> 00:02:17,530
they can be in a database but they can

00:02:15,040 --> 00:02:21,609
also be just in memory they want to do

00:02:17,530 --> 00:02:24,999
maps reductions and also filled

00:02:21,609 --> 00:02:29,200
filtering operations many of these

00:02:24,999 --> 00:02:31,209
frameworks run on are built on Java or

00:02:29,200 --> 00:02:32,109
Scala for example for for SPARC in

00:02:31,209 --> 00:02:35,139
Hadoop

00:02:32,109 --> 00:02:36,549
some parts are written in C++ but this

00:02:35,139 --> 00:02:39,400
actually means that a lot of these

00:02:36,549 --> 00:02:41,290
frameworks run on virtual on a virtual

00:02:39,400 --> 00:02:43,749
machine in this case a Java Virtual

00:02:41,290 --> 00:02:46,569
Machine which if I give examples about

00:02:43,749 --> 00:02:49,359
how things are going internally I will

00:02:46,569 --> 00:02:50,889
use that as an example but others like

00:02:49,359 --> 00:02:55,599
Python and are they run on different

00:02:50,889 --> 00:02:57,159
interpreters so now in this landscape

00:02:55,599 --> 00:02:59,349
there are countless of libraries

00:02:57,159 --> 00:03:04,139
extensions there's a huge variety of

00:02:59,349 --> 00:03:08,439
tools and languages being used and this

00:03:04,139 --> 00:03:13,000
yeah this can this can cause a couple of

00:03:08,439 --> 00:03:15,280
problems so when I start I'm a PhD

00:03:13,000 --> 00:03:17,560
student in Delft and my supervisor asked

00:03:15,280 --> 00:03:20,709
me a simple question he said I want to

00:03:17,560 --> 00:03:22,989
attach an FPGA to spark go and do it

00:03:20,709 --> 00:03:26,889
yeah I thought okay that shouldn't be

00:03:22,989 --> 00:03:29,349
too hard I've done accelerators before

00:03:26,889 --> 00:03:31,569
it's a it's probably gonna work but it

00:03:29,349 --> 00:03:33,819
turned out that yeah we can we could do

00:03:31,569 --> 00:03:35,939
it and actually we did it a couple times

00:03:33,819 --> 00:03:38,709
but the performance wasn't so great and

00:03:35,939 --> 00:03:43,389
and one of the main reasons for this was

00:03:38,709 --> 00:03:45,639
that if you use these kinds of languages

00:03:43,389 --> 00:03:48,159
very high level languages you're losing

00:03:45,639 --> 00:03:50,349
something that you that we all know from

00:03:48,159 --> 00:03:52,180
languages such as C and C++ and

00:03:50,349 --> 00:03:54,009
basically that's that you have your

00:03:52,180 --> 00:03:57,069
total control of what the data looks

00:03:54,009 --> 00:03:58,719
like in memory and that is lost if you

00:03:57,069 --> 00:04:00,579
run on a virtual machine that does all

00:03:58,719 --> 00:04:02,290
these memory management and layouting

00:04:00,579 --> 00:04:08,469
off your objects in memory for you

00:04:02,290 --> 00:04:10,569
that's actually quite quite annoying so

00:04:08,469 --> 00:04:12,819
this was the question

00:04:10,569 --> 00:04:14,590
well in the end this was the question

00:04:12,819 --> 00:04:18,669
that's attaching accelerated to a JVM

00:04:14,590 --> 00:04:20,769
and so what did we see so first of all

00:04:18,669 --> 00:04:23,380
the data is in runtime objects managed

00:04:20,769 --> 00:04:24,530
by the VM so you can ask two questions

00:04:23,380 --> 00:04:26,600
where

00:04:24,530 --> 00:04:29,720
where are these objects where is the

00:04:26,600 --> 00:04:34,669
where are the data well it turns out

00:04:29,720 --> 00:04:36,740
that the VMS they manage this they

00:04:34,669 --> 00:04:38,600
manage the objects in memory and they

00:04:36,740 --> 00:04:41,690
allocated it in some place it's

00:04:38,600 --> 00:04:44,030
somewhere but at the same time this

00:04:41,690 --> 00:04:45,800
memory is subject to garbage collection

00:04:44,030 --> 00:04:47,720
so it might be that the virtual machine

00:04:45,800 --> 00:04:50,020
moves the data around in memory and you

00:04:47,720 --> 00:04:52,490
don't know where it is at some point

00:04:50,020 --> 00:04:53,840
this is like for for a lot of

00:04:52,490 --> 00:04:55,190
programmers this is nice because they

00:04:53,840 --> 00:04:58,430
don't have to think about freeing

00:04:55,190 --> 00:05:01,460
objects for freeing data anymore but for

00:04:58,430 --> 00:05:02,900
an accelerator developer yeah you want

00:05:01,460 --> 00:05:05,540
to know where the data is because

00:05:02,900 --> 00:05:08,350
otherwise you can't really move it to

00:05:05,540 --> 00:05:11,000
the accelerator so this is one problem

00:05:08,350 --> 00:05:13,780
now there's another problem what do what

00:05:11,000 --> 00:05:16,780
does the data look like in memory it's

00:05:13,780 --> 00:05:18,919
for for Java for example the Java

00:05:16,780 --> 00:05:20,960
specification doesn't standardize what

00:05:18,919 --> 00:05:23,000
the data objects actually have to look

00:05:20,960 --> 00:05:24,590
like in memory so it's standardized

00:05:23,000 --> 00:05:27,200
there's a lot of things but not that so

00:05:24,590 --> 00:05:30,950
many different VM implementations can do

00:05:27,200 --> 00:05:33,350
it in any way they want now we usually

00:05:30,950 --> 00:05:36,229
look at the open JDK because we can read

00:05:33,350 --> 00:05:40,729
the source code so and it's also like a

00:05:36,229 --> 00:05:43,970
baseline implementation for a from any

00:05:40,729 --> 00:05:46,970
other jadi case from any other

00:05:43,970 --> 00:05:48,950
implementation sorry of the Java Virtual

00:05:46,970 --> 00:05:51,470
Machine and usually what we see there's

00:05:48,950 --> 00:05:53,419
a header that had hole specific

00:05:51,470 --> 00:05:56,450
information about monitors and locks and

00:05:53,419 --> 00:05:58,520
and treads there's a pointer to the

00:05:56,450 --> 00:06:01,190
class of the object there are many other

00:05:58,520 --> 00:06:03,050
bits and finally we get to the fields of

00:06:01,190 --> 00:06:05,390
the object where actually the data is

00:06:03,050 --> 00:06:07,820
that we're interested in processing and

00:06:05,390 --> 00:06:10,130
even the ordering of this few fields is

00:06:07,820 --> 00:06:12,740
done in a specific way that not might

00:06:10,130 --> 00:06:17,090
not always be suitable for processing on

00:06:12,740 --> 00:06:19,460
your accelerator so usually if you

00:06:17,090 --> 00:06:22,100
wanted to solve this what you had to do

00:06:19,460 --> 00:06:23,900
is go to Cheryl eyes the data you go

00:06:22,100 --> 00:06:25,700
into the data structure you figure out

00:06:23,900 --> 00:06:27,680
where all the fields are and you write

00:06:25,700 --> 00:06:32,030
into some sort of buffer that eventually

00:06:27,680 --> 00:06:35,390
you copy to your accelerator so I can

00:06:32,030 --> 00:06:38,220
give an example about a collection of

00:06:35,390 --> 00:06:40,710
strings for example in a JVM

00:06:38,220 --> 00:06:42,210
so what you have to do you have some

00:06:40,710 --> 00:06:44,970
collection objects so this can be like a

00:06:42,210 --> 00:06:48,860
vector or a dictionary or something

00:06:44,970 --> 00:06:50,940
high-level and usually there will be

00:06:48,860 --> 00:06:53,100
internally the data will be stored

00:06:50,940 --> 00:06:55,350
somehow so in the collection in the case

00:06:53,100 --> 00:06:58,110
of a large collection of strings there

00:06:55,350 --> 00:06:59,670
can be a string array object in there so

00:06:58,110 --> 00:07:01,200
you have to traverse two references

00:06:59,670 --> 00:07:03,870
there that doesn't take a lot of time

00:07:01,200 --> 00:07:06,750
but then for every string you have to

00:07:03,870 --> 00:07:09,240
traverse more references or well see

00:07:06,750 --> 00:07:13,050
people know the dennis pointers the java

00:07:09,240 --> 00:07:15,660
world they call them references so then

00:07:13,050 --> 00:07:17,280
for every string in the arrow you you

00:07:15,660 --> 00:07:19,170
have to traverse a reference to the

00:07:17,280 --> 00:07:20,400
string object and then for every string

00:07:19,170 --> 00:07:22,350
object you have to traverse another

00:07:20,400 --> 00:07:23,610
reference to the character arrays for

00:07:22,350 --> 00:07:26,240
example if you're interested in

00:07:23,610 --> 00:07:28,980
processing the characters in the string

00:07:26,240 --> 00:07:30,780
so if you have a lot of strings this is

00:07:28,980 --> 00:07:32,220
gonna take a lot of time first of all to

00:07:30,780 --> 00:07:36,380
figure out where are all the bytes and

00:07:32,220 --> 00:07:38,640
then you have to make many short copies

00:07:36,380 --> 00:07:40,320
depending on the length of the strings

00:07:38,640 --> 00:07:42,720
but say they are tweets or something

00:07:40,320 --> 00:07:45,450
that's a lot of small copies that you

00:07:42,720 --> 00:07:47,460
have to make and you paid the latency

00:07:45,450 --> 00:07:52,110
every time and it's just not gonna give

00:07:47,460 --> 00:07:55,919
you very very good performance in fact I

00:07:52,110 --> 00:07:59,210
will skip over this one quickly at the

00:07:55,919 --> 00:08:03,690
previous and there was a output power

00:07:59,210 --> 00:08:05,669
workshop at ISC last year and we showed

00:08:03,690 --> 00:08:07,860
this slide already where we try to do

00:08:05,669 --> 00:08:12,330
this as fast as possible and we just

00:08:07,860 --> 00:08:15,840
realized that we're basically looking at

00:08:12,330 --> 00:08:17,220
these these lines here because the top

00:08:15,840 --> 00:08:19,320
ones here are actually cheating and a

00:08:17,220 --> 00:08:22,650
Java programmer would never never ever

00:08:19,320 --> 00:08:27,120
try that method but basically you're

00:08:22,650 --> 00:08:29,280
you're pretty much bound by by you know

00:08:27,120 --> 00:08:31,590
the structure of this data in memory a

00:08:29,280 --> 00:08:33,450
lot and you can't change it as a program

00:08:31,590 --> 00:08:36,300
I mean you can do some tricks maybe but

00:08:33,450 --> 00:08:40,979
then you can process the data like

00:08:36,300 --> 00:08:42,419
you're used to anymore so you can you

00:08:40,979 --> 00:08:44,700
can see the bandwidth here and actually

00:08:42,419 --> 00:08:47,040
if the if the objects are very small

00:08:44,700 --> 00:08:48,779
you're not even gonna go over a gigabyte

00:08:47,040 --> 00:08:50,939
per second of bandwidth

00:08:48,779 --> 00:08:53,069
and nowadays where everybody's talking

00:08:50,939 --> 00:08:55,170
well today we've heard many people talk

00:08:53,069 --> 00:08:57,689
about like okay we're increasing your

00:08:55,170 --> 00:09:00,480
benefit a lot but yeah I mean if you if

00:08:57,689 --> 00:09:01,410
your data structure is nicely contiguous

00:09:00,480 --> 00:09:03,360
in memory

00:09:01,410 --> 00:09:08,100
you're never gonna achieve that that

00:09:03,360 --> 00:09:09,480
bandwidth all right so this this doesn't

00:09:08,100 --> 00:09:12,470
only happen with Java it happens with

00:09:09,480 --> 00:09:14,579
all all kinds of tools that are you know

00:09:12,470 --> 00:09:16,350
written in these high-level languages

00:09:14,579 --> 00:09:18,120
that run on for two machines and

00:09:16,350 --> 00:09:20,309
interpreters that handle all this memory

00:09:18,120 --> 00:09:23,040
management for you and this gives a lot

00:09:20,309 --> 00:09:25,319
of what we call D serialization or

00:09:23,040 --> 00:09:27,329
serialization overhead so it might be

00:09:25,319 --> 00:09:29,220
that you have a sort of a setup like

00:09:27,329 --> 00:09:31,259
this you have some application running

00:09:29,220 --> 00:09:36,240
on top of a Portuguese patchy spark

00:09:31,259 --> 00:09:38,639
there's a JVM there you might want to go

00:09:36,240 --> 00:09:40,920
and send some stuff over the network or

00:09:38,639 --> 00:09:43,259
to disk or to some Python library you're

00:09:40,920 --> 00:09:45,029
using to do some analytics maybe there's

00:09:43,259 --> 00:09:47,639
even a native library and maybe there's

00:09:45,029 --> 00:09:52,170
even an accelerator there that wants to

00:09:47,639 --> 00:09:54,329
use that data and well we've just seen

00:09:52,170 --> 00:09:56,160
that this thing here it just doesn't you

00:09:54,329 --> 00:10:00,300
know it's it's not gonna give you the

00:09:56,160 --> 00:10:03,959
performance that you want so this is

00:10:00,300 --> 00:10:07,170
something that's addressed by the Apache

00:10:03,959 --> 00:10:08,550
Aero project people have recognized this

00:10:07,170 --> 00:10:11,629
and they said well we have so many tools

00:10:08,550 --> 00:10:15,420
so many different platforms why don't we

00:10:11,629 --> 00:10:18,990
why don't we sort of come up with a

00:10:15,420 --> 00:10:21,540
unified format for the data in memory

00:10:18,990 --> 00:10:24,269
and then we build a lot of libraries on

00:10:21,540 --> 00:10:27,029
top of that through which all these

00:10:24,269 --> 00:10:29,279
different languages and interpreters can

00:10:27,029 --> 00:10:31,079
actually access the data right right the

00:10:29,279 --> 00:10:33,209
data structures in that format and read

00:10:31,079 --> 00:10:39,209
from the data structures in the format

00:10:33,209 --> 00:10:41,339
so basically you're kind of trading your

00:10:39,209 --> 00:10:44,639
it costs you a couple more instructions

00:10:41,339 --> 00:10:47,670
of course to go through those library

00:10:44,639 --> 00:10:49,290
calls for the Apache arrow library but

00:10:47,670 --> 00:10:51,110
you gain a lot of speed because you

00:10:49,290 --> 00:10:54,480
don't have to share lies anymore

00:10:51,110 --> 00:10:57,329
many of these projects like Apache spark

00:10:54,480 --> 00:10:59,459
are actively integrating these as exit

00:10:57,329 --> 00:11:01,630
are actively integrating batchi arrow

00:10:59,459 --> 00:11:05,210
into their back and said as we

00:11:01,630 --> 00:11:07,040
so we thought well that's nice and

00:11:05,210 --> 00:11:08,660
there's a lot of implementations going

00:11:07,040 --> 00:11:12,260
on for many different languages and

00:11:08,660 --> 00:11:15,200
frameworks there's even a GPU part of

00:11:12,260 --> 00:11:17,540
Apache Aero that does it for GPUs but

00:11:15,200 --> 00:11:19,940
there was not nobody was working on

00:11:17,540 --> 00:11:23,060
FPGAs so we thought let's let's do that

00:11:19,940 --> 00:11:28,130
we can do that because of several

00:11:23,060 --> 00:11:30,200
reasons the main reason is that the data

00:11:28,130 --> 00:11:33,650
in memory is standardized through the

00:11:30,200 --> 00:11:37,370
format specification of arrow so that

00:11:33,650 --> 00:11:40,880
means that if we know the type of the

00:11:37,370 --> 00:11:44,390
data we also and we know how big or

00:11:40,880 --> 00:11:48,230
where the brother where the data is in

00:11:44,390 --> 00:11:49,810
memory then we can load it on to the

00:11:48,230 --> 00:12:00,530
FPGA

00:11:49,810 --> 00:12:02,600
and what we gain is we don't have to

00:12:00,530 --> 00:12:04,670
serialize anymore if we are able to read

00:12:02,600 --> 00:12:06,860
from that format as well and with an

00:12:04,670 --> 00:12:10,850
FPGA so the same advantage for all these

00:12:06,860 --> 00:12:14,930
other libraries and interfaces goes for

00:12:10,850 --> 00:12:17,830
our our framework as well so we call the

00:12:14,930 --> 00:12:20,240
framework fletcher and it generates

00:12:17,830 --> 00:12:23,620
structures in the fpga to read from the

00:12:20,240 --> 00:12:27,860
arrow format now I will give you a

00:12:23,620 --> 00:12:30,020
super-fast crash course on on the format

00:12:27,860 --> 00:12:32,150
in memory so the goal of the format is

00:12:30,020 --> 00:12:34,760
to store everything in buffers that are

00:12:32,150 --> 00:12:37,040
as contagious as possible it is possible

00:12:34,760 --> 00:12:38,480
to have some fragmentation there but the

00:12:37,040 --> 00:12:44,410
ultimate goal is do not have any

00:12:38,480 --> 00:12:47,290
fragmentation not be burdened by a

00:12:44,410 --> 00:12:50,990
garbage collection if you don't want to

00:12:47,290 --> 00:12:53,620
so I have an example here with with a

00:12:50,990 --> 00:12:56,900
table there's a table there are some

00:12:53,620 --> 00:12:58,550
columns here there are some rows this is

00:12:56,900 --> 00:13:02,060
a column with some floating-point

00:12:58,550 --> 00:13:05,620
numbers this is a column with some some

00:13:02,060 --> 00:13:07,790
strings and this is a column with some

00:13:05,620 --> 00:13:09,680
structure that maybe some programmer

00:13:07,790 --> 00:13:11,060
thought would be nice to have in this

00:13:09,680 --> 00:13:16,220
program

00:13:11,060 --> 00:13:16,910
now aro defines that if you have a

00:13:16,220 --> 00:13:19,580
schema

00:13:16,910 --> 00:13:21,380
well sorry let me put it like this so if

00:13:19,580 --> 00:13:24,260
you have a table like this there needs

00:13:21,380 --> 00:13:25,820
to be a schema that tells you what what

00:13:24,260 --> 00:13:29,210
the types are so this is basically like

00:13:25,820 --> 00:13:31,100
a type definition for the whole table so

00:13:29,210 --> 00:13:33,140
these are the fields of the of the

00:13:31,100 --> 00:13:34,760
schema so in the case of this

00:13:33,140 --> 00:13:37,730
floating-point number here in the first

00:13:34,760 --> 00:13:39,710
column you can just say like oh the name

00:13:37,730 --> 00:13:41,510
of this column is a and it contains

00:13:39,710 --> 00:13:43,540
floats and there's another thing that

00:13:41,510 --> 00:13:47,390
arrow allows you to do and that's to

00:13:43,540 --> 00:13:49,970
specify that data can be null or invalid

00:13:47,390 --> 00:13:53,780
or whatever so that's where the why

00:13:49,970 --> 00:13:55,730
these things here to simplify the other

00:13:53,780 --> 00:13:58,910
examples I left that out in these ones

00:13:55,730 --> 00:14:03,170
so it allows you to define lists of

00:13:58,910 --> 00:14:05,180
other types list of in this case it's a

00:14:03,170 --> 00:14:07,820
character so all the types that are

00:14:05,180 --> 00:14:09,530
fixed with we'd call them primitives but

00:14:07,820 --> 00:14:12,020
you can also make lists of lists or a

00:14:09,530 --> 00:14:15,230
list of structure or a list of lists of

00:14:12,020 --> 00:14:17,000
lists if you wanted to so you're really

00:14:15,230 --> 00:14:20,480
flexible as a programmer to define any

00:14:17,000 --> 00:14:24,710
data type you want and then there's also

00:14:20,480 --> 00:14:26,570
structures which is just a grouping of

00:14:24,710 --> 00:14:28,550
other types so you can also have a

00:14:26,570 --> 00:14:30,320
struct of with lists or something if you

00:14:28,550 --> 00:14:31,490
wanted to just for the sake of the

00:14:30,320 --> 00:14:34,310
demonstration I'm trying to keep it

00:14:31,490 --> 00:14:38,600
simple there are more data types in Aero

00:14:34,310 --> 00:14:40,280
but we do not support them yet so

00:14:38,600 --> 00:14:44,270
there's also dictionaries and union

00:14:40,280 --> 00:14:47,360
types but I will I will just focus on

00:14:44,270 --> 00:14:49,160
these so what arrow will do for you is

00:14:47,360 --> 00:14:51,560
you can build up this table in memory

00:14:49,160 --> 00:14:54,920
and then you will have them in what they

00:14:51,560 --> 00:14:57,380
call buffers so for a fixed width thing

00:14:54,920 --> 00:14:59,270
like a float you have a buffer that just

00:14:57,380 --> 00:15:02,210
holds the data like we're used to in C

00:14:59,270 --> 00:15:03,710
for example a big array with floats but

00:15:02,210 --> 00:15:06,710
because this thing is not able there's

00:15:03,710 --> 00:15:08,750
also a validity bid map so it says it

00:15:06,710 --> 00:15:11,480
has a bit to say if this value is

00:15:08,750 --> 00:15:13,370
actually valid or not so that's two

00:15:11,480 --> 00:15:15,380
buffers and they can be huge

00:15:13,370 --> 00:15:18,500
or it can be small whatever but in any

00:15:15,380 --> 00:15:20,600
case if we go from one framework to the

00:15:18,500 --> 00:15:22,790
other from one tool to the other it

00:15:20,600 --> 00:15:23,930
stays there in memory is not being

00:15:22,790 --> 00:15:25,490
copied the member

00:15:23,930 --> 00:15:27,410
is gonna be shared between the programs

00:15:25,490 --> 00:15:30,140
and through the arrow libraries you can

00:15:27,410 --> 00:15:33,730
access these these elements without

00:15:30,140 --> 00:15:35,839
doing any serialization or copying

00:15:33,730 --> 00:15:37,610
furthermore because the so continuous is

00:15:35,839 --> 00:15:40,029
easier to send it over the network for

00:15:37,610 --> 00:15:42,830
example which these cluster process

00:15:40,029 --> 00:15:44,720
cluster computing frameworks do a lot so

00:15:42,830 --> 00:15:48,110
they might shuffle at some point shuffle

00:15:44,720 --> 00:15:49,850
the data over to other nodes and then if

00:15:48,110 --> 00:15:54,050
it's in a continuous buffer that's gonna

00:15:49,850 --> 00:15:57,470
be faster so then there's also the lists

00:15:54,050 --> 00:16:01,970
so they do this quite nicely so there's

00:15:57,470 --> 00:16:04,339
a there's gonna be a big buffer that

00:16:01,970 --> 00:16:06,140
holds all the all the data that's in all

00:16:04,339 --> 00:16:08,959
of the lists and then it just has a

00:16:06,140 --> 00:16:11,540
second buffer that points you to a

00:16:08,959 --> 00:16:13,190
specific offset for a specific index in

00:16:11,540 --> 00:16:16,640
the table so for example here where the

00:16:13,190 --> 00:16:20,270
word tasty starts I just have to look at

00:16:16,640 --> 00:16:22,310
this index I get an offset of 6 for this

00:16:20,270 --> 00:16:25,730
table so it's basically somewhat like a

00:16:22,310 --> 00:16:27,980
pointer now this is nice because if you

00:16:25,730 --> 00:16:29,410
would just do length for example you

00:16:27,980 --> 00:16:31,970
would have to go through the whole

00:16:29,410 --> 00:16:34,209
buffer to figure out where actually the

00:16:31,970 --> 00:16:36,740
word starts in there in the data buffer

00:16:34,209 --> 00:16:38,300
but but because it's offset you can you

00:16:36,740 --> 00:16:41,720
could process this thing in parallel for

00:16:38,300 --> 00:16:42,950
example now for the structure you just

00:16:41,720 --> 00:16:45,650
basically have two buffers where the

00:16:42,950 --> 00:16:47,779
indexes have to correspond to whatever

00:16:45,650 --> 00:16:50,779
is in the whatever belongs together in

00:16:47,779 --> 00:16:53,029
the structure so we took this format

00:16:50,779 --> 00:16:54,470
specifications again you can make crazy

00:16:53,029 --> 00:16:57,140
schemas with this if you wanted to

00:16:54,470 --> 00:16:58,610
trying to keep it simple here so we took

00:16:57,140 --> 00:17:00,560
the format specification and we thought

00:16:58,610 --> 00:17:05,660
about the following approach to get this

00:17:00,560 --> 00:17:08,390
to FPGA so what we do is we basically

00:17:05,660 --> 00:17:09,980
start with a narrow schema that tells us

00:17:08,390 --> 00:17:12,350
what the format of the data in memory is

00:17:09,980 --> 00:17:15,620
then we throw this into a thing we call

00:17:12,350 --> 00:17:17,600
our interface generation framework this

00:17:15,620 --> 00:17:21,620
is basically what fletcher is our our

00:17:17,600 --> 00:17:25,910
framework you then get some an HDL

00:17:21,620 --> 00:17:27,470
template or and as your template to

00:17:25,910 --> 00:17:30,400
which you can add your own accelerator

00:17:27,470 --> 00:17:34,690
designs so you could use HLS or manual

00:17:30,400 --> 00:17:37,010
HDL coding to get your accelerator

00:17:34,690 --> 00:17:37,820
implementation and you combine this

00:17:37,010 --> 00:17:40,820
together since this

00:17:37,820 --> 00:17:43,790
synthesized place and route and that's

00:17:40,820 --> 00:17:47,120
what you do during compile time so

00:17:43,790 --> 00:17:49,100
during run time we provide you start

00:17:47,120 --> 00:17:51,280
with the data source on disk or from the

00:17:49,100 --> 00:17:55,840
network to the error library if you

00:17:51,280 --> 00:17:59,240
build your error table in memory and

00:17:55,840 --> 00:18:01,790
then on the FPGA side our generated

00:17:59,240 --> 00:18:03,590
interface will be here and the data

00:18:01,790 --> 00:18:05,230
streams are delivered to your hardware

00:18:03,590 --> 00:18:07,670
accelerators function as we call it

00:18:05,230 --> 00:18:13,520
it's nap would be called something like

00:18:07,670 --> 00:18:15,860
an action and this the strength of this

00:18:13,520 --> 00:18:16,970
is that what we allow is that this

00:18:15,860 --> 00:18:19,670
hardware acceleration

00:18:16,970 --> 00:18:21,380
accelerated function can speak about the

00:18:19,670 --> 00:18:24,080
data to the interface in terms of

00:18:21,380 --> 00:18:25,370
indices in the row in the table so it

00:18:24,080 --> 00:18:27,500
means you don't have to provide like a

00:18:25,370 --> 00:18:29,870
byte address or something for your data

00:18:27,500 --> 00:18:32,720
here now you just say from that column I

00:18:29,870 --> 00:18:34,400
want that row and basically the streams

00:18:32,720 --> 00:18:36,400
will provide you with exactly the data

00:18:34,400 --> 00:18:39,440
type that was specified in the schema

00:18:36,400 --> 00:18:43,010
so one thing you have at HLS you're kind

00:18:39,440 --> 00:18:46,100
of limited to the to the data types that

00:18:43,010 --> 00:18:48,260
are known in C just characters integers

00:18:46,100 --> 00:18:51,020
etc but we actually provide the actual

00:18:48,260 --> 00:18:54,800
data type here as a stream that you

00:18:51,020 --> 00:18:59,510
define in a schema so that's quite power

00:18:54,800 --> 00:19:02,270
of powerful I'm not sure how many how

00:18:59,510 --> 00:19:06,230
many people have ever done FPGA

00:19:02,270 --> 00:19:08,570
acceleration but yeah it usually takes a

00:19:06,230 --> 00:19:11,000
lot of time to figure out all the bytes

00:19:08,570 --> 00:19:13,310
addresses the alignments reorder all the

00:19:11,000 --> 00:19:16,430
data and in such a way that it becomes

00:19:13,310 --> 00:19:18,470
usually usable finally in your in your

00:19:16,430 --> 00:19:21,350
in the functional part of your

00:19:18,470 --> 00:19:23,690
accelerator so this is a big advantage

00:19:21,350 --> 00:19:27,620
now let's focus a little bit on this

00:19:23,690 --> 00:19:29,540
this part here depending on what type of

00:19:27,620 --> 00:19:31,310
platform you use this is going to look

00:19:29,540 --> 00:19:34,100
slightly different so this part is

00:19:31,310 --> 00:19:38,060
getting a data to degenerated interface

00:19:34,100 --> 00:19:41,990
now if you use a classical type of

00:19:38,060 --> 00:19:45,050
accelerator setup with PCI maybe on an

00:19:41,990 --> 00:19:47,060
Intel machine or whatever it's gonna

00:19:45,050 --> 00:19:50,540
look like this so

00:19:47,060 --> 00:19:54,920
the FPGA is not able to access the hosts

00:19:50,540 --> 00:19:56,180
memory not easily at least so what you

00:19:54,920 --> 00:20:00,770
have to do is first you have to make a

00:19:56,180 --> 00:20:03,350
copy of your of your data on board

00:20:00,770 --> 00:20:06,380
so most accelerator cars have some DDR

00:20:03,350 --> 00:20:09,290
you just copy the table there and then

00:20:06,380 --> 00:20:11,840
we can start reading from that now with

00:20:09,290 --> 00:20:13,730
with capi we don't have to do that

00:20:11,840 --> 00:20:15,490
because first of all this is a streaming

00:20:13,730 --> 00:20:19,010
interface it just streams the data there

00:20:15,490 --> 00:20:21,770
so we don't have we don't have some sort

00:20:19,010 --> 00:20:23,420
of data locality if you don't want to so

00:20:21,770 --> 00:20:25,640
that means that just streaming them from

00:20:23,420 --> 00:20:27,260
here right away from here to the true

00:20:25,640 --> 00:20:28,580
the generated interface to your hardware

00:20:27,260 --> 00:20:31,490
accelerated function is good enough for

00:20:28,580 --> 00:20:33,080
us so we don't make that extra copy here

00:20:31,490 --> 00:20:35,330
that you would need if you didn't didn't

00:20:33,080 --> 00:20:39,350
have copy if you weren't able to access

00:20:35,330 --> 00:20:43,250
that virtual address space that's that's

00:20:39,350 --> 00:20:44,900
that's a big advantage now looking at

00:20:43,250 --> 00:20:48,410
the time I will not go into too much

00:20:44,900 --> 00:20:52,930
detail about how we do the hardware

00:20:48,410 --> 00:20:56,090
generation this is basically to read

00:20:52,930 --> 00:20:57,800
fixed with data there are some units

00:20:56,090 --> 00:21:00,170
there that figure out everything do the

00:20:57,800 --> 00:21:03,170
reordering for you and provide you with

00:21:00,170 --> 00:21:06,950
the actual stream or some logic there to

00:21:03,170 --> 00:21:08,540
interface with the host memory you just

00:21:06,950 --> 00:21:10,550
provided with the first and the last

00:21:08,540 --> 00:21:13,880
index in arranged from the table that

00:21:10,550 --> 00:21:16,190
you want to load so there's the same for

00:21:13,880 --> 00:21:20,630
variable length data we actually combine

00:21:16,190 --> 00:21:23,870
two of those previous blocks to turn the

00:21:20,630 --> 00:21:26,390
offsets into new commands for the other

00:21:23,870 --> 00:21:28,670
block that then requests the appropriate

00:21:26,390 --> 00:21:31,940
range of the data here and then streams

00:21:28,670 --> 00:21:34,000
it to the user again seeing the time I'm

00:21:31,940 --> 00:21:36,680
just gonna skip a little bit of this

00:21:34,000 --> 00:21:39,620
also the struct is just two of those

00:21:36,680 --> 00:21:41,060
together now there is a lot of other

00:21:39,620 --> 00:21:42,770
hardware features I have to thank my

00:21:41,060 --> 00:21:47,480
colleague you do a lot for this he's a

00:21:42,770 --> 00:21:48,770
amazing feature our programmer all kinds

00:21:47,480 --> 00:21:54,530
of stuff experiment risible in our

00:21:48,770 --> 00:21:57,110
framework so we wanted to validate the

00:21:54,530 --> 00:21:59,840
proper functioning of our framework so

00:21:57,110 --> 00:22:00,419
what we did we just generate over 10,000

00:21:59,840 --> 00:22:03,330
random scan

00:22:00,419 --> 00:22:07,499
mas that get increasingly more like a

00:22:03,330 --> 00:22:09,119
primitive as they grow so for example

00:22:07,499 --> 00:22:12,230
this crazy schema here I'm not even sure

00:22:09,119 --> 00:22:16,470
if anybody would ever think of that but

00:22:12,230 --> 00:22:20,549
we generate test vectors and then we run

00:22:16,470 --> 00:22:22,769
in simulation the you know the requests

00:22:20,549 --> 00:22:25,590
and we check the streams output if it's

00:22:22,769 --> 00:22:28,379
correct so all of this works it's been

00:22:25,590 --> 00:22:30,659
quite well tested and in the end of

00:22:28,379 --> 00:22:34,049
course it only works if it actually

00:22:30,659 --> 00:22:38,249
works in reality of course so we did a

00:22:34,049 --> 00:22:42,389
little experiment we used two systems

00:22:38,249 --> 00:22:47,669
one system is the Amazon system they

00:22:42,389 --> 00:22:50,070
have a cloud-based system that that yeah

00:22:47,669 --> 00:22:52,710
they provide a VGA accelerators on them

00:22:50,070 --> 00:22:56,159
it's called the easy to f1 they have 30

00:22:52,710 --> 00:22:58,409
ultra fuel plus accelerators are

00:22:56,159 --> 00:23:01,950
officially it's not known what the what

00:22:58,409 --> 00:23:05,759
the card is I think I know but it's a

00:23:01,950 --> 00:23:09,659
big FPGA we have on the other hand the

00:23:05,759 --> 00:23:13,549
power it capi system this the one we use

00:23:09,659 --> 00:23:16,109
had a KO tree and we basically have an

00:23:13,549 --> 00:23:19,710
experiment where we doing regular

00:23:16,109 --> 00:23:21,179
expression matching and we have a little

00:23:19,710 --> 00:23:23,489
design here that is the same for both

00:23:21,179 --> 00:23:24,989
systems so the lower part is the same

00:23:23,489 --> 00:23:28,460
for both systems we also have some

00:23:24,989 --> 00:23:32,249
abstraction layers in our framework and

00:23:28,460 --> 00:23:33,869
there's one of those things that we

00:23:32,249 --> 00:23:37,230
generate from the schemas we call the

00:23:33,869 --> 00:23:40,129
column reader and this one is gonna do

00:23:37,230 --> 00:23:42,570
the bus requests on the host memory and

00:23:40,129 --> 00:23:44,220
it provides the streams to the

00:23:42,570 --> 00:23:48,570
functional units and these are the

00:23:44,220 --> 00:23:50,700
little purple things here and they're

00:23:48,570 --> 00:23:53,999
actually the regular expression matching

00:23:50,700 --> 00:23:56,220
units so we have many of these so we do

00:23:53,999 --> 00:23:58,649
16 of them in parallel and then we also

00:23:56,220 --> 00:24:00,840
paralyze this whole thing so they all

00:23:58,649 --> 00:24:03,929
process the same string and then we have

00:24:00,840 --> 00:24:06,809
multiple of these in parallel as well so

00:24:03,929 --> 00:24:09,299
this guy was able to fit 16 of those

00:24:06,809 --> 00:24:10,799
well actually did we could put way more

00:24:09,299 --> 00:24:12,580
of those but you will see it didn't

00:24:10,799 --> 00:24:15,580
matter much

00:24:12,580 --> 00:24:17,830
this is a much smaller FPGA we were able

00:24:15,580 --> 00:24:19,120
to fit eight of them comfortably I also

00:24:17,830 --> 00:24:24,250
have one with ten but that didn't really

00:24:19,120 --> 00:24:27,010
help much so we have twice as many of

00:24:24,250 --> 00:24:30,340
them being matched in parallel and and

00:24:27,010 --> 00:24:33,580
half of that here so on the top part for

00:24:30,340 --> 00:24:36,010
power it with capi we're using the snap

00:24:33,580 --> 00:24:37,870
flavour there so I'm I'm not I'm not

00:24:36,010 --> 00:24:39,760
that actual s guy so I this was just

00:24:37,870 --> 00:24:42,490
normal video but still it was really

00:24:39,760 --> 00:24:46,120
easy to get all this connected once you

00:24:42,490 --> 00:24:48,279
once you're using a X I it's it worked

00:24:46,120 --> 00:24:50,710
very well so one of the great advantages

00:24:48,279 --> 00:24:53,679
for this for the snap framework and

00:24:50,710 --> 00:24:55,960
that's also a the lip see accel and

00:24:53,679 --> 00:24:58,330
decel thing is that you can I actually

00:24:55,960 --> 00:25:04,299
do hardware software co-simulation

00:24:58,330 --> 00:25:06,250
so like Brunel was saying I mean if you

00:25:04,299 --> 00:25:09,250
if you have some sort of error in your

00:25:06,250 --> 00:25:13,179
software or in your heart where you will

00:25:09,250 --> 00:25:15,520
be able to the PSL simulation engine to

00:25:13,179 --> 00:25:17,890
find that error and you don't have to

00:25:15,520 --> 00:25:21,220
build your whole fpga image first to to

00:25:17,890 --> 00:25:23,409
fit to figure it out so on this system

00:25:21,220 --> 00:25:28,539
you also have simulation but either you

00:25:23,409 --> 00:25:30,100
do fairlock simulation yeah but for your

00:25:28,539 --> 00:25:31,720
software part of the whole thing you

00:25:30,100 --> 00:25:33,669
don't really you can't really connect

00:25:31,720 --> 00:25:38,470
with the simulator that's a bit of a

00:25:33,669 --> 00:25:40,950
drawback here this is a bit of a this is

00:25:38,470 --> 00:25:44,110
a customized thing that Amazon builds

00:25:40,950 --> 00:25:45,549
this is classical set up for for an fpga

00:25:44,110 --> 00:25:49,330
accelerator i'm gonna go into too much

00:25:45,549 --> 00:25:52,570
detail so the results are are more

00:25:49,330 --> 00:25:54,100
interesting so on the Left we have one

00:25:52,570 --> 00:25:56,620
of those instances this is an Intel

00:25:54,100 --> 00:25:59,710
machine it has only eight Hardware

00:25:56,620 --> 00:26:03,490
treads available for us so what we did

00:25:59,710 --> 00:26:06,039
is we measured how long java took to do

00:26:03,490 --> 00:26:08,230
this job how long java did it with eight

00:26:06,039 --> 00:26:10,330
threads we have a very fast c++

00:26:08,230 --> 00:26:13,480
implementation that we also do with

00:26:10,330 --> 00:26:16,950
eight threads we also do it where we

00:26:13,480 --> 00:26:20,440
store the data in the arrow format and

00:26:16,950 --> 00:26:23,800
we also have the FPGA implementation

00:26:20,440 --> 00:26:25,390
so on the x-axis this is the amount of

00:26:23,800 --> 00:26:27,580
bytes of the whole data set including

00:26:25,390 --> 00:26:31,120
the you know including all the buffers

00:26:27,580 --> 00:26:34,090
and this is the run time so we can see

00:26:31,120 --> 00:26:36,190
that yeah the single-threaded

00:26:34,090 --> 00:26:38,080
implementations on the on the Intel

00:26:36,190 --> 00:26:41,050
machine are a bit better but the

00:26:38,080 --> 00:26:42,610
multi-threaded implementations of course

00:26:41,050 --> 00:26:44,650
we have way more treads on the power

00:26:42,610 --> 00:26:46,420
machine so they're they're much better

00:26:44,650 --> 00:26:49,860
starting up the threads for low data

00:26:46,420 --> 00:26:52,060
sizes taking taking some time there

00:26:49,860 --> 00:26:54,460
now the FPGA part is the most

00:26:52,060 --> 00:26:56,170
interesting part of course for us well

00:26:54,460 --> 00:26:57,640
of course the first thing we were happy

00:26:56,170 --> 00:27:01,120
with is that's actually faster than the

00:26:57,640 --> 00:27:02,470
software that's always good so on the

00:27:01,120 --> 00:27:05,440
next slide I have a slightly more

00:27:02,470 --> 00:27:08,140
interesting picture related to what we

00:27:05,440 --> 00:27:11,980
build and this is the this is the

00:27:08,140 --> 00:27:16,230
runtime is zoomed in for the one for a

00:27:11,980 --> 00:27:19,260
data set of one gigabyte of of strings

00:27:16,230 --> 00:27:22,060
slightly over one gigabyte of strings so

00:27:19,260 --> 00:27:24,100
yeah I mean this took so long and I

00:27:22,060 --> 00:27:28,210
didn't want to make it a logarithmic

00:27:24,100 --> 00:27:30,190
plot here but yes we can see that Java

00:27:28,210 --> 00:27:32,140
just takes a lot of time even even if

00:27:30,190 --> 00:27:34,510
you spawn more threads it's gonna be

00:27:32,140 --> 00:27:40,090
much better but still taking a lot of

00:27:34,510 --> 00:27:42,010
time and the FPGA just takes way less

00:27:40,090 --> 00:27:44,440
time regular expression matching is a

00:27:42,010 --> 00:27:45,910
quite well-known example of an

00:27:44,440 --> 00:27:49,330
application that works well on FPGA

00:27:45,910 --> 00:27:51,280
that's why we we picked it so now the

00:27:49,330 --> 00:27:53,350
interesting thing to look here is I also

00:27:51,280 --> 00:27:55,660
plot the time for serializing that data

00:27:53,350 --> 00:27:57,880
set that's the thing we were trying to

00:27:55,660 --> 00:27:59,410
solve right so how much time does it

00:27:57,880 --> 00:28:02,080
take to serialize that data set before I

00:27:59,410 --> 00:28:04,420
could send it to the FPGA now in Java

00:28:02,080 --> 00:28:07,720
took us 3.14 seconds to serialize that

00:28:04,420 --> 00:28:09,550
data as C++ it because 1.99 for the

00:28:07,720 --> 00:28:14,470
amazon system on the power system is

00:28:09,550 --> 00:28:16,080
slightly more 5.34 and 3.6 for Java and

00:28:14,470 --> 00:28:22,510
C++ respectively

00:28:16,080 --> 00:28:25,030
now if the FPGA just takes well 1/2 or

00:28:22,510 --> 00:28:26,940
1/3 of a second to process that data but

00:28:25,030 --> 00:28:29,740
you're Cheryl ization is this much

00:28:26,940 --> 00:28:32,110
actually in this case it will even be it

00:28:29,740 --> 00:28:33,760
will even take more time to serialize

00:28:32,110 --> 00:28:36,850
and process the data on an FPGA

00:28:33,760 --> 00:28:39,100
and to just do it on C++ with all the

00:28:36,850 --> 00:28:41,650
threads enabled on the power system so

00:28:39,100 --> 00:28:44,770
even if you have this tremendous

00:28:41,650 --> 00:28:47,350
throughput in the hardware the software

00:28:44,770 --> 00:28:50,710
stack also has to kind of you know

00:28:47,350 --> 00:28:52,680
correspond to that a little bit and

00:28:50,710 --> 00:28:55,150
using all these very high-level things

00:28:52,680 --> 00:29:00,640
yeah it's not going to be good enough so

00:28:55,150 --> 00:29:03,420
I think you know combining Apache Aero

00:29:00,640 --> 00:29:06,610
with with with our framework has been a

00:29:03,420 --> 00:29:08,170
quite nice experience so far it's it's

00:29:06,610 --> 00:29:10,270
doing exactly what we hoped it would do

00:29:08,170 --> 00:29:12,490
it is just gonna get rid of the

00:29:10,270 --> 00:29:14,590
serialization overhead for us that is

00:29:12,490 --> 00:29:18,220
usually gonna kill the performance in

00:29:14,590 --> 00:29:21,610
many cases that we can do now here you

00:29:18,220 --> 00:29:23,050
have the true put on both machines one

00:29:21,610 --> 00:29:26,080
nice thing that we saw is even though we

00:29:23,050 --> 00:29:27,550
have less functional units I mean

00:29:26,080 --> 00:29:30,730
actually in both cases it's bounded by

00:29:27,550 --> 00:29:32,680
the by the bandwidth this is on copy 1.0

00:29:30,730 --> 00:29:36,280
still I'm hoping to do this on copy to

00:29:32,680 --> 00:29:37,780
point oh soon yeah I mean the bandwidth

00:29:36,280 --> 00:29:40,750
is just much better and it's also much

00:29:37,780 --> 00:29:42,340
easier to do it on with Cathy and snap

00:29:40,750 --> 00:29:45,990
because we don't actually have to copy

00:29:42,340 --> 00:29:48,790
the data to the local DDR memory first

00:29:45,990 --> 00:29:51,970
so we get about two three gigabytes per

00:29:48,790 --> 00:29:54,970
second total application to boot so we

00:29:51,970 --> 00:29:56,590
were we're kind of close to that big

00:29:54,970 --> 00:29:59,710
band hood that we saw and one of the

00:29:56,590 --> 00:30:02,710
previous slides of 3.8 or something of

00:29:59,710 --> 00:30:04,450
read bandwidth from captive 1.0 so that

00:30:02,710 --> 00:30:09,160
also shows that our framework itself is

00:30:04,450 --> 00:30:11,380
quite efficient already yeah so amazon

00:30:09,160 --> 00:30:13,720
says that their system can go up to 1.5

00:30:11,380 --> 00:30:17,110
gigabytes per second we did some tricks

00:30:13,720 --> 00:30:20,590
to make it go faster I still have to

00:30:17,110 --> 00:30:24,850
tell them what to do how I did that I

00:30:20,590 --> 00:30:26,740
think but yeah this has to go to the OS

00:30:24,850 --> 00:30:28,360
so it's much slower than what we are

00:30:26,740 --> 00:30:30,970
used to from Kathy we don't have to go

00:30:28,360 --> 00:30:35,050
to the OS just give it a pointer of the

00:30:30,970 --> 00:30:38,950
buffer and it starts going all right to

00:30:35,050 --> 00:30:40,210
conclude my my talk serialization make a

00:30:38,950 --> 00:30:41,710
significant bottlenecks in big data

00:30:40,210 --> 00:30:45,610
frameworks we've seen that from previous

00:30:41,710 --> 00:30:47,260
measurements - it's going to prevent the

00:30:45,610 --> 00:30:49,780
effective deployment of accelerated

00:30:47,260 --> 00:30:51,940
in some cases especially in these guys I

00:30:49,780 --> 00:30:54,450
work on very high level language

00:30:51,940 --> 00:30:56,470
frameworks well patch ero has been

00:30:54,450 --> 00:30:58,600
designed to alleviate these bottlenecks

00:30:56,470 --> 00:31:01,270
and we've shown that we can you can also

00:30:58,600 --> 00:31:03,070
do that for FPGAs and actually you can

00:31:01,270 --> 00:31:05,560
generate a nice interface on the FPGA

00:31:03,070 --> 00:31:06,810
that will increase the program ability

00:31:05,560 --> 00:31:10,020
for that one as well

00:31:06,810 --> 00:31:12,700
now we made it work on snap on our

00:31:10,020 --> 00:31:14,920
repositories open sourced actually

00:31:12,700 --> 00:31:18,700
there's in a little tutorial how you can

00:31:14,920 --> 00:31:21,130
how you can run it on snap so if there

00:31:18,700 --> 00:31:23,760
is any questions I would be happy to

00:31:21,130 --> 00:31:23,760
address those

00:31:30,680 --> 00:31:33,430
that's right

00:31:48,400 --> 00:31:51,059
yes

00:31:53,680 --> 00:31:56,400
okay

00:31:56,460 --> 00:32:02,280
yeah let me go go there so this is what

00:32:00,060 --> 00:32:05,010
you would normally do right because all

00:32:02,280 --> 00:32:07,050
the data is in this JVM here and you

00:32:05,010 --> 00:32:09,000
have to serialize it before you can send

00:32:07,050 --> 00:32:10,620
it to be accelerated exactly because of

00:32:09,000 --> 00:32:14,340
this problem the data structure is not

00:32:10,620 --> 00:32:17,160
the one you want in your FPGA so we

00:32:14,340 --> 00:32:19,380
looked at the arrow format and we said

00:32:17,160 --> 00:32:23,250
well that's a pretty nice format that

00:32:19,380 --> 00:32:25,680
can actually also work on FPGA now this

00:32:23,250 --> 00:32:28,380
is this is how arrow works it there's

00:32:25,680 --> 00:32:31,470
some sort of layer that will will make

00:32:28,380 --> 00:32:33,630
sure that this the the the data and

00:32:31,470 --> 00:32:38,180
memory is formatted according to that

00:32:33,630 --> 00:32:38,180
specification okay so

00:32:41,890 --> 00:32:49,649
yep yeah yeah exactly yeah any other

00:32:47,169 --> 00:32:49,649
questions

00:32:58,440 --> 00:33:09,820
okay yeah so this is the speed of

00:33:06,190 --> 00:33:11,770
compared to one thread for yeah for the

00:33:09,820 --> 00:33:15,540
various types of things that we tried to

00:33:11,770 --> 00:33:15,540
serialize as fast as possible

00:33:16,980 --> 00:33:25,450
that no no that this point three one one

00:33:22,390 --> 00:33:28,150
is 101 is the speed-up of one so it's a

00:33:25,450 --> 00:33:30,150
point three is a slowdown of yeah that's

00:33:28,150 --> 00:33:32,620
thirty percent of what the original yeah

00:33:30,150 --> 00:33:34,570
you're looking at this of course so it

00:33:32,620 --> 00:33:36,340
gets slower as you spawn more treads and

00:33:34,570 --> 00:33:39,790
this is because there's a lot of

00:33:36,340 --> 00:33:41,170
resource contention and multiple threads

00:33:39,790 --> 00:33:44,980
trying to access the same objects in

00:33:41,170 --> 00:33:46,450
Java at some point it's just gonna yeah

00:33:44,980 --> 00:33:47,920
you have these locks and monitors and

00:33:46,450 --> 00:33:49,870
everything that are just gonna slow it

00:33:47,920 --> 00:33:52,060
down this is what we experienced here

00:33:49,870 --> 00:33:54,370
I'm more of a hardware guy and probably

00:33:52,060 --> 00:34:00,520
Java people will tell me okay you can do

00:33:54,370 --> 00:34:03,640
it in this very you very exotic way I'm

00:34:00,520 --> 00:34:06,280
not saying that this is like the best

00:34:03,640 --> 00:34:08,080
ever but this is what we see so yeah a

00:34:06,280 --> 00:34:10,060
couple of treads will do fine until you

00:34:08,080 --> 00:34:12,310
know it just just collapses there for

00:34:10,060 --> 00:34:14,860
this for this implementation at least

00:34:12,310 --> 00:34:16,659
yeah this is the Jain I think yeah

00:34:14,860 --> 00:34:19,830
that's the classical way of getting your

00:34:16,659 --> 00:34:23,020
data out of Java into Sealand

00:34:19,830 --> 00:34:24,820
but it's a good question yeah we have a

00:34:23,020 --> 00:34:28,510
paper on this end of published in the

00:34:24,820 --> 00:34:32,880
workshop at ISC 2017 so you can read the

00:34:28,510 --> 00:34:35,140
details any other questions

00:34:32,880 --> 00:34:37,830
all right so thank you for your

00:34:35,140 --> 00:34:37,830

YouTube URL: https://www.youtube.com/watch?v=BUkVXjAxQRE


