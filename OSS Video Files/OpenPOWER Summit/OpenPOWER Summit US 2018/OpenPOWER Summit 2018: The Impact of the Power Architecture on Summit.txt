Title: OpenPOWER Summit 2018: The Impact of the Power Architecture on Summit
Publication date: 2018-04-05
Playlist: OpenPOWER Summit US 2018
Description: 
	Christopher Zimmer of Oak Ridge National Laboratory shares the impact of Power architecture on the Summit supercomputer at the OpenPOWER Summit 2018. 

For more information, please visit: http://www.openpowerfoundation.org
Captions: 
	00:00:00,000 --> 00:00:06,870
all right for this talk my name is

00:00:03,149 --> 00:00:09,840
Christopher Zimmer I'm a HPC systems

00:00:06,870 --> 00:00:13,710
engineer with the okra jury Oh courage

00:00:09,840 --> 00:00:16,410
leadership computing facility so I do a

00:00:13,710 --> 00:00:18,600
lot of stuff on the technical side for

00:00:16,410 --> 00:00:20,789
the procurement I'm also involved on the

00:00:18,600 --> 00:00:23,189
acceptance teams for the messaging and

00:00:20,789 --> 00:00:27,810
burst buffer portions of our of summit

00:00:23,189 --> 00:00:29,970
as has been mentioned the Machine is

00:00:27,810 --> 00:00:31,769
coming in right now in December I was

00:00:29,970 --> 00:00:34,980
part of the acceptance team for the

00:00:31,769 --> 00:00:36,719
first thousand 80 nodes it the first

00:00:34,980 --> 00:00:38,309
thousand 80 nodes passed most of their

00:00:36,719 --> 00:00:43,230
acceptance and we've made those

00:00:38,309 --> 00:00:45,480
available to users in early January and

00:00:43,230 --> 00:00:48,510
so today I'm just going to give you kind

00:00:45,480 --> 00:00:51,149
of a brief talk on actually the way the

00:00:48,510 --> 00:00:52,620
power architecture shaped the system

00:00:51,149 --> 00:00:55,289
that summit is and so I'm going to talk

00:00:52,620 --> 00:00:57,629
about some of the architectural features

00:00:55,289 --> 00:00:59,399
and how we're using some of those and

00:00:57,629 --> 00:01:01,559
from a systems level perspective how

00:00:59,399 --> 00:01:03,809
those are going to impact user behaviors

00:01:01,559 --> 00:01:05,280
the title of this talk is the impact of

00:01:03,809 --> 00:01:08,700
the power architecture on the summit

00:01:05,280 --> 00:01:11,520
supercomputer all right so this is the

00:01:08,700 --> 00:01:12,810
de facto who we are slide you guys have

00:01:11,520 --> 00:01:13,950
heard it a bunch today I'm gonna go

00:01:12,810 --> 00:01:17,520
through it again just because my talk

00:01:13,950 --> 00:01:18,869
would only be five minutes without it we

00:01:17,520 --> 00:01:21,990
are the Oakridge leadership computing

00:01:18,869 --> 00:01:24,840
facility we run flagship supercomputers

00:01:21,990 --> 00:01:27,509
for the Department of Energy our current

00:01:24,840 --> 00:01:29,369
flagship supercomputer is Titan it is an

00:01:27,509 --> 00:01:31,170
open science supercomputer so that means

00:01:29,369 --> 00:01:33,630
that if you are from government or

00:01:31,170 --> 00:01:37,259
academia or industry that you can get

00:01:33,630 --> 00:01:38,579
hours on our machine or if you're just a

00:01:37,259 --> 00:01:39,930
personal user I'm sure you could submit

00:01:38,579 --> 00:01:43,649
submit a proposal and Jack would be

00:01:39,930 --> 00:01:46,290
happy to read it access to our machines

00:01:43,649 --> 00:01:49,200
are based on a proposal process that go

00:01:46,290 --> 00:01:51,329
through primarily to programs it is

00:01:49,200 --> 00:01:53,780
evolving but right now those two

00:01:51,329 --> 00:01:56,820
programs are insight and al-sisi

00:01:53,780 --> 00:01:59,759
it's just like an NSF proposal you write

00:01:56,820 --> 00:02:03,270
up your interesting science use cases

00:01:59,759 --> 00:02:05,520
and you submit them to us we review them

00:02:03,270 --> 00:02:11,130
and instead of getting money back you

00:02:05,520 --> 00:02:13,140
get ours on the machines back so

00:02:11,130 --> 00:02:15,360
then the other aspect of that is that

00:02:13,140 --> 00:02:16,770
we're a capability computing center so I

00:02:15,360 --> 00:02:18,450
believe Jack earlier mentioned that

00:02:16,770 --> 00:02:20,370
we're a leadership computing center and

00:02:18,450 --> 00:02:21,780
I think the terminology is in flight

00:02:20,370 --> 00:02:23,820
right now I think historically we've

00:02:21,780 --> 00:02:25,380
been leadership and we're calling

00:02:23,820 --> 00:02:26,880
ourselves the capability center I

00:02:25,380 --> 00:02:28,380
believe at the end of the day it means

00:02:26,880 --> 00:02:30,150
the same thing that we target the

00:02:28,380 --> 00:02:32,900
largest science applications possible

00:02:30,150 --> 00:02:35,220
that you can't run anywhere else so we

00:02:32,900 --> 00:02:38,250
primarily from a scheduling perspective

00:02:35,220 --> 00:02:39,960
will prioritize jobs that run over 20%

00:02:38,250 --> 00:02:42,750
of the machines so on Titan those are

00:02:39,960 --> 00:02:46,800
jobs that are 4,000 nodes or greater are

00:02:42,750 --> 00:02:49,470
what we're prioritizing this contrasts

00:02:46,800 --> 00:02:51,780
to other centers that the DOA runs like

00:02:49,470 --> 00:02:53,460
nurse for instance which are capacity

00:02:51,780 --> 00:02:55,230
centers where they're trying to more

00:02:53,460 --> 00:02:56,850
optimize for the number of jobs that are

00:02:55,230 --> 00:02:59,490
moving through the system through their

00:02:56,850 --> 00:03:02,670
Center overall instead we look at the

00:02:59,490 --> 00:03:05,040
larger jobs as I'm part of the proposal

00:03:02,670 --> 00:03:06,150
process that you actually submit you

00:03:05,040 --> 00:03:11,970
have to demonstrate that your

00:03:06,150 --> 00:03:13,650
applications can actually scale all

00:03:11,970 --> 00:03:17,580
right that was the that was the standard

00:03:13,650 --> 00:03:21,330
disclaimer slide so right now our

00:03:17,580 --> 00:03:23,670
flagship supercomputer is Titan and we

00:03:21,330 --> 00:03:26,070
are transitioning to Summit and so both

00:03:23,670 --> 00:03:28,410
of them are sitting on the floor at the

00:03:26,070 --> 00:03:31,110
exact are you know once well across the

00:03:28,410 --> 00:03:32,630
hall from each other being installed

00:03:31,110 --> 00:03:35,720
right now while the other one is

00:03:32,630 --> 00:03:38,310
operating as our production machine so

00:03:35,720 --> 00:03:40,320
to understand how this kind of all

00:03:38,310 --> 00:03:41,820
meshes together we have to look at where

00:03:40,320 --> 00:03:43,370
we're sitting out right now and how the

00:03:41,820 --> 00:03:46,410
machine has evolved to the next machine

00:03:43,370 --> 00:03:48,530
so Titan is eighteen thousand six

00:03:46,410 --> 00:03:52,140
hundred and eighty eight compute nodes

00:03:48,530 --> 00:03:56,130
put together in a 3d torus network so

00:03:52,140 --> 00:04:00,209
we're talking a 3d torus with dimensions

00:03:56,130 --> 00:04:02,340
of 25 by 16 by 24 so that's 9600 Network

00:04:00,209 --> 00:04:07,620
points that are sitting inside of this

00:04:02,340 --> 00:04:10,230
thin node model Titan is 27 petal flops

00:04:07,620 --> 00:04:14,340
and it's powered through a combination

00:04:10,230 --> 00:04:18,660
of NAMD Interlagos 16 core cpus and a

00:04:14,340 --> 00:04:20,400
single tesla k 20x GPU per node the bulk

00:04:18,660 --> 00:04:22,800
of the flops as you can imagine come

00:04:20,400 --> 00:04:25,740
from the NVIDIA GPU

00:04:22,800 --> 00:04:32,909
each of the nodes has 32 gigabytes of

00:04:25,740 --> 00:04:35,879
ddr3 DRAM and that's Titan

00:04:32,909 --> 00:04:38,159
Oh also the interconnect performance and

00:04:35,879 --> 00:04:40,530
I don't have it listed here is you have

00:04:38,159 --> 00:04:43,199
an injection rate from each node of 5.2

00:04:40,530 --> 00:04:45,539
gigabytes per second the system that

00:04:43,199 --> 00:04:48,180
we're putting on the floor now across

00:04:45,539 --> 00:04:50,550
the hall from it is summit and will be

00:04:48,180 --> 00:04:53,159
when it's fully installed a 200 petaflop

00:04:50,550 --> 00:04:54,629
machine again we've accepted a thousand

00:04:53,159 --> 00:04:57,599
80 nodes of it but it's going to be

00:04:54,629 --> 00:04:59,550
roughly 4,600 nodes I believe most of

00:04:57,599 --> 00:05:01,259
the nodes are actually in now we just

00:04:59,550 --> 00:05:03,449
not have have not accepted those or

00:05:01,259 --> 00:05:06,090
pulled those back in each summit node

00:05:03,449 --> 00:05:10,740
will contain two IBM power nine course

00:05:06,090 --> 00:05:16,800
of 44 course 512 gigabytes of ddr4 DRAM

00:05:10,740 --> 00:05:21,150
and six volt of V 100 GPUs and so it's a

00:05:16,800 --> 00:05:23,370
pretty pretty big jump and so to kind of

00:05:21,150 --> 00:05:25,650
understand what the power architecture

00:05:23,370 --> 00:05:28,860
is bringing to this system we have to

00:05:25,650 --> 00:05:31,080
dive a little bit deeper into what Titan

00:05:28,860 --> 00:05:32,490
is and then you're going to see this the

00:05:31,080 --> 00:05:33,840
same figure that you know he's been

00:05:32,490 --> 00:05:35,819
showing on some it all day and the next

00:05:33,840 --> 00:05:38,039
one so that we can see where we've

00:05:35,819 --> 00:05:41,880
sought to improve some of the the gaps

00:05:38,039 --> 00:05:44,639
on on Titan Titan contained a single x86

00:05:41,880 --> 00:05:47,430
processor the DRAM bus or the system bus

00:05:44,639 --> 00:05:51,629
talked to DRAM at about 55 gigabytes per

00:05:47,430 --> 00:05:54,300
second and then the GPU will sis it was

00:05:51,629 --> 00:05:57,419
sitting on a PCI gen 2 slot a by 16 slot

00:05:54,300 --> 00:05:59,550
by 16 slot can achieve about 6 gigabytes

00:05:57,419 --> 00:06:01,259
per second theoretical when you start

00:05:59,550 --> 00:06:04,080
throwing in PCI overheads you have about

00:06:01,259 --> 00:06:06,509
5 second 5 gigabytes per second out to

00:06:04,080 --> 00:06:08,669
your device this is coupled with the

00:06:06,509 --> 00:06:14,669
fact that the GPU itself has its own 6

00:06:08,669 --> 00:06:17,009
gigabytes of gddr5 i'ts per second so I

00:06:14,669 --> 00:06:19,500
think it's very clear on this slide what

00:06:17,009 --> 00:06:24,229
our weakest link is and moving data into

00:06:19,500 --> 00:06:24,229
our GPUs and how painful that might be

00:06:28,439 --> 00:06:33,719
this is our summit node just got a

00:06:31,030 --> 00:06:37,539
little bit more complicated not too bad

00:06:33,719 --> 00:06:41,199
the summit node will have three GPUs per

00:06:37,539 --> 00:06:44,620
power nine socket each of those GPUs

00:06:41,199 --> 00:06:47,219
will be connected together via 50

00:06:44,620 --> 00:06:49,960
gigabytes per second of NV linked to

00:06:47,219 --> 00:06:51,939
this Contreras contrast with the Sierra

00:06:49,960 --> 00:06:54,159
model which actually has four GPUs

00:06:51,939 --> 00:06:54,939
Sierra model actually has 75 gigabytes

00:06:54,159 --> 00:06:58,750
per second

00:06:54,939 --> 00:07:01,659
between each GPU the to power nines will

00:06:58,750 --> 00:07:05,349
be connected to two banks of 256

00:07:01,659 --> 00:07:09,449
gigabytes of DRAM at about 135 gigabytes

00:07:05,349 --> 00:07:13,479
per second as well as a two-port

00:07:09,449 --> 00:07:15,129
Mellanox EDR HCA which will it's

00:07:13,479 --> 00:07:18,879
connected to about 32 gigabytes per

00:07:15,129 --> 00:07:20,229
second of PCI but it only has about 25

00:07:18,879 --> 00:07:22,750
gigabytes per second of network

00:07:20,229 --> 00:07:24,729
injection rate all of this and we're

00:07:22,750 --> 00:07:25,900
also throwing in an nvme burst buffer

00:07:24,729 --> 00:07:28,180
that's going to be used to be able to

00:07:25,900 --> 00:07:30,879
capture checkpoint data from the system

00:07:28,180 --> 00:07:33,219
and that device will operate at us about

00:07:30,879 --> 00:07:37,599
6 gigabytes per second or 2.2 gigabytes

00:07:33,219 --> 00:07:40,539
per second for writes and reads so I

00:07:37,599 --> 00:07:42,279
think it's pretty clear to you guys

00:07:40,539 --> 00:07:44,080
where I'm going with this what does the

00:07:42,279 --> 00:07:46,680
Power Architecture provide that we

00:07:44,080 --> 00:07:49,060
couldn't have gotten in competing

00:07:46,680 --> 00:07:50,830
architectures today is the IBM

00:07:49,060 --> 00:07:52,690
architecture provides more bandwidth in

00:07:50,830 --> 00:07:53,830
fact it's really disappointing sitting

00:07:52,690 --> 00:07:55,270
in the keynotes this morning and

00:07:53,830 --> 00:07:58,000
watching everybody say the same thing I

00:07:55,270 --> 00:07:59,469
was hoping to be the first anyways so

00:07:58,000 --> 00:08:01,930
we're moving from 55 gigabytes per

00:07:59,469 --> 00:08:03,849
second to 270 gigabytes per second of

00:08:01,930 --> 00:08:06,789
DRAM performance and that stream

00:08:03,849 --> 00:08:09,189
performance 5 to 25 gigabytes per second

00:08:06,789 --> 00:08:11,409
of five to 25 gigabytes per second of

00:08:09,189 --> 00:08:14,080
interconnect performance and an

00:08:11,409 --> 00:08:20,199
aggregate 600 gigabytes per second of

00:08:14,080 --> 00:08:21,789
envy link to performance so the memory

00:08:20,199 --> 00:08:23,740
bandwidth and this slide will be just

00:08:21,789 --> 00:08:26,620
really quick this is just a little bit

00:08:23,740 --> 00:08:28,180
about the memory bandwidth memory

00:08:26,620 --> 00:08:31,539
bandwidth is going to come from 8

00:08:28,180 --> 00:08:33,789
channels of memory bandwidth the power

00:08:31,539 --> 00:08:35,380
architecture is one of the first to get

00:08:33,789 --> 00:08:37,539
this out there we are starting to see

00:08:35,380 --> 00:08:38,770
this now in our arm systems that are

00:08:37,539 --> 00:08:40,039
coming out in fact we just recently

00:08:38,770 --> 00:08:42,919
pulled in

00:08:40,039 --> 00:08:45,740
ooh thunder x2 armed system that will

00:08:42,919 --> 00:08:47,810
have eight channels of memory bandwidth

00:08:45,740 --> 00:08:49,940
available but coupled with the power of

00:08:47,810 --> 00:08:52,220
the power 9 processor is a still more

00:08:49,940 --> 00:08:53,899
attractive system one thing I would like

00:08:52,220 --> 00:08:55,339
to point out is I keep on talking about

00:08:53,899 --> 00:08:57,709
a hundred and thirty-five gigabytes per

00:08:55,339 --> 00:08:59,209
second of performance to DRAM that's is

00:08:57,709 --> 00:09:01,399
actually from the measured performance

00:08:59,209 --> 00:09:03,110
from the stream benchmark the actual

00:09:01,399 --> 00:09:05,269
theoretical performance of the DRAM is

00:09:03,110 --> 00:09:07,339
actually 170 gigabytes per second and

00:09:05,269 --> 00:09:09,980
for for an aggregate from both sockets

00:09:07,339 --> 00:09:13,940
of 340 peak abides per second that's a

00:09:09,980 --> 00:09:16,279
6x jump over what we saw in Titan just a

00:09:13,940 --> 00:09:19,550
traditional SMP bust between the two

00:09:16,279 --> 00:09:21,529
nodes at 64 gigabytes per second nothing

00:09:19,550 --> 00:09:23,509
too fancy here but we're hoping that it

00:09:21,529 --> 00:09:29,509
will reduce some of the Numa impacts on

00:09:23,509 --> 00:09:31,279
the system that we traditionally see so

00:09:29,509 --> 00:09:34,190
one of the the more special things we

00:09:31,279 --> 00:09:37,759
like about this box and one of the

00:09:34,190 --> 00:09:41,750
enabling factors is it's the first

00:09:37,759 --> 00:09:45,019
server to market with PCI gen 4 as far

00:09:41,750 --> 00:09:47,420
as I know Intel and AMD are a little

00:09:45,019 --> 00:09:50,480
further out on this arm has talked about

00:09:47,420 --> 00:09:52,760
it but it's not there yet so what this

00:09:50,480 --> 00:09:55,339
means is that we can pack more into

00:09:52,760 --> 00:09:57,079
every single node it means that we're

00:09:55,339 --> 00:09:59,569
going to be able to you know with PCI

00:09:57,079 --> 00:10:03,170
gen for these EDR ports would have taken

00:09:59,569 --> 00:10:05,779
32 PCI slots instead it's going to sit

00:10:03,170 --> 00:10:08,029
in a by 8 bifurcated slot which means

00:10:05,779 --> 00:10:10,160
that applications running in either Numa

00:10:08,029 --> 00:10:13,459
Numa domain are going to be able to

00:10:10,160 --> 00:10:15,199
access the HC a with almost direct with

00:10:13,459 --> 00:10:16,819
very low latency because they don't have

00:10:15,199 --> 00:10:20,389
to cross the Numa bus to be able to get

00:10:16,819 --> 00:10:22,100
down to the system we used a virtualized

00:10:20,389 --> 00:10:24,380
mechanism to be able to achieve this and

00:10:22,100 --> 00:10:28,339
so actually manifest in the system as 4

00:10:24,380 --> 00:10:31,279
ports we're also able to pack in a by 8

00:10:28,339 --> 00:10:34,819
p.m. 1725 a that's going to be used for

00:10:31,279 --> 00:10:37,160
our check pointing data so if we take a

00:10:34,819 --> 00:10:39,380
look at one of the better systems out

00:10:37,160 --> 00:10:41,000
there in terms of PCI lanes we're going

00:10:39,380 --> 00:10:42,110
to be talking about the AMD epoch system

00:10:41,000 --> 00:10:43,639
the Intel's today I believe are only

00:10:42,110 --> 00:10:47,810
going to support around 80 lanes of PCI

00:10:43,639 --> 00:10:50,360
gen3 so in an equivalent system our HCA

00:10:47,810 --> 00:10:51,829
by itself will take up 32 lanes and if

00:10:50,360 --> 00:10:53,570
you start thinking about you know trying

00:10:51,829 --> 00:10:55,520
to pack GPUs and some of other things

00:10:53,570 --> 00:10:58,910
there you're not gonna really be able to

00:10:55,520 --> 00:11:01,370
do that and so this you know the

00:10:58,910 --> 00:11:04,160
expansion of PCI Gen 4 has allowed us to

00:11:01,370 --> 00:11:05,720
pack more in it also allows us to do

00:11:04,160 --> 00:11:07,100
more with the system should we choose to

00:11:05,720 --> 00:11:09,890
upgrade the system and then in the

00:11:07,100 --> 00:11:11,870
future to HDR we have the bandwidth

00:11:09,890 --> 00:11:14,300
available to get a doubling of our

00:11:11,870 --> 00:11:22,280
network performance should we choose to

00:11:14,300 --> 00:11:25,430
go that route another pretty special

00:11:22,280 --> 00:11:30,350
thing that we have in our system is capi

00:11:25,430 --> 00:11:32,510
to over the is capi to and our system

00:11:30,350 --> 00:11:36,260
we're using cap e to protocol over PCI

00:11:32,510 --> 00:11:38,800
Gen 4 and the goal here is to reduce the

00:11:36,260 --> 00:11:41,330
latency between the host h CA and

00:11:38,800 --> 00:11:44,960
between the host processor in the HCA

00:11:41,330 --> 00:11:46,730
and I can tell you we're expecting

00:11:44,960 --> 00:11:49,370
really good things here again I'm on the

00:11:46,730 --> 00:11:50,750
messaging acceptance tests for the

00:11:49,370 --> 00:11:52,850
actual like bringing the systems on

00:11:50,750 --> 00:11:54,860
their part of these are latency

00:11:52,850 --> 00:11:56,870
measurements for our HPC applications

00:11:54,860 --> 00:12:00,290
and so we run a bunch of latency

00:11:56,870 --> 00:12:02,390
benchmarks and we are seeing very very

00:12:00,290 --> 00:12:04,750
low latency in our preliminary network

00:12:02,390 --> 00:12:07,730
measurements on Summit we're seeing sub

00:12:04,750 --> 00:12:10,670
microsecond trick Layton sees between

00:12:07,730 --> 00:12:13,220
nodes for our communication and part of

00:12:10,670 --> 00:12:18,680
this has been enabled by the the Cappy

00:12:13,220 --> 00:12:20,390
Cappy protocol over PCI Gen 4 the other

00:12:18,680 --> 00:12:24,590
big thing that this is bringing is

00:12:20,390 --> 00:12:28,790
address translation services to the

00:12:24,590 --> 00:12:30,170
participating Cathy devices and so I'm

00:12:28,790 --> 00:12:31,220
gonna go into the slide in this in a

00:12:30,170 --> 00:12:33,350
second that's going to explain this

00:12:31,220 --> 00:12:35,720
better but through a combination of CUDA

00:12:33,350 --> 00:12:38,570
managed memory that comes from the

00:12:35,720 --> 00:12:42,140
Nvidia devices on demand paging that

00:12:38,570 --> 00:12:44,570
comes from the Mellanox devices and

00:12:42,140 --> 00:12:48,200
address translation services that come

00:12:44,570 --> 00:12:50,720
from the Cappy Cappy - we're going to be

00:12:48,200 --> 00:12:53,180
able to access UVM buffers directly from

00:12:50,720 --> 00:12:55,280
the HCA and we're going to use this to

00:12:53,180 --> 00:12:57,380
be able to do what is looks like

00:12:55,280 --> 00:13:00,350
essentially true GPU initiated

00:12:57,380 --> 00:13:01,700
communication and so we're you know it's

00:13:00,350 --> 00:13:04,670
coming in spectrum and it's going to be

00:13:01,700 --> 00:13:06,350
GPU async direct communication but our

00:13:04,670 --> 00:13:07,170
user visible results are going to see

00:13:06,350 --> 00:13:10,079
significant

00:13:07,170 --> 00:13:13,380
reduce latency and being able to run MPI

00:13:10,079 --> 00:13:16,139
sins or MPI receives directly from the

00:13:13,380 --> 00:13:20,220
GPUs themselves and actually have the

00:13:16,139 --> 00:13:22,860
the work done by the GPU and so this is

00:13:20,220 --> 00:13:25,230
a figure that's actually been provided

00:13:22,860 --> 00:13:27,660
to us by Robert Blackmore of the

00:13:25,230 --> 00:13:29,760
spectrum MPI team and this is

00:13:27,660 --> 00:13:33,089
essentially how it works in spectrum MPI

00:13:29,760 --> 00:13:36,149
the CPU is still responsible for setting

00:13:33,089 --> 00:13:38,610
up the IB descriptors but it passes this

00:13:36,149 --> 00:13:41,699
information to the GPU which then fills

00:13:38,610 --> 00:13:44,870
up its data buffers to get ready for MPI

00:13:41,699 --> 00:13:48,360
send the GPU now has the ability to

00:13:44,870 --> 00:13:50,100
write the doorbell on the HCA directly

00:13:48,360 --> 00:13:52,769
and then through those technology

00:13:50,100 --> 00:13:55,410
services as technologies I mentioned on

00:13:52,769 --> 00:13:57,690
demand paging and address translation

00:13:55,410 --> 00:14:00,930
services from cap e to we can pull the

00:13:57,690 --> 00:14:02,820
data off of the GPU to the HCA and put

00:14:00,930 --> 00:14:05,850
it out on the network for transmission

00:14:02,820 --> 00:14:07,589
this is pretty close to GPU initiated

00:14:05,850 --> 00:14:09,360
communication though the only thing the

00:14:07,589 --> 00:14:15,750
GPU is lacking is the ability to set up

00:14:09,360 --> 00:14:17,220
the cue pairs so finally I think one of

00:14:15,750 --> 00:14:18,600
the things that power provides and is

00:14:17,220 --> 00:14:22,130
bringing to us that we didn't have

00:14:18,600 --> 00:14:25,140
before is in viewing 2 to power 9 and so

00:14:22,130 --> 00:14:27,180
you guys are all familiar with dgx boxes

00:14:25,140 --> 00:14:30,899
the Nvidia machine learning boxes with

00:14:27,180 --> 00:14:32,820
two Intel processors and eight GPUs and

00:14:30,899 --> 00:14:35,850
all eight of those GPUs in those boxes

00:14:32,820 --> 00:14:38,399
are working together over MV link but

00:14:35,850 --> 00:14:40,920
what they lack is they lack NV link up

00:14:38,399 --> 00:14:43,740
to the host processors the cpus are

00:14:40,920 --> 00:14:45,720
connected by 16 lanes of PCI gen3 that

00:14:43,740 --> 00:14:48,240
means your hosts can only talk to your

00:14:45,720 --> 00:14:49,470
GPUs as 16 gigabytes per second when

00:14:48,240 --> 00:14:52,380
they need to move data out there and

00:14:49,470 --> 00:14:54,360
that's a bottleneck in the system in our

00:14:52,380 --> 00:14:56,490
system the power 9 is actually

00:14:54,360 --> 00:14:59,250
participating in MV link which is why we

00:14:56,490 --> 00:15:01,769
get 150 gigabytes per second from each

00:14:59,250 --> 00:15:04,949
of our host CPUs to all three of the

00:15:01,769 --> 00:15:07,500
GPUs that are attached to it so this is

00:15:04,949 --> 00:15:09,180
a big deal for us and so if I go back to

00:15:07,500 --> 00:15:10,889
that Titan side I was talking about a

00:15:09,180 --> 00:15:13,550
few minutes ago there was 5 gigabytes

00:15:10,889 --> 00:15:16,319
per second of PCI bandwidth to the GPU

00:15:13,550 --> 00:15:18,089
the impact of this was actually well

00:15:16,319 --> 00:15:18,899
well stated a little while ago we did

00:15:18,089 --> 00:15:21,209
some analysis

00:15:18,899 --> 00:15:23,009
titin where we took a look at all the

00:15:21,209 --> 00:15:24,720
jobs that were ever run on titan we took

00:15:23,009 --> 00:15:27,420
a look at the amount of memory they used

00:15:24,720 --> 00:15:29,639
and one of our findings we came back to

00:15:27,420 --> 00:15:32,579
was there was an uncomfortably high

00:15:29,639 --> 00:15:36,480
amount of jobs that would use only six

00:15:32,579 --> 00:15:37,709
gigabytes per node of memory and if you

00:15:36,480 --> 00:15:40,290
can't guess if we go back a couple

00:15:37,709 --> 00:15:46,439
slides that was because there was only

00:15:40,290 --> 00:15:48,990
six gigabytes of gddr5 and forth between

00:15:46,439 --> 00:15:50,819
the host processors and the GPUs were so

00:15:48,990 --> 00:15:53,009
painful applications were setting their

00:15:50,819 --> 00:15:55,529
working set sizes smaller to only fit

00:15:53,009 --> 00:15:57,779
out of those GPUs in fact the ratio of

00:15:55,529 --> 00:16:00,269
memory performance to PCI performance

00:15:57,779 --> 00:16:02,819
was fifty to one with sumit we've

00:16:00,269 --> 00:16:06,179
reduced that from twenty to one the goal

00:16:02,819 --> 00:16:08,939
here is to enable applications to use

00:16:06,179 --> 00:16:10,740
larger working sets than just the GPU

00:16:08,939 --> 00:16:12,749
memory that sits on board and I think

00:16:10,740 --> 00:16:15,779
Jax talked prior to this did a good job

00:16:12,749 --> 00:16:17,339
of stating that application engine

00:16:15,779 --> 00:16:19,709
designers are actually starting to look

00:16:17,339 --> 00:16:21,809
at how we can do this using large

00:16:19,709 --> 00:16:24,329
working set sizes that are more than

00:16:21,809 --> 00:16:29,550
just the the HP m2 we have sitting on

00:16:24,329 --> 00:16:33,660
our voltage GPUs so the the other aspect

00:16:29,550 --> 00:16:39,779
of this is a portability aspect so since

00:16:33,660 --> 00:16:43,079
the onset of Titan which was since Titan

00:16:39,779 --> 00:16:45,480
came in it was the largest ten petabyte

00:16:43,079 --> 00:16:47,730
it was the first ten petabyte machine

00:16:45,480 --> 00:16:50,189
with GPUs installed and so there was a

00:16:47,730 --> 00:16:53,069
litany of application designers yet that

00:16:50,189 --> 00:16:55,110
had not ported their codes to GPU and so

00:16:53,069 --> 00:16:57,389
we've been in a continual you know

00:16:55,110 --> 00:16:59,519
systematic cycle every year where the

00:16:57,389 --> 00:17:00,990
number of applications using GPUs are

00:16:59,519 --> 00:17:03,120
increasing year over year but we still

00:17:00,990 --> 00:17:05,789
have applications out there which do not

00:17:03,120 --> 00:17:08,179
use GPUs on a GPU enabled machine like

00:17:05,789 --> 00:17:11,520
Titan and so with the house

00:17:08,179 --> 00:17:14,339
participating in in viewing too we now

00:17:11,520 --> 00:17:16,529
get a new ability which we hadn't seen

00:17:14,339 --> 00:17:19,289
before which is going to hopefully ease

00:17:16,529 --> 00:17:22,230
portability essentially the hosts are

00:17:19,289 --> 00:17:23,850
now able to participate and in be linked

00:17:22,230 --> 00:17:27,419
to address translations and paging

00:17:23,850 --> 00:17:30,000
services so that pointers referenced on

00:17:27,419 --> 00:17:31,750
the applications can pointers created on

00:17:30,000 --> 00:17:36,010
the host processors can actually be

00:17:31,750 --> 00:17:38,230
referenced from the nvidia gpus without

00:17:36,010 --> 00:17:40,630
crashing the application and so our hope

00:17:38,230 --> 00:17:43,840
here is that as we're onboarding new GPU

00:17:40,630 --> 00:17:45,880
applications that it makes it easier for

00:17:43,840 --> 00:17:48,070
the life of the programmer now this

00:17:45,880 --> 00:17:49,600
isn't going to be performant it's mostly

00:17:48,070 --> 00:17:52,240
going to be a portability or debugging

00:17:49,600 --> 00:18:01,390
statement but this should make life a

00:17:52,240 --> 00:18:03,340
lot easier so I'll conclude I'll tell

00:18:01,390 --> 00:18:06,610
you from a procurement standpoint we

00:18:03,340 --> 00:18:09,250
when we release an RFP to vendors we

00:18:06,610 --> 00:18:11,680
create a set of figures of Merit and as

00:18:09,250 --> 00:18:13,510
Adam mentioned earlier we released these

00:18:11,680 --> 00:18:18,430
benchmarks that are proxies of the

00:18:13,510 --> 00:18:21,340
applications that we and they run and

00:18:18,430 --> 00:18:22,900
one of the most consistent thing one of

00:18:21,340 --> 00:18:24,730
the most consistent feedbacks we get

00:18:22,900 --> 00:18:25,990
from vendors and what some of the most

00:18:24,730 --> 00:18:29,050
consistent feedback would get from

00:18:25,990 --> 00:18:31,900
applications is that bandwidth is what

00:18:29,050 --> 00:18:35,950
gates HPC performance application

00:18:31,900 --> 00:18:38,100
performance and so in this talk I'm

00:18:35,950 --> 00:18:40,210
stating that to mean memory bandwidth

00:18:38,100 --> 00:18:43,480
interconnect band with an internal i/o

00:18:40,210 --> 00:18:46,210
bandwidth and I think if you take a look

00:18:43,480 --> 00:18:49,960
at what the power architecture is

00:18:46,210 --> 00:18:52,270
providing with sumit it is providing

00:18:49,960 --> 00:18:55,120
that bandwidth that we need we're moving

00:18:52,270 --> 00:18:57,490
from 270 gigabytes per second we're

00:18:55,120 --> 00:19:01,060
moving from 55 to 270 gigabytes per

00:18:57,490 --> 00:19:03,670
second of DRAM we're moving from five to

00:19:01,060 --> 00:19:05,950
six hundred gigabytes per second of GPU

00:19:03,670 --> 00:19:07,930
connectivity and we're moving to 25

00:19:05,950 --> 00:19:10,630
gigabytes per second of interconnect

00:19:07,930 --> 00:19:12,790
performance from 5.2 gigabytes per

00:19:10,630 --> 00:19:14,590
second of interconnect performance all

00:19:12,790 --> 00:19:18,130
of this put together has allowed us to

00:19:14,590 --> 00:19:21,130
create the summit system and at the same

00:19:18,130 --> 00:19:22,960
time the sea-air system as well and I

00:19:21,130 --> 00:19:24,580
don't have a very neat photo of our

00:19:22,960 --> 00:19:27,040
system being put out I actually had

00:19:24,580 --> 00:19:28,480
those in earlier but then I was I was

00:19:27,040 --> 00:19:31,660
shamed into removing them so I just have

00:19:28,480 --> 00:19:33,740
a picture of summit in the room there

00:19:31,660 --> 00:19:37,720
any questions

00:19:33,740 --> 00:19:37,720
[Applause]

00:19:49,030 --> 00:19:52,780
this wasn't earlier photo

00:19:53,230 --> 00:19:56,800

YouTube URL: https://www.youtube.com/watch?v=8AuFT2tapFo


