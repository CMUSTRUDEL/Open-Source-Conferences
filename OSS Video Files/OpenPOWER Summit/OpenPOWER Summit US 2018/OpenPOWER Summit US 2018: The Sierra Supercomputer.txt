Title: OpenPOWER Summit US 2018: The Sierra Supercomputer
Publication date: 2018-04-05
Playlist: OpenPOWER Summit US 2018
Description: 
	Adam Bertsch of LLNL presented on collaborations that have made the CORAL Sierra computer at LLNL and the Summit computer at ORNL possible at the OpenPOWER Summit 2018.

For more information, please visit: http://www.openpowerfoundation.org
Captions: 
	00:00:00,030 --> 00:00:04,799
my name is Adam Burch I work at Lawrence

00:00:02,250 --> 00:00:06,270
Livermore National Laboratory and I'm

00:00:04,799 --> 00:00:09,420
our integration project lead for the

00:00:06,270 --> 00:00:11,250
Sierra supercomputer so this is the

00:00:09,420 --> 00:00:13,049
corollary to the summit machine you

00:00:11,250 --> 00:00:18,060
heard you heard about in the in the

00:00:13,049 --> 00:00:19,410
keynote and this is my real title slide

00:00:18,060 --> 00:00:20,939
because it made me change it but it has

00:00:19,410 --> 00:00:24,480
all the legal mumbo-jumbo from the lab

00:00:20,939 --> 00:00:25,439
on it I'm going to talk about a couple

00:00:24,480 --> 00:00:26,820
of different things I'm gonna give a

00:00:25,439 --> 00:00:28,650
short introduction to the sierra

00:00:26,820 --> 00:00:30,179
computer itself and you'll get a lot

00:00:28,650 --> 00:00:31,710
more technical information about the

00:00:30,179 --> 00:00:33,690
platform and chris's talk at the end of

00:00:31,710 --> 00:00:36,090
the day so make sure you stay for that

00:00:33,690 --> 00:00:40,500
and I'm going to talk kind of a lot

00:00:36,090 --> 00:00:43,170
about the collaboration that is that is

00:00:40,500 --> 00:00:48,539
really the key to making these computers

00:00:43,170 --> 00:00:50,820
work now for me the Sierra supercomputer

00:00:48,539 --> 00:00:52,590
started to kind of an odd place riding

00:00:50,820 --> 00:00:55,289
my bike with with one of our computer

00:00:52,590 --> 00:00:58,109
center directors and talking about you

00:00:55,289 --> 00:01:00,359
know what what was my career and my and

00:00:58,109 --> 00:01:01,379
my job gonna look like in the in the

00:01:00,359 --> 00:01:03,989
future because I've been doing this

00:01:01,379 --> 00:01:06,390
admin thing for a long time and you know

00:01:03,989 --> 00:01:08,450
what what comes next and said well we're

00:01:06,390 --> 00:01:10,920
gonna do this pre exascale computer in

00:01:08,450 --> 00:01:13,530
in a few years and and that might be a

00:01:10,920 --> 00:01:16,080
thing like what's a pre exascale

00:01:13,530 --> 00:01:18,900
computer it's that like not really a

00:01:16,080 --> 00:01:20,759
computer but it turns out that it's a

00:01:18,900 --> 00:01:25,320
you know just the next generation of

00:01:20,759 --> 00:01:28,080
machine for us so advanced simulation

00:01:25,320 --> 00:01:30,030
and computing is the program at

00:01:28,080 --> 00:01:31,409
Livermore that's responsible for the

00:01:30,030 --> 00:01:33,420
computer systems that we use for

00:01:31,409 --> 00:01:35,460
stockpile stewardship so our file

00:01:33,420 --> 00:01:37,200
stewardship is ensuring the safety

00:01:35,460 --> 00:01:39,329
security and reliability in the United

00:01:37,200 --> 00:01:41,119
States nuclear weapon stockpile but

00:01:39,329 --> 00:01:43,710
without the reliance on nuclear testing

00:01:41,119 --> 00:01:45,960
so instead of you know doing this really

00:01:43,710 --> 00:01:47,850
ugly messy rolling up nuclear weapons we

00:01:45,960 --> 00:01:49,439
do that in the computer and there's some

00:01:47,850 --> 00:01:51,470
significant advantages to doing that in

00:01:49,439 --> 00:01:54,270
the computer there's other kinds of

00:01:51,470 --> 00:01:55,530
simulations of you know of experiments

00:01:54,270 --> 00:01:56,549
that you'd rather do in the computer but

00:01:55,530 --> 00:01:58,409
this is probably like the canonical

00:01:56,549 --> 00:02:01,560
example of the thing you really don't

00:01:58,409 --> 00:02:03,060
want to do in the outside so we we have

00:02:01,560 --> 00:02:05,610
our current generation platform which is

00:02:03,060 --> 00:02:08,009
Sequoia that's an IBM Blue Gene cue

00:02:05,610 --> 00:02:11,239
system and you can see we alternate with

00:02:08,009 --> 00:02:14,069
our partner labs Los Alamos and Sandia

00:02:11,239 --> 00:02:15,780
with these advanced technology systems

00:02:14,069 --> 00:02:17,730
forms so these are our platforms that

00:02:15,780 --> 00:02:20,609
are not just you know pizza boxes

00:02:17,730 --> 00:02:22,799
InfiniBand and go this is we're gonna

00:02:20,609 --> 00:02:25,439
try to innovate with a very large

00:02:22,799 --> 00:02:26,879
platform typically one or more advanced

00:02:25,439 --> 00:02:29,310
technologies that you can't find in the

00:02:26,879 --> 00:02:32,569
commodity market yet so things like env

00:02:29,310 --> 00:02:35,099
link right show up in these platforms

00:02:32,569 --> 00:02:36,840
and the last bit of interesting this on

00:02:35,099 --> 00:02:38,430
this slide I guess is that we did extend

00:02:36,840 --> 00:02:42,209
the life of Sequoia out so we have

00:02:38,430 --> 00:02:44,549
significant overlap with Sierra so the

00:02:42,209 --> 00:02:47,489
people can port right the the previous

00:02:44,549 --> 00:02:50,040
platform Sequoia was you know just

00:02:47,489 --> 00:02:51,959
massively parallel homogeneous system

00:02:50,040 --> 00:02:54,150
and now we have this heterogeneous

00:02:51,959 --> 00:02:56,489
computing world that were we're talking

00:02:54,150 --> 00:03:01,799
about at this whole summit so here's

00:02:56,489 --> 00:03:05,849
what our system looks like every every

00:03:01,799 --> 00:03:09,419
compute node has four Voltas in it to

00:03:05,849 --> 00:03:10,680
power nine CPUs we have 256 gigs of

00:03:09,419 --> 00:03:12,060
memory in each node which I'll talk

00:03:10,680 --> 00:03:15,629
about in a little more detail later how

00:03:12,060 --> 00:03:19,620
we ended up there we also have the HP m2

00:03:15,629 --> 00:03:21,419
on a CPU same rack layout as the

00:03:19,620 --> 00:03:24,870
Oakridge system we had in the in the

00:03:21,419 --> 00:03:27,389
keynote and then we're gonna have 4320

00:03:24,870 --> 00:03:30,120
nodes and that means that we're at about

00:03:27,389 --> 00:03:32,729
12 megawatts and so that's a that's

00:03:30,120 --> 00:03:34,590
actually a really disruptive bump on the

00:03:32,729 --> 00:03:38,250
on the power performance curve for us

00:03:34,590 --> 00:03:40,199
right Sequoia which is only a a 20

00:03:38,250 --> 00:03:42,720
petaflop computer was about 10 megawatts

00:03:40,199 --> 00:03:44,609
so we haven't really raised the amount

00:03:42,720 --> 00:03:46,049
of power that we have to use and we've

00:03:44,609 --> 00:03:49,739
gotten this huge bump in performance so

00:03:46,049 --> 00:03:51,569
that's great the bad news is that for an

00:03:49,739 --> 00:03:53,040
exascale system we don't necessarily see

00:03:51,569 --> 00:03:54,720
another huge bump in the power

00:03:53,040 --> 00:03:56,340
performance curve so we're gonna need a

00:03:54,720 --> 00:03:58,919
lot more electricity for the next

00:03:56,340 --> 00:04:01,859
generation we do have a spectrum scale

00:03:58,919 --> 00:04:03,479
file system and that's a hundred fifty

00:04:01,859 --> 00:04:06,090
four petabytes and we have a slide on on

00:04:03,479 --> 00:04:10,859
that a little bit later so this is a

00:04:06,090 --> 00:04:13,639
major shift for our application teams we

00:04:10,859 --> 00:04:16,349
were very much on the MPI everywhere

00:04:13,639 --> 00:04:18,780
homogeneous system bandwagon for a very

00:04:16,349 --> 00:04:21,780
long time our applications are very

00:04:18,780 --> 00:04:26,400
complex millions of lines of code lots

00:04:21,780 --> 00:04:28,050
of data movement very hard to look at

00:04:26,400 --> 00:04:29,400
the traditional accelerator model

00:04:28,050 --> 00:04:32,820
where you're transferring all that data

00:04:29,400 --> 00:04:35,010
over the PCI bus and say oh yeah we can

00:04:32,820 --> 00:04:37,020
do that it was more no we can't possibly

00:04:35,010 --> 00:04:40,820
do that and so we had we had been very

00:04:37,020 --> 00:04:44,600
much on on this NPI everywhere

00:04:40,820 --> 00:04:47,639
homogeneous curve and then we said well

00:04:44,600 --> 00:04:48,960
that's not gonna make it right that that

00:04:47,639 --> 00:04:50,910
curve we've been riding for so long

00:04:48,960 --> 00:04:53,310
isn't gonna get us to the next

00:04:50,910 --> 00:04:55,889
generation not inside the power envelope

00:04:53,310 --> 00:04:58,110
that is you know conceivably doable with

00:04:55,889 --> 00:05:01,350
a with the data center and that kind of

00:04:58,110 --> 00:05:04,290
thing but there's this idea of ending

00:05:01,350 --> 00:05:06,510
link and unified memory that will solve

00:05:04,290 --> 00:05:09,150
all of our data problems or at least

00:05:06,510 --> 00:05:10,830
make them a little bit tractable and so

00:05:09,150 --> 00:05:14,520
that's where we are we've taken this 180

00:05:10,830 --> 00:05:17,850
turn and and we're doing we're new in

00:05:14,520 --> 00:05:20,400
GPUs now you know Oak Ridge was on the

00:05:17,850 --> 00:05:22,020
GPU bandwagon little earlier than us so

00:05:20,400 --> 00:05:27,030
they have they have some more experience

00:05:22,020 --> 00:05:29,520
but we looked at this this whole this

00:05:27,030 --> 00:05:32,610
whole thing and said we are going to

00:05:29,520 --> 00:05:34,140
have to to change radically and we don't

00:05:32,610 --> 00:05:36,690
know what the next architecture is going

00:05:34,140 --> 00:05:39,510
to look like we don't know if it's gonna

00:05:36,690 --> 00:05:42,090
be just like this or if it'll have some

00:05:39,510 --> 00:05:47,789
other kind of accelerator is it going to

00:05:42,090 --> 00:05:51,690
have FPGAs so our codes really started

00:05:47,789 --> 00:05:53,820
to evolve not just the codes but a lot

00:05:51,690 --> 00:05:55,350
of tools and abstractions and we started

00:05:53,820 --> 00:05:57,810
you know years in advance this time

00:05:55,350 --> 00:06:00,539
which the center of excellence concept

00:05:57,810 --> 00:06:02,580
that Oakridge had used on titan and

00:06:00,539 --> 00:06:05,460
we've really made that the basis for how

00:06:02,580 --> 00:06:07,800
we evolve our codes to to go forward and

00:06:05,460 --> 00:06:10,080
that's that's our that's really what

00:06:07,800 --> 00:06:13,410
we're we're hanging our hopes on is

00:06:10,080 --> 00:06:16,410
being able to continue to to use these

00:06:13,410 --> 00:06:18,660
abstractions that we developed so we did

00:06:16,410 --> 00:06:22,680
make some cost neutral trade-offs in the

00:06:18,660 --> 00:06:24,390
architecture we got hit by this well

00:06:22,680 --> 00:06:25,440
DRAM actually cost twice as much as we

00:06:24,390 --> 00:06:28,410
thought it was going to cost in this

00:06:25,440 --> 00:06:29,550
time frame and we some choices in

00:06:28,410 --> 00:06:31,620
Oakridge made some choices and that

00:06:29,550 --> 00:06:33,180
actually has caused the appearance of

00:06:31,620 --> 00:06:35,520
our machines to diverge more than we had

00:06:33,180 --> 00:06:38,099
originally planned we have a pretty

00:06:35,520 --> 00:06:40,110
fixed budget so our our choices had to

00:06:38,099 --> 00:06:41,430
be cost neutral and we had to option

00:06:40,110 --> 00:06:42,570
that we can buy half as much memory

00:06:41,430 --> 00:06:45,000
that's an easy way to deal with the fact

00:06:42,570 --> 00:06:46,320
that it cost twice as much right or we

00:06:45,000 --> 00:06:49,620
can take a hit somewhere else on the

00:06:46,320 --> 00:06:51,270
system and what we did figure out was we

00:06:49,620 --> 00:06:53,280
could taper the network half as many top

00:06:51,270 --> 00:06:54,600
level director switches and half as much

00:06:53,280 --> 00:06:56,340
bang with going up to them that would

00:06:54,600 --> 00:06:59,160
have about the same cost impact and we

00:06:56,340 --> 00:07:00,930
can make that work and then we did a

00:06:59,160 --> 00:07:01,860
bunch of performance analysis about

00:07:00,930 --> 00:07:03,870
what's going to happen to our

00:07:01,860 --> 00:07:06,450
applications if we make these choices

00:07:03,870 --> 00:07:07,530
and it turns out that neither one of

00:07:06,450 --> 00:07:09,030
those choices was going to hurt our

00:07:07,530 --> 00:07:14,250
applications too much so we selected

00:07:09,030 --> 00:07:16,500
both options and that means we get about

00:07:14,250 --> 00:07:18,750
5% more nodes because we saved some

00:07:16,500 --> 00:07:20,370
money there and for the workload that

00:07:18,750 --> 00:07:23,610
we're expecting to take up most of the

00:07:20,370 --> 00:07:25,980
cycles that's only gonna have a

00:07:23,610 --> 00:07:28,110
performance loss of about 1% no it over

00:07:25,980 --> 00:07:29,640
node and by getting 5% more nodes we

00:07:28,110 --> 00:07:33,960
actually get more performance out of the

00:07:29,640 --> 00:07:34,850
computer so this is what our systems are

00:07:33,960 --> 00:07:37,830
gonna look like

00:07:34,850 --> 00:07:40,050
ciara which is the big the big system

00:07:37,830 --> 00:07:42,060
and we also have an unclassified system

00:07:40,050 --> 00:07:43,320
we're currently calling you Ciara but

00:07:42,060 --> 00:07:45,300
it's not going to be its final name and

00:07:43,320 --> 00:07:46,470
then we have another environment called

00:07:45,300 --> 00:07:49,020
rrz environment which is another

00:07:46,470 --> 00:07:52,710
unclassified environment where we do

00:07:49,020 --> 00:07:54,150
some some development and the important

00:07:52,710 --> 00:07:55,980
thing you know here is that even though

00:07:54,150 --> 00:07:56,760
Ciara is going to be classified and you

00:07:55,980 --> 00:07:57,840
know people are going to have an

00:07:56,760 --> 00:07:59,730
opportunity to use it if they're not

00:07:57,840 --> 00:08:01,260
part of certain programs we do have a

00:07:59,730 --> 00:08:04,020
significantly sized

00:08:01,260 --> 00:08:07,170
unclassified you know open science

00:08:04,020 --> 00:08:08,790
platform in you Ciara and it is exactly

00:08:07,170 --> 00:08:16,560
like Ciara it just has fewer compute

00:08:08,790 --> 00:08:19,260
nodes now the you know when you do one

00:08:16,560 --> 00:08:20,790
of these procurements you are setting

00:08:19,260 --> 00:08:22,350
some targets that are kind of shots in

00:08:20,790 --> 00:08:24,240
the dark it would be nice to get these

00:08:22,350 --> 00:08:26,570
things because we'd like to evolve the

00:08:24,240 --> 00:08:29,280
applications to get to a certain place

00:08:26,570 --> 00:08:32,190
and then you go and talk to vendors and

00:08:29,280 --> 00:08:34,110
you can you value proposals and usually

00:08:32,190 --> 00:08:36,000
you don't get everything you want right

00:08:34,110 --> 00:08:37,380
you have to make some trade-offs and in

00:08:36,000 --> 00:08:39,900
this case we did not have to make very

00:08:37,380 --> 00:08:41,550
many trade-offs the proposal even though

00:08:39,900 --> 00:08:43,410
it was completely different than what we

00:08:41,550 --> 00:08:45,030
were kind of expecting for a

00:08:43,410 --> 00:08:46,260
next-generation system what we you know

00:08:45,030 --> 00:08:50,490
we had envisioned and we put the

00:08:46,260 --> 00:08:52,380
proposal out there what we got back is

00:08:50,490 --> 00:08:54,360
basically everything that we wanted

00:08:52,380 --> 00:08:55,170
except in the case of Livermore we don't

00:08:54,360 --> 00:08:58,640
have

00:08:55,170 --> 00:09:00,959
aggregate of four petabytes of DRAM

00:08:58,640 --> 00:09:02,640
partly because of the memory costs and

00:09:00,959 --> 00:09:04,709
partly because we had envisioned name a

00:09:02,640 --> 00:09:06,510
system with a much higher node count and

00:09:04,709 --> 00:09:09,000
therefore you would end up with a ton of

00:09:06,510 --> 00:09:11,760
aggregate memory just as a way to have

00:09:09,000 --> 00:09:13,140
enough memory per MPI task but we ended

00:09:11,760 --> 00:09:14,820
up with way more entering for more

00:09:13,140 --> 00:09:17,880
memory for MPI tasks than we were gonna

00:09:14,820 --> 00:09:21,209
need so so we ended up with just a

00:09:17,880 --> 00:09:22,500
really great great system so let's talk

00:09:21,209 --> 00:09:24,899
about the collaboration that got us

00:09:22,500 --> 00:09:27,260
there so you heard a little bit about

00:09:24,899 --> 00:09:29,160
quarrel in the in the keynote

00:09:27,260 --> 00:09:34,019
collaboration invoke raid you argon and

00:09:29,160 --> 00:09:36,810
Livermore and so we first of all

00:09:34,019 --> 00:09:37,709
collaborated on our requirements did we

00:09:36,810 --> 00:09:41,660
have slightly different requirements

00:09:37,709 --> 00:09:41,660
each each of our computer centers and

00:09:42,079 --> 00:09:47,670
you know typically that has led us to do

00:09:44,730 --> 00:09:49,350
separate procurements but what if we

00:09:47,670 --> 00:09:51,180
look at what we have in common and it

00:09:49,350 --> 00:09:54,269
turns out we have almost everything in

00:09:51,180 --> 00:09:55,800
common so that's one layer of

00:09:54,269 --> 00:09:59,220
collaboration is just on the

00:09:55,800 --> 00:10:00,390
requirements and then by putting out a

00:09:59,220 --> 00:10:03,450
procurement where we're gonna buy

00:10:00,390 --> 00:10:06,060
multiple systems and guarantee we were

00:10:03,450 --> 00:10:07,770
gonna pick multiple vendors that means

00:10:06,060 --> 00:10:10,440
it doesn't have to be just one winner

00:10:07,770 --> 00:10:12,660
right well you know the ecosystem can

00:10:10,440 --> 00:10:17,220
can be funded in a number of different

00:10:12,660 --> 00:10:20,220
winners and that hopefully gives some

00:10:17,220 --> 00:10:21,750
incentive for for people to actually bid

00:10:20,220 --> 00:10:23,339
right because it costs a lot to respond

00:10:21,750 --> 00:10:25,649
to one of these huge procurements for a

00:10:23,339 --> 00:10:27,690
for a leadership class supercomputer

00:10:25,649 --> 00:10:29,370
right there's a lot of investment that

00:10:27,690 --> 00:10:30,630
goes in from the company so they need to

00:10:29,370 --> 00:10:31,950
feel like there's some chance they're

00:10:30,630 --> 00:10:34,079
gonna win before they make that

00:10:31,950 --> 00:10:37,709
investment so we said we're gonna award

00:10:34,079 --> 00:10:41,640
to build contracts for three systems and

00:10:37,709 --> 00:10:43,920
to NRI contracts so those are NRI is

00:10:41,640 --> 00:10:46,020
non-recurring engineering where we say

00:10:43,920 --> 00:10:47,850
we need to develop these technologies to

00:10:46,020 --> 00:10:49,980
make the computer a success and we're

00:10:47,850 --> 00:10:52,140
just gonna fund those technologies and

00:10:49,980 --> 00:10:54,660
your deliverable is a report that says

00:10:52,140 --> 00:10:57,000
you created the technology even if we

00:10:54,660 --> 00:11:02,269
never buy the computer and that's that's

00:10:57,000 --> 00:11:02,269
another way for DOA to accelerate HPC

00:11:03,649 --> 00:11:08,100
now another place where we have a bunch

00:11:06,600 --> 00:11:08,390
of collaboration and this is where open

00:11:08,100 --> 00:11:10,910
power

00:11:08,390 --> 00:11:12,710
comes in is on the vendor side it used

00:11:10,910 --> 00:11:15,830
to be that we would get these you know

00:11:12,710 --> 00:11:18,100
monolithic build contracts for one

00:11:15,830 --> 00:11:20,180
vendor to bring everything to the table

00:11:18,100 --> 00:11:21,800
and make it happen and we still have a

00:11:20,180 --> 00:11:24,260
prime contractor right IBM is a prime

00:11:21,800 --> 00:11:26,450
contractor for our systems but it's no

00:11:24,260 --> 00:11:27,620
longer necessary that iBM has to do

00:11:26,450 --> 00:11:29,570
in-house engineering

00:11:27,620 --> 00:11:32,240
to create every single piece of the

00:11:29,570 --> 00:11:33,830
system it's just too complicated there's

00:11:32,240 --> 00:11:35,900
too much going on especially with

00:11:33,830 --> 00:11:38,840
accelerators right there are companies

00:11:35,900 --> 00:11:40,160
that do accelerators really well there

00:11:38,840 --> 00:11:41,720
are companies that do interconnect

00:11:40,160 --> 00:11:43,310
really well so we don't have to have one

00:11:41,720 --> 00:11:46,610
vendor bring everything in the table

00:11:43,310 --> 00:11:50,320
anymore and if you see here we've got in

00:11:46,610 --> 00:11:53,090
open power we have IBM Mellanox Nvidia

00:11:50,320 --> 00:11:54,770
this is a slightly older graphic but Red

00:11:53,090 --> 00:11:56,720
Hat is in the his in the new graphic

00:11:54,770 --> 00:11:59,060
right all of the major players for the

00:11:56,720 --> 00:12:02,000
systems are all part of open power and

00:11:59,060 --> 00:12:04,460
without the collaboration and the

00:12:02,000 --> 00:12:05,870
cooperation and the IP agreements in

00:12:04,460 --> 00:12:12,320
place for open power they wouldn't have

00:12:05,870 --> 00:12:13,970
been able to do this so the n R you know

00:12:12,320 --> 00:12:15,380
there's a bunch of areas that we wanted

00:12:13,970 --> 00:12:17,330
to accelerate technology but the thing

00:12:15,380 --> 00:12:20,390
that came out of the n re that was most

00:12:17,330 --> 00:12:22,310
critical to me is not even just these

00:12:20,390 --> 00:12:25,610
technologies but these working groups

00:12:22,310 --> 00:12:28,100
that were around to feed into the

00:12:25,610 --> 00:12:30,530
technology proposals evaluate the

00:12:28,100 --> 00:12:32,390
reports that came out and even though

00:12:30,530 --> 00:12:34,640
the NRE contract is basically over now

00:12:32,390 --> 00:12:36,650
right technology has been created all of

00:12:34,640 --> 00:12:38,690
the working groups are still are still

00:12:36,650 --> 00:12:40,280
together right all those working groups

00:12:38,690 --> 00:12:42,500
have just transitioned from an NRI

00:12:40,280 --> 00:12:44,060
working group into a system deployment

00:12:42,500 --> 00:12:46,190
working group with the same portfolio

00:12:44,060 --> 00:12:49,160
they started with so this is another

00:12:46,190 --> 00:12:51,050
place where collaboration is the only

00:12:49,160 --> 00:12:57,190
way that we're able to have success with

00:12:51,050 --> 00:12:59,720
these systems so a couple examples of

00:12:57,190 --> 00:13:02,030
success in in each of these

00:12:59,720 --> 00:13:05,780
collaborations so the file system

00:13:02,030 --> 00:13:08,120
working group came together and you know

00:13:05,780 --> 00:13:11,510
the original planner record had a pretty

00:13:08,120 --> 00:13:13,550
capable system and in particular we had

00:13:11,510 --> 00:13:18,080
concern about metadata performance on a

00:13:13,550 --> 00:13:19,400
really large file system GPFS has you

00:13:18,080 --> 00:13:20,600
know some areas it's really strong in

00:13:19,400 --> 00:13:21,320
and some areas that at the time it

00:13:20,600 --> 00:13:23,060
wasn't a strong

00:13:21,320 --> 00:13:25,250
and one of those areas it wasn't as

00:13:23,060 --> 00:13:27,649
strong and was metadata performance in

00:13:25,250 --> 00:13:28,820
in creating multiple and creating a

00:13:27,649 --> 00:13:30,110
bunch of files in a single directory

00:13:28,820 --> 00:13:33,170
right single shared directory

00:13:30,110 --> 00:13:36,560
performance and so we invested a lot of

00:13:33,170 --> 00:13:40,310
time money energy into accelerating that

00:13:36,560 --> 00:13:43,370
development and at the same time you're

00:13:40,310 --> 00:13:45,110
you're you're offering up you know a

00:13:43,370 --> 00:13:48,050
hardware solution that looks like this

00:13:45,110 --> 00:13:49,699
and when there's some there's some newer

00:13:48,050 --> 00:13:50,930
stuff out there on the market that it

00:13:49,699 --> 00:13:52,880
might be better for everybody

00:13:50,930 --> 00:13:54,139
and this is an area where you know

00:13:52,880 --> 00:13:56,089
Oakridge really went out and did the

00:13:54,139 --> 00:13:57,560
heavy lifting on this and said we think

00:13:56,089 --> 00:14:01,420
we can do better on the storage solution

00:13:57,560 --> 00:14:04,190
and IBM came back with at the same cost

00:14:01,420 --> 00:14:06,920
you know significantly more space evilly

00:14:04,190 --> 00:14:10,160
more performance and we did have success

00:14:06,920 --> 00:14:13,670
with without getting all that code into

00:14:10,160 --> 00:14:15,350
gpfs so that's that's an area where this

00:14:13,670 --> 00:14:17,209
this close partnership and this

00:14:15,350 --> 00:14:18,920
collaboration really led to improvement

00:14:17,209 --> 00:14:22,670
of this whole ecosystem in the storage

00:14:18,920 --> 00:14:24,259
space another place that the

00:14:22,670 --> 00:14:26,000
collaboration was just key is in the

00:14:24,259 --> 00:14:27,769
proposal we always put out these

00:14:26,000 --> 00:14:29,660
benchmarks right

00:14:27,769 --> 00:14:32,180
you can't really evaluate performance on

00:14:29,660 --> 00:14:35,089
on really big apps but you can you can

00:14:32,180 --> 00:14:37,250
take some benchmark codes and you can

00:14:35,089 --> 00:14:40,519
and you can try to predict what they're

00:14:37,250 --> 00:14:44,029
gonna perform like on an eventual system

00:14:40,519 --> 00:14:46,850
and in this case we had IBM with the

00:14:44,029 --> 00:14:49,730
power 9 and Nvidia with the voltage GPU

00:14:46,850 --> 00:14:53,300
neither of which existed in 2014 we

00:14:49,730 --> 00:14:55,040
signed this contract right but we needed

00:14:53,300 --> 00:14:57,769
to know how they were gonna perform and

00:14:55,040 --> 00:15:00,949
have a believable story that says if you

00:14:57,769 --> 00:15:05,810
invest in this technology real apps that

00:15:00,949 --> 00:15:08,420
you care about will perform and so you

00:15:05,810 --> 00:15:10,279
know open power and the agreements in

00:15:08,420 --> 00:15:11,060
place we Nvidia and IBM allowed them to

00:15:10,279 --> 00:15:14,630
collaborate

00:15:11,060 --> 00:15:17,209
incredibly closely to to get there right

00:15:14,630 --> 00:15:20,089
to to not only predict how well

00:15:17,209 --> 00:15:22,970
occations might perform on the potential

00:15:20,089 --> 00:15:25,399
architecture but then to follow through

00:15:22,970 --> 00:15:30,079
over the next four years and actually

00:15:25,399 --> 00:15:32,779
hit those targets the the amount the

00:15:30,079 --> 00:15:35,240
amount of credibility that they were

00:15:32,779 --> 00:15:38,200
able to carry forth is a big reason

00:15:35,240 --> 00:15:41,360
why they ended up you know winning the

00:15:38,200 --> 00:15:43,220
contract and the fact that that they

00:15:41,360 --> 00:15:46,430
were so credible and then so successful

00:15:43,220 --> 00:15:48,830
in you know bringing the the advertised

00:15:46,430 --> 00:15:50,120
performance to delivery is just really

00:15:48,830 --> 00:15:51,200
exciting and without the ability to

00:15:50,120 --> 00:15:53,570
collaborate like that it just can't

00:15:51,200 --> 00:15:56,120
happen you know you can't you can't have

00:15:53,570 --> 00:15:58,610
IBM say well we think the GPU guys might

00:15:56,120 --> 00:16:00,230
make it work and the GPA guys going that

00:15:58,610 --> 00:16:02,240
the the cpu guys will feed us the data

00:16:00,230 --> 00:16:03,500
we need and then just you know buy the

00:16:02,240 --> 00:16:04,910
computer right you really you really

00:16:03,500 --> 00:16:07,180
need this collaboration to make it

00:16:04,910 --> 00:16:07,180
happen

00:16:07,839 --> 00:16:13,790
so when we do these things we always buy

00:16:11,360 --> 00:16:15,649
some sort of early access system so that

00:16:13,790 --> 00:16:18,680
we can get get going with our

00:16:15,649 --> 00:16:20,149
application teams ahead of time and it

00:16:18,680 --> 00:16:21,260
also provides a good check point that

00:16:20,149 --> 00:16:23,360
the technology is moving in the

00:16:21,260 --> 00:16:25,370
direction it needs to move to to get to

00:16:23,360 --> 00:16:29,380
the big system in the time frame as a

00:16:25,370 --> 00:16:31,850
big system is is gonna be delivered so

00:16:29,380 --> 00:16:35,240
both Livermore and Oak Ridge purchased

00:16:31,850 --> 00:16:38,990
early access systems based on the Minsky

00:16:35,240 --> 00:16:41,510
server with 4 Pascal GPUs instead of

00:16:38,990 --> 00:16:42,310
Voltas and to power 8 processors instead

00:16:41,510 --> 00:16:46,430
of power nines

00:16:42,310 --> 00:16:51,829
now and d-link similar kind of big

00:16:46,430 --> 00:16:54,529
memory footprint and we also got really

00:16:51,829 --> 00:16:56,750
early access to a bunch the technology

00:16:54,529 --> 00:16:58,760
beta compilers being probably the

00:16:56,750 --> 00:17:00,589
biggest one right the compiler story for

00:16:58,760 --> 00:17:04,459
heterogeneous architectures is a tough

00:17:00,589 --> 00:17:06,620
story it's far from a solved problem and

00:17:04,459 --> 00:17:07,910
so that was one of our big concerns

00:17:06,620 --> 00:17:10,010
coming in is what's your what's your

00:17:07,910 --> 00:17:13,600
compiler story and how are we going to

00:17:10,010 --> 00:17:17,059
know if if that story is is gonna work

00:17:13,600 --> 00:17:18,589
in a timeframe that that we can still

00:17:17,059 --> 00:17:19,880
you know bail out if it's not gonna work

00:17:18,589 --> 00:17:21,709
or make significant changes of the

00:17:19,880 --> 00:17:24,140
program so that it will work and so

00:17:21,709 --> 00:17:24,920
that's really what what this is what

00:17:24,140 --> 00:17:27,290
this is about

00:17:24,920 --> 00:17:29,150
and we had all the components there

00:17:27,290 --> 00:17:33,020
right very similar kind of rack and

00:17:29,150 --> 00:17:35,090
physical layout early access to the to

00:17:33,020 --> 00:17:36,559
the file system technology you know it's

00:17:35,090 --> 00:17:38,050
everything is there and everybody is

00:17:36,559 --> 00:17:42,260
actually running on these systems today

00:17:38,050 --> 00:17:47,179
and you know codes have have really come

00:17:42,260 --> 00:17:48,590
a long way for us we did buy three of

00:17:47,179 --> 00:17:49,100
them right for our three environments

00:17:48,590 --> 00:17:52,490
the open

00:17:49,100 --> 00:17:54,920
the kind of open science environment we

00:17:52,490 --> 00:17:56,420
called the cz the RZ which is our

00:17:54,920 --> 00:17:58,520
slightly more restricted unclassified

00:17:56,420 --> 00:17:59,720
environment and the SCF our classified

00:17:58,520 --> 00:18:02,180
environment where the where the real

00:17:59,720 --> 00:18:04,700
work of the program is going on and so

00:18:02,180 --> 00:18:14,750
people in all of those places were able

00:18:04,700 --> 00:18:16,570
to do to get there so this is this is

00:18:14,750 --> 00:18:20,030
the beginning of an accelerator based

00:18:16,570 --> 00:18:23,080
era of computing a lever more we are

00:18:20,030 --> 00:18:25,580
making a big change from where we were

00:18:23,080 --> 00:18:29,210
to where we think the the industry has

00:18:25,580 --> 00:18:30,830
to go to continue to perform for all the

00:18:29,210 --> 00:18:35,560
reasons that the pequeños who are

00:18:30,830 --> 00:18:38,480
watching this morning kind of covered

00:18:35,560 --> 00:18:40,160
power efficiency is is really important

00:18:38,480 --> 00:18:41,810
for all of us you can only bring so much

00:18:40,160 --> 00:18:46,850
electricity into a building you can only

00:18:41,810 --> 00:18:48,530
cool so much footprint right there are

00:18:46,850 --> 00:18:50,300
some significant Network advantages to

00:18:48,530 --> 00:18:51,890
having these really big nodes you don't

00:18:50,300 --> 00:18:54,410
have to have nearly as many endpoints in

00:18:51,890 --> 00:18:56,090
your network that's a much simpler place

00:18:54,410 --> 00:18:57,080
to come from so that was that was an

00:18:56,090 --> 00:18:59,180
advantage for us there are some

00:18:57,080 --> 00:19:01,190
downsides to the to the super large

00:18:59,180 --> 00:19:03,350
nodes though you need more bandwidth per

00:19:01,190 --> 00:19:07,160
node so that's you know that's a

00:19:03,350 --> 00:19:08,750
trade-off that has to be considered the

00:19:07,160 --> 00:19:10,880
most important thing I think was that

00:19:08,750 --> 00:19:12,340
there was a clear path for performance

00:19:10,880 --> 00:19:15,080
portability that by creating

00:19:12,340 --> 00:19:17,240
abstractions you know software

00:19:15,080 --> 00:19:19,120
abstractions for defining where the

00:19:17,240 --> 00:19:21,560
parallelism lives in your code and then

00:19:19,120 --> 00:19:23,780
porting our codes to those abstractions

00:19:21,560 --> 00:19:25,730
instead of porting our codes directly to

00:19:23,780 --> 00:19:27,860
something like CUDA that means that we

00:19:25,730 --> 00:19:29,720
can still run on CPU machines we can run

00:19:27,860 --> 00:19:30,680
on GPU machines like this and we can run

00:19:29,720 --> 00:19:33,110
on whatever is going to come out

00:19:30,680 --> 00:19:35,990
tomorrow by plugging in a new back end

00:19:33,110 --> 00:19:38,120
to our portability layer without that we

00:19:35,990 --> 00:19:39,500
could not have convinced our

00:19:38,120 --> 00:19:44,140
applications teams to make the

00:19:39,500 --> 00:19:46,880
investment in in this kind of a system

00:19:44,140 --> 00:19:49,730
you know we have we have really complex

00:19:46,880 --> 00:19:51,410
memory hierarchies and it's important

00:19:49,730 --> 00:19:53,000
for us to have some sort of defined way

00:19:51,410 --> 00:19:55,250
to interact with those hierarchies to

00:19:53,000 --> 00:19:58,040
get performance out of the system the

00:19:55,250 --> 00:20:00,890
current accelerator model you know we

00:19:58,040 --> 00:20:04,400
have a very clear understanding of

00:20:00,890 --> 00:20:06,110
where you need to move data you want to

00:20:04,400 --> 00:20:08,120
leave data in the accelerator as long as

00:20:06,110 --> 00:20:09,770
you can and then move it back out you

00:20:08,120 --> 00:20:11,780
know this was an understood model

00:20:09,770 --> 00:20:13,340
already we didn't consider that a big

00:20:11,780 --> 00:20:17,240
risk where some of the other models we

00:20:13,340 --> 00:20:19,130
considered there was a lot of you know

00:20:17,240 --> 00:20:22,100
this look there's a complex multi-level

00:20:19,130 --> 00:20:24,170
memory hierarchy and makkac and and here

00:20:22,100 --> 00:20:26,740
at least we understood what was you know

00:20:24,170 --> 00:20:29,620
what was gonna what was gonna go on I

00:20:26,740 --> 00:20:32,810
think the most

00:20:29,620 --> 00:20:36,230
the most important thing for for systems

00:20:32,810 --> 00:20:38,840
going forward is that we all continue to

00:20:36,230 --> 00:20:43,880
and increase our collaboration right

00:20:38,840 --> 00:20:47,990
teams collaborate in creation is on the

00:20:43,880 --> 00:20:49,610
better side everything is too complex

00:20:47,990 --> 00:20:52,250
right there are too many areas of

00:20:49,610 --> 00:20:57,530
Technology with too much exciting stuff

00:20:52,250 --> 00:20:59,360
going on for any one company or or or

00:20:57,530 --> 00:21:00,530
platform to have all the answers

00:20:59,360 --> 00:21:02,990
so by collaborating and sharing

00:21:00,530 --> 00:21:05,600
information you know we can we can get

00:21:02,990 --> 00:21:08,930
there right so you have NVIDIA and Knox

00:21:05,600 --> 00:21:10,850
and IBM in this case and on the

00:21:08,930 --> 00:21:12,830
procurement side you know having

00:21:10,850 --> 00:21:14,870
multiple laboratories share in the

00:21:12,830 --> 00:21:17,300
burden of creating these procurements

00:21:14,870 --> 00:21:19,940
and getting all the right requirements

00:21:17,300 --> 00:21:21,560
and frankly learning how some other

00:21:19,940 --> 00:21:24,020
people are doing this right like at

00:21:21,560 --> 00:21:26,300
Livermore we've been doing big HPC

00:21:24,020 --> 00:21:28,520
procurements for a long time and we've

00:21:26,300 --> 00:21:29,570
got our way of doing it right and Oak

00:21:28,520 --> 00:21:31,130
Ridge has been doing it for a long time

00:21:29,570 --> 00:21:33,260
and they had their way of doing it and

00:21:31,130 --> 00:21:35,570
being able to learn from each other has

00:21:33,260 --> 00:21:37,490
been it's just been really powerful and

00:21:35,570 --> 00:21:40,100
and giving us both

00:21:37,490 --> 00:21:41,480
you know better systems that we are

00:21:40,100 --> 00:21:45,500
better prepared for than we would have

00:21:41,480 --> 00:21:47,390
otherwise been so this really we think

00:21:45,500 --> 00:21:48,400
it represents a very viable path to

00:21:47,390 --> 00:21:50,990
exascale

00:21:48,400 --> 00:21:52,700
to go with an architecture like this and

00:21:50,990 --> 00:21:55,520
and that's really what a pre exascale

00:21:52,700 --> 00:21:57,710
computer is about is do we have a path

00:21:55,520 --> 00:21:59,960
is going to get us we want to go now I

00:21:57,710 --> 00:22:02,480
do have a time-lapse video that covers

00:21:59,960 --> 00:22:06,860
the first quarter of our system

00:22:02,480 --> 00:22:09,310
integration and I'll go ahead and run

00:22:06,860 --> 00:22:09,310
through this

00:22:11,200 --> 00:22:15,440
you know our system and the Oakridge

00:22:13,880 --> 00:22:17,570
system are still under integration right

00:22:15,440 --> 00:22:19,370
now but this is pretty cool because you

00:22:17,570 --> 00:22:22,610
can see it starts with a giant hole in

00:22:19,370 --> 00:22:26,720
the floor and lots of water and cable

00:22:22,610 --> 00:22:28,910
tray and power and then you know we're

00:22:26,720 --> 00:22:31,490
collaborating with our facilities teams

00:22:28,910 --> 00:22:33,410
and and operationally so we're actually

00:22:31,490 --> 00:22:36,050
starting this site the infrastructure of

00:22:33,410 --> 00:22:38,540
the system Wow the hole is still in the

00:22:36,050 --> 00:22:40,310
floor right I mean this is the only way

00:22:38,540 --> 00:22:42,890
to make the schedule for these kind of

00:22:40,310 --> 00:22:44,990
things work you've got to start the

00:22:42,890 --> 00:22:52,550
facilities you know months and months

00:22:44,990 --> 00:22:54,380
and months in advance now you can see

00:22:52,550 --> 00:22:57,020
compute rack showing up and those racks

00:22:54,380 --> 00:23:03,260
show up empty in this case and then we

00:22:57,020 --> 00:23:04,880
fill them with with compute nodes now

00:23:03,260 --> 00:23:07,070
there are some differences so we had the

00:23:04,880 --> 00:23:09,170
giant hole in the floor so we had to get

00:23:07,070 --> 00:23:12,560
all that stuff in before the computer

00:23:09,170 --> 00:23:14,780
showed up Oak Ridge is on is on a slab

00:23:12,560 --> 00:23:16,160
right so they did the opposite they they

00:23:14,780 --> 00:23:17,810
have everything coming in from the top

00:23:16,160 --> 00:23:23,660
and the computer starts in and it starts

00:23:17,810 --> 00:23:25,130
underneath it and I think it's a it's a

00:23:23,660 --> 00:23:27,050
testament to the flexibility of the

00:23:25,130 --> 00:23:29,000
solution that that both of those things

00:23:27,050 --> 00:23:31,930
can actually happen with pretty much the

00:23:29,000 --> 00:23:31,930
exact same hardware

00:23:41,670 --> 00:23:52,900
so you can see we're starting to power

00:23:43,600 --> 00:23:55,860
on systems in the in the back there it's

00:23:52,900 --> 00:24:02,920
about four feet deep and it is full

00:23:55,860 --> 00:24:05,470
right the it's pretty much immutable at

00:24:02,920 --> 00:24:07,390
this point there are you know there's an

00:24:05,470 --> 00:24:10,030
amazing amount of CAD and design that

00:24:07,390 --> 00:24:13,590
goes into layering you know the water

00:24:10,030 --> 00:24:15,910
and the power and the cable tray and

00:24:13,590 --> 00:24:18,340
everything is there's really there is no

00:24:15,910 --> 00:24:24,090
space there's no crawling right that it

00:24:18,340 --> 00:24:27,390
is completely full of infrastructure now

00:24:24,090 --> 00:24:27,390
yeah Chris

00:24:28,560 --> 00:24:32,830
so floor loading is definitely a thing

00:24:31,720 --> 00:24:36,220
right

00:24:32,830 --> 00:24:38,410
this floor is is half as strong as our

00:24:36,220 --> 00:24:40,000
as our newest floor because everything

00:24:38,410 --> 00:24:43,300
keeps getting getting heavier so yes

00:24:40,000 --> 00:24:45,850
there are concerns on on Sequoia which

00:24:43,300 --> 00:24:49,270
had a which had a really heavy compute

00:24:45,850 --> 00:24:52,390
rack we actually had to build these

00:24:49,270 --> 00:24:54,850
giant steel platforms and replaced the

00:24:52,390 --> 00:24:57,730
section of floor with these platforms

00:24:54,850 --> 00:25:00,370
that actually sighted the racks and this

00:24:57,730 --> 00:25:03,040
system is actually a little more kind on

00:25:00,370 --> 00:25:05,410
floor loading it turns out you also may

00:25:03,040 --> 00:25:06,880
have noticed that there's a bunch of

00:25:05,410 --> 00:25:08,890
platforms that went underneath these

00:25:06,880 --> 00:25:10,870
racks before before they went down

00:25:08,890 --> 00:25:13,510
that's cuz we're in California where

00:25:10,870 --> 00:25:17,620
sometimes the ground shakes and so those

00:25:13,510 --> 00:25:20,020
arms are actually kind of like a large

00:25:17,620 --> 00:25:22,030
building where they have has a kind of a

00:25:20,020 --> 00:25:24,670
ball it has a big old ball bearing in a

00:25:22,030 --> 00:25:28,470
concave dish and so it allows the floor

00:25:24,670 --> 00:25:30,750
to move while the computer stays still

00:25:28,470 --> 00:25:35,380
so that's that's what that's about

00:25:30,750 --> 00:25:37,120
so I think I have just time for a couple

00:25:35,380 --> 00:25:44,370
of questions pretty much right on time

00:25:37,120 --> 00:25:44,370
does anybody have any questions well

00:25:46,240 --> 00:25:53,710
so that time-lapse goes from like July

00:25:52,090 --> 00:25:56,530
when we started the facilities work

00:25:53,710 --> 00:25:59,110
through the end of December when we

00:25:56,530 --> 00:26:00,430
finished the 25 percent the first 25

00:25:59,110 --> 00:26:03,550
percent of the system was accepted and

00:26:00,430 --> 00:26:04,960
then we'll have another one that will

00:26:03,550 --> 00:26:06,160
cover the whole thing that's kind of a

00:26:04,960 --> 00:26:08,080
rough at it we would try to make them

00:26:06,160 --> 00:26:10,960
cleaner than that but but this is where

00:26:08,080 --> 00:26:27,220
we were where we were at so for four or

00:26:10,960 --> 00:26:28,750
five months yeah so we made a

00:26:27,220 --> 00:26:31,390
significant investment in early access

00:26:28,750 --> 00:26:33,250
systems right those are you know one of

00:26:31,390 --> 00:26:35,860
those is like a petaflop resource right

00:26:33,250 --> 00:26:37,420
right so we're gonna do a couple

00:26:35,860 --> 00:26:38,560
different things with them one is we

00:26:37,420 --> 00:26:40,240
really want to get the software

00:26:38,560 --> 00:26:42,250
environment on them to be the same as a

00:26:40,240 --> 00:26:46,240
sierra software environment and we have

00:26:42,250 --> 00:26:48,070
not quite closed that yet I think is a

00:26:46,240 --> 00:26:49,810
fair statement so we want that

00:26:48,070 --> 00:26:52,480
environment to be the same so that

00:26:49,810 --> 00:26:55,120
people can import back and forth as

00:26:52,480 --> 00:26:57,190
easily as possible not just their codes

00:26:55,120 --> 00:26:59,590
which are already pretty much completely

00:26:57,190 --> 00:27:01,570
portable between them but also their run

00:26:59,590 --> 00:27:03,820
scripts and you know all that kind of

00:27:01,570 --> 00:27:07,390
ancillary software plumbing that allows

00:27:03,820 --> 00:27:09,760
you to actually run on HPC system in the

00:27:07,390 --> 00:27:12,400
case of Rae we also we got we got a

00:27:09,760 --> 00:27:13,840
little extra money to do some some data

00:27:12,400 --> 00:27:14,830
intensive computing work on that

00:27:13,840 --> 00:27:17,170
machines we're able to make a little

00:27:14,830 --> 00:27:19,690
bigger so it's going to also transition

00:27:17,170 --> 00:27:23,800
into doing a lot of data intensive work

00:27:19,690 --> 00:27:27,100
right so we're we're heavily IIST

00:27:23,800 --> 00:27:28,750
towards the you know physics molecular

00:27:27,100 --> 00:27:31,200
dynamics multiphysics

00:27:28,750 --> 00:27:35,320
you know simulate the the real-world

00:27:31,200 --> 00:27:37,090
problem space for the ASC program but

00:27:35,320 --> 00:27:38,920
Livermore is a big multidisciplinary lab

00:27:37,090 --> 00:27:42,790
just like Oak Ridge and if people doing

00:27:38,920 --> 00:27:44,080
all sorts of different stuff so we you

00:27:42,790 --> 00:27:45,910
know we're going to definitely use them

00:27:44,080 --> 00:27:54,300
for some of that other other work as

00:27:45,910 --> 00:27:54,300
well anything else yes sir

00:28:23,000 --> 00:28:28,260
yeah so the question was about running a

00:28:25,860 --> 00:28:30,660
you knows a blue jean custom cnk

00:28:28,260 --> 00:28:34,610
microkernel versus the you know the full

00:28:30,660 --> 00:28:39,270
Red Hat basically commodity kernel on on

00:28:34,610 --> 00:28:41,250
Sierra and you know from a purely

00:28:39,270 --> 00:28:45,660
technical perspective there were some

00:28:41,250 --> 00:28:47,670
great things about the the cnk right the

00:28:45,660 --> 00:28:50,220
runs here on reproducibility in terms of

00:28:47,670 --> 00:28:53,790
performance and expectations with that

00:28:50,220 --> 00:28:55,260
microkernel was you know near-perfect so

00:28:53,790 --> 00:28:57,630
when people would make changes to their

00:28:55,260 --> 00:28:59,460
code and run performance studies

00:28:57,630 --> 00:29:01,799
back-to-back to see whether they had you

00:28:59,460 --> 00:29:04,110
know improved or not improved things you

00:29:01,799 --> 00:29:06,270
know it was very very easy for them to

00:29:04,110 --> 00:29:08,580
make very accurate comparison between

00:29:06,270 --> 00:29:12,450
the old of the new and also meant that

00:29:08,580 --> 00:29:15,390
we were able to keep OS noise to a near

00:29:12,450 --> 00:29:18,150
zero level in fact some guys from

00:29:15,390 --> 00:29:20,700
Argonne did a study on a blue gene where

00:29:18,150 --> 00:29:22,460
they basically considered the blue gene

00:29:20,700 --> 00:29:25,169
to be a zero noise environment and

00:29:22,460 --> 00:29:26,820
injected false noise to look at what the

00:29:25,169 --> 00:29:31,830
performance that implications were to an

00:29:26,820 --> 00:29:33,330
MPI application now on on Sierra you

00:29:31,830 --> 00:29:38,640
know we have that kind of normal

00:29:33,330 --> 00:29:40,470
slightly noisy system and so we've had

00:29:38,640 --> 00:29:43,110
to make some some big technology

00:29:40,470 --> 00:29:46,380
investments in trying to reduce OS noise

00:29:43,110 --> 00:29:48,030
and I think that both computer centers

00:29:46,380 --> 00:29:51,780
are planning the set of soft and cores

00:29:48,030 --> 00:29:54,240
just for OS tasks in order to reduce the

00:29:51,780 --> 00:29:55,950
impact of noise I think it's kind of

00:29:54,240 --> 00:29:57,809
it's an ongoing area exactly how many

00:29:55,950 --> 00:30:00,600
cores were each going to end up setting

00:29:57,809 --> 00:30:02,190
aside and you know whether users with

00:30:00,600 --> 00:30:04,410
applications will be able to select how

00:30:02,190 --> 00:30:06,929
many cores based on the application

00:30:04,410 --> 00:30:10,040
needs balance against like their io /

00:30:06,929 --> 00:30:12,059
formitz needs which you know gpfs can

00:30:10,040 --> 00:30:13,470
have it has to do a lot of work and

00:30:12,059 --> 00:30:14,880
therefore can create noise right that's

00:30:13,470 --> 00:30:15,900
those are compute cycles that aren't

00:30:14,880 --> 00:30:19,770
going to your application if they're

00:30:15,900 --> 00:30:23,549
going to to GPFS right as far as the the

00:30:19,770 --> 00:30:25,950
movement of the technology and for

00:30:23,549 --> 00:30:29,070
better or for worse the sequoia system

00:30:25,950 --> 00:30:30,240
started with a particular set of you

00:30:29,070 --> 00:30:32,010
know functioning system calls in

00:30:30,240 --> 00:30:34,440
environment and it pretty much didn't

00:30:32,010 --> 00:30:36,330
change so the good news is if you were

00:30:34,440 --> 00:30:39,240
able to run on day one its

00:30:36,330 --> 00:30:40,950
works today six years later you can't

00:30:39,240 --> 00:30:42,840
really underestimate how valuable that

00:30:40,950 --> 00:30:45,750
is to an application team on the other

00:30:42,840 --> 00:30:49,559
hand you know some of our abstraction

00:30:45,750 --> 00:30:52,019
layers rely on C++ 11 and 13 and and 15

00:30:49,559 --> 00:30:55,799
features right and we don't get any of

00:30:52,019 --> 00:30:57,390
those features in sequoia because the

00:30:55,799 --> 00:31:00,299
software stack is frozen six years ago

00:30:57,390 --> 00:31:03,059
pretty much so there are they are

00:31:00,299 --> 00:31:05,700
upsides and downsides you know whereas

00:31:03,059 --> 00:31:07,559
the the Red Hat stack on on sierra has

00:31:05,700 --> 00:31:11,120
everything at Red Hat has in it and it

00:31:07,559 --> 00:31:13,200
continues to move with Red Hat so I

00:31:11,120 --> 00:31:15,120
can't say one is better than the other

00:31:13,200 --> 00:31:16,710
they're different and you know there's

00:31:15,120 --> 00:31:19,100
there's very real reasons to make both

00:31:16,710 --> 00:31:19,100

YouTube URL: https://www.youtube.com/watch?v=AFPsLRVPIek


