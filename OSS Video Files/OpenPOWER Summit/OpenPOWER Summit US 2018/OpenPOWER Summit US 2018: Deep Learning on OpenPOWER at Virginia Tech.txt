Title: OpenPOWER Summit US 2018: Deep Learning on OpenPOWER at Virginia Tech
Publication date: 2018-04-06
Playlist: OpenPOWER Summit US 2018
Description: 
	An overview of Virginia Tech leveraged distributed deep learning by creating a 16,000 layers deep model at the OpenPOWER Summit 2018.

For more information please visit www.openpowerfoundation.org.
Captions: 
	00:00:00,290 --> 00:00:08,550
hello everybody so my name is Ahmed

00:00:05,370 --> 00:00:11,969
Ibrahim I'm a computational scientist in

00:00:08,550 --> 00:00:17,550
Virginia Tech my job is to support

00:00:11,969 --> 00:00:20,279
researchers to better utilize our IBM

00:00:17,550 --> 00:00:22,199
Bauer cluster which we bought

00:00:20,279 --> 00:00:28,740
specifically for deep learning

00:00:22,199 --> 00:00:30,330
applications let me give you a brief

00:00:28,740 --> 00:00:35,430
introduction about Virginia Tech but

00:00:30,330 --> 00:00:39,030
genetic is a 35,000 students land-grant

00:00:35,430 --> 00:00:42,719
University it has seven colleges over

00:00:39,030 --> 00:00:46,500
100 Department and plenty of

00:00:42,719 --> 00:00:50,120
institutions by a complexity Life

00:00:46,500 --> 00:00:53,280
Science Institute creativity and art

00:00:50,120 --> 00:00:56,399
transportation Karelian for medical

00:00:53,280 --> 00:01:04,320
applications so we have a huge amount of

00:00:56,399 --> 00:01:06,750
researchers doing variety of research I

00:01:04,320 --> 00:01:08,580
want to I wanted to give a brief

00:01:06,750 --> 00:01:11,159
introduction about machine learning and

00:01:08,580 --> 00:01:14,310
deep learning but the guys and the

00:01:11,159 --> 00:01:15,750
earlier session stole my thunder already

00:01:14,310 --> 00:01:18,000
and except Lee and a lot of stuff about

00:01:15,750 --> 00:01:21,060
machine learning and deep learning but

00:01:18,000 --> 00:01:24,140
and you already know that one of the

00:01:21,060 --> 00:01:27,930
bottlenecks of deep learning is a

00:01:24,140 --> 00:01:31,250
bandwidth between CPU and GPU data

00:01:27,930 --> 00:01:38,460
transfer and I will talk about this

00:01:31,250 --> 00:01:40,850
shortly so the verge a tech deep

00:01:38,460 --> 00:01:46,380
learning system we call it huckleberry

00:01:40,850 --> 00:01:52,259
it's on IBM Minsky compute nodes this is

00:01:46,380 --> 00:01:56,939
a the power eight compute nodes each

00:01:52,259 --> 00:02:00,210
node have to bar eight CPUs blast for

00:01:56,939 --> 00:02:04,399
NVIDIA be 100 GB you and we have 14 of

00:02:00,210 --> 00:02:07,560
them we have two of them reserve it for

00:02:04,399 --> 00:02:10,560
testing purposes so what's actually

00:02:07,560 --> 00:02:13,840
being utilized for end users are 12

00:02:10,560 --> 00:02:17,340
nodes that's total of 48 GPUs

00:02:13,840 --> 00:02:22,390
and of course we have the envy link

00:02:17,340 --> 00:02:26,190
version of the about eight and as you

00:02:22,390 --> 00:02:30,970
can see in this graph from Nvidia

00:02:26,190 --> 00:02:35,020
scaling up having multiple GPUs on the

00:02:30,970 --> 00:02:39,640
same node with such high Throwbot

00:02:35,020 --> 00:02:45,880
connection such as envy link scale up

00:02:39,640 --> 00:02:55,390
pretty well into what we call lots of

00:02:45,880 --> 00:02:58,690
wheat on this system we have the power

00:02:55,390 --> 00:03:01,390
AI platform it has a lot of deep

00:02:58,690 --> 00:03:03,790
learning frameworks cafe and we have I

00:03:01,390 --> 00:03:08,260
think three versions of coffee

00:03:03,790 --> 00:03:13,709
on this on the system cafe IBM coffee

00:03:08,260 --> 00:03:16,900
and the or and our own version of coffee

00:03:13,709 --> 00:03:19,980
also there is torrents of Lothian or

00:03:16,900 --> 00:03:29,560
China and also we could combine our own

00:03:19,980 --> 00:03:36,579
software so the system came to virgin

00:03:29,560 --> 00:03:38,410
etic around March April 2017 and of

00:03:36,579 --> 00:03:41,230
course we had to do some integration

00:03:38,410 --> 00:03:44,950
with infrastructure we have Mellanox

00:03:41,230 --> 00:03:49,329
network and we have to add login nodes

00:03:44,950 --> 00:03:53,049
and integrate the LDAP authentication

00:03:49,329 --> 00:03:55,900
and follow the IT policy for genetic

00:03:53,049 --> 00:03:59,079
that wasn't it that was not easy and

00:03:55,900 --> 00:04:00,880
then during the high performance

00:03:59,079 --> 00:04:02,440
computing day we do an annual high

00:04:00,880 --> 00:04:07,060
performance computing day at Virginia

00:04:02,440 --> 00:04:10,269
Tech we did some demos on on the

00:04:07,060 --> 00:04:12,730
huckleberry cluster and then we provided

00:04:10,269 --> 00:04:15,280
an early access to subset of researchers

00:04:12,730 --> 00:04:17,620
to give us some feedback about the

00:04:15,280 --> 00:04:21,810
cluster what they need

00:04:17,620 --> 00:04:24,280
what are they their concerns and then

00:04:21,810 --> 00:04:25,970
the system was open to the entire

00:04:24,280 --> 00:04:31,270
genetic research community and

00:04:25,970 --> 00:04:35,600
September 2017 and then kindly IBM came

00:04:31,270 --> 00:04:39,410
near the end of September and gave two

00:04:35,600 --> 00:04:42,380
day workshop to introduce the system to

00:04:39,410 --> 00:04:49,370
Virgin etic researchers first day war

00:04:42,380 --> 00:04:55,160
was a set of speeches by IBM and

00:04:49,370 --> 00:05:00,770
Mullenix representatives and we had 770

00:04:55,160 --> 00:05:02,720
attendees and then in the next day we

00:05:00,770 --> 00:05:07,100
have hands-on Tutera tutorials on the

00:05:02,720 --> 00:05:10,400
IBM bar AI software some work workshops

00:05:07,100 --> 00:05:13,640
using Jupiter notebook and a little bit

00:05:10,400 --> 00:05:18,020
about distributed deep learning and this

00:05:13,640 --> 00:05:21,500
is a picture from the open bar workshop

00:05:18,020 --> 00:05:24,290
at Virginia Tech I will give you a

00:05:21,500 --> 00:05:27,410
little bit of research highlights what

00:05:24,290 --> 00:05:30,800
we are doing in Virginia Tech this is a

00:05:27,410 --> 00:05:33,440
work of dr. Ryan Williams group in

00:05:30,800 --> 00:05:38,440
electric and Computer Engineering he's

00:05:33,440 --> 00:05:42,200
doing deserted autonomous systems so

00:05:38,440 --> 00:05:44,840
computational intensive simulations for

00:05:42,200 --> 00:05:48,380
coordinated motion control multi-agent

00:05:44,840 --> 00:05:55,910
systems decision making and multi-agent

00:05:48,380 --> 00:05:58,070
systems Markov models and this has a lot

00:05:55,910 --> 00:06:08,720
of applications of course and defense

00:05:58,070 --> 00:06:13,100
and agriculture as well our next biggest

00:06:08,720 --> 00:06:18,680
group is jeolban Huang group they are

00:06:13,100 --> 00:06:22,310
doing computer vision research so and

00:06:18,680 --> 00:06:26,840
they have a lot of publication in top

00:06:22,310 --> 00:06:29,890
conferences and journals they are our

00:06:26,840 --> 00:06:35,590
biggest users of huckleberry right now

00:06:29,890 --> 00:06:35,590
they are doing semantic segmentation

00:06:35,950 --> 00:06:48,509
video completion

00:06:39,240 --> 00:06:53,110
regular classification so another

00:06:48,509 --> 00:06:55,389
research we are doing in virgin etic and

00:06:53,110 --> 00:06:58,389
this is done by me and another professor

00:06:55,389 --> 00:07:02,979
and Virginia Tech is trying to boost the

00:06:58,389 --> 00:07:06,240
limits of IBM power systems because we

00:07:02,979 --> 00:07:13,710
can see potential in in the power system

00:07:06,240 --> 00:07:13,710
so you know deep learning started around

00:07:14,009 --> 00:07:20,189
98 with only 5 layers and this was

00:07:18,520 --> 00:07:27,009
called deep learning with the Linnet

00:07:20,189 --> 00:07:28,439
architecture and then in 2012 the 8

00:07:27,009 --> 00:07:31,569
layer

00:07:28,439 --> 00:07:35,999
alex net1 the imagenet competition and

00:07:31,569 --> 00:07:42,250
this was also called deep model so and

00:07:35,999 --> 00:07:45,939
in only 4 years it went from 8 layers to

00:07:42,250 --> 00:07:48,729
around 1,000 layers and 1000 layer model

00:07:45,939 --> 00:07:53,680
is called ResNet if you are familiar

00:07:48,729 --> 00:07:56,740
with the deep network architectures so

00:07:53,680 --> 00:08:01,810
this 1,000 layers model is created by

00:07:56,740 --> 00:08:07,210
Microsoft Japan and at 1 2016 imagenet

00:08:01,810 --> 00:08:10,810
competition so and they stopped at 1000

00:08:07,210 --> 00:08:15,219
layer for a reason because after that

00:08:10,810 --> 00:08:17,620
you start to push the system limits from

00:08:15,219 --> 00:08:20,979
many aspects from the design of the

00:08:17,620 --> 00:08:22,300
network itself and from the system

00:08:20,979 --> 00:08:24,399
limits GPU memory

00:08:22,300 --> 00:08:27,250
how many GPUs do you have the bandwidth

00:08:24,399 --> 00:08:31,770
between CPU and GPU you start to push

00:08:27,250 --> 00:08:31,770
the limits a lot and we started to go be

00:08:32,279 --> 00:08:45,660
what Microsoft did by doing 4000 layer

00:08:36,130 --> 00:08:50,840
in 2017 4000 layers model that run on

00:08:45,660 --> 00:08:52,730
our IDM Bower cluster so and this

00:08:50,840 --> 00:08:56,060
we have many other clusters we have

00:08:52,730 --> 00:08:58,670
around four or five other clusters only

00:08:56,060 --> 00:09:02,600
three of them have GPUs one of them have

00:08:58,670 --> 00:09:04,700
be 100 GPUs and this model easily run

00:09:02,600 --> 00:09:08,870
out of memory on the other clusters it

00:09:04,700 --> 00:09:12,440
can only run on Huckleberry and right

00:09:08,870 --> 00:09:16,750
now in 2018 we are trying to the limits

00:09:12,440 --> 00:09:20,390
more by creating 16,000 layers

00:09:16,750 --> 00:09:23,540
model and that's not easy in many

00:09:20,390 --> 00:09:26,570
aspects we already fail the GPU memory

00:09:23,540 --> 00:09:29,450
the 16 gigabyte of the GPU memory with

00:09:26,570 --> 00:09:33,560
the 4000 layers with a very small batch

00:09:29,450 --> 00:09:36,470
size we are using we are doing image

00:09:33,560 --> 00:09:40,790
classification on a small images 62 by

00:09:36,470 --> 00:09:43,250
62 RGB images with batch size of 32

00:09:40,790 --> 00:09:46,550
which is pretty low batch size already

00:09:43,250 --> 00:09:52,820
so we cannot go any lower and our main

00:09:46,550 --> 00:09:57,680
idea is to implement this model over 4gb

00:09:52,820 --> 00:10:00,890
use so run there's the same model over

00:09:57,680 --> 00:10:02,900
4gb use and this this is not a new idea

00:10:00,890 --> 00:10:05,960
about nobody implemented it because

00:10:02,900 --> 00:10:09,410
usually your model will fit into a

00:10:05,960 --> 00:10:14,750
single GPU and you process multiple data

00:10:09,410 --> 00:10:19,490
streams on multiple GPUs but running the

00:10:14,750 --> 00:10:22,670
same model on multiple GPUs this would

00:10:19,490 --> 00:10:25,580
be the first of its kind to be created

00:10:22,670 --> 00:10:30,200
and we are hitting the limits from many

00:10:25,580 --> 00:10:32,540
aspects as we said the limits of the

00:10:30,200 --> 00:10:34,850
systems and also the limits of the

00:10:32,540 --> 00:10:37,970
tipline in packages itself so we found

00:10:34,850 --> 00:10:42,020
out that tensor flow that is the flow

00:10:37,970 --> 00:10:47,930
package cannot create models more than

00:10:42,020 --> 00:10:51,130
around 4 or 5,000 layers because how the

00:10:47,930 --> 00:10:54,440
developers of the Akkad's

00:10:51,130 --> 00:10:58,580
implemented the meta architecture like

00:10:54,440 --> 00:11:01,220
the data structure to create to store

00:10:58,580 --> 00:11:04,460
the meta information of the network the

00:11:01,220 --> 00:11:08,460
name of each layer how many

00:11:04,460 --> 00:11:10,890
how many parameters in each layer and

00:11:08,460 --> 00:11:15,830
all the meta information about the model

00:11:10,890 --> 00:11:23,070
when you go beyond 4,000 to 5,000 layers

00:11:15,830 --> 00:11:27,540
you really hit also them the deep

00:11:23,070 --> 00:11:33,060
learning package limits so we we are

00:11:27,540 --> 00:11:36,540
faced with plenty of weird errors and

00:11:33,060 --> 00:11:40,380
issues when we are trying to create this

00:11:36,540 --> 00:11:45,150
model so our plan is to utilize the 48

00:11:40,380 --> 00:11:49,860
GPUs so running each model on 4gb use in

00:11:45,150 --> 00:11:53,190
a single node and then use 12 nodes to

00:11:49,860 --> 00:11:59,130
feed data into the same model for

00:11:53,190 --> 00:12:01,890
training the 2017 model is already under

00:11:59,130 --> 00:12:07,170
review and better recognition letters

00:12:01,890 --> 00:12:09,590
and the 2018 model is still under under

00:12:07,170 --> 00:12:09,590
development

00:12:17,560 --> 00:12:23,660
image classification we are using so we

00:12:22,100 --> 00:12:26,270
are using so first we are brought to

00:12:23,660 --> 00:12:27,770
typing on em nest the data said that the

00:12:26,270 --> 00:12:29,510
guys earlier talked about the

00:12:27,770 --> 00:12:32,360
handwritten digit recognition this is a

00:12:29,510 --> 00:12:34,640
very tiny data set everybody's using for

00:12:32,360 --> 00:12:37,640
prototyping but we are also but this is

00:12:34,640 --> 00:12:39,980
not a realistic situation there is no

00:12:37,640 --> 00:12:43,250
real that I like em nest anymore it is

00:12:39,980 --> 00:12:45,260
very tiny so we are prototyping using em

00:12:43,250 --> 00:12:50,050
nest and we are using another that set

00:12:45,260 --> 00:12:52,910
called C 410 it's developed by the c4

00:12:50,050 --> 00:12:57,140
institution in Canada this is an image

00:12:52,910 --> 00:13:00,590
classification data set

00:12:57,140 --> 00:13:01,550
it has RGB images of different kinds of

00:13:00,590 --> 00:13:04,550
objects horse

00:13:01,550 --> 00:13:17,750
Blayne chip and then your task is to

00:13:04,550 --> 00:13:21,740
classify the input image bigger images

00:13:17,750 --> 00:13:24,170
yeah like not not actually it's not much

00:13:21,740 --> 00:13:27,110
more mixes it's that challenge actually

00:13:24,170 --> 00:13:28,940
is that it is colored images versus

00:13:27,110 --> 00:13:30,890
black and white and M list illnesses

00:13:28,940 --> 00:13:34,850
black and white it's not even grayscale

00:13:30,890 --> 00:13:38,630
so it's just 0 & 1 but when you use RGB

00:13:34,850 --> 00:13:41,420
that's that's much bigger than but

00:13:38,630 --> 00:13:44,600
nearly the same resolution next step is

00:13:41,420 --> 00:13:49,360
to apply image net which is a much

00:13:44,600 --> 00:13:49,360
bigger data set you had question

00:13:55,140 --> 00:14:00,250
yeah it's still under development we are

00:13:57,550 --> 00:14:02,860
doing it so you never you never hear me

00:14:00,250 --> 00:14:04,570
and also the 4000 layer I don't think

00:14:02,860 --> 00:14:07,380
you have heard of it because this is

00:14:04,570 --> 00:14:07,380
still under review

00:14:12,030 --> 00:14:20,650
yeah and that's thousand or their there

00:14:15,820 --> 00:14:24,100
is a 1000 version and 1200 version but

00:14:20,650 --> 00:14:27,220
yeah that's that's yeah the biggest one

00:14:24,100 --> 00:14:30,580
till now but there is a difference

00:14:27,220 --> 00:14:41,200
between the dibs - and so that's that's

00:14:30,580 --> 00:14:43,300
not that yeah it doesn't mean it doesn't

00:14:41,200 --> 00:14:45,280
mean more weights of course but that's

00:14:43,300 --> 00:14:49,210
not the winner of the imagenet

00:14:45,280 --> 00:14:51,760
competition so Microsoft created two

00:14:49,210 --> 00:14:55,510
versions of their network one of them is

00:14:51,760 --> 00:14:57,970
150 and that's that that achieves a

00:14:55,510 --> 00:15:00,070
better accuracy and this one they were

00:14:57,970 --> 00:15:04,360
just showing that their model can push

00:15:00,070 --> 00:15:07,000
the limits so so this model actually

00:15:04,360 --> 00:15:09,840
does not perform very well there 100 or

00:15:07,000 --> 00:15:09,840
1000 layers

00:15:15,170 --> 00:15:25,489
so we are trying to push the limits and

00:15:18,290 --> 00:15:28,399
also so why adding many layers work and

00:15:25,489 --> 00:15:30,529
there is there is a joke that says deep

00:15:28,399 --> 00:15:32,869
learning people are just clowns adding

00:15:30,529 --> 00:15:37,100
more layers and that's not that's not

00:15:32,869 --> 00:15:39,410
true I guess when you add so here it is

00:15:37,100 --> 00:15:42,980
theoretically any model can be

00:15:39,410 --> 00:15:44,629
represented by a single layer model any

00:15:42,980 --> 00:15:47,239
problem can be solved by a single layer

00:15:44,629 --> 00:15:50,660
model with enough parameters so it's

00:15:47,239 --> 00:15:52,730
like a wide network but what is the

00:15:50,660 --> 00:16:01,329
benefit of adding layer it's like you

00:15:52,730 --> 00:16:01,329
add in complexities so yeah

00:16:26,640 --> 00:16:33,220
yeah so as I as I just said the main

00:16:30,940 --> 00:16:37,980
motivation behind this is just to push

00:16:33,220 --> 00:16:41,500
the limits and think so

00:16:37,980 --> 00:16:44,320
and there is applications for this think

00:16:41,500 --> 00:16:46,030
of RNN recurrent neural network

00:16:44,320 --> 00:16:47,920
recurrent neural network is is a

00:16:46,030 --> 00:16:50,710
completely convolutional neural network

00:16:47,920 --> 00:16:53,860
and then you best the data multiple

00:16:50,710 --> 00:16:56,980
times and then you can expand it into a

00:16:53,860 --> 00:16:59,080
very deep model at the end so can you

00:16:56,980 --> 00:17:02,170
run it on a single GPU you can't do that

00:16:59,080 --> 00:17:06,850
of course but maybe with this being

00:17:02,170 --> 00:17:09,459
tested and we know how to to run very

00:17:06,850 --> 00:17:11,199
deep models we can expand our in n to

00:17:09,459 --> 00:17:15,910
achieve better performance because RNN

00:17:11,199 --> 00:17:24,430
are slow because you run iterations so

00:17:15,910 --> 00:17:28,329
yeah I agree with you I agree that this

00:17:24,430 --> 00:17:30,730
doesn't seem very strong if you just

00:17:28,329 --> 00:17:32,530
look at it as adding more layers but we

00:17:30,730 --> 00:17:34,980
are pushing the system limits and we

00:17:32,530 --> 00:17:40,000
discovered that test flow cannot create

00:17:34,980 --> 00:17:42,370
more than 4,000 layers but we didn't try

00:17:40,000 --> 00:17:44,500
this this with a new version of tensor

00:17:42,370 --> 00:17:48,430
flow because the new version of tensor

00:17:44,500 --> 00:17:51,490
flow requires for the line and code

00:17:48,430 --> 00:17:53,890
online requires new drivers so it's not

00:17:51,490 --> 00:17:56,080
easy it's not just Pippin stole tensor

00:17:53,890 --> 00:17:57,550
flow anymore we need to update the whole

00:17:56,080 --> 00:18:00,100
system to get the new version of this

00:17:57,550 --> 00:18:05,740
flow and we are waiting for that which

00:18:00,100 --> 00:18:09,330
will happen soon and we will try to see

00:18:05,740 --> 00:18:12,370
if this limitation still exists in 1.5

00:18:09,330 --> 00:18:16,990
the other thing we are waiting for is we

00:18:12,370 --> 00:18:20,500
already ordered bar 9 some bar 9 nodes

00:18:16,990 --> 00:18:22,740
and we will see how it will perform was

00:18:20,500 --> 00:18:22,740
such

00:18:22,900 --> 00:18:36,070
the last thing I will talk about is is

00:18:30,540 --> 00:18:40,720
so we started by trying to to implement

00:18:36,070 --> 00:18:44,380
em nest on to run on multiple GPUs and

00:18:40,720 --> 00:18:47,380
see what's what's what is the return

00:18:44,380 --> 00:18:51,310
what what performance gain can we get

00:18:47,380 --> 00:18:53,740
from just running multiple streams of

00:18:51,310 --> 00:18:56,170
data so that's what we are trying to do

00:18:53,740 --> 00:18:59,650
simple model on multiple GPUs using data

00:18:56,170 --> 00:19:01,450
parallelism and we try different batch

00:18:59,650 --> 00:19:03,610
sizes because this is another hyper

00:19:01,450 --> 00:19:05,250
parameter we should take care of and

00:19:03,610 --> 00:19:09,070
then we measure the training time

00:19:05,250 --> 00:19:12,670
required to train this model 410 a box

00:19:09,070 --> 00:19:18,270
so run over the data set ten times and

00:19:12,670 --> 00:19:23,200
we found out that with there is a linear

00:19:18,270 --> 00:19:27,160
gain in the time required to train this

00:19:23,200 --> 00:19:31,510
model over ten a box with different

00:19:27,160 --> 00:19:35,710
batch sizes and if you see the y-axis

00:19:31,510 --> 00:19:39,520
you will see that this is not a 100%

00:19:35,710 --> 00:19:44,560
efficient system so the gain you get by

00:19:39,520 --> 00:19:48,370
using 4gb use is not four times the gain

00:19:44,560 --> 00:19:53,260
right so we still need to adjust many

00:19:48,370 --> 00:19:56,530
parameters to get better performance but

00:19:53,260 --> 00:19:59,800
this is this is promising and this

00:19:56,530 --> 00:20:01,870
cannot be done on any of our other

00:19:59,800 --> 00:20:04,320
clusters because our other clusters does

00:20:01,870 --> 00:20:09,210
not have 4gb use in a single node zone

00:20:04,320 --> 00:20:11,740
this was impressive and I think the code

00:20:09,210 --> 00:20:16,930
that we used to create this graph is

00:20:11,740 --> 00:20:20,970
available online on my github so that's

00:20:16,930 --> 00:20:20,970
all thank you and any questions

00:20:24,130 --> 00:20:36,290
okay thank you yeah sure yeah so that's

00:20:34,160 --> 00:20:42,590
another hyper barometer to to take care

00:20:36,290 --> 00:20:44,900
of the four thousand so the 16 K we

00:20:42,590 --> 00:20:46,730
didn't implement it just floated and let

00:20:44,900 --> 00:20:48,890
us implement but then when you add more

00:20:46,730 --> 00:20:50,690
layers you can reduce the number of

00:20:48,890 --> 00:20:54,620
parameters in each layer and ended up

00:20:50,690 --> 00:20:58,280
with the same size so this this network

00:20:54,620 --> 00:21:00,860
this 4,000 layer network is the same

00:20:58,280 --> 00:21:03,500
size as the 1000 layer network so it is

00:21:00,860 --> 00:21:11,680
the same size so we make it thinner and

00:21:03,500 --> 00:21:11,680

YouTube URL: https://www.youtube.com/watch?v=KvNBrbZBppY


