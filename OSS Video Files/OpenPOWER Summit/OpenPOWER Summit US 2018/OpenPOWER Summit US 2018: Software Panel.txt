Title: OpenPOWER Summit US 2018: Software Panel
Publication date: 2018-04-02
Playlist: OpenPOWER Summit US 2018
Description: 
	Members of the OpenPOWER Foundation discuss software at OpenPOWER Summit 2018.

Panel members include:
- Ari Juntunen, CTO, Elinar
- Richard Heyns, Founder and CEO, BrytLyt
- Vinod Lyengar, Director of Marketing & Alliances, H2O
- Ashish Bambroo, VP of Business Development, MapD
- Woody Christy, Head of Partner Engineering, Kinetica
- Robbie Williamson, VP, Field CTO, Canonical

For more information, please visit: http://www.openpowerfoundation.org
Captions: 
	00:00:00,610 --> 00:00:07,599
[Music]

00:00:09,740 --> 00:00:12,890
[Music]

00:00:13,430 --> 00:00:16,320
so I'm gonna kind of jump right into

00:00:15,330 --> 00:00:18,029
this cuz I know we're a little bit

00:00:16,320 --> 00:00:22,140
behind and we have a few more talks and

00:00:18,029 --> 00:00:27,359
I'm good okay alright I'm getting hungry

00:00:22,140 --> 00:00:29,670
I guess the most minute sorry so I guess

00:00:27,359 --> 00:00:33,510
I'll start with woody from Connecticut

00:00:29,670 --> 00:00:36,449
real quick here and just briefly just

00:00:33,510 --> 00:00:39,559
discuss I guess how you're how I guess

00:00:36,449 --> 00:00:41,899
the power platform itself has a impacted

00:00:39,559 --> 00:00:44,219
your products performance in general

00:00:41,899 --> 00:00:45,840
yeah as others have been mentioning

00:00:44,219 --> 00:00:47,129
earlier today one of the big advantages

00:00:45,840 --> 00:00:49,829
we have with power

00:00:47,129 --> 00:00:51,840
is the host of buzz communication on n V

00:00:49,829 --> 00:00:53,520
Lincoln in b-wing 2 on power 8 in power

00:00:51,840 --> 00:00:55,649
9 and what

00:00:53,520 --> 00:00:58,739
Kinetico the company I work for we have

00:00:55,649 --> 00:01:00,510
a gpu-accelerated an insight engine what

00:00:58,739 --> 00:01:03,059
that is is we we pushed a lot of the

00:01:00,510 --> 00:01:05,309
data processing off to the GPU and the

00:01:03,059 --> 00:01:07,170
big limitation we have as host of us and

00:01:05,309 --> 00:01:08,940
so over an intel-based system which we

00:01:07,170 --> 00:01:10,369
also run on we see between two and a

00:01:08,940 --> 00:01:12,780
half to four and a half times

00:01:10,369 --> 00:01:15,210
performance improvement and what that

00:01:12,780 --> 00:01:16,650
does is that drives a lower TCO and as

00:01:15,210 --> 00:01:18,450
Maura said from Google that's one of the

00:01:16,650 --> 00:01:20,610
big issues we have in today's industry

00:01:18,450 --> 00:01:22,740
and the software is that it costs so

00:01:20,610 --> 00:01:24,630
much more to get more performance it's

00:01:22,740 --> 00:01:27,570
definitely impacting us and what we see

00:01:24,630 --> 00:01:29,369
is the lower TCO on power it's actually

00:01:27,570 --> 00:01:31,409
driving customers to be able to use our

00:01:29,369 --> 00:01:34,229
products to be able to use us for either

00:01:31,409 --> 00:01:36,150
visualization and geospatial or OLAP

00:01:34,229 --> 00:01:38,159
engine type queries and on the really

00:01:36,150 --> 00:01:40,259
cool stuff like AI and deep learning

00:01:38,159 --> 00:01:42,150
that is actually driving the cost down

00:01:40,259 --> 00:01:43,560
and it's getting more value out of the

00:01:42,150 --> 00:01:44,340
customers and that's just a win win for

00:01:43,560 --> 00:01:48,030
us all

00:01:44,340 --> 00:01:52,110
ok one I guess attempt to are a real

00:01:48,030 --> 00:01:54,570
quickly in terms of artificial

00:01:52,110 --> 00:01:56,520
intelligence AI can you tell me about

00:01:54,570 --> 00:01:59,100
the different types of guess AI engaged

00:01:56,520 --> 00:02:02,060
in it says Eleanor has been doing with

00:01:59,100 --> 00:02:06,149
with power thank you so we have been

00:02:02,060 --> 00:02:09,570
doing lots of different engagements

00:02:06,149 --> 00:02:11,940
radhe I from understanding what does txt

00:02:09,570 --> 00:02:14,610
mean and how business should react on

00:02:11,940 --> 00:02:16,500
whatever is being said in a text to kind

00:02:14,610 --> 00:02:18,690
of form or what I call moon mundane

00:02:16,500 --> 00:02:20,300
things like image recognition finding

00:02:18,690 --> 00:02:23,210
fraud from videos

00:02:20,300 --> 00:02:26,090
and since we standardized all our

00:02:23,210 --> 00:02:28,310
development power AI it has been very

00:02:26,090 --> 00:02:30,740
smooth going on ramping up new

00:02:28,310 --> 00:02:33,320
developers or whatever we need to do

00:02:30,740 --> 00:02:38,270
because we essentially take the equation

00:02:33,320 --> 00:02:40,190
that the the director ma this is my mind

00:02:38,270 --> 00:02:41,900
we are taking the infrastructure there

00:02:40,190 --> 00:02:44,510
your frameworks out of the question how

00:02:41,900 --> 00:02:47,060
you set up how do you set them up and do

00:02:44,510 --> 00:02:49,430
things like that and now we are actually

00:02:47,060 --> 00:02:50,960
facing a kind of a next bigger challenge

00:02:49,430 --> 00:02:54,680
for us because we have launched a

00:02:50,960 --> 00:02:58,340
repeatable product on AI that connects

00:02:54,680 --> 00:03:01,220
with recognize very kind of deep things

00:02:58,340 --> 00:03:02,870
from text and extract them like privacy

00:03:01,220 --> 00:03:05,510
information which can be highly complex

00:03:02,870 --> 00:03:07,610
and that's something that companies are

00:03:05,510 --> 00:03:10,160
not fully prepared to push into cloud

00:03:07,610 --> 00:03:12,380
yet which means that we have to be able

00:03:10,160 --> 00:03:15,350
to provide on-prem or very close to um

00:03:12,380 --> 00:03:17,870
prime AR inferencing and train and

00:03:15,350 --> 00:03:19,820
that's where a povery eye comes to you

00:03:17,870 --> 00:03:22,010
into play because it's so easy to kind

00:03:19,820 --> 00:03:26,390
of set up a repeatable environment where

00:03:22,010 --> 00:03:28,400
you just start running your code okay

00:03:26,390 --> 00:03:29,690
thanks

00:03:28,400 --> 00:03:31,580
and we'll kind of want to keep on the

00:03:29,690 --> 00:03:33,860
same AI track here I guess we'll

00:03:31,580 --> 00:03:36,050
probably talk a lot about that and kind

00:03:33,860 --> 00:03:41,300
of jump to I guess for node over at h2o

00:03:36,050 --> 00:03:42,650
and and and kind of discuss I guess why

00:03:41,300 --> 00:03:46,910
the power architecture especially the

00:03:42,650 --> 00:03:50,180
latest gen p8 p9 servers are been sued

00:03:46,910 --> 00:03:51,320
afraid workloads for you all yep so when

00:03:50,180 --> 00:03:52,790
people think RPI

00:03:51,320 --> 00:03:54,140
typically most people think of the

00:03:52,790 --> 00:03:56,180
algorithms and especially the deep

00:03:54,140 --> 00:03:58,750
neural networks and associated algos

00:03:56,180 --> 00:04:01,130
which have been accelerated what for us

00:03:58,750 --> 00:04:02,870
there's a whole bunch of workloads a lot

00:04:01,130 --> 00:04:04,130
more than on top of the algorithms so

00:04:02,870 --> 00:04:06,100
there is the data wrangling read

00:04:04,130 --> 00:04:08,660
emerging the fee changing portion of it

00:04:06,100 --> 00:04:10,220
alongside statistical learning

00:04:08,660 --> 00:04:11,330
algorithms machine a traditional machine

00:04:10,220 --> 00:04:15,680
learning algorithms plus deep learning

00:04:11,330 --> 00:04:18,260
and what each of these all goes can be

00:04:15,680 --> 00:04:21,080
accelerated much more on GPUs as we are

00:04:18,260 --> 00:04:22,250
very well aware of but the data

00:04:21,080 --> 00:04:22,880
wrangling pushin which can happen the

00:04:22,250 --> 00:04:24,590
CPUs

00:04:22,880 --> 00:04:26,810
the data transfer back and forth but in

00:04:24,590 --> 00:04:29,210
CPA GPUs itself can take a lot of time

00:04:26,810 --> 00:04:32,110
and with the p8 and p9 and we've seen

00:04:29,210 --> 00:04:33,249
tremendous acceleration because you're

00:04:32,110 --> 00:04:34,899
then

00:04:33,249 --> 00:04:38,499
like connection between the system and

00:04:34,899 --> 00:04:41,759
the GP memory so what means that

00:04:38,499 --> 00:04:44,499
end-to-end workloads from data ingest to

00:04:41,759 --> 00:04:46,479
feature engineering wrangling machine

00:04:44,499 --> 00:04:47,739
learning and scoring can all be

00:04:46,479 --> 00:04:50,589
accelerated a much higher rate and

00:04:47,739 --> 00:04:52,239
that's what we're finding okay thanks

00:04:50,589 --> 00:04:53,979
and guess you passed it - she's real

00:04:52,239 --> 00:04:57,779
quick I want to jump into a little bit

00:04:53,979 --> 00:05:00,969
on a map D you know you claim to have

00:04:57,779 --> 00:05:03,459
extremely fast SQL engine harnessing

00:05:00,969 --> 00:05:04,869
power of GPUs and how does that really

00:05:03,459 --> 00:05:06,939
translate into better AI machine

00:05:04,869 --> 00:05:10,929
learning output for data science great

00:05:06,939 --> 00:05:12,669
so thank you for inviting me here so my

00:05:10,929 --> 00:05:15,159
name is Ashish bamboo I lead partner

00:05:12,669 --> 00:05:18,610
business at map D quick background about

00:05:15,159 --> 00:05:22,299
map D we are an accelerated GPU

00:05:18,610 --> 00:05:24,909
accelerated analytics platform and we

00:05:22,299 --> 00:05:27,459
help customers in just extremely large

00:05:24,909 --> 00:05:32,889
datasets we're talking about billions of

00:05:27,459 --> 00:05:34,809
records and then be able to analyze and

00:05:32,889 --> 00:05:38,110
visualize those extremely large datasets

00:05:34,809 --> 00:05:41,860
in milliseconds we also provide data

00:05:38,110 --> 00:05:44,529
science community features that actually

00:05:41,860 --> 00:05:50,319
enable them to have better outputs on

00:05:44,529 --> 00:05:52,299
their ml and AI processes and we do that

00:05:50,319 --> 00:05:56,289
as part of our involvement in the

00:05:52,299 --> 00:05:59,259
feature engineering part of ml if you

00:05:56,289 --> 00:06:01,649
talk to data scientists about how much

00:05:59,259 --> 00:06:05,139
time they spend on data preparation

00:06:01,649 --> 00:06:07,089
versus actually running algorithms while

00:06:05,139 --> 00:06:09,369
running algorithms always a little bit

00:06:07,089 --> 00:06:10,959
more and more exciting and sexy but it's

00:06:09,369 --> 00:06:13,029
the data preparation the feature

00:06:10,959 --> 00:06:15,639
engineering part that really takes the

00:06:13,029 --> 00:06:19,839
majority of their time so what map D

00:06:15,639 --> 00:06:23,679
does is we help data scientists visually

00:06:19,839 --> 00:06:29,919
explore huge data sets that they can

00:06:23,679 --> 00:06:33,279
then use to train their models and once

00:06:29,919 --> 00:06:34,929
you get the the predictions then it

00:06:33,279 --> 00:06:37,719
becomes notoriously difficult sometimes

00:06:34,929 --> 00:06:41,439
to really explain why the predictions

00:06:37,719 --> 00:06:46,300
are the way they are and in that area

00:06:41,439 --> 00:06:47,110
map D can visually help explain to the

00:06:46,300 --> 00:06:51,190
data scientist

00:06:47,110 --> 00:06:53,650
and their customers how the predictions

00:06:51,190 --> 00:06:57,580
are actually performing that generates

00:06:53,650 --> 00:07:00,220
more trust and adoption of AI and

00:06:57,580 --> 00:07:02,080
finally when you have the predictions

00:07:00,220 --> 00:07:05,380
initially and then the final outcomes

00:07:02,080 --> 00:07:08,140
the outcomes vary from the predictions

00:07:05,380 --> 00:07:11,290
and over a period of time that variants

00:07:08,140 --> 00:07:15,160
actually increases and our platform

00:07:11,290 --> 00:07:18,640
helps data scientists to overlay

00:07:15,160 --> 00:07:20,860
predictions and actual outcomes and be

00:07:18,640 --> 00:07:24,370
able to then take that in a feedback

00:07:20,860 --> 00:07:27,660
loop and retrain the data and get better

00:07:24,370 --> 00:07:30,730
predictions over a period of time okay

00:07:27,660 --> 00:07:32,350
jump to richard real quickly oh and

00:07:30,730 --> 00:07:33,820
before I ask I mean I congratulations I

00:07:32,350 --> 00:07:36,550
guess you guys announced the partnership

00:07:33,820 --> 00:07:39,100
between a bright light and Maria dB yeah

00:07:36,550 --> 00:07:41,350
yeah many DB's yeah very very excitable

00:07:39,100 --> 00:07:42,580
yeah I mean you know you know coming

00:07:41,350 --> 00:07:44,080
from a bunch in the software world I

00:07:42,580 --> 00:07:46,900
know how popular Maria DB is an

00:07:44,080 --> 00:07:48,250
exploding recently right sure and I

00:07:46,900 --> 00:07:52,660
guess you know what I would want to know

00:07:48,250 --> 00:07:54,520
is how I guess that partnership between

00:07:52,660 --> 00:07:57,670
you all fits into like the whole ai ai

00:07:54,520 --> 00:08:00,520
space sure so just to start with bright

00:07:57,670 --> 00:08:03,310
lights is a technology that uses GPUs to

00:08:00,520 --> 00:08:05,290
massively accelerate SQL relational

00:08:03,310 --> 00:08:06,790
workloads

00:08:05,290 --> 00:08:08,590
we've already integrated with post

00:08:06,790 --> 00:08:10,480
grades but it's the partnership with

00:08:08,590 --> 00:08:12,160
Maria DB that we've really really

00:08:10,480 --> 00:08:15,010
excited about because it means that we

00:08:12,160 --> 00:08:16,990
can now use that as a vehicle or

00:08:15,010 --> 00:08:18,790
mechanism to get our technology into the

00:08:16,990 --> 00:08:21,700
hands of literally millions of users

00:08:18,790 --> 00:08:25,210
right so really excited about that but

00:08:21,700 --> 00:08:27,580
you know sequel and relational workloads

00:08:25,210 --> 00:08:29,920
are not that well-suited for the machine

00:08:27,580 --> 00:08:32,610
learning and I space they they used a

00:08:29,920 --> 00:08:34,720
lot predominantly for data prep and

00:08:32,610 --> 00:08:36,010
comment earlier I was you know a lot of

00:08:34,720 --> 00:08:37,450
the data scientists time is actually

00:08:36,010 --> 00:08:40,810
spent in daily prep I think it's

00:08:37,450 --> 00:08:43,120
something like 66% and so what we what

00:08:40,810 --> 00:08:45,550
we've done is we've looked at some of

00:08:43,120 --> 00:08:49,210
the frameworks aren't there like Google

00:08:45,550 --> 00:08:51,820
tensorflow and torch and integrated

00:08:49,210 --> 00:08:54,520
really tightly with torch in fact we've

00:08:51,820 --> 00:08:57,880
taken the memory management module from

00:08:54,520 --> 00:09:00,400
torch extended it enhanced it and that's

00:08:57,880 --> 00:09:01,930
what we used for memory management

00:09:00,400 --> 00:09:04,840
the bright light from the sequel side

00:09:01,930 --> 00:09:06,610
and that means that as far as the

00:09:04,840 --> 00:09:08,440
computer is concerned the stuff that you

00:09:06,610 --> 00:09:09,940
using to create tables and columns is

00:09:08,440 --> 00:09:13,150
the same stuff that you can use for

00:09:09,940 --> 00:09:15,280
creating tensors and the really powerful

00:09:13,150 --> 00:09:18,340
outcome of that is you can now have zero

00:09:15,280 --> 00:09:21,130
copy you can use an SQL we are glad to

00:09:18,340 --> 00:09:23,080
prepare your data straight out of that

00:09:21,130 --> 00:09:26,320
consuming literally the same piece of

00:09:23,080 --> 00:09:28,570
memory into a torch framework for your

00:09:26,320 --> 00:09:30,370
AI and machine learning work later okay

00:09:28,570 --> 00:09:32,560
thanks

00:09:30,370 --> 00:09:34,120
probably jump to woody really quickly

00:09:32,560 --> 00:09:35,920
here and earlier they were talking about

00:09:34,120 --> 00:09:39,250
you know switching over to power from

00:09:35,920 --> 00:09:40,510
Intel based architecture and what has

00:09:39,250 --> 00:09:42,100
your experience in been in terms of

00:09:40,510 --> 00:09:44,590
operationalizing your product on one

00:09:42,100 --> 00:09:47,740
power yeah that's a really good question

00:09:44,590 --> 00:09:49,630
as the speaker earlier from Morgan said

00:09:47,740 --> 00:09:51,250
from work in state one of the big

00:09:49,630 --> 00:09:52,930
differences is just compiling in

00:09:51,250 --> 00:09:55,480
packaging that's the biggest difference

00:09:52,930 --> 00:09:57,730
we have is from how to run on power we

00:09:55,480 --> 00:09:58,990
just go and we compile we did have some

00:09:57,730 --> 00:10:01,060
specific things that we change before

00:09:58,990 --> 00:10:03,070
specifically around the memory

00:10:01,060 --> 00:10:04,750
management but we're good to go the

00:10:03,070 --> 00:10:06,520
other cool part is is all of our scripts

00:10:04,750 --> 00:10:08,470
because we run on Linux all work the

00:10:06,520 --> 00:10:10,380
same as well and so what that does is

00:10:08,470 --> 00:10:12,820
that allows it i'm significantly

00:10:10,380 --> 00:10:16,150
significantly easier to run our product

00:10:12,820 --> 00:10:20,190
on power or on any other framework

00:10:16,150 --> 00:10:20,190
because it's essentially the same okay

00:10:20,730 --> 00:10:27,000
real quick again sorry i want to ask

00:10:22,690 --> 00:10:29,530
question i noticed i guess in a recent

00:10:27,000 --> 00:10:31,180
third-party benchmark last year that you

00:10:29,530 --> 00:10:33,550
guys are through the fastest GPU

00:10:31,180 --> 00:10:37,540
database on the market by quite some

00:10:33,550 --> 00:10:39,310
margin yeah quite so much from your

00:10:37,540 --> 00:10:40,930
point of view though what are the

00:10:39,310 --> 00:10:43,480
challenges that you are I guess are seen

00:10:40,930 --> 00:10:46,120
in the accelerated GPU database market

00:10:43,480 --> 00:10:48,190
today sure so we all know that GPUs are

00:10:46,120 --> 00:10:50,590
really good at accelerating data and

00:10:48,190 --> 00:10:52,360
it's a fantastic new technology for

00:10:50,590 --> 00:10:54,220
accelerating sequel workloads machine

00:10:52,360 --> 00:10:56,580
learning workloads all those kinds of

00:10:54,220 --> 00:10:59,350
things but one of the big problems

00:10:56,580 --> 00:11:00,790
particularly for sequel they're two

00:10:59,350 --> 00:11:03,430
really big problems actually one of them

00:11:00,790 --> 00:11:06,040
is joins because using a lot of devices

00:11:03,430 --> 00:11:07,360
you need to have a distributor join and

00:11:06,040 --> 00:11:09,680
when you start running an actual join

00:11:07,360 --> 00:11:12,710
itself you need a huge

00:11:09,680 --> 00:11:17,240
data transfer needs to take place across

00:11:12,710 --> 00:11:19,760
all those GP devices and so with IBM and

00:11:17,240 --> 00:11:21,830
with power and the collaboration of

00:11:19,760 --> 00:11:24,440
Nvidia for envy link you've got this

00:11:21,830 --> 00:11:28,850
fantastic ability to shift data around a

00:11:24,440 --> 00:11:31,130
system from GPU to GPU GPU to CPU and

00:11:28,850 --> 00:11:32,480
then across across the entire cluster so

00:11:31,130 --> 00:11:34,430
that's the one thing that is really

00:11:32,480 --> 00:11:37,339
important one of the big challenges the

00:11:34,430 --> 00:11:39,890
second thing is again you know the GPUs

00:11:37,339 --> 00:11:42,050
are great but you need a huge amount of

00:11:39,890 --> 00:11:44,540
them to take on board some of the really

00:11:42,050 --> 00:11:45,680
bigger workloads and so the next

00:11:44,540 --> 00:11:48,320
challenge is actually a heterogeneous

00:11:45,680 --> 00:11:52,100
solution that brings more of the CPU

00:11:48,320 --> 00:11:55,400
resources into play and again it's envy

00:11:52,100 --> 00:11:57,320
link that really helps address that

00:11:55,400 --> 00:11:59,060
problem in a way that just the

00:11:57,320 --> 00:12:02,360
alternatives just are just not able to

00:11:59,060 --> 00:12:04,670
do thanks

00:12:02,360 --> 00:12:09,500
can I switch a little bit topics here

00:12:04,670 --> 00:12:12,380
slightly with a sheet and map D and and

00:12:09,500 --> 00:12:14,180
I guess you are a founding member of the

00:12:12,380 --> 00:12:16,820
go AI initiative could you kind of

00:12:14,180 --> 00:12:18,440
briefly describe what that is and and I

00:12:16,820 --> 00:12:20,510
guess what led you all to develop the

00:12:18,440 --> 00:12:24,709
new standard sure thanks

00:12:20,510 --> 00:12:28,100
so the go AI initiative is GP open

00:12:24,709 --> 00:12:32,150
analytics initiative we are a founding

00:12:28,100 --> 00:12:35,330
member along with h2o and anaconda in

00:12:32,150 --> 00:12:39,200
the the premise is really that on one

00:12:35,330 --> 00:12:44,029
hand the GPU architecture and processing

00:12:39,200 --> 00:12:45,440
power is increasing but there is kind of

00:12:44,029 --> 00:12:48,250
a disconnect between systems and

00:12:45,440 --> 00:12:51,620
platforms because if you have to

00:12:48,250 --> 00:12:53,779
leverage the the CPU in between in the

00:12:51,620 --> 00:12:56,839
processes then you basically have that

00:12:53,779 --> 00:12:58,490
that overhead off you you're doing

00:12:56,839 --> 00:13:01,040
something really fast on the GPUs but

00:12:58,490 --> 00:13:04,700
the air passing on on to CPU so that's

00:13:01,040 --> 00:13:10,279
really the the genesis of the go AI

00:13:04,700 --> 00:13:12,740
initiative and the idea is that three

00:13:10,279 --> 00:13:17,120
companies came together map day h2 and

00:13:12,740 --> 00:13:20,750
anaconda to create some open frameworks

00:13:17,120 --> 00:13:22,760
that we could enable data scientists and

00:13:20,750 --> 00:13:23,430
analysts to follow so that they can get

00:13:22,760 --> 00:13:25,740
the

00:13:23,430 --> 00:13:28,580
throughput the first project that

00:13:25,740 --> 00:13:32,310
actually has been released by this

00:13:28,580 --> 00:13:36,060
consortium is the G d/f the GPU data

00:13:32,310 --> 00:13:40,500
frame and the idea here is to have an

00:13:36,060 --> 00:13:43,649
absolute zero copy interchange we

00:13:40,500 --> 00:13:46,470
identify all the processes that can stay

00:13:43,649 --> 00:13:48,720
on the GPU as much as possible and we

00:13:46,470 --> 00:13:51,720
provide an end-to-end framework that

00:13:48,720 --> 00:13:54,630
allows the the data to actually just

00:13:51,720 --> 00:13:58,440
stay within the GPU realm leverage the

00:13:54,630 --> 00:14:01,440
increase in the speed of the memory and

00:13:58,440 --> 00:14:03,930
that really provides both the hardware

00:14:01,440 --> 00:14:07,290
and the software combination of high

00:14:03,930 --> 00:14:14,640
throughput both for machine learning and

00:14:07,290 --> 00:14:16,620
also for accelerated analytics in your

00:14:14,640 --> 00:14:19,170
opinion how do you think we can

00:14:16,620 --> 00:14:23,300
accelerate the I guess the enterprise AI

00:14:19,170 --> 00:14:25,470
adoption sure so be a dish to have been

00:14:23,300 --> 00:14:27,180
independent working on open source

00:14:25,470 --> 00:14:30,510
machine learning for over six years now

00:14:27,180 --> 00:14:32,190
or $10,000 musicians users globally but

00:14:30,510 --> 00:14:34,260
what we found time and again is there is

00:14:32,190 --> 00:14:36,330
sort of three main roadblocks the first

00:14:34,260 --> 00:14:37,470
is speed to insights when I mean speed

00:14:36,330 --> 00:14:40,440
it's not just about the hardware

00:14:37,470 --> 00:14:42,510
performance but it's also the expertise

00:14:40,440 --> 00:14:43,950
required to tune algorithms to do the

00:14:42,510 --> 00:14:47,220
work load do the feature engineering

00:14:43,950 --> 00:14:49,230
that she's mentioned the second is of

00:14:47,220 --> 00:14:50,490
course understanding explaining the

00:14:49,230 --> 00:14:52,980
results of machine learning algorithms

00:14:50,490 --> 00:14:54,750
or AI so once you create some

00:14:52,980 --> 00:14:56,459
predictions the big challenges you don't

00:14:54,750 --> 00:14:58,709
understand why a prediction was made so

00:14:56,459 --> 00:15:02,850
there's that becomes a big challenge in

00:14:58,709 --> 00:15:04,320
adoption and the final piece is the day

00:15:02,850 --> 00:15:06,420
and time it takes for deployment so even

00:15:04,320 --> 00:15:07,620
if data scientist are able to or machine

00:15:06,420 --> 00:15:09,420
learning researchers able to build these

00:15:07,620 --> 00:15:09,959
great models they don't get deployed to

00:15:09,420 --> 00:15:11,670
production

00:15:09,959 --> 00:15:13,050
so there's a big lag between research

00:15:11,670 --> 00:15:16,890
and development and going to production

00:15:13,050 --> 00:15:18,570
so what we try to do is optimize for all

00:15:16,890 --> 00:15:21,149
those problems by creating a fully

00:15:18,570 --> 00:15:23,700
automated solution call travel Asiya

00:15:21,149 --> 00:15:26,220
which and there's a demo allowed to show

00:15:23,700 --> 00:15:29,430
you later in the day where we have taken

00:15:26,220 --> 00:15:32,070
the process of the entire questionnaire

00:15:29,430 --> 00:15:33,180
from future engineering data prep and

00:15:32,070 --> 00:15:34,920
machine learning doing all of that

00:15:33,180 --> 00:15:36,200
iterative B and this is where the

00:15:34,920 --> 00:15:38,300
hardware

00:15:36,200 --> 00:15:39,350
exploration from the power systems

00:15:38,300 --> 00:15:40,880
becomes a really effective because

00:15:39,350 --> 00:15:42,710
you're going back and forth from CP o to

00:15:40,880 --> 00:15:43,970
CP is doing the feature engineering on

00:15:42,710 --> 00:15:45,350
the cpus and then the machine learning

00:15:43,970 --> 00:15:47,060
on the GPUs

00:15:45,350 --> 00:15:48,860
we also accelerated traditional machine

00:15:47,060 --> 00:15:51,140
learning algorithms like XG boosts

00:15:48,860 --> 00:15:53,480
k-means GLM in addition to the deep

00:15:51,140 --> 00:15:55,460
learning algorithms and that's all

00:15:53,480 --> 00:15:57,410
available in open source and order h2o

00:15:55,460 --> 00:15:59,960
for cheap you package and that's part of

00:15:57,410 --> 00:16:02,240
the go ahead initiative as well and then

00:15:59,960 --> 00:16:03,320
finally providing interpretations for

00:16:02,240 --> 00:16:05,630
every single prediction you make

00:16:03,320 --> 00:16:08,560
agnostic of the algorithms I think the

00:16:05,630 --> 00:16:10,400
combination of all three with the latest

00:16:08,560 --> 00:16:11,570
accelerations from the hardware is

00:16:10,400 --> 00:16:15,580
what's required for it

00:16:11,570 --> 00:16:15,580
enterprise the arid option right okay

00:16:17,450 --> 00:16:25,880
and I guess I got one last question for

00:16:20,150 --> 00:16:31,640
re and it's more around empower AI and I

00:16:25,880 --> 00:16:34,040
guess what I would like to know is from

00:16:31,640 --> 00:16:35,750
I guess aspects what are the most

00:16:34,040 --> 00:16:38,870
important aspects I think for power AI

00:16:35,750 --> 00:16:42,320
from a developer and an MSP perspective

00:16:38,870 --> 00:16:43,970
well I think the most important thing

00:16:42,320 --> 00:16:46,520
about power AI in the kind of big

00:16:43,970 --> 00:16:49,400
picture is that it's horribly boring in

00:16:46,520 --> 00:16:51,770
a good way because you know you take the

00:16:49,400 --> 00:16:54,830
equation of of the frameworks you need

00:16:51,770 --> 00:16:57,380
to do to development and inferencing of

00:16:54,830 --> 00:16:59,900
the question you can get these things up

00:16:57,380 --> 00:17:03,290
so fast let's say you have the baseline

00:16:59,900 --> 00:17:05,600
Linux supported Linux installed it takes

00:17:03,290 --> 00:17:07,280
about a half an hour to install power AI

00:17:05,600 --> 00:17:09,199
and 20 minutes is spent on the

00:17:07,280 --> 00:17:12,290
downloading from the distributor sized

00:17:09,199 --> 00:17:14,990
site so so that's kind of a and for

00:17:12,290 --> 00:17:17,690
example for us because now we are we

00:17:14,990 --> 00:17:19,670
have a Hydra beatable product in our

00:17:17,690 --> 00:17:22,370
hands that we need to push out into the

00:17:19,670 --> 00:17:24,650
market through the partners being able

00:17:22,370 --> 00:17:27,620
to set up our AI as the platform for

00:17:24,650 --> 00:17:30,950
inferencing effort as little effort as

00:17:27,620 --> 00:17:33,800
possible is just crucial no excitement

00:17:30,950 --> 00:17:35,120
around compiling everything and seeing

00:17:33,800 --> 00:17:37,370
okay this time we grabbed some

00:17:35,120 --> 00:17:40,280
dependencies that don't work over

00:17:37,370 --> 00:17:41,750
differently it's only did we have tested

00:17:40,280 --> 00:17:43,190
everything and we can just zip to the

00:17:41,750 --> 00:17:45,890
partners okay

00:17:43,190 --> 00:17:47,990
you have the power box over there do

00:17:45,890 --> 00:17:49,820
this you're up and running without a

00:17:47,990 --> 00:17:53,000
solution in two or three hours

00:17:49,820 --> 00:17:55,490
and how Rai automates traditionally a

00:17:53,000 --> 00:17:58,240
huge part of kind of what you need to do

00:17:55,490 --> 00:17:58,240
to set up everything

00:17:59,620 --> 00:18:03,440
well I better wrap up now I know we have

00:18:02,060 --> 00:18:05,750
another keynote coming up after this but

00:18:03,440 --> 00:18:06,770
um I'd like to thank you guys for taking

00:18:05,750 --> 00:18:09,370
the time to come up here and answer

00:18:06,770 --> 00:18:09,370

YouTube URL: https://www.youtube.com/watch?v=5y_wFFVH0oE


