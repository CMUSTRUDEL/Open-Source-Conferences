Title: OpenPOWER Summit US 2018: Empowering OpenPOWER
Publication date: 2018-04-05
Playlist: OpenPOWER Summit US 2018
Description: 
	Andrea Bartolini, Assistant Professor at the University of Bologna shares how OpenPOWER systems can be enhanced for energy-aware computing by presenting work done on the D.A.V.I.D.E. supercomputer.

For more information, please visit: http://www.openpowerfoundation.org
Captions: 
	00:00:02,990 --> 00:00:08,550
okay good good afternoon everyone Sandra

00:00:06,899 --> 00:00:11,669
bartolini from University of Bologna and

00:00:08,550 --> 00:00:13,620
I'm today here for presenting the hard

00:00:11,669 --> 00:00:17,580
work on empowering hope and power in

00:00:13,620 --> 00:00:19,260
terms of energy awareness so the outline

00:00:17,580 --> 00:00:21,840
of my presentation we'll start with

00:00:19,260 --> 00:00:24,720
presenting The Da Vida system which is a

00:00:21,840 --> 00:00:29,550
super computer based on open power

00:00:24,720 --> 00:00:31,260
system and Pascal GPU installed in inch

00:00:29,550 --> 00:00:35,100
neck and italy which has been designed

00:00:31,260 --> 00:00:36,930
together with the for engineering and we

00:00:35,100 --> 00:00:40,410
saw a presenter introduce the avila then

00:00:36,930 --> 00:00:43,560
I will I will give some word on why it's

00:00:40,410 --> 00:00:46,140
important to to have fine grain

00:00:43,560 --> 00:00:48,989
monitoring in future HPC systems and why

00:00:46,140 --> 00:00:50,790
hard the que use cases for that and then

00:00:48,989 --> 00:00:53,130
I will describe the key innovation so

00:00:50,790 --> 00:00:58,460
that we develop together with the

00:00:53,130 --> 00:01:01,320
abaddon so first a few word so we are

00:00:58,460 --> 00:01:03,270
the on the organization that is founded

00:01:01,320 --> 00:01:05,420
the design of the W supercomputers of

00:01:03,270 --> 00:01:10,079
race which is the organization which

00:01:05,420 --> 00:01:11,490
which which is a format by all the

00:01:10,079 --> 00:01:14,729
different super come to the member

00:01:11,490 --> 00:01:17,100
country of European Union and aggregate

00:01:14,729 --> 00:01:20,389
all the different computer all the

00:01:17,100 --> 00:01:23,369
different data centers in Europe and

00:01:20,389 --> 00:01:25,259
they as you can see here in the map we

00:01:23,369 --> 00:01:28,649
have the different countries which are

00:01:25,259 --> 00:01:31,740
involved in the price consortium and in

00:01:28,649 --> 00:01:34,200
a small in the centre you can barely see

00:01:31,740 --> 00:01:36,299
we have the three partner that the two

00:01:34,200 --> 00:01:38,700
partners we for engineering and

00:01:36,299 --> 00:01:40,859
university of bologna danda and Shaniqua

00:01:38,700 --> 00:01:42,479
so there we have a rural small area in

00:01:40,859 --> 00:01:44,579
italy and it's not in the northern

00:01:42,479 --> 00:01:46,590
center of Italy in which we have a lot

00:01:44,579 --> 00:01:48,389
of competences in supercomputing centers

00:01:46,590 --> 00:01:50,329
where data center priests data center

00:01:48,389 --> 00:01:52,319
theater at the center we have

00:01:50,329 --> 00:01:53,729
universities such as a switch does the

00:01:52,319 --> 00:01:55,499
research on this topic and we have

00:01:53,729 --> 00:01:59,759
before engineering which is a system

00:01:55,499 --> 00:02:02,159
integrator computing systems so The Da

00:01:59,759 --> 00:02:04,349
Vida system is a where Da Vida stands

00:02:02,159 --> 00:02:06,719
for developed of an added value

00:02:04,349 --> 00:02:08,160
infrastructure design in Europe it's a

00:02:06,719 --> 00:02:11,730
supercomputing Center is the three

00:02:08,160 --> 00:02:15,060
clusters 45 nodes

00:02:11,730 --> 00:02:17,640
hope our 8 plus MV Lincoln

00:02:15,060 --> 00:02:20,850
for GP per node I will go more in depth

00:02:17,640 --> 00:02:23,700
later on and this been ranked on July

00:02:20,850 --> 00:02:25,740
and November in the top 500 in 500 list

00:02:23,700 --> 00:02:26,970
so the target of this design and this

00:02:25,740 --> 00:02:29,040
was there

00:02:26,970 --> 00:02:31,410
this was the omission that we got in

00:02:29,040 --> 00:02:33,239
designing the system was to improve the

00:02:31,410 --> 00:02:35,370
way in which energy efficiency is

00:02:33,239 --> 00:02:37,920
perceived and now can be handled and

00:02:35,370 --> 00:02:40,530
control in future supercomputers up

00:02:37,920 --> 00:02:45,360
today is the only open power system in

00:02:40,530 --> 00:02:50,430
the top 500 list so the computing node

00:02:45,360 --> 00:02:53,430
of the a Vida is a is a in Ossipee feh

00:02:50,430 --> 00:02:56,160
is a ho CP form factor it is based on

00:02:53,430 --> 00:02:59,880
the IBM misc in computer design it has a

00:02:56,160 --> 00:03:03,560
true IBM power8 plus NV link it was a

00:02:59,880 --> 00:03:08,400
for NVIDIA PS Tesla P hundred card and

00:03:03,560 --> 00:03:11,160
it takes power from a bus bar and it has

00:03:08,400 --> 00:03:14,250
liquid cooling on top liquid cooling has

00:03:11,160 --> 00:03:17,430
been designed by acetic which has been a

00:03:14,250 --> 00:03:21,030
partner of the design and has been

00:03:17,430 --> 00:03:23,579
actually the the the cooling support for

00:03:21,030 --> 00:03:27,030
the open for the power it has been a

00:03:23,579 --> 00:03:28,950
design using the project and then in

00:03:27,030 --> 00:03:31,140
addition to what you have in the system

00:03:28,950 --> 00:03:33,390
we have a University of Bologna we have

00:03:31,140 --> 00:03:35,190
designed a fine-grain power and

00:03:33,390 --> 00:03:37,650
performance support monitoring support

00:03:35,190 --> 00:03:39,570
in integrated with in the computing

00:03:37,650 --> 00:03:42,360
nodes so we have had that and these were

00:03:39,570 --> 00:03:44,730
they are all points we had an additional

00:03:42,360 --> 00:03:46,859
component which is in charge of of

00:03:44,730 --> 00:03:50,760
handling the out-of-band monitoring of

00:03:46,859 --> 00:03:54,150
the system so the targeted case and the

00:03:50,760 --> 00:03:56,489
scenario that we have for which we have

00:03:54,150 --> 00:03:58,320
designed this component in the system is

00:03:56,489 --> 00:04:02,340
the following so the targeted case that

00:03:58,320 --> 00:04:04,950
David tried to to solve is the following

00:04:02,340 --> 00:04:07,019
so on the left side we have a sketch in

00:04:04,950 --> 00:04:09,269
which we have the different compute

00:04:07,019 --> 00:04:11,700
nodes of the system we have a compute

00:04:09,269 --> 00:04:13,440
node of the system we have all the

00:04:11,700 --> 00:04:15,720
several components we have CPU

00:04:13,440 --> 00:04:17,910
accelerators card we have hardware

00:04:15,720 --> 00:04:21,150
component needs answers the DRAM and we

00:04:17,910 --> 00:04:23,190
have also some mechanical components and

00:04:21,150 --> 00:04:25,800
such they are rotating fans which I used

00:04:23,190 --> 00:04:27,240
to cool down the system then we have in

00:04:25,800 --> 00:04:29,550
the screen there in

00:04:27,240 --> 00:04:31,110
in the design we have two users so the

00:04:29,550 --> 00:04:33,330
red and the blue which are normal user

00:04:31,110 --> 00:04:36,660
and allow user that access the system

00:04:33,330 --> 00:04:39,960
and then we have said the malsu's user

00:04:36,660 --> 00:04:42,000
the the one on the top which tries to

00:04:39,960 --> 00:04:43,740
given - I have an attack on the system

00:04:42,000 --> 00:04:45,720
and at the same time you have that some

00:04:43,740 --> 00:04:48,020
parts in your system may have some

00:04:45,720 --> 00:04:51,660
bigger dictionary ability folds and

00:04:48,020 --> 00:04:54,479
these are also which is also live by

00:04:51,660 --> 00:04:57,509
them by their heart so in today's system

00:04:54,479 --> 00:05:00,000
that uses a coarse graining power power

00:04:57,509 --> 00:05:01,680
monitoring of the of the consumption

00:05:00,000 --> 00:05:03,449
what you what you have is that you

00:05:01,680 --> 00:05:04,830
cannot really see a lot from the

00:05:03,449 --> 00:05:07,500
powertrains in sense of what is

00:05:04,830 --> 00:05:10,710
happening you you usually can see the

00:05:07,500 --> 00:05:13,380
power at at the speed of seconds and and

00:05:10,710 --> 00:05:15,630
this doesn't give a low enough speed for

00:05:13,380 --> 00:05:18,360
understanding what was happening so they

00:05:15,630 --> 00:05:20,880
would find her in power measurement what

00:05:18,360 --> 00:05:22,979
we target is the fact of actually be

00:05:20,880 --> 00:05:24,389
having a resolution on the way in which

00:05:22,979 --> 00:05:27,180
we monitor the power consumption of each

00:05:24,389 --> 00:05:30,120
of the node which is a fine enough to

00:05:27,180 --> 00:05:32,130
see the different users of the machine

00:05:30,120 --> 00:05:35,550
so Twilight the part of the world hood

00:05:32,130 --> 00:05:38,190
that had opposed the tire on the on the

00:05:35,550 --> 00:05:39,509
red user the one for the blue and also

00:05:38,190 --> 00:05:42,630
there were the parts which are spikes

00:05:39,509 --> 00:05:45,599
which are which are related to the to

00:05:42,630 --> 00:05:47,130
the malicious user or to a fault which

00:05:45,599 --> 00:05:48,479
is happening on the machine so power

00:05:47,130 --> 00:05:51,659
finger and power and performance

00:05:48,479 --> 00:05:54,349
measurement can really help in verifying

00:05:51,659 --> 00:05:56,310
classify node power performance

00:05:54,349 --> 00:05:59,219
understand if something's inspector

00:05:56,310 --> 00:06:01,199
photo speck and also enable predictive

00:05:59,219 --> 00:06:03,810
maintenance on the node and be capable

00:06:01,199 --> 00:06:06,180
of accounting for user consumption it

00:06:03,810 --> 00:06:09,240
part of the plot what we have is instead

00:06:06,180 --> 00:06:11,610
this other scenario in which we have a

00:06:09,240 --> 00:06:14,639
system power capping so basically for

00:06:11,610 --> 00:06:16,830
several reason which are related to the

00:06:14,639 --> 00:06:18,509
fact that you may have a new

00:06:16,830 --> 00:06:20,729
installation on your systems you need to

00:06:18,509 --> 00:06:22,259
to compress the part to reduce the power

00:06:20,729 --> 00:06:25,199
consumption of the system that you are

00:06:22,259 --> 00:06:27,090
running or you may have that you have

00:06:25,199 --> 00:06:28,710
some service level agreement with the

00:06:27,090 --> 00:06:30,630
power grid so you have some requirements

00:06:28,710 --> 00:06:32,009
that can allow you to be feasible in

00:06:30,630 --> 00:06:34,259
terms of the power consumption or you

00:06:32,009 --> 00:06:35,940
may have that you have some power

00:06:34,259 --> 00:06:37,620
shortage we have some natural disasters

00:06:35,940 --> 00:06:38,980
that decrease the amount of power that

00:06:37,620 --> 00:06:41,290
can be fitted in

00:06:38,980 --> 00:06:43,060
computer in your machine so what you

00:06:41,290 --> 00:06:45,700
want to do is basically be capable of

00:06:43,060 --> 00:06:47,830
controlling the power the maximum power

00:06:45,700 --> 00:06:50,140
profile of your machine and keeping them

00:06:47,830 --> 00:06:51,970
in a shape which may vary and assist in

00:06:50,140 --> 00:06:53,920
during the time and you want to do with

00:06:51,970 --> 00:06:55,810
them in a way that doesn't compromise

00:06:53,920 --> 00:06:58,600
much the performance and the way in

00:06:55,810 --> 00:07:00,160
which users use your machines in the way

00:06:58,600 --> 00:07:04,680
in which your account uses of the

00:07:00,160 --> 00:07:07,600
machine so the main app so you do that

00:07:04,680 --> 00:07:10,690
we we developed together with dielectric

00:07:07,600 --> 00:07:12,940
innovation technologies so here I sketch

00:07:10,690 --> 00:07:15,520
basically the computing nodes of the

00:07:12,940 --> 00:07:17,560
system and some additional component

00:07:15,520 --> 00:07:20,200
which I'm going to dig in and also I'm

00:07:17,560 --> 00:07:21,880
going to end the front end of the system

00:07:20,200 --> 00:07:23,890
which is running excluding some

00:07:21,880 --> 00:07:26,410
additional software components that we

00:07:23,890 --> 00:07:28,030
have designed together adopted so the

00:07:26,410 --> 00:07:30,130
first component is what we call the

00:07:28,030 --> 00:07:31,930
black box system so we have added the

00:07:30,130 --> 00:07:33,730
board that was showing before it's

00:07:31,930 --> 00:07:35,650
basically a black box which is comes

00:07:33,730 --> 00:07:38,020
together with your node is based on open

00:07:35,650 --> 00:07:40,810
arc in open open open hydrant and

00:07:38,020 --> 00:07:44,920
software installation and it's basically

00:07:40,810 --> 00:07:46,420
capable of being a always seeing the

00:07:44,920 --> 00:07:49,060
power of performance of your system

00:07:46,420 --> 00:07:51,640
without completing being out of Bend so

00:07:49,060 --> 00:07:53,860
we leverage the what the open power

00:07:51,640 --> 00:07:55,270
system give us in terms of the

00:07:53,860 --> 00:07:59,260
capability of being capable of

00:07:55,270 --> 00:08:00,850
monitoring the performance and a and all

00:07:59,260 --> 00:08:02,650
of the components which are inside and

00:08:00,850 --> 00:08:05,260
some detailed power consumption of

00:08:02,650 --> 00:08:06,730
internal components from out of bands so

00:08:05,260 --> 00:08:08,290
without running any additional software

00:08:06,730 --> 00:08:09,910
in the system so being completely

00:08:08,290 --> 00:08:12,850
transparent to the user and without

00:08:09,910 --> 00:08:16,300
stealing any computing cycle to the

00:08:12,850 --> 00:08:19,770
system then these are we edition we

00:08:16,300 --> 00:08:21,940
added to this inaccurate and fine-grain

00:08:19,770 --> 00:08:24,040
capability on the power measurements so

00:08:21,940 --> 00:08:26,440
we can actually measure the power of

00:08:24,040 --> 00:08:28,270
each of the node will with the fine

00:08:26,440 --> 00:08:31,090
granular I written with you later what

00:08:28,270 --> 00:08:33,400
this can lead to and then we use is for

00:08:31,090 --> 00:08:35,110
for handling the data we use a an

00:08:33,400 --> 00:08:37,870
Internet of Things communication with

00:08:35,110 --> 00:08:40,900
MQTT is the protocol that that are

00:08:37,870 --> 00:08:43,720
allowed to push out the D values with

00:08:40,900 --> 00:08:46,120
the really low hover it in terms of the

00:08:43,720 --> 00:08:47,860
packet which transmit that are which are

00:08:46,120 --> 00:08:49,960
traveling and also these are also

00:08:47,860 --> 00:08:52,000
supports time synchronization protocol

00:08:49,960 --> 00:08:52,850
so you can really have a timestamp you

00:08:52,000 --> 00:08:53,839
just associate

00:08:52,850 --> 00:08:55,490
with this of the point of your

00:08:53,839 --> 00:08:56,870
measurement and be capable of

00:08:55,490 --> 00:09:00,980
correlating with what is happening

00:08:56,870 --> 00:09:04,339
inside the note here just a a snapshot

00:09:00,980 --> 00:09:08,329
of what we can measure with diabetes so

00:09:04,339 --> 00:09:10,250
the first with with with this additional

00:09:08,329 --> 00:09:12,079
component so you can really see you

00:09:10,250 --> 00:09:14,839
start from an application for several

00:09:12,079 --> 00:09:17,449
minutes of your of a computation of a

00:09:14,839 --> 00:09:19,579
benchmark and then you if you do mean so

00:09:17,449 --> 00:09:21,440
the question is can you really see

00:09:19,579 --> 00:09:23,509
things that will find and write in terms

00:09:21,440 --> 00:09:27,259
of the power so if you look the first

00:09:23,509 --> 00:09:29,300
zooming is about a zoom off of a first

00:09:27,259 --> 00:09:30,889
second time we can see that there is a

00:09:29,300 --> 00:09:32,630
lot of activity which is going on the

00:09:30,889 --> 00:09:35,029
power so we are actually measuring at

00:09:32,630 --> 00:09:37,160
every millisecond year so we are pushing

00:09:35,029 --> 00:09:38,569
out at every millisecond for each of the

00:09:37,160 --> 00:09:40,459
know the power consumption of the node

00:09:38,569 --> 00:09:42,440
and we can see actually the trees a lot

00:09:40,459 --> 00:09:44,720
of activity which is ongoing and we can

00:09:42,440 --> 00:09:46,339
also see here on the on the on the

00:09:44,720 --> 00:09:48,350
bottom plot in which we have for all the

00:09:46,339 --> 00:09:50,600
node the power consumption of the in

00:09:48,350 --> 00:09:52,370
just one one second window and you can

00:09:50,600 --> 00:09:54,170
see all the all the time in which the

00:09:52,370 --> 00:09:56,449
GPUs are active you see the spikes in

00:09:54,170 --> 00:09:58,730
power so you see also a huge variation

00:09:56,449 --> 00:10:00,410
in the power so it makes sense to see

00:09:58,730 --> 00:10:02,660
power at this granularity and you can

00:10:00,410 --> 00:10:05,720
really appreciate changes so this was

00:10:02,660 --> 00:10:07,160
the first question but okay then second

00:10:05,720 --> 00:10:09,199
question can we really correlate this

00:10:07,160 --> 00:10:10,819
power measurement at one millisecond

00:10:09,199 --> 00:10:12,829
scale with what's happening in the

00:10:10,819 --> 00:10:15,949
application so we run on that we do some

00:10:12,829 --> 00:10:17,360
tests so here I'm showing you a in

00:10:15,949 --> 00:10:19,579
application with wrote an MPI

00:10:17,360 --> 00:10:22,939
application which was basically starting

00:10:19,579 --> 00:10:25,430
getting a timestamp and then starting in

00:10:22,939 --> 00:10:27,740
up in execution in all of the node of

00:10:25,430 --> 00:10:30,470
the system in eight of the node of the

00:10:27,740 --> 00:10:32,569
system we take this timestamp at after

00:10:30,470 --> 00:10:34,160
one day these are all the application

00:10:32,569 --> 00:10:37,130
was starting the running and we took the

00:10:34,160 --> 00:10:39,649
measurement from the additional board at

00:10:37,130 --> 00:10:42,199
we good we added to monitor the power

00:10:39,649 --> 00:10:43,399
and what we can see actually is that we

00:10:42,199 --> 00:10:45,259
really get the point

00:10:43,399 --> 00:10:46,819
so the timestamp is really getting at

00:10:45,259 --> 00:10:50,120
the point which we see the power changes

00:10:46,819 --> 00:10:51,500
and I let you see that the one the plot

00:10:50,120 --> 00:10:53,509
which I'm showing here is it is the

00:10:51,500 --> 00:10:56,060
power measured at AV at 20 meters

00:10:53,509 --> 00:10:58,819
microsecond granularity so we really

00:10:56,060 --> 00:11:01,189
have a synchronization because thanks to

00:10:58,819 --> 00:11:04,010
a good configuration of NTP and also the

00:11:01,189 --> 00:11:06,230
PTP protocol that we have in the system

00:11:04,010 --> 00:11:08,900
we can really have soup micros

00:11:06,230 --> 00:11:10,810
like on the header on the timing of of

00:11:08,900 --> 00:11:12,890
the time stamps and we can really

00:11:10,810 --> 00:11:14,630
synchronize what we see on the power

00:11:12,890 --> 00:11:17,600
trace with what we see an application of

00:11:14,630 --> 00:11:18,800
microsecond scale so that's a thing is

00:11:17,600 --> 00:11:21,560
really impressing in a sense that we can

00:11:18,800 --> 00:11:23,180
really correlate hooli final rarity

00:11:21,560 --> 00:11:24,920
traces on the power measurement and if

00:11:23,180 --> 00:11:26,660
you think of and this is happening all

00:11:24,920 --> 00:11:28,670
out of Bend so there is nothing which

00:11:26,660 --> 00:11:30,860
runs within the core if you think at

00:11:28,670 --> 00:11:32,210
other system we choose a way of

00:11:30,860 --> 00:11:33,860
exchanging communication to the

00:11:32,210 --> 00:11:35,630
operating system to the application or

00:11:33,860 --> 00:11:37,670
to see what's happening in terms of

00:11:35,630 --> 00:11:39,800
power you are bounded by the operating

00:11:37,670 --> 00:11:40,370
system refresh rate and you barely can

00:11:39,800 --> 00:11:43,010
go

00:11:40,370 --> 00:11:45,950
azar to go below the millisecond scale

00:11:43,010 --> 00:11:50,780
here we are at 20 microsecond so it's an

00:11:45,950 --> 00:11:53,030
really super created time then what do

00:11:50,780 --> 00:11:58,280
we do with these fine grain and morality

00:11:53,030 --> 00:12:00,170
here ISM hey turns out that in the the

00:11:58,280 --> 00:12:02,600
board that we are using for doing they

00:12:00,170 --> 00:12:04,280
for measuring the power is also capable

00:12:02,600 --> 00:12:06,770
of doing some additional computation so

00:12:04,280 --> 00:12:08,620
if you remember our primary goal that

00:12:06,770 --> 00:12:11,240
was the one of be capable of identifying

00:12:08,620 --> 00:12:13,280
some good application with respect to

00:12:11,240 --> 00:12:15,290
some attacker on the system so here are

00:12:13,280 --> 00:12:17,230
two application an application attack

00:12:15,290 --> 00:12:20,300
and is an FFT which is done online

00:12:17,230 --> 00:12:22,040
hundred power traces computed at 20

00:12:20,300 --> 00:12:24,560
microseconds so each of the nodes are

00:12:22,040 --> 00:12:26,900
capable of doing a spectrum analysis of

00:12:24,560 --> 00:12:28,970
the of the power consumption is kind of

00:12:26,900 --> 00:12:30,830
having an oscilloscope always on and

00:12:28,970 --> 00:12:33,020
connected to each of the node which can

00:12:30,830 --> 00:12:35,360
tell you what's going on and you can see

00:12:33,020 --> 00:12:37,640
that actually different frequency spike

00:12:35,360 --> 00:12:39,170
so different a spectrum a related to

00:12:37,640 --> 00:12:41,450
different behavior of the application so

00:12:39,170 --> 00:12:43,280
you can really be capable of of

00:12:41,450 --> 00:12:44,630
recognizing the one application with

00:12:43,280 --> 00:12:46,340
respect to an order so if you have a

00:12:44,630 --> 00:12:48,620
normal utilization of your system a

00:12:46,340 --> 00:12:50,570
profile it you can you can correlate it

00:12:48,620 --> 00:12:53,090
with with other application which are

00:12:50,570 --> 00:12:55,430
not in terms of spectrum is what you you

00:12:53,090 --> 00:12:56,990
thought it was good and then you can

00:12:55,430 --> 00:12:59,180
apply machine learning technique on top

00:12:56,990 --> 00:13:00,140
on top of that to recognize good usage

00:12:59,180 --> 00:13:03,770
and usage

00:13:00,140 --> 00:13:06,530
okay but can you do it this kind of

00:13:03,770 --> 00:13:08,810
analysis on a hundred large machine in

00:13:06,530 --> 00:13:10,730
real time so this is the second

00:13:08,810 --> 00:13:12,950
technology that we created in the avid

00:13:10,730 --> 00:13:17,360
saw which was the one of actually using

00:13:12,950 --> 00:13:18,620
a using using Big Data technologies an

00:13:17,360 --> 00:13:19,100
artificial intelligent technologies

00:13:18,620 --> 00:13:21,050
friend

00:13:19,100 --> 00:13:25,400
the data that we produce him monitors we

00:13:21,050 --> 00:13:26,870
produce more than 1,000 matrix 1000

00:13:25,400 --> 00:13:29,180
front row matrix pairing order per

00:13:26,870 --> 00:13:31,040
second on each node and then we have all

00:13:29,180 --> 00:13:33,440
to end of them in order to create

00:13:31,040 --> 00:13:34,820
correlation in between what we sense in

00:13:33,440 --> 00:13:36,770
terms of power and performance and what

00:13:34,820 --> 00:13:39,650
application and what the system is doing

00:13:36,770 --> 00:13:41,690
so we use the Hinna analog in nor do we

00:13:39,650 --> 00:13:44,450
have a is a tool chain which is actually

00:13:41,690 --> 00:13:46,370
capable of feeding receiving all the

00:13:44,450 --> 00:13:48,500
data which are coming creating a

00:13:46,370 --> 00:13:51,260
structured way the information then

00:13:48,500 --> 00:13:53,840
create some view which are helpful so we

00:13:51,260 --> 00:13:55,160
call this system it's called xamon which

00:13:53,840 --> 00:13:57,580
is actually the framework that we use

00:13:55,160 --> 00:14:01,490
for collecting the data so we'll say on

00:13:57,580 --> 00:14:04,190
the octave a tab I was saying before on

00:14:01,490 --> 00:14:05,750
the first layer we have the the daemon

00:14:04,190 --> 00:14:07,400
which is actually on there which runs on

00:14:05,750 --> 00:14:09,200
a by the system which produce publish

00:14:07,400 --> 00:14:12,740
the data as soon as it monitor with the

00:14:09,200 --> 00:14:15,860
timestamp then we have a an MDT broker

00:14:12,740 --> 00:14:17,480
then we have a in abstract which is

00:14:15,860 --> 00:14:19,700
Kairos DB that gives you an abstraction

00:14:17,480 --> 00:14:21,560
for creating time traces on the data to

00:14:19,700 --> 00:14:23,470
collect and then we have a Cassandra

00:14:21,560 --> 00:14:25,940
Basin which would store the data

00:14:23,470 --> 00:14:28,700
scalably and then you can also so if you

00:14:25,940 --> 00:14:30,140
add you fraud they monitor note if you

00:14:28,700 --> 00:14:32,570
increase them you can increase and know

00:14:30,140 --> 00:14:34,640
that use the amount of resources for the

00:14:32,570 --> 00:14:37,190
for handling the data and then with this

00:14:34,640 --> 00:14:38,840
one you can have then a structured view

00:14:37,190 --> 00:14:41,390
for your information from which you can

00:14:38,840 --> 00:14:44,390
actually correlate the date and give

00:14:41,390 --> 00:14:46,490
different view so how the things works

00:14:44,390 --> 00:14:48,500
in reality so we have that each of the

00:14:46,490 --> 00:14:51,590
embedded system is reading the data and

00:14:48,500 --> 00:14:53,090
is publishing them to centralize the

00:14:51,590 --> 00:14:55,450
entity which is the broker which

00:14:53,090 --> 00:14:58,400
received the data and then it's

00:14:55,450 --> 00:15:00,440
propagating them to a to a software and

00:14:58,400 --> 00:15:02,800
IP to an entity which is sync which

00:15:00,440 --> 00:15:05,270
actually receive this information and

00:15:02,800 --> 00:15:07,520
store them in multiple node in a

00:15:05,270 --> 00:15:09,980
Cassandra database then all this data

00:15:07,520 --> 00:15:12,260
are VAR a times three times the

00:15:09,980 --> 00:15:13,940
timestamp associated and then you can

00:15:12,260 --> 00:15:15,830
correlate with other times time that you

00:15:13,940 --> 00:15:18,680
have in your application on top of that

00:15:15,830 --> 00:15:21,140
you can also apply some machine learning

00:15:18,680 --> 00:15:22,880
technique so you can have SPARC

00:15:21,140 --> 00:15:24,920
PI tours and other things that you can

00:15:22,880 --> 00:15:26,720
use for a do we artificially to ever

00:15:24,920 --> 00:15:28,310
here I on top of the data that you are

00:15:26,720 --> 00:15:30,800
collecting for your seasons so now you

00:15:28,310 --> 00:15:31,750
want to apply AI technology to the

00:15:30,800 --> 00:15:33,910
management of

00:15:31,750 --> 00:15:37,360
in order to understand what's happening

00:15:33,910 --> 00:15:40,449
and we simplified the life of the system

00:15:37,360 --> 00:15:42,970
administrator and for the users and so

00:15:40,449 --> 00:15:44,889
we you can also with this with this

00:15:42,970 --> 00:15:46,060
approach you can also have some micro

00:15:44,889 --> 00:15:48,339
batching approaches in which you

00:15:46,060 --> 00:15:49,990
actually stream generate models and do

00:15:48,339 --> 00:15:53,829
some analytics on the data that you

00:15:49,990 --> 00:15:56,889
collect from all the nodes and and then

00:15:53,829 --> 00:15:58,089
you can say ensure we have spark and we

00:15:56,889 --> 00:16:00,879
have spark in which we can actually

00:15:58,089 --> 00:16:03,129
create the data create events and then

00:16:00,879 --> 00:16:04,959
are stored in the database again so but

00:16:03,129 --> 00:16:07,569
ok now we have everything in a database

00:16:04,959 --> 00:16:09,670
what we do so we create two different

00:16:07,569 --> 00:16:11,889
visualization for data that we monitor I

00:16:09,670 --> 00:16:17,160
will try out to give you a live demo a

00:16:11,889 --> 00:16:21,160
few seconds if it's if it works so just

00:16:17,160 --> 00:16:24,250
here ok so we have a first view which is

00:16:21,160 --> 00:16:28,120
graph fauna and but I need to of course

00:16:24,250 --> 00:16:30,850
move it here so these actually are all

00:16:28,120 --> 00:16:32,529
the automatics that we collect for all

00:16:30,850 --> 00:16:35,319
the nodes so we have actually here

00:16:32,529 --> 00:16:37,509
several this is the node of the Aveda

00:16:35,319 --> 00:16:39,579
now live which is actually doing some

00:16:37,509 --> 00:16:41,920
computation and you can see that out of

00:16:39,579 --> 00:16:44,410
been so we monitor a lot of matters

00:16:41,920 --> 00:16:47,410
which are interesting so is the node 45

00:16:44,410 --> 00:16:50,920
of the system we have the GPU power

00:16:47,410 --> 00:16:53,860
consumption live in time we have the de

00:16:50,920 --> 00:16:57,279
PMI the power consumption of the of the

00:16:53,860 --> 00:16:59,170
know of the part of the fan we have we

00:16:57,279 --> 00:17:01,269
have served me just also need to move

00:16:59,170 --> 00:17:03,610
the mouse here ok then we have a lot of

00:17:01,269 --> 00:17:05,289
other metrics so we can create if you

00:17:03,610 --> 00:17:07,449
want to know how your node is consuming

00:17:05,289 --> 00:17:10,120
in terms of power we dis out of a man

00:17:07,449 --> 00:17:12,400
without this out of band monitoring you

00:17:10,120 --> 00:17:14,169
can actually know that too now you have

00:17:12,400 --> 00:17:16,030
this balancing of the power within your

00:17:14,169 --> 00:17:18,339
node then you have a lot of you have

00:17:16,030 --> 00:17:20,199
also other interesting metrics such as

00:17:18,339 --> 00:17:21,880
you want to know how the till ization of

00:17:20,199 --> 00:17:24,760
the different cores were running the

00:17:21,880 --> 00:17:27,309
benchmark is done how the frequency of

00:17:24,760 --> 00:17:29,890
all of the course is executing in real

00:17:27,309 --> 00:17:31,600
time you want to know also maybe how is

00:17:29,890 --> 00:17:33,250
the throughput of your system or the

00:17:31,600 --> 00:17:36,100
memory bandwidth that you are using

00:17:33,250 --> 00:17:38,440
lively on each node and this happens

00:17:36,100 --> 00:17:40,240
even if you are you don't have to call a

00:17:38,440 --> 00:17:42,159
profiler for seeing the information so

00:17:40,240 --> 00:17:44,380
it's a always own information that you

00:17:42,159 --> 00:17:47,170
collect 24 hour day seven

00:17:44,380 --> 00:17:48,640
a week and we always get this data in

00:17:47,170 --> 00:17:51,640
order to of course this data would

00:17:48,640 --> 00:17:53,680
explode if you store them always so what

00:17:51,640 --> 00:17:55,870
we do is we actually keep a buffer of of

00:17:53,680 --> 00:17:57,970
two days or three days of this data and

00:17:55,870 --> 00:18:00,460
then naturally these are actually

00:17:57,970 --> 00:18:02,860
expiring it and in time we actually

00:18:00,460 --> 00:18:04,750
create an aggregated value of this data

00:18:02,860 --> 00:18:07,360
which are for job and these come the

00:18:04,750 --> 00:18:11,200
second tool that we have created in them

00:18:07,360 --> 00:18:14,140
that we have created and is a hex among

00:18:11,200 --> 00:18:15,730
web so actually they have a portal for

00:18:14,140 --> 00:18:18,670
their for the data so you basically

00:18:15,730 --> 00:18:21,010
justa you have we also have instrumented

00:18:18,670 --> 00:18:23,020
of course their what the job scheduler

00:18:21,010 --> 00:18:26,380
so you actually after your job is

00:18:23,020 --> 00:18:28,390
executed you just have your slurm job ID

00:18:26,380 --> 00:18:30,070
you place in the tool and then you can

00:18:28,390 --> 00:18:32,560
actually view all the data that we have

00:18:30,070 --> 00:18:34,480
collected so you can actually get the

00:18:32,560 --> 00:18:37,030
node utilization whether your

00:18:34,480 --> 00:18:38,350
application was running the flops

00:18:37,030 --> 00:18:40,720
the how the frequency was actually

00:18:38,350 --> 00:18:43,120
security is it good if you actually want

00:18:40,720 --> 00:18:44,800
to the some nasty things which happens

00:18:43,120 --> 00:18:47,260
with the power management of your system

00:18:44,800 --> 00:18:49,240
so they may crane some some performance

00:18:47,260 --> 00:18:51,160
changes in in your season then you can

00:18:49,240 --> 00:18:52,750
also have a know if you are good in

00:18:51,160 --> 00:18:54,640
terms of power management so if you are

00:18:52,750 --> 00:18:56,980
actually when you are haidle if you are

00:18:54,640 --> 00:18:59,200
really reducing the power consumption of

00:18:56,980 --> 00:19:01,030
the node then you have also a live

00:18:59,200 --> 00:19:02,950
profiling of what is the man what is

00:19:01,030 --> 00:19:05,770
your memory usage so and it's super

00:19:02,950 --> 00:19:08,020
useful as a user may actually at end of

00:19:05,770 --> 00:19:10,570
the job a user may have not selected to

00:19:08,020 --> 00:19:12,070
profile his job but he may had some dub

00:19:10,570 --> 00:19:14,710
that something was not running as

00:19:12,070 --> 00:19:16,800
expected so by using this tool you

00:19:14,710 --> 00:19:19,690
actually ever situate and always own

00:19:16,800 --> 00:19:21,220
profiler for your for your application

00:19:19,690 --> 00:19:24,280
which doesn't cost me anything because

00:19:21,220 --> 00:19:26,380
it goes everything is out of band so it

00:19:24,280 --> 00:19:28,120
doesn't really cost nothing so it is in

00:19:26,380 --> 00:19:30,460
the system and you can over have

00:19:28,120 --> 00:19:35,680
analysis which run out there on your app

00:19:30,460 --> 00:19:37,300
on your system and so coming up coming

00:19:35,680 --> 00:19:38,500
back to the presentation so this is what

00:19:37,300 --> 00:19:40,630
the view that I was showing you before

00:19:38,500 --> 00:19:42,400
so we have also view which you actually

00:19:40,630 --> 00:19:44,530
can see lively out of the different

00:19:42,400 --> 00:19:46,060
location of your system higher in terms

00:19:44,530 --> 00:19:48,760
of power temperatures and other

00:19:46,060 --> 00:19:50,410
information you can also see there as I

00:19:48,760 --> 00:19:52,990
was saying as much power as consume your

00:19:50,410 --> 00:19:55,720
job and things like that then we have

00:19:52,990 --> 00:19:57,100
the on top of this realization of the

00:19:55,720 --> 00:19:58,780
date okay then okay

00:19:57,100 --> 00:20:01,390
then this means that the user should be

00:19:58,780 --> 00:20:03,160
in front of the data tray but can we

00:20:01,390 --> 00:20:07,169
apply some machine learning technique to

00:20:03,160 --> 00:20:09,610
this data in order to ever sum that

00:20:07,169 --> 00:20:11,650
somehow automated way of handling the

00:20:09,610 --> 00:20:13,660
data so this is done actually so then

00:20:11,650 --> 00:20:16,270
the third technology is actually the one

00:20:13,660 --> 00:20:19,600
that we wanted is to 2m to add the

00:20:16,270 --> 00:20:22,419
capability to the system to to work at a

00:20:19,600 --> 00:20:23,950
fixed power budget so you can select the

00:20:22,419 --> 00:20:25,960
power consumption of the northern work

00:20:23,950 --> 00:20:28,299
at this fixed power budget so how we did

00:20:25,960 --> 00:20:29,500
it basically we had a technology which

00:20:28,299 --> 00:20:31,860
we don't want to change the way in which

00:20:29,500 --> 00:20:34,360
the computer where are are actually

00:20:31,860 --> 00:20:36,010
charged and accounted to the user so if

00:20:34,360 --> 00:20:38,919
you have a user and Christina in the

00:20:36,010 --> 00:20:41,110
site in Europe the user is charged for

00:20:38,919 --> 00:20:42,669
computing our so if you are under power

00:20:41,110 --> 00:20:44,590
cap and you change the power consumption

00:20:42,669 --> 00:20:46,600
of the node the user may see a

00:20:44,590 --> 00:20:49,570
performance degradation but then how you

00:20:46,600 --> 00:20:50,950
should how much the user should pay when

00:20:49,570 --> 00:20:52,120
you have decreased the performance of

00:20:50,950 --> 00:20:54,309
the season so what we do here is

00:20:52,120 --> 00:20:55,809
actually the smart filter so we create a

00:20:54,309 --> 00:20:57,490
predictor based on the information that

00:20:55,809 --> 00:20:58,990
we collect from the job we carry the

00:20:57,490 --> 00:21:01,090
predictor that is capable of telling you

00:20:58,990 --> 00:21:03,190
how much power consumption is gonna is

00:21:01,090 --> 00:21:05,289
gonna consume a given job before the

00:21:03,190 --> 00:21:07,299
execution based on the user request and

00:21:05,289 --> 00:21:10,179
the user node and then what we do is

00:21:07,299 --> 00:21:11,830
basically we don't we we use an

00:21:10,179 --> 00:21:14,140
additional resource the power

00:21:11,830 --> 00:21:16,120
consumption and we let only the job that

00:21:14,140 --> 00:21:18,010
fits with power consumption budget to

00:21:16,120 --> 00:21:20,049
enter in the system and this actually is

00:21:18,010 --> 00:21:22,750
a is always sketch it there so you don't

00:21:20,049 --> 00:21:24,789
want actually to have more nodes more

00:21:22,750 --> 00:21:27,250
power consumption what is your power cap

00:21:24,789 --> 00:21:28,960
and and this is actually but you can

00:21:27,250 --> 00:21:31,120
also combine this with other mechanism

00:21:28,960 --> 00:21:32,799
which actually control the power so you

00:21:31,120 --> 00:21:35,409
can okay predict the power consumption

00:21:32,799 --> 00:21:37,179
gonna be 10 kilowatt and then I give a

00:21:35,409 --> 00:21:39,130
budget in our code of tanking over to

00:21:37,179 --> 00:21:41,679
these nodes so you are also safe in

00:21:39,130 --> 00:21:45,159
terms of that isn't this power will not

00:21:41,679 --> 00:21:46,720
exceed your power budget so some but can

00:21:45,159 --> 00:21:49,570
we really predict the power consumption

00:21:46,720 --> 00:21:52,090
of node before their execution so here

00:21:49,570 --> 00:21:54,850
some a plot that we got with them with

00:21:52,090 --> 00:21:56,530
our with our monitoring of different

00:21:54,850 --> 00:21:58,510
jobs which execute in the system and you

00:21:56,530 --> 00:22:00,850
can see that is actually you can get a

00:21:58,510 --> 00:22:03,039
five to ten percent of peers in the job

00:22:00,850 --> 00:22:04,960
prediction before the execution so final

00:22:03,039 --> 00:22:06,940
word about davidon so it's been a is

00:22:04,960 --> 00:22:09,159
really efficient actually turns out that

00:22:06,940 --> 00:22:11,149
it is efficient so at the beginning of

00:22:09,159 --> 00:22:12,950
our project we had a we

00:22:11,149 --> 00:22:15,169
the an energy-efficient average of

00:22:12,950 --> 00:22:17,749
energy efficiency of the top of a green

00:22:15,169 --> 00:22:20,089
500 system of three dot Shuggie probes

00:22:17,749 --> 00:22:21,799
overwrap and ended in the agency that

00:22:20,089 --> 00:22:24,979
founded the project asked us to have a

00:22:21,799 --> 00:22:26,629
30% per generation of computer we should

00:22:24,979 --> 00:22:28,399
have been more efficient than this 30%

00:22:26,629 --> 00:22:30,619
of generation so in the first

00:22:28,399 --> 00:22:32,299
implementation of davidís when we tested

00:22:30,619 --> 00:22:34,690
the first time without the OCP and

00:22:32,299 --> 00:22:38,089
without a liquid cooling we're around at

00:22:34,690 --> 00:22:40,460
7.7 gr flopper rot which was higher than

00:22:38,089 --> 00:22:43,609
which was better than the track data it

00:22:40,460 --> 00:22:45,979
was a we were it's back then finally we

00:22:43,609 --> 00:22:49,489
after we configure so thanks to the

00:22:45,979 --> 00:22:52,639
liquid cooling and also to the to the CP

00:22:49,489 --> 00:22:54,139
form factor we reach 9.3 gigaflops per

00:22:52,639 --> 00:22:55,700
watt and we can see that the liquid

00:22:54,139 --> 00:22:58,429
cooling is really working because we

00:22:55,700 --> 00:23:01,070
have below the 1.5 percent of power

00:22:58,429 --> 00:23:03,200
consumption on the fans and this is a

00:23:01,070 --> 00:23:05,029
snapshot of the power consumption during

00:23:03,200 --> 00:23:07,460
the Linpack of of each of the nodes

00:23:05,029 --> 00:23:09,830
computed with our monitoring system so

00:23:07,460 --> 00:23:12,080
in conclusion yes david with dobby that

00:23:09,830 --> 00:23:14,210
we actually show that we can create

00:23:12,080 --> 00:23:18,499
system which are turnkey in terms of

00:23:14,210 --> 00:23:19,940
self self monitoring of our what they're

00:23:18,499 --> 00:23:22,009
doing and we leverage a lot of

00:23:19,940 --> 00:23:24,559
technology that our key technology in

00:23:22,009 --> 00:23:25,849
open power system and the technology we

00:23:24,559 --> 00:23:29,330
have developed on that either can be

00:23:25,849 --> 00:23:31,429
applied to any data center so I do why

00:23:29,330 --> 00:23:33,649
we want to see it on your next p9 and

00:23:31,429 --> 00:23:35,809
voltage system so here here you can come

00:23:33,649 --> 00:23:37,549
to talk with us and yeah there's a so

00:23:35,809 --> 00:23:39,499
much knowledge meant of the team which

00:23:37,549 --> 00:23:41,659
has work on the project so from

00:23:39,499 --> 00:23:44,330
University of Bologna he for computing

00:23:41,659 --> 00:23:46,879
engineering also Western and SWAT

00:23:44,330 --> 00:23:54,009
knowledge IBM for the support and also

00:23:46,879 --> 00:23:54,009
nvidia for the support okay thank you

00:24:08,450 --> 00:24:25,289
sorry can't say yes is a five to ten

00:24:23,549 --> 00:24:27,840
percent prediction but this was actually

00:24:25,289 --> 00:24:30,029
so we did it on a previous so the

00:24:27,840 --> 00:24:33,659
question is can we really presumption

00:24:30,029 --> 00:24:35,490
and can how is it working as a power the

00:24:33,659 --> 00:24:45,690
power capping I guess the question is

00:24:35,490 --> 00:24:47,880
about so then then so the prediction is

00:24:45,690 --> 00:24:50,460
as an error so I say between five to ten

00:24:47,880 --> 00:24:52,980
percent so we didn't actually write is a

00:24:50,460 --> 00:24:54,960
good test we could run actually against

00:24:52,980 --> 00:24:57,899
the same job with different data to see

00:24:54,960 --> 00:24:59,580
how the things change so we give as a

00:24:57,899 --> 00:25:01,740
predict you'll give a lot of information

00:24:59,580 --> 00:25:03,779
so we give the users we give all the all

00:25:01,740 --> 00:25:06,260
the text which is inside the running

00:25:03,779 --> 00:25:08,970
script so somehow he has also a way of

00:25:06,260 --> 00:25:12,750
learning something additional with just

00:25:08,970 --> 00:25:14,960
the pure components but on top of that

00:25:12,750 --> 00:25:17,250
you have also the fact that has you ever

00:25:14,960 --> 00:25:19,620
attend you are interested in capping the

00:25:17,250 --> 00:25:21,659
total power of your system it's also

00:25:19,620 --> 00:25:23,850
true that all the error somehow average

00:25:21,659 --> 00:25:25,470
themselves so even if you commit

00:25:23,850 --> 00:25:26,940
somewhere statistical error on one job

00:25:25,470 --> 00:25:29,880
then at the end of the day you are going

00:25:26,940 --> 00:25:32,549
to also have some other jobs which are

00:25:29,880 --> 00:25:35,309
statistically going to compensate that

00:25:32,549 --> 00:25:37,169
that part and as I was saying this

00:25:35,309 --> 00:25:39,480
technique we made some research in a

00:25:37,169 --> 00:25:43,350
last year that can be really combined

00:25:39,480 --> 00:25:45,750
with not per node level power capping so

00:25:43,350 --> 00:25:49,440
if you use the prediction together to

00:25:45,750 --> 00:25:51,360
the capability of be sure of enforcing a

00:25:49,440 --> 00:25:53,730
dead power capping in the best way so

00:25:51,360 --> 00:25:55,860
using some hardware technology for

00:25:53,730 --> 00:25:58,230
supporting the other power cap you can

00:25:55,860 --> 00:25:59,820
basically combine the two bennett the

00:25:58,230 --> 00:26:02,340
benefit of the two so the prediction

00:25:59,820 --> 00:26:05,130
give you a good int on the total power

00:26:02,340 --> 00:26:07,620
of that job and then you ensure that you

00:26:05,130 --> 00:26:09,720
don't overpass this limit by using the

00:26:07,620 --> 00:26:10,919
other mechanism and this also will

00:26:09,720 --> 00:26:12,010
basically tells you that you are

00:26:10,919 --> 00:26:13,840
minimizing

00:26:12,010 --> 00:26:16,090
the impact on this question time of your

00:26:13,840 --> 00:26:19,000
job so the true can be combined together

00:26:16,090 --> 00:26:56,710
and this is shown that worked well when

00:26:19,000 --> 00:26:59,380
you combine the two together okay so

00:26:56,710 --> 00:27:01,210
here I will say two words to answer

00:26:59,380 --> 00:27:03,400
these questions the question is which

00:27:01,210 --> 00:27:05,140
one is the cost model which stays behind

00:27:03,400 --> 00:27:07,090
the fact of controlling the park

00:27:05,140 --> 00:27:08,350
ensemble at the center with respect of

00:27:07,090 --> 00:27:11,650
actually maximizing the throughput

00:27:08,350 --> 00:27:15,520
always so there are different scenarios

00:27:11,650 --> 00:27:17,200
so some so first the first goal of the

00:27:15,520 --> 00:27:18,970
avi that was not only controlling the

00:27:17,200 --> 00:27:21,370
power was creating power awareness so

00:27:18,970 --> 00:27:23,560
which means that until you don't have it

00:27:21,370 --> 00:27:25,240
in always-on and fine granularity system

00:27:23,560 --> 00:27:27,160
that tells you the true power of your

00:27:25,240 --> 00:27:29,320
system you don't know really how you

00:27:27,160 --> 00:27:32,440
spend your power so the first thing is

00:27:29,320 --> 00:27:35,020
that being part be energy aware so now

00:27:32,440 --> 00:27:37,090
in power aware means to be capable of

00:27:35,020 --> 00:27:37,480
understanding how you spend each of the

00:27:37,090 --> 00:27:39,070
giallo

00:27:37,480 --> 00:27:41,230
each dowel of your water this is the

00:27:39,070 --> 00:27:43,180
first goal and this the cost model is

00:27:41,230 --> 00:27:45,700
just to understand when you have a good

00:27:43,180 --> 00:27:47,440
understanding so then you have different

00:27:45,700 --> 00:27:49,810
techniques that you can do which are

00:27:47,440 --> 00:27:52,720
power aware so a technique will be a

00:27:49,810 --> 00:27:54,280
technique that is not against the fact

00:27:52,720 --> 00:27:56,140
of running your system at the maximum

00:27:54,280 --> 00:27:58,750
performance always is the fact of

00:27:56,140 --> 00:28:01,240
minimizing the energy of your C of your

00:27:58,750 --> 00:28:03,550
application but when you are under power

00:28:01,240 --> 00:28:05,770
cap so even if you think you are not

00:28:03,550 --> 00:28:08,140
power capped all of your nodes are power

00:28:05,770 --> 00:28:11,350
cap because then a scalings and so

00:28:08,140 --> 00:28:13,120
basically you could go faster in theory

00:28:11,350 --> 00:28:15,640
of the performance that you can really

00:28:13,120 --> 00:28:17,710
of what you get because you are power

00:28:15,640 --> 00:28:21,070
limited at an odd level each CPU is

00:28:17,710 --> 00:28:22,840
power limited so in that sorts of in if

00:28:21,070 --> 00:28:25,580
you if you enter in that domain so

00:28:22,840 --> 00:28:27,800
whatever you can save in terms of

00:28:25,580 --> 00:28:29,990
I can give you our throughput so you can

00:28:27,800 --> 00:28:31,490
even show that you can go faster if you

00:28:29,990 --> 00:28:34,070
are energy efficient in some

00:28:31,490 --> 00:28:35,660
circumstances but it's not the case then

00:28:34,070 --> 00:28:37,460
you have inside the case of the total

00:28:35,660 --> 00:28:39,800
power consumption of your system and

00:28:37,460 --> 00:28:43,190
this actually is a sometimes in

00:28:39,800 --> 00:28:46,640
something that is needed by external

00:28:43,190 --> 00:28:50,570
causes so you may not you may have a

00:28:46,640 --> 00:28:52,280
natural disaster so for a one month you

00:28:50,570 --> 00:28:53,990
cannot have the power consumption to

00:28:52,280 --> 00:28:55,460
power on your system so it's something

00:28:53,990 --> 00:28:57,770
that you need to have in terms for being

00:28:55,460 --> 00:29:00,080
safe so - even at under that

00:28:57,770 --> 00:29:04,040
circumstances you want to be capable of

00:29:00,080 --> 00:29:05,450
operating your system at the at the

00:29:04,040 --> 00:29:07,940
maximum efficient with this new power

00:29:05,450 --> 00:29:10,160
cap so in that sense is not something

00:29:07,940 --> 00:29:12,830
that increased then deficiency or cost

00:29:10,160 --> 00:29:14,960
efficiency might give you allow you to

00:29:12,830 --> 00:29:17,510
be capable of running your system under

00:29:14,960 --> 00:29:20,510
a natural disasters case this happens in

00:29:17,510 --> 00:29:25,040
Japan for instance and you may also have

00:29:20,510 --> 00:29:27,260
that if you then you have also these are

00:29:25,040 --> 00:29:30,680
you have other cases in which actually

00:29:27,260 --> 00:29:32,990
you you may you are not to truly

00:29:30,680 --> 00:29:35,180
feasible in terms of power consumption

00:29:32,990 --> 00:29:36,920
so you may when you design your data

00:29:35,180 --> 00:29:38,750
center okay you know that your average

00:29:36,920 --> 00:29:40,370
power concern you have a digital edition

00:29:38,750 --> 00:29:42,350
of your system is not hundred percent is

00:29:40,370 --> 00:29:45,380
not a limp ekran but it's gonna be the

00:29:42,350 --> 00:29:47,690
70% or 80% so you may want to design

00:29:45,380 --> 00:29:50,750
your power supply for your system to

00:29:47,690 --> 00:29:53,750
that 80% and the cooling to this 80% but

00:29:50,750 --> 00:29:56,150
in cases that you have so then you need

00:29:53,750 --> 00:29:58,190
to have somehow a way to ensure that you

00:29:56,150 --> 00:30:01,400
will never pass this one because you are

00:29:58,190 --> 00:30:03,140
not designed for it then you have so

00:30:01,400 --> 00:30:05,750
these are actually I will say the the

00:30:03,140 --> 00:30:08,890
first goal so per se is not the tool

00:30:05,750 --> 00:30:11,600
that you will not so there are some

00:30:08,890 --> 00:30:13,220
theory that also you can also think

00:30:11,600 --> 00:30:15,380
about our provision in your system but

00:30:13,220 --> 00:30:16,760
it really depends on it depends on the

00:30:15,380 --> 00:30:18,560
turnaround of your system so if you

00:30:16,760 --> 00:30:20,480
really throw away your system after

00:30:18,560 --> 00:30:22,940
three years your cost re dominated by

00:30:20,480 --> 00:30:24,440
their by the fact by the arguer so you

00:30:22,940 --> 00:30:26,480
want to use it at the maximum have the

00:30:24,440 --> 00:30:28,130
maximum science out of it but you have

00:30:26,480 --> 00:30:33,570
external codons which this technology

00:30:28,130 --> 00:30:35,700
want them to add it and

00:30:33,570 --> 00:30:37,890
of course I was saying before having the

00:30:35,700 --> 00:30:40,290
awareness of the how the power

00:30:37,890 --> 00:30:43,290
consumption happens in in the refinery

00:30:40,290 --> 00:30:45,570
right you may discover that you can save

00:30:43,290 --> 00:30:47,460
a lot when you are in different phases

00:30:45,570 --> 00:30:50,310
of your application and you can see that

00:30:47,460 --> 00:30:53,010
in that sense is gonna go to speed up

00:30:50,310 --> 00:30:55,790
your application under any power limit

00:30:53,010 --> 00:30:55,790
of a single node

00:31:05,860 --> 00:31:09,080

YouTube URL: https://www.youtube.com/watch?v=bidau4cdpFo


