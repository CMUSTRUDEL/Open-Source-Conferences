Title: OpenPOWER Summit US 2018: Revolutionary Performance and Scalability for HPC and AI with Mellanox
Publication date: 2018-04-03
Playlist: OpenPOWER Summit US 2018
Description: 
	OpenPOWER member from Mellanox discusses their use of POWER technology at OpenPOWER Summit 2018.

Presenter:
- Scot Schultz, Director HPC-Technical Computing, Mellanox

For more information, please visit: http://www.openpowerfoundation.org
Captions: 
	00:00:00,480 --> 00:00:06,799
so thanks for for joining I'm Scott

00:00:02,909 --> 00:00:10,500
Schultz I'm one of the managers at

00:00:06,799 --> 00:00:13,170
Mellanox I one of my tasks is HPC and

00:00:10,500 --> 00:00:16,379
and more recently AI so I manage all the

00:00:13,170 --> 00:00:19,529
AI activities I'm also a board member

00:00:16,379 --> 00:00:20,970
here with an open power and very active

00:00:19,529 --> 00:00:23,460
in the technical steering committee and

00:00:20,970 --> 00:00:26,789
and whatnot from the power side so

00:00:23,460 --> 00:00:30,750
really what I wanted to begin the this

00:00:26,789 --> 00:00:32,130
talk on it was around as far as

00:00:30,750 --> 00:00:35,340
interconnect technology and how it

00:00:32,130 --> 00:00:37,050
applies to open power HPC and AI so

00:00:35,340 --> 00:00:40,140
really in order to ensure the highest

00:00:37,050 --> 00:00:42,300
performance for you know a diverse set

00:00:40,140 --> 00:00:45,180
of workloads it's really important to

00:00:42,300 --> 00:00:47,010
understand how data is used and how it

00:00:45,180 --> 00:00:49,500
really Co exists in similar environments

00:00:47,010 --> 00:00:53,399
right so high-performance computing

00:00:49,500 --> 00:00:56,910
AI and Big Data really have very similar

00:00:53,399 --> 00:00:58,559
underlying hardware needs especially

00:00:56,910 --> 00:01:00,600
when it comes to the to the networking

00:00:58,559 --> 00:01:05,220
requirements and it's really important

00:01:00,600 --> 00:01:07,049
to it it's it's it's all about the

00:01:05,220 --> 00:01:09,950
ability to to store the data and move

00:01:07,049 --> 00:01:13,229
the data and to exchange you know

00:01:09,950 --> 00:01:15,810
messages and even the computed results

00:01:13,229 --> 00:01:18,180
right of thousands of parallel tasks and

00:01:15,810 --> 00:01:20,280
and and get and have that done fast

00:01:18,180 --> 00:01:22,500
enough in order to keep the compute

00:01:20,280 --> 00:01:24,299
resources busy and running at peak

00:01:22,500 --> 00:01:24,930
efficiency and this is really where

00:01:24,299 --> 00:01:29,369
Mellanox

00:01:24,930 --> 00:01:33,240
plays an important role so with with key

00:01:29,369 --> 00:01:38,159
offloads which will go into GPU direct

00:01:33,240 --> 00:01:40,259
our our DMA our sharp is one of the

00:01:38,159 --> 00:01:42,780
topics that we'll talk about you know

00:01:40,259 --> 00:01:46,430
these types of things are now becoming

00:01:42,780 --> 00:01:49,530
the de facto standard when it comes to

00:01:46,430 --> 00:01:53,070
these widely adopted technologies that

00:01:49,530 --> 00:01:55,140
are being deployed today so now not to

00:01:53,070 --> 00:01:57,420
forget about the number one system in

00:01:55,140 --> 00:01:58,860
China but Mellanox is also connecting

00:01:57,420 --> 00:02:02,460
the number one systems all around the

00:01:58,860 --> 00:02:04,290
world so today you know alongside the US

00:02:02,460 --> 00:02:07,259
leadership systems that are on track the

00:02:04,290 --> 00:02:09,739
summit and Sierra systems that are going

00:02:07,259 --> 00:02:12,910
to be deployed here shortly

00:02:09,739 --> 00:02:15,640
as well as

00:02:12,910 --> 00:02:18,010
the fastest supercomputer in Japan and

00:02:15,640 --> 00:02:20,050
then in the one in from sigh net in

00:02:18,010 --> 00:02:21,910
Canada and what's what's interesting

00:02:20,050 --> 00:02:24,670
about about this system as well is that

00:02:21,910 --> 00:02:27,459
it's being deployed with dragonfly plus

00:02:24,670 --> 00:02:31,390
which is one of the newest topologies

00:02:27,459 --> 00:02:34,030
from Mellanox and what what's

00:02:31,390 --> 00:02:35,739
interesting there is you're able to

00:02:34,030 --> 00:02:38,319
achieve a greater amount of efficiency

00:02:35,739 --> 00:02:41,230
and very similar performance of that of

00:02:38,319 --> 00:02:43,959
a native fat tree topology but really in

00:02:41,230 --> 00:02:45,670
a much more cost-effective way and so we

00:02:43,959 --> 00:02:49,390
believe that that dragonfly plus is

00:02:45,670 --> 00:02:52,030
going to be more pervasive as as we move

00:02:49,390 --> 00:02:53,560
into 200 and 400 and and beyond as far

00:02:52,030 --> 00:02:56,670
as the in as far as the network is

00:02:53,560 --> 00:02:59,200
concerned so this year really has

00:02:56,670 --> 00:03:01,450
already proven to be a huge game-changer

00:02:59,200 --> 00:03:04,390
in the next generation of performance

00:03:01,450 --> 00:03:08,650
right for both HPC is as well as AI and

00:03:04,390 --> 00:03:11,310
you know the race continues to to be

00:03:08,650 --> 00:03:14,049
around solving that the challenges of

00:03:11,310 --> 00:03:17,069
not only the traditional HPC workloads

00:03:14,049 --> 00:03:20,220
but also for scalable deep learning and

00:03:17,069 --> 00:03:23,079
you know keep in mind that most of the

00:03:20,220 --> 00:03:25,329
almost all of the most popular diene

00:03:23,079 --> 00:03:29,590
frameworks deep learning frameworks

00:03:25,329 --> 00:03:30,760
today can scale to multiple GPUs within

00:03:29,590 --> 00:03:33,040
a server that's really not a problem

00:03:30,760 --> 00:03:36,959
really what the challenge is is it's

00:03:33,040 --> 00:03:40,150
it's more difficult to to scale to

00:03:36,959 --> 00:03:43,630
multiple servers each having multiple

00:03:40,150 --> 00:03:44,980
GPUs and so we have you know and the

00:03:43,630 --> 00:03:47,109
challenge in particular is precisely

00:03:44,980 --> 00:03:49,150
where Mellanox has been the the the

00:03:47,109 --> 00:03:53,079
clear leader as the only interconnect

00:03:49,150 --> 00:03:56,109
solution needed to be able to deliver

00:03:53,079 --> 00:03:58,269
the needed performance around AI an HPC

00:03:56,109 --> 00:04:00,190
so you know we have a complete

00:03:58,269 --> 00:04:03,549
end-to-end solution of intelligent

00:04:00,190 --> 00:04:06,310
network endpoints switches and and

00:04:03,549 --> 00:04:07,780
cables and and really it's it's the most

00:04:06,310 --> 00:04:09,370
reliable and the most suitable

00:04:07,780 --> 00:04:14,639
interconnect for the for the power

00:04:09,370 --> 00:04:17,650
architecture so okay so so date today

00:04:14,639 --> 00:04:19,720
you know data growth remains the biggest

00:04:17,650 --> 00:04:22,300
challenge in in today's data center

00:04:19,720 --> 00:04:23,890
infrastructures and while there are many

00:04:22,300 --> 00:04:25,630
many sources that predict you know

00:04:23,890 --> 00:04:29,500
exponential growth of

00:04:25,630 --> 00:04:31,570
data out to 2020 and beyond they all are

00:04:29,500 --> 00:04:33,310
really much in broad agreement that the

00:04:31,570 --> 00:04:34,480
size of the digital universe is going to

00:04:33,310 --> 00:04:37,630
double right

00:04:34,480 --> 00:04:39,130
almost every two years so storage data

00:04:37,630 --> 00:04:43,360
movement and the complexity of keeping

00:04:39,130 --> 00:04:46,990
did the data secure is these are really

00:04:43,360 --> 00:04:49,210
key challenges for any data center now

00:04:46,990 --> 00:04:51,310
here's a couple of really interesting

00:04:49,210 --> 00:04:53,380
examples of how much data is really

00:04:51,310 --> 00:04:57,150
being generated today and what I mean

00:04:53,380 --> 00:04:59,650
Big Data I mean really big data right so

00:04:57,150 --> 00:05:02,560
if you take a look at these two examples

00:04:59,650 --> 00:05:04,270
you know these are these are fascinating

00:05:02,560 --> 00:05:07,300
examples when you you you you think

00:05:04,270 --> 00:05:10,570
about one Pratt & Whitney engine being

00:05:07,300 --> 00:05:13,450
able to generate up to 844 terabytes of

00:05:10,570 --> 00:05:15,910
of data on a 12-hour flight that's a lot

00:05:13,450 --> 00:05:19,660
of data but also keep in mind that you

00:05:15,910 --> 00:05:22,090
know today even most companies that are

00:05:19,660 --> 00:05:25,120
based in the US have at least about a

00:05:22,090 --> 00:05:27,850
hundred terabytes of data right and and

00:05:25,120 --> 00:05:29,410
if you take a look at at Nasdaq or the

00:05:27,850 --> 00:05:30,520
New York Stock Exchange in particular

00:05:29,410 --> 00:05:32,290
right

00:05:30,520 --> 00:05:34,210
they actually capture more than a

00:05:32,290 --> 00:05:39,100
terabyte of data every single trading

00:05:34,210 --> 00:05:43,930
day right every trading session so it's

00:05:39,100 --> 00:05:45,580
it's a it's it's really an an amazing

00:05:43,930 --> 00:05:47,620
amount of data that we're going to be

00:05:45,580 --> 00:05:50,230
able to rip through and process and the

00:05:47,620 --> 00:05:52,800
network really is critical from from

00:05:50,230 --> 00:05:55,630
moving the data for point A to point B

00:05:52,800 --> 00:05:57,220
so this is just a very simple kind of

00:05:55,630 --> 00:05:58,960
slide that basically says hey well you

00:05:57,220 --> 00:06:01,330
know what okay the network's important

00:05:58,960 --> 00:06:03,520
but really what is important you know

00:06:01,330 --> 00:06:05,560
why is not bandwidth just the only thing

00:06:03,520 --> 00:06:07,210
that you're thinking about so really

00:06:05,560 --> 00:06:11,170
what's important for machine learning

00:06:07,210 --> 00:06:13,330
and Big Data it's listed right here

00:06:11,170 --> 00:06:15,790
right so you know high bandwidth and low

00:06:13,330 --> 00:06:17,260
latency and you know there's from a

00:06:15,790 --> 00:06:21,280
Mellanox perspective we have both

00:06:17,260 --> 00:06:23,530
InfiniBand as well as Ethernet so

00:06:21,280 --> 00:06:24,700
depending on the infrastructure you know

00:06:23,530 --> 00:06:26,710
you're always going to get better

00:06:24,700 --> 00:06:28,180
performance with InfiniBand quite quite

00:06:26,710 --> 00:06:29,950
honestly because our switches actually

00:06:28,180 --> 00:06:31,960
have less than 999 a seconds of

00:06:29,950 --> 00:06:33,700
port-to-port latency but even on the

00:06:31,960 --> 00:06:36,700
Ethernet side if you're thinking about

00:06:33,700 --> 00:06:38,890
moving data using our DMA over converged

00:06:36,700 --> 00:06:39,660
Ethernet and taking advantage of all the

00:06:38,890 --> 00:06:43,530
acceleration

00:06:39,660 --> 00:06:46,230
engines there right spectrum is one of

00:06:43,530 --> 00:06:48,510
the world's lowest latency Ethernet

00:06:46,230 --> 00:06:51,150
switches available so the port latency

00:06:48,510 --> 00:06:54,750
there is lesson I think 300 nanoseconds

00:06:51,150 --> 00:06:57,210
so spectrum is also designed around zero

00:06:54,750 --> 00:06:59,220
packet loss alright so in the world of

00:06:57,210 --> 00:07:00,780
our DMA right it's very critical and

00:06:59,220 --> 00:07:02,760
basically the rules of engagement there

00:07:00,780 --> 00:07:05,670
says you you know you know you the

00:07:02,760 --> 00:07:07,070
packets must arrive in order and on the

00:07:05,670 --> 00:07:11,040
Ethernet side that's really a challenge

00:07:07,070 --> 00:07:14,310
so spectrum is very well balanced in in

00:07:11,040 --> 00:07:16,050
nature to be able to to be a zero packet

00:07:14,310 --> 00:07:18,660
loss switch being able to deliver

00:07:16,050 --> 00:07:23,820
packets in order with very very very

00:07:18,660 --> 00:07:25,860
infrequent a packet drop if ever but but

00:07:23,820 --> 00:07:28,890
mostly I think a most important is

00:07:25,860 --> 00:07:31,650
around these you know it's all about the

00:07:28,890 --> 00:07:33,480
native offload so our DMA GPU direct our

00:07:31,650 --> 00:07:36,690
DMA you know I'll cover those in just a

00:07:33,480 --> 00:07:38,820
few minutes you know people are moving

00:07:36,690 --> 00:07:42,000
to containers support for virtualization

00:07:38,820 --> 00:07:45,390
in containers is very critical but also

00:07:42,000 --> 00:07:48,660
I'm gonna highlight and talk about sharp

00:07:45,390 --> 00:07:52,560
which is a our approach to what is

00:07:48,660 --> 00:07:54,180
called in network computing and and and

00:07:52,560 --> 00:07:57,180
then of course the storage acceleration

00:07:54,180 --> 00:07:59,450
right so you know our DMA and nvme over

00:07:57,180 --> 00:08:02,070
fabrics is is beginning to take off and

00:07:59,450 --> 00:08:06,210
storage acceleration is very very very

00:08:02,070 --> 00:08:10,290
important as well as security ok so

00:08:06,210 --> 00:08:11,610
really our approach to solving some of

00:08:10,290 --> 00:08:14,130
the key bottlenecks in terms of moving

00:08:11,610 --> 00:08:17,670
data is actually working on the data as

00:08:14,130 --> 00:08:19,350
it traverses the network so you know by

00:08:17,670 --> 00:08:21,480
introducing this in this notion of

00:08:19,350 --> 00:08:23,760
intelligent co-processing capabilities

00:08:21,480 --> 00:08:26,520
into the network reduces communication

00:08:23,760 --> 00:08:29,760
latency by it by an order of magnitude

00:08:26,520 --> 00:08:32,010
right so you know and and that really

00:08:29,760 --> 00:08:33,719
results in exceptional performance to

00:08:32,010 --> 00:08:35,849
the application for not only traditional

00:08:33,719 --> 00:08:39,000
HPC but also for the the new workloads

00:08:35,849 --> 00:08:41,280
around AI and deep learning and and and

00:08:39,000 --> 00:08:43,289
really when I when I talk about the some

00:08:41,280 --> 00:08:45,060
of the core features of the interconnect

00:08:43,289 --> 00:08:48,960
today right it's not just a network that

00:08:45,060 --> 00:08:52,260
just moves data sharp is gives you the

00:08:48,960 --> 00:08:53,939
ability to move to compute data as it

00:08:52,260 --> 00:08:56,129
traverses the network

00:08:53,939 --> 00:08:57,629
so instead of having to move it from one

00:08:56,129 --> 00:08:59,519
endpoint to another and then start

00:08:57,629 --> 00:09:00,810
working on the data right we start

00:08:59,519 --> 00:09:05,339
working on the data as it moves through

00:09:00,810 --> 00:09:07,769
the network MPI is is the this is an

00:09:05,339 --> 00:09:10,980
example of working on the data and the

00:09:07,769 --> 00:09:14,730
endpoints so we're able to now move more

00:09:10,980 --> 00:09:16,649
collectives and more MPI operations not

00:09:14,730 --> 00:09:23,579
not only onto the switches but we're

00:09:16,649 --> 00:09:25,829
able to handle things like MPI in at the

00:09:23,579 --> 00:09:28,680
endpoint all right so being able to do

00:09:25,829 --> 00:09:31,319
tag matching and hardware results in

00:09:28,680 --> 00:09:33,079
more than 35 X performance improvement

00:09:31,319 --> 00:09:37,040
when you're when you're when you're

00:09:33,079 --> 00:09:39,990
talking about traditional HPC and MPI

00:09:37,040 --> 00:09:43,829
workloads so we also have this notion of

00:09:39,990 --> 00:09:45,540
shield which is is quite interesting in

00:09:43,829 --> 00:09:48,930
in the sense that it allows you to

00:09:45,540 --> 00:09:50,100
recover from from you know let's say

00:09:48,930 --> 00:09:52,620
you're doing maintenance or you pull a

00:09:50,100 --> 00:09:54,480
cable or a port goes down or something

00:09:52,620 --> 00:09:57,240
like that right we're able to actually

00:09:54,480 --> 00:09:59,670
use communication techniques within the

00:09:57,240 --> 00:10:02,129
network to notify the switches that hey

00:09:59,670 --> 00:10:06,389
there's a problem and and make immediate

00:10:02,129 --> 00:10:09,720
changes into the the path in which the

00:10:06,389 --> 00:10:12,449
the the data movement is taken and so

00:10:09,720 --> 00:10:14,459
that really is turns the the network

00:10:12,449 --> 00:10:16,550
into our Mellanox network in particular

00:10:14,459 --> 00:10:19,589
into a very robust type of self-healing

00:10:16,550 --> 00:10:22,230
capable network and then I'll talk a

00:10:19,589 --> 00:10:24,569
little bit more than two GPU direct our

00:10:22,230 --> 00:10:27,360
DMA and you know thanks for being here

00:10:24,569 --> 00:10:29,309
because I know DK pandas talking about

00:10:27,360 --> 00:10:33,600
his M that pitch over in the other room

00:10:29,309 --> 00:10:35,309
as we speak so you know like I mentioned

00:10:33,600 --> 00:10:37,920
InfiniBand delivers the best return on

00:10:35,309 --> 00:10:40,829
investment and you know it really is

00:10:37,920 --> 00:10:42,240
awesome to be able to to take a look at

00:10:40,829 --> 00:10:44,220
a technology that can give you

00:10:42,240 --> 00:10:46,559
out-of-the-box about two and a half x

00:10:44,220 --> 00:10:48,629
times better performance than any of the

00:10:46,559 --> 00:10:50,759
competitive solutions and it's easy to

00:10:48,629 --> 00:10:53,129
see here why Mellanox has always been

00:10:50,759 --> 00:10:54,769
the de facto standard when it comes to

00:10:53,129 --> 00:10:56,790
you know high performance computing

00:10:54,769 --> 00:10:59,009
especially for weather codes in

00:10:56,790 --> 00:11:01,410
automotive and chemistry right and these

00:10:59,009 --> 00:11:03,600
are not just Mellanox numbers right

00:11:01,410 --> 00:11:05,040
these are customer numbers that that

00:11:03,600 --> 00:11:07,500
real customers have actually achieved

00:11:05,040 --> 00:11:09,300
achieved and said you know

00:11:07,500 --> 00:11:10,860
you know thang-thang thankfully we were

00:11:09,300 --> 00:11:12,779
able to just put in the Mellanox

00:11:10,860 --> 00:11:15,089
interconnect and it just works out of

00:11:12,779 --> 00:11:16,980
the box and and you know four codes like

00:11:15,089 --> 00:11:20,670
wrf this is the performance that we're

00:11:16,980 --> 00:11:24,839
getting so you know but also we can

00:11:20,670 --> 00:11:26,400
unlock better performance for for AI not

00:11:24,839 --> 00:11:29,279
not only from a performance perspective

00:11:26,400 --> 00:11:32,070
but also from a scalability perspective

00:11:29,279 --> 00:11:34,140
right getting getting these boxes are

00:11:32,070 --> 00:11:38,250
getting the algorithms to move outside

00:11:34,140 --> 00:11:40,950
of just the single fat box right so

00:11:38,250 --> 00:11:44,430
distributed AI is is is really the the

00:11:40,950 --> 00:11:46,830
wave of the future and I'm gonna spend a

00:11:44,430 --> 00:11:49,310
little bit more time talking to this and

00:11:46,830 --> 00:11:51,960
today you know nearly every leading main

00:11:49,310 --> 00:11:53,660
machine learning framework actually is

00:11:51,960 --> 00:11:56,490
natively enabled with Mellanox

00:11:53,660 --> 00:11:59,100
accelerations under the hood so being

00:11:56,490 --> 00:12:01,980
able to take advantage of our DMA GPU

00:11:59,100 --> 00:12:03,750
direct our DMA then these types of

00:12:01,980 --> 00:12:05,880
things are key to reduce the training

00:12:03,750 --> 00:12:09,839
times and actually increase accuracies

00:12:05,880 --> 00:12:11,430
in a in a much reduced time so being

00:12:09,839 --> 00:12:13,470
able to start a model training before

00:12:11,430 --> 00:12:15,209
leaving at the end of the day right and

00:12:13,470 --> 00:12:17,310
having those results there for you early

00:12:15,209 --> 00:12:19,580
in the morning really this is this is

00:12:17,310 --> 00:12:25,380
the game changer for the data scientist

00:12:19,580 --> 00:12:26,910
and and so you know when we talk to you

00:12:25,380 --> 00:12:28,920
know the types of performance that we're

00:12:26,910 --> 00:12:31,220
talking about GPU direct is one of them

00:12:28,920 --> 00:12:34,709
in particular that has been around for

00:12:31,220 --> 00:12:36,750
many many years now and it's really more

00:12:34,709 --> 00:12:39,660
optimized than ever and if you take a

00:12:36,750 --> 00:12:42,120
look we're moving data from one GPU to a

00:12:39,660 --> 00:12:44,400
remote GPU in a different compute note

00:12:42,120 --> 00:12:46,860
across the fabric no longer requires the

00:12:44,400 --> 00:12:48,900
CPU to be involved in the movement of

00:12:46,860 --> 00:12:51,630
that data and so that really reduces

00:12:48,900 --> 00:12:54,600
your Layton sees by 10x so what used to

00:12:51,630 --> 00:12:57,839
take you more than 19 to 20 microseconds

00:12:54,600 --> 00:13:01,290
to to move a message from point A to

00:12:57,839 --> 00:13:05,310
point B now takes you you know well

00:13:01,290 --> 00:13:07,800
under two and so you know you know GPU

00:13:05,310 --> 00:13:10,170
director really was purpose-built for

00:13:07,800 --> 00:13:11,880
for things like deep learning and and

00:13:10,170 --> 00:13:13,740
really gives you the you know ultimate

00:13:11,880 --> 00:13:16,140
in in in lowest latency latency

00:13:13,740 --> 00:13:20,490
communications across any acceleration

00:13:16,140 --> 00:13:22,709
devices but in particular GPUs

00:13:20,490 --> 00:13:25,529
now there's also a newer version of GPU

00:13:22,709 --> 00:13:28,380
direct called async and in the previous

00:13:25,529 --> 00:13:30,870
slide and you can see that the the data

00:13:28,380 --> 00:13:33,630
path is basically what what's being

00:13:30,870 --> 00:13:35,820
offloaded here the control path is is is

00:13:33,630 --> 00:13:37,560
not right so the CUDA threads are

00:13:35,820 --> 00:13:40,100
launched and the CPU still needs to

00:13:37,560 --> 00:13:43,830
manage those those those those threads

00:13:40,100 --> 00:13:45,690
and those kernels and so the CPU is

00:13:43,830 --> 00:13:47,700
somewhat still involved but with GPU

00:13:45,690 --> 00:13:51,029
direct async we're not only removing the

00:13:47,700 --> 00:13:52,920
data path from the from CPU involvement

00:13:51,029 --> 00:13:55,050
but we're also moving the removing the

00:13:52,920 --> 00:13:57,120
control path so now what you'll

00:13:55,050 --> 00:14:01,290
basically be able to launch batches of

00:13:57,120 --> 00:14:03,660
of kernels from from CUDA and the we're

00:14:01,290 --> 00:14:10,380
giving all the control of the network

00:14:03,660 --> 00:14:12,680
basically over to the GPU and so so as

00:14:10,380 --> 00:14:15,149
both the data path and the control path

00:14:12,680 --> 00:14:17,250
go directly between the GPU and the

00:14:15,149 --> 00:14:20,310
Mellanox interconnect that really allows

00:14:17,250 --> 00:14:22,620
for several things in particular you'd

00:14:20,310 --> 00:14:25,610
be able to to fundamentally use a much

00:14:22,620 --> 00:14:29,130
lower cost lower power and slower

00:14:25,610 --> 00:14:33,870
clocked CPU actually that that's one of

00:14:29,130 --> 00:14:37,829
the biggest benefits but you know we we

00:14:33,870 --> 00:14:39,270
believe that that this is going to you

00:14:37,829 --> 00:14:42,510
know give give even better performance

00:14:39,270 --> 00:14:45,240
than just by offloading the data path

00:14:42,510 --> 00:14:49,649
alone now this is available already

00:14:45,240 --> 00:14:52,320
today it is one of the features I think

00:14:49,649 --> 00:14:55,490
it's supported in Mellanox ill-fed today

00:14:52,320 --> 00:14:58,110
so you can always take take a look at it

00:14:55,490 --> 00:15:01,200
and then what I wanted to do is talk a

00:14:58,110 --> 00:15:03,779
little bit around the notion of

00:15:01,200 --> 00:15:05,579
distributed training and training on

00:15:03,779 --> 00:15:07,500
large data sets can take a really really

00:15:05,579 --> 00:15:09,300
long time right and so in many cases the

00:15:07,500 --> 00:15:14,250
training has to to happen very

00:15:09,300 --> 00:15:16,820
frequently and and as as you know and

00:15:14,250 --> 00:15:18,839
the notion is is as you add more workers

00:15:16,820 --> 00:15:21,320
it's going to actually reduce your

00:15:18,839 --> 00:15:23,310
training time and hopefully you know

00:15:21,320 --> 00:15:25,980
accuracy is not affected by that

00:15:23,310 --> 00:15:29,279
whatsoever and really data parallelism

00:15:25,980 --> 00:15:33,150
is the is a solution and and when you

00:15:29,279 --> 00:15:33,990
talk to model parallelism right in some

00:15:33,150 --> 00:15:36,209
cases the model

00:15:33,990 --> 00:15:38,279
doesn't even fit into compute right so

00:15:36,209 --> 00:15:41,250
you know if you had a smaller compute

00:15:38,279 --> 00:15:44,490
engine engine or something like an FPGA

00:15:41,250 --> 00:15:46,260
right the model won't even fit and so

00:15:44,490 --> 00:15:48,899
here networking really does become a

00:15:46,260 --> 00:15:50,790
critical element and in particular the

00:15:48,899 --> 00:15:52,589
high bandwidth and low latency and and

00:15:50,790 --> 00:15:55,680
and our team a are almost mandatory when

00:15:52,589 --> 00:16:01,050
you move into this type of model

00:15:55,680 --> 00:16:02,970
parallelism so so I understand you had a

00:16:01,050 --> 00:16:04,440
little bit interest in Sharpe and how

00:16:02,970 --> 00:16:12,350
it's going to help you with your AI

00:16:04,440 --> 00:16:15,060
models so so one of the key elements of

00:16:12,350 --> 00:16:17,700
using this network as a coprocessor is

00:16:15,060 --> 00:16:21,810
is is with what is called sharp and

00:16:17,700 --> 00:16:24,630
sharp being a is really a reliable

00:16:21,810 --> 00:16:27,660
scalable general-purpose type of design

00:16:24,630 --> 00:16:31,860
which makes in in network tree-based

00:16:27,660 --> 00:16:34,350
algorithm mechanisms and today it's it's

00:16:31,860 --> 00:16:39,690
it's already used in and deployed in HPC

00:16:34,350 --> 00:16:41,100
installations in particular like like

00:16:39,690 --> 00:16:42,360
you mentioned for the coral deployments

00:16:41,100 --> 00:16:45,450
and whatnot

00:16:42,360 --> 00:16:48,470
these are based on on the switch I v2

00:16:45,450 --> 00:16:51,149
which is a hundred gig in infrastructure

00:16:48,470 --> 00:16:54,029
this this type of in network computing

00:16:51,149 --> 00:16:58,140
manages and execute these MPI operations

00:16:54,029 --> 00:17:00,270
very seamlessly from the switches but

00:16:58,140 --> 00:17:05,850
are very much now today geared towards

00:17:00,270 --> 00:17:09,209
the the smaller message sizes and and so

00:17:05,850 --> 00:17:15,089
with our upcoming quantum base switches

00:17:09,209 --> 00:17:17,610
and 200 gigabit technology then then

00:17:15,089 --> 00:17:24,000
then basically what we're able to do is

00:17:17,610 --> 00:17:26,780
very similar operations to spend all

00:17:24,000 --> 00:17:29,220
reduce being being the primary

00:17:26,780 --> 00:17:32,160
collective for AI that that's most

00:17:29,220 --> 00:17:35,580
important we're able to actually handle

00:17:32,160 --> 00:17:41,460
up to a two gigabyte message size at

00:17:35,580 --> 00:17:42,750
wire speed right and so basically you

00:17:41,460 --> 00:17:46,650
know sharp can perform these high

00:17:42,750 --> 00:17:47,820
collective operations not not only of

00:17:46,650 --> 00:17:50,820
course you know all reduce

00:17:47,820 --> 00:17:53,430
which is very very important for AI but

00:17:50,820 --> 00:17:56,100
barrier reduce you know broadcast these

00:17:53,430 --> 00:17:58,680
are other collectives that are already

00:17:56,100 --> 00:18:00,300
supported in the switches and and when I

00:17:58,680 --> 00:18:01,560
when I said that we actually manipulate

00:18:00,300 --> 00:18:04,350
and we can actually work on the data

00:18:01,560 --> 00:18:07,800
right the the actual switches themselves

00:18:04,350 --> 00:18:11,130
have the ability to to perform some min

00:18:07,800 --> 00:18:16,350
and Max and you know or an X or in these

00:18:11,130 --> 00:18:18,090
types of operations on the data so so

00:18:16,350 --> 00:18:22,680
that's the concept in the in the notion

00:18:18,090 --> 00:18:27,840
behind sharp and and this is is one

00:18:22,680 --> 00:18:28,920
example right of being able to to show

00:18:27,840 --> 00:18:32,550
you the performance of how it's

00:18:28,920 --> 00:18:34,800
basically done today or how you can do

00:18:32,550 --> 00:18:37,980
it within the network using using the

00:18:34,800 --> 00:18:39,810
Mellanox switches right so this shows

00:18:37,980 --> 00:18:42,240
you fundamentally that as you add more

00:18:39,810 --> 00:18:45,150
compute infrastructure to to these

00:18:42,240 --> 00:18:48,390
parallel communications right the the

00:18:45,150 --> 00:18:50,220
latency remains very flat and and that

00:18:48,390 --> 00:18:54,210
that's a good sign of being able to show

00:18:50,220 --> 00:18:57,240
how scalable a network is so if you're

00:18:54,210 --> 00:18:59,100
doing it in software today right you can

00:18:57,240 --> 00:19:02,430
see that that the latency goes up as you

00:18:59,100 --> 00:19:04,890
continue to add more compute elements so

00:19:02,430 --> 00:19:08,010
so that's a kind of a non scalable cut

00:19:04,890 --> 00:19:09,750
type of approach it is some approach so

00:19:08,010 --> 00:19:11,490
we believe that sharp is really a

00:19:09,750 --> 00:19:13,200
game-changer in the fact that you can

00:19:11,490 --> 00:19:15,210
you can you can you can enable about a

00:19:13,200 --> 00:19:20,300
75 percent reduction in latency and

00:19:15,210 --> 00:19:24,900
really keep that that latency very flat

00:19:20,300 --> 00:19:28,020
this is a kind of an animated slide of

00:19:24,900 --> 00:19:31,580
fundamentally how sharp will influence

00:19:28,020 --> 00:19:36,060
the the AI the distributed model right

00:19:31,580 --> 00:19:39,630
instead of having a parallel a parameter

00:19:36,060 --> 00:19:41,550
server basically sharp will replace the

00:19:39,630 --> 00:19:43,920
need for an external parameter server

00:19:41,550 --> 00:19:46,140
and and where you once use that

00:19:43,920 --> 00:19:47,790
parameter server right to perform maybe

00:19:46,140 --> 00:19:49,380
the gradient averaging sharp actually

00:19:47,790 --> 00:19:52,500
removes the the physical server

00:19:49,380 --> 00:19:54,090
completely right and read and actually

00:19:52,500 --> 00:19:56,550
reduces the amount of data that has to

00:19:54,090 --> 00:19:58,860
traverse the network so we believe that

00:19:56,550 --> 00:20:01,179
you know for for AI and deep learning

00:19:58,860 --> 00:20:04,359
sharp is going to be a very

00:20:01,179 --> 00:20:08,379
significant technology and a game

00:20:04,359 --> 00:20:10,029
changer when it comes to performance and

00:20:08,379 --> 00:20:12,519
and what not for for these types of

00:20:10,029 --> 00:20:13,809
workloads now this these these I'm just

00:20:12,519 --> 00:20:15,609
going to go through a few samples and

00:20:13,809 --> 00:20:19,149
then we'll be able to wrap up and maybe

00:20:15,609 --> 00:20:21,580
take a few questions this is this is

00:20:19,149 --> 00:20:24,279
tensorflow the some of the initial work

00:20:21,580 --> 00:20:26,739
that was done I mentioned that you know

00:20:24,279 --> 00:20:29,710
most of the frameworks tensorflow and

00:20:26,739 --> 00:20:33,940
cafe and Microsoft cognitive tool

00:20:29,710 --> 00:20:36,099
computing cognitive toolkit and and many

00:20:33,940 --> 00:20:39,179
others most frameworks paddle paddle

00:20:36,099 --> 00:20:41,979
they all now enable RDMA out-of-the-box

00:20:39,179 --> 00:20:52,649
and and so this was some of the early

00:20:41,979 --> 00:20:54,460
work where you can see that amount of

00:20:52,649 --> 00:20:59,320
compute that you're actually gonna get

00:20:54,460 --> 00:21:02,139
out of these GPUs this in particular is

00:20:59,320 --> 00:21:03,609
on a smaller set resin at 50 but you can

00:21:02,139 --> 00:21:06,419
see that you know as you scale out

00:21:03,609 --> 00:21:08,080
across GPUs that you you're achieving

00:21:06,419 --> 00:21:09,969
very much

00:21:08,080 --> 00:21:12,190
you know the linear scalability that you

00:21:09,969 --> 00:21:13,719
would expect you know and keep in mind

00:21:12,190 --> 00:21:18,190
that these GPUs are very power hungry

00:21:13,719 --> 00:21:19,419
they're like 300 watts a pop so they're

00:21:18,190 --> 00:21:21,219
they're they're they're not that

00:21:19,419 --> 00:21:23,080
inexpensive so you want to keep them

00:21:21,219 --> 00:21:25,089
busy especially when you multiply them

00:21:23,080 --> 00:21:28,299
by eight in a server you want to be able

00:21:25,089 --> 00:21:30,339
to maximize you know the the GPUs and

00:21:28,299 --> 00:21:33,099
how they're being used so that that's

00:21:30,339 --> 00:21:36,179
actually really critical now we've been

00:21:33,099 --> 00:21:38,769
doing a number of additional

00:21:36,179 --> 00:21:39,989
optimizations and accelerations into

00:21:38,769 --> 00:21:42,789
tensorflow

00:21:39,989 --> 00:21:44,999
advanced verbs is probably not not a

00:21:42,789 --> 00:21:48,509
correct word it they are the same verbs

00:21:44,999 --> 00:21:52,559
so they're just being somewhat optimized

00:21:48,509 --> 00:21:54,669
and and basically what this really shows

00:21:52,559 --> 00:21:58,419
and this is this is the latest

00:21:54,669 --> 00:22:01,229
tensorflow 1.6 which which is not on the

00:21:58,419 --> 00:22:04,359
slide I don't think I apologize for that

00:22:01,229 --> 00:22:06,580
but we're really what the shows is you

00:22:04,359 --> 00:22:08,349
know if you take the network into

00:22:06,580 --> 00:22:10,659
account and you take a look at the at

00:22:08,349 --> 00:22:13,690
the graph on the left here right if you

00:22:10,659 --> 00:22:14,830
were to run a 10 gig network even with

00:22:13,690 --> 00:22:16,659
our DMA and all

00:22:14,830 --> 00:22:20,590
the bells and with offloads that

00:22:16,659 --> 00:22:22,690
Mellanox offers you will completely fall

00:22:20,590 --> 00:22:24,789
short of being able to maximize the

00:22:22,690 --> 00:22:27,970
throughput of what those GPUs are

00:22:24,789 --> 00:22:30,100
capable of right so so 10 gig is just

00:22:27,970 --> 00:22:31,690
not good enough it's just not not enough

00:22:30,100 --> 00:22:35,679
especially when you move into larger

00:22:31,690 --> 00:22:39,309
models this in particular is vgg 16

00:22:35,679 --> 00:22:41,260
right and it not only exploits the

00:22:39,309 --> 00:22:43,179
benefits of already MA in most cases but

00:22:41,260 --> 00:22:45,279
it but it really shows you that the

00:22:43,179 --> 00:22:47,590
network really is a critical critical

00:22:45,279 --> 00:22:51,279
factor in being able to maximize the

00:22:47,590 --> 00:22:53,769
performance of the GPUs so this is a

00:22:51,279 --> 00:22:57,700
pretty awesome slide for IBM has

00:22:53,769 --> 00:22:59,919
introduced what's called power AI and in

00:22:57,700 --> 00:23:02,440
their research of course this has all

00:22:59,919 --> 00:23:05,350
been done on Mellanox infrastructure

00:23:02,440 --> 00:23:08,799
InfiniBand in particular and you know

00:23:05,350 --> 00:23:10,510
that near linear line that would be in

00:23:08,799 --> 00:23:12,850
the perfect world being able to maximize

00:23:10,510 --> 00:23:15,460
all the GPUs at their very peak

00:23:12,850 --> 00:23:17,320
efficiency iBM has been able to achieve

00:23:15,460 --> 00:23:21,100
with the power AI stack

00:23:17,320 --> 00:23:23,169
showing 95% linear scalability all right

00:23:21,100 --> 00:23:25,450
so that's being able to take advantage

00:23:23,169 --> 00:23:30,299
of our DMA and the core capabilities of

00:23:25,450 --> 00:23:32,679
the Mellanox interconnect fabric okay so

00:23:30,299 --> 00:23:35,230
one more last slide I think this is

00:23:32,679 --> 00:23:37,330
another performance slide nickel is

00:23:35,230 --> 00:23:40,059
nvidia x' collective communication

00:23:37,330 --> 00:23:44,980
library they have a new versions called

00:23:40,059 --> 00:23:48,279
nickel 2 and they're also an enabled our

00:23:44,980 --> 00:23:50,200
DMA and GPU direct aware you know out of

00:23:48,279 --> 00:23:54,279
the box and being able to show you know

00:23:50,200 --> 00:23:56,380
pretty pretty awesome performance when

00:23:54,279 --> 00:23:57,970
when you take a look at some of the

00:23:56,380 --> 00:24:03,940
smaller work clothes as well like resin

00:23:57,970 --> 00:24:05,730
at 50 all right so that puts me about

00:24:03,940 --> 00:24:10,330
six minutes ahead of schedule I

00:24:05,730 --> 00:24:12,100
apologize if I went a little fast but

00:24:10,330 --> 00:24:14,470
really what I would like to do is just

00:24:12,100 --> 00:24:17,620
maybe take a last couple of minutes and

00:24:14,470 --> 00:24:20,070
and take any questions I know you got

00:24:17,620 --> 00:24:20,070
questions

00:24:39,840 --> 00:24:45,130
it would it would probably depend so I

00:24:43,240 --> 00:24:48,640
believe the message sighs I'd have to go

00:24:45,130 --> 00:24:53,340
back and check but but it's it's far

00:24:48,640 --> 00:24:55,990
significantly less than than 64 Meg even

00:24:53,340 --> 00:24:57,490
the the current implementation of Sharpe

00:24:55,990 --> 00:25:00,820
we've done a lot of testing to show

00:24:57,490 --> 00:25:03,070
performance in in in the current state

00:25:00,820 --> 00:25:05,410
so for smaller messages and smaller

00:25:03,070 --> 00:25:09,330
workloads around ResNet and these types

00:25:05,410 --> 00:25:14,049
of things it's been just as effective as

00:25:09,330 --> 00:25:16,900
maybe nickel okay but but then that

00:25:14,049 --> 00:25:20,559
changes as as as the as the workload

00:25:16,900 --> 00:25:23,549
changes right so for vgg or much larger

00:25:20,559 --> 00:25:28,270
models that may not always be the case

00:25:23,549 --> 00:25:30,419
now q3 is is when we bring out HDR this

00:25:28,270 --> 00:25:33,460
is when it'll be generally available and

00:25:30,419 --> 00:25:35,790
and and all of that capability like I

00:25:33,460 --> 00:25:40,059
mentioned much much larger message sizes

00:25:35,790 --> 00:25:42,990
will be supported in in the HDR 208

00:25:40,059 --> 00:25:42,990
devices

00:25:48,509 --> 00:25:54,440
you would have to ask them what their

00:25:51,149 --> 00:25:56,759
plans is so what's interesting though

00:25:54,440 --> 00:26:00,210
okay because I can't comment one way or

00:25:56,759 --> 00:26:01,950
the other on the roadmap of coral summit

00:26:00,210 --> 00:26:04,620
in Sierra systems but what is

00:26:01,950 --> 00:26:06,330
interesting about the HDR devices today

00:26:04,620 --> 00:26:09,059
in particular the quantum switch is

00:26:06,330 --> 00:26:13,379
forty ports of 200 gigabytes a second

00:26:09,059 --> 00:26:17,129
okay so today's radix with EDR 100 is 36

00:26:13,379 --> 00:26:18,539
so then we move to a 40 port rate X now

00:26:17,129 --> 00:26:20,279
what's interesting is that you can split

00:26:18,539 --> 00:26:22,950
each one of those ports logically into

00:26:20,279 --> 00:26:24,779
two okay so now you fundamentally have

00:26:22,950 --> 00:26:27,769
an 80 portrayed X for that one device

00:26:24,779 --> 00:26:30,720
being able to do what's called HDR 100

00:26:27,769 --> 00:26:35,190
and and so it's it's possible that

00:26:30,720 --> 00:26:37,649
upgrades could be made because I know

00:26:35,190 --> 00:26:40,529
you have projects coming on online in

00:26:37,649 --> 00:26:42,470
like December or something with with the

00:26:40,529 --> 00:26:47,549
with the latest

00:26:42,470 --> 00:26:49,080
summit right so you know keep me and

00:26:47,549 --> 00:26:52,019
keep me involved let's let's make sure

00:26:49,080 --> 00:26:57,419
you're optimized that's my goal right so

00:26:52,019 --> 00:27:00,179
make sure your data is optimized so

00:26:57,419 --> 00:27:04,379
again you'll have to wait

00:27:00,179 --> 00:27:06,600
now GTC is next week so maybe we can

00:27:04,379 --> 00:27:10,559
talk to Nvidia about the plans around

00:27:06,600 --> 00:27:12,499
Nikhil they're in offline but it would

00:27:10,559 --> 00:27:16,950
seem like an ideal type of marriage

00:27:12,499 --> 00:27:18,509
between the two two technologies yes

00:27:16,950 --> 00:27:19,970
sorry I can't just directly you know

00:27:18,509 --> 00:27:23,659
answer some of your questions

00:27:19,970 --> 00:27:23,659
only because I'm on camera

00:27:32,620 --> 00:27:38,390
so okay so this is a little bit more of

00:27:37,250 --> 00:27:41,290
a technical answer but I have two

00:27:38,390 --> 00:27:47,240
minutes I think I can do it so so today

00:27:41,290 --> 00:27:51,470
EDR is four lanes of 25 gig basically

00:27:47,240 --> 00:27:53,120
right and HDR is four lanes of 50 and so

00:27:51,470 --> 00:27:55,220
in order to be able to increase the

00:27:53,120 --> 00:27:58,220
radix you use a splitter cable so you're

00:27:55,220 --> 00:27:59,870
basically running two ports of 50 now if

00:27:58,220 --> 00:28:04,100
you try to marry a current generation

00:27:59,870 --> 00:28:06,230
EDR HCA an endpoint to the switch on two

00:28:04,100 --> 00:28:08,960
lanes you're going to only marry two of

00:28:06,230 --> 00:28:11,900
them and so fundamentally that will

00:28:08,960 --> 00:28:14,990
actually drop your expected bandwidth

00:28:11,900 --> 00:28:17,150
even even in half again right

00:28:14,990 --> 00:28:21,320
now it's quite possible to plug in a

00:28:17,150 --> 00:28:24,140
switch or to an HCA into an HDR port and

00:28:21,320 --> 00:28:26,030
and the links will negotiate at the

00:28:24,140 --> 00:28:29,030
correct speed but when you're using the

00:28:26,030 --> 00:28:31,520
the splitter technology to to increase

00:28:29,030 --> 00:28:35,330
the radix then you'll have that that

00:28:31,520 --> 00:28:38,720
concern but the core capabilities are

00:28:35,330 --> 00:28:40,070
forwards backwards compatible yeah

00:28:38,720 --> 00:28:42,559
that's what one thing that's interesting

00:28:40,070 --> 00:28:44,840
about Mellanox and InfiniBand and our

00:28:42,559 --> 00:28:46,940
offloads it's it's forward compatible

00:28:44,840 --> 00:28:50,150
and and backwards compatible so we're

00:28:46,940 --> 00:28:52,820
not gonna break anything entirely unless

00:28:50,150 --> 00:28:54,830
you're doing something strange like that

00:28:52,820 --> 00:28:56,630
you know then then you're not then then

00:28:54,830 --> 00:28:57,980
even all the features are there it's

00:28:56,630 --> 00:28:59,360
just the link speed doesn't negotiate

00:28:57,980 --> 00:29:06,040
properly because you don't have all the

00:28:59,360 --> 00:29:06,040

YouTube URL: https://www.youtube.com/watch?v=tKsG807bPGM


