Title: OpenPOWER Summit US 2018: Optimized P9 PCIe for OpenPOWER
Publication date: 2018-04-03
Playlist: OpenPOWER Summit US 2018
Description: 
	Rakesh Sharma discusses optimized P9 PCIe for OpenPOWER. 

For more information, please visit: http://www.openpowerfoundation.org
Captions: 
	00:00:01,100 --> 00:00:10,620
this is my last presentation I guess of

00:00:04,859 --> 00:00:12,660
that summit it's like so good news is

00:00:10,620 --> 00:00:22,850
that you know we have all the time we

00:00:12,660 --> 00:00:27,390
can take know so I was struck by that

00:00:22,850 --> 00:00:32,570
all throughout summit you have seen a

00:00:27,390 --> 00:00:36,270
lot of presentations you know focused on

00:00:32,570 --> 00:00:39,719
you know acceleration power line high

00:00:36,270 --> 00:00:43,440
memory bandwidth we have machine

00:00:39,719 --> 00:00:46,590
learning and you know deep learning and

00:00:43,440 --> 00:00:51,989
lot of most of these applications rely a

00:00:46,590 --> 00:00:56,329
lot on IO part so this my presentation

00:00:51,989 --> 00:01:04,530
is based on introducing to you powerline

00:00:56,329 --> 00:01:07,890
IO infrastructure we do have accelerator

00:01:04,530 --> 00:01:14,340
a specific interfaces which are much

00:01:07,890 --> 00:01:16,159
more you know suited for accelerator

00:01:14,340 --> 00:01:20,490
attachments also some IO

00:01:16,159 --> 00:01:23,220
but the truth of the matter is that vast

00:01:20,490 --> 00:01:26,880
number of devices get attached in the

00:01:23,220 --> 00:01:33,600
system through PCIe so this presentation

00:01:26,880 --> 00:01:35,759
is focused on PCIe attached i/o so I'm

00:01:33,600 --> 00:01:39,650
going to take you give you a kind of

00:01:35,759 --> 00:01:44,850
overview then how we differentiate

00:01:39,650 --> 00:01:53,369
ourselves power 9 against x86 even most

00:01:44,850 --> 00:01:56,820
modern x86 we also it is less covered in

00:01:53,369 --> 00:02:00,380
the presentations that although we have

00:01:56,820 --> 00:02:04,890
PCI Gen 4 which doubles the bandwidth

00:02:00,380 --> 00:02:07,920
still be also improved upon latency of

00:02:04,890 --> 00:02:09,989
i/o transfers on PCI bus so I have some

00:02:07,920 --> 00:02:11,170
data on that I think you'll find it

00:02:09,989 --> 00:02:15,790
interesting

00:02:11,170 --> 00:02:21,270
so I'll start with bit of history we had

00:02:15,790 --> 00:02:24,100
power 7 plus at that time IO bridge was

00:02:21,270 --> 00:02:27,730
actually discrete component with a

00:02:24,100 --> 00:02:31,390
separate chip attaches through memory GX

00:02:27,730 --> 00:02:33,880
bus of the system and then we had PCI

00:02:31,390 --> 00:02:37,840
slots from that and that was PCI gentle

00:02:33,880 --> 00:02:41,140
in power 8 we did two major enhancements

00:02:37,840 --> 00:02:44,800
one is that directly integrating the

00:02:41,140 --> 00:02:47,580
PCIe route complex into the processor

00:02:44,800 --> 00:02:51,280
chip itself that gave a pretty good

00:02:47,580 --> 00:02:52,600
latency and performance boost in also

00:02:51,280 --> 00:02:57,640
PCI gen3

00:02:52,600 --> 00:03:01,810
and cap even Oh God introduced also but

00:02:57,640 --> 00:03:04,810
in power 9 we truly took a big step in

00:03:01,810 --> 00:03:09,280
the i/o space so one is that of course

00:03:04,810 --> 00:03:13,959
we kept directly integrated PCIe into

00:03:09,280 --> 00:03:18,989
the processor chip but we actually have

00:03:13,959 --> 00:03:26,920
leadership PCIe standard with PCI gen 4

00:03:18,989 --> 00:03:30,250
and in addition we also did enhancements

00:03:26,920 --> 00:03:33,280
that improve the latency so cache full

00:03:30,250 --> 00:03:37,360
cache in integration so meaning when a

00:03:33,280 --> 00:03:40,900
small packet is coming in it's it in

00:03:37,360 --> 00:03:44,079
power 8 it will if the cache line is not

00:03:40,900 --> 00:03:46,860
right if it is dirty or it's partial

00:03:44,079 --> 00:03:51,610
things like that then we went through a

00:03:46,860 --> 00:03:54,400
long cycle of you know IO to go into

00:03:51,610 --> 00:03:56,650
memory syncing it up back and forth so

00:03:54,400 --> 00:04:00,820
that introduced quite a bit of latency

00:03:56,650 --> 00:04:04,799
so we have done cash injection which is

00:04:00,820 --> 00:04:04,799
very helpful and we see the results

00:04:04,920 --> 00:04:12,700
so I'll just review for you you know

00:04:08,380 --> 00:04:18,669
with the power 9 io this is a typical I

00:04:12,700 --> 00:04:22,660
picked the IBM machine type 9006 code

00:04:18,669 --> 00:04:25,270
names easy system so it has 2 power 9

00:04:22,660 --> 00:04:28,810
processors and you can see number of

00:04:25,270 --> 00:04:31,210
PCI gen4 slots and these are real PC and

00:04:28,810 --> 00:04:35,500
four slots we also if you need

00:04:31,210 --> 00:04:37,270
expandability we have switch end up and

00:04:35,500 --> 00:04:41,620
you can see in the picture on the right

00:04:37,270 --> 00:04:47,500
bottom so we can get more slots current

00:04:41,620 --> 00:04:50,680
systems have PCI gen3 switch but reaper

00:04:47,500 --> 00:04:55,720
is we will have later on PCI Gen 4

00:04:50,680 --> 00:04:58,720
switches so with PCI Gen 4 you get

00:04:55,720 --> 00:05:01,780
double the bandwidth for the same number

00:04:58,720 --> 00:05:06,760
of lanes because it's a 16 gigabyte per

00:05:01,780 --> 00:05:08,950
second per Lane also as you have seen

00:05:06,760 --> 00:05:11,770
coverage in the summit we have

00:05:08,950 --> 00:05:14,980
high-performance accelerator buses with

00:05:11,770 --> 00:05:19,090
that are constructed using twenty five

00:05:14,980 --> 00:05:22,450
gigabit lanes with any link to o capital

00:05:19,090 --> 00:05:25,480
and was sorry kappa to oh he's actually

00:05:22,450 --> 00:05:27,460
still on PCIe but it still doubles

00:05:25,480 --> 00:05:30,390
because now I've got PCI Gen 4

00:05:27,460 --> 00:05:30,390
underneath

00:05:41,130 --> 00:05:49,620
it's a future enhancement the main

00:05:45,960 --> 00:05:52,200
reason is that it's the number of lanes

00:05:49,620 --> 00:06:05,390
is fewer right now but it will get

00:05:52,200 --> 00:06:08,760
increased yeah that's a good question

00:06:05,390 --> 00:06:13,850
let me get back to you on that but

00:06:08,760 --> 00:06:16,920
because it's it's something in power 9

00:06:13,850 --> 00:06:19,050
think should be possible but there may

00:06:16,920 --> 00:06:27,930
be some limitation so I want to check on

00:06:19,050 --> 00:06:30,120
that okay so as I said that we also

00:06:27,930 --> 00:06:33,740
improved the latency part and I have

00:06:30,120 --> 00:06:42,050
some data to share with you on that part

00:06:33,740 --> 00:06:45,360
in addition to bringing the i/o

00:06:42,050 --> 00:06:48,480
infrastructure improvements we also

00:06:45,360 --> 00:06:52,290
working with partners to bring you know

00:06:48,480 --> 00:06:58,250
higher speed I or adapter supported and

00:06:52,290 --> 00:07:04,140
we are currently shipping a 100 gigabit

00:06:58,250 --> 00:07:07,920
40 20 22 10 gigabit InfiniBand

00:07:04,140 --> 00:07:10,980
and ethernet adapters these are our DMA

00:07:07,920 --> 00:07:16,260
capable and this is Magnus connect x5

00:07:10,980 --> 00:07:18,690
and this is actually a PCI gen 4 truly

00:07:16,260 --> 00:07:22,490
PC agent for adapter and I have a use

00:07:18,690 --> 00:07:22,490
case coming up I'll show you in a minute

00:07:22,790 --> 00:07:29,640
but the good news is that PCI Gen 4

00:07:26,450 --> 00:07:31,890
ecosystem is growing very fast we have

00:07:29,640 --> 00:07:34,710
partners already working on such as

00:07:31,890 --> 00:07:37,440
caveum it's working on PCI gen 4 we

00:07:34,710 --> 00:07:40,050
working very closely to get the pci gen

00:07:37,440 --> 00:07:42,390
for networking adapters that can do up

00:07:40,050 --> 00:07:45,600
to 200 gigabit per second you know

00:07:42,390 --> 00:07:46,289
introduced quickly similarly be working

00:07:45,600 --> 00:07:50,460
with broad

00:07:46,289 --> 00:07:53,550
to Vietnam again more PCI gen4 adapters

00:07:50,460 --> 00:07:58,589
you know introduced on storage side and

00:07:53,550 --> 00:08:01,529
networking side in addition we also have

00:07:58,589 --> 00:08:04,259
working with partners to get PCI Gen 4

00:08:01,529 --> 00:08:08,219
switch so that gives us more fan-out

00:08:04,259 --> 00:08:11,879
more options to attach in one project we

00:08:08,219 --> 00:08:15,659
are working on is to have a large number

00:08:11,879 --> 00:08:20,539
of nvme devices connected of using PCI

00:08:15,659 --> 00:08:20,539
Gen 4 switch to the system without

00:08:20,990 --> 00:08:28,469
oversubscription because in industry you

00:08:25,139 --> 00:08:31,080
will see systems with 24 nbme device but

00:08:28,469 --> 00:08:37,349
when you dig down you see actually it's

00:08:31,080 --> 00:08:39,659
not 24 nvme worth of bandwidth there you

00:08:37,349 --> 00:08:41,250
also have 32 gigabit fiber channel we

00:08:39,659 --> 00:08:45,839
actually released it you know it's

00:08:41,250 --> 00:08:48,750
available other sort of peek came up

00:08:45,839 --> 00:08:51,540
with this new design point for our power

00:08:48,750 --> 00:08:55,260
enterprise server is embedding and via

00:08:51,540 --> 00:08:58,199
me devices in the planer itself very

00:08:55,260 --> 00:08:59,600
close to planar and devices so those are

00:08:58,199 --> 00:09:02,970
available

00:08:59,600 --> 00:09:07,230
capi to a flash and and milling in flash

00:09:02,970 --> 00:09:09,120
gt+ and Volta card so we have a device

00:09:07,230 --> 00:09:14,639
is taking advantage of this

00:09:09,120 --> 00:09:20,940
infrastructure so this is the demo we

00:09:14,639 --> 00:09:26,899
did on for flash summit and in this one

00:09:20,940 --> 00:09:32,850
we had power 9 servers acting as a

00:09:26,899 --> 00:09:38,370
target server in initiator using nvme

00:09:32,850 --> 00:09:43,860
devices but over 100 Gigabit Ethernet

00:09:38,370 --> 00:09:48,029
and PCI gen for networking adapters on

00:09:43,860 --> 00:09:52,529
the both side so we had to put in 2 PCI

00:09:48,029 --> 00:09:57,329
gen3 by 8 devices to fill the bandwidth

00:09:52,529 --> 00:10:00,720
supported by PCI Gen 4 by 8 1 1 adapter

00:09:57,329 --> 00:10:03,899
so it kind of shows the advantage of

00:10:00,720 --> 00:10:06,329
that and this actually we can be

00:10:03,899 --> 00:10:09,000
measured it we got the full bandwidth

00:10:06,329 --> 00:10:12,630
you know hundred gigabit per second with

00:10:09,000 --> 00:10:18,750
that so like a real use case and you can

00:10:12,630 --> 00:10:22,769
see you know PC agent for helps alone ok

00:10:18,750 --> 00:10:28,740
let me talk about latency a little bit

00:10:22,769 --> 00:10:32,910
so as I said we improved latency and we

00:10:28,740 --> 00:10:37,829
wanted to test it out closely to see how

00:10:32,910 --> 00:10:40,620
competitive are we with against x86 so

00:10:37,829 --> 00:10:44,010
we wanted to make sure we do a very fair

00:10:40,620 --> 00:10:47,699
comparison so for that reason we had a

00:10:44,010 --> 00:10:51,720
fpga adapter and what that adapter will

00:10:47,699 --> 00:10:53,939
do is basically turn around data from

00:10:51,720 --> 00:10:56,670
the host to that so we get both

00:10:53,939 --> 00:11:00,300
direction very simple workload because

00:10:56,670 --> 00:11:04,800
we are focused on just a PCI latency

00:11:00,300 --> 00:11:09,360
part of it and basically it simulates

00:11:04,800 --> 00:11:13,350
the communication over the bus and we

00:11:09,360 --> 00:11:15,300
didn't want to rely on the clock of the

00:11:13,350 --> 00:11:18,209
CPU or anything because there are always

00:11:15,300 --> 00:11:20,970
variations so we actually use PCI

00:11:18,209 --> 00:11:24,510
analyzer tries to see when the transfer

00:11:20,970 --> 00:11:27,839
is actually occurring in response times

00:11:24,510 --> 00:11:31,290
in statistics or mansard so so anyway so

00:11:27,839 --> 00:11:34,889
this card was used so then we did the

00:11:31,290 --> 00:11:39,870
numbers comparison on power and line we

00:11:34,889 --> 00:11:41,579
saw that you know our transfer CPU

00:11:39,870 --> 00:11:44,220
turnaround time is like three hundred

00:11:41,579 --> 00:11:47,100
thirty seven point three nine o second

00:11:44,220 --> 00:11:49,110
so I want you to focus on two ways you

00:11:47,100 --> 00:11:53,069
know first of all it is better than best

00:11:49,110 --> 00:11:55,319
on x86 from KB Lake but more importantly

00:11:53,069 --> 00:11:58,079
just see the how the low this number is

00:11:55,319 --> 00:12:00,059
so you can see the bus transaction you

00:11:58,079 --> 00:12:02,370
are not consuming much latency as long

00:12:00,059 --> 00:12:05,130
as you're hitting the cache it comes out

00:12:02,370 --> 00:12:08,459
very good the other bigger number point

00:12:05,130 --> 00:12:10,529
is that they you see our standard

00:12:08,459 --> 00:12:13,439
deviation is very low

00:12:10,529 --> 00:12:18,720
so we we don't kind of you know change

00:12:13,439 --> 00:12:21,689
over a lot of things um so the question

00:12:18,720 --> 00:12:25,860
is once you have this low latency what

00:12:21,689 --> 00:12:28,110
are the use cases and as I was saying

00:12:25,860 --> 00:12:32,670
there are lots and lots of possible use

00:12:28,110 --> 00:12:36,660
cases but having low latency networking

00:12:32,670 --> 00:12:39,420
can give you lots of benefits by the way

00:12:36,660 --> 00:12:43,649
you will get the lowest latency if you

00:12:39,420 --> 00:12:45,449
use protocols like our DMA protocol

00:12:43,649 --> 00:12:47,670
because that's our kind of but

00:12:45,449 --> 00:12:49,350
unfortunately those are not as

00:12:47,670 --> 00:12:52,319
interoperable you have to have a same

00:12:49,350 --> 00:12:55,680
protocol both ends not as widely

00:12:52,319 --> 00:12:58,050
deployed interfaces are complex most of

00:12:55,680 --> 00:13:02,730
the industry runs over tcp/ip networking

00:12:58,050 --> 00:13:07,470
so even there we benefit from it but the

00:13:02,730 --> 00:13:10,009
case we did was we are as you saw

00:13:07,470 --> 00:13:14,730
earlier we have lots of high performance

00:13:10,009 --> 00:13:19,259
databases GPU you know in-memory

00:13:14,730 --> 00:13:22,230
databases you know so we used to have

00:13:19,259 --> 00:13:26,939
sort of network latency didn't used to

00:13:22,230 --> 00:13:32,670
matter because the IO was slow but with

00:13:26,939 --> 00:13:39,300
that these high-performance databases we

00:13:32,670 --> 00:13:40,889
have high performance number of

00:13:39,300 --> 00:13:44,819
transactions they can support is very

00:13:40,889 --> 00:13:48,059
large so given that we need good latency

00:13:44,819 --> 00:13:50,490
in the network side also so one power

00:13:48,059 --> 00:13:53,879
line especially as again as you saw

00:13:50,490 --> 00:13:56,040
earlier we have lots of success with the

00:13:53,879 --> 00:13:58,620
high performance databases that are

00:13:56,040 --> 00:14:01,850
accelerated because we have a high

00:13:58,620 --> 00:14:06,720
memory bandwidth we have accelerator

00:14:01,850 --> 00:14:10,050
attachments which is world class and so

00:14:06,720 --> 00:14:13,249
basically we have a ideal infrastructure

00:14:10,050 --> 00:14:18,269
an examples are radius memcache

00:14:13,249 --> 00:14:21,550
t-connector car near 4j we had Mattie

00:14:18,269 --> 00:14:24,009
and basically lots of databases are

00:14:21,550 --> 00:14:29,079
that could use with high transaction

00:14:24,009 --> 00:14:31,899
rate so we do show we can increase

00:14:29,079 --> 00:14:34,990
throughput by using low latency

00:14:31,899 --> 00:14:36,910
networking now you can do low latency

00:14:34,990 --> 00:14:38,730
networking with TCP IP number of

00:14:36,910 --> 00:14:42,009
different ways just have a very tuned

00:14:38,730 --> 00:14:46,720
stack you know a lot of acceleration you

00:14:42,009 --> 00:14:49,959
can do there are specialized stack there

00:14:46,720 --> 00:14:52,899
is open source DP DK there is solar

00:14:49,959 --> 00:14:56,410
flare open unload so the one we tested

00:14:52,899 --> 00:14:58,360
with is open on load this work is sort

00:14:56,410 --> 00:15:02,110
of just a starting so a very initial

00:14:58,360 --> 00:15:04,389
data still it was fairly good I want to

00:15:02,110 --> 00:15:07,360
show you that so just a kind of a little

00:15:04,389 --> 00:15:09,639
bit background this open on load stack

00:15:07,360 --> 00:15:14,410
is actually very straightforward rather

00:15:09,639 --> 00:15:21,610
than using kernel based tcp/ip it uses

00:15:14,410 --> 00:15:23,769
user space tcp/ip and it's it still

00:15:21,610 --> 00:15:26,620
maintains the sockets in our face that's

00:15:23,769 --> 00:15:29,949
the claim to the fame that you can take

00:15:26,620 --> 00:15:31,959
same application and just run it in open

00:15:29,949 --> 00:15:33,880
our load is stack and you don't have to

00:15:31,959 --> 00:15:37,930
make changes and this is for sockets

00:15:33,880 --> 00:15:39,699
applications so this way and it doesn't

00:15:37,930 --> 00:15:42,310
have to be running on the both hands you

00:15:39,699 --> 00:15:44,970
could be server side could be this and

00:15:42,310 --> 00:15:49,660
the other side could be standard tcp/ip

00:15:44,970 --> 00:15:53,110
so kind of good good use case for doing

00:15:49,660 --> 00:15:57,269
the latency so when we did this test we

00:15:53,110 --> 00:16:02,050
it's literally out of box running on p8

00:15:57,269 --> 00:16:04,120
and you can see we got 50 percent

00:16:02,050 --> 00:16:07,269
improvement on number of transactions

00:16:04,120 --> 00:16:10,269
and with good reason because the typical

00:16:07,269 --> 00:16:14,170
latency using this kind of technique is

00:16:10,269 --> 00:16:21,370
of the order of like 2 3 microsecond and

00:16:14,170 --> 00:16:23,740
we compared against Intel adapter 10

00:16:21,370 --> 00:16:26,949
gigabit adapters but the difference

00:16:23,740 --> 00:16:29,139
being open or load versus that and we

00:16:26,949 --> 00:16:31,300
got 50 percent transaction improvement

00:16:29,139 --> 00:16:31,779
that's pretty big deal especially in

00:16:31,300 --> 00:16:35,709
data

00:16:31,779 --> 00:16:38,560
world so so anyway so those are this is

00:16:35,709 --> 00:16:40,629
just one example of you know he once he

00:16:38,560 --> 00:16:43,180
a good latency what you can do but you

00:16:40,629 --> 00:16:47,139
can do you know many other things

00:16:43,180 --> 00:16:50,589
whether it's a GPU scaling using your

00:16:47,139 --> 00:16:54,129
 award or tensorflow you know when

00:16:50,589 --> 00:16:57,579
you have you can take advantage of it in

00:16:54,129 --> 00:16:59,740
your HPC applications of course so so we

00:16:57,579 --> 00:17:01,870
are pretty excited about having that low

00:16:59,740 --> 00:17:06,850
latency also now we have a competitive

00:17:01,870 --> 00:17:14,730
one so that's what I wanted to cover are

00:17:06,850 --> 00:17:14,730
there any questions ok ok thank you guys

00:17:26,829 --> 00:17:33,660
current technology that's available from

00:17:30,270 --> 00:17:38,410
partners is actually gen3

00:17:33,660 --> 00:17:40,750
but again we working with partners can

00:17:38,410 --> 00:17:43,450
disclose and all because of their you

00:17:40,750 --> 00:17:48,310
know confidentiality that are going to

00:17:43,450 --> 00:17:50,770
be gentle yeah that's of each other one

00:17:48,310 --> 00:17:53,140
thing I that does remind me that caveum

00:17:50,770 --> 00:17:56,800
has a kind of solution called nvme

00:17:53,140 --> 00:18:01,480
direct right pitch kinda can take the

00:17:56,800 --> 00:18:05,620
data directly from the network in kernel

00:18:01,480 --> 00:18:10,170
transfer to to the nvme so that gives

00:18:05,620 --> 00:18:10,170

YouTube URL: https://www.youtube.com/watch?v=cYoLBOQ2ucs


