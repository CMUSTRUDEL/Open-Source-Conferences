Title: OpenPOWER Summit US 2018: Learning at Leadership Scale
Publication date: 2018-04-05
Playlist: OpenPOWER Summit US 2018
Description: 
	Fernanda Foertter, Arjun Shankur and Jack Wells of Oak Ridge National Laboratory discuss learning leadership at scale at the OpenPOWER Summit 2018. 

For more information, please visit: http://www.openpowerfoundation.org
Captions: 
	00:00:00,000 --> 00:00:08,400
I'm Jack wells from crazy National Lab

00:00:03,830 --> 00:00:12,269
I'm gonna give a talk here learning

00:00:08,400 --> 00:00:13,889
through leadership scale computing my

00:00:12,269 --> 00:00:16,199
collaborators and putting this together

00:00:13,889 --> 00:00:18,720
or Fernanda Ford or who we wrote the

00:00:16,199 --> 00:00:22,320
abstract was with us but she now works

00:00:18,720 --> 00:00:24,900
for NVIDIA it's a bad trend and Arjun

00:00:22,320 --> 00:00:27,590
Shankar and Jack wells from Oak Ridge

00:00:24,900 --> 00:00:31,679
National Laboratory it's not sustainable

00:00:27,590 --> 00:00:33,390
so what is machine learning machine

00:00:31,679 --> 00:00:35,309
learning is a type of artificial

00:00:33,390 --> 00:00:37,200
intelligence that provides computers

00:00:35,309 --> 00:00:41,010
with the ability to learn without being

00:00:37,200 --> 00:00:42,660
explicitly programmed and machine

00:00:41,010 --> 00:00:44,219
learning focuses on the development of

00:00:42,660 --> 00:00:46,500
computer programs that can change when

00:00:44,219 --> 00:00:47,670
exposed to new data the process of

00:00:46,500 --> 00:00:50,820
machine learning is similar to that of

00:00:47,670 --> 00:00:53,250
data mining both systems look through

00:00:50,820 --> 00:00:54,809
data for patterns but instead of

00:00:53,250 --> 00:00:57,180
extracting the data for human

00:00:54,809 --> 00:00:59,550
comprehension it is the case of data

00:00:57,180 --> 00:01:01,890
mining as in data mining machine

00:00:59,550 --> 00:01:03,780
learning uses data that detects patterns

00:01:01,890 --> 00:01:05,970
and data and just programmed actions

00:01:03,780 --> 00:01:07,890
accordingly machine learning algorithms

00:01:05,970 --> 00:01:10,770
are often categorized as means

00:01:07,890 --> 00:01:12,270
supervised or unsupervised supervisors

00:01:10,770 --> 00:01:13,650
algorithms can't apply what has been

00:01:12,270 --> 00:01:16,439
learned from the past a new data

00:01:13,650 --> 00:01:19,290
unsupervised can draw inferences from

00:01:16,439 --> 00:01:21,990
data sets so what is leadership

00:01:19,290 --> 00:01:24,689
computing I define this this morning if

00:01:21,990 --> 00:01:28,140
you were in the keynote but basically it

00:01:24,689 --> 00:01:31,100
means computing that is capability

00:01:28,140 --> 00:01:32,840
limited that you need some capability

00:01:31,100 --> 00:01:34,799
architectural feature of our

00:01:32,840 --> 00:01:37,950
supercomputing Center that you don't

00:01:34,799 --> 00:01:39,869
have enough of at your your Center and

00:01:37,950 --> 00:01:42,540
so you in order to do something unique

00:01:39,869 --> 00:01:45,750
you need more of it be it processing

00:01:42,540 --> 00:01:47,310
power or memory or bandwidth etc and it

00:01:45,750 --> 00:01:49,350
goes along with of what we call

00:01:47,310 --> 00:01:51,360
leadership scale computing where we

00:01:49,350 --> 00:01:54,270
basically mean we want to provide the

00:01:51,360 --> 00:01:56,369
largest compute and data infrastructure

00:01:54,270 --> 00:02:01,229
for solving the most challenging

00:01:56,369 --> 00:02:04,140
problems and so what I'm going to

00:02:01,229 --> 00:02:07,079
present here in the talk concerns the

00:02:04,140 --> 00:02:09,209
following of three points I'm going to

00:02:07,079 --> 00:02:11,250
discuss science requirements gathering

00:02:09,209 --> 00:02:13,709
that we've done over the last couple of

00:02:11,250 --> 00:02:17,790
years where we've seen the straw

00:02:13,709 --> 00:02:20,879
growth in data analytics and artificial

00:02:17,790 --> 00:02:24,299
intelligence type applications within

00:02:20,879 --> 00:02:26,459
our stakeholder base and how that's been

00:02:24,299 --> 00:02:28,920
documented and how we're trying to

00:02:26,459 --> 00:02:30,810
respond to that I'll give a brief

00:02:28,920 --> 00:02:33,060
introduction to summit it's already been

00:02:30,810 --> 00:02:36,900
introduced and then I'll go deeper on

00:02:33,060 --> 00:02:38,790
four case studies actually this same

00:02:36,900 --> 00:02:42,239
four case studies I outlined this

00:02:38,790 --> 00:02:45,180
morning so experimental and

00:02:42,239 --> 00:02:47,609
observational data science is exploding

00:02:45,180 --> 00:02:49,319
within do we office of science there's a

00:02:47,609 --> 00:02:53,340
vast array of experimental and

00:02:49,319 --> 00:02:56,280
observational user facilities and they

00:02:53,340 --> 00:02:59,489
are producing more higher rate and

00:02:56,280 --> 00:03:04,049
multi-dimensional data sets that are

00:02:59,489 --> 00:03:06,290
pushing experimental computing off of

00:03:04,049 --> 00:03:10,760
the desktop and into the data center

00:03:06,290 --> 00:03:13,919
this is a workshop that was organized in

00:03:10,760 --> 00:03:15,900
2015 by a Lusine old who's a program

00:03:13,919 --> 00:03:20,190
manager and do a Office of Science and

00:03:15,900 --> 00:03:22,739
to me it was one of the first do a wide

00:03:20,190 --> 00:03:25,680
efforts to document these requirements

00:03:22,739 --> 00:03:28,229
from compute computing requirements and

00:03:25,680 --> 00:03:31,919
experimental observational science in

00:03:28,229 --> 00:03:34,979
the context of our scientific computing

00:03:31,919 --> 00:03:37,019
enterprise this is a slide that was

00:03:34,979 --> 00:03:39,120
contributed to that workshop from one of

00:03:37,019 --> 00:03:41,819
my collaborators at Oak Ridge Sergei

00:03:39,120 --> 00:03:43,590
Clennon he is the director of the

00:03:41,819 --> 00:03:45,419
Institute for functional imaging and

00:03:43,590 --> 00:03:47,669
materials at the Center for Nana phase

00:03:45,419 --> 00:03:50,060
material sciences and it's a summary

00:03:47,669 --> 00:03:53,609
chart from one of his publications that

00:03:50,060 --> 00:03:55,250
documents a variety of techniques that

00:03:53,609 --> 00:03:59,989
have been developed in his laboratory

00:03:55,250 --> 00:04:03,530
for scanning probe microscopy and the

00:03:59,989 --> 00:04:06,449
details of this chart are not what so

00:04:03,530 --> 00:04:09,120
what I want to communicate but it's

00:04:06,449 --> 00:04:12,540
rather that over about a ten year period

00:04:09,120 --> 00:04:14,729
because of the innovations within his

00:04:12,540 --> 00:04:18,930
laboratory and putting multiple sensors

00:04:14,729 --> 00:04:23,460
to extract new channels of information

00:04:18,930 --> 00:04:26,969
from the experimental process his data

00:04:23,460 --> 00:04:27,840
requirements went from tens of megabytes

00:04:26,969 --> 00:04:31,440
to

00:04:27,840 --> 00:04:36,000
hundreds of gigabytes right and this is

00:04:31,440 --> 00:04:39,840
in a one room in one nanoscience center

00:04:36,000 --> 00:04:42,060
at one action laboratory so the

00:04:39,840 --> 00:04:44,310
dimensionality increase here it just

00:04:42,060 --> 00:04:47,970
drives the data requirements very

00:04:44,310 --> 00:04:50,690
strongly of course there's a lot of new

00:04:47,970 --> 00:04:53,310
here for experimental and observational

00:04:50,690 --> 00:04:55,680
scientists who in the past have used

00:04:53,310 --> 00:04:58,320
homegrown techniques for data management

00:04:55,680 --> 00:05:01,590
home homegrown techniques for data

00:04:58,320 --> 00:05:03,620
analysis they quickly were swimming over

00:05:01,590 --> 00:05:06,150
their head even some of the best groups

00:05:03,620 --> 00:05:08,400
and so they were trying to understand

00:05:06,150 --> 00:05:10,200
how to respond to this and this was sort

00:05:08,400 --> 00:05:12,800
of his contribution to this workshop I

00:05:10,200 --> 00:05:17,220
referenced about we we need help we need

00:05:12,800 --> 00:05:19,080
more hardened more systematic better

00:05:17,220 --> 00:05:19,970
founded techniques for dealing with this

00:05:19,080 --> 00:05:23,550
data

00:05:19,970 --> 00:05:26,760
following up with that our sponsor asked

00:05:23,550 --> 00:05:29,550
the deal.we supercomputing centers in

00:05:26,760 --> 00:05:31,320
the Office of Science being a Lawrence

00:05:29,550 --> 00:05:33,450
Berkeley National Laboratory Argonne

00:05:31,320 --> 00:05:35,610
National Laboratory in Oak Ridge to

00:05:33,450 --> 00:05:38,280
develop a series of exascale

00:05:35,610 --> 00:05:41,820
requirements workshops and over a

00:05:38,280 --> 00:05:43,979
two-year period we did six workshops

00:05:41,820 --> 00:05:46,260
each with one of the six program offices

00:05:43,979 --> 00:05:48,479
within do-e and then we summarized that

00:05:46,260 --> 00:05:50,310
with a cross-cutting workshop the goals

00:05:48,479 --> 00:05:51,660
were to identify the mission science

00:05:50,310 --> 00:05:53,729
objectives that require advanced

00:05:51,660 --> 00:05:56,099
scientific computing stores networking

00:05:53,729 --> 00:05:57,960
at the exascale timeframe and determine

00:05:56,099 --> 00:06:00,450
the future requirements for a computing

00:05:57,960 --> 00:06:03,599
ecosystem that included data software

00:06:00,450 --> 00:06:05,810
tools and libraries and one of the

00:06:03,599 --> 00:06:09,110
themes of theme relevant to this talk is

00:06:05,810 --> 00:06:11,099
large-scale data storage and analysis Oh

00:06:09,110 --> 00:06:12,930
high-level summaries were that

00:06:11,099 --> 00:06:14,880
experimental and simulated data set

00:06:12,930 --> 00:06:17,849
volumes were growing exponentially in

00:06:14,880 --> 00:06:21,330
the scientific domains examples were the

00:06:17,849 --> 00:06:23,700
the high luminosity runs expected at the

00:06:21,330 --> 00:06:26,550
Large Hadron Collider at sarin light

00:06:23,700 --> 00:06:29,639
sources like those that are built and

00:06:26,550 --> 00:06:31,950
being extended at the Stanford national

00:06:29,639 --> 00:06:34,440
accelerator facility climate science

00:06:31,950 --> 00:06:36,780
which is multimodal cosmology data all

00:06:34,440 --> 00:06:39,479
these data sets are or will soon be at

00:06:36,780 --> 00:06:40,990
hundreds of petabytes and we don't have

00:06:39,479 --> 00:06:43,770
the capability

00:06:40,990 --> 00:06:46,360
to process all these today and that

00:06:43,770 --> 00:06:48,310
likewise methods and workflows for data

00:06:46,360 --> 00:06:51,760
analytics are much different from those

00:06:48,310 --> 00:06:54,730
in traditional HPC that are built on the

00:06:51,760 --> 00:06:58,000
foundation of continuum based analytics

00:06:54,730 --> 00:07:00,130
and partial differential equations etc

00:06:58,000 --> 00:07:03,490
and that the machine learning is

00:07:00,130 --> 00:07:08,680
revolution our our field this

00:07:03,490 --> 00:07:09,400
established analysis program they still

00:07:08,680 --> 00:07:11,980
hanging in there

00:07:09,400 --> 00:07:14,290
so from our crosscut report there are

00:07:11,980 --> 00:07:16,960
these couple of summary statements a

00:07:14,290 --> 00:07:18,790
performing analysis of big data sets and

00:07:16,960 --> 00:07:20,860
drawing inferences based on these data

00:07:18,790 --> 00:07:22,810
are revolutionized in many fields and

00:07:20,860 --> 00:07:24,370
new approaches are needed for analyzing

00:07:22,810 --> 00:07:26,650
large data sets included in advanced

00:07:24,370 --> 00:07:28,510
statistics and machine learning and on

00:07:26,650 --> 00:07:31,870
the software and application development

00:07:28,510 --> 00:07:34,140
side scalable data analysis process in

00:07:31,870 --> 00:07:36,870
data analysis machine learning discrete

00:07:34,140 --> 00:07:39,130
and understanding these large data

00:07:36,870 --> 00:07:42,550
large-scale data that will be produced

00:07:39,130 --> 00:07:46,300
by exascale systems is a new driver for

00:07:42,550 --> 00:07:49,300
our our infrastructure the one that we

00:07:46,300 --> 00:07:52,390
we need to build out and this is new

00:07:49,300 --> 00:07:54,700
this wasn't a requirement really for

00:07:52,390 --> 00:07:56,680
example when we wrote the procurement

00:07:54,700 --> 00:07:59,050
for Jaguar when we wrote the procurement

00:07:56,680 --> 00:08:03,370
for for Titan or even really when we

00:07:59,050 --> 00:08:07,150
wrote the procurement for summon this is

00:08:03,370 --> 00:08:09,340
relatively new I would say in 2014 when

00:08:07,150 --> 00:08:12,910
we signed the contract with our vendor

00:08:09,340 --> 00:08:14,830
partners for Summit we didn't have a lot

00:08:12,910 --> 00:08:17,380
of users in this space but I just made a

00:08:14,830 --> 00:08:21,940
list of a number of user projects with

00:08:17,380 --> 00:08:23,760
PI's that have been awarded projects at

00:08:21,940 --> 00:08:27,820
our Center over the last couple of years

00:08:23,760 --> 00:08:31,510
and it it's been a rapidly growing

00:08:27,820 --> 00:08:34,450
activity scientists are coming to us

00:08:31,510 --> 00:08:37,080
because we have a large gpu-accelerated

00:08:34,450 --> 00:08:40,210
cluster now because they know that

00:08:37,080 --> 00:08:42,909
summits coming with even more data

00:08:40,210 --> 00:08:46,790
analysis capabilities that

00:08:42,909 --> 00:08:50,240
this is a place where this kind of

00:08:46,790 --> 00:08:52,579
analysis will need to happen so

00:08:50,240 --> 00:08:55,850
someone's coming it's slated to be

00:08:52,579 --> 00:08:59,269
powerful and smart a supercomputer and

00:08:55,850 --> 00:09:02,420
for open science as I said this morning

00:08:59,269 --> 00:09:04,309
it's it's smart the architecture is

00:09:02,420 --> 00:09:07,189
expected to be smart in the sense that

00:09:04,309 --> 00:09:09,949
it has the the graphics processing brawn

00:09:07,189 --> 00:09:13,850
high speed data movement and has a

00:09:09,949 --> 00:09:18,170
memory where it matters here's a note

00:09:13,850 --> 00:09:20,800
overview and just from our advanced data

00:09:18,170 --> 00:09:23,149
and workflow group our early preliminary

00:09:20,800 --> 00:09:25,579
observations on running these workflows

00:09:23,149 --> 00:09:28,579
attempting to run these workflows on on

00:09:25,579 --> 00:09:31,220
the summit architecture that the deep

00:09:28,579 --> 00:09:35,209
learning codes like convolutional neural

00:09:31,220 --> 00:09:39,949
networks resident 50 excel on the GPUs

00:09:35,209 --> 00:09:42,410
and with the non-volatile memory are

00:09:39,949 --> 00:09:45,920
enabling the tensor operations very

00:09:42,410 --> 00:09:47,540
efficiently and pre loading the data on

00:09:45,920 --> 00:09:50,300
the non-volatile memory shows

00:09:47,540 --> 00:09:52,819
near-perfect scaling for a preliminary

00:09:50,300 --> 00:09:55,550
data sets on resident through about 64

00:09:52,819 --> 00:09:58,009
nodes it's early days but these are some

00:09:55,550 --> 00:10:01,009
of our early experiences and then

00:09:58,009 --> 00:10:03,499
likewise on sort of the middle part of

00:10:01,009 --> 00:10:06,110
that diagram traditional principal

00:10:03,499 --> 00:10:09,559
component analysis k-means etc are

00:10:06,110 --> 00:10:12,429
excelling due to the nodes memory the

00:10:09,559 --> 00:10:15,350
the very strong CPU and the on chip

00:10:12,429 --> 00:10:17,809
bandwidth so things are looking very

00:10:15,350 --> 00:10:19,639
good in the early days and most of the

00:10:17,809 --> 00:10:24,259
energy is going to scaling up the

00:10:19,639 --> 00:10:27,079
frameworks to scale out the frameworks

00:10:24,259 --> 00:10:28,999
to use more and more of the

00:10:27,079 --> 00:10:33,049
infrastructure we have on individual

00:10:28,999 --> 00:10:35,449
applications so in the remainder of this

00:10:33,049 --> 00:10:39,170
talk I want to just do a little bit of a

00:10:35,449 --> 00:10:42,309
dive on each of these applications that

00:10:39,170 --> 00:10:46,639
are challenges for a smart supercomputer

00:10:42,309 --> 00:10:49,069
in materials the example I've already

00:10:46,639 --> 00:10:52,069
started is multimodal characterization

00:10:49,069 --> 00:10:53,660
of materials novel microscopic and

00:10:52,069 --> 00:10:56,209
spectroscopic techniques allow the

00:10:53,660 --> 00:10:56,390
characterization of different aspects of

00:10:56,209 --> 00:10:59,000
the

00:10:56,390 --> 00:11:00,680
materials at the nanoscale and the

00:10:59,000 --> 00:11:03,650
associated data analysis is very

00:11:00,680 --> 00:11:06,020
difficult and outcomes are big data up

00:11:03,650 --> 00:11:08,630
to terabytes and it's multi-dimensional

00:11:06,020 --> 00:11:10,550
and the combination of the microscopic

00:11:08,630 --> 00:11:13,070
and the spectroscopic techniques is what

00:11:10,550 --> 00:11:15,380
is really mean by multimodal is required

00:11:13,070 --> 00:11:17,840
for the comprehension characterization

00:11:15,380 --> 00:11:19,820
of materials if we had been thoughtful

00:11:17,840 --> 00:11:21,800
enough about this it was inevitable that

00:11:19,820 --> 00:11:25,010
this problem would have presented itself

00:11:21,800 --> 00:11:27,470
because the experiments have always said

00:11:25,010 --> 00:11:31,520
that we need to understand the structure

00:11:27,470 --> 00:11:32,990
of materials to then move on to to the

00:11:31,520 --> 00:11:35,210
function of materials and the

00:11:32,990 --> 00:11:36,860
performance of materials and then beyond

00:11:35,210 --> 00:11:39,710
the performance to be able to design

00:11:36,860 --> 00:11:43,400
materials so the kinds of information

00:11:39,710 --> 00:11:45,110
you need from structure to function to

00:11:43,400 --> 00:11:49,730
overall

00:11:45,110 --> 00:11:53,870
and then on to design observation data

00:11:49,730 --> 00:11:56,630
analytics integrated so for example a

00:11:53,870 --> 00:11:58,750
secondary ion mass spectroscopy can be

00:11:56,630 --> 00:12:01,790
combined with atomic force microscopy

00:11:58,750 --> 00:12:03,170
enabling a correlated characterization

00:12:01,790 --> 00:12:06,280
of the functional response at the

00:12:03,170 --> 00:12:09,280
nanoscale and that can be

00:12:06,280 --> 00:12:12,040
or composition information optical

00:12:09,280 --> 00:12:13,240
spectroscopy can be added for the

00:12:12,040 --> 00:12:15,070
characterization of the optical

00:12:13,240 --> 00:12:17,760
properties and studied at the same time

00:12:15,070 --> 00:12:20,590
buying sample crystal crystal grass

00:12:17,760 --> 00:12:22,270
increasing in dimensionality so analysis

00:12:20,590 --> 00:12:23,530
of multimodal data is even more

00:12:22,270 --> 00:12:25,810
complicated because of this

00:12:23,530 --> 00:12:27,640
dimensionality and it requires new

00:12:25,810 --> 00:12:30,070
infrastructure it also requires new

00:12:27,640 --> 00:12:32,260
mathematical methods so here's just a

00:12:30,070 --> 00:12:38,410
chart that tries to show how on the

00:12:32,260 --> 00:12:40,540
visual material these kinds of sensors

00:12:38,410 --> 00:12:44,550
can be integrated to produce in

00:12:40,540 --> 00:12:44,550
dimensional datasets

00:12:45,750 --> 00:12:53,440
okay moving on to the fusion application

00:12:49,830 --> 00:12:55,780
so deep learning can be used to predict

00:12:53,440 --> 00:12:57,880
plasma disruptions in a tokamak reactor

00:12:55,780 --> 00:13:00,610
the most critical problem for fusion

00:12:57,880 --> 00:13:03,010
energy you can say is how to avoid or

00:13:00,610 --> 00:13:07,120
mitigate large-scale major disruptions

00:13:03,010 --> 00:13:09,340
that is that your the energy of your of

00:13:07,120 --> 00:13:13,560
your tokamak could be dissipated in a

00:13:09,340 --> 00:13:16,660
way that could disrupt or harm that the

00:13:13,560 --> 00:13:18,370
reactor itself so the approach being

00:13:16,660 --> 00:13:21,190
tried at Princeton Plasma Physics

00:13:18,370 --> 00:13:24,370
Laboratory on a team led by Bill Tang is

00:13:21,190 --> 00:13:26,380
to use a big data-driven statistical

00:13:24,370 --> 00:13:28,900
machine learning predictions for the

00:13:26,380 --> 00:13:31,080
occurrence of disruption in the joint

00:13:28,900 --> 00:13:34,630
European torus jet which is the largest

00:13:31,080 --> 00:13:38,590
tokamak on our planet right now and has

00:13:34,630 --> 00:13:40,180
a lot of data on this from experimental

00:13:38,590 --> 00:13:43,780
studies of disruptions over about a

00:13:40,180 --> 00:13:44,740
decade long period after getting access

00:13:43,780 --> 00:13:46,600
to this data

00:13:44,740 --> 00:13:49,390
the Princeton teams goals include

00:13:46,600 --> 00:13:51,100
improving the physics fidelity of the

00:13:49,390 --> 00:13:53,860
development of their multi-dimensional

00:13:51,100 --> 00:13:57,250
time-dependent software to include

00:13:53,860 --> 00:13:59,500
better classifiers to develop well in

00:13:57,250 --> 00:14:04,210
the the challenges is pretty stiff

00:13:59,500 --> 00:14:05,560
because in order to ameliorate the

00:14:04,210 --> 00:14:08,770
disruption they need to be able to

00:14:05,560 --> 00:14:11,230
respond within about 50 microseconds and

00:14:08,770 --> 00:14:14,200
to develop a portable cross machine

00:14:11,230 --> 00:14:16,270
prediction software beyond jet to other

00:14:14,200 --> 00:14:16,920
devices and eventually eater what that

00:14:16,270 --> 00:14:18,810
means is

00:14:16,920 --> 00:14:20,880
they need to be able to train the

00:14:18,810 --> 00:14:24,089
classifiers on one tokamak and then

00:14:20,880 --> 00:14:25,500
apply them to a yet constructed tokamak

00:14:24,089 --> 00:14:28,639
because that's going to be the challenge

00:14:25,500 --> 00:14:33,089
a Teeter say to train on jet or other

00:14:28,639 --> 00:14:35,760
tokamaks and then apply the the

00:14:33,089 --> 00:14:39,690
classifiers on eater because it may be

00:14:35,760 --> 00:14:41,610
too dangerous at eater to expose the

00:14:39,690 --> 00:14:45,209
machine to development of this large

00:14:41,610 --> 00:14:50,420
data set without the ability to mitigate

00:14:45,209 --> 00:14:52,320
disruptions okay so and then there's the

00:14:50,420 --> 00:14:54,329
computational science and engineering

00:14:52,320 --> 00:14:56,730
task of developing deploying these the

00:14:54,329 --> 00:14:58,500
machine learning software via a deep

00:14:56,730 --> 00:15:01,050
learning recurrent neural networks and

00:14:58,500 --> 00:15:04,230
and they have been acted in his field

00:15:01,050 --> 00:15:06,570
over the last two years they've been

00:15:04,230 --> 00:15:08,430
scaling on many of the large GPU

00:15:06,570 --> 00:15:11,040
accelerated supercomputers around the

00:15:08,430 --> 00:15:14,220
world in the US and Switzerland and in

00:15:11,040 --> 00:15:16,800
Japan and they are getting going on

00:15:14,220 --> 00:15:20,339
summit these days this there are some

00:15:16,800 --> 00:15:20,940
scaling data for tensorflow + MPI using

00:15:20,339 --> 00:15:23,490
singulated

00:15:20,940 --> 00:15:29,269
singularity containers on untighten

00:15:23,490 --> 00:15:32,430
scaling up to about 6,000 GPUs ok

00:15:29,269 --> 00:15:34,860
another case study comes from the

00:15:32,430 --> 00:15:37,320
Fermilab firming national accelerator

00:15:34,860 --> 00:15:40,490
facility and a particular neutrino

00:15:37,320 --> 00:15:43,110
experiment they're called Minerva

00:15:40,490 --> 00:15:46,170
collaborators from Fermi and Oakridge

00:15:43,110 --> 00:15:48,209
like improved Fermilab's machine

00:15:46,170 --> 00:15:50,970
learning networks for vertex

00:15:48,209 --> 00:15:53,579
reconstruction this is where I showed in

00:15:50,970 --> 00:15:56,550
the data and the blue charts are the

00:15:53,579 --> 00:15:59,459
tracks that indicate the path of

00:15:56,550 --> 00:16:02,339
particles inside the detectors and are

00:15:59,459 --> 00:16:05,730
really the subjects of the study need to

00:16:02,339 --> 00:16:07,829
be classified and this is a an image

00:16:05,730 --> 00:16:10,350
classification problem in three

00:16:07,829 --> 00:16:16,050
dimensions but one that needs to be done

00:16:10,350 --> 00:16:17,579
very rapidly and it's scale so the tool

00:16:16,050 --> 00:16:20,550
that was applied here it was one

00:16:17,579 --> 00:16:22,769
developed at Oakridge called mindle the

00:16:20,550 --> 00:16:25,110
the premise of Mendel is that for every

00:16:22,769 --> 00:16:26,360
data set there exists a corresponding

00:16:25,110 --> 00:16:28,399
neural network that

00:16:26,360 --> 00:16:30,980
forms optimally with that data you may

00:16:28,399 --> 00:16:34,579
not know what that architecture of the

00:16:30,980 --> 00:16:36,230
network is but but it exists so this

00:16:34,579 --> 00:16:39,140
group said let's search for it with the

00:16:36,230 --> 00:16:41,029
genetic algorithm so they developed

00:16:39,140 --> 00:16:42,680
these evolutionary algorithms to search

00:16:41,029 --> 00:16:44,630
the optimal hyperplane ders and

00:16:42,680 --> 00:16:46,610
topologies of the machine learning

00:16:44,630 --> 00:16:49,399
networks and they district they

00:16:46,610 --> 00:16:52,100
demonstrated on Titan using all of the

00:16:49,399 --> 00:16:55,040
resource that the high energy physics

00:16:52,100 --> 00:16:57,950
data could the classification of the

00:16:55,040 --> 00:16:59,839
high energy physics data on from the

00:16:57,950 --> 00:17:02,750
Minerva experiment couldn't be improved

00:16:59,839 --> 00:17:05,030
and this was exact this has been it's

00:17:02,750 --> 00:17:08,000
the same technique has been evaluated on

00:17:05,030 --> 00:17:11,540
multiple diverse data sets like standard

00:17:08,000 --> 00:17:13,610
computer vision data sets and data from

00:17:11,540 --> 00:17:16,010
the scible Asian neutron source small

00:17:13,610 --> 00:17:19,100
angle scattering from the Oak Ridge

00:17:16,010 --> 00:17:22,880
National Laboratory so this is now being

00:17:19,100 --> 00:17:26,959
ported to two summit in order to do even

00:17:22,880 --> 00:17:30,080
bigger problems and the last example

00:17:26,959 --> 00:17:32,720
comes from a healthcare so there is a

00:17:30,080 --> 00:17:34,580
big data revolution in healthcare it's

00:17:32,720 --> 00:17:35,500
well underway and advances in machine

00:17:34,580 --> 00:17:37,730
learning

00:17:35,500 --> 00:17:40,280
coupled with the explosion of healthcare

00:17:37,730 --> 00:17:41,929
data is showing promise for accelerated

00:17:40,280 --> 00:17:46,309
biomedical research and discovery

00:17:41,929 --> 00:17:48,140
clinical decision support guiding the

00:17:46,309 --> 00:17:50,090
vision of personalized health treatments

00:17:48,140 --> 00:17:52,190
helping uncover better preventive

00:17:50,090 --> 00:17:53,510
practices and improving workflow and

00:17:52,190 --> 00:17:56,690
streaming communications and

00:17:53,510 --> 00:17:58,340
coordinating so and also offering new

00:17:56,690 --> 00:18:01,340
ways to handle waste fraud and abuse

00:17:58,340 --> 00:18:03,440
which is actually the initial driver at

00:18:01,340 --> 00:18:05,809
Oak Ridge National Laboratory for

00:18:03,440 --> 00:18:10,309
getting involved in the health data

00:18:05,809 --> 00:18:15,049
sciences one particular use case that

00:18:10,309 --> 00:18:19,100
comes from Kent Cancer pathology reports

00:18:15,049 --> 00:18:21,290
and registries of cancer pathology

00:18:19,100 --> 00:18:23,510
reports in different states each state

00:18:21,290 --> 00:18:25,490
manages these cancer pathology reports

00:18:23,510 --> 00:18:29,210
in different ways and different data

00:18:25,490 --> 00:18:31,760
sets they are text-based documents that

00:18:29,210 --> 00:18:33,559
are full of errors and our sole volume

00:18:31,760 --> 00:18:36,080
is that probably no one single person

00:18:33,559 --> 00:18:38,630
can read them so natural language

00:18:36,080 --> 00:18:41,899
processing and

00:18:38,630 --> 00:18:45,130
is being applied to these datasets in

00:18:41,899 --> 00:18:47,840
order to scale up their analysis and

00:18:45,130 --> 00:18:50,529
extract more information that would have

00:18:47,840 --> 00:18:54,049
laid dormant inside their registries

00:18:50,529 --> 00:18:56,139
without machine learning techniques this

00:18:54,049 --> 00:19:00,409
is our this has been ported to Summit

00:18:56,139 --> 00:19:02,360
right now so with that I'll conclude the

00:19:00,409 --> 00:19:06,230
talk summit is still under construction

00:19:02,360 --> 00:19:09,169
and we expect to accept the machine this

00:19:06,230 --> 00:19:12,159
summer and we're getting users on and

00:19:09,169 --> 00:19:16,870
we'll start our user programs next year

00:19:12,159 --> 00:19:16,870
so with that are there any questions

00:19:22,240 --> 00:19:30,020
are we close so eater is a is a science

00:19:27,470 --> 00:19:32,720
experiment the goals of eater are not to

00:19:30,020 --> 00:19:36,409
produce electricity but it's to sustain

00:19:32,720 --> 00:19:43,429
a burning plasma a reaction that

00:19:36,409 --> 00:19:45,980
produces more energy than is then is

00:19:43,429 --> 00:19:48,230
required to generate the plasma so it

00:19:45,980 --> 00:19:50,690
won't produce any electricity so for

00:19:48,230 --> 00:19:54,380
example part of the experiment is to

00:19:50,690 --> 00:19:56,120
actually cool the walls so that the

00:19:54,380 --> 00:19:58,490
issues with the plasma material

00:19:56,120 --> 00:20:01,399
interface can be more effectively

00:19:58,490 --> 00:20:03,110
managed right in order to produce

00:20:01,399 --> 00:20:06,590
electricity you would want the walls to

00:20:03,110 --> 00:20:08,990
get hot to have a big Delta T so you can

00:20:06,590 --> 00:20:12,679
boil water and make steam and turn a

00:20:08,990 --> 00:20:16,279
turbine right the the community

00:20:12,679 --> 00:20:17,750
experiment that comes after eater will

00:20:16,279 --> 00:20:20,270
be the one that takes on those

00:20:17,750 --> 00:20:21,350
engineering challenges of actually

00:20:20,270 --> 00:20:24,320
producing electricity

00:20:21,350 --> 00:20:27,409
it's name is demo so if you hear about

00:20:24,320 --> 00:20:29,179
demo being constructed then you'll know

00:20:27,409 --> 00:20:33,289
we're getting closer to engineering

00:20:29,179 --> 00:20:36,169
scale application of fusion energy right

00:20:33,289 --> 00:20:41,020
now it's a very large very challenging

00:20:36,169 --> 00:20:41,020

YouTube URL: https://www.youtube.com/watch?v=m_jaJ7aeN7M


