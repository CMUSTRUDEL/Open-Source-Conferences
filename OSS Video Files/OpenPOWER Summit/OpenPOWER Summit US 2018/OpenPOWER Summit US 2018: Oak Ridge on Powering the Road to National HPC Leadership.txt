Title: OpenPOWER Summit US 2018: Oak Ridge on Powering the Road to National HPC Leadership
Publication date: 2018-04-02
Playlist: OpenPOWER Summit US 2018
Description: 
	Jack Wells, Director of Science at Oak Ridge National Laboratory discusses Powering the Road to National HPC Leadership at OpenPOWER Summit 2018.

For more information, please visit: http://www.openpowerfoundation.org
Captions: 
	00:00:00,030 --> 00:00:04,740
ladies and gentlemen please welcome

00:00:01,909 --> 00:00:06,750
director of science National Center for

00:00:04,740 --> 00:00:16,710
computational sciences at Oak Ridge

00:00:06,750 --> 00:00:18,750
National Laboratory Jack wells so thank

00:00:16,710 --> 00:00:22,619
you very much it's great to be here as a

00:00:18,750 --> 00:00:24,390
member of the open Power Foundation I'm

00:00:22,619 --> 00:00:25,949
Jack Wells I'm the director of science

00:00:24,390 --> 00:00:30,599
of the National Center for computational

00:00:25,949 --> 00:00:32,189
sciences our role at the National Center

00:00:30,599 --> 00:00:35,250
for computational science has simply

00:00:32,189 --> 00:00:37,559
said is to build the largest

00:00:35,250 --> 00:00:40,050
supercomputer we can for science and

00:00:37,559 --> 00:00:42,840
engineering research for a compute and

00:00:40,050 --> 00:00:45,539
data intensive purposes and make that

00:00:42,840 --> 00:00:48,600
resource available to us industry

00:00:45,539 --> 00:00:51,210
universities National Laboratories and

00:00:48,600 --> 00:00:54,420
other federal agencies so the the scope

00:00:51,210 --> 00:00:56,789
of our supercomputing vision is is broad

00:00:54,420 --> 00:00:58,440
relative to the science but it's focused

00:00:56,789 --> 00:01:02,969
in the terms that we want to leverage

00:00:58,440 --> 00:01:06,090
the leading-edge technology so Oak Ridge

00:01:02,969 --> 00:01:08,790
National Laboratory is one of 17

00:01:06,090 --> 00:01:11,580
national laboratories in our US system

00:01:08,790 --> 00:01:13,549
one of 10 that's managed directly by the

00:01:11,580 --> 00:01:16,979
Office of Science therefore it has a

00:01:13,549 --> 00:01:20,189
science research ethic a focus and of

00:01:16,979 --> 00:01:22,500
those are five large multi programmatic

00:01:20,189 --> 00:01:26,939
laboratories Oak Ridge is the largest

00:01:22,500 --> 00:01:29,360
science lab in this regard so what is

00:01:26,939 --> 00:01:31,259
the leadership computing facility it's a

00:01:29,360 --> 00:01:35,700
collaborative Department of Energy

00:01:31,259 --> 00:01:38,070
Office of Science user facility program

00:01:35,700 --> 00:01:40,619
that has two centers one at Oak Ridge

00:01:38,070 --> 00:01:42,810
and one at Argonne and the mission as I

00:01:40,619 --> 00:01:45,090
said the mission is to provide the most

00:01:42,810 --> 00:01:47,490
capable resources we can typically that

00:01:45,090 --> 00:01:49,799
means you can get access to resources

00:01:47,490 --> 00:01:52,409
that are 10 to a hundred times more

00:01:49,799 --> 00:01:56,490
capable than what you would have in a

00:01:52,409 --> 00:01:58,500
regional or a local resource do we has

00:01:56,490 --> 00:02:02,700
prioritized the to Center to

00:01:58,500 --> 00:02:04,619
architecture strategy in order to create

00:02:02,700 --> 00:02:07,560
diversity and opportunity in the

00:02:04,619 --> 00:02:10,080
marketplace and these resources are made

00:02:07,560 --> 00:02:12,540
available through highly competitive

00:02:10,080 --> 00:02:15,060
user programs

00:02:12,540 --> 00:02:17,189
these resources are for you if you would

00:02:15,060 --> 00:02:18,900
like to use them at Oakridge I am a good

00:02:17,189 --> 00:02:20,519
point of contact and so I would look

00:02:18,900 --> 00:02:22,650
forward to talking to you after the day

00:02:20,519 --> 00:02:24,239
if you're interested in what's going on

00:02:22,650 --> 00:02:28,019
at the Oakridge leadership computing

00:02:24,239 --> 00:02:31,319
facility so at Oak Ridge our program

00:02:28,019 --> 00:02:33,750
started in 2004 this was two years after

00:02:31,319 --> 00:02:37,260
the earth simulator happened in Japan

00:02:33,750 --> 00:02:39,269
where the Japanese built a system for

00:02:37,260 --> 00:02:42,420
science that was seven times more

00:02:39,269 --> 00:02:44,159
capable than the sum total of all

00:02:42,420 --> 00:02:47,040
supercomputers in the United States

00:02:44,159 --> 00:02:48,359
right jackdaw garrow was quoted in the

00:02:47,040 --> 00:02:50,669
New York Times as saying that was a

00:02:48,359 --> 00:02:53,159
compute mnek moment right to have that

00:02:50,669 --> 00:02:55,620
kind of capability our policy engine

00:02:53,159 --> 00:02:57,419
engaged and the leadership computing

00:02:55,620 --> 00:03:00,750
facility was created that I've just

00:02:57,419 --> 00:03:02,970
defined our first machine OLC f1 was

00:03:00,750 --> 00:03:06,209
called Phoenix it was a vector processor

00:03:02,970 --> 00:03:08,099
based on very much motivated by the

00:03:06,209 --> 00:03:11,310
excellent performance that the earth

00:03:08,099 --> 00:03:13,769
simulator received in Japan but then we

00:03:11,310 --> 00:03:16,919
rapidly realized we needed to transition

00:03:13,769 --> 00:03:19,709
to a massively scalar processor with a

00:03:16,919 --> 00:03:22,799
project called Jaguar that had many

00:03:19,709 --> 00:03:25,979
instant instantiations over multiple

00:03:22,799 --> 00:03:28,519
years that lead 'add that led up to the

00:03:25,979 --> 00:03:32,280
Jaguar being a two petaflop

00:03:28,519 --> 00:03:35,849
supercomputer in 2009 it was with Jaguar

00:03:32,280 --> 00:03:37,769
that Oak Ridge really joined the set of

00:03:35,849 --> 00:03:40,139
elite supercomputer centers around the

00:03:37,769 --> 00:03:42,299
world that could deliver capabilities at

00:03:40,139 --> 00:03:45,329
the highest level at the same time we

00:03:42,299 --> 00:03:47,629
were delivering Jaguar in 2009 we were

00:03:45,329 --> 00:03:49,560
trying to understand what comes next

00:03:47,629 --> 00:03:52,109
what comes next

00:03:49,560 --> 00:03:54,799
if we had stayed on an evolutionary path

00:03:52,109 --> 00:03:57,900
would have been a computer that consumed

00:03:54,799 --> 00:04:00,540
say 30 megawatts rather than seven mega

00:03:57,900 --> 00:04:02,849
watts as Jaguar and required three times

00:04:00,540 --> 00:04:05,519
the floor print three times the

00:04:02,849 --> 00:04:07,620
footprint in our datacenter Jaguar was

00:04:05,519 --> 00:04:10,109
already the size of a basketball court

00:04:07,620 --> 00:04:12,180
right we didn't build that supercomputer

00:04:10,109 --> 00:04:15,750
rather we joined a new partnership with

00:04:12,180 --> 00:04:17,909
Nvidia to basically upgrade Jaguar to

00:04:15,750 --> 00:04:20,549
create Titan and if I were to summarize

00:04:17,909 --> 00:04:22,380
the achievement there that within the

00:04:20,549 --> 00:04:23,580
same footprint within the same

00:04:22,380 --> 00:04:25,160
electricity bill

00:04:23,580 --> 00:04:27,470
we delivered 10x

00:04:25,160 --> 00:04:29,180
or application performance across wide

00:04:27,470 --> 00:04:31,760
areas of science and engineering

00:04:29,180 --> 00:04:35,240
right so at the same time we did we were

00:04:31,760 --> 00:04:37,610
fielding Titan which was also it

00:04:35,240 --> 00:04:40,250
continues to be at the leading edge of a

00:04:37,610 --> 00:04:42,050
scientific supercomputing centers we

00:04:40,250 --> 00:04:46,990
were already planning what would replace

00:04:42,050 --> 00:04:50,030
Titan and that comes very soon this year

00:04:46,990 --> 00:04:54,170
with Titan we are operating it now but

00:04:50,030 --> 00:04:57,110
we are we are very much engaged with our

00:04:54,170 --> 00:04:59,240
partnership with IBM and Vidya Mellanox

00:04:57,110 --> 00:05:01,430
RedHat to deliver the summit

00:04:59,240 --> 00:05:03,500
supercomputer which will be a 200

00:05:01,430 --> 00:05:05,810
petaflop supercomputer and which is the

00:05:03,500 --> 00:05:09,410
subject of my presentation to you here

00:05:05,810 --> 00:05:11,450
today beyond that again when you've

00:05:09,410 --> 00:05:13,940
filled one of these computers you better

00:05:11,450 --> 00:05:15,710
get be well down the road of planning

00:05:13,940 --> 00:05:19,160
the project that was going to replace it

00:05:15,710 --> 00:05:21,230
and indeed we're very much engaged with

00:05:19,160 --> 00:05:23,960
do-e and the vendor community right now

00:05:21,230 --> 00:05:26,720
in planning frontier which will be our

00:05:23,960 --> 00:05:30,290
exascale supercomputer to be delivered

00:05:26,720 --> 00:05:32,510
in 2021 so that's some a time scale

00:05:30,290 --> 00:05:34,300
above something like 17 years where you

00:05:32,510 --> 00:05:38,810
had a couple of order of magnitude

00:05:34,300 --> 00:05:41,780
excuse me about a factor of a million

00:05:38,810 --> 00:05:44,480
increase in performance over that time

00:05:41,780 --> 00:05:47,360
frame and so we're very excited about

00:05:44,480 --> 00:05:50,390
what's going on currently with Summit so

00:05:47,360 --> 00:05:52,910
coming soon in 2008 summit will replace

00:05:50,390 --> 00:05:54,620
Titan as our the Oakridge leadership

00:05:52,910 --> 00:05:57,650
computing facilities leadership

00:05:54,620 --> 00:05:59,090
supercomputer summit is slated to be one

00:05:57,650 --> 00:06:01,730
of the world's most powerful and

00:05:59,090 --> 00:06:03,350
smartest supercomputer for the

00:06:01,730 --> 00:06:04,970
Department of Energy of course it's

00:06:03,350 --> 00:06:10,030
stakeholders are really much broader

00:06:04,970 --> 00:06:13,490
than deal we as I've explained here is a

00:06:10,030 --> 00:06:16,310
quick overview of the architecture my

00:06:13,490 --> 00:06:18,470
colleague Chris Zimmer it gives a

00:06:16,310 --> 00:06:23,150
technical deep dive on this topic in the

00:06:18,470 --> 00:06:26,590
afternoon so summit combines the power 9

00:06:23,150 --> 00:06:30,050
processor with envy link with the Nvidia

00:06:26,590 --> 00:06:33,169
GV 100 with envy link to create the

00:06:30,050 --> 00:06:36,919
compute unit on a computer that has two

00:06:33,169 --> 00:06:39,259
power 9s and six Volta GPUs right that's

00:06:36,919 --> 00:06:41,509
all packaged with

00:06:39,259 --> 00:06:46,249
very efficient cooling technology

00:06:41,509 --> 00:06:48,710
provided by IBM 18 of these servers goes

00:06:46,249 --> 00:06:54,830
into a rack that are warm water cooled

00:06:48,710 --> 00:06:58,699
and has a max power profile of about 55

00:06:54,830 --> 00:07:02,650
kilowatts for these racks we're going to

00:06:58,699 --> 00:07:05,509
have 256 of them in our data center and

00:07:02,650 --> 00:07:08,360
provide with the overall memory

00:07:05,509 --> 00:07:11,060
hierarchy a total of 10 petabytes of

00:07:08,360 --> 00:07:14,210
memory accessible to the programmer and

00:07:11,060 --> 00:07:19,669
we're building a 250 petabyte file

00:07:14,210 --> 00:07:22,370
system to support the resource so this

00:07:19,669 --> 00:07:24,439
is a both a photograph of one of the

00:07:22,370 --> 00:07:26,419
server nodes one of these was shown

00:07:24,439 --> 00:07:28,610
earlier this morning this one's fully

00:07:26,419 --> 00:07:31,370
populated with all the components you

00:07:28,610 --> 00:07:34,219
can see the copper cold plates the

00:07:31,370 --> 00:07:38,960
direct water cooling going into the form

00:07:34,219 --> 00:07:41,389
factor it's a truly a rich node our

00:07:38,960 --> 00:07:44,089
programmers our users are very excited

00:07:41,389 --> 00:07:47,779
about having this much fast memory where

00:07:44,089 --> 00:07:51,949
it's needed colocasia the the power 9

00:07:47,779 --> 00:07:54,399
processor with the the the six GPUs the

00:07:51,949 --> 00:07:59,089
two power lines with the six GPUs

00:07:54,399 --> 00:08:01,129
there's the fast HDR connection to the

00:07:59,089 --> 00:08:03,409
Mellanox interconnect and we've got

00:08:01,129 --> 00:08:05,839
novel use cases coming for the

00:08:03,409 --> 00:08:09,979
non-volatile random access memory that's

00:08:05,839 --> 00:08:12,589
on board each node this is a direct

00:08:09,979 --> 00:08:16,370
comparison between our current platform

00:08:12,589 --> 00:08:18,710
Titan which is a Cray NVIDIA product and

00:08:16,370 --> 00:08:21,560
sumit you can see we have many fewer

00:08:18,710 --> 00:08:23,810
nodes we have much more powerful nodes

00:08:21,560 --> 00:08:26,569
much more memory per node for a total in

00:08:23,810 --> 00:08:28,849
a much larger total system memory a much

00:08:26,569 --> 00:08:31,370
faster interconnect much higher

00:08:28,849 --> 00:08:33,469
bandwidth between the CPUs and the GPUs

00:08:31,370 --> 00:08:36,320
I would say if we would summarize the

00:08:33,469 --> 00:08:38,899
morning in one word so far its bandwidth

00:08:36,320 --> 00:08:41,899
our computers are not necessarily faster

00:08:38,899 --> 00:08:43,849
they're wider right the bandwidth is

00:08:41,899 --> 00:08:46,550
what gets you the faster time to

00:08:43,849 --> 00:08:48,680
solution and we have a much larger and

00:08:46,550 --> 00:08:51,350
faster file system that we're building

00:08:48,680 --> 00:08:54,320
with this we expect to be able to

00:08:51,350 --> 00:08:59,180
deliver this within a 13 mega watt power

00:08:54,320 --> 00:09:03,410
profile so what is coral.you if you

00:08:59,180 --> 00:09:05,450
follow the do e supercomputing world you

00:09:03,410 --> 00:09:08,089
may have heard of this acronym the Kuril

00:09:05,450 --> 00:09:10,760
systems well a Korell is a program

00:09:08,089 --> 00:09:13,130
through which a summit at Oak Ridge and

00:09:10,760 --> 00:09:15,170
Sierra it's sister computer is being

00:09:13,130 --> 00:09:17,270
procured Sierra's being built in

00:09:15,170 --> 00:09:19,520
parallel right now at Lawrence Livermore

00:09:17,270 --> 00:09:21,529
National Laboratories a truly great

00:09:19,520 --> 00:09:22,070
Department of Energy supercomputing

00:09:21,529 --> 00:09:25,070
Center

00:09:22,070 --> 00:09:28,040
so do E's labs have several DOA Labs

00:09:25,070 --> 00:09:30,470
have strong supercomputing programs and

00:09:28,040 --> 00:09:32,240
facilities in order to bring that next

00:09:30,470 --> 00:09:34,520
generation of supercomputers to these

00:09:32,240 --> 00:09:36,200
labs deal we created coral the

00:09:34,520 --> 00:09:38,240
collaboration of Oak Ridge Argonne a

00:09:36,200 --> 00:09:40,550
Livermore to jointly procure these

00:09:38,240 --> 00:09:42,560
systems and in so doing align strategy

00:09:40,550 --> 00:09:45,140
and resources across the Department of

00:09:42,560 --> 00:09:47,240
Energy Enterprise so this collaborative

00:09:45,140 --> 00:09:49,820
grouping of DOA labs is done based on

00:09:47,240 --> 00:09:51,890
common acquisition and timings and this

00:09:49,820 --> 00:09:54,290
collaboration has been a win-win for our

00:09:51,890 --> 00:09:57,200
laboratories so through that procurement

00:09:54,290 --> 00:09:59,870
Livermore and Oak Ridge selected the

00:09:57,200 --> 00:10:02,060
winning bid from IBM and the open power

00:09:59,870 --> 00:10:05,089
partners and we've been executing on

00:10:02,060 --> 00:10:09,050
that since 2004 when the contracts were

00:10:05,089 --> 00:10:11,959
signed so we've been working from day

00:10:09,050 --> 00:10:14,330
one to get our users ready for this new

00:10:11,959 --> 00:10:16,490
resource we're preparing users through

00:10:14,330 --> 00:10:19,430
our of Center for accelerated

00:10:16,490 --> 00:10:21,980
application readiness I'll have a dive

00:10:19,430 --> 00:10:24,380
into this topic in the afternoon session

00:10:21,980 --> 00:10:26,810
we've already been developing our

00:10:24,380 --> 00:10:31,010
training and web-based documentation and

00:10:26,810 --> 00:10:33,500
we had an early access summit dev system

00:10:31,010 --> 00:10:35,660
that's still running today power AIDS

00:10:33,500 --> 00:10:38,720
and Pascal based systems so that we

00:10:35,660 --> 00:10:41,180
could exercise the rich functionality on

00:10:38,720 --> 00:10:43,550
the summit node well before summit would

00:10:41,180 --> 00:10:45,680
be available the goals for this program

00:10:43,550 --> 00:10:48,140
include early scientific achievements on

00:10:45,680 --> 00:10:50,270
the system were never so visible as we

00:10:48,140 --> 00:10:52,279
are when we turn on a machine that's

00:10:50,270 --> 00:10:54,170
brand new so we want to make impact

00:10:52,279 --> 00:10:56,750
right away demonstrate application

00:10:54,170 --> 00:10:59,150
readiness prepare users for our major

00:10:56,750 --> 00:11:00,560
user programs which go by the names

00:10:59,150 --> 00:11:03,440
inside in ALC

00:11:00,560 --> 00:11:05,779
see and otherwise harden the system with

00:11:03,440 --> 00:11:07,750
real applications the four full

00:11:05,779 --> 00:11:13,130
operations that will begin in January

00:11:07,750 --> 00:11:15,230
2019 so just to give a sense of the

00:11:13,130 --> 00:11:18,350
interest in the community we put a call

00:11:15,230 --> 00:11:21,890
out for early science proposals and in

00:11:18,350 --> 00:11:24,260
December and we had over 60 applicants

00:11:21,890 --> 00:11:26,089
the early science period is a time

00:11:24,260 --> 00:11:28,220
between when we accept the machine which

00:11:26,089 --> 00:11:32,570
we will will do later this summer and

00:11:28,220 --> 00:11:35,990
begin full operations in January and we

00:11:32,570 --> 00:11:37,820
got applications from universities

00:11:35,990 --> 00:11:41,060
national laboratories industrial

00:11:37,820 --> 00:11:44,320
projects some familiar users some brand

00:11:41,060 --> 00:11:48,080
new users so we were very satisfied that

00:11:44,320 --> 00:11:49,670
not so broadly distributed call for

00:11:48,080 --> 00:11:54,380
proposals would receive this much

00:11:49,670 --> 00:11:56,180
interest so if I use the word smartest

00:11:54,380 --> 00:11:58,130
supercomputer so if some is will be the

00:11:56,180 --> 00:11:59,900
world's smartest supercomputer for open

00:11:58,130 --> 00:12:02,060
science what does what makes the

00:11:59,900 --> 00:12:05,050
supercomputer smart so this is our

00:12:02,060 --> 00:12:07,130
articulation of that summer provides an

00:12:05,050 --> 00:12:09,050
unprecedent an opportunity for the

00:12:07,130 --> 00:12:11,930
integration of artificial intelligence

00:12:09,050 --> 00:12:14,720
and scientific discovery because of the

00:12:11,930 --> 00:12:17,000
GPU brawn that's available on the node

00:12:14,720 --> 00:12:19,280
the high speed data movement that's

00:12:17,000 --> 00:12:22,550
provided by NV link and the Mellanox

00:12:19,280 --> 00:12:24,680
interconnect and basically the placing

00:12:22,550 --> 00:12:26,930
the memory hierarchy where it matters

00:12:24,680 --> 00:12:29,300
that's the way our developers are seeing

00:12:26,930 --> 00:12:33,589
this system in the early days and I'll

00:12:29,300 --> 00:12:35,870
also have a dive in the afternoon on

00:12:33,589 --> 00:12:40,130
application of a leadership scale

00:12:35,870 --> 00:12:43,070
learning in our data center so summit

00:12:40,130 --> 00:12:44,720
but what can a smart supercomputer do so

00:12:43,070 --> 00:12:48,320
these are some of the topics that are

00:12:44,720 --> 00:12:51,339
with which we're engaged today as we

00:12:48,320 --> 00:12:53,600
transition from Titan to Summit so we're

00:12:51,339 --> 00:12:57,260
material scientists condensed matter

00:12:53,600 --> 00:13:01,070
physicists are using AI techniques in

00:12:57,260 --> 00:13:03,380
order to synthesize large amounts of

00:13:01,070 --> 00:13:05,420
multimode data that comes from multiple

00:13:03,380 --> 00:13:08,839
centers like electron microscopes

00:13:05,420 --> 00:13:10,760
scanning probe microscopes other kinds

00:13:08,839 --> 00:13:13,520
of chemical analysis you end up with

00:13:10,760 --> 00:13:14,110
large multi-dimensional data sets in

00:13:13,520 --> 00:13:16,930
order to

00:13:14,110 --> 00:13:19,240
process these this is ten years ago

00:13:16,930 --> 00:13:20,980
started moving them off of the desktop

00:13:19,240 --> 00:13:23,589
into the data center and now the

00:13:20,980 --> 00:13:25,600
algorithms have to be reworked and but

00:13:23,589 --> 00:13:28,480
anyway this is a very forefront problem

00:13:25,600 --> 00:13:32,290
I will say in the panel that follows

00:13:28,480 --> 00:13:34,810
this alex c give from uber is

00:13:32,290 --> 00:13:36,910
collaborating with our scientists at Oak

00:13:34,810 --> 00:13:40,360
Ridge to scale up his hoar Avadh

00:13:36,910 --> 00:13:42,519
framework on to summit in order to make

00:13:40,360 --> 00:13:44,940
the most impact as soon as possible as

00:13:42,519 --> 00:13:48,610
we can on these materials projects

00:13:44,940 --> 00:13:50,230
likewise we're collaborating with a

00:13:48,610 --> 00:13:53,950
scientist at the Fermi National

00:13:50,230 --> 00:13:56,019
Accelerator facility on neutrino physics

00:13:53,950 --> 00:14:00,070
applications of artificial intelligence

00:13:56,019 --> 00:14:03,190
deep learning basically to power through

00:14:00,070 --> 00:14:05,279
the large amounts of data for the event

00:14:03,190 --> 00:14:07,839
reconstruction one of the critical

00:14:05,279 --> 00:14:09,880
experimental physics tasks that they

00:14:07,839 --> 00:14:11,560
have to accomplish there they were

00:14:09,880 --> 00:14:14,019
already using some of these techniques

00:14:11,560 --> 00:14:17,250
but they ran out the top of their data

00:14:14,019 --> 00:14:19,839
center and they've been scaling up on

00:14:17,250 --> 00:14:21,690
Titan to the full size of the machine in

00:14:19,839 --> 00:14:23,890
order to optimize these networks and

00:14:21,690 --> 00:14:27,190
they're looking forward to getting going

00:14:23,890 --> 00:14:28,630
on Summit likewise one of the big fusion

00:14:27,190 --> 00:14:31,810
energy Sciences one of the big

00:14:28,630 --> 00:14:34,870
challenges in fusion is so called plasma

00:14:31,810 --> 00:14:39,250
disruptions when if you will the plasma

00:14:34,870 --> 00:14:41,709
will have a hiccup and have a nonlinear

00:14:39,250 --> 00:14:44,670
phenomena that can be damaging to the

00:14:41,709 --> 00:14:47,370
reactor these need to be detected and

00:14:44,670 --> 00:14:50,050
ameliorated it's hard to imagine that

00:14:47,370 --> 00:14:52,449
straightforward first principles based

00:14:50,050 --> 00:14:55,060
approach to simulating these very

00:14:52,449 --> 00:14:57,699
complex multi-dimensional nonlinear

00:14:55,060 --> 00:15:00,490
events will ever happen in say 50

00:14:57,699 --> 00:15:03,250
microsecond time scale that is needed in

00:15:00,490 --> 00:15:04,839
order to achieve the disruption but

00:15:03,250 --> 00:15:07,810
colleagues at Princeton Plasma Physics

00:15:04,839 --> 00:15:10,060
Laboratory are exploring massive

00:15:07,810 --> 00:15:11,829
datasets that come from today's tokamaks

00:15:10,060 --> 00:15:14,170
in order to Train deep neural networks

00:15:11,829 --> 00:15:18,100
to identify if you will these heart

00:15:14,170 --> 00:15:20,199
attacks of a fusion reactor and and

00:15:18,100 --> 00:15:23,170
lastly scientists at Oak Ridge National

00:15:20,199 --> 00:15:25,779
Laboratory are battling cancer using AI

00:15:23,170 --> 00:15:26,190
and scalable bringing in large amounts

00:15:25,779 --> 00:15:29,220
of

00:15:26,190 --> 00:15:30,480
for example pathology datasets and using

00:15:29,220 --> 00:15:33,450
techniques from natural language

00:15:30,480 --> 00:15:36,810
processing to process this material in

00:15:33,450 --> 00:15:44,220
order to get new insights that would not

00:15:36,810 --> 00:15:47,070
have been possible otherwise one more so

00:15:44,220 --> 00:15:48,810
so some it's still under construction we

00:15:47,070 --> 00:15:51,840
expect to accept the machine in the

00:15:48,810 --> 00:15:54,270
summer of 2008 that's just around the

00:15:51,840 --> 00:15:57,120
corner and allow you early users on

00:15:54,270 --> 00:15:59,250
throughout the summer and allocate our

00:15:57,120 --> 00:16:02,970
first users through our main user

00:15:59,250 --> 00:16:04,920
programs beginning in January 2009 we're

00:16:02,970 --> 00:16:06,840
continuing to work on the node and file

00:16:04,920 --> 00:16:09,270
storage installation and very

00:16:06,840 --> 00:16:12,240
importantly the software testing because

00:16:09,270 --> 00:16:14,460
it's a it's a big challenge to scale our

00:16:12,240 --> 00:16:17,550
software up to four thousand six hundred

00:16:14,460 --> 00:16:19,350
and eight of these large nodes so with

00:16:17,550 --> 00:16:22,940
that I'll conclude my presentation and

00:16:19,350 --> 00:16:22,940

YouTube URL: https://www.youtube.com/watch?v=RT6pa0isxCA


