Title: CINECA OpenPower-based HPC Infrastructure: Some porting results - Massimiliano Guarrasi, Cineca
Publication date: 2020-09-21
Playlist: OpenPOWER Summit NA 2020
Description: 
	CINECA OpenPower-based HPC Infrastructure: Some porting results - Massimiliano Guarrasi, Cineca

Speakers: Massimiliano Guarrasi

n Q2 of 2020 CINECA deployed its new Tier-0 HPC cluster: Marconi100. The cluster is currently ranked at position 9 in the Top500 Supercomputer list. The nodes of this machine are equipped with 2 IBM Power9 processors and Nvidia V100 GPUs. Currently we are porting several codes on this new machine. In this lecture we will present the new machine, the methods to have an access on the machine and a couple of successful examples of porting.
Captions: 
	00:00:01,040 --> 00:00:06,399
okay i think that i

00:00:03,199 --> 00:00:10,080
can start uh good morning

00:00:06,399 --> 00:00:10,960
my name is massimino borasi i'm an hpc

00:00:10,080 --> 00:00:14,960
specialist

00:00:10,960 --> 00:00:19,440
at china but also

00:00:14,960 --> 00:00:19,440
yeah i want to present to you

00:00:20,160 --> 00:00:26,880
our hpc facility

00:00:23,519 --> 00:00:26,880
mainly based on

00:00:26,960 --> 00:00:33,760
an open power system and i want

00:00:30,000 --> 00:00:33,760
to show you also some

00:00:34,480 --> 00:00:39,200
deployment of code we did in the last

00:00:38,399 --> 00:00:45,840
months

00:00:39,200 --> 00:00:45,840
on this new architecture first of all

00:00:50,800 --> 00:00:58,640
but also one of the

00:00:53,920 --> 00:01:02,480
largest hpc facility in europe

00:00:58,640 --> 00:01:03,199
it gives a service bot for university

00:01:02,480 --> 00:01:06,240
and

00:01:03,199 --> 00:01:10,080
ministry of university education

00:01:06,240 --> 00:01:12,640
and give software solutions servity to

00:01:10,080 --> 00:01:16,400
the university administration

00:01:12,640 --> 00:01:18,640
but also information for minister

00:01:16,400 --> 00:01:20,240
for the ministry for the management a

00:01:18,640 --> 00:01:24,000
system and

00:01:20,240 --> 00:01:28,720
so on here i want to show you

00:01:24,000 --> 00:01:33,600
only the hpc part of china that

00:01:28,720 --> 00:01:36,400
have as a main aim to

00:01:33,600 --> 00:01:36,880
the scientific research and particularly

00:01:36,400 --> 00:01:40,240
promote

00:01:36,880 --> 00:01:44,399
the scientific research by the use of

00:01:40,240 --> 00:01:48,960
the most advanced high performance

00:01:44,399 --> 00:01:48,960
computing system in europe

00:01:50,000 --> 00:01:53,520
apart these chineca give also support

00:01:52,799 --> 00:01:56,640
for

00:01:53,520 --> 00:01:59,840
innovation and technology transfers both

00:01:56,640 --> 00:02:00,799
for a public and private institution

00:01:59,840 --> 00:02:03,920
promoting

00:02:00,799 --> 00:02:06,840
in this case the use of big data

00:02:03,920 --> 00:02:08,399
solution but also artificial

00:02:06,840 --> 00:02:11,440
intelligence scientific visualization

00:02:08,399 --> 00:02:11,440
computer graphics

00:02:11,760 --> 00:02:19,120
chineca was funded

00:02:15,520 --> 00:02:22,319
more than 50 years ago and it

00:02:19,120 --> 00:02:25,680
for what i remember was always

00:02:22,319 --> 00:02:28,879
in the top 500 list of

00:02:25,680 --> 00:02:32,160
super computer and this is the

00:02:28,879 --> 00:02:35,120
position of chinek hpc cluster in the

00:02:32,160 --> 00:02:38,640
least in the last

00:02:35,120 --> 00:02:42,160
10 10-year we was in 2012

00:02:38,640 --> 00:02:46,080
position 12 in the list and with our

00:02:42,160 --> 00:02:50,080
ibm bulging ecosystem fermi and again

00:02:46,080 --> 00:02:54,480
at position 12 in 2006

00:02:50,080 --> 00:02:57,519
with our old tier zero cluster marconi

00:02:54,480 --> 00:03:01,519
currently we i think uh

00:02:57,519 --> 00:03:02,239
five months ago we changed our machine

00:03:01,519 --> 00:03:05,200
and

00:03:02,239 --> 00:03:07,760
now we have also another tier zero

00:03:05,200 --> 00:03:11,599
machine that is marconi 100

00:03:07,760 --> 00:03:15,200
that is an ebm power system

00:03:11,599 --> 00:03:16,560
with but i will show you the system in

00:03:15,200 --> 00:03:20,239
the next slide

00:03:16,560 --> 00:03:23,760
for now i want to commend you that

00:03:20,239 --> 00:03:28,080
the we are at position i'm

00:03:23,760 --> 00:03:31,200
the top 500 with our machine but

00:03:28,080 --> 00:03:34,319
as italy we are the

00:03:31,200 --> 00:03:38,080
only european nation in the top 10

00:03:34,319 --> 00:03:38,560
of the list and also the only nation

00:03:38,080 --> 00:03:42,080
with

00:03:38,560 --> 00:03:43,599
two system in the list because there is

00:03:42,080 --> 00:03:46,640
also another system the

00:03:43,599 --> 00:03:49,760
hpc 5 system

00:03:46,640 --> 00:03:52,959
that was in the list was managed by

00:03:49,760 --> 00:03:56,480
the system administrator of chineka so

00:03:52,959 --> 00:04:00,159
currently chineka managed two of the

00:03:56,480 --> 00:04:02,920
system of the top 500 list

00:04:00,159 --> 00:04:04,080
this is the structure of our

00:04:02,920 --> 00:04:07,840
infrastructure

00:04:04,080 --> 00:04:08,959
currently we have two tier zero system

00:04:07,840 --> 00:04:12,159
the old one

00:04:08,959 --> 00:04:15,599
that is marconi that was based on

00:04:12,159 --> 00:04:18,720
the skylake and knl processor and the

00:04:15,599 --> 00:04:21,759
newest one that is marconi 100

00:04:18,720 --> 00:04:25,840
that was based on power 9 and

00:04:21,759 --> 00:04:28,320
voltage pews for this we have out there

00:04:25,840 --> 00:04:29,120
two tier one system that will be

00:04:28,320 --> 00:04:32,720
replaced

00:04:29,120 --> 00:04:35,520
in six months i think that

00:04:32,720 --> 00:04:36,639
is galileo all these systems are

00:04:35,520 --> 00:04:40,840
connected

00:04:36,639 --> 00:04:43,919
throughout uh 100 gigabit ethernet

00:04:40,840 --> 00:04:47,759
uh and

00:04:43,919 --> 00:04:48,880
thanks to this network it also a tape

00:04:47,759 --> 00:04:52,479
library when

00:04:48,880 --> 00:04:56,080
disk resource was connected and the

00:04:52,479 --> 00:05:00,000
chineka observer is connected to the

00:04:56,080 --> 00:05:03,600
internet thanks to agent gear network

00:05:00,000 --> 00:05:06,800
uh this is a short overview of

00:05:03,600 --> 00:05:07,280
our system because i think that all of

00:05:06,800 --> 00:05:10,880
you

00:05:07,280 --> 00:05:14,160
knows what is power line system

00:05:10,880 --> 00:05:17,280
currently we have a machine with

00:05:14,160 --> 00:05:21,120
about 1000 nodes each node

00:05:17,280 --> 00:05:24,560
has the true ebm power now

00:05:21,120 --> 00:05:25,199
in processor at 3 with three gigahertz

00:05:24,560 --> 00:05:28,800
of

00:05:25,199 --> 00:05:32,720
of clock frequency 266

00:05:28,800 --> 00:05:37,120
gigabyte of ram and four nvidia

00:05:32,720 --> 00:05:41,880
100 gpus uh in total we have

00:05:37,120 --> 00:05:44,560
about uh 350 000 cores

00:05:41,880 --> 00:05:48,000
250 000

00:05:44,560 --> 00:05:52,960
gigabit of ram and about 40 000

00:05:48,000 --> 00:05:56,160
sorry 4 000 gpus all these

00:05:52,960 --> 00:05:59,680
cards are connected throughout a

00:05:56,160 --> 00:06:03,440
dual realm and linux adier infiniband

00:05:59,680 --> 00:06:06,479
network and currently the the

00:06:03,440 --> 00:06:08,479
sustained link up impact performance of

00:06:06,479 --> 00:06:11,680
the system is about

00:06:08,479 --> 00:06:11,680
21 teraflop

00:06:12,000 --> 00:06:15,919
it is also possible to receive resources

00:06:14,800 --> 00:06:19,039
on the system

00:06:15,919 --> 00:06:21,199
yeah with various methods first of all

00:06:19,039 --> 00:06:23,039
thanks to agreement you can pay for

00:06:21,199 --> 00:06:24,400
having the resource you have to contact

00:06:23,039 --> 00:06:27,440
cnec and

00:06:24,400 --> 00:06:30,160
uh your institution have to do

00:06:27,440 --> 00:06:31,280
to make an agreement with china for the

00:06:30,160 --> 00:06:34,800
resources

00:06:31,280 --> 00:06:38,240
and uh for free thanks

00:06:34,800 --> 00:06:41,520
to thanks to true

00:06:38,240 --> 00:06:43,840
initiative one it is an european

00:06:41,520 --> 00:06:46,720
initiative that is praised

00:06:43,840 --> 00:06:48,720
and and the other one is at the national

00:06:46,720 --> 00:06:51,490
level thanks to eskra

00:06:48,720 --> 00:06:52,880
for what regards praise the

00:06:51,490 --> 00:06:55,919
[Applause]

00:06:52,880 --> 00:06:58,560
initiative was for all

00:06:55,919 --> 00:07:00,160
european researchers that european

00:06:58,560 --> 00:07:02,560
researcher intend

00:07:00,160 --> 00:07:03,759
a researcher with an european

00:07:02,560 --> 00:07:06,800
affiliation

00:07:03,759 --> 00:07:11,440
and this for these uh

00:07:06,800 --> 00:07:15,199
allocation is for a very large project

00:07:11,440 --> 00:07:18,479
as an order magnitude from 100

00:07:15,199 --> 00:07:22,720
000 to 1 million of non hour

00:07:18,479 --> 00:07:26,800
on our marconi 100 system and or

00:07:22,720 --> 00:07:29,199
it is also possible to reach to require

00:07:26,800 --> 00:07:30,479
small resources in order to test your

00:07:29,199 --> 00:07:34,080
code to port

00:07:30,479 --> 00:07:36,639
your code on our system but or

00:07:34,080 --> 00:07:38,400
to improve the performance of your code

00:07:36,639 --> 00:07:40,400
obviously

00:07:38,400 --> 00:07:42,400
in this case you have to request to

00:07:40,400 --> 00:07:44,879
place a preparatory access

00:07:42,400 --> 00:07:46,800
here in the slide of the presentation

00:07:44,879 --> 00:07:49,039
you can find the link for

00:07:46,800 --> 00:07:50,000
this initiative with all the information

00:07:49,039 --> 00:07:53,599
you need

00:07:50,000 --> 00:07:56,879
if furthermore if you are an

00:07:53,599 --> 00:08:00,000
italian researcher

00:07:56,879 --> 00:08:01,199
you can request a more moderate amount

00:08:00,000 --> 00:08:04,400
of resources

00:08:01,199 --> 00:08:05,039
from one thousand to fifty thousand not

00:08:04,400 --> 00:08:07,840
hour

00:08:05,039 --> 00:08:08,479
uh thanks to the screen initiative and

00:08:07,840 --> 00:08:12,000
again

00:08:08,479 --> 00:08:15,199
here you can find the link to

00:08:12,000 --> 00:08:16,960
to find all the information you need to

00:08:15,199 --> 00:08:20,000
submit the proposal

00:08:16,960 --> 00:08:23,039
and that's it for the part

00:08:20,000 --> 00:08:25,919
of the presentation that regards

00:08:23,039 --> 00:08:26,639
chinec infrastructure now i want to show

00:08:25,919 --> 00:08:30,560
you

00:08:26,639 --> 00:08:35,200
three example of

00:08:30,560 --> 00:08:38,399
porting of code on our system

00:08:35,200 --> 00:08:42,560
done with three different method

00:08:38,399 --> 00:08:45,600
and i want to show you both the method

00:08:42,560 --> 00:08:46,240
especially for what concerned the first

00:08:45,600 --> 00:08:50,320
code

00:08:46,240 --> 00:08:53,519
but also the results we obtained

00:08:50,320 --> 00:08:57,519
first of all the first example

00:08:53,519 --> 00:09:01,600
is about the stream code the sim code

00:08:57,519 --> 00:09:04,880
was developed by the cf disappend

00:09:01,600 --> 00:09:08,240
sapience group that was

00:09:04,880 --> 00:09:11,200
uh that what

00:09:08,240 --> 00:09:12,959
is leaded by professor pirozzoli and

00:09:11,200 --> 00:09:15,440
professor bernardini

00:09:12,959 --> 00:09:17,120
and chineca contribute to the

00:09:15,440 --> 00:09:19,600
development of this

00:09:17,120 --> 00:09:21,960
software thanks to our colleague

00:09:19,600 --> 00:09:23,440
francesco salvatore

00:09:21,960 --> 00:09:26,800
[Applause]

00:09:23,440 --> 00:09:30,640
the code is a classical dns

00:09:26,800 --> 00:09:33,839
code for a compressible fluid sole

00:09:30,640 --> 00:09:36,959
fluid flow it solved the navier stock

00:09:33,839 --> 00:09:39,360
question with uh discretized

00:09:36,959 --> 00:09:39,360
grid

00:09:40,720 --> 00:09:45,200
discretize the cartesian grid and

00:09:45,360 --> 00:09:53,760
it was also a three stage three

00:09:48,560 --> 00:09:57,360
third order that is quite standard in

00:09:53,760 --> 00:09:57,360
this field and

00:09:58,800 --> 00:10:06,480
the original code was developed to run

00:10:02,839 --> 00:10:10,000
on a classical gpu system or

00:10:06,480 --> 00:10:15,680
on and was else optimized for knl

00:10:10,000 --> 00:10:15,680
now we ported it on a multi gpu system

00:10:16,240 --> 00:10:19,839
to do this the

00:10:18,190 --> 00:10:22,560
[Applause]

00:10:19,839 --> 00:10:23,760
cost in term of person month so the

00:10:22,560 --> 00:10:26,800
pressure is about

00:10:23,760 --> 00:10:30,320
60 percent month uh

00:10:26,800 --> 00:10:33,519
eight for the first uh porting and date

00:10:30,320 --> 00:10:35,680
for the optimization

00:10:33,519 --> 00:10:36,959
in order to choose the best approach for

00:10:35,680 --> 00:10:40,480
the code we

00:10:36,959 --> 00:10:44,240
have various solutions we can choose

00:10:40,480 --> 00:10:48,720
to use an approach with

00:10:44,240 --> 00:10:51,440
mpi plus openmp or openscc

00:10:48,720 --> 00:10:52,320
approach and this is the simpler

00:10:51,440 --> 00:10:57,839
approach

00:10:52,320 --> 00:11:00,320
that use directive the second appliance

00:10:57,839 --> 00:11:01,279
possible approaches using cuda and the

00:11:00,320 --> 00:11:05,200
third one

00:11:01,279 --> 00:11:09,680
is using more exotic language

00:11:05,200 --> 00:11:13,519
like opencl or similar

00:11:09,680 --> 00:11:14,160
in our case we choose to use coda

00:11:13,519 --> 00:11:17,680
fortran

00:11:14,160 --> 00:11:21,279
and inside to cudafortune

00:11:17,680 --> 00:11:25,200
we choose to use the caf

00:11:21,279 --> 00:11:28,240
kernels that allow us

00:11:25,200 --> 00:11:31,600
to to use

00:11:28,240 --> 00:11:35,120
a directive inside cuda

00:11:31,600 --> 00:11:36,160
and as you can see from this example on

00:11:35,120 --> 00:11:40,839
the

00:11:36,160 --> 00:11:44,079
right bottom it is quite simple to

00:11:40,839 --> 00:11:48,079
move do loop into gpu

00:11:44,079 --> 00:11:48,079
simply using uh

00:11:49,040 --> 00:11:55,360
cuda kernel simply by adding one row

00:11:52,720 --> 00:11:56,320
at the beginning and another row at the

00:11:55,360 --> 00:12:00,000
end

00:11:56,320 --> 00:12:03,440
clearly not all the code can be used

00:12:00,000 --> 00:12:07,519
can be reported simply with this

00:12:03,440 --> 00:12:11,040
command you have uh you know in any case

00:12:07,519 --> 00:12:14,639
to use other instruction in

00:12:11,040 --> 00:12:15,680
this case we use some process processor

00:12:14,639 --> 00:12:18,720
instruction

00:12:15,680 --> 00:12:20,880
in order to switch

00:12:18,720 --> 00:12:23,440
between the standard version of the code

00:12:20,880 --> 00:12:26,800
and the cuda version of the code because

00:12:23,440 --> 00:12:32,320
we want to have the same code bot for

00:12:26,800 --> 00:12:35,040
gpu and gpu and simply by switching

00:12:32,320 --> 00:12:35,680
the definition of a system variable in

00:12:35,040 --> 00:12:39,360
this case

00:12:35,680 --> 00:12:39,920
use coda we can switch between gpu and

00:12:39,360 --> 00:12:43,440
cpu

00:12:39,920 --> 00:12:44,959
in this case we can define a gpu

00:12:43,440 --> 00:12:49,760
specific variable

00:12:44,959 --> 00:12:52,880
you see that was enabled only

00:12:49,760 --> 00:12:55,360
if you scuda is defined we can

00:12:52,880 --> 00:12:56,480
at with the same method we can allocate

00:12:55,360 --> 00:12:59,680
this variable

00:12:56,480 --> 00:13:02,240
and at the end using a

00:12:59,680 --> 00:13:04,000
more complicated a slightly more

00:13:02,240 --> 00:13:07,760
complicated uh

00:13:04,000 --> 00:13:11,279
instruction if else and if we can

00:13:07,760 --> 00:13:16,560
select the right instruction for the

00:13:11,279 --> 00:13:16,560
gpu code or for the cpu code

00:13:17,600 --> 00:13:23,760
using this method we completed the first

00:13:20,959 --> 00:13:24,800
cuda version of the code and starting

00:13:23,760 --> 00:13:28,680
from this

00:13:24,800 --> 00:13:31,760
we proceed with the

00:13:28,680 --> 00:13:35,519
optimization of the code

00:13:31,760 --> 00:13:38,560
following quite standard best practice

00:13:35,519 --> 00:13:41,680
so first of all we start with a basic

00:13:38,560 --> 00:13:45,040
baseline code the code then

00:13:41,680 --> 00:13:45,279
we change the memory our layout to adapt

00:13:45,040 --> 00:13:48,560
it

00:13:45,279 --> 00:13:52,240
to the structure of cuda core

00:13:48,560 --> 00:13:55,440
after this we change the

00:13:52,240 --> 00:13:57,440
transposition of matrices to improve the

00:13:55,440 --> 00:14:00,720
performance of the code

00:13:57,440 --> 00:14:02,399
then we pre-compute the primitive sum

00:14:00,720 --> 00:14:05,440
primitive variable

00:14:02,399 --> 00:14:07,199
to reduce the number of access to the

00:14:05,440 --> 00:14:11,839
main memory of the node

00:14:07,199 --> 00:14:17,279
and fam finally we readjusted the

00:14:11,839 --> 00:14:20,560
memory layout also for mpi buffer

00:14:17,279 --> 00:14:23,600
here we can see the results of

00:14:20,560 --> 00:14:26,639
this optimization in the first row you

00:14:23,600 --> 00:14:30,560
see the start

00:14:26,639 --> 00:14:34,160
in red the start execution

00:14:30,560 --> 00:14:38,560
time of the code and

00:14:34,160 --> 00:14:41,600
in the third column you can see that

00:14:38,560 --> 00:14:42,639
the time of execution increase after the

00:14:41,600 --> 00:14:45,440
changing of the

00:14:42,639 --> 00:14:47,440
layout and this is quite common but the

00:14:45,440 --> 00:14:50,240
changing of the layout of memory

00:14:47,440 --> 00:14:51,920
it is necessary in order to proceed with

00:14:50,240 --> 00:14:54,720
the following

00:14:51,920 --> 00:14:55,519
optimization step and i as you can see

00:14:54,720 --> 00:14:58,880
in the

00:14:55,519 --> 00:15:02,800
in all the next step

00:14:58,880 --> 00:15:06,880
the time to solution will uh strongly

00:15:02,800 --> 00:15:09,920
reduce it and at the end

00:15:06,880 --> 00:15:12,959
the final time to solution is uh

00:15:09,920 --> 00:15:16,800
less than one half of the

00:15:12,959 --> 00:15:20,240
original time it is also possible

00:15:16,800 --> 00:15:21,199
to boost further the performance of the

00:15:20,240 --> 00:15:25,680
code using

00:15:21,199 --> 00:15:30,639
remote direct memory access for red

00:15:25,680 --> 00:15:33,519
and right but it is possible only in

00:15:30,639 --> 00:15:35,279
some machine in our case on power line

00:15:33,519 --> 00:15:38,320
it is possible but

00:15:35,279 --> 00:15:41,680
not in all the machine it is possible

00:15:38,320 --> 00:15:45,279
since it it depends from the

00:15:41,680 --> 00:15:48,399
version of mpi you are using and

00:15:45,279 --> 00:15:50,639
final this is

00:15:48,399 --> 00:15:50,639
the

00:15:52,000 --> 00:15:58,560
the results obtained the

00:15:55,360 --> 00:16:01,600
comparing the time to solution

00:15:58,560 --> 00:16:05,360
on different system as you can see

00:16:01,600 --> 00:16:08,720
on broadwell we have the first

00:16:05,360 --> 00:16:12,079
column results we

00:16:08,720 --> 00:16:15,360
use uh 2.8

00:16:12,079 --> 00:16:19,279
2.9 second then we have

00:16:15,360 --> 00:16:23,120
the results on a single k40 gpu

00:16:19,279 --> 00:16:26,560
then on a p100 gpu then

00:16:23,120 --> 00:16:28,800
uh the next one is on skelet node and

00:16:26,560 --> 00:16:32,160
finally on

00:16:28,800 --> 00:16:35,040
on 100 gpu as you can see

00:16:32,160 --> 00:16:36,720
the bus of performance passing from a

00:16:35,040 --> 00:16:39,759
standard broadwell

00:16:36,720 --> 00:16:44,000
node to a single gpu

00:16:39,759 --> 00:16:47,440
it is enormous if you consider that

00:16:44,000 --> 00:16:51,519
in a single power 9 ibm

00:16:47,440 --> 00:16:54,959
power 9 node you have for 100 gpu

00:16:51,519 --> 00:16:56,079
if you have an out grid point you can

00:16:54,959 --> 00:16:59,440
see that

00:16:56,079 --> 00:17:04,720
the boost of thermos performance is

00:16:59,440 --> 00:17:06,240
really high here i report but i have to

00:17:04,720 --> 00:17:09,520
be fast because we have

00:17:06,240 --> 00:17:11,919
time you can see the

00:17:09,520 --> 00:17:12,799
weak and strong scaling and also here

00:17:11,919 --> 00:17:15,760
you see that

00:17:12,799 --> 00:17:17,120
the performance are very good up to some

00:17:15,760 --> 00:17:20,959
thousand co

00:17:17,120 --> 00:17:24,880
of gpus and also the stroke scaling

00:17:20,959 --> 00:17:28,160
is good here up to eight gpus but only

00:17:24,880 --> 00:17:28,559
because the number of grid point is very

00:17:28,160 --> 00:17:31,520
low

00:17:28,559 --> 00:17:32,000
increasing the number of point you can

00:17:31,520 --> 00:17:35,280
increase

00:17:32,000 --> 00:17:39,520
also the scalability 2 000 gpus

00:17:35,280 --> 00:17:42,000
easily and that's it from the first

00:17:39,520 --> 00:17:42,880
code this as a second code i want to

00:17:42,000 --> 00:17:46,080
show you

00:17:42,880 --> 00:17:49,280
the work done by

00:17:46,080 --> 00:17:52,559
the shi's center of excellence

00:17:49,280 --> 00:17:53,840
that and schnecke is part of this center

00:17:52,559 --> 00:17:57,280
of excellence

00:17:53,840 --> 00:18:00,640
to port the say soul code

00:17:57,280 --> 00:18:05,840
on gpu cheese is a

00:18:00,640 --> 00:18:09,039
center of excellence code that

00:18:05,840 --> 00:18:12,200
have us aim the area

00:18:09,039 --> 00:18:16,720
of solidart and to port and improve the

00:18:12,200 --> 00:18:16,720
performance of solidart codes

00:18:16,960 --> 00:18:24,000
to be ready for the exascale era

00:18:20,240 --> 00:18:28,000
uh in this framework

00:18:24,000 --> 00:18:31,679
10 flagship code was chosen and

00:18:28,000 --> 00:18:36,240
the cisco code was one of this fab

00:18:31,679 --> 00:18:37,760
ish club it was developed at the

00:18:36,240 --> 00:18:41,520
university of

00:18:37,760 --> 00:18:45,440
technical university of munich and

00:18:41,520 --> 00:18:48,640
it solved large-scale simulation

00:18:45,440 --> 00:18:51,919
for seismic wake and earthquake

00:18:48,640 --> 00:18:55,360
dynamics it was based on

00:18:51,919 --> 00:18:58,160
discontinuous generic method and

00:18:55,360 --> 00:18:59,120
it also implement a local timestamping

00:18:58,160 --> 00:19:01,360
procedure

00:18:59,120 --> 00:19:02,640
in order to speed up the time

00:19:01,360 --> 00:19:06,160
integration pro

00:19:02,640 --> 00:19:06,880
process uh some of the most important

00:19:06,160 --> 00:19:10,080
hpc

00:19:06,880 --> 00:19:13,360
main feature of the uh

00:19:10,080 --> 00:19:16,160
code is the yayto yate

00:19:13,360 --> 00:19:17,760
or domain specific language that was

00:19:16,160 --> 00:19:20,799
developed

00:19:17,760 --> 00:19:22,160
for this code it is machine independent

00:19:20,799 --> 00:19:25,840
it is based on

00:19:22,160 --> 00:19:29,200
mpi plus some pnp and

00:19:25,840 --> 00:19:32,640
the parallelism is based on blast-like

00:19:29,200 --> 00:19:37,280
libraries in order to put

00:19:32,640 --> 00:19:40,640
this code on gpu we have to

00:19:37,280 --> 00:19:42,960
give a new structure and of

00:19:40,640 --> 00:19:44,640
to the code and particularly we pass

00:19:42,960 --> 00:19:47,679
through we

00:19:44,640 --> 00:19:50,799
the developer and our

00:19:47,679 --> 00:19:54,320
collaborator pass from mpi

00:19:50,799 --> 00:19:58,000
plus openmp to mpi plus coda

00:19:54,320 --> 00:20:00,799
uh to do this uh we

00:19:58,000 --> 00:20:02,880
we pass from single to batch at gem

00:20:00,799 --> 00:20:05,919
computing and

00:20:02,880 --> 00:20:06,480
at the same time we generate bot code

00:20:05,919 --> 00:20:10,559
for

00:20:06,480 --> 00:20:10,559
gpu and gpu and

00:20:11,919 --> 00:20:18,880
we also develop a new module for a

00:20:15,679 --> 00:20:22,320
yatito library

00:20:18,880 --> 00:20:26,320
this is these are the results of the

00:20:22,320 --> 00:20:30,159
benchmark on the gem part of the library

00:20:26,320 --> 00:20:32,400
because we start we compare the

00:20:30,159 --> 00:20:35,679
performance of the standard

00:20:32,400 --> 00:20:39,120
uh blast library with the

00:20:35,679 --> 00:20:42,799
jam inside the yeti library and

00:20:39,120 --> 00:20:46,080
as you can see the pair in the

00:20:42,799 --> 00:20:49,840
left part of the screen the performance

00:20:46,080 --> 00:20:54,640
of the gem forge library

00:20:49,840 --> 00:20:57,440
was always better than

00:20:54,640 --> 00:21:00,080
the standard blast library bot for

00:20:57,440 --> 00:21:03,760
single and double precision

00:21:00,080 --> 00:21:06,960
another important thing thing we can see

00:21:03,760 --> 00:21:12,320
is uh comparing

00:21:06,960 --> 00:21:12,320
the performance of the interior code

00:21:14,080 --> 00:21:22,159
passing from a standard axiom system

00:21:18,080 --> 00:21:26,559
to an ah an nvidia system

00:21:22,159 --> 00:21:28,960
and changing the number of elements

00:21:26,559 --> 00:21:29,840
and the number of i want to say the

00:21:28,960 --> 00:21:33,440
number

00:21:29,840 --> 00:21:37,039
of grid point as you can see

00:21:33,440 --> 00:21:40,080
the for high number of elements

00:21:37,039 --> 00:21:43,840
the use of gpu is

00:21:40,080 --> 00:21:48,240
better the performance are more

00:21:43,840 --> 00:21:51,840
a rugger than the standard cpu system

00:21:48,240 --> 00:21:55,760
and the drop down for a small number

00:21:51,840 --> 00:21:58,640
of points is due probably to

00:21:55,760 --> 00:22:00,640
the call the major cost of the

00:21:58,640 --> 00:22:04,000
communication in case or small

00:22:00,640 --> 00:22:07,280
packages and finally

00:22:04,000 --> 00:22:10,240
i we want also to test

00:22:07,280 --> 00:22:11,039
the code on a real system and here you

00:22:10,240 --> 00:22:14,240
can see

00:22:11,039 --> 00:22:15,679
the strong scaling strong and weak

00:22:14,240 --> 00:22:19,200
scaling of

00:22:15,679 --> 00:22:23,280
the code changing the number of

00:22:19,200 --> 00:22:23,280
grid point in this case

00:22:23,360 --> 00:22:27,840
we reached 0.5

00:22:28,480 --> 00:22:31,919
single precision precision petaflock

00:22:31,120 --> 00:22:36,880
using

00:22:31,919 --> 00:22:40,480
only 64 nodes of our machine

00:22:36,880 --> 00:22:44,480
and in in any case this scalability is

00:22:40,480 --> 00:22:46,060
quite perfect uh we want to show you

00:22:44,480 --> 00:22:49,280
also the

00:22:46,060 --> 00:22:53,600
[Applause]

00:22:49,280 --> 00:22:56,799
the scalability of the code using

00:22:53,600 --> 00:22:59,280
the local time stack procedure that is

00:22:56,799 --> 00:23:02,320
still not complete and since

00:22:59,280 --> 00:23:03,280
it is uh highly unbalanced as you can

00:23:02,320 --> 00:23:06,960
see from the

00:23:03,280 --> 00:23:09,440
left left side figure

00:23:06,960 --> 00:23:11,600
the performance are not perfect but in

00:23:09,440 --> 00:23:16,000
any case we have a gain of

00:23:11,600 --> 00:23:19,120
uh of time to solution performance

00:23:16,000 --> 00:23:22,640
since in this case the time to solution

00:23:19,120 --> 00:23:26,159
is slightly

00:23:22,640 --> 00:23:31,600
less than the global time step

00:23:26,159 --> 00:23:34,640
case passing to the last one

00:23:31,600 --> 00:23:37,360
code i have i think three minutes

00:23:34,640 --> 00:23:39,440
more or less i want to show you another

00:23:37,360 --> 00:23:42,799
code that is

00:23:39,440 --> 00:23:45,840
developed by the turmergata group of uh

00:23:42,799 --> 00:23:50,159
sauro-such and jack mafalcucci

00:23:45,840 --> 00:23:53,919
with the help of our colleague

00:23:50,159 --> 00:23:57,679
george amati and uh this code is

00:23:53,919 --> 00:24:01,200
a quite standard lattice bozap man

00:23:57,679 --> 00:24:04,559
code and

00:24:01,200 --> 00:24:09,840
that was written in fortran that support

00:24:04,559 --> 00:24:13,120
multi-block the parallel paradigm

00:24:09,840 --> 00:24:14,400
is used at its mpi or on pipu plus

00:24:13,120 --> 00:24:17,760
openmp and

00:24:14,400 --> 00:24:20,400
now also mpi plus open sec so

00:24:17,760 --> 00:24:21,919
in this case we are developing a code

00:24:20,400 --> 00:24:26,000
using not cuda but

00:24:21,919 --> 00:24:29,760
directly opposite ccd directory

00:24:26,000 --> 00:24:32,320
directives uh

00:24:29,760 --> 00:24:34,720
we want to try to keep performance

00:24:32,320 --> 00:24:37,840
portability also in the porting

00:24:34,720 --> 00:24:42,000
and here i don't

00:24:37,840 --> 00:24:44,480
want to show you the code but

00:24:42,000 --> 00:24:45,360
simply an example of the performance

00:24:44,480 --> 00:24:47,919
richard

00:24:45,360 --> 00:24:49,120
of a code that is right now in

00:24:47,919 --> 00:24:52,640
production

00:24:49,120 --> 00:24:56,559
uh the case uh we want to show you

00:24:52,640 --> 00:25:00,320
is the eu plectel aspergillum that

00:24:56,559 --> 00:25:03,520
is a sponge the la that live in the sea

00:25:00,320 --> 00:25:05,440
and uh that was uh quite interesting

00:25:03,520 --> 00:25:07,200
from from the computational freedom

00:25:05,440 --> 00:25:10,799
dynamics point of view

00:25:07,200 --> 00:25:15,600
since uh it modified the flux since

00:25:10,799 --> 00:25:18,799
that is inside the sponge itself

00:25:15,600 --> 00:25:22,320
uh the simulation that we perform use

00:25:18,799 --> 00:25:23,760
about 50 billions of grit points set one

00:25:22,320 --> 00:25:27,520
terabyte of ram

00:25:23,760 --> 00:25:30,640
and use about five day or simulation

00:25:27,520 --> 00:25:34,880
using 500 gpus on

00:25:30,640 --> 00:25:39,919
our system uh with this various system

00:25:34,880 --> 00:25:39,919
we use a brute force approach and

00:25:40,080 --> 00:25:46,559
as you can see from the speed

00:25:43,200 --> 00:25:48,000
weak scaling speed up we have more or

00:25:46,559 --> 00:25:52,320
less

00:25:48,000 --> 00:25:57,200
the same and quasi idle speed up

00:25:52,320 --> 00:26:00,080
the performance using 1000 gpu is uh

00:25:57,200 --> 00:26:00,880
the parallel efficiency is about 73

00:26:00,080 --> 00:26:04,240
percent

00:26:00,880 --> 00:26:06,960
of the total idle performance in this

00:26:04,240 --> 00:26:08,720
this is very good and to finish the

00:26:06,960 --> 00:26:12,720
presentation i want to show

00:26:08,720 --> 00:26:16,480
you the flux using

00:26:12,720 --> 00:26:19,760
different grid meshes to model uh

00:26:16,480 --> 00:26:23,679
the sponge and that's it

00:26:19,760 --> 00:26:27,200
if you have some

00:26:23,679 --> 00:26:31,200
no now i'm trying to

00:26:27,200 --> 00:26:34,799
block this the presentation

00:26:31,200 --> 00:26:38,559
and now i try to see

00:26:34,799 --> 00:26:42,799
also your comment but i didn't see

00:26:38,559 --> 00:26:42,799
any comment by now

00:26:44,559 --> 00:26:47,520
okay no

00:26:47,600 --> 00:26:55,440
no comment since there is no comment i

00:26:52,320 --> 00:26:58,559
want to stress

00:26:55,440 --> 00:26:58,559
the fact that

00:26:58,640 --> 00:27:04,159
we choose an as an example 3 cfd code

00:27:02,720 --> 00:27:07,360
but the codes

00:27:04,159 --> 00:27:10,240
are interesting uh

00:27:07,360 --> 00:27:11,919
from the scientific point of view but

00:27:10,240 --> 00:27:15,360
will be used

00:27:11,919 --> 00:27:15,679
in production and the approaches we use

00:27:15,360 --> 00:27:17,840
are

00:27:15,679 --> 00:27:19,039
different from the three code in the

00:27:17,840 --> 00:27:22,320
first one

00:27:19,039 --> 00:27:22,320
we has

00:27:23,840 --> 00:27:27,919
we has we used

00:27:29,039 --> 00:27:32,880
a couf directive procedure in the second

00:27:32,480 --> 00:27:35,760
one

00:27:32,880 --> 00:27:36,720
we use directory could afford and in the

00:27:35,760 --> 00:27:40,080
third one

00:27:36,720 --> 00:27:41,360
we use open as acc in any case but the

00:27:40,080 --> 00:27:44,480
method used

00:27:41,360 --> 00:27:48,399
to perform the

00:27:44,480 --> 00:27:52,320
to do the porting uh

00:27:48,399 --> 00:27:54,720
reached their scopes uh that is improv

00:27:52,320 --> 00:27:57,840
boosts the performance of the code of at

00:27:54,720 --> 00:28:00,960
least a factor four but in many cases

00:27:57,840 --> 00:28:04,640
more than these and

00:28:00,960 --> 00:28:07,840
it is not so difficult to do because

00:28:04,640 --> 00:28:12,080
the effort for a production code

00:28:07,840 --> 00:28:15,520
can be high but for smaller code

00:28:12,080 --> 00:28:18,559
it require only few months of work

00:28:15,520 --> 00:28:21,279
of a single person so

00:28:18,559 --> 00:28:21,279
i think that

00:28:21,919 --> 00:28:25,440
it is a very good and fast approach to

00:28:24,399 --> 00:28:27,919
do this

00:28:25,440 --> 00:28:28,840
but this i want to stress also the fact

00:28:27,919 --> 00:28:31,840
that

00:28:28,840 --> 00:28:35,120
on power system

00:28:31,840 --> 00:28:38,240
the performance of this code are

00:28:35,120 --> 00:28:39,120
even better because the use of the

00:28:38,240 --> 00:28:41,919
dedicated

00:28:39,120 --> 00:28:44,240
and link network boost the performance

00:28:41,919 --> 00:28:49,360
and the communication between

00:28:44,240 --> 00:28:51,200
the memory the main memory and the gpu

00:28:49,360 --> 00:28:54,080
memory and

00:28:51,200 --> 00:28:55,360
that's it i think that we have two

00:28:54,080 --> 00:28:59,679
minutes to

00:28:55,360 --> 00:29:02,480
the end so therefore this is the

00:28:59,679 --> 00:29:02,480
moment to

00:29:02,880 --> 00:29:09,840
to ask a question

00:29:06,480 --> 00:29:12,240
if not uh thank you for

00:29:09,840 --> 00:29:13,919
attending the meeting and thank you also

00:29:12,240 --> 00:29:16,960
to the

00:29:13,919 --> 00:29:16,960
staff of uh

00:29:17,600 --> 00:29:24,000
not yet i see now a question it used

00:29:21,440 --> 00:29:26,640
intensive code for power nine not yet

00:29:24,000 --> 00:29:30,480
but probably in the future in order to

00:29:26,640 --> 00:29:32,960
better optimize for our machine probably

00:29:30,480 --> 00:29:34,480
some power 9 instruction will be

00:29:32,960 --> 00:29:37,520
inserted

00:29:34,480 --> 00:29:40,640
and that's it thank you again to

00:29:37,520 --> 00:29:45,360
the organizer and to the tenders

00:29:40,640 --> 00:29:45,360

YouTube URL: https://www.youtube.com/watch?v=zsK_83J5sgo


