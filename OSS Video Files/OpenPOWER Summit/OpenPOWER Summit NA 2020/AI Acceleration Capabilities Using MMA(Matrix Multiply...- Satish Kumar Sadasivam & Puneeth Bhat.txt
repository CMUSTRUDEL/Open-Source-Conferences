Title: AI Acceleration Capabilities Using MMA(Matrix Multiply...- Satish Kumar Sadasivam & Puneeth Bhat
Publication date: 2020-09-21
Playlist: OpenPOWER Summit NA 2020
Description: 
	AI Acceleration Capabilities Using MMA(Matrix Multiply Assist) on POWER Architecture - Satish Kumar Sadasivam & Puneeth Bhat AH, IBM

Speakers: Satish Kumar Sadasivam, Puneeth Bhat A H

Power ISA v3.01 has introduced a new set of architecture capabilities to accelerate matrix math operations called MMA (Matrix Multiply Assist). This will improve the performance of key compute kernels such as Matrix Multiplication, Convolution, Fourier Transform etc. which are fundamental building blocks for ML/DL workloads. This talk will introduce the audience to the POWER ISA MMA capabilities and detail the following: MMAâ€™s architectural features, new instructions and lower/mixed precision data type support. The talk will cover programming a simple matrix multiplication function using MMA code and differentiate it from the traditional VSX (Vector Scalar Extension) based code sequence. The talk will delve into details of exploiting the full capability of MMA with assembly-level optimal code sequences for different data types. This talk will also cover compiler built-ins from GCC-10 and discuss exploitation of MMA capability in ML/DL frameworks using MMA-enabled OpenBLAS library.
Captions: 
	00:00:01,439 --> 00:00:05,440
hello everyone in today's talk

00:00:03,840 --> 00:00:07,520
we are going to take you through a hot

00:00:05,440 --> 00:00:10,240
topic in the compute world

00:00:07,520 --> 00:00:10,880
air acceleration on chip we are going to

00:00:10,240 --> 00:00:13,200
show you

00:00:10,880 --> 00:00:14,160
how to exploit matrix math assist

00:00:13,200 --> 00:00:16,640
architecture

00:00:14,160 --> 00:00:20,880
in short known as mma for ai

00:00:16,640 --> 00:00:20,880
acceleration on powerton microprocessor

00:00:24,160 --> 00:00:27,519
my colleague puneet bhatt and i will

00:00:26,800 --> 00:00:30,400
take you through

00:00:27,519 --> 00:00:32,640
the following topics first i will cover

00:00:30,400 --> 00:00:34,320
the basics of my mma architecture

00:00:32,640 --> 00:00:36,320
we'll discuss details on the register

00:00:34,320 --> 00:00:38,160
handling supported data types

00:00:36,320 --> 00:00:39,680
and the compute instructions supported

00:00:38,160 --> 00:00:41,920
by mma

00:00:39,680 --> 00:00:44,559
i'll also introduce you to the recently

00:00:41,920 --> 00:00:47,039
announced ibm power 10 microprocessor

00:00:44,559 --> 00:00:49,120
and will discuss its mma compute and

00:00:47,039 --> 00:00:51,199
bandwidth capabilities

00:00:49,120 --> 00:00:53,680
puneet will cover the basics of mma

00:00:51,199 --> 00:00:55,360
programming using gcc built-ins

00:00:53,680 --> 00:00:57,520
will take you through a simple mma

00:00:55,360 --> 00:00:59,359
programming example and will touch base

00:00:57,520 --> 00:01:01,120
on open blast support

00:00:59,359 --> 00:01:02,719
he will conclude the talk with the

00:01:01,120 --> 00:01:07,199
overall performance uplifts

00:01:02,719 --> 00:01:07,199
achieved in power 10 for ai workloads

00:01:09,439 --> 00:01:13,040
mma architecture support is introduced

00:01:11,840 --> 00:01:16,560
in power isa

00:01:13,040 --> 00:01:17,520
version 3.1 this introduces a new set of

00:01:16,560 --> 00:01:19,360
instructions

00:01:17,520 --> 00:01:21,200
along with required architecture

00:01:19,360 --> 00:01:24,880
enhancements in register handling

00:01:21,200 --> 00:01:27,200
to support dense matrix math operations

00:01:24,880 --> 00:01:28,159
dense numerical linear linear algebra

00:01:27,200 --> 00:01:30,479
computations

00:01:28,159 --> 00:01:32,640
are the core of the emerging ai machine

00:01:30,479 --> 00:01:35,119
learning and deep learning workloads

00:01:32,640 --> 00:01:35,680
this mma architecture will cater to the

00:01:35,119 --> 00:01:38,000
needs

00:01:35,680 --> 00:01:39,280
to fulfill the huge compute demands

00:01:38,000 --> 00:01:42,960
presented by the m

00:01:39,280 --> 00:01:44,799
dl workloads there is a wide scope for

00:01:42,960 --> 00:01:45,920
innovation to apply these fundamental

00:01:44,799 --> 00:01:48,240
math operations

00:01:45,920 --> 00:01:50,079
supported in mma for other computations

00:01:48,240 --> 00:01:52,720
like first fourier transform

00:01:50,079 --> 00:01:55,840
arbitrary precision arithmetic extra

00:01:52,720 --> 00:01:55,840
moving on to the next slide

00:01:56,560 --> 00:02:00,719
power isa has support for 64 vector

00:01:59,439 --> 00:02:02,719
scalar registers

00:02:00,719 --> 00:02:05,119
to fulfill all the compute requirements

00:02:02,719 --> 00:02:07,360
of the vsx instruction set

00:02:05,119 --> 00:02:08,160
as part of mma architecture eight

00:02:07,360 --> 00:02:11,360
registers

00:02:08,160 --> 00:02:12,959
each of size 512 bit called accumulators

00:02:11,360 --> 00:02:15,520
are introduced

00:02:12,959 --> 00:02:17,920
these are software managed shadow to a

00:02:15,520 --> 00:02:21,520
set of four consecutive vsrs

00:02:17,920 --> 00:02:24,560
as shown in the figure 32 vsrs

00:02:21,520 --> 00:02:25,520
0 to 31 are mapped to 8 accumulator

00:02:24,560 --> 00:02:29,280
registers

00:02:25,520 --> 00:02:32,160
0 to 7. users must choose

00:02:29,280 --> 00:02:33,680
to either use the register as vsrs or

00:02:32,160 --> 00:02:37,200
accumulators

00:02:33,680 --> 00:02:39,519
it cannot be used both ways state

00:02:37,200 --> 00:02:40,480
must be explicitly transferred between

00:02:39,519 --> 00:02:43,760
the vsrs

00:02:40,480 --> 00:02:45,519
and the accumulators in order to load

00:02:43,760 --> 00:02:47,519
the data into the accumulator

00:02:45,519 --> 00:02:48,640
users should first load the values into

00:02:47,519 --> 00:02:51,280
the vsrs

00:02:48,640 --> 00:02:53,280
once the data is loaded to vsr the state

00:02:51,280 --> 00:02:57,440
can be transferred to accumulator

00:02:53,280 --> 00:02:59,519
using vsx move to accumulate instruction

00:02:57,440 --> 00:03:00,560
after the computation is done the values

00:02:59,519 --> 00:03:03,120
can be brought back

00:03:00,560 --> 00:03:05,840
from accumulator to vsrs using the move

00:03:03,120 --> 00:03:08,319
vsx move from accumulator instruction

00:03:05,840 --> 00:03:09,280
once the data is available in bsr other

00:03:08,319 --> 00:03:12,400
instructions

00:03:09,280 --> 00:03:14,959
can operate on this data moving on to

00:03:12,400 --> 00:03:14,959
the next slide

00:03:15,760 --> 00:03:19,120
mma architecture support math operations

00:03:18,480 --> 00:03:22,720
on four

00:03:19,120 --> 00:03:26,400
different floating point data types fp64

00:03:22,720 --> 00:03:28,400
fp32 fp16 and beef root 16.

00:03:26,400 --> 00:03:30,000
in addition it also supports three

00:03:28,400 --> 00:03:33,360
integer data types

00:03:30,000 --> 00:03:35,360
in 16 in 8 and m4

00:03:33,360 --> 00:03:36,640
these diverse set of precision levels

00:03:35,360 --> 00:03:38,959
and data type support

00:03:36,640 --> 00:03:40,080
is provided to cater to the needs of

00:03:38,959 --> 00:03:42,720
on-ship ai

00:03:40,080 --> 00:03:44,640
inference acceleration equivalent

00:03:42,720 --> 00:03:46,400
compute instructions are supported

00:03:44,640 --> 00:03:49,360
to handle each of the above mentioned

00:03:46,400 --> 00:03:52,879
data types each of the mma instruction

00:03:49,360 --> 00:03:56,080
takes 228 bit input values

00:03:52,879 --> 00:03:56,720
and it generates a single 512 bit output

00:03:56,080 --> 00:03:58,480
value

00:03:56,720 --> 00:04:00,879
which can get accumulated to the

00:03:58,480 --> 00:04:02,879
respective accumulators

00:04:00,879 --> 00:04:04,560
this is explained in more detail in the

00:04:02,879 --> 00:04:06,239
next slide

00:04:04,560 --> 00:04:08,080
to highlight the capability of the

00:04:06,239 --> 00:04:11,599
architecture a single

00:04:08,080 --> 00:04:15,519
fp32 mma instruction can perform

00:04:11,599 --> 00:04:18,560
16 multiply add operations in one go

00:04:15,519 --> 00:04:21,519
fp 16 and 816 mma instruction

00:04:18,560 --> 00:04:23,680
can perform 32 multiply add operations

00:04:21,519 --> 00:04:26,560
intake can perform 64.

00:04:23,680 --> 00:04:27,280
in 4 can perform 128 multiplier

00:04:26,560 --> 00:04:30,639
operations

00:04:27,280 --> 00:04:32,240
using a single instruction the outer

00:04:30,639 --> 00:04:34,720
product mma instruction

00:04:32,240 --> 00:04:36,320
can be read using the format mentioned

00:04:34,720 --> 00:04:39,759
in the slide

00:04:36,320 --> 00:04:40,800
type and rank determines what data type

00:04:39,759 --> 00:04:44,240
you are operating on

00:04:40,800 --> 00:04:46,639
and the respective ram for the operation

00:04:44,240 --> 00:04:48,000
mma instruction also supports lane

00:04:46,639 --> 00:04:50,320
masking feature

00:04:48,000 --> 00:04:51,199
where it provides capability to reduce

00:04:50,320 --> 00:04:57,120
the number of

00:04:51,199 --> 00:04:59,840
multiplier add by masking certain input

00:04:57,120 --> 00:05:00,800
values and produce less than 512 big

00:04:59,840 --> 00:05:04,400
output

00:05:00,800 --> 00:05:06,639
moving on to the next slide this slide

00:05:04,400 --> 00:05:08,320
gives you two examples of how mma

00:05:06,639 --> 00:05:12,080
instructions operate

00:05:08,320 --> 00:05:14,720
the first picture shows how a fp 32-bit

00:05:12,080 --> 00:05:17,600
mma instruction will operate this

00:05:14,720 --> 00:05:18,479
instruction takes 228 bit values as

00:05:17,600 --> 00:05:22,320
inputs

00:05:18,479 --> 00:05:24,960
input 1 has 4 32-bit values input 2 has

00:05:22,320 --> 00:05:26,880
4 32-bit values each of this 32-bit

00:05:24,960 --> 00:05:29,199
value will get multiplied with

00:05:26,880 --> 00:05:30,560
each of the other 32-bit value to

00:05:29,199 --> 00:05:33,680
generate 16

00:05:30,560 --> 00:05:36,000
32-bit outputs this

00:05:33,680 --> 00:05:37,680
in turn will get accumulated into the

00:05:36,000 --> 00:05:40,080
accumulated register

00:05:37,680 --> 00:05:41,360
we can loop on this operation till the

00:05:40,080 --> 00:05:44,400
entire math function

00:05:41,360 --> 00:05:46,400
is complete the second diagram is a

00:05:44,400 --> 00:05:47,840
representation of an int 8 mma

00:05:46,400 --> 00:05:51,440
instruction

00:05:47,840 --> 00:05:54,400
input 1 has 16 8-bit values input 2 has

00:05:51,440 --> 00:05:54,880
68 bit values please note that here the

00:05:54,400 --> 00:05:57,840
first

00:05:54,880 --> 00:05:59,440
4 8 bit values of input 1 gets

00:05:57,840 --> 00:06:03,199
multiplied with the respective

00:05:59,440 --> 00:06:05,919
4 8 8 bit values of input 2 and these 4

00:06:03,199 --> 00:06:07,600
output results will get summed up to get

00:06:05,919 --> 00:06:10,720
a 32-bit output

00:06:07,600 --> 00:06:11,680
overall 16 such 32-bit outputs will get

00:06:10,720 --> 00:06:13,600
generated

00:06:11,680 --> 00:06:15,680
as part of this whole operation to

00:06:13,600 --> 00:06:17,919
generate a 512-bit output

00:06:15,680 --> 00:06:19,280
which in turn will get much accumulated

00:06:17,919 --> 00:06:22,240
to an accumulator

00:06:19,280 --> 00:06:23,680
in total 64 multiplier operations will

00:06:22,240 --> 00:06:26,800
be performed

00:06:23,680 --> 00:06:26,800
moving on to the next slide

00:06:28,160 --> 00:06:32,560
ibm recently announced this power 10

00:06:30,479 --> 00:06:35,199
microprocessor which is fully compliant

00:06:32,560 --> 00:06:37,440
with power isa version 3.1

00:06:35,199 --> 00:06:39,199
powertrain processor can support up to

00:06:37,440 --> 00:06:42,800
30 smt-8 cores

00:06:39,199 --> 00:06:44,080
or 60 smt4 cores in a single socket dcm

00:06:42,800 --> 00:06:46,160
module

00:06:44,080 --> 00:06:47,280
the diagram in the top left shows a

00:06:46,160 --> 00:06:50,560
picture of

00:06:47,280 --> 00:06:52,560
a power 10 smt8 core

00:06:50,560 --> 00:06:54,639
this core can be partitioned into two

00:06:52,560 --> 00:06:57,120
smp4 cores

00:06:54,639 --> 00:06:58,960
each smt8 core can support 4 mma

00:06:57,120 --> 00:07:01,280
instruction in a single cycle

00:06:58,960 --> 00:07:03,440
in terms of blocks capability for double

00:07:01,280 --> 00:07:04,080
precision it can support 64 blocks per

00:07:03,440 --> 00:07:06,160
cycle

00:07:04,080 --> 00:07:09,520
and it goes all the way up to 1024

00:07:06,160 --> 00:07:11,840
operations per cycle for info data type

00:07:09,520 --> 00:07:12,960
apart from the mma unit power 10 also

00:07:11,840 --> 00:07:16,319
has capability

00:07:12,960 --> 00:07:20,160
to perform eight regular cmd

00:07:16,319 --> 00:07:21,919
operations per smd8 core to support

00:07:20,160 --> 00:07:24,160
regular simply operations through vsx

00:07:21,919 --> 00:07:26,560
instructions

00:07:24,160 --> 00:07:29,039
in comparison to power 9 power 10 has

00:07:26,560 --> 00:07:32,880
two times the regular cmd capability

00:07:29,039 --> 00:07:36,400
and a brand new 4 512 bit matrix math

00:07:32,880 --> 00:07:39,120
acceleration engine per st8 core

00:07:36,400 --> 00:07:40,160
which can generate 2048 bits of output

00:07:39,120 --> 00:07:42,319
every cycle

00:07:40,160 --> 00:07:43,440
these huge compute capabilities will

00:07:42,319 --> 00:07:45,840
help accelerate

00:07:43,440 --> 00:07:47,280
ai infused workflows in the enterprise

00:07:45,840 --> 00:07:48,960
market

00:07:47,280 --> 00:07:51,039
now that we have seen the compute

00:07:48,960 --> 00:07:54,240
capabilities it is very important to

00:07:51,039 --> 00:07:56,319
feed these mma and simply engines

00:07:54,240 --> 00:07:58,080
in order to support the data demand to

00:07:56,319 --> 00:08:00,080
feed these huge compute engines

00:07:58,080 --> 00:08:01,919
the data bandwidth has been doubled

00:08:00,080 --> 00:08:04,720
starting from the oma interface

00:08:01,919 --> 00:08:05,680
all the way up to l1 cache features like

00:08:04,720 --> 00:08:08,080
fusion

00:08:05,680 --> 00:08:10,560
32-bit load instruction support aids

00:08:08,080 --> 00:08:13,680
these data demands for the mma unit

00:08:10,560 --> 00:08:14,960
overall each smt8 core can support four

00:08:13,680 --> 00:08:18,080
32 byte loads

00:08:14,960 --> 00:08:19,360
and two 32 byte stores every cycle from

00:08:18,080 --> 00:08:21,520
l1 cache

00:08:19,360 --> 00:08:22,560
with this i will hand it over to my

00:08:21,520 --> 00:08:26,319
colleague

00:08:22,560 --> 00:08:26,319
to cover the rest of the agenda

00:08:32,880 --> 00:08:36,880
from here on i will introduce few

00:08:35,279 --> 00:08:39,440
programming aspects of

00:08:36,880 --> 00:08:40,000
mma in the following section and

00:08:39,440 --> 00:08:44,159
conclude

00:08:40,000 --> 00:08:44,159
the talk with few performance numbers

00:08:44,399 --> 00:08:48,880
so programming these mma encore

00:08:47,040 --> 00:08:52,000
accelerators have been made

00:08:48,880 --> 00:08:53,040
easy by the compiler built-ins provided

00:08:52,000 --> 00:08:56,080
by the latest

00:08:53,040 --> 00:08:59,519
new c compilers

00:08:56,080 --> 00:09:03,680
these extensions can be enabled in gcc

00:08:59,519 --> 00:09:06,399
by providing power 10 cpu flag

00:09:03,680 --> 00:09:07,279
as described in the table there are

00:09:06,399 --> 00:09:09,519
various

00:09:07,279 --> 00:09:10,800
built-in types to support all

00:09:09,519 --> 00:09:14,880
functionalities

00:09:10,800 --> 00:09:18,080
and instructions of mma

00:09:14,880 --> 00:09:20,880
new data type of vector quad can be used

00:09:18,080 --> 00:09:23,680
to specify an accumulator

00:09:20,880 --> 00:09:27,839
and a normal vsx variable can be

00:09:23,680 --> 00:09:27,839
specified using vector data types

00:09:28,399 --> 00:09:33,120
there are apis to assemble or

00:09:30,560 --> 00:09:33,440
disassemble the accumulators to be used

00:09:33,120 --> 00:09:38,000
as

00:09:33,440 --> 00:09:41,360
space or independent vsx registers

00:09:38,000 --> 00:09:44,399
one can also make use of move from

00:09:41,360 --> 00:09:45,920
or two instructions to move data across

00:09:44,399 --> 00:09:49,040
vsx registers

00:09:45,920 --> 00:09:52,080
and accumulators all

00:09:49,040 --> 00:09:56,000
sets of ger instructions that is

00:09:52,080 --> 00:09:59,440
matrix multiply instructions are covered

00:09:56,000 --> 00:10:02,079
to perform just multiply or multiply

00:09:59,440 --> 00:10:02,880
accumulate or multiply negate and

00:10:02,079 --> 00:10:06,880
accumulate

00:10:02,880 --> 00:10:10,079
with various suffixes these combinations

00:10:06,880 --> 00:10:14,079
are useful in computing complex and

00:10:10,079 --> 00:10:17,120
real type data types the prefix

00:10:14,079 --> 00:10:20,399
or mask instructions are also supported

00:10:17,120 --> 00:10:23,680
by the compiler built-ins wherein masks

00:10:20,399 --> 00:10:27,519
can be specified as unsigned info

00:10:23,680 --> 00:10:28,320
flags these prefixed or masked

00:10:27,519 --> 00:10:31,360
instructions

00:10:28,320 --> 00:10:32,000
can be used for partial operations in

00:10:31,360 --> 00:10:34,959
boundary

00:10:32,000 --> 00:10:36,160
conditions of a matrix multiplication

00:10:34,959 --> 00:10:40,240
where the sizes

00:10:36,160 --> 00:10:40,240
are not multiples of four

00:10:41,480 --> 00:10:46,720
resistance unpairing apis are also

00:10:44,640 --> 00:10:47,519
available to be used with double

00:10:46,720 --> 00:10:50,560
precision

00:10:47,519 --> 00:10:53,040
ger instructions

00:10:50,560 --> 00:10:54,480
these instructions are little different

00:10:53,040 --> 00:10:58,079
unlike others

00:10:54,480 --> 00:11:01,839
as it takes a pair of vsx registers

00:10:58,079 --> 00:11:03,680
for first matrix and one vsx register

00:11:01,839 --> 00:11:06,640
for the second matrix

00:11:03,680 --> 00:11:07,440
resulting in a 4 cross 2 smaller

00:11:06,640 --> 00:11:11,279
resultant

00:11:07,440 --> 00:11:14,240
sub matrix the full list of

00:11:11,279 --> 00:11:17,440
all available built-ins can be found in

00:11:14,240 --> 00:11:17,440
the links specified

00:11:19,519 --> 00:11:22,240
moving on

00:11:22,560 --> 00:11:28,800
in this section we present a simple see

00:11:25,920 --> 00:11:30,240
example of a single precision matrix

00:11:28,800 --> 00:11:33,760
multiplication

00:11:30,240 --> 00:11:34,480
using vector outer product using gsa

00:11:33,760 --> 00:11:37,839
built-ins

00:11:34,480 --> 00:11:40,959
to accelerate on mma

00:11:37,839 --> 00:11:43,600
the accumulator acc underscore t

00:11:40,959 --> 00:11:44,800
defined in the code section is defined

00:11:43,600 --> 00:11:48,240
as a vector quad

00:11:44,800 --> 00:11:52,000
type and initially is set to zero

00:11:48,240 --> 00:11:52,000
using the built-in api

00:11:53,440 --> 00:11:57,519
can also be achieved in optimized

00:11:55,519 --> 00:12:01,360
kernels by pre-loading

00:11:57,519 --> 00:12:04,079
partial result sums to vsx registers

00:12:01,360 --> 00:12:04,800
and moving data across accumulators

00:12:04,079 --> 00:12:09,360
using

00:12:04,800 --> 00:12:13,040
move to instructions the compute kernel

00:12:09,360 --> 00:12:16,240
loops over the transposed first matrix

00:12:13,040 --> 00:12:19,760
and b matrix over k rows

00:12:16,240 --> 00:12:24,160
in strides of m and n to compute

00:12:19,760 --> 00:12:26,360
1 4 cross 4 resultant sub matrix

00:12:24,160 --> 00:12:27,760
finally the accumulators are

00:12:26,360 --> 00:12:31,040
disassembled

00:12:27,760 --> 00:12:34,079
and stored back to the resultant matrix

00:12:31,040 --> 00:12:37,200
in strides of 4.

00:12:34,079 --> 00:12:38,160
to compute the full resultant matrix it

00:12:37,200 --> 00:12:41,600
is required

00:12:38,160 --> 00:12:43,920
to loop over whole of m columns of the

00:12:41,600 --> 00:12:47,760
transpose matrix a

00:12:43,920 --> 00:12:49,360
and in columns of matrix b in strides of

00:12:47,760 --> 00:12:52,800
four

00:12:49,360 --> 00:12:56,079
so the same kernel can be called m by 4

00:12:52,800 --> 00:12:59,200
into n by 4 times to generate full

00:12:56,079 --> 00:13:02,399
resultant matrix which is not shown

00:12:59,200 --> 00:13:02,399
but assumed here

00:13:02,480 --> 00:13:06,639
at the right hand side we present with

00:13:04,800 --> 00:13:10,320
the detail diagram

00:13:06,639 --> 00:13:14,000
and advanced and optimized sjm kernel

00:13:10,320 --> 00:13:16,880
generation procedure here

00:13:14,000 --> 00:13:18,000
the same four cross four example is

00:13:16,880 --> 00:13:21,040
optimized

00:13:18,000 --> 00:13:24,320
using eight accumulators and

00:13:21,040 --> 00:13:27,920
reusing the same loaded a and b

00:13:24,320 --> 00:13:31,920
matrix values to generate a bigger

00:13:27,920 --> 00:13:35,839
8 cross 16 resultant sub matrix

00:13:31,920 --> 00:13:36,240
this results in two plus four which is

00:13:35,839 --> 00:13:40,000
six

00:13:36,240 --> 00:13:43,199
loads and eight compute instructions

00:13:40,000 --> 00:13:45,839
with a better optimized and faster

00:13:43,199 --> 00:13:45,839
gem kernel

00:13:46,480 --> 00:13:50,160
moving on to the next slide

00:13:53,199 --> 00:13:57,040
describes mixed and lower precision

00:13:56,240 --> 00:14:00,480
support

00:13:57,040 --> 00:14:04,480
in mma to perform a mixed

00:14:00,480 --> 00:14:08,560
or lower precision matrix multiplication

00:14:04,480 --> 00:14:09,360
operation using mma the input matrices

00:14:08,560 --> 00:14:13,199
needs to be

00:14:09,360 --> 00:14:16,160
transformed since the result matrix

00:14:13,199 --> 00:14:17,760
for any lower precision matrix math is

00:14:16,160 --> 00:14:21,040
still 32-bit

00:14:17,760 --> 00:14:24,079
one accumulator can still hold a 4 cross

00:14:21,040 --> 00:14:26,959
4 resultant sub-matrix

00:14:24,079 --> 00:14:28,320
to achieve the same and as described

00:14:26,959 --> 00:14:31,600
previously

00:14:28,320 --> 00:14:35,040
isa performs internal multiply

00:14:31,600 --> 00:14:37,360
accumulations within a subset and then

00:14:35,040 --> 00:14:40,000
accumulates the partial result to a

00:14:37,360 --> 00:14:40,000
bigger set

00:14:40,160 --> 00:14:44,000
a simple transformation for an int 8

00:14:43,440 --> 00:14:47,040
matrix

00:14:44,000 --> 00:14:47,600
multiplication is explained in the

00:14:47,040 --> 00:14:50,800
figure

00:14:47,600 --> 00:14:53,199
at the right hand side

00:14:50,800 --> 00:14:54,560
similar transformations can be performed

00:14:53,199 --> 00:14:57,600
for infor

00:14:54,560 --> 00:14:59,199
which extends and include 8 elements

00:14:57,600 --> 00:15:02,480
within the subset

00:14:59,199 --> 00:15:04,959
and for in 16 and half precision

00:15:02,480 --> 00:15:07,440
this gets reduced to two elements in the

00:15:04,959 --> 00:15:07,440
subset

00:15:08,720 --> 00:15:12,399
moving on to the next slide where we

00:15:11,519 --> 00:15:16,880
discuss

00:15:12,399 --> 00:15:20,160
on the community adoption of mma

00:15:16,880 --> 00:15:20,639
for community adoption mma support has

00:15:20,160 --> 00:15:24,959
been

00:15:20,639 --> 00:15:27,760
enabled in the latest open blast library

00:15:24,959 --> 00:15:28,160
open glass should be built with power 10

00:15:27,760 --> 00:15:31,120
as

00:15:28,160 --> 00:15:31,839
core and lip core to enable a memory

00:15:31,120 --> 00:15:35,680
kernels

00:15:31,839 --> 00:15:39,199
for l3 blast routines

00:15:35,680 --> 00:15:42,800
currently support is available for float

00:15:39,199 --> 00:15:46,560
double complex and real type gem

00:15:42,800 --> 00:15:49,199
and trmm kernels which are triangular

00:15:46,560 --> 00:15:52,720
matrix multiplication

00:15:49,199 --> 00:15:53,120
using this open class support libraries

00:15:52,720 --> 00:15:56,240
like

00:15:53,120 --> 00:15:59,680
numpy and frameworks like pytorch can

00:15:56,240 --> 00:16:02,880
make use of mma encore accelerator

00:15:59,680 --> 00:16:05,360
readily to optimize real time machine

00:16:02,880 --> 00:16:09,440
learning applications and deep learning

00:16:05,360 --> 00:16:10,800
inference workloads easy integration can

00:16:09,440 --> 00:16:13,839
be possible with

00:16:10,800 --> 00:16:18,000
any library or framework which can link

00:16:13,839 --> 00:16:21,519
to open blast for class routines

00:16:18,000 --> 00:16:24,880
most common hpc and a applications

00:16:21,519 --> 00:16:28,240
which include compute patterns of gem

00:16:24,880 --> 00:16:31,759
convolution clustering algorithms

00:16:28,240 --> 00:16:34,800
variance of fast fourier transform using

00:16:31,759 --> 00:16:35,680
vector outer product etc can get

00:16:34,800 --> 00:16:40,079
benefited

00:16:35,680 --> 00:16:40,079
by mma encore accelerators

00:16:40,399 --> 00:16:48,560
finally looking on the performance

00:16:43,680 --> 00:16:50,880
aspect as per our pre-silicon-based

00:16:48,560 --> 00:16:52,079
lab experiments and performance

00:16:50,880 --> 00:16:55,040
assessments

00:16:52,079 --> 00:16:55,920
powerton systems with mma shows

00:16:55,040 --> 00:16:59,120
significant

00:16:55,920 --> 00:17:02,560
improvements in compute and hence

00:16:59,120 --> 00:17:05,839
improved ai applications

00:17:02,560 --> 00:17:08,640
open blast sgm shows more than 4x

00:17:05,839 --> 00:17:09,679
per core performance improvements on an

00:17:08,640 --> 00:17:14,160
average

00:17:09,679 --> 00:17:18,240
for multiple matrix sizes compared to p9

00:17:14,160 --> 00:17:18,959
traditional linpack and fp32 resnet 50

00:17:18,240 --> 00:17:22,079
inference

00:17:18,959 --> 00:17:25,280
workloads shows around 10x

00:17:22,079 --> 00:17:28,400
improvements lower precision

00:17:25,280 --> 00:17:32,000
resonant 50 inference workloads shows

00:17:28,400 --> 00:17:34,480
up to 20x improvements as compared to p9

00:17:32,000 --> 00:17:34,480
systems

00:17:37,200 --> 00:17:43,520
today's presentation on matrix

00:17:40,480 --> 00:18:01,840
math assist for ai acceleration

00:17:43,520 --> 00:18:01,840
on power 10 thank you for listening

00:18:26,840 --> 00:18:29,840
uh

00:18:41,120 --> 00:18:47,679
okay i see a few questions

00:18:44,559 --> 00:18:52,640
popping in in the q a section

00:18:47,679 --> 00:18:52,640
so let me try to answer some of those

00:18:55,679 --> 00:19:00,960
so one question from mandy uh is in

00:18:58,400 --> 00:19:02,480
theory no code changes required assuming

00:19:00,960 --> 00:19:04,480
you compile your frameworks using

00:19:02,480 --> 00:19:06,320
openglass yes i think you're right uh we

00:19:04,480 --> 00:19:07,520
have the optimized kernel integrated as

00:19:06,320 --> 00:19:09,440
part of the

00:19:07,520 --> 00:19:11,039
open glass uh our open blast library

00:19:09,440 --> 00:19:11,600
team did a tremendous job in getting

00:19:11,039 --> 00:19:14,559
that done

00:19:11,600 --> 00:19:15,440
quickly so uh we we can just take the

00:19:14,559 --> 00:19:17,280
frameworks

00:19:15,440 --> 00:19:19,360
uh just link it to the optimized open

00:19:17,280 --> 00:19:20,960
glass and you should have your framework

00:19:19,360 --> 00:19:22,480
running as long as your framework

00:19:20,960 --> 00:19:24,880
supports that particular

00:19:22,480 --> 00:19:25,679
open blast version of library so that's

00:19:24,880 --> 00:19:30,080
that's the

00:19:25,679 --> 00:19:30,080
answer for that question um

00:19:35,760 --> 00:19:39,760
yeah so one of the uh i mean so the

00:19:38,799 --> 00:19:41,840
other question from

00:19:39,760 --> 00:19:43,600
i think mandy or the optimized

00:19:41,840 --> 00:19:45,440
frameworks which are part of open blast

00:19:43,600 --> 00:19:49,120
listed somewhere

00:19:45,440 --> 00:19:50,240
um okay so one example is by torch

00:19:49,120 --> 00:19:52,640
company do you want to take that

00:19:50,240 --> 00:19:56,160
question um

00:19:52,640 --> 00:19:59,280
yeah sure so uh so currently as

00:19:56,160 --> 00:20:03,039
as we mentioned it's been enabled for uh

00:19:59,280 --> 00:20:05,840
those four ten routines uh like

00:20:03,039 --> 00:20:06,559
the sdm the double precision complex and

00:20:05,840 --> 00:20:09,840
real

00:20:06,559 --> 00:20:12,880
so those are the kernels which currently

00:20:09,840 --> 00:20:15,360
use mma instructions

00:20:12,880 --> 00:20:16,240
as part of both the matrix

00:20:15,360 --> 00:20:19,760
multiplication

00:20:16,240 --> 00:20:23,280
and drm kernels so

00:20:19,760 --> 00:20:26,000
you can look at these kernels

00:20:23,280 --> 00:20:26,480
within the open glass but i'm not sure

00:20:26,000 --> 00:20:29,679
if

00:20:26,480 --> 00:20:30,240
any documentation exists as such on the

00:20:29,679 --> 00:20:35,840
list of

00:20:30,240 --> 00:20:35,840
these anywhere in open black

00:20:36,640 --> 00:20:42,320
okay the next question comes from vipin

00:20:40,240 --> 00:20:43,360
can you share the number of instructions

00:20:42,320 --> 00:20:46,480
and cycle count

00:20:43,360 --> 00:20:49,760
for performing the

00:20:46,480 --> 00:20:51,679
mma instruction uh

00:20:49,760 --> 00:20:53,440
so if i understand right are you asking

00:20:51,679 --> 00:20:56,720
about if you are asking about the

00:20:53,440 --> 00:20:59,440
number of loads to the number of mma

00:20:56,720 --> 00:21:02,960
instructions yes we do have the numbers

00:20:59,440 --> 00:21:06,799
in terms of cycle count uh it depends

00:21:02,960 --> 00:21:10,799
i'm not okay yes so

00:21:06,799 --> 00:21:14,240
um it's six loads for eight mma

00:21:10,799 --> 00:21:17,280
compute instructions right uh

00:21:14,240 --> 00:21:20,080
that's the number we have right i mean

00:21:17,280 --> 00:21:20,720
i'm not sure if i understood the

00:21:20,080 --> 00:21:24,000
question

00:21:20,720 --> 00:21:26,960
correctly but if it is like

00:21:24,000 --> 00:21:27,840
uh how many instructions are required to

00:21:26,960 --> 00:21:31,200
perform

00:21:27,840 --> 00:21:34,480
one four cross 4 matrix multiplication

00:21:31,200 --> 00:21:37,840
then it is just one as explained from

00:21:34,480 --> 00:21:41,039
the isa but if it is like

00:21:37,840 --> 00:21:43,520
how much is required for an m

00:21:41,039 --> 00:21:44,159
cross and matrix multiplication i think

00:21:43,520 --> 00:21:47,440
it depends

00:21:44,159 --> 00:21:50,799
on how optimized kernel you write

00:21:47,440 --> 00:21:51,679
and as per the optimized kernel which

00:21:50,799 --> 00:21:54,559
was shown

00:21:51,679 --> 00:21:55,440
which which can compute at most 8 cross

00:21:54,559 --> 00:21:59,120
00:21:55,440 --> 00:22:02,559
matrix sub matrix at a time so for that

00:21:59,120 --> 00:22:05,840
we need eight loads versus

00:22:02,559 --> 00:22:06,720
uh six loads versus eight compute

00:22:05,840 --> 00:22:09,200
instructions

00:22:06,720 --> 00:22:10,480
to generate one eight cross sixteen

00:22:09,200 --> 00:22:13,440
matrix

00:22:10,480 --> 00:22:14,159
right and and power 10 supports uh at

00:22:13,440 --> 00:22:16,799
smt

00:22:14,159 --> 00:22:19,280
uh four core level it supports two mma

00:22:16,799 --> 00:22:22,480
instructions as described in the

00:22:19,280 --> 00:22:24,640
slide so effectively you can do every

00:22:22,480 --> 00:22:26,400
cycle to mma instructions and uh

00:22:24,640 --> 00:22:28,880
open glass if you are like having the

00:22:26,400 --> 00:22:31,600
right input data set right you will

00:22:28,880 --> 00:22:32,320
see the max utilization of the mma unit

00:22:31,600 --> 00:22:34,400
uh

00:22:32,320 --> 00:22:35,919
to large extent right it will be there

00:22:34,400 --> 00:22:37,760
are certainly some

00:22:35,919 --> 00:22:39,120
support instructions that has to come in

00:22:37,760 --> 00:22:41,440
for prefix or prolog

00:22:39,120 --> 00:22:42,320
this kind of cases so over that i think

00:22:41,440 --> 00:22:44,559
mma

00:22:42,320 --> 00:22:45,840
utilization is every cycle you can do

00:22:44,559 --> 00:22:47,919
too and

00:22:45,840 --> 00:22:50,400
that way i think you can get achieve the

00:22:47,919 --> 00:22:52,960
best numbers there

00:22:50,400 --> 00:22:54,640
uh what about the next question comes

00:22:52,960 --> 00:22:57,679
from flooring

00:22:54,640 --> 00:23:01,760
what about essl uh support uh

00:22:57,679 --> 00:23:05,280
yes so essl will be supported uh for mma

00:23:01,760 --> 00:23:07,280
so that will be officially released

00:23:05,280 --> 00:23:08,960
in some time right so the open glass is

00:23:07,280 --> 00:23:10,480
ready now so we will

00:23:08,960 --> 00:23:12,000
have to work through some of the

00:23:10,480 --> 00:23:14,640
essential aspects and

00:23:12,000 --> 00:23:16,480
it will be at least uh it will be ready

00:23:14,640 --> 00:23:22,960
by the time we have the meeting

00:23:16,480 --> 00:23:25,440
systems launched

00:23:22,960 --> 00:23:27,840
okay the other question comes from jay

00:23:25,440 --> 00:23:30,799
uh tensorflow of course uses eigen

00:23:27,840 --> 00:23:33,120
uh no open glass how's the mma enabled

00:23:30,799 --> 00:23:36,240
work in eigengoing

00:23:33,120 --> 00:23:38,559
yes i think i can support this in

00:23:36,240 --> 00:23:41,200
plan and we will have the support for

00:23:38,559 --> 00:23:44,559
eigen as well

00:23:41,200 --> 00:23:46,720
and yeah so it will be certainly

00:23:44,559 --> 00:23:50,559
supported tensorflow you can run it

00:23:46,720 --> 00:23:52,960
with mma enabled core as part of eigen

00:23:50,559 --> 00:23:52,960
support

00:23:53,520 --> 00:23:57,279
so again another question from jay which

00:23:56,000 --> 00:24:00,480
version of gcc

00:23:57,279 --> 00:24:03,360
is the iphone and cpu uh

00:24:00,480 --> 00:24:04,240
power 10 option enabled and it's it's

00:24:03,360 --> 00:24:06,880
been

00:24:04,240 --> 00:24:08,159
out in the community in their version uh

00:24:06,880 --> 00:24:11,679
the next release would

00:24:08,159 --> 00:24:14,400
have that um enabled

00:24:11,679 --> 00:24:14,960
but now you can just build the gcc which

00:24:14,400 --> 00:24:19,840
is

00:24:14,960 --> 00:24:19,840
in the near version

00:24:20,480 --> 00:24:24,080
okay i think uh those are the questions

00:24:23,039 --> 00:24:26,060
pepin i hope you

00:24:24,080 --> 00:24:27,360
answered your question as well uh

00:24:26,060 --> 00:24:28,799
[Music]

00:24:27,360 --> 00:24:31,279
yeah in case if you have any further

00:24:28,799 --> 00:24:35,520
questions please drop us an email and

00:24:31,279 --> 00:24:35,520
we'd be happy to respond to you

00:24:41,760 --> 00:24:45,200
okay so there is one more question

00:24:43,360 --> 00:24:48,159
coming in from

00:24:45,200 --> 00:24:50,640
yeah i think that's that will is

00:24:48,159 --> 00:24:54,720
answering okay gcc 10.2

00:24:50,640 --> 00:24:54,720
has the mma support okay

00:24:56,440 --> 00:24:59,440

YouTube URL: https://www.youtube.com/watch?v=zVhVXMKVStA


