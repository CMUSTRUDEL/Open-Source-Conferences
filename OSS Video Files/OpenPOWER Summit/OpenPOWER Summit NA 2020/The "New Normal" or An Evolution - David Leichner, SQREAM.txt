Title: The "New Normal" or An Evolution - David Leichner, SQREAM
Publication date: 2020-09-21
Playlist: OpenPOWER Summit NA 2020
Description: 
	The "New Normal" or An Evolution - David Leichner, SQREAM

Speakers: David Leichner

The "New Normal" that we are experiencing with COVID-19 together with the exponential growth of IoT devices and the digitalization taking hold in the market, is moving us out of the Big Data generation and into the era of Massive Data. Enterprises who were struggling to keep up with maintaining and analyzing the data in their legacy Data Warehouses and data lakes, are being completely overwhelmed with the rapid growth of data that they are experiencing. This session will discuss various ways that companies are changing their data management infrastructure to try to keep up. It will look at the movement to the cloud, upgrades of existing legacy systems and other technologies being implemented to resolve some of these issues. The session will also highlight the use of POWER 9 and GPUs as the underlying processing power being used to resolve these challenging issues.
Captions: 
	00:00:00,000 --> 00:00:03,120
hello everybody it's great to be here

00:00:02,480 --> 00:00:05,920
today

00:00:03,120 --> 00:00:07,040
at this open power north america 2020

00:00:05,920 --> 00:00:09,760
virtual event

00:00:07,040 --> 00:00:10,719
my name is david leichen i'm the cmo at

00:00:09,760 --> 00:00:12,559
scream

00:00:10,719 --> 00:00:14,559
and today i'll be talking about the new

00:00:12,559 --> 00:00:17,600
normal or an evolution

00:00:14,559 --> 00:00:20,880
what kind of effect has coveted 19 had

00:00:17,600 --> 00:00:22,240
on data generation on how we're handling

00:00:20,880 --> 00:00:25,039
data management

00:00:22,240 --> 00:00:26,240
and we're going to just uh dive right in

00:00:25,039 --> 00:00:27,599
so

00:00:26,240 --> 00:00:29,519
the first thing i'd like to talk to you

00:00:27,599 --> 00:00:32,000
about is

00:00:29,519 --> 00:00:34,000
that we have common data challenges now

00:00:32,000 --> 00:00:34,880
as data exploded and we became a data

00:00:34,000 --> 00:00:36,640
driven world

00:00:34,880 --> 00:00:38,879
there came all these new challenges here

00:00:36,640 --> 00:00:40,480
are some of the roadblocks organizations

00:00:38,879 --> 00:00:41,760
typically encounter in the quest to

00:00:40,480 --> 00:00:44,079
manage and analyze

00:00:41,760 --> 00:00:46,239
the full scope of their data stores so

00:00:44,079 --> 00:00:47,840
we have continually growing data stores

00:00:46,239 --> 00:00:49,760
they often leave data managers and

00:00:47,840 --> 00:00:51,440
analysts overwhelmed with multiple

00:00:49,760 --> 00:00:53,520
interlocking data sets

00:00:51,440 --> 00:00:55,360
legacy systems mean that many

00:00:53,520 --> 00:00:56,320
organizations struggle daily when it

00:00:55,360 --> 00:00:58,719
comes to managing

00:00:56,320 --> 00:01:00,719
accessing and analyzing their data data

00:00:58,719 --> 00:01:02,640
preparation is long and arduous

00:01:00,719 --> 00:01:04,559
it can take hours or days to clean and

00:01:02,640 --> 00:01:07,760
prepare data for anal

00:01:04,559 --> 00:01:08,240
analytics we're unable to ingest full

00:01:07,760 --> 00:01:10,000
data

00:01:08,240 --> 00:01:11,680
many organizations just don't have the

00:01:10,000 --> 00:01:13,119
means to ingest their data

00:01:11,680 --> 00:01:16,080
and they have to settle for less than

00:01:13,119 --> 00:01:18,960
the full scope queries along

00:01:16,080 --> 00:01:20,720
so long in fact that some eat into too

00:01:18,960 --> 00:01:22,880
many system resources and get

00:01:20,720 --> 00:01:24,000
shut down by it and they can never

00:01:22,880 --> 00:01:26,159
complete

00:01:24,000 --> 00:01:28,000
and finally many organizations simply

00:01:26,159 --> 00:01:28,960
cannot analyze the full scope of their

00:01:28,000 --> 00:01:31,439
data

00:01:28,960 --> 00:01:32,079
they just do not have the capability to

00:01:31,439 --> 00:01:33,920
do this

00:01:32,079 --> 00:01:35,920
so their reports end up being limited in

00:01:33,920 --> 00:01:38,720
scope and they just can't drill down

00:01:35,920 --> 00:01:39,840
to get even more of the uh analytics

00:01:38,720 --> 00:01:41,119
that they're looking for more of the

00:01:39,840 --> 00:01:43,119
insights that they're looking for

00:01:41,119 --> 00:01:44,960
so basically many organizations are

00:01:43,119 --> 00:01:47,920
suddenly settling

00:01:44,960 --> 00:01:49,200
for good enough instead of utilizing

00:01:47,920 --> 00:01:52,560
their data stores

00:01:49,200 --> 00:01:54,000
to their fullest so when we think about

00:01:52,560 --> 00:01:56,640
the data challenges in the new

00:01:54,000 --> 00:01:58,000
normal with covid19 so there are a lot

00:01:56,640 --> 00:01:59,600
of things that are happening in the

00:01:58,000 --> 00:02:01,280
market today there's financial

00:01:59,600 --> 00:02:02,399
uncertainty

00:02:01,280 --> 00:02:03,920
there are a lot of people who are out of

00:02:02,399 --> 00:02:04,640
work a lot of people who are working

00:02:03,920 --> 00:02:06,079
from home

00:02:04,640 --> 00:02:08,239
there are a lot of companies who have

00:02:06,079 --> 00:02:10,479
changed the way that they operate

00:02:08,239 --> 00:02:12,239
there's an urgency to act and deliver

00:02:10,479 --> 00:02:12,800
fast we have to change the way that

00:02:12,239 --> 00:02:14,480
where

00:02:12,800 --> 00:02:16,160
we're working with our customers with

00:02:14,480 --> 00:02:17,680
our supply chains there are many

00:02:16,160 --> 00:02:19,200
companies that have all

00:02:17,680 --> 00:02:20,879
changed that the the way they're working

00:02:19,200 --> 00:02:23,599
on premise moved online

00:02:20,879 --> 00:02:25,440
some companies are handling um their

00:02:23,599 --> 00:02:27,440
their internal systems by having

00:02:25,440 --> 00:02:28,080
skeleton staffs working inside the

00:02:27,440 --> 00:02:29,520
company

00:02:28,080 --> 00:02:31,040
so there are there are different ways

00:02:29,520 --> 00:02:32,080
that people are reacting the company is

00:02:31,040 --> 00:02:34,560
reacting and

00:02:32,080 --> 00:02:36,160
overall we need to change the way that

00:02:34,560 --> 00:02:38,879
we work

00:02:36,160 --> 00:02:40,080
so i have to tell you a personal story

00:02:38,879 --> 00:02:41,840
last november

00:02:40,080 --> 00:02:43,360
i was in berlin presenting at a data

00:02:41,840 --> 00:02:44,959
leader summit and

00:02:43,360 --> 00:02:47,599
i was coming back from a run in their

00:02:44,959 --> 00:02:48,879
tier garden and i saw a statue on the

00:02:47,599 --> 00:02:52,319
side of the road

00:02:48,879 --> 00:02:53,519
and uh it was uh a person a statue

00:02:52,319 --> 00:02:55,280
holding a globe

00:02:53,519 --> 00:02:57,120
the globe was really small the person

00:02:55,280 --> 00:02:58,720
was really big so when i came back to my

00:02:57,120 --> 00:03:00,800
room in the hotel i looked it up and i

00:02:58,720 --> 00:03:02,640
found that it was alexander von humboldt

00:03:00,800 --> 00:03:04,000
who was a mathematician and a world

00:03:02,640 --> 00:03:08,159
traveler and uh

00:03:04,000 --> 00:03:10,640
and a philosopher back in the late 1700s

00:03:08,159 --> 00:03:12,239
and he said something incredible he had

00:03:10,640 --> 00:03:14,560
this quote and i said wow

00:03:12,239 --> 00:03:16,080
this is perfect i saw with regret and

00:03:14,560 --> 00:03:16,560
all scientific men have shared this

00:03:16,080 --> 00:03:18,159
feeling

00:03:16,560 --> 00:03:20,400
that what's the number of accurate

00:03:18,159 --> 00:03:23,519
instruments was daily increasing

00:03:20,400 --> 00:03:24,319
we were still ignorant so we're 200

00:03:23,519 --> 00:03:26,799
years later

00:03:24,319 --> 00:03:28,640
has the situation really changed so with

00:03:26,799 --> 00:03:30,000
all the the challenges in mind what

00:03:28,640 --> 00:03:32,319
typically ends up happening is that

00:03:30,000 --> 00:03:34,239
organizations settle for analyzing only

00:03:32,319 --> 00:03:36,319
a small fraction of their data even

00:03:34,239 --> 00:03:38,480
though we have all of these new tools

00:03:36,319 --> 00:03:40,879
in the market today we're still only

00:03:38,480 --> 00:03:41,519
analyzing a short a small amount of our

00:03:40,879 --> 00:03:43,200
data

00:03:41,519 --> 00:03:45,200
and if you look at what the analysts are

00:03:43,200 --> 00:03:45,680
saying what some of the industry experts

00:03:45,200 --> 00:03:47,440
are saying

00:03:45,680 --> 00:03:49,360
they're saying that only ten percent of

00:03:47,440 --> 00:03:50,879
the actual data is being analyzed

00:03:49,360 --> 00:03:52,720
but that's if we have let's say a

00:03:50,879 --> 00:03:55,360
hundred terabyte data lake

00:03:52,720 --> 00:03:58,000
but once we grow you know into the 500

00:03:55,360 --> 00:03:58,640
terabyte one petabyte 10 petabytes of

00:03:58,000 --> 00:04:00,879
data

00:03:58,640 --> 00:04:03,360
we actually reduce significantly the

00:04:00,879 --> 00:04:04,959
percentage of data that we're analyzing

00:04:03,360 --> 00:04:07,519
and we're leaving a lot of business

00:04:04,959 --> 00:04:09,200
insights lost now what does that mean

00:04:07,519 --> 00:04:11,680
that means number one if your

00:04:09,200 --> 00:04:13,360
competition is analyzing that data

00:04:11,680 --> 00:04:15,599
so you're going to be at a company

00:04:13,360 --> 00:04:17,600
competitive disadvantage

00:04:15,599 --> 00:04:19,040
and number two you're losing out on all

00:04:17,600 --> 00:04:20,959
of the potential insights

00:04:19,040 --> 00:04:22,240
that your business could be gaining in

00:04:20,959 --> 00:04:24,479
order to

00:04:22,240 --> 00:04:26,240
better you know handle risk management

00:04:24,479 --> 00:04:27,840
better handle customer segmentation

00:04:26,240 --> 00:04:29,120
we'll talk about some of these things as

00:04:27,840 --> 00:04:31,919
we go along

00:04:29,120 --> 00:04:32,960
so there are valuable insights that are

00:04:31,919 --> 00:04:35,120
being missed

00:04:32,960 --> 00:04:37,199
so many organizations data lakes and

00:04:35,120 --> 00:04:39,199
other data stores have turned into

00:04:37,199 --> 00:04:41,280
dumping grounds and we all know that

00:04:39,199 --> 00:04:42,080
over the last two decades data has

00:04:41,280 --> 00:04:44,000
exploded

00:04:42,080 --> 00:04:45,919
and the reasons are numerous the rise in

00:04:44,000 --> 00:04:47,840
mobile communications growth of cloud

00:04:45,919 --> 00:04:51,040
traffic the expansion of technical

00:04:47,840 --> 00:04:53,040
capacities artificial intelligence iot

00:04:51,040 --> 00:04:54,840
proliferation of data collecting devices

00:04:53,040 --> 00:04:56,880
in every aspect of our life

00:04:54,840 --> 00:04:59,199
organizations build business models

00:04:56,880 --> 00:05:01,600
completely based on data acquisition

00:04:59,199 --> 00:05:03,360
some might say that the data has become

00:05:01,600 --> 00:05:05,280
the center of our universe

00:05:03,360 --> 00:05:07,280
so this explosion of data has brought

00:05:05,280 --> 00:05:09,520
with it many opportunities as well as

00:05:07,280 --> 00:05:11,280
great challenges on the one hand

00:05:09,520 --> 00:05:12,960
organizations and businesses can use

00:05:11,280 --> 00:05:14,720
their data to expand their offerings

00:05:12,960 --> 00:05:16,000
optimize their services grow revenues

00:05:14,720 --> 00:05:18,720
while reducing risk

00:05:16,000 --> 00:05:20,320
but at the same time they are inundated

00:05:18,720 --> 00:05:22,400
with so much information

00:05:20,320 --> 00:05:25,120
that often they can only access and

00:05:22,400 --> 00:05:28,240
analyze a very small portion

00:05:25,120 --> 00:05:30,160
of their data assets so

00:05:28,240 --> 00:05:31,680
my question to you is so you know what's

00:05:30,160 --> 00:05:34,080
keeping you up at night

00:05:31,680 --> 00:05:35,600
right there we're hearing from a lot of

00:05:34,080 --> 00:05:37,440
our customers and uh

00:05:35,600 --> 00:05:39,840
the people that we talk to is that there

00:05:37,440 --> 00:05:42,560
are common struggles across the board

00:05:39,840 --> 00:05:44,400
from the ingest of data to the insights

00:05:42,560 --> 00:05:46,000
a lot of those have to do with queries

00:05:44,400 --> 00:05:48,400
that are running way too long

00:05:46,000 --> 00:05:49,520
they could take 10 hours 20 hours even a

00:05:48,400 --> 00:05:51,360
day and a half

00:05:49,520 --> 00:05:53,520
and a lot of those queries are being

00:05:51,360 --> 00:05:55,840
cancelled because the system

00:05:53,520 --> 00:05:57,680
resources are getting overloaded uh

00:05:55,840 --> 00:05:59,919
there they have lots of data

00:05:57,680 --> 00:06:01,199
you know companies have 500 terabytes

00:05:59,919 --> 00:06:03,600
petabyte of data but they're only

00:06:01,199 --> 00:06:04,479
analyzing 20 terabytes or 50 terabytes

00:06:03,600 --> 00:06:06,400
of that data

00:06:04,479 --> 00:06:08,160
why because there are lengthy data

00:06:06,400 --> 00:06:12,000
preparation and query

00:06:08,160 --> 00:06:13,840
development cycles it takes too much

00:06:12,000 --> 00:06:15,120
you know system resources in order to

00:06:13,840 --> 00:06:16,560
analyze those queries

00:06:15,120 --> 00:06:18,240
some of those queries are just simply

00:06:16,560 --> 00:06:21,280
too complex to run

00:06:18,240 --> 00:06:23,759
so at the end of the day they say

00:06:21,280 --> 00:06:24,400
our analytic reports are limited in

00:06:23,759 --> 00:06:25,840
scope

00:06:24,400 --> 00:06:27,840
and what that means is we're holding

00:06:25,840 --> 00:06:30,400
back information from the business

00:06:27,840 --> 00:06:30,960
and it's keeping um let's say it's it's

00:06:30,400 --> 00:06:32,720
uh

00:06:30,960 --> 00:06:34,080
keeping our business stakeholders from

00:06:32,720 --> 00:06:37,120
gaining as much as they can

00:06:34,080 --> 00:06:37,919
from their data so you know what's

00:06:37,120 --> 00:06:39,520
happening

00:06:37,919 --> 00:06:41,680
over the last couple of decades

00:06:39,520 --> 00:06:44,080
organizations have increasingly relied

00:06:41,680 --> 00:06:45,759
on legacy systems and mpps to manage

00:06:44,080 --> 00:06:47,440
access and analyze their data

00:06:45,759 --> 00:06:49,440
but for an mpp to achieve fast

00:06:47,440 --> 00:06:50,080
performance especially with data source

00:06:49,440 --> 00:06:52,160
growing

00:06:50,080 --> 00:06:53,919
the system had to be constantly tuned

00:06:52,160 --> 00:06:55,919
data had to be duplicated in multiple

00:06:53,919 --> 00:06:57,039
distributions loaded and then statistics

00:06:55,919 --> 00:07:00,240
had to be collected

00:06:57,039 --> 00:07:02,639
all of this is very time intensive

00:07:00,240 --> 00:07:04,720
collecting and loading the data requires

00:07:02,639 --> 00:07:06,479
expertise on which columns to collect

00:07:04,720 --> 00:07:07,680
statistics on which makes the process

00:07:06,479 --> 00:07:09,280
even more challenging

00:07:07,680 --> 00:07:12,800
together with this you need to also

00:07:09,280 --> 00:07:14,720
consider os resources such as cpu memory

00:07:12,800 --> 00:07:16,720
disk io calculations to prepare for

00:07:14,720 --> 00:07:19,840
generating indexes

00:07:16,720 --> 00:07:22,639
materialized views projections and more

00:07:19,840 --> 00:07:24,479
this process requires significant time

00:07:22,639 --> 00:07:26,560
which directly increases

00:07:24,479 --> 00:07:27,520
the latency of data availability for

00:07:26,560 --> 00:07:29,120
analysis

00:07:27,520 --> 00:07:30,639
to put it simply you spend all of your

00:07:29,120 --> 00:07:33,120
time getting things ready

00:07:30,639 --> 00:07:34,160
and have little time less for the actual

00:07:33,120 --> 00:07:36,240
analytics

00:07:34,160 --> 00:07:37,919
and if we look at statistics that uh

00:07:36,240 --> 00:07:40,800
that we've seen in the market

00:07:37,919 --> 00:07:43,199
so organizations report that they spend

00:07:40,800 --> 00:07:44,400
more than 60 percent of their time in

00:07:43,199 --> 00:07:47,599
data preparation

00:07:44,400 --> 00:07:50,400
leaving little time for actual analytics

00:07:47,599 --> 00:07:51,840
and forester noted that data scientists

00:07:50,400 --> 00:07:53,440
spend most of their time

00:07:51,840 --> 00:07:54,879
searching for managing and cleaning

00:07:53,440 --> 00:07:57,280
their massive data stores

00:07:54,879 --> 00:07:58,160
with only 20 percent of their time going

00:07:57,280 --> 00:08:02,400
to actual

00:07:58,160 --> 00:08:03,680
analytics so i always tell uh the

00:08:02,400 --> 00:08:05,840
you know the companies that i'm talking

00:08:03,680 --> 00:08:07,919
to to ask themselves two questions

00:08:05,840 --> 00:08:10,000
first of all are you able to analyze

00:08:07,919 --> 00:08:11,680
enough of your financial data or

00:08:10,000 --> 00:08:13,680
any other kind of data that you have in

00:08:11,680 --> 00:08:14,800
your organization to deliver new and

00:08:13,680 --> 00:08:17,199
critical insights

00:08:14,800 --> 00:08:19,120
to address critical risk compliance

00:08:17,199 --> 00:08:21,199
customer and security issues

00:08:19,120 --> 00:08:23,520
and do your financial analytics and

00:08:21,199 --> 00:08:26,000
reports to your retail reports to your

00:08:23,520 --> 00:08:26,639
telcom reports take too long to be

00:08:26,000 --> 00:08:28,879
useful

00:08:26,639 --> 00:08:30,319
or you simply not able to execute the

00:08:28,879 --> 00:08:33,200
analytics that you need

00:08:30,319 --> 00:08:34,240
to drive your business okay so let's

00:08:33,200 --> 00:08:36,159
dive in to see

00:08:34,240 --> 00:08:37,519
how you know we're handling a lot of

00:08:36,159 --> 00:08:40,240
these challenges

00:08:37,519 --> 00:08:40,880
so first of all i'd like to give you a

00:08:40,240 --> 00:08:43,440
quote

00:08:40,880 --> 00:08:45,680
from sumit gupta the the who is at the

00:08:43,440 --> 00:08:48,720
time the vp of hpc and ai for

00:08:45,680 --> 00:08:51,519
ibm cognitive systems this was um

00:08:48,720 --> 00:08:53,760
a little while back and he talked about

00:08:51,519 --> 00:08:55,839
screaming and actually it was a uh

00:08:53,760 --> 00:08:57,519
press release that we had put out about

00:08:55,839 --> 00:09:00,560
um ibm power9

00:08:57,519 --> 00:09:02,959
being used at lg and he said

00:09:00,560 --> 00:09:05,200
gpu accelerated screamd boosts query

00:09:02,959 --> 00:09:07,920
performance by up to 50 150

00:09:05,200 --> 00:09:08,959
for ibm power 9 users this was the the

00:09:07,920 --> 00:09:11,440
title of the

00:09:08,959 --> 00:09:12,800
press release and sumit said gpu

00:09:11,440 --> 00:09:14,240
accelerated analytics are an

00:09:12,800 --> 00:09:14,880
increasingly important part of our

00:09:14,240 --> 00:09:16,720
industry

00:09:14,880 --> 00:09:18,720
the announcement of scream on the ibm

00:09:16,720 --> 00:09:20,160
power9 platform takes this concept to

00:09:18,720 --> 00:09:22,959
another level of performance

00:09:20,160 --> 00:09:25,519
as the power9 cpu with embedded nvidia

00:09:22,959 --> 00:09:27,760
nv link interface to nvidia gpu

00:09:25,519 --> 00:09:28,640
allows screen to enable even faster

00:09:27,760 --> 00:09:30,800
processing

00:09:28,640 --> 00:09:32,080
of data on power9 servers and i'll talk

00:09:30,800 --> 00:09:34,800
about that in a minute and

00:09:32,080 --> 00:09:36,240
what kind of results we've seen so a

00:09:34,800 --> 00:09:36,959
little bit about scream corporate

00:09:36,240 --> 00:09:39,360
profile

00:09:36,959 --> 00:09:40,800
we're in over 90 people today we have 10

00:09:39,360 --> 00:09:43,200
10 significant patents

00:09:40,800 --> 00:09:44,560
we have strategic partnerships with yes

00:09:43,200 --> 00:09:47,200
nvidia ibm

00:09:44,560 --> 00:09:49,360
and some of the other hardware vendors

00:09:47,200 --> 00:09:50,959
business intelligence vendors and so on

00:09:49,360 --> 00:09:53,920
we have our headquarters in new york

00:09:50,959 --> 00:09:55,200
city we have offices in the uk in france

00:09:53,920 --> 00:09:57,440
nordic's

00:09:55,200 --> 00:10:00,240
significant office in south korea and we

00:09:57,440 --> 00:10:03,040
have our r d center in tel aviv

00:10:00,240 --> 00:10:04,480
so scream in a nutshell and i won't turn

00:10:03,040 --> 00:10:07,920
this into a sales pitch

00:10:04,480 --> 00:10:09,200
so basically um scream is a data

00:10:07,920 --> 00:10:11,600
acceleration platform

00:10:09,200 --> 00:10:13,839
we have an underlying columnar database

00:10:11,600 --> 00:10:15,839
sql database so you work with familiar

00:10:13,839 --> 00:10:18,160
ncsql

00:10:15,839 --> 00:10:19,040
what makes this really really fast is

00:10:18,160 --> 00:10:22,480
that we run

00:10:19,040 --> 00:10:24,160
on gpu processors so you can ingest data

00:10:22,480 --> 00:10:26,160
at three terabytes an hour

00:10:24,160 --> 00:10:27,920
we have adaptive auto compression so if

00:10:26,160 --> 00:10:29,200
you're bringing in 100 terabytes we can

00:10:27,920 --> 00:10:32,079
compress that down to

00:10:29,200 --> 00:10:32,959
anywhere between 10 and 20 terabytes

00:10:32,079 --> 00:10:35,360
which means that

00:10:32,959 --> 00:10:37,120
once you're doing your analytics on on

00:10:35,360 --> 00:10:38,240
that data it's going to be much quicker

00:10:37,120 --> 00:10:40,000
to get to that data

00:10:38,240 --> 00:10:42,320
we are massively scalable so you can

00:10:40,000 --> 00:10:43,760
start off at two three five terabytes

00:10:42,320 --> 00:10:45,279
but you can build that up and we have

00:10:43,760 --> 00:10:47,839
companies that have started at 20

00:10:45,279 --> 00:10:50,160
terabytes and gone to petabytes of data

00:10:47,839 --> 00:10:52,240
the footprint is extremely small because

00:10:50,160 --> 00:10:54,160
as we know gpus are much

00:10:52,240 --> 00:10:55,760
much smaller and much more cost

00:10:54,160 --> 00:10:58,959
efficient and much

00:10:55,760 --> 00:11:01,920
more energy efficient than cpus and

00:10:58,959 --> 00:11:02,560
uh of course we have the uh the high

00:11:01,920 --> 00:11:05,440
throughput

00:11:02,560 --> 00:11:06,320
uh compute which makes us very fast also

00:11:05,440 --> 00:11:09,040
for ingest

00:11:06,320 --> 00:11:10,880
straight through to analytics some of

00:11:09,040 --> 00:11:11,519
our key customers and partners you can

00:11:10,880 --> 00:11:13,120
see here

00:11:11,519 --> 00:11:15,040
we're extremely strong in the telcom

00:11:13,120 --> 00:11:16,160
industry we have customers in retail we

00:11:15,040 --> 00:11:19,360
have customers

00:11:16,160 --> 00:11:21,279
in finance we have customers who are

00:11:19,360 --> 00:11:23,120
using us as an embedded database

00:11:21,279 --> 00:11:24,480
and we have some very significant

00:11:23,120 --> 00:11:27,839
partners

00:11:24,480 --> 00:11:28,399
also in the also in the hardware space

00:11:27,839 --> 00:11:31,440
and also

00:11:28,399 --> 00:11:34,079
in storage and other areas

00:11:31,440 --> 00:11:35,680
so let's talk about the four pillars of

00:11:34,079 --> 00:11:37,600
massive data analytics

00:11:35,680 --> 00:11:39,680
what if we if i told you that there was

00:11:37,600 --> 00:11:41,279
a way to have your cake and needed to

00:11:39,680 --> 00:11:42,959
to keep collecting data and let it grow

00:11:41,279 --> 00:11:44,000
and actually analyze the full scope of

00:11:42,959 --> 00:11:46,560
these data stores

00:11:44,000 --> 00:11:48,240
so first let's call things what they are

00:11:46,560 --> 00:11:50,639
big data is no longer big

00:11:48,240 --> 00:11:52,480
it's massive and chances are that your

00:11:50,639 --> 00:11:53,519
organization data stores fall within

00:11:52,480 --> 00:11:54,880
this definition

00:11:53,519 --> 00:11:56,560
what if you could analyze your data

00:11:54,880 --> 00:11:58,800
faster even on the most

00:11:56,560 --> 00:12:00,160
complex sql queries what if you could

00:11:58,800 --> 00:12:02,320
analyze more data

00:12:00,160 --> 00:12:03,760
what if you can analyze and run queries

00:12:02,320 --> 00:12:05,760
on terabytes of data

00:12:03,760 --> 00:12:07,360
and then grow that into petabytes what

00:12:05,760 --> 00:12:09,440
if you can analyze more dimensions

00:12:07,360 --> 00:12:10,880
add more complex joins to your queries

00:12:09,440 --> 00:12:12,320
and finally what if you could shorten

00:12:10,880 --> 00:12:14,160
the time it takes you to prepare that

00:12:12,320 --> 00:12:16,320
data and to prepare those queries

00:12:14,160 --> 00:12:17,920
even when you want to run ad hoc queries

00:12:16,320 --> 00:12:20,079
on raw data

00:12:17,920 --> 00:12:21,600
is it possible well you don't have to

00:12:20,079 --> 00:12:22,240
stick with these long and drawn out

00:12:21,600 --> 00:12:25,440
processes

00:12:22,240 --> 00:12:26,880
on on partial data source you can try

00:12:25,440 --> 00:12:28,720
screen

00:12:26,880 --> 00:12:30,399
so where you can try screen because we

00:12:28,720 --> 00:12:31,200
rapidly integrate into your existing

00:12:30,399 --> 00:12:33,680
ecosystem

00:12:31,200 --> 00:12:34,880
so if you today have you know you're

00:12:33,680 --> 00:12:36,959
working with hadoop

00:12:34,880 --> 00:12:38,000
you're working with some of the big data

00:12:36,959 --> 00:12:41,279
warehouses

00:12:38,000 --> 00:12:43,600
whether it's exudate or teradata or

00:12:41,279 --> 00:12:45,440
netiza or green plum or some of the

00:12:43,600 --> 00:12:46,480
others you're running your systems on

00:12:45,440 --> 00:12:47,920
power9

00:12:46,480 --> 00:12:50,160
you can take screen and you can

00:12:47,920 --> 00:12:52,880
implement screen into your existing

00:12:50,160 --> 00:12:53,519
ecosystem seamlessly all you have to do

00:12:52,880 --> 00:12:56,399
is then

00:12:53,519 --> 00:12:57,920
direct the data that you want to analyze

00:12:56,399 --> 00:13:00,160
that very let's say

00:12:57,920 --> 00:13:01,680
um log those large data stores that you

00:13:00,160 --> 00:13:02,959
want to do heavy crunching and heavy

00:13:01,680 --> 00:13:05,519
complex

00:13:02,959 --> 00:13:06,160
analytics and you run that into scream

00:13:05,519 --> 00:13:08,480
and

00:13:06,160 --> 00:13:09,760
then you have the ability to run queries

00:13:08,480 --> 00:13:12,160
extremely quickly

00:13:09,760 --> 00:13:12,959
cut down the time it takes to run those

00:13:12,160 --> 00:13:15,680
queries from

00:13:12,959 --> 00:13:17,200
hours to minutes from we had situations

00:13:15,680 --> 00:13:18,399
where we cut down from a day and a half

00:13:17,200 --> 00:13:20,399
to two hours

00:13:18,399 --> 00:13:22,079
and you can also then drill down on that

00:13:20,399 --> 00:13:24,079
data because it's available to you

00:13:22,079 --> 00:13:25,600
with ad hoc sql queries without having

00:13:24,079 --> 00:13:28,720
to go back to the data engineers

00:13:25,600 --> 00:13:31,200
to recreate new queries

00:13:28,720 --> 00:13:32,000
we can also be used for high throughput

00:13:31,200 --> 00:13:34,320
aiml

00:13:32,000 --> 00:13:36,399
financial data modeling or retail data

00:13:34,320 --> 00:13:36,959
modeling this is just an example of one

00:13:36,399 --> 00:13:38,720
of them

00:13:36,959 --> 00:13:40,240
where you would ingest data let's say

00:13:38,720 --> 00:13:42,079
from hadoop into screen

00:13:40,240 --> 00:13:44,720
screen would then be used to break down

00:13:42,079 --> 00:13:45,600
that data into data classification

00:13:44,720 --> 00:13:47,839
subsets

00:13:45,600 --> 00:13:49,120
and then you can train your models and

00:13:47,839 --> 00:13:51,199
you can do model scoring

00:13:49,120 --> 00:13:53,120
and at the end of the day you can work

00:13:51,199 --> 00:13:57,040
with some of the leading

00:13:53,120 --> 00:13:58,160
tools to the ai front ends like sashfia

00:13:57,040 --> 00:14:01,600
or some of the others

00:13:58,160 --> 00:14:05,440
in order to have more reliable ai models

00:14:01,600 --> 00:14:08,240
based on more classified and better

00:14:05,440 --> 00:14:10,000
classified data

00:14:08,240 --> 00:14:12,000
so just i want to give you a couple of

00:14:10,000 --> 00:14:12,560
examples of some of the tests that we

00:14:12,000 --> 00:14:15,519
ran

00:14:12,560 --> 00:14:16,959
so you can see here that one of the

00:14:15,519 --> 00:14:19,839
tests that we ran

00:14:16,959 --> 00:14:20,160
uh with scream on power nine we had up

00:14:19,839 --> 00:14:23,360
to

00:14:20,160 --> 00:14:26,240
two times faster loading so

00:14:23,360 --> 00:14:28,079
we rely both on the cpu as well as on

00:14:26,240 --> 00:14:28,639
the gpu for loading in fact we have

00:14:28,079 --> 00:14:30,880
let's say

00:14:28,639 --> 00:14:32,320
what i could call a traffic warden where

00:14:30,880 --> 00:14:35,839
scream understands which

00:14:32,320 --> 00:14:37,120
operations should be used um or which i

00:14:35,839 --> 00:14:38,800
should say which processes should be

00:14:37,120 --> 00:14:40,720
used depending on the operation in order

00:14:38,800 --> 00:14:43,279
to maximize

00:14:40,720 --> 00:14:44,560
the efficiency and the speed by which

00:14:43,279 --> 00:14:47,360
those operations

00:14:44,560 --> 00:14:49,199
are then executed so you can see here

00:14:47,360 --> 00:14:50,560
this is an example of a load time for

00:14:49,199 --> 00:14:53,600
six billion

00:14:50,560 --> 00:14:55,839
tpch records and we were able to do this

00:14:53,600 --> 00:14:58,079
two times faster on the loading

00:14:55,839 --> 00:15:00,000
and you can see here another example um

00:14:58,079 --> 00:15:01,040
this time we're talking about swim db on

00:15:00,000 --> 00:15:05,760
power 9

00:15:01,040 --> 00:15:06,399
which was between 150 to 370 percent

00:15:05,760 --> 00:15:08,720
faster

00:15:06,399 --> 00:15:10,720
than comparable x86 architectures and

00:15:08,720 --> 00:15:14,240
comparable cpu architectures

00:15:10,720 --> 00:15:17,279
so we're talking about up to 3.7 times

00:15:14,240 --> 00:15:19,839
faster queries and this was uh

00:15:17,279 --> 00:15:20,480
also a very uh significant test which

00:15:19,839 --> 00:15:24,839
showed

00:15:20,480 --> 00:15:27,360
the power of using scream together with

00:15:24,839 --> 00:15:29,839
power9 so an example

00:15:27,360 --> 00:15:31,920
uh of a use case that actually this is a

00:15:29,839 --> 00:15:33,440
uh this is a real use case that one of

00:15:31,920 --> 00:15:35,279
our big telecom customers

00:15:33,440 --> 00:15:36,959
so they're using power nine they're

00:15:35,279 --> 00:15:39,360
using scream they brought in

00:15:36,959 --> 00:15:40,639
data into hadoop from various data

00:15:39,360 --> 00:15:43,360
sources

00:15:40,639 --> 00:15:44,800
we took that data which was running

00:15:43,360 --> 00:15:47,600
queries off of hadoop

00:15:44,800 --> 00:15:48,880
and taking well over a day we took that

00:15:47,600 --> 00:15:51,279
data we ingested it

00:15:48,880 --> 00:15:54,160
into screen and we were able to reduce

00:15:51,279 --> 00:15:56,160
that those queries to a couple of hours

00:15:54,160 --> 00:15:57,759
so this was in particular this was used

00:15:56,160 --> 00:15:58,880
for modeling of call noise using

00:15:57,759 --> 00:16:01,199
frequency

00:15:58,880 --> 00:16:02,399
analysis for better service so we were

00:16:01,199 --> 00:16:05,920
able to help this

00:16:02,399 --> 00:16:08,240
particular tier 1 telecom to

00:16:05,920 --> 00:16:10,240
supply their customers with much better

00:16:08,240 --> 00:16:12,320
service and then they actually started

00:16:10,240 --> 00:16:14,959
using um the product in

00:16:12,320 --> 00:16:16,160
other areas of the organization for

00:16:14,959 --> 00:16:18,079
different use cases

00:16:16,160 --> 00:16:19,680
and uh you know if you're interested i'd

00:16:18,079 --> 00:16:22,800
be happy to share that with you

00:16:19,680 --> 00:16:25,279
um in a separate conversation so

00:16:22,800 --> 00:16:26,079
at this point i would uh i would like to

00:16:25,279 --> 00:16:28,959
um

00:16:26,079 --> 00:16:30,560
you know say thank you very much to uh

00:16:28,959 --> 00:16:33,199
the open power organization

00:16:30,560 --> 00:16:33,600
for having me uh here and i would like

00:16:33,199 --> 00:16:36,800
to

00:16:33,600 --> 00:16:38,880
open up the session for questions so

00:16:36,800 --> 00:16:41,279
again thank you very much for listening

00:16:38,880 --> 00:16:43,279
and i look forward to taking any

00:16:41,279 --> 00:16:45,120
questions that you might have

00:16:43,279 --> 00:16:46,800
well i see that we have one question

00:16:45,120 --> 00:16:49,759
that just came in

00:16:46,800 --> 00:16:50,880
that question is how long does it take

00:16:49,759 --> 00:16:53,920
uh to implement

00:16:50,880 --> 00:16:56,079
screen inside of an existing environment

00:16:53,920 --> 00:16:57,680
uh so the answer is that that really

00:16:56,079 --> 00:16:59,199
depends on the use case it really

00:16:57,680 --> 00:17:00,000
depends on how much data we're talking

00:16:59,199 --> 00:17:03,360
about

00:17:00,000 --> 00:17:05,039
um if we're talking about you know 50

00:17:03,360 --> 00:17:06,959
terabytes of data

00:17:05,039 --> 00:17:08,319
up to 100 terabytes of data that can be

00:17:06,959 --> 00:17:11,439
installed on one

00:17:08,319 --> 00:17:13,439
uh scream instance on one gpu server

00:17:11,439 --> 00:17:15,199
if we're talking about you know a couple

00:17:13,439 --> 00:17:16,640
of petabytes of data that might be

00:17:15,199 --> 00:17:18,640
something else just the you know the

00:17:16,640 --> 00:17:21,839
time it takes to ingest the data

00:17:18,640 --> 00:17:24,160
uh and it could be uh several uh gp

00:17:21,839 --> 00:17:26,720
servers so

00:17:24,160 --> 00:17:28,000
it depends on whether those uh gpus are

00:17:26,720 --> 00:17:29,679
readily available

00:17:28,000 --> 00:17:32,000
or whether that's something that would

00:17:29,679 --> 00:17:35,360
need to be ordered you know from ibm

00:17:32,000 --> 00:17:37,679
uh together with power9 uh and uh

00:17:35,360 --> 00:17:39,280
sometimes that's actually uh takes

00:17:37,679 --> 00:17:40,799
longer than actually installing the

00:17:39,280 --> 00:17:44,080
software itself

00:17:40,799 --> 00:17:46,480
so thank you for that question

00:17:44,080 --> 00:17:47,840
and i don't see any other questions

00:17:46,480 --> 00:17:52,480
coming in

00:17:47,840 --> 00:17:55,600
so maybe um it would be best if uh

00:17:52,480 --> 00:17:59,600
anybody else has any questions if uh

00:17:55,600 --> 00:18:03,120
we can hook up on on the slack channel

00:17:59,600 --> 00:18:05,919
and that would be a good place to

00:18:03,120 --> 00:18:09,039
uh to continue the conversation if that

00:18:05,919 --> 00:18:09,039

YouTube URL: https://www.youtube.com/watch?v=7fZo-Hu4iqk


