Title: AI at Scale in the Enterprise - Clarisse Taaffe Hedglin, IBM
Publication date: 2020-09-21
Playlist: OpenPOWER Summit NA 2020
Description: 
	AI at Scale in the Enterprise - Clarisse Taaffe Hedglin, IBM

Speakers: Clarisse Hedglin

As the adoption of AI technologies increases and matures, the focus will shift from exploration to time to market and productivity. Governing Enterprise data, scaling AI model development, providing a complete, collaborative platform and tools for rapid solution deployments are key focus areas for growing data scientist teams tasked to respond to business challenges. The drive for performance, productivity acceleration, smarter infrastructure resource utilization and management efficiencies can be seen in Enterprises across all industries and modernization initiatives. This talk will cover the challenges and innovations for AI at scale for the Enterprise focusing on the modernization of data analytics, the AI ladder and AI lifecycle, infrastructure considerations and conclude with Data and AI architecture use case scenarios.
Captions: 
	00:00:03,600 --> 00:00:07,200
okay hi everybody

00:00:05,040 --> 00:00:08,800
let's uh let's get started this is

00:00:07,200 --> 00:00:11,920
clarisse tafe hedgeline

00:00:08,800 --> 00:00:12,719
it architect with ibm systems and i'm

00:00:11,920 --> 00:00:17,920
going to talk about

00:00:12,719 --> 00:00:17,920
ai at scale in the enterprise

00:00:21,520 --> 00:00:27,680
so our agenda today will be to uh cover

00:00:24,560 --> 00:00:30,080
the state of data analytics today

00:00:27,680 --> 00:00:31,199
what we mean by the ai ladder and life

00:00:30,080 --> 00:00:33,280
cycle

00:00:31,199 --> 00:00:34,480
some of the themes that we see coming

00:00:33,280 --> 00:00:37,200
back in

00:00:34,480 --> 00:00:41,200
ai in the enterprise some infrastructure

00:00:37,200 --> 00:00:41,200
considerations in a couple examples

00:00:42,399 --> 00:00:47,600
so traditionally data analytics has been

00:00:45,280 --> 00:00:50,399
from the very simple

00:00:47,600 --> 00:00:52,399
spreadsheet type of data collection with

00:00:50,399 --> 00:00:56,320
no governance no collaboration

00:00:52,399 --> 00:01:00,079
and limited complexity all the way to

00:00:56,320 --> 00:01:01,039
specialized applications like crms and

00:01:00,079 --> 00:01:03,760
erps

00:01:01,039 --> 00:01:05,680
that have some level of analytics but

00:01:03,760 --> 00:01:09,119
are typically hard-coded

00:01:05,680 --> 00:01:11,920
or driven by business rules to um

00:01:09,119 --> 00:01:14,720
to do the analytics some of the

00:01:11,920 --> 00:01:17,280
homegrown applications that you see also

00:01:14,720 --> 00:01:19,200
take this approach of preset rules and

00:01:17,280 --> 00:01:22,799
what this means is that

00:01:19,200 --> 00:01:26,240
a lot of the analytics a piece of it

00:01:22,799 --> 00:01:29,200
is hard-coded and not very flexible

00:01:26,240 --> 00:01:30,640
so the idea is to introduce ai into

00:01:29,200 --> 00:01:33,680
these systems

00:01:30,640 --> 00:01:38,880
and have a greater dynamic use

00:01:33,680 --> 00:01:38,880
of of tools to analyze your data

00:01:39,439 --> 00:01:43,439
the idea is to go from something that's

00:01:42,079 --> 00:01:47,280
very descriptive

00:01:43,439 --> 00:01:50,079
and static in terms of what has happened

00:01:47,280 --> 00:01:52,560
to coming up with some predictive models

00:01:50,079 --> 00:01:55,200
of what will happen

00:01:52,560 --> 00:01:56,479
that drive into prescriptive models of

00:01:55,200 --> 00:01:59,600
what we should do

00:01:56,479 --> 00:02:01,360
and finally in this cognitive era what

00:01:59,600 --> 00:02:03,280
can we learn dynamically

00:02:01,360 --> 00:02:05,439
in terms of what has happened what will

00:02:03,280 --> 00:02:08,479
happen and what we can do

00:02:05,439 --> 00:02:11,520
and so have the systems learn

00:02:08,479 --> 00:02:14,560
based on on past knowledge

00:02:11,520 --> 00:02:17,440
and on information that's

00:02:14,560 --> 00:02:19,200
input dynamically and learned directly

00:02:17,440 --> 00:02:22,080
by the systems

00:02:19,200 --> 00:02:22,640
so the idea is to go from data to action

00:02:22,080 --> 00:02:25,520
and

00:02:22,640 --> 00:02:26,080
reducing the amount of human input as

00:02:25,520 --> 00:02:28,080
you go

00:02:26,080 --> 00:02:29,440
from that data to action and have more

00:02:28,080 --> 00:02:32,640
and more of the system

00:02:29,440 --> 00:02:34,640
being able to guide us to the right

00:02:32,640 --> 00:02:37,840
decisions and the right actions based on

00:02:34,640 --> 00:02:37,840
the data insights

00:02:38,160 --> 00:02:45,680
some common patterns relative to that

00:02:42,080 --> 00:02:48,640
data analysis is tied to

00:02:45,680 --> 00:02:50,319
planning activities where you're looking

00:02:48,640 --> 00:02:53,599
at forecasting

00:02:50,319 --> 00:02:56,080
and budgeting based on past activities

00:02:53,599 --> 00:02:56,720
how you're going to move forward to

00:02:56,080 --> 00:02:58,959
predict

00:02:56,720 --> 00:03:00,879
future events and this is where you can

00:02:58,959 --> 00:03:04,800
start looking at supervised

00:03:00,879 --> 00:03:07,120
versus unsupervised learning and

00:03:04,800 --> 00:03:09,519
use new techniques such as deep learning

00:03:07,120 --> 00:03:12,239
and natural language processing

00:03:09,519 --> 00:03:12,879
to come up with better insights as you

00:03:12,239 --> 00:03:16,319
move

00:03:12,879 --> 00:03:18,560
towards your journey in

00:03:16,319 --> 00:03:19,519
building a more sophisticated data

00:03:18,560 --> 00:03:22,319
analytics

00:03:19,519 --> 00:03:22,319
set of patterns

00:03:25,360 --> 00:03:28,959
in terms of structured versus

00:03:27,200 --> 00:03:31,280
unstructured data

00:03:28,959 --> 00:03:32,480
these are some of the examples that we

00:03:31,280 --> 00:03:34,400
talk about

00:03:32,480 --> 00:03:36,959
when we uh look at some of the most

00:03:34,400 --> 00:03:40,400
common patterns in use cases

00:03:36,959 --> 00:03:43,280
with uh structured data you have

00:03:40,400 --> 00:03:45,200
some traditional analytics with lots of

00:03:43,280 --> 00:03:48,319
rows and columns a lot of

00:03:45,200 --> 00:03:49,680
a lot of feature

00:03:48,319 --> 00:03:52,560
feature engineering where you're

00:03:49,680 --> 00:03:55,840
determining some rules-based

00:03:52,560 --> 00:03:59,120
views of your data or looking at

00:03:55,840 --> 00:04:02,239
very specific columns

00:03:59,120 --> 00:04:05,200
and labels for for your data

00:04:02,239 --> 00:04:06,400
you get a lot of accuracy as you um are

00:04:05,200 --> 00:04:09,120
able to develop

00:04:06,400 --> 00:04:09,599
better future engineering and you get

00:04:09,120 --> 00:04:12,959
better

00:04:09,599 --> 00:04:14,159
and faster results as you can use gpu

00:04:12,959 --> 00:04:18,000
servers

00:04:14,159 --> 00:04:21,519
and gpus on servers to uh to accelerate

00:04:18,000 --> 00:04:24,400
your compute part for this the

00:04:21,519 --> 00:04:26,840
second type of use cases typically

00:04:24,400 --> 00:04:29,759
unstructured data like

00:04:26,840 --> 00:04:31,680
images that's computer vision models

00:04:29,759 --> 00:04:34,320
and this is where deep learning

00:04:31,680 --> 00:04:36,720
techniques and algorithms come into play

00:04:34,320 --> 00:04:37,520
and a model is learning to detect

00:04:36,720 --> 00:04:41,680
objects

00:04:37,520 --> 00:04:45,120
classify objects classify images

00:04:41,680 --> 00:04:48,320
do action detection

00:04:45,120 --> 00:04:51,280
segmentation and the

00:04:48,320 --> 00:04:52,639
vision models can take a broad spectrum

00:04:51,280 --> 00:04:55,360
of use cases

00:04:52,639 --> 00:04:56,000
where it's not just pixels and images as

00:04:55,360 --> 00:04:58,320
we think about

00:04:56,000 --> 00:04:59,199
them but basically anything that you can

00:04:58,320 --> 00:05:02,560
transform

00:04:59,199 --> 00:05:05,120
into a representation of um

00:05:02,560 --> 00:05:05,600
of an image so that includes satellite

00:05:05,120 --> 00:05:09,840
data

00:05:05,600 --> 00:05:12,080
infrared data sound spectography

00:05:09,840 --> 00:05:14,960
basically anything that you can

00:05:12,080 --> 00:05:16,560
apply deep learning techniques to

00:05:14,960 --> 00:05:19,440
matrices of data

00:05:16,560 --> 00:05:20,479
you can apply these these algorithms and

00:05:19,440 --> 00:05:23,039
then the final

00:05:20,479 --> 00:05:24,080
category of use cases is around a

00:05:23,039 --> 00:05:27,840
natural language

00:05:24,080 --> 00:05:30,639
processing and this is um a lot of

00:05:27,840 --> 00:05:32,639
techniques that have developed to to

00:05:30,639 --> 00:05:36,639
really get a better understanding for

00:05:32,639 --> 00:05:39,680
speech to text and text processing

00:05:36,639 --> 00:05:44,800
word to vec model for example is

00:05:39,680 --> 00:05:44,800
a very popular use case in that space

00:05:45,120 --> 00:05:49,440
and of course these are all applied to

00:05:47,919 --> 00:05:52,720
different industries

00:05:49,440 --> 00:05:55,840
i'm sure you've all seen examples across

00:05:52,720 --> 00:05:57,840
basically every industry where you can

00:05:55,840 --> 00:06:01,919
look at

00:05:57,840 --> 00:06:04,479
ai as a as a builder of improving your

00:06:01,919 --> 00:06:05,520
your insights so whether it be fraud and

00:06:04,479 --> 00:06:08,800
banking

00:06:05,520 --> 00:06:11,360
safety inspection or manufacturing

00:06:08,800 --> 00:06:14,000
quality inspection process improvements

00:06:11,360 --> 00:06:17,840
and of course defense and security

00:06:14,000 --> 00:06:17,840
have plenty of use cases as well

00:06:18,000 --> 00:06:21,840
so we know that it's one of the fastest

00:06:20,479 --> 00:06:23,840
growing workloads

00:06:21,840 --> 00:06:24,880
and um what we're going to talk about

00:06:23,840 --> 00:06:26,960
today is not

00:06:24,880 --> 00:06:28,000
just building the models or training the

00:06:26,960 --> 00:06:31,520
models but

00:06:28,000 --> 00:06:34,720
how these models apply in the enterprise

00:06:31,520 --> 00:06:37,919
and how it really is truly an end-to-end

00:06:34,720 --> 00:06:40,960
view view of the world

00:06:37,919 --> 00:06:43,039
so we talked about the ai ladder

00:06:40,960 --> 00:06:44,160
in ibm there's been some some books

00:06:43,039 --> 00:06:46,960
recently published

00:06:44,160 --> 00:06:47,600
on this as well and the idea is it

00:06:46,960 --> 00:06:50,479
starts

00:06:47,600 --> 00:06:52,639
with data you need to collect your data

00:06:50,479 --> 00:06:55,199
make it simple and accessible

00:06:52,639 --> 00:06:56,319
you need to organize it know what your

00:06:55,199 --> 00:06:58,800
data is

00:06:56,319 --> 00:06:59,840
and create a trusted foundation for that

00:06:58,800 --> 00:07:01,919
data

00:06:59,840 --> 00:07:03,680
then you need to be able to analyze it

00:07:01,919 --> 00:07:06,319
this is where building models and

00:07:03,680 --> 00:07:08,560
iterating on that process and making

00:07:06,319 --> 00:07:09,840
sure you've got trust and transparency

00:07:08,560 --> 00:07:12,560
in those models

00:07:09,840 --> 00:07:13,759
and then finally being able to apply

00:07:12,560 --> 00:07:15,599
those models

00:07:13,759 --> 00:07:17,120
because without that you're really not

00:07:15,599 --> 00:07:19,759
leveraging

00:07:17,120 --> 00:07:20,960
the benefits of the model so the idea is

00:07:19,759 --> 00:07:23,440
to infuse

00:07:20,960 --> 00:07:24,400
we sometimes talk talk about deploying

00:07:23,440 --> 00:07:27,440
the model

00:07:24,400 --> 00:07:30,639
or inferencing and that's basically

00:07:27,440 --> 00:07:34,720
putting ai models and applying them

00:07:30,639 --> 00:07:37,840
across your your business processes

00:07:34,720 --> 00:07:40,800
so it starts with the data and it ends

00:07:37,840 --> 00:07:42,639
with being able to apply it in your

00:07:40,800 --> 00:07:46,960
processes in your enterprise

00:07:42,639 --> 00:07:46,960
and then it iterates from there

00:07:48,560 --> 00:07:55,199
so when i say data i mean

00:07:51,599 --> 00:07:57,120
any type of data so not just

00:07:55,199 --> 00:07:58,800
traditional enterprise data like

00:07:57,120 --> 00:08:01,520
transactional

00:07:58,800 --> 00:08:02,400
data and application data we're also

00:08:01,520 --> 00:08:06,160
talking about

00:08:02,400 --> 00:08:09,039
sensor data other enterprise com

00:08:06,160 --> 00:08:11,840
content coming from various parts of

00:08:09,039 --> 00:08:16,160
your data center your infrastructure

00:08:11,840 --> 00:08:19,280
images geospatial data sound data

00:08:16,160 --> 00:08:22,560
videos social data weather

00:08:19,280 --> 00:08:24,960
pattern data third party data

00:08:22,560 --> 00:08:26,000
any type of data that you can come come

00:08:24,960 --> 00:08:28,400
up with that will give

00:08:26,000 --> 00:08:29,919
you insights into your into your

00:08:28,400 --> 00:08:33,200
business

00:08:29,919 --> 00:08:36,640
and those are applied to come out

00:08:33,200 --> 00:08:37,519
with outcomes and those range from chat

00:08:36,640 --> 00:08:40,800
bots

00:08:37,519 --> 00:08:43,599
supply chain optimization assembly line

00:08:40,800 --> 00:08:45,440
quality inspection pricing

00:08:43,599 --> 00:08:49,519
recommendation engines

00:08:45,440 --> 00:08:52,640
risk fraud decision support

00:08:49,519 --> 00:08:56,000
vision systems autonomous systems like

00:08:52,640 --> 00:08:58,160
autonomous driving or even autonomous

00:08:56,000 --> 00:09:01,519
yachts these days

00:08:58,160 --> 00:09:04,000
behavior modeling sentiment analysis

00:09:01,519 --> 00:09:05,920
all those types of applications that are

00:09:04,000 --> 00:09:08,480
emerging

00:09:05,920 --> 00:09:09,519
leverage the data that sits in your

00:09:08,480 --> 00:09:13,680
enterprise

00:09:09,519 --> 00:09:13,680
and outside of your enterprise

00:09:16,320 --> 00:09:22,399
when we talk about the end to end

00:09:19,440 --> 00:09:24,560
process in the pipeline if you look at

00:09:22,399 --> 00:09:27,519
it from a data perspective

00:09:24,560 --> 00:09:28,080
you really are starting with data coming

00:09:27,519 --> 00:09:30,640
in

00:09:28,080 --> 00:09:31,120
typically from edge devices or at the

00:09:30,640 --> 00:09:34,160
edge

00:09:31,120 --> 00:09:37,839
or at the consumer or client level

00:09:34,160 --> 00:09:40,880
you then ingest the data organize

00:09:37,839 --> 00:09:43,440
analyze do your runs

00:09:40,880 --> 00:09:44,800
with training of machine learning deep

00:09:43,440 --> 00:09:47,760
learning models to gain

00:09:44,800 --> 00:09:48,880
insights and at every step of the way

00:09:47,760 --> 00:09:52,320
you've got some

00:09:48,880 --> 00:09:55,519
data patterns that emerge that require

00:09:52,320 --> 00:09:59,120
a full data and ai

00:09:55,519 --> 00:10:01,680
architecture and the idea here is that

00:09:59,120 --> 00:10:03,680
the different types of data that you

00:10:01,680 --> 00:10:05,040
need and the different types of

00:10:03,680 --> 00:10:08,320
activities

00:10:05,040 --> 00:10:10,880
that you're needing that data for

00:10:08,320 --> 00:10:12,160
will drive some of this data

00:10:10,880 --> 00:10:14,560
architecture

00:10:12,160 --> 00:10:15,279
whether it's transient storage when

00:10:14,560 --> 00:10:18,320
you're

00:10:15,279 --> 00:10:20,800
bringing data in or ingestion how you're

00:10:18,320 --> 00:10:23,680
doing the ingestion whether you need

00:10:20,800 --> 00:10:25,680
a performance tier to support that

00:10:23,680 --> 00:10:29,680
because you're ingesting

00:10:25,680 --> 00:10:31,279
real time or how you organize your data

00:10:29,680 --> 00:10:33,920
whether you can leverage

00:10:31,279 --> 00:10:34,720
metadata and tagging of your data such

00:10:33,920 --> 00:10:37,360
as

00:10:34,720 --> 00:10:38,320
location size and any kind of

00:10:37,360 --> 00:10:41,040
information

00:10:38,320 --> 00:10:42,079
that goes along with your data and how

00:10:41,040 --> 00:10:44,160
you

00:10:42,079 --> 00:10:45,440
extract transform and load your data

00:10:44,160 --> 00:10:48,560
also drives

00:10:45,440 --> 00:10:50,079
your your storage requirements and how

00:10:48,560 --> 00:10:51,760
you're going to use your data for your

00:10:50,079 --> 00:10:54,160
models

00:10:51,760 --> 00:10:55,920
finally you get your data into a state

00:10:54,160 --> 00:10:59,040
that you're going to be able to

00:10:55,920 --> 00:11:01,839
bring it in and push it back out as you

00:10:59,040 --> 00:11:05,600
leverage your trained ai models

00:11:01,839 --> 00:11:09,360
and archive your data as you need to

00:11:05,600 --> 00:11:11,600
as a byproduct of that and then finally

00:11:09,360 --> 00:11:12,959
as i mentioned before everything is very

00:11:11,600 --> 00:11:15,680
iterative so

00:11:12,959 --> 00:11:17,360
everything that you go through one pass

00:11:15,680 --> 00:11:18,640
to train a model and then to leverage

00:11:17,360 --> 00:11:22,000
that train model

00:11:18,640 --> 00:11:25,120
you're going to want to have a quick and

00:11:22,000 --> 00:11:28,480
organized way to

00:11:25,120 --> 00:11:30,000
take that data and evaluate it again

00:11:28,480 --> 00:11:32,320
and see if you need to update your

00:11:30,000 --> 00:11:35,600
models update your business processes

00:11:32,320 --> 00:11:39,120
and how you use the models as a result

00:11:35,600 --> 00:11:40,880
of of new data that's always coming in

00:11:39,120 --> 00:11:43,839
and corrections to the models that are

00:11:40,880 --> 00:11:43,839
always coming in

00:11:44,880 --> 00:11:48,560
so i mentioned already a little bit

00:11:46,640 --> 00:11:51,760
about the data piece

00:11:48,560 --> 00:11:53,279
the training piece is the um

00:11:51,760 --> 00:11:54,959
the part where you're developing the

00:11:53,279 --> 00:11:57,040
models where you

00:11:54,959 --> 00:11:59,279
really need to have a good understanding

00:11:57,040 --> 00:12:02,079
of what problem you're trying to solve

00:11:59,279 --> 00:12:04,079
and a good way to validate and trust

00:12:02,079 --> 00:12:07,120
your models to make sure

00:12:04,079 --> 00:12:09,200
that what you're building is

00:12:07,120 --> 00:12:10,560
is going to be useful in solving your

00:12:09,200 --> 00:12:13,600
business problems

00:12:10,560 --> 00:12:15,279
and then finally when you're ready to to

00:12:13,600 --> 00:12:18,399
deploy the model

00:12:15,279 --> 00:12:19,920
you're going to look at the types of key

00:12:18,399 --> 00:12:23,279
performance indicators

00:12:19,920 --> 00:12:23,279
and performance

00:12:23,839 --> 00:12:28,959
metrics that you need to make sure that

00:12:26,720 --> 00:12:32,320
your model is being used the way you

00:12:28,959 --> 00:12:35,839
you intend it to be used

00:12:32,320 --> 00:12:39,200
this also drives a lot of skills

00:12:35,839 --> 00:12:42,000
work that goes on whether it's

00:12:39,200 --> 00:12:42,800
data scientists or data engineers up

00:12:42,000 --> 00:12:46,639
front

00:12:42,800 --> 00:12:48,880
to it operations and devops folks

00:12:46,639 --> 00:12:49,839
at the inference level that are building

00:12:48,880 --> 00:12:52,720
your models

00:12:49,839 --> 00:12:53,760
you need a new way of communicating with

00:12:52,720 --> 00:12:57,600
these teams

00:12:53,760 --> 00:13:00,720
and a new way to manage the iterative

00:12:57,600 --> 00:13:03,839
process of how how this is working

00:13:00,720 --> 00:13:06,240
so we're seeing a lot of uh changes in

00:13:03,839 --> 00:13:07,279
in process flows and end-to-end

00:13:06,240 --> 00:13:09,839
workflows

00:13:07,279 --> 00:13:10,720
as a result of the communication that

00:13:09,839 --> 00:13:14,959
needs to happen

00:13:10,720 --> 00:13:14,959
between between the various parties

00:13:17,120 --> 00:13:21,920
so the um the piece of code

00:13:20,160 --> 00:13:23,279
that we typically talk about when we're

00:13:21,920 --> 00:13:26,480
talking about ai is

00:13:23,279 --> 00:13:28,000
very often thought of as the uh the

00:13:26,480 --> 00:13:31,680
training the building the model

00:13:28,000 --> 00:13:33,440
and that's in a sense the fun part

00:13:31,680 --> 00:13:34,720
but when you're deploying this in an

00:13:33,440 --> 00:13:37,200
enterprise there are

00:13:34,720 --> 00:13:39,600
many many other pieces that need to

00:13:37,200 --> 00:13:43,279
factor into this

00:13:39,600 --> 00:13:44,560
into this process starting from the data

00:13:43,279 --> 00:13:47,760
side in terms of

00:13:44,560 --> 00:13:50,880
how you collect the data feature

00:13:47,760 --> 00:13:51,839
extraction data verification having the

00:13:50,880 --> 00:13:54,959
right tools

00:13:51,839 --> 00:13:55,920
and the right processes to manage the

00:13:54,959 --> 00:13:59,760
models

00:13:55,920 --> 00:14:02,320
makes a big difference how you deploy

00:13:59,760 --> 00:14:03,600
in terms of resources system resources

00:14:02,320 --> 00:14:05,839
whether it's

00:14:03,600 --> 00:14:07,279
in the cloud on-premise or hybrid

00:14:05,839 --> 00:14:10,240
solutions

00:14:07,279 --> 00:14:11,120
understanding how all those pieces come

00:14:10,240 --> 00:14:12,800
together

00:14:11,120 --> 00:14:16,000
how they're monitored and how you

00:14:12,800 --> 00:14:18,639
maintain security privacy and governance

00:14:16,000 --> 00:14:19,680
all take an important role in how you

00:14:18,639 --> 00:14:24,800
deploy

00:14:19,680 --> 00:14:24,800
an nai solution in in the enterprise

00:14:27,920 --> 00:14:34,639
the pillars of trust for this is also

00:14:31,440 --> 00:14:38,079
critical we see this in the news as well

00:14:34,639 --> 00:14:41,440
and factoring how you're using

00:14:38,079 --> 00:14:44,560
ai in in your enterprise is

00:14:41,440 --> 00:14:47,600
is critical is is it fair

00:14:44,560 --> 00:14:50,560
is it easy to understand is it secure

00:14:47,600 --> 00:14:51,920
is it accountable so all these pieces

00:14:50,560 --> 00:14:55,120
are components

00:14:51,920 --> 00:14:57,040
of uh of an enterprise deployment that

00:14:55,120 --> 00:15:01,120
need to come into play before

00:14:57,040 --> 00:15:01,120
a full production deployment

00:15:04,079 --> 00:15:10,800
now coming back into where open power

00:15:07,519 --> 00:15:13,440
plays and infrastructure matters

00:15:10,800 --> 00:15:15,120
is is the fact that through that

00:15:13,440 --> 00:15:17,360
workflow

00:15:15,120 --> 00:15:18,880
you can't be slowed down by any piece

00:15:17,360 --> 00:15:21,920
and you can't

00:15:18,880 --> 00:15:24,240
ignore any of the pieces so it is

00:15:21,920 --> 00:15:26,240
important to have

00:15:24,240 --> 00:15:27,920
a good understanding of your pipeline

00:15:26,240 --> 00:15:30,880
and and to understand

00:15:27,920 --> 00:15:31,360
where your bottlenecks might be and to

00:15:30,880 --> 00:15:33,519
have

00:15:31,360 --> 00:15:35,759
an infrastructure that's built to

00:15:33,519 --> 00:15:38,399
support the types of

00:15:35,759 --> 00:15:40,000
compute storage and network requirements

00:15:38,399 --> 00:15:43,920
that are required

00:15:40,000 --> 00:15:43,920
for your ai deployment

00:15:45,440 --> 00:15:51,920
when we talk about deployment relative

00:15:48,639 --> 00:15:53,680
to um to that end-to-end workflow some

00:15:51,920 --> 00:15:56,959
of the considerations

00:15:53,680 --> 00:16:00,079
to to factor in are um

00:15:56,959 --> 00:16:02,079
what happens at the front end in terms

00:16:00,079 --> 00:16:05,680
of ingestion

00:16:02,079 --> 00:16:08,800
are we able to keep up with um with the

00:16:05,680 --> 00:16:11,839
performance and the scale of

00:16:08,800 --> 00:16:12,560
your data coming in is it secure do you

00:16:11,839 --> 00:16:15,920
have a

00:16:12,560 --> 00:16:16,720
good tiered architecture that puts the

00:16:15,920 --> 00:16:18,800
focus

00:16:16,720 --> 00:16:20,000
on the type of storage that you need for

00:16:18,800 --> 00:16:23,040
the type of data

00:16:20,000 --> 00:16:26,240
that you're storing in terms of the

00:16:23,040 --> 00:16:29,759
model piece you're looking at

00:16:26,240 --> 00:16:31,519
understanding whether your data has

00:16:29,759 --> 00:16:32,880
metadata that you can leverage and do

00:16:31,519 --> 00:16:35,199
you have a good system

00:16:32,880 --> 00:16:36,079
for making sure that that's well

00:16:35,199 --> 00:16:38,720
communicated

00:16:36,079 --> 00:16:39,360
and accessible do you have a single name

00:16:38,720 --> 00:16:42,240
space

00:16:39,360 --> 00:16:43,519
uh file system if that's what um you

00:16:42,240 --> 00:16:46,399
need to access

00:16:43,519 --> 00:16:47,040
your data in inside your data lake or

00:16:46,399 --> 00:16:49,440
directly

00:16:47,040 --> 00:16:51,040
for uh for training a model and doing

00:16:49,440 --> 00:16:54,399
the data prep

00:16:51,040 --> 00:16:57,600
and at the edge you have

00:16:54,399 --> 00:16:58,160
sufficiently low latency systems that

00:16:57,600 --> 00:17:00,639
get you

00:16:58,160 --> 00:17:02,720
um the the information that you need

00:17:00,639 --> 00:17:05,839
from your trained models

00:17:02,720 --> 00:17:05,839
fast enough

00:17:07,199 --> 00:17:11,039
when we talk about some of the

00:17:09,439 --> 00:17:12,880
infrastructure needs

00:17:11,039 --> 00:17:14,640
they are different whether it's for

00:17:12,880 --> 00:17:19,199
training or inferencing of

00:17:14,640 --> 00:17:23,039
ai models training typically needs

00:17:19,199 --> 00:17:25,679
fast accelerators um and

00:17:23,039 --> 00:17:26,319
typically a lot of systems if you can

00:17:25,679 --> 00:17:30,160
leverage

00:17:26,319 --> 00:17:34,559
training uh in parallel high bandwidth

00:17:30,160 --> 00:17:37,280
io and low latency and some

00:17:34,559 --> 00:17:38,720
data center support for for those

00:17:37,280 --> 00:17:43,120
systems

00:17:38,720 --> 00:17:46,080
for inference you you may need

00:17:43,120 --> 00:17:47,360
faster latency you may need to be able

00:17:46,080 --> 00:17:50,799
to distribute

00:17:47,360 --> 00:17:54,160
your data your models and

00:17:50,799 --> 00:17:55,280
your applications across multiple data

00:17:54,160 --> 00:17:58,880
centers

00:17:55,280 --> 00:18:02,480
scalability also matters here and

00:17:58,880 --> 00:18:05,679
you may also need a wider variety of

00:18:02,480 --> 00:18:08,880
flexible storage solutions depending on

00:18:05,679 --> 00:18:09,600
the data demands of of your applications

00:18:08,880 --> 00:18:15,840
and the

00:18:09,600 --> 00:18:15,840
model inference that's taking place

00:18:16,559 --> 00:18:20,880
some of the typical inferencing

00:18:19,039 --> 00:18:25,520
scenarios

00:18:20,880 --> 00:18:29,200
can be on-prem or in cloud or hybrid

00:18:25,520 --> 00:18:32,080
and it can be at the data center level

00:18:29,200 --> 00:18:33,600
at a server individual server level and

00:18:32,080 --> 00:18:36,640
can also be

00:18:33,600 --> 00:18:39,710
on standalone devices and

00:18:36,640 --> 00:18:40,880
on edge edge devices

00:18:39,710 --> 00:18:43,760
[Music]

00:18:40,880 --> 00:18:44,640
connected back to servers when you're

00:18:43,760 --> 00:18:48,400
looking at

00:18:44,640 --> 00:18:50,880
um the your inferencing considerations

00:18:48,400 --> 00:18:52,000
you're still looking at your model

00:18:50,880 --> 00:18:54,880
accuracy

00:18:52,000 --> 00:18:56,000
as the outcome of your trained models

00:18:54,880 --> 00:18:57,840
accuracy meaning

00:18:56,000 --> 00:19:00,240
a wide range of things depending on your

00:18:57,840 --> 00:19:02,640
business needs it could be

00:19:00,240 --> 00:19:03,520
recall it could be something else but

00:19:02,640 --> 00:19:06,880
the key here

00:19:03,520 --> 00:19:08,480
is to look at all the different pieces

00:19:06,880 --> 00:19:10,559
that that matter to you when you're

00:19:08,480 --> 00:19:13,760
deploying an inferencing

00:19:10,559 --> 00:19:14,559
solution are you doing real-time

00:19:13,760 --> 00:19:17,360
inferencing

00:19:14,559 --> 00:19:18,799
versus batch inferencing how much do you

00:19:17,360 --> 00:19:21,600
need to scale

00:19:18,799 --> 00:19:22,000
what type of data are you feeding into

00:19:21,600 --> 00:19:25,200
your

00:19:22,000 --> 00:19:28,559
inference engines

00:19:25,200 --> 00:19:32,080
what what security is required

00:19:28,559 --> 00:19:35,280
are you running a multi-tenancy

00:19:32,080 --> 00:19:37,600
environment are you

00:19:35,280 --> 00:19:39,120
supporting multiple tools multiple

00:19:37,600 --> 00:19:41,840
applications

00:19:39,120 --> 00:19:42,240
and how are you keeping track of all of

00:19:41,840 --> 00:19:45,039
this

00:19:42,240 --> 00:19:46,480
whether it's the model management the

00:19:45,039 --> 00:19:49,360
data management

00:19:46,480 --> 00:19:51,120
the resource management and systems

00:19:49,360 --> 00:19:53,520
management in general

00:19:51,120 --> 00:19:55,120
and finally as i mentioned before the

00:19:53,520 --> 00:20:00,160
trust and transparency

00:19:55,120 --> 00:20:00,160
so that everything is always explainable

00:20:04,400 --> 00:20:11,760
so when you put all this together

00:20:07,919 --> 00:20:16,080
a typical architecture

00:20:11,760 --> 00:20:19,280
might look like transactions coming in

00:20:16,080 --> 00:20:21,039
or sensor data emerging iot information

00:20:19,280 --> 00:20:24,240
coming in

00:20:21,039 --> 00:20:27,919
feeding into operational

00:20:24,240 --> 00:20:31,440
database and modern databases

00:20:27,919 --> 00:20:35,200
mongodb or data lakes where you've got

00:20:31,440 --> 00:20:37,120
this whole data governance component

00:20:35,200 --> 00:20:38,240
that comes in this is also where you

00:20:37,120 --> 00:20:42,799
might

00:20:38,240 --> 00:20:46,320
focus on metadata tagging and management

00:20:42,799 --> 00:20:49,919
and basically building out uh

00:20:46,320 --> 00:20:52,559
knowledge databases then you've got a

00:20:49,919 --> 00:20:53,520
data science workbench this could be

00:20:52,559 --> 00:20:56,559
open source

00:20:53,520 --> 00:20:59,919
tools it could be other

00:20:56,559 --> 00:21:02,240
pre-packaged training

00:20:59,919 --> 00:21:05,360
instruments basically it's where the

00:21:02,240 --> 00:21:05,360
data scientists

00:21:05,440 --> 00:21:13,360
work and develop the models and then

00:21:08,720 --> 00:21:16,400
finally you could have an ai grid or

00:21:13,360 --> 00:21:19,919
as a service model in the cloud

00:21:16,400 --> 00:21:20,480
that manages your your inference and

00:21:19,919 --> 00:21:24,320
does

00:21:20,480 --> 00:21:28,559
the um the model deployment

00:21:24,320 --> 00:21:31,600
for that so we do see

00:21:28,559 --> 00:21:34,640
an emerging platform for ai

00:21:31,600 --> 00:21:36,320
that talks about co-location and where

00:21:34,640 --> 00:21:39,760
hybrid cloud

00:21:36,320 --> 00:21:42,799
play plays a role and manages the uh

00:21:39,760 --> 00:21:46,960
the different application requirements

00:21:42,799 --> 00:21:46,960
relative to the ai deployment

00:21:49,360 --> 00:21:58,799
so an example of this in manufacturing

00:21:53,919 --> 00:22:02,159
would be an initial factory location

00:21:58,799 --> 00:22:04,880
would do a a proof of concept

00:22:02,159 --> 00:22:06,240
would collect data to build an initial

00:22:04,880 --> 00:22:10,320
model

00:22:06,240 --> 00:22:13,120
would iterate on that model

00:22:10,320 --> 00:22:14,320
make sure that the model itself meets

00:22:13,120 --> 00:22:18,080
the

00:22:14,320 --> 00:22:20,240
kpis of the business for for that model

00:22:18,080 --> 00:22:21,280
and there would be some work to uh

00:22:20,240 --> 00:22:24,559
integrate it

00:22:21,280 --> 00:22:27,679
with the application pieces in this case

00:22:24,559 --> 00:22:30,880
it might be a robotics

00:22:27,679 --> 00:22:32,640
communication with

00:22:30,880 --> 00:22:34,159
with the outcomes of the model to take

00:22:32,640 --> 00:22:37,760
certain actions

00:22:34,159 --> 00:22:41,919
and you would have servers

00:22:37,760 --> 00:22:45,919
and storage to uh to manage all that

00:22:41,919 --> 00:22:48,400
at at the factory uh level

00:22:45,919 --> 00:22:50,480
once you're happy with that proof of

00:22:48,400 --> 00:22:53,280
concept and uh maybe even the

00:22:50,480 --> 00:22:54,880
initial deployment into production you

00:22:53,280 --> 00:22:57,600
would then

00:22:54,880 --> 00:22:59,120
be able to bring that in in a more

00:22:57,600 --> 00:23:02,400
centralized area

00:22:59,120 --> 00:23:03,360
and cloud environment where you can

00:23:02,400 --> 00:23:05,840
automate

00:23:03,360 --> 00:23:05,840
more of the

00:23:06,559 --> 00:23:13,760
model deployment and roll that out

00:23:10,080 --> 00:23:16,159
to uh to multiple factories and then

00:23:13,760 --> 00:23:17,200
use the incoming information that comes

00:23:16,159 --> 00:23:21,200
back in

00:23:17,200 --> 00:23:24,960
to um to manage dashboards on

00:23:21,200 --> 00:23:26,880
on overall performance business

00:23:24,960 --> 00:23:29,440
business application performance that

00:23:26,880 --> 00:23:32,559
have these ai components in them

00:23:29,440 --> 00:23:36,400
and then feed that back into

00:23:32,559 --> 00:23:40,240
new new new systems

00:23:36,400 --> 00:23:43,520
um and new updates for for your models

00:23:40,240 --> 00:23:43,520
in in your inferencing

00:23:45,760 --> 00:23:50,400
um a specific example that that we've

00:23:49,120 --> 00:23:54,080
used

00:23:50,400 --> 00:23:56,320
using some some ibm tools that were

00:23:54,080 --> 00:23:58,080
basically taking some of the open source

00:23:56,320 --> 00:23:59,919
ideas and packaging them up

00:23:58,080 --> 00:24:02,080
so that you didn't have to have deep

00:23:59,919 --> 00:24:04,640
data scientist skills

00:24:02,080 --> 00:24:07,120
is um something called the ibm maximo

00:24:04,640 --> 00:24:10,000
visual inspection

00:24:07,120 --> 00:24:11,039
which basically looks at inspection

00:24:10,000 --> 00:24:14,080
points

00:24:11,039 --> 00:24:17,039
within a supply chain and you build

00:24:14,080 --> 00:24:20,960
models that look for specific

00:24:17,039 --> 00:24:24,080
anomalies or quality defects

00:24:20,960 --> 00:24:25,200
at specific stations and you feed all

00:24:24,080 --> 00:24:28,720
that back in

00:24:25,200 --> 00:24:28,720
in an automated way from

00:24:29,200 --> 00:24:36,060
ios devices that feed back into

00:24:32,320 --> 00:24:38,159
dashboards and basically look at

00:24:36,060 --> 00:24:41,600
[Music]

00:24:38,159 --> 00:24:44,799
actions to take based on

00:24:41,600 --> 00:24:47,120
triggers that that you see

00:24:44,799 --> 00:24:49,600
when you've got a defective part for

00:24:47,120 --> 00:24:49,600
example

00:24:52,159 --> 00:24:55,039
so in summary

00:24:55,360 --> 00:25:01,440
putting ai in the enterprise

00:24:59,120 --> 00:25:02,400
is not is not just about building a

00:25:01,440 --> 00:25:05,039
model

00:25:02,400 --> 00:25:07,520
it's about thinking of all the pieces

00:25:05,039 --> 00:25:09,919
that have to go into

00:25:07,520 --> 00:25:11,120
being able to get to deploy a model and

00:25:09,919 --> 00:25:14,400
it starts

00:25:11,120 --> 00:25:17,760
with understanding the use case and how

00:25:14,400 --> 00:25:19,760
you might be able to leverage ai

00:25:17,760 --> 00:25:21,679
making sure that you've got the right

00:25:19,760 --> 00:25:26,320
data to train

00:25:21,679 --> 00:25:30,080
and accept new data to inference

00:25:26,320 --> 00:25:31,600
those models it's just a piece of the

00:25:30,080 --> 00:25:34,559
overall workflow

00:25:31,600 --> 00:25:35,679
and the trusted component of it and the

00:25:34,559 --> 00:25:39,919
governance

00:25:35,679 --> 00:25:42,640
is truly critical to get into production

00:25:39,919 --> 00:25:44,640
finally infrastructure does matter being

00:25:42,640 --> 00:25:47,039
able to have

00:25:44,640 --> 00:25:49,200
the right resources at the right time in

00:25:47,039 --> 00:25:51,600
the right environments

00:25:49,200 --> 00:25:52,559
is all something that needs to be

00:25:51,600 --> 00:25:56,320
thought out

00:25:52,559 --> 00:25:59,440
as you deploy an ai architecture

00:25:56,320 --> 00:26:02,480
and as i said um

00:25:59,440 --> 00:26:05,679
earlier is that collaboratively this

00:26:02,480 --> 00:26:06,400
is a new a new way of thinking in the

00:26:05,679 --> 00:26:08,960
journey

00:26:06,400 --> 00:26:10,559
to get to this because you've got all

00:26:08,960 --> 00:26:13,600
the pieces that need to talk

00:26:10,559 --> 00:26:17,200
to each other to um to get the most

00:26:13,600 --> 00:26:17,200
out of your ai systems

00:26:19,679 --> 00:26:28,480
that concludes my presentation

00:26:24,799 --> 00:26:30,320
if there are any questions

00:26:28,480 --> 00:26:34,000
please put them in the q a or i can

00:26:30,320 --> 00:26:34,000

YouTube URL: https://www.youtube.com/watch?v=4zC9uiwb20w


