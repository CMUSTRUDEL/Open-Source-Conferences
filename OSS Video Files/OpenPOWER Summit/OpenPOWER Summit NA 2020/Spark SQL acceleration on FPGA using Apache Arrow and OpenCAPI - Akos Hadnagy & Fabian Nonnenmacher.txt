Title: Spark SQL acceleration on FPGA using Apache Arrow and OpenCAPI - Akos Hadnagy & Fabian Nonnenmacher
Publication date: 2020-09-21
Playlist: OpenPOWER Summit NA 2020
Description: 
	Spark SQL acceleration on FPGA using Apache Arrow and OpenCAPI - Akos Hadnagy & Fabian Nonnenmacher, Delft University of Technology

Speakers: Ãkos Hadnagy, Fabian Nonnenmacher

One of the research topics of the Accelerated Big Data Systems group at TU Delft is transparent acceleration of Spark SQL programs on FPGA. This talk discusses our design, the modifications that were necessary in Spark, FPGA programming tools that were developed, and a fully integrated proof-of-concept on an OpenCAPI platform.
Captions: 
	00:00:01,360 --> 00:00:05,920
okay hi i think we can start thank you

00:00:03,360 --> 00:00:08,880
all for being here i'm akos and my

00:00:05,920 --> 00:00:10,320
co-presenter is fabian and in the next

00:00:08,880 --> 00:00:11,120
about half an hour we're going to start

00:00:10,320 --> 00:00:13,360
about

00:00:11,120 --> 00:00:16,640
we're going to talk about accelerating

00:00:13,360 --> 00:00:18,800
spark sql queries on fpgas using apache

00:00:16,640 --> 00:00:21,279
aero and opencapi

00:00:18,800 --> 00:00:23,519
so first we're gonna dive into some of

00:00:21,279 --> 00:00:24,960
the challenges of accelerating big data

00:00:23,519 --> 00:00:27,039
analytics because

00:00:24,960 --> 00:00:28,840
this is a particularly challenging field

00:00:27,039 --> 00:00:30,480
involving many technologies and

00:00:28,840 --> 00:00:32,399
expertise and

00:00:30,480 --> 00:00:34,000
then we're going to take a look at spark

00:00:32,399 --> 00:00:37,040
square and

00:00:34,000 --> 00:00:38,719
its integration with apache aero and

00:00:37,040 --> 00:00:39,680
we're going to show a proof of concept

00:00:38,719 --> 00:00:42,480
application

00:00:39,680 --> 00:00:44,399
for you which was already tased in the

00:00:42,480 --> 00:00:45,360
previous presentation from this lab by

00:00:44,399 --> 00:00:48,399
yoast

00:00:45,360 --> 00:00:50,559
so those who were there

00:00:48,399 --> 00:00:52,160
you you've seen it already and then

00:00:50,559 --> 00:00:54,960
we're going to look at the results and

00:00:52,160 --> 00:00:58,480
some lessons learned

00:00:54,960 --> 00:01:01,600
so let's start let's let's look at the

00:00:58,480 --> 00:01:03,920
big data analytics big picture so a big

00:01:01,600 --> 00:01:06,960
data application developers

00:01:03,920 --> 00:01:09,840
usually like to work on uh

00:01:06,960 --> 00:01:12,320
high level high levels actually ever

00:01:09,840 --> 00:01:15,920
higher levels of abstractions

00:01:12,320 --> 00:01:20,320
they are using frameworks that

00:01:15,920 --> 00:01:23,280
offer distributed execution resilience

00:01:20,320 --> 00:01:26,960
for tolerance and all in all a very high

00:01:23,280 --> 00:01:30,400
level of ah

00:01:26,960 --> 00:01:32,880
to express the application

00:01:30,400 --> 00:01:34,720
uh most of the time it's going to be

00:01:32,880 --> 00:01:37,200
represented as a lazily

00:01:34,720 --> 00:01:38,560
evaluated data flow graph for example

00:01:37,200 --> 00:01:41,200
and the

00:01:38,560 --> 00:01:42,880
framework takes care of the rest and on

00:01:41,200 --> 00:01:44,320
the other side of the spectrum we have

00:01:42,880 --> 00:01:48,000
the computing hardware

00:01:44,320 --> 00:01:49,920
in this case an fpga where

00:01:48,000 --> 00:01:51,520
you should present your design most of

00:01:49,920 --> 00:01:53,840
the time as a low-level

00:01:51,520 --> 00:01:56,079
hardware description or you might be

00:01:53,840 --> 00:01:56,719
able to use some high level synthesis

00:01:56,079 --> 00:01:59,360
too

00:01:56,719 --> 00:02:00,479
but all in all just the hardware

00:01:59,360 --> 00:02:02,799
development

00:02:00,479 --> 00:02:04,479
requires a wide range of expertise and

00:02:02,799 --> 00:02:06,840
we're not even talking about

00:02:04,479 --> 00:02:08,160
integration with higher level software

00:02:06,840 --> 00:02:10,160
frameworks

00:02:08,160 --> 00:02:11,280
and in the middle we have a knowledge

00:02:10,160 --> 00:02:14,319
gap that

00:02:11,280 --> 00:02:16,480
is i think still increasing

00:02:14,319 --> 00:02:17,920
but the accelerated big data system

00:02:16,480 --> 00:02:21,360
group proposes

00:02:17,920 --> 00:02:23,360
a stack of tools to uh to close this gap

00:02:21,360 --> 00:02:24,480
not to close to make it easier to jump

00:02:23,360 --> 00:02:27,920
over it

00:02:24,480 --> 00:02:28,959
so let's see how an accelerated

00:02:27,920 --> 00:02:32,000
application

00:02:28,959 --> 00:02:34,959
looks like so first of all we need

00:02:32,000 --> 00:02:36,959
some kind of horse side integration and

00:02:34,959 --> 00:02:39,840
that includes the memory layout

00:02:36,959 --> 00:02:41,280
of the data uh that we're gonna use for

00:02:39,840 --> 00:02:44,000
the acceleration

00:02:41,280 --> 00:02:45,680
also where is it in the memory and how

00:02:44,000 --> 00:02:48,840
can we access it

00:02:45,680 --> 00:02:50,160
and we also need an api to interact with

00:02:48,840 --> 00:02:52,640
this

00:02:50,160 --> 00:02:54,720
accelerator hardware then we have to

00:02:52,640 --> 00:02:57,840
choose a target platform a host

00:02:54,720 --> 00:03:01,680
accelerator interface that basically

00:02:57,840 --> 00:03:03,760
defines how the accelerator hardware is

00:03:01,680 --> 00:03:06,640
integrated into the rest of the system

00:03:03,760 --> 00:03:08,159
and how it interacts with it obviously

00:03:06,640 --> 00:03:11,760
one of these

00:03:08,159 --> 00:03:14,640
frameworks and technologies is opencapi

00:03:11,760 --> 00:03:16,640
and finally we do have to design the

00:03:14,640 --> 00:03:19,040
accelerator hardware itself

00:03:16,640 --> 00:03:19,760
including the control structure we

00:03:19,040 --> 00:03:22,239
usually

00:03:19,760 --> 00:03:25,200
want some kind of performance scaling to

00:03:22,239 --> 00:03:27,200
run some design space exploration

00:03:25,200 --> 00:03:29,280
on the hardware and arrive at an

00:03:27,200 --> 00:03:32,159
accelerator that satisfies

00:03:29,280 --> 00:03:32,879
all the design constraints and most of

00:03:32,159 --> 00:03:35,599
the time

00:03:32,879 --> 00:03:36,560
since engineering hours are expensive we

00:03:35,599 --> 00:03:40,000
do want some

00:03:36,560 --> 00:03:42,080
ip reuse we may have a lot of ip cores

00:03:40,000 --> 00:03:45,519
that we already developed

00:03:42,080 --> 00:03:47,120
eos already presented some compressors

00:03:45,519 --> 00:03:49,519
the compressors

00:03:47,120 --> 00:03:50,560
filters and all that and we would like

00:03:49,519 --> 00:03:53,360
to reuse these

00:03:50,560 --> 00:03:54,640
in many use cases and streamline

00:03:53,360 --> 00:03:57,439
hardware development

00:03:54,640 --> 00:03:58,080
so let's see where we are there is a new

00:03:57,439 --> 00:04:01,439
movement

00:03:58,080 --> 00:04:04,480
in the big data world apache arrow

00:04:01,439 --> 00:04:05,040
which aims to be a language agnostic in

00:04:04,480 --> 00:04:07,439
memory

00:04:05,040 --> 00:04:09,120
data format across languages and

00:04:07,439 --> 00:04:12,239
frameworks

00:04:09,120 --> 00:04:15,920
it's a columnar data format so all the

00:04:12,239 --> 00:04:17,840
data with the same data type and

00:04:15,920 --> 00:04:19,359
the data that resides in the same column

00:04:17,840 --> 00:04:21,199
are grouped together

00:04:19,359 --> 00:04:22,880
it's also meant to be a hardware

00:04:21,199 --> 00:04:24,880
friendly representation

00:04:22,880 --> 00:04:26,960
mostly meaning that whenever it is

00:04:24,880 --> 00:04:30,800
possible or the data is stored

00:04:26,960 --> 00:04:33,600
in memory in a contiguous way this

00:04:30,800 --> 00:04:35,600
makes it a very good vehicle to fit data

00:04:33,600 --> 00:04:37,360
flow hardware because this way we can

00:04:35,600 --> 00:04:41,040
utilize memory bandwidth

00:04:37,360 --> 00:04:44,160
as much as possible also by being

00:04:41,040 --> 00:04:46,800
a physical definition how the memory

00:04:44,160 --> 00:04:47,840
look how the data looks like in memory

00:04:46,800 --> 00:04:50,080
it offers

00:04:47,840 --> 00:04:51,120
zero copy interprocess communications

00:04:50,080 --> 00:04:53,199
and in this case

00:04:51,120 --> 00:04:54,240
not just between frameworks and

00:04:53,199 --> 00:04:56,479
languages

00:04:54,240 --> 00:04:57,360
between the host software and the

00:04:56,479 --> 00:05:01,520
accelerator

00:04:57,360 --> 00:05:04,240
as well alright so how can we use this

00:05:01,520 --> 00:05:06,320
uh the accelerated big data system group

00:05:04,240 --> 00:05:09,600
has a framework called fletcher

00:05:06,320 --> 00:05:12,400
which meant to bridge a huge gap

00:05:09,600 --> 00:05:13,120
in this figure it brings apache aero

00:05:12,400 --> 00:05:16,479
data

00:05:13,120 --> 00:05:18,800
to the fpga and allows the developer

00:05:16,479 --> 00:05:19,600
to read and write arrow formatted data

00:05:18,800 --> 00:05:22,080
in memory

00:05:19,600 --> 00:05:23,039
so as you can see in the high level

00:05:22,080 --> 00:05:25,759
workflow

00:05:23,039 --> 00:05:26,240
we have an application that reads and

00:05:25,759 --> 00:05:29,440
writes

00:05:26,240 --> 00:05:32,000
arrow record batches record batch is the

00:05:29,440 --> 00:05:35,039
abstraction abstraction introduced by

00:05:32,000 --> 00:05:37,280
arrow for tabular data

00:05:35,039 --> 00:05:39,840
based on this schema the record-based

00:05:37,280 --> 00:05:42,000
schema which defines how the columns

00:05:39,840 --> 00:05:44,560
look like basically what's the data type

00:05:42,000 --> 00:05:45,759
what is the data type of every column

00:05:44,560 --> 00:05:49,199
including that

00:05:45,759 --> 00:05:52,240
flasher can generate its interfaces

00:05:49,199 --> 00:05:54,240
and infrastructure and present the data

00:05:52,240 --> 00:05:56,240
to the developer and the accelerated

00:05:54,240 --> 00:05:58,960
algorithm

00:05:56,240 --> 00:06:01,280
it's important to point out that most of

00:05:58,960 --> 00:06:05,520
fletcher's hardware is platform agnostic

00:06:01,280 --> 00:06:08,000
but and it supports opencapi

00:06:05,520 --> 00:06:10,560
the important point is that this data is

00:06:08,000 --> 00:06:12,800
not presented as bytes or memory words

00:06:10,560 --> 00:06:15,840
to the accelerated algorithm

00:06:12,800 --> 00:06:16,800
the type of these interfaces are

00:06:15,840 --> 00:06:20,800
actually derived

00:06:16,800 --> 00:06:24,639
from the error schema so when we request

00:06:20,800 --> 00:06:27,520
data from the memory we do not send

00:06:24,639 --> 00:06:27,840
pointers or memory addresses we operate

00:06:27,520 --> 00:06:31,120
with

00:06:27,840 --> 00:06:31,759
row indices but let's take a look at

00:06:31,120 --> 00:06:34,960
what these

00:06:31,759 --> 00:06:37,520
interfaces look like and

00:06:34,960 --> 00:06:38,960
that's something for tidy a recently

00:06:37,520 --> 00:06:41,919
released specification

00:06:38,960 --> 00:06:43,440
for representing complex and dynamically

00:06:41,919 --> 00:06:46,080
sized data structures

00:06:43,440 --> 00:06:46,479
over hardware streams so the issue here

00:06:46,080 --> 00:06:48,720
is

00:06:46,479 --> 00:06:49,759
in big data analytics it is common to

00:06:48,720 --> 00:06:52,800
have

00:06:49,759 --> 00:06:54,319
complicated data structures for example

00:06:52,800 --> 00:06:55,520
on the left you can see a message that

00:06:54,319 --> 00:06:59,840
has a timestamp

00:06:55,520 --> 00:06:59,840
and has some text in its body

00:07:00,400 --> 00:07:03,759
in software you would represent this as

00:07:02,160 --> 00:07:05,840
a struct for example

00:07:03,759 --> 00:07:07,360
and if you look at the memory layout it

00:07:05,840 --> 00:07:09,840
is typically

00:07:07,360 --> 00:07:11,759
represented as a sequence of bytes or

00:07:09,840 --> 00:07:12,880
memory words depends on how you look at

00:07:11,759 --> 00:07:16,000
it

00:07:12,880 --> 00:07:18,240
but whenever we go to hardware

00:07:16,000 --> 00:07:20,479
and we want to stream this data we have

00:07:18,240 --> 00:07:22,319
to think about two resources spatial and

00:07:20,479 --> 00:07:24,319
temporal resources

00:07:22,319 --> 00:07:26,160
spatial as in bits and temporal

00:07:24,319 --> 00:07:29,520
resources as in transfers

00:07:26,160 --> 00:07:32,800
and tidy proposes the stream space

00:07:29,520 --> 00:07:35,840
abstraction to make this mapping

00:07:32,800 --> 00:07:37,120
in a formal way between complex data

00:07:35,840 --> 00:07:42,160
structures and software

00:07:37,120 --> 00:07:42,160
to streams in hardware

00:07:43,199 --> 00:07:48,720
right so where are we we have

00:07:46,560 --> 00:07:52,319
arrow as a pretty good asset to

00:07:48,720 --> 00:07:55,360
integrate with higher level frameworks

00:07:52,319 --> 00:07:56,080
data frame libraries and all that we

00:07:55,360 --> 00:07:59,440
have

00:07:56,080 --> 00:08:04,000
fletcher to bring this data to the fpga

00:07:59,440 --> 00:08:07,199
and use it we have tidy to represent

00:08:04,000 --> 00:08:08,639
aero and software data structures in

00:08:07,199 --> 00:08:11,360
hardware in an efficient

00:08:08,639 --> 00:08:12,639
and formal way so what do we what are we

00:08:11,360 --> 00:08:14,960
missing

00:08:12,639 --> 00:08:16,800
uh there is a meaningful way to

00:08:14,960 --> 00:08:17,199
integrate with higher level frameworks

00:08:16,800 --> 00:08:19,440
because

00:08:17,199 --> 00:08:20,319
arrow is just a data layer we want

00:08:19,440 --> 00:08:23,520
something more

00:08:20,319 --> 00:08:24,560
automated to capture the algorithm or

00:08:23,520 --> 00:08:27,759
the application

00:08:24,560 --> 00:08:30,720
from this very high level description

00:08:27,759 --> 00:08:32,240
and and develop or generate some

00:08:30,720 --> 00:08:34,719
hardware for it

00:08:32,240 --> 00:08:36,560
and on the other side of the spectrum we

00:08:34,719 --> 00:08:37,440
have tidy to represent these data types

00:08:36,560 --> 00:08:39,440
but we

00:08:37,440 --> 00:08:41,039
still have to do the computation somehow

00:08:39,440 --> 00:08:44,240
so either you have to develop

00:08:41,039 --> 00:08:45,440
your cores in hdr or with hrs or

00:08:44,240 --> 00:08:49,519
anything like that

00:08:45,440 --> 00:08:49,519
and we have to bridge this gap

00:08:49,760 --> 00:08:53,200
so let's start from the left and now i'm

00:08:52,160 --> 00:08:57,200
going to let fabian

00:08:53,200 --> 00:08:57,200
explain the spark sql parts

00:08:57,360 --> 00:09:02,560
thanks also hello from my side i am

00:09:00,320 --> 00:09:04,080
fabian and i'm going to tell you how we

00:09:02,560 --> 00:09:07,200
have extended spark

00:09:04,080 --> 00:09:08,880
to be able to call fpga kernels

00:09:07,200 --> 00:09:11,040
spark is a general purpose cluster

00:09:08,880 --> 00:09:12,640
computing system which became quite

00:09:11,040 --> 00:09:14,320
popular in the last years

00:09:12,640 --> 00:09:16,160
and clearly is one of the most used

00:09:14,320 --> 00:09:19,360
tools in the big data community

00:09:16,160 --> 00:09:21,120
today our work is especially based on

00:09:19,360 --> 00:09:24,480
one specific module

00:09:21,120 --> 00:09:28,160
on apache spark sql this module

00:09:24,480 --> 00:09:31,360
is built on top of the classical rdd api

00:09:28,160 --> 00:09:34,320
and it extends this functionality

00:09:31,360 --> 00:09:35,040
it enables spark to process structured

00:09:34,320 --> 00:09:37,360
data

00:09:35,040 --> 00:09:40,080
by letting the users define declarative

00:09:37,360 --> 00:09:42,880
queries such as sql

00:09:40,080 --> 00:09:44,800
with this approach spark sql gets us a

00:09:42,880 --> 00:09:47,360
lot of information about the data

00:09:44,800 --> 00:09:49,600
and the query and it can use this

00:09:47,360 --> 00:09:53,120
information to optimize the query

00:09:49,600 --> 00:09:56,080
before executing it when integrating

00:09:53,120 --> 00:09:59,200
spark sql we faced two main challenges

00:09:56,080 --> 00:10:00,959
one challenge is to modify spark's data

00:09:59,200 --> 00:10:02,000
format so that the data is stored in the

00:10:00,959 --> 00:10:04,560
error format

00:10:02,000 --> 00:10:05,440
and we can avoid costly transformations

00:10:04,560 --> 00:10:07,760
and secondly

00:10:05,440 --> 00:10:08,959
we had to tell spark that it actually

00:10:07,760 --> 00:10:11,600
executes our own

00:10:08,959 --> 00:10:12,640
code on not the internal default

00:10:11,600 --> 00:10:15,519
computation

00:10:12,640 --> 00:10:15,519
implementation

00:10:17,600 --> 00:10:24,560
traditionally uh spark

00:10:21,279 --> 00:10:26,959
processes the data row by row this

00:10:24,560 --> 00:10:28,560
means it loads one row into the memory

00:10:26,959 --> 00:10:32,720
processes the row

00:10:28,560 --> 00:10:35,760
and yeah and now with

00:10:32,720 --> 00:10:36,959
uh sorry however we rely actually on a

00:10:35,760 --> 00:10:39,360
batch processing

00:10:36,959 --> 00:10:40,399
because we want to use error and so we

00:10:39,360 --> 00:10:42,480
need to load the

00:10:40,399 --> 00:10:47,839
data in batches and processed in one

00:10:42,480 --> 00:10:47,839
data batch

00:10:48,800 --> 00:10:53,519
released in june 2020 spark has

00:10:51,120 --> 00:10:56,160
introduced a new feature

00:10:53,519 --> 00:10:57,920
the columnar processing support in

00:10:56,160 --> 00:11:00,959
public apis

00:10:57,920 --> 00:11:03,600
as you might know within spark the data

00:11:00,959 --> 00:11:06,000
set is split into so-called partitions

00:11:03,600 --> 00:11:07,920
and this is spark's way of parallelizing

00:11:06,000 --> 00:11:10,800
the computation because now

00:11:07,920 --> 00:11:13,040
multiple workers threats can process

00:11:10,800 --> 00:11:15,360
different partitions in parallel

00:11:13,040 --> 00:11:18,560
with the new feature this partition is

00:11:15,360 --> 00:11:20,480
actually actually split further

00:11:18,560 --> 00:11:23,040
and and the new abstraction layer is

00:11:20,480 --> 00:11:25,040
indirectly used the columnar batch

00:11:23,040 --> 00:11:26,399
as you can see now it's possible to load

00:11:25,040 --> 00:11:29,940
a whole chunk of data

00:11:26,399 --> 00:11:31,600
and process one chunk or one batch

00:11:29,940 --> 00:11:33,839
[Music]

00:11:31,600 --> 00:11:34,800
this unfortunately is not yet based on

00:11:33,839 --> 00:11:37,519
the error format

00:11:34,800 --> 00:11:39,279
however the concepts work quite well

00:11:37,519 --> 00:11:42,560
together with apache arrow

00:11:39,279 --> 00:11:44,880
and spark sql columnar batch abstraction

00:11:42,560 --> 00:11:48,320
consists of multiple column vectors

00:11:44,880 --> 00:11:50,480
and also here spark sql already provides

00:11:48,320 --> 00:11:52,399
implementation which is compatible with

00:11:50,480 --> 00:11:56,240
or which internally use

00:11:52,399 --> 00:11:58,399
the apache arrow abstractions

00:11:56,240 --> 00:11:59,680
now to the second challenge how can we

00:11:58,399 --> 00:12:02,320
tell spark to

00:11:59,680 --> 00:12:03,440
execute our own code our own custom

00:12:02,320 --> 00:12:05,920
operators

00:12:03,440 --> 00:12:06,480
i'm going to explain this on an example

00:12:05,920 --> 00:12:09,200
query

00:12:06,480 --> 00:12:10,959
which is processed within spark sql

00:12:09,200 --> 00:12:13,760
before the actually

00:12:10,959 --> 00:12:14,480
execution there are multiple ways of

00:12:13,760 --> 00:12:16,639
defining

00:12:14,480 --> 00:12:17,839
a query probably the most

00:12:16,639 --> 00:12:21,120
straightforward one

00:12:17,839 --> 00:12:22,639
is to define a sql query as here shown

00:12:21,120 --> 00:12:25,680
in the example

00:12:22,639 --> 00:12:28,399
this query is then analyzed validated

00:12:25,680 --> 00:12:29,200
and optimized optimized until it's

00:12:28,399 --> 00:12:31,680
finally

00:12:29,200 --> 00:12:32,399
stored in a spark internal format the

00:12:31,680 --> 00:12:34,959
so-called

00:12:32,399 --> 00:12:37,120
logical plan as you can see here this

00:12:34,959 --> 00:12:39,279
logical plan is actually a tree of

00:12:37,120 --> 00:12:41,360
operators

00:12:39,279 --> 00:12:43,040
and i start by explaining the scan

00:12:41,360 --> 00:12:45,120
operator here in violet which is

00:12:43,040 --> 00:12:46,720
responsible for reading the data

00:12:45,120 --> 00:12:48,800
the filter operator in green which

00:12:46,720 --> 00:12:52,079
filters out all records not filling

00:12:48,800 --> 00:12:54,160
the not fitting the expression

00:12:52,079 --> 00:12:55,200
and then finally in yellow a project

00:12:54,160 --> 00:12:56,720
operator which

00:12:55,200 --> 00:12:59,040
only returns the fields we are

00:12:56,720 --> 00:13:02,160
interested in

00:12:59,040 --> 00:13:03,920
after that this logical plan is mapped

00:13:02,160 --> 00:13:05,680
to a so-called physical plan

00:13:03,920 --> 00:13:07,839
here on the slides these plans look

00:13:05,680 --> 00:13:10,320
quite similar but the physical plan

00:13:07,839 --> 00:13:12,560
actually describes now how the logical

00:13:10,320 --> 00:13:14,720
plan is executed

00:13:12,560 --> 00:13:16,639
so we can say this is the actual

00:13:14,720 --> 00:13:18,000
implementation or one implementation of

00:13:16,639 --> 00:13:19,680
a logical plan

00:13:18,000 --> 00:13:21,760
and that's actually the point where we

00:13:19,680 --> 00:13:22,639
can jump in and we can replace this

00:13:21,760 --> 00:13:25,120
physical plan

00:13:22,639 --> 00:13:26,320
with our own physical plan so a plan

00:13:25,120 --> 00:13:29,120
containing our own

00:13:26,320 --> 00:13:31,360
operators in which we then can call for

00:13:29,120 --> 00:13:34,000
example fletcher

00:13:31,360 --> 00:13:37,200
and we have this actually done for one

00:13:34,000 --> 00:13:39,519
specific proof of concept

00:13:37,200 --> 00:13:41,360
our proof of concept use case is based

00:13:39,519 --> 00:13:44,800
on the chicago taxi drip

00:13:41,360 --> 00:13:47,440
use data set which is public available

00:13:44,800 --> 00:13:49,839
this data set contains all taxi trips

00:13:47,440 --> 00:13:52,160
conducted since 2013

00:13:49,839 --> 00:13:55,120
and as you can see here on the table

00:13:52,160 --> 00:13:57,920
every record describes one trip

00:13:55,120 --> 00:13:59,600
and which is defined by multiple fields

00:13:57,920 --> 00:14:00,240
for us relevant is the name of the

00:13:59,600 --> 00:14:02,959
company

00:14:00,240 --> 00:14:05,519
and the duration of the trip because our

00:14:02,959 --> 00:14:08,480
goal is to calculate the total duration

00:14:05,519 --> 00:14:11,440
of all trips conducted by companies

00:14:08,480 --> 00:14:14,480
starting with the letter b

00:14:11,440 --> 00:14:17,519
this obviously is not a very complex

00:14:14,480 --> 00:14:18,160
analyst analytics query and this brings

00:14:17,519 --> 00:14:22,000
us to

00:14:18,160 --> 00:14:23,839
this very straightforward sql query

00:14:22,000 --> 00:14:25,279
what are we doing we're reading the data

00:14:23,839 --> 00:14:27,519
from a parquet file

00:14:25,279 --> 00:14:28,639
and then we are summing up the durations

00:14:27,519 --> 00:14:31,279
of all trips

00:14:28,639 --> 00:14:32,399
which match and regex and in this case

00:14:31,279 --> 00:14:34,880
the wreckage means

00:14:32,399 --> 00:14:35,600
all trips all trips where the taxi

00:14:34,880 --> 00:14:39,120
company

00:14:35,600 --> 00:14:41,360
starts with the letter b spark maps this

00:14:39,120 --> 00:14:43,920
now internally to a physical plan

00:14:41,360 --> 00:14:45,600
here shown in the center of the slides

00:14:43,920 --> 00:14:47,839
i'm going to describe a

00:14:45,600 --> 00:14:50,720
bottom up with the blue part there we

00:14:47,839 --> 00:14:53,600
have a parque reader

00:14:50,720 --> 00:14:54,720
which imports the data from a parqv file

00:14:53,600 --> 00:14:56,560
and this operator is

00:14:54,720 --> 00:14:58,320
actually already in the columnar format

00:14:56,560 --> 00:15:00,959
by default and therefore

00:14:58,320 --> 00:15:02,160
spark adds here this operator in yellow

00:15:00,959 --> 00:15:04,560
which automatically

00:15:02,160 --> 00:15:05,279
converts the columnar format into the

00:15:04,560 --> 00:15:08,240
classic

00:15:05,279 --> 00:15:08,880
row base format and then this rows are

00:15:08,240 --> 00:15:12,399
filtered

00:15:08,880 --> 00:15:15,360
by the operator in violet

00:15:12,399 --> 00:15:16,800
and with the green part then the

00:15:15,360 --> 00:15:20,079
duration of the trips

00:15:16,800 --> 00:15:22,240
is summed together

00:15:20,079 --> 00:15:23,199
important to notice is here that this

00:15:22,240 --> 00:15:25,519
summing

00:15:23,199 --> 00:15:27,600
requires actually exchanging data

00:15:25,519 --> 00:15:29,360
between multiple partitions or between

00:15:27,600 --> 00:15:31,360
multiple

00:15:29,360 --> 00:15:32,720
workers threats and therefore a network

00:15:31,360 --> 00:15:36,160
exchange is required

00:15:32,720 --> 00:15:39,279
shown here with the shuffle operation

00:15:36,160 --> 00:15:40,000
so as i said before we have now replaced

00:15:39,279 --> 00:15:42,480
this physical

00:15:40,000 --> 00:15:43,199
plan with a custom physical plan with

00:15:42,480 --> 00:15:45,920
our own

00:15:43,199 --> 00:15:47,759
implementation first of all we have

00:15:45,920 --> 00:15:49,680
replaced the reader

00:15:47,759 --> 00:15:51,040
with a custom packet reader which

00:15:49,680 --> 00:15:53,120
actually directly

00:15:51,040 --> 00:15:55,279
loads the data in the error format so

00:15:53,120 --> 00:15:55,839
that we don't do not need to convert the

00:15:55,279 --> 00:15:58,959
data

00:15:55,839 --> 00:16:01,040
when we want to call fletcher

00:15:58,959 --> 00:16:02,639
and now the most important operator is

00:16:01,040 --> 00:16:04,720
now the flexure operator

00:16:02,639 --> 00:16:05,920
it takes as an input this stream of

00:16:04,720 --> 00:16:10,160
batches

00:16:05,920 --> 00:16:13,360
and calls an fpga kernel for every batch

00:16:10,160 --> 00:16:14,000
in our use case this kernel takes one

00:16:13,360 --> 00:16:17,360
batch

00:16:14,000 --> 00:16:20,240
and sums up the duration of all trips

00:16:17,360 --> 00:16:21,519
within this batch so we kind of get an

00:16:20,240 --> 00:16:25,279
intermediate sum

00:16:21,519 --> 00:16:28,240
which is then sum together with the

00:16:25,279 --> 00:16:28,720
default mechanism of spark here still

00:16:28,240 --> 00:16:32,800
shown

00:16:28,720 --> 00:16:36,240
in green i want to present you now a bit

00:16:32,800 --> 00:16:38,240
in detail the fletcher operator

00:16:36,240 --> 00:16:40,320
fletcher the fletcher framework provides

00:16:38,240 --> 00:16:41,920
the run time to integrate fpga

00:16:40,320 --> 00:16:44,480
accelerators with software

00:16:41,920 --> 00:16:44,959
it currently exists for example in c

00:16:44,480 --> 00:16:47,839
plus

00:16:44,959 --> 00:16:49,440
and therefore we are calling it by using

00:16:47,839 --> 00:16:52,480
the java native interface

00:16:49,440 --> 00:16:56,959
from the javaside aka from

00:16:52,480 --> 00:17:00,160
spark and this operator processes

00:16:56,959 --> 00:17:02,320
multiple batches and so we basically do

00:17:00,160 --> 00:17:04,160
a computation for every batch

00:17:02,320 --> 00:17:06,640
and as you can see we firstly have to

00:17:04,160 --> 00:17:07,199
transfer the data from the java side to

00:17:06,640 --> 00:17:09,919
the c

00:17:07,199 --> 00:17:10,799
plus side but actually we can make use

00:17:09,919 --> 00:17:13,839
here of the

00:17:10,799 --> 00:17:17,360
apache arrow language agnostic

00:17:13,839 --> 00:17:19,439
feature meaning we only need to

00:17:17,360 --> 00:17:21,439
copy the addresses from the java side to

00:17:19,439 --> 00:17:24,559
the c plus side and then we can

00:17:21,439 --> 00:17:28,000
construct the arrow abstractions again

00:17:24,559 --> 00:17:30,480
in c plus and now we are

00:17:28,000 --> 00:17:32,640
referencing from both sides to the

00:17:30,480 --> 00:17:34,080
actually same data and no copying was

00:17:32,640 --> 00:17:35,919
necessary

00:17:34,080 --> 00:17:37,679
as soon as we have the c plus plus

00:17:35,919 --> 00:17:41,440
abstractions we are ready

00:17:37,679 --> 00:17:43,600
to call uh our fpga kernel by using

00:17:41,440 --> 00:17:46,240
fletcher

00:17:43,600 --> 00:17:48,400
and as i said this kernel is then

00:17:46,240 --> 00:17:50,880
returning the intermediate sum

00:17:48,400 --> 00:17:52,880
which is then at the end combined

00:17:50,880 --> 00:17:55,679
together by spark

00:17:52,880 --> 00:17:56,640
i return now back to akos who is going

00:17:55,679 --> 00:17:59,039
to tell you

00:17:56,640 --> 00:18:01,840
how we can build fpga kernels way more

00:17:59,039 --> 00:18:01,840
efficient

00:18:02,559 --> 00:18:06,240
so back to the hardware side but let's

00:18:04,480 --> 00:18:09,360
zoom out a bit first

00:18:06,240 --> 00:18:11,520
so you might recognize this this is the

00:18:09,360 --> 00:18:12,559
special case of the general threshold

00:18:11,520 --> 00:18:16,000
workflow

00:18:12,559 --> 00:18:19,120
now we have a batch spark application

00:18:16,000 --> 00:18:20,960
in this case it writes

00:18:19,120 --> 00:18:22,880
a narrow record bash that has two

00:18:20,960 --> 00:18:25,760
columns a string array

00:18:22,880 --> 00:18:26,880
and an integer array and based on the

00:18:25,760 --> 00:18:30,400
schema we can

00:18:26,880 --> 00:18:32,640
generate the flasher infrastructure and

00:18:30,400 --> 00:18:34,320
fracture is going to supply two incoming

00:18:32,640 --> 00:18:36,320
streams to our kernel

00:18:34,320 --> 00:18:37,600
one of which is a string stream the

00:18:36,320 --> 00:18:40,240
other one is an

00:18:37,600 --> 00:18:41,120
integer stream and we're going to return

00:18:40,240 --> 00:18:44,320
the

00:18:41,120 --> 00:18:47,039
results as well in this case uh since

00:18:44,320 --> 00:18:50,640
we're at an open power conference the

00:18:47,039 --> 00:18:53,360
interface that's being used is opencapi

00:18:50,640 --> 00:18:54,400
uh so how would the kernel look like

00:18:53,360 --> 00:18:58,240
what the kernel would

00:18:54,400 --> 00:19:01,840
look like as we said we have the second

00:18:58,240 --> 00:19:04,160
incoming that's an integer value

00:19:01,840 --> 00:19:07,360
and we have a list of company names that

00:19:04,160 --> 00:19:10,160
is a list of strings

00:19:07,360 --> 00:19:12,320
we do want a filter for specific

00:19:10,160 --> 00:19:15,600
companies that

00:19:12,320 --> 00:19:17,600
that match a regular expression we use a

00:19:15,600 --> 00:19:19,360
regular expression measure for that

00:19:17,600 --> 00:19:21,200
that's going to generate a predicate

00:19:19,360 --> 00:19:23,360
stream which is basically a boolean

00:19:21,200 --> 00:19:26,640
stream

00:19:23,360 --> 00:19:26,960
that is true when the particular company

00:19:26,640 --> 00:19:30,160
name

00:19:26,960 --> 00:19:33,360
matched the other incoming stream

00:19:30,160 --> 00:19:35,600
is uh is fed into a theater stream

00:19:33,360 --> 00:19:36,960
construct that is gonna filter the

00:19:35,600 --> 00:19:40,240
incoming integers

00:19:36,960 --> 00:19:42,720
based on that boolean predicate stream

00:19:40,240 --> 00:19:43,600
this filter these filtered values are

00:19:42,720 --> 00:19:46,720
then fed

00:19:43,600 --> 00:19:49,679
to the reduce stream construct which

00:19:46,720 --> 00:19:51,280
instantiates a sum operator and sums all

00:19:49,679 --> 00:19:54,400
the values together

00:19:51,280 --> 00:19:54,720
and then we return that singular value

00:19:54,400 --> 00:19:57,280
for

00:19:54,720 --> 00:19:58,080
for an array all right but how can we

00:19:57,280 --> 00:20:01,520
build this

00:19:58,080 --> 00:20:04,159
uh efficiently let's say we want

00:20:01,520 --> 00:20:04,960
tidy interfaces for all of these blocks

00:20:04,159 --> 00:20:06,799
because

00:20:04,960 --> 00:20:08,799
at least now we have a standard to do

00:20:06,799 --> 00:20:10,799
this we want to define

00:20:08,799 --> 00:20:12,960
operations on these streams and

00:20:10,799 --> 00:20:15,760
interfaces

00:20:12,960 --> 00:20:16,720
we also want to instantiate library

00:20:15,760 --> 00:20:20,240
components

00:20:16,720 --> 00:20:22,080
such as the regular expression measure

00:20:20,240 --> 00:20:23,440
and it will be great to have hardware

00:20:22,080 --> 00:20:27,440
templates for

00:20:23,440 --> 00:20:31,520
for regular computations for filters

00:20:27,440 --> 00:20:35,919
map reduces and to handle this

00:20:31,520 --> 00:20:39,520
we propose a new language called tidal

00:20:35,919 --> 00:20:41,440
for tidy and it's basically a data flow

00:20:39,520 --> 00:20:45,600
composition language

00:20:41,440 --> 00:20:47,520
featuring these so let's look at

00:20:45,600 --> 00:20:49,440
this particular accelerator in this

00:20:47,520 --> 00:20:51,760
language at the top you can see the

00:20:49,440 --> 00:20:54,159
generated dot diagram by the two

00:20:51,760 --> 00:20:56,320
it has exactly the same same three

00:20:54,159 --> 00:21:00,080
straight stages

00:20:56,320 --> 00:21:01,600
as i shown earlier and on the bottom you

00:21:00,080 --> 00:21:03,520
can see

00:21:01,600 --> 00:21:05,600
the description so we define a

00:21:03,520 --> 00:21:08,159
structural implementation

00:21:05,600 --> 00:21:10,240
for this kernel we instantiate

00:21:08,159 --> 00:21:10,559
streamlets which are basically the data

00:21:10,240 --> 00:21:12,720
for

00:21:10,559 --> 00:21:13,679
data flow components that have

00:21:12,720 --> 00:21:16,720
interfaces

00:21:13,679 --> 00:21:18,559
adhering to the tidy specification in

00:21:16,720 --> 00:21:19,919
this case this is a regular expression

00:21:18,559 --> 00:21:22,720
measure that takes

00:21:19,919 --> 00:21:24,159
a parameter that is the regular

00:21:22,720 --> 00:21:26,640
expression

00:21:24,159 --> 00:21:28,080
and then we make data flow connections

00:21:26,640 --> 00:21:31,120
between these modules

00:21:28,080 --> 00:21:33,679
and we also instantiate uh

00:21:31,120 --> 00:21:34,880
two uh hardware templates uh namely the

00:21:33,679 --> 00:21:38,000
filter stream

00:21:34,880 --> 00:21:40,159
and the reduce stream pattern and these

00:21:38,000 --> 00:21:42,480
are what's called parallel patterns

00:21:40,159 --> 00:21:44,559
that sound familiar from functional

00:21:42,480 --> 00:21:45,840
languages and these are actually really

00:21:44,559 --> 00:21:48,880
good constructs

00:21:45,840 --> 00:21:51,440
for one-to-one mapping between

00:21:48,880 --> 00:21:52,799
spark data flow graphs or or or any kind

00:21:51,440 --> 00:21:56,480
of computational graph

00:21:52,799 --> 00:22:00,320
to hardware and as you can see

00:21:56,480 --> 00:22:02,720
it's nine lines of readable codes

00:22:00,320 --> 00:22:04,559
if you want to do this in vhdl that

00:22:02,720 --> 00:22:08,240
would be around 600 lines

00:22:04,559 --> 00:22:09,120
of boilerplate vhdl this is excluding

00:22:08,240 --> 00:22:10,720
comments and

00:22:09,120 --> 00:22:13,360
and all the components this would be

00:22:10,720 --> 00:22:16,480
just the instantiation and connection

00:22:13,360 --> 00:22:16,480
of the components

00:22:16,960 --> 00:22:20,720
let's see some results so we implemented

00:22:20,240 --> 00:22:23,440
this

00:22:20,720 --> 00:22:25,600
and to end we chose the string stream to

00:22:23,440 --> 00:22:26,159
be able to transfer at most 20 bucks per

00:22:25,600 --> 00:22:28,799
psycho

00:22:26,159 --> 00:22:31,440
in this case that is 20 characters

00:22:28,799 --> 00:22:34,720
because we're talking about ascii

00:22:31,440 --> 00:22:35,280
and this kernel was able to achieve 135

00:22:34,720 --> 00:22:37,600
million

00:22:35,280 --> 00:22:38,720
records per second it can process that

00:22:37,600 --> 00:22:41,520
many

00:22:38,720 --> 00:22:43,200
in this data set that accounts for

00:22:41,520 --> 00:22:44,400
around four gigabytes per second

00:22:43,200 --> 00:22:48,080
throughput

00:22:44,400 --> 00:22:50,799
the whole design uh took about

00:22:48,080 --> 00:22:53,200
twenty percent of the whole fpga from

00:22:50,799 --> 00:22:53,760
which flasher and the kernel i presented

00:22:53,200 --> 00:22:55,520
to you

00:22:53,760 --> 00:22:57,280
accounts for only about three and a half

00:22:55,520 --> 00:22:59,360
percent uh

00:22:57,280 --> 00:23:01,919
running this application we've seen an

00:22:59,360 --> 00:23:04,159
overall speed up of eight x

00:23:01,919 --> 00:23:05,679
uh as you can see on the graph as well

00:23:04,159 --> 00:23:08,400
the actual fpga

00:23:05,679 --> 00:23:08,799
compute is even lower than that the fpga

00:23:08,400 --> 00:23:11,039
other

00:23:08,799 --> 00:23:12,159
you can see there is basically the i o

00:23:11,039 --> 00:23:16,080
reading the par k

00:23:12,159 --> 00:23:18,720
file if you want to saturate

00:23:16,080 --> 00:23:19,679
the open cappy bandwidth that means six

00:23:18,720 --> 00:23:22,799
of this kernel

00:23:19,679 --> 00:23:25,039
would need to be instantiated

00:23:22,799 --> 00:23:26,080
and that would easily fit into the fpga

00:23:25,039 --> 00:23:29,200
as as you can

00:23:26,080 --> 00:23:31,120
as you can see and even considering that

00:23:29,200 --> 00:23:31,600
most of the hardware would be shared so

00:23:31,120 --> 00:23:33,360
the

00:23:31,600 --> 00:23:34,640
aussie accelerated hardware will be

00:23:33,360 --> 00:23:36,320
shared

00:23:34,640 --> 00:23:37,679
some of the fresher infrastructure would

00:23:36,320 --> 00:23:40,000
be shared

00:23:37,679 --> 00:23:41,279
we would just have to instantiate the

00:23:40,000 --> 00:23:44,559
corner multiple

00:23:41,279 --> 00:23:45,919
times uh so let's see what have we

00:23:44,559 --> 00:23:48,159
achieved here

00:23:45,919 --> 00:23:50,400
now we have a complete workflow for

00:23:48,159 --> 00:23:52,720
accelerating big data workloads

00:23:50,400 --> 00:23:53,440
starting from high level framework down

00:23:52,720 --> 00:23:57,679
to the

00:23:53,440 --> 00:24:00,080
fpga hardware design we also looked at

00:23:57,679 --> 00:24:01,600
integration with apache spark we

00:24:00,080 --> 00:24:03,520
presented a

00:24:01,600 --> 00:24:05,120
proof of concept accelerator that

00:24:03,520 --> 00:24:08,720
achieved atx speed up

00:24:05,120 --> 00:24:11,760
on this particular application

00:24:08,720 --> 00:24:14,480
and we proved that by choosing suitable

00:24:11,760 --> 00:24:16,640
extractions in every layer it is

00:24:14,480 --> 00:24:18,799
actually possible to go very far with

00:24:16,640 --> 00:24:21,440
structural hardware composition

00:24:18,799 --> 00:24:24,000
without without many high-level

00:24:21,440 --> 00:24:27,200
synthesis magic

00:24:24,000 --> 00:24:30,240
the language we presented

00:24:27,200 --> 00:24:32,400
allows us to map data flow components

00:24:30,240 --> 00:24:33,840
to hardware using library components and

00:24:32,400 --> 00:24:35,919
hardware templates

00:24:33,840 --> 00:24:37,120
these hardware templates are a very good

00:24:35,919 --> 00:24:40,320
match to represent

00:24:37,120 --> 00:24:42,240
big data analytics uh computations in

00:24:40,320 --> 00:24:44,159
hardware

00:24:42,240 --> 00:24:46,080
if you want to develop your kernel by

00:24:44,159 --> 00:24:47,600
hand this offers your code size

00:24:46,080 --> 00:24:50,559
reduction

00:24:47,600 --> 00:24:51,760
or if you want to utilize this in your

00:24:50,559 --> 00:24:54,720
own tools this is

00:24:51,760 --> 00:24:56,480
actually a good internal representation

00:24:54,720 --> 00:24:59,440
for code generation

00:24:56,480 --> 00:25:01,279
and synthesis tools and with this i

00:24:59,440 --> 00:25:04,880
would like to conclude and we're open

00:25:01,279 --> 00:25:04,880
for questions thank you very much

00:25:11,600 --> 00:25:15,840
let me pop on the q a

00:25:17,440 --> 00:25:23,520
you did not use oc accelerate this

00:25:20,480 --> 00:25:26,320
this is a question we did it's hidden by

00:25:23,520 --> 00:25:26,320
fletcher so

00:25:26,960 --> 00:25:30,159
it's it's one of the fletcher backhands

00:25:28,799 --> 00:25:33,440
and and support its

00:25:30,159 --> 00:25:33,440
uh targets

00:25:34,400 --> 00:25:38,159
so fletcher wraps it around

00:25:39,760 --> 00:25:47,200
okay we got a question uh

00:25:43,120 --> 00:25:47,200
and it's about

00:25:48,880 --> 00:25:51,919
versus one worker thread oh yes i leave

00:25:51,679 --> 00:25:53,760
this

00:25:51,919 --> 00:25:56,320
fabian to explain this is this is

00:25:53,760 --> 00:25:58,320
actually a very good question

00:25:56,320 --> 00:25:59,919
akash can you maybe go back to the slide

00:25:58,320 --> 00:26:04,159
with the um

00:25:59,919 --> 00:26:07,279
partition spark partitions sure

00:26:04,159 --> 00:26:11,440
um here we go yeah we have the the

00:26:07,279 --> 00:26:14,480
problem how does spark doing parallelism

00:26:11,440 --> 00:26:16,240
and it does this actually by

00:26:14,480 --> 00:26:18,720
doing by splitting the data set into

00:26:16,240 --> 00:26:20,159
partitions and now we have multiple

00:26:18,720 --> 00:26:23,440
virgo threads

00:26:20,159 --> 00:26:26,559
which process this one partitions in

00:26:23,440 --> 00:26:29,919
in parallel and where we now

00:26:26,559 --> 00:26:33,520
actually worked on we worked more on

00:26:29,919 --> 00:26:37,279
um yeah on making

00:26:33,520 --> 00:26:40,880
the processing of one partition faster

00:26:37,279 --> 00:26:43,760
and this is done here now by calling the

00:26:40,880 --> 00:26:46,320
fpga within one partition within one

00:26:43,760 --> 00:26:48,159
worker thread

00:26:46,320 --> 00:26:49,360
but here for sure it will be still

00:26:48,159 --> 00:26:51,679
possible to put

00:26:49,360 --> 00:26:52,640
another layer of parallelism so to call

00:26:51,679 --> 00:26:54,960
multiple

00:26:52,640 --> 00:26:56,559
the batches process multiple batches in

00:26:54,960 --> 00:26:59,039
parallel

00:26:56,559 --> 00:27:00,720
and in general we can we can say that

00:26:59,039 --> 00:27:04,400
it's possible to

00:27:00,720 --> 00:27:07,520
to replace with one fpga

00:27:04,400 --> 00:27:12,559
in our example it was

00:27:07,520 --> 00:27:12,559
we we could replace one one word yeah

00:27:14,000 --> 00:27:20,880
sorry if it's no go go ahead akash

00:27:18,880 --> 00:27:22,799
so so you can look at it in another way

00:27:20,880 --> 00:27:23,919
so this is an atex performance

00:27:22,799 --> 00:27:26,640
improvement and

00:27:23,919 --> 00:27:27,919
we compared it to one cpu thread because

00:27:26,640 --> 00:27:30,720
uh

00:27:27,919 --> 00:27:32,240
you basically do a polarization inspire

00:27:30,720 --> 00:27:32,880
splitting the data into multiple

00:27:32,240 --> 00:27:36,159
partition

00:27:32,880 --> 00:27:38,880
and partitioning them independently so

00:27:36,159 --> 00:27:41,679
you can look at it in a way that you can

00:27:38,880 --> 00:27:44,240
replace eight cpu cores

00:27:41,679 --> 00:27:46,080
uh with one kernel in this case because

00:27:44,240 --> 00:27:50,720
it would be easy to instantiate more

00:27:46,080 --> 00:27:50,720
and feed data independently into those

00:27:51,360 --> 00:27:58,000
um the next one is how difficult

00:27:55,120 --> 00:27:58,559
uh to use multiple kernels currently we

00:27:58,000 --> 00:28:02,799
don't have

00:27:58,559 --> 00:28:04,240
uh it is possible to do manually

00:28:02,799 --> 00:28:06,000
it doesn't have we don't have the

00:28:04,240 --> 00:28:07,520
language construct for that currently

00:28:06,000 --> 00:28:10,000
but we can define those and

00:28:07,520 --> 00:28:12,000
and that would be possible although in

00:28:10,000 --> 00:28:12,399
certain types of computations you would

00:28:12,000 --> 00:28:16,159
need

00:28:12,399 --> 00:28:19,360
a software support for this and

00:28:16,159 --> 00:28:20,960
about can the language support tiring

00:28:19,360 --> 00:28:23,440
and dse stuff

00:28:20,960 --> 00:28:24,320
uh so we don't instantiate multiple

00:28:23,440 --> 00:28:26,640
kernels

00:28:24,320 --> 00:28:28,640
at this time you can run design through

00:28:26,640 --> 00:28:31,840
this exploration currently on how

00:28:28,640 --> 00:28:33,679
wide the streams should be because

00:28:31,840 --> 00:28:36,480
you can choose how many elements you

00:28:33,679 --> 00:28:38,720
want to transfer

00:28:36,480 --> 00:28:39,600
at most in in one transfer in these

00:28:38,720 --> 00:28:42,840
streams

00:28:39,600 --> 00:28:44,559
and obviously you do have to choose that

00:28:42,840 --> 00:28:46,880
carefully

00:28:44,559 --> 00:28:46,880
right

00:28:50,880 --> 00:28:54,159
beyond we want to go ahead and answer

00:28:52,320 --> 00:28:58,880
the parking question

00:28:54,159 --> 00:28:58,880
yeah yeah um i'm going to

00:28:59,230 --> 00:29:03,039
[Music]

00:29:00,720 --> 00:29:04,000
and we started here with the most simple

00:29:03,039 --> 00:29:07,840
case

00:29:04,000 --> 00:29:11,360
um so the pakistan is uh not compressed

00:29:07,840 --> 00:29:12,399
and it's also not using a dictionary

00:29:11,360 --> 00:29:15,679
encoding

00:29:12,399 --> 00:29:20,960
so yeah that the mapping into

00:29:15,679 --> 00:29:20,960
the error format is the easiest possible

00:29:29,600 --> 00:29:35,120
okay so i think we hit 30 minutes uh

00:29:33,039 --> 00:29:36,159
we're gonna be still here since there's

00:29:35,120 --> 00:29:40,480
gonna be a break

00:29:36,159 --> 00:29:44,960
uh but you can reach us on on

00:29:40,480 --> 00:29:44,960
on slack as well after this yeah

00:29:48,799 --> 00:29:52,480

YouTube URL: https://www.youtube.com/watch?v=47xa4VfWbms


