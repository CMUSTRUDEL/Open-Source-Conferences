Title: Accelerated Big Data Systems - Experiences with OpenCAPI - Joost Hoozemans
Publication date: 2020-09-21
Playlist: OpenPOWER Summit NA 2020
Description: 
	Accelerated Big Data Systems - Experiences with OpenCAPI - Joost Hoozemans, Delft University of Technology
Speakers: Joost Hoozemans
In this talk, the Accelerated Big Data Systems group of Delft University of Technology shares experiences on the POWER9 OpenCAPI platform. A key goal of the group is to make programming accelerators (and specifically FPGAs) easier, more portable and more efficient. To this end, ABS has contributed several open source tools and technologies to the community, and created multiple demonstrators that apply these tools in big data analytics and genomics application domains. The presentation provides an overview of the ABS toolchain, the new tools that were developed, and show results of the use cases that were created.
Captions: 
	00:00:01,280 --> 00:00:06,640
okay so hello everyone

00:00:03,360 --> 00:00:07,440
um my name is josemans i'm talking to

00:00:06,640 --> 00:00:10,960
you from

00:00:07,440 --> 00:00:13,200
the netherlands i'm

00:00:10,960 --> 00:00:14,719
working with the accelerated big data

00:00:13,200 --> 00:00:17,760
systems group

00:00:14,719 --> 00:00:20,800
in a computer engineering laboratory of

00:00:17,760 --> 00:00:23,359
delft university of technology um

00:00:20,800 --> 00:00:23,920
where i'm postdoc here's a picture of

00:00:23,359 --> 00:00:26,240
our

00:00:23,920 --> 00:00:28,640
campus which is indeed this empty at

00:00:26,240 --> 00:00:28,640
this time

00:00:29,599 --> 00:00:34,079
so today um i'm going to have a simple

00:00:32,719 --> 00:00:37,600
presentation there's an

00:00:34,079 --> 00:00:40,000
introduction um gradually

00:00:37,600 --> 00:00:40,640
uh transitioning into some of the tools

00:00:40,000 --> 00:00:43,920
that

00:00:40,640 --> 00:00:47,039
we've developed that we think can be

00:00:43,920 --> 00:00:49,680
very valuable to the community

00:00:47,039 --> 00:00:50,160
and then a couple of use cases that we

00:00:49,680 --> 00:00:53,520
build

00:00:50,160 --> 00:00:56,719
using those tools on

00:00:53,520 --> 00:00:58,079
open copy systems some results and then

00:00:56,719 --> 00:00:59,520
there will be time for some

00:00:58,079 --> 00:01:01,920
for some questions at the end you can

00:00:59,520 --> 00:01:03,520
use a q a for that

00:01:01,920 --> 00:01:05,519
and i've also been told that you can

00:01:03,520 --> 00:01:08,479
drop them in the slack so that

00:01:05,519 --> 00:01:08,880
afterwards i'll have a look on there as

00:01:08,479 --> 00:01:12,320
well

00:01:08,880 --> 00:01:15,200
we can continue the discussion

00:01:12,320 --> 00:01:17,040
so the accelerate big data systems group

00:01:15,200 --> 00:01:20,560
or abs as we

00:01:17,040 --> 00:01:23,920
call it is led by zayd lars and

00:01:20,560 --> 00:01:26,960
peter hofste from ibm and

00:01:23,920 --> 00:01:29,520
we're working on different

00:01:26,960 --> 00:01:31,600
layers of the technology stack in a

00:01:29,520 --> 00:01:35,920
couple of

00:01:31,600 --> 00:01:38,799
application domains related to big data

00:01:35,920 --> 00:01:39,280
so here's some pictures of our staff and

00:01:38,799 --> 00:01:43,040
your

00:01:39,280 --> 00:01:43,040
presenters this hello this is me

00:01:44,320 --> 00:01:47,759
and i'd like to let you know that most

00:01:47,200 --> 00:01:49,520
of the

00:01:47,759 --> 00:01:51,680
technology that we built is actually

00:01:49,520 --> 00:01:54,720
open source and we publish it on our git

00:01:51,680 --> 00:01:57,280
website so if you're interested

00:01:54,720 --> 00:01:58,479
and look there most of the stuff that

00:01:57,280 --> 00:02:00,320
i'm presenting

00:01:58,479 --> 00:02:03,280
um i'll be presenting here you will be

00:02:00,320 --> 00:02:03,280
able to find there

00:02:03,600 --> 00:02:10,000
okay so let's have a look at some of the

00:02:06,960 --> 00:02:13,280
trends that we're seeing in the big data

00:02:10,000 --> 00:02:14,400
world let's start at the software side

00:02:13,280 --> 00:02:17,599
of it

00:02:14,400 --> 00:02:19,440
so in the old days

00:02:17,599 --> 00:02:23,360
computers used to be very expensive

00:02:19,440 --> 00:02:26,480
large machines mainframes clusters

00:02:23,360 --> 00:02:28,319
that were being programmed by yeah

00:02:26,480 --> 00:02:30,319
programmers that were writing very

00:02:28,319 --> 00:02:34,400
highly optimized codes for it

00:02:30,319 --> 00:02:35,840
lots of effort on that but in the last

00:02:34,400 --> 00:02:37,519
yeah in the last couple of years this

00:02:35,840 --> 00:02:40,640
has changed a little bit

00:02:37,519 --> 00:02:43,120
so currently it's actually often more

00:02:40,640 --> 00:02:44,879
economic to just add a couple of nodes

00:02:43,120 --> 00:02:46,560
to a rack of machines

00:02:44,879 --> 00:02:48,480
than it is to spend a lot of time

00:02:46,560 --> 00:02:52,160
optimizing code

00:02:48,480 --> 00:02:55,120
so actually these costs they've they've

00:02:52,160 --> 00:02:57,599
changed a little bit and what we're

00:02:55,120 --> 00:03:00,879
seeing now is that

00:02:57,599 --> 00:03:04,080
for programmers or data scientists

00:03:00,879 --> 00:03:06,000
to run some code on a very large machine

00:03:04,080 --> 00:03:08,640
there's now popular frameworks for

00:03:06,000 --> 00:03:12,640
example picture spark

00:03:08,640 --> 00:03:14,879
that allows that have greatly

00:03:12,640 --> 00:03:16,239
increased accessibility of using very

00:03:14,879 --> 00:03:19,360
large clusters

00:03:16,239 --> 00:03:21,120
sometimes even commodity hardware um to

00:03:19,360 --> 00:03:24,080
get the high performance

00:03:21,120 --> 00:03:25,200
for very large data sets using not

00:03:24,080 --> 00:03:28,319
really that much

00:03:25,200 --> 00:03:30,480
effort so these frameworks provide

00:03:28,319 --> 00:03:31,360
very high levels of abstractions they

00:03:30,480 --> 00:03:33,440
use

00:03:31,360 --> 00:03:34,640
scripting languages sometimes even like

00:03:33,440 --> 00:03:38,799
python

00:03:34,640 --> 00:03:41,120
or sql they

00:03:38,799 --> 00:03:43,599
they provide very high obstructions like

00:03:41,120 --> 00:03:46,640
record oriented tables

00:03:43,599 --> 00:03:49,760
so all of these new technologies

00:03:46,640 --> 00:03:50,720
they allow data scientists to create a

00:03:49,760 --> 00:03:53,599
new program

00:03:50,720 --> 00:03:55,599
write a new query in a matter of days or

00:03:53,599 --> 00:03:57,680
weeks

00:03:55,599 --> 00:03:59,200
now on the hardware side things have not

00:03:57,680 --> 00:04:02,959
been this

00:03:59,200 --> 00:04:05,120
easy let's call it easy um

00:04:02,959 --> 00:04:07,040
so we're seeing that the creation of

00:04:05,120 --> 00:04:09,439
data volumes is still

00:04:07,040 --> 00:04:10,080
scaling along pretty much with moore's

00:04:09,439 --> 00:04:13,120
law right

00:04:10,080 --> 00:04:16,479
it's still very much exponential

00:04:13,120 --> 00:04:18,320
but the rate in which the performance of

00:04:16,479 --> 00:04:20,320
cpu is increasing

00:04:18,320 --> 00:04:22,320
and i guess many of you have seen this

00:04:20,320 --> 00:04:26,320
graph dozens of times

00:04:22,320 --> 00:04:30,080
already it's not it's not been scaling

00:04:26,320 --> 00:04:33,520
as quickly anymore so

00:04:30,080 --> 00:04:34,240
what we call vertical scaling with which

00:04:33,520 --> 00:04:36,160
i mean

00:04:34,240 --> 00:04:37,680
adding compute resources to a single

00:04:36,160 --> 00:04:40,240
node to increase increase its

00:04:37,680 --> 00:04:43,520
performance

00:04:40,240 --> 00:04:46,479
it's sort of reaching the limits

00:04:43,520 --> 00:04:47,840
so to make matters even worse what we're

00:04:46,479 --> 00:04:51,040
seeing is that

00:04:47,840 --> 00:04:54,960
the throughput provided by storage

00:04:51,040 --> 00:04:57,360
and networking it's slowly catching up

00:04:54,960 --> 00:04:58,000
with the speed in which a single cpu

00:04:57,360 --> 00:05:01,120
socket

00:04:58,000 --> 00:05:04,400
is able to access data from

00:05:01,120 --> 00:05:07,360
its uh dram from its main memory

00:05:04,400 --> 00:05:08,000
so the classifical yeah this is from a

00:05:07,360 --> 00:05:09,680
blog

00:05:08,000 --> 00:05:12,160
that you might be interested in in

00:05:09,680 --> 00:05:12,160
reading

00:05:12,240 --> 00:05:18,160
so these observations they

00:05:15,680 --> 00:05:19,840
more or less create this realization of

00:05:18,160 --> 00:05:22,320
that the classical rule of thumb that

00:05:19,840 --> 00:05:23,759
the cpu is fast and storage and network

00:05:22,320 --> 00:05:28,080
are slow

00:05:23,759 --> 00:05:32,560
it's no longer the case so

00:05:28,080 --> 00:05:33,919
this this gives us the idea that this

00:05:32,560 --> 00:05:36,639
was the vision that

00:05:33,919 --> 00:05:38,639
probably in the future the data is not

00:05:36,639 --> 00:05:41,680
going to be processed purely

00:05:38,639 --> 00:05:45,120
by a bunch of cpus anymore as

00:05:41,680 --> 00:05:47,440
it's a as it's been in the past so

00:05:45,120 --> 00:05:49,360
what we're already seeing is a couple of

00:05:47,440 --> 00:05:51,680
these accelerators

00:05:49,360 --> 00:05:52,720
showing up like for example there's of

00:05:51,680 --> 00:05:56,639
course the gpu

00:05:52,720 --> 00:06:00,400
and the tpu for ai workloads

00:05:56,639 --> 00:06:03,840
we're seeing smartnix being introduced

00:06:00,400 --> 00:06:06,240
accelerated storage solutions

00:06:03,840 --> 00:06:07,199
so there's already a little bit of

00:06:06,240 --> 00:06:10,240
acceleration

00:06:07,199 --> 00:06:11,759
being done in the present but in the

00:06:10,240 --> 00:06:12,479
apps group we're thinking that in the

00:06:11,759 --> 00:06:14,720
future

00:06:12,479 --> 00:06:17,600
this will only increase to a situation

00:06:14,720 --> 00:06:19,280
where the cpus will be there to

00:06:17,600 --> 00:06:21,360
more or less orchestrate the

00:06:19,280 --> 00:06:24,080
distribution of the workload

00:06:21,360 --> 00:06:26,160
over a bunch of accelerators each of

00:06:24,080 --> 00:06:29,120
which is very specialized to their

00:06:26,160 --> 00:06:29,120
particular task

00:06:30,160 --> 00:06:36,560
so this this being said

00:06:34,240 --> 00:06:39,039
i guess this is also the reason that

00:06:36,560 --> 00:06:40,240
we're considering open copy as a very

00:06:39,039 --> 00:06:43,360
important

00:06:40,240 --> 00:06:44,160
component in these future systems not

00:06:43,360 --> 00:06:46,639
only because

00:06:44,160 --> 00:06:48,400
it's currently the highest performance

00:06:46,639 --> 00:06:51,199
performing interconnect

00:06:48,400 --> 00:06:52,479
but also because it allows your

00:06:51,199 --> 00:06:56,000
accelerator to

00:06:52,479 --> 00:06:58,880
function as a peer of the cpu instead of

00:06:56,000 --> 00:07:01,680
being a slave device

00:06:58,880 --> 00:07:02,000
and what i also personally like very

00:07:01,680 --> 00:07:05,680
much

00:07:02,000 --> 00:07:08,800
is that open copy opens the door to

00:07:05,680 --> 00:07:09,840
for example other fabrics besides fpgas

00:07:08,800 --> 00:07:11,199
what are

00:07:09,840 --> 00:07:13,919
what's currently used to create

00:07:11,199 --> 00:07:14,560
accelerators so for example is there's

00:07:13,919 --> 00:07:17,599
going to be

00:07:14,560 --> 00:07:18,400
a like a cga array-based fabric or maybe

00:07:17,599 --> 00:07:22,479
even

00:07:18,400 --> 00:07:24,960
multi-project way for asics and

00:07:22,479 --> 00:07:25,599
you will be able to connect them to your

00:07:24,960 --> 00:07:28,960
system

00:07:25,599 --> 00:07:28,960
through to open copy

00:07:29,520 --> 00:07:35,840
so um as you're seeing

00:07:32,960 --> 00:07:37,039
uh i kind of set up this um this

00:07:35,840 --> 00:07:41,120
contrast here where

00:07:37,039 --> 00:07:44,560
on one side we have software

00:07:41,120 --> 00:07:47,440
creating these high level obstructions

00:07:44,560 --> 00:07:48,800
and on the hardware side you know

00:07:47,440 --> 00:07:50,960
there's a reason we're calling it

00:07:48,800 --> 00:07:52,960
accelerators they're supposed to be

00:07:50,960 --> 00:07:54,080
very fast faster than the cpu which

00:07:52,960 --> 00:07:56,560
means that

00:07:54,080 --> 00:07:57,360
they need to be highly specialized you

00:07:56,560 --> 00:08:01,360
are working with

00:07:57,360 --> 00:08:04,639
very optimized hardware designs

00:08:01,360 --> 00:08:07,199
um that can take an fpga designer months

00:08:04,639 --> 00:08:09,280
to implement on fpga

00:08:07,199 --> 00:08:10,639
so on the one hand you have these

00:08:09,280 --> 00:08:13,759
software

00:08:10,639 --> 00:08:14,960
pulling towards higher levels of

00:08:13,759 --> 00:08:17,280
productivity

00:08:14,960 --> 00:08:18,240
easier programming quicker time to

00:08:17,280 --> 00:08:20,080
solution

00:08:18,240 --> 00:08:21,919
and then on the other side we have

00:08:20,080 --> 00:08:23,759
hearts are requiring more low life

00:08:21,919 --> 00:08:26,960
implementation

00:08:23,759 --> 00:08:27,680
more difficult to develop for so we're

00:08:26,960 --> 00:08:31,120
seeing this

00:08:27,680 --> 00:08:33,919
this big gap and we think that is

00:08:31,120 --> 00:08:35,919
it's it's i think it's increasing

00:08:33,919 --> 00:08:39,120
earlier then it will be decreasing

00:08:35,919 --> 00:08:42,000
so um this is the

00:08:39,120 --> 00:08:42,880
this is where the abs group is um is

00:08:42,000 --> 00:08:45,920
working

00:08:42,880 --> 00:08:47,600
primarily on trying to address this

00:08:45,920 --> 00:08:49,440
discrepancy this gap between the

00:08:47,600 --> 00:08:51,839
software and the hardware world

00:08:49,440 --> 00:08:53,440
and we're doing this particularly in the

00:08:51,839 --> 00:08:56,800
big data

00:08:53,440 --> 00:08:58,160
application domain because the big data

00:08:56,800 --> 00:09:01,040
application domain gives us

00:08:58,160 --> 00:09:02,640
also besides these challenges a couple

00:09:01,040 --> 00:09:06,320
of opportunities

00:09:02,640 --> 00:09:06,320
so let's have a look at that

00:09:06,720 --> 00:09:12,640
here i have a yeah a

00:09:10,160 --> 00:09:14,000
a an example like the hello world of big

00:09:12,640 --> 00:09:18,399
data analytics right this

00:09:14,000 --> 00:09:20,800
is um this is word count um

00:09:18,399 --> 00:09:22,080
you can find this on the spark website

00:09:20,800 --> 00:09:25,200
is probably their first

00:09:22,080 --> 00:09:26,000
example what does this look like

00:09:25,200 --> 00:09:29,680
internally

00:09:26,000 --> 00:09:32,959
in in spark it gets represented

00:09:29,680 --> 00:09:36,480
as a graph directed acyclic

00:09:32,959 --> 00:09:36,800
graph with a couple of uh operations

00:09:36,480 --> 00:09:40,240
like

00:09:36,800 --> 00:09:44,000
transformations um

00:09:40,240 --> 00:09:46,720
so these transformations they are

00:09:44,000 --> 00:09:47,120
very parallel programming construct

00:09:46,720 --> 00:09:51,120
these

00:09:47,120 --> 00:09:54,720
patterns um and that combined with the

00:09:51,120 --> 00:09:57,200
graph representation both of these

00:09:54,720 --> 00:09:58,480
are actually quite suitable to map to an

00:09:57,200 --> 00:10:00,399
fpga

00:09:58,480 --> 00:10:02,320
because you can lay them out on the chip

00:10:00,399 --> 00:10:04,880
you can unfold them so

00:10:02,320 --> 00:10:06,320
as long as there's room left on the fpga

00:10:04,880 --> 00:10:10,320
you can

00:10:06,320 --> 00:10:13,360
keep adding units and add parallelism

00:10:10,320 --> 00:10:16,480
so this is actually an opportunity

00:10:13,360 --> 00:10:16,959
for example if we look at this graph and

00:10:16,480 --> 00:10:20,560
say

00:10:16,959 --> 00:10:22,640
okay maybe we can map these two

00:10:20,560 --> 00:10:24,160
operations that i've highlighted in

00:10:22,640 --> 00:10:27,200
green here

00:10:24,160 --> 00:10:30,320
to an fpga what would this look like

00:10:27,200 --> 00:10:33,440
for example you can say um

00:10:30,320 --> 00:10:34,399
i'll have a look in my library to see if

00:10:33,440 --> 00:10:37,040
i have any

00:10:34,399 --> 00:10:37,760
if there's any components that i already

00:10:37,040 --> 00:10:41,360
have

00:10:37,760 --> 00:10:41,920
or maybe you can implement them um that

00:10:41,360 --> 00:10:45,920
are able

00:10:41,920 --> 00:10:48,640
to perform this transformation on fpga

00:10:45,920 --> 00:10:50,000
then you can synthesize them to do an

00:10:48,640 --> 00:10:51,680
fpga and then what you

00:10:50,000 --> 00:10:54,320
need is of course you need to interface

00:10:51,680 --> 00:10:57,279
that to your big data framework

00:10:54,320 --> 00:10:57,839
um and i'm actually quite proud to say

00:10:57,279 --> 00:11:00,240
that

00:10:57,839 --> 00:11:01,839
last year in the group we we've made a

00:11:00,240 --> 00:11:04,240
proof of concept

00:11:01,839 --> 00:11:06,399
that does actually this whole workflow

00:11:04,240 --> 00:11:10,560
is we build a prototype that does this

00:11:06,399 --> 00:11:14,000
and it will be presented uh in a later

00:11:10,560 --> 00:11:15,120
presentation by uh fabian students of

00:11:14,000 --> 00:11:17,360
ours

00:11:15,120 --> 00:11:18,320
so if you're interested in this they're

00:11:17,360 --> 00:11:19,839
going to talk

00:11:18,320 --> 00:11:23,279
more in depth about it i'm not going to

00:11:19,839 --> 00:11:25,120
steal too much of their thunder

00:11:23,279 --> 00:11:26,880
but i am going to talk a little bit more

00:11:25,120 --> 00:11:29,120
about

00:11:26,880 --> 00:11:30,399
this infrastructure how are you going to

00:11:29,120 --> 00:11:33,760
interface uh

00:11:30,399 --> 00:11:34,560
this and some more of the tools that

00:11:33,760 --> 00:11:38,000
we've developed

00:11:34,560 --> 00:11:41,120
that it actually facilitates

00:11:38,000 --> 00:11:44,800
designs like these so

00:11:41,120 --> 00:11:46,260
um yeah on the software side

00:11:44,800 --> 00:11:49,040
these big data frameworks they're

00:11:46,260 --> 00:11:50,560
[Music]

00:11:49,040 --> 00:11:52,399
very they're not using the bits and

00:11:50,560 --> 00:11:53,519
bytes they're using strings and

00:11:52,399 --> 00:11:56,639
characters and

00:11:53,519 --> 00:11:57,200
tables and on the hardware side if

00:11:56,639 --> 00:11:59,120
you're

00:11:57,200 --> 00:12:01,360
if you want to write data into an fpga

00:11:59,120 --> 00:12:03,519
you're probably using axi

00:12:01,360 --> 00:12:04,800
it's very bite oriented there's bits

00:12:03,519 --> 00:12:08,240
there's gates

00:12:04,800 --> 00:12:11,440
so again there's this big gap right

00:12:08,240 --> 00:12:12,480
um and um in the apps group we've

00:12:11,440 --> 00:12:15,040
developed a

00:12:12,480 --> 00:12:17,040
software framework that actually

00:12:15,040 --> 00:12:17,519
generates the interfaces that are able

00:12:17,040 --> 00:12:20,240
to

00:12:17,519 --> 00:12:21,360
make this transition this was the topic

00:12:20,240 --> 00:12:24,959
of

00:12:21,360 --> 00:12:27,200
of the talk at the last year's summit

00:12:24,959 --> 00:12:28,160
and it's called fletcher so i'm not

00:12:27,200 --> 00:12:32,160
going to talk

00:12:28,160 --> 00:12:35,200
a lot about it but just to recap

00:12:32,160 --> 00:12:38,000
that that topic as well

00:12:35,200 --> 00:12:40,240
um yeah i should probably mention

00:12:38,000 --> 00:12:43,040
fletcher is able to do this

00:12:40,240 --> 00:12:43,680
because it relies on the in-memory

00:12:43,040 --> 00:12:45,760
format

00:12:43,680 --> 00:12:48,079
that's called apache arrow that

00:12:45,760 --> 00:12:49,760
specifies how data is supposed to lay

00:12:48,079 --> 00:12:51,200
out so that you don't have to copy and

00:12:49,760 --> 00:12:53,440
convert it between

00:12:51,200 --> 00:12:57,120
different big data frameworks or in this

00:12:53,440 --> 00:12:59,680
case big data frameworks and fpga

00:12:57,120 --> 00:13:00,880
so fletcher is an interface generator

00:12:59,680 --> 00:13:04,160
which means that

00:13:00,880 --> 00:13:06,399
if you have an application with the

00:13:04,160 --> 00:13:07,839
corresponding schema of how the data is

00:13:06,399 --> 00:13:11,040
laid out

00:13:07,839 --> 00:13:13,839
fletcher will generate interfaces and

00:13:11,040 --> 00:13:15,600
open copy is one of the interfaces that

00:13:13,839 --> 00:13:19,200
that we support it's actually

00:13:15,600 --> 00:13:21,040
probably our favorite one and then

00:13:19,200 --> 00:13:23,440
on the kernel side instead of having to

00:13:21,040 --> 00:13:27,440
worry about where your data is located

00:13:23,440 --> 00:13:30,079
and interfacing with axi and

00:13:27,440 --> 00:13:31,040
finding finding the correct location and

00:13:30,079 --> 00:13:33,600
everything

00:13:31,040 --> 00:13:36,000
you can just specify which record you

00:13:33,600 --> 00:13:37,680
want which element from your schema

00:13:36,000 --> 00:13:39,120
and you will receive that data in a

00:13:37,680 --> 00:13:42,720
stream that is

00:13:39,120 --> 00:13:44,399
uh very high in performing and

00:13:42,720 --> 00:13:46,480
yeah for the rest the infrastructure

00:13:44,399 --> 00:13:48,959
side is being taken care of

00:13:46,480 --> 00:13:50,079
for you so if you're interested there's

00:13:48,959 --> 00:13:53,839
a publication

00:13:50,079 --> 00:13:56,639
about that on fbl last year

00:13:53,839 --> 00:13:58,399
so that's part one of this interfacing

00:13:56,639 --> 00:14:00,959
question right

00:13:58,399 --> 00:14:03,680
the second part is once the data is on

00:14:00,959 --> 00:14:05,680
the fpga

00:14:03,680 --> 00:14:07,199
how do you communicate it between the

00:14:05,680 --> 00:14:11,199
components

00:14:07,199 --> 00:14:14,399
because again fpgas they work with bits

00:14:11,199 --> 00:14:16,399
buses there's timing involved

00:14:14,399 --> 00:14:18,560
and on the software side you just want

00:14:16,399 --> 00:14:20,000
to say okay i have this data structure i

00:14:18,560 --> 00:14:21,600
want to send it from one place to

00:14:20,000 --> 00:14:24,079
another

00:14:21,600 --> 00:14:25,360
um this is why and i think this is

00:14:24,079 --> 00:14:27,279
probably one of the most

00:14:25,360 --> 00:14:29,839
interesting results from the group in

00:14:27,279 --> 00:14:32,720
the past year

00:14:29,839 --> 00:14:33,519
we've proposed a specification to do

00:14:32,720 --> 00:14:37,279
exactly

00:14:33,519 --> 00:14:39,519
this to represent data on the wire on

00:14:37,279 --> 00:14:41,199
for example on fpga or other digital

00:14:39,519 --> 00:14:44,800
hardware

00:14:41,199 --> 00:14:48,959
uh this was published in ieee micro

00:14:44,800 --> 00:14:51,120
this year so just to give an example

00:14:48,959 --> 00:14:52,560
if you have a structure that has a

00:14:51,120 --> 00:14:55,360
timestamp and an

00:14:52,560 --> 00:14:58,560
array of a couple of strings right

00:14:55,360 --> 00:14:58,560
normally in software

00:14:58,880 --> 00:15:02,399
you express it in a struct or in

00:15:00,800 --> 00:15:05,519
whatever kind of

00:15:02,399 --> 00:15:07,360
data structure that is normally used and

00:15:05,519 --> 00:15:10,720
it gets mapped according to

00:15:07,360 --> 00:15:13,839
how this language works into your

00:15:10,720 --> 00:15:13,839
memory into this

00:15:14,079 --> 00:15:19,760
yeah it's basically a one-dimensional

00:15:17,839 --> 00:15:22,720
array of cells

00:15:19,760 --> 00:15:23,920
um and you always know what it will look

00:15:22,720 --> 00:15:25,440
like because

00:15:23,920 --> 00:15:27,440
the language will take care of it for

00:15:25,440 --> 00:15:30,399
you but if you want to

00:15:27,440 --> 00:15:32,639
send this data on hardware between

00:15:30,399 --> 00:15:34,240
components or whatever

00:15:32,639 --> 00:15:36,079
you need to figure out how you're going

00:15:34,240 --> 00:15:38,560
to express this right so you need to

00:15:36,079 --> 00:15:39,600
write some interface that can actually

00:15:38,560 --> 00:15:42,639
encode

00:15:39,600 --> 00:15:46,160
and interpret this

00:15:42,639 --> 00:15:47,920
this data there's no there's no way that

00:15:46,160 --> 00:15:49,680
you can do it

00:15:47,920 --> 00:15:50,959
in some kind of industry standard

00:15:49,680 --> 00:15:54,079
fashion

00:15:50,959 --> 00:15:57,519
so this is why we've proposed

00:15:54,079 --> 00:16:00,639
to do it uh this is why we proposed tidy

00:15:57,519 --> 00:16:03,440
because tidy actually specifies how this

00:16:00,639 --> 00:16:05,519
transfer should look like in hardware so

00:16:03,440 --> 00:16:07,839
instead of the one-dimensional memory

00:16:05,519 --> 00:16:09,839
we say okay a stream has two dimensions

00:16:07,839 --> 00:16:12,240
which is the width of the bus

00:16:09,839 --> 00:16:14,480
and then of course there's the transfers

00:16:12,240 --> 00:16:17,360
each clock cycles

00:16:14,480 --> 00:16:19,920
so if you develop a component that uses

00:16:17,360 --> 00:16:21,759
the tidy interface specification

00:16:19,920 --> 00:16:23,040
you know exactly how the data will look

00:16:21,759 --> 00:16:25,920
like and how to interface

00:16:23,040 --> 00:16:27,279
with it so this will help increasing

00:16:25,920 --> 00:16:30,480
design reuse

00:16:27,279 --> 00:16:30,480
interoperability a lot

00:16:30,880 --> 00:16:34,959
so going back to this picture we've seen

00:16:32,560 --> 00:16:37,920
before

00:16:34,959 --> 00:16:39,120
the blue interfaces uh are generated by

00:16:37,920 --> 00:16:42,240
fletcher

00:16:39,120 --> 00:16:43,920
and then the the data that's being sent

00:16:42,240 --> 00:16:45,120
between fletcher and between the

00:16:43,920 --> 00:16:48,000
components

00:16:45,120 --> 00:16:51,360
um we can specify how this how this will

00:16:48,000 --> 00:16:54,320
work using tidy

00:16:51,360 --> 00:16:55,680
so another way to look at this is to see

00:16:54,320 --> 00:17:00,480
um

00:16:55,680 --> 00:17:03,120
tidy as in the same way as a file format

00:17:00,480 --> 00:17:05,520
specifies how data should be laid out on

00:17:03,120 --> 00:17:07,679
disk or other types of storage

00:17:05,520 --> 00:17:10,559
and an example that i'm given here is

00:17:07,679 --> 00:17:14,000
aperture 4k because it's very popular in

00:17:10,559 --> 00:17:16,640
big data analytics then

00:17:14,000 --> 00:17:17,280
apache arrow specifies how data should

00:17:16,640 --> 00:17:21,039
look like

00:17:17,280 --> 00:17:21,760
in memory to allow cooperability between

00:17:21,039 --> 00:17:24,319
different

00:17:21,760 --> 00:17:26,799
programming languages and frameworks and

00:17:24,319 --> 00:17:27,439
then tidy specifies how this data will

00:17:26,799 --> 00:17:30,000
look like

00:17:27,439 --> 00:17:32,000
on the wire on digital hardware on fpgas

00:17:30,000 --> 00:17:35,200
or asics

00:17:32,000 --> 00:17:38,320
so actually we think that um

00:17:35,200 --> 00:17:41,679
tidy could be spiritual successor

00:17:38,320 --> 00:17:44,880
of axi for example because currently axi

00:17:41,679 --> 00:17:46,240
is the industry standard for connecting

00:17:44,880 --> 00:17:49,280
ip cores

00:17:46,240 --> 00:17:52,559
but tidy is much more powerful

00:17:49,280 --> 00:17:54,799
to express interfaces you can even

00:17:52,559 --> 00:17:57,440
express an axi interface using tiny if

00:17:54,799 --> 00:17:57,440
you want to

00:17:57,760 --> 00:18:04,160
so um now let's continue with

00:18:01,200 --> 00:18:05,280
some other tools that create more or

00:18:04,160 --> 00:18:08,480
less

00:18:05,280 --> 00:18:10,480
a flow of creating designs around big

00:18:08,480 --> 00:18:13,679
data analytics right

00:18:10,480 --> 00:18:18,000
so starting with

00:18:13,679 --> 00:18:23,120
apache parquet which is the very popular

00:18:18,000 --> 00:18:23,120
format that that data is often stored in

00:18:23,520 --> 00:18:27,200
we've actually implemented the decoder

00:18:25,520 --> 00:18:30,240
for this so that you can

00:18:27,200 --> 00:18:31,679
fetch a file directly from storage onto

00:18:30,240 --> 00:18:33,919
your fpga

00:18:31,679 --> 00:18:35,360
then write it into an arrow table so

00:18:33,919 --> 00:18:37,679
that your cpu can

00:18:35,360 --> 00:18:39,200
continue processing online after it's

00:18:37,679 --> 00:18:41,919
being decoded on your

00:18:39,200 --> 00:18:42,400
fpga of course we use fletcher to write

00:18:41,919 --> 00:18:45,600
these

00:18:42,400 --> 00:18:49,280
arrow tables um

00:18:45,600 --> 00:18:51,600
and then the next step is okay

00:18:49,280 --> 00:18:53,120
often these parquet files are compressed

00:18:51,600 --> 00:18:57,840
using snappy which is a

00:18:53,120 --> 00:18:59,520
compression standard developed by google

00:18:57,840 --> 00:19:03,520
so we've also developed a snappy

00:18:59,520 --> 00:19:07,039
decompressor unit that will be able to

00:19:03,520 --> 00:19:08,960
decompress this data stream directly out

00:19:07,039 --> 00:19:10,320
using fletcher for example to write it

00:19:08,960 --> 00:19:14,160
into your host

00:19:10,320 --> 00:19:16,880
drm so that basically you free your cpu

00:19:14,160 --> 00:19:18,559
of task to decode and it compresses data

00:19:16,880 --> 00:19:22,320
and you can

00:19:18,559 --> 00:19:24,400
run your queries with

00:19:22,320 --> 00:19:26,160
without losing any course uh to the to

00:19:24,400 --> 00:19:29,120
the first steps

00:19:26,160 --> 00:19:29,840
so for example uh we're seeing okay in

00:19:29,120 --> 00:19:32,720
the future

00:19:29,840 --> 00:19:34,240
you will have uh accelerated storage uh

00:19:32,720 --> 00:19:36,480
situations where you have

00:19:34,240 --> 00:19:37,520
a file in non-full time memory for

00:19:36,480 --> 00:19:40,640
example

00:19:37,520 --> 00:19:43,760
then you decompress it on your open copy

00:19:40,640 --> 00:19:46,960
connected fpga and it gets

00:19:43,760 --> 00:19:47,840
written into your drm ready for for your

00:19:46,960 --> 00:19:51,039
query

00:19:47,840 --> 00:19:52,720
to be run on it but you know you can

00:19:51,039 --> 00:19:55,440
also say okay

00:19:52,720 --> 00:19:56,480
let's instead of writing it into dram

00:19:55,440 --> 00:19:59,760
directly

00:19:56,480 --> 00:20:03,280
let's do a part of our query

00:19:59,760 --> 00:20:05,440
on it on fpga as well so

00:20:03,280 --> 00:20:06,720
think back to the use case that i've

00:20:05,440 --> 00:20:07,500
described before

00:20:06,720 --> 00:20:09,840
right where you can

00:20:07,500 --> 00:20:13,760
[Music]

00:20:09,840 --> 00:20:17,360
implement a kernel based on your

00:20:13,760 --> 00:20:19,200
your application graph and our student

00:20:17,360 --> 00:20:20,960
aggro actually made a composition

00:20:19,200 --> 00:20:21,520
language that will help you to create

00:20:20,960 --> 00:20:24,480
these

00:20:21,520 --> 00:20:28,320
kernels he called it title he will talk

00:20:24,480 --> 00:20:28,320
about it some more in this presentation

00:20:29,039 --> 00:20:32,880
and for example something that we are

00:20:31,280 --> 00:20:33,440
currently working on and still work in

00:20:32,880 --> 00:20:36,799
progress

00:20:33,440 --> 00:20:39,440
is okay you want to um

00:20:36,799 --> 00:20:40,880
work on this data using you know not

00:20:39,440 --> 00:20:42,240
just one kernel you

00:20:40,880 --> 00:20:44,159
you probably want to have several

00:20:42,240 --> 00:20:47,760
kernels operating in parallel

00:20:44,159 --> 00:20:49,919
but in that case you need to buffer this

00:20:47,760 --> 00:20:51,760
this file this data and then this would

00:20:49,919 --> 00:20:53,679
be to all of the kernels

00:20:51,760 --> 00:20:56,320
which is difficult in fpga because you

00:20:53,679 --> 00:20:59,440
have very limited vrams

00:20:56,320 --> 00:21:01,280
normally speaking so we're developing an

00:20:59,440 --> 00:21:04,240
hbm buffer that

00:21:01,280 --> 00:21:06,240
that does exactly this which in a file

00:21:04,240 --> 00:21:07,840
that has a certain

00:21:06,240 --> 00:21:09,360
block size and then it feeds all of

00:21:07,840 --> 00:21:12,880
these blocks into

00:21:09,360 --> 00:21:14,480
parallel kernels so if you're keeping an

00:21:12,880 --> 00:21:16,640
eye on the github you'll probably see it

00:21:14,480 --> 00:21:19,600
appear soon

00:21:16,640 --> 00:21:20,320
so next i'd like to mention a couple of

00:21:19,600 --> 00:21:23,360
other

00:21:20,320 --> 00:21:26,000
uh tools that

00:21:23,360 --> 00:21:26,880
yeah you can call it an fpga design

00:21:26,000 --> 00:21:28,240
toolbox

00:21:26,880 --> 00:21:31,840
they're meant to help you create these

00:21:28,240 --> 00:21:34,799
kernel designs so for example

00:21:31,840 --> 00:21:36,080
fletcher makes use of a structural rtl

00:21:34,799 --> 00:21:39,840
generator called

00:21:36,080 --> 00:21:42,799
kerata there's a

00:21:39,840 --> 00:21:43,280
regular expression compiler so that you

00:21:42,799 --> 00:21:47,360
can do

00:21:43,280 --> 00:21:50,240
filtering in in the first steps of your

00:21:47,360 --> 00:21:51,440
your query you can move that to the fpga

00:21:50,240 --> 00:21:54,080
quite easily

00:21:51,440 --> 00:21:55,200
uh there's a streaming component library

00:21:54,080 --> 00:21:58,799
there's a control

00:21:55,200 --> 00:22:00,799
register generator and the dependency

00:21:58,799 --> 00:22:02,540
resolver it's quite

00:22:00,799 --> 00:22:03,919
handy for starting

00:22:02,540 --> 00:22:07,600
[Music]

00:22:03,919 --> 00:22:09,440
simulations and testing creating a test

00:22:07,600 --> 00:22:12,559
infrastructure

00:22:09,440 --> 00:22:15,760
so for example the regular expression

00:22:12,559 --> 00:22:19,360
matcher generator it basically

00:22:15,760 --> 00:22:20,400
generates circuits um like for example

00:22:19,360 --> 00:22:23,600
in sql you have

00:22:20,400 --> 00:22:26,880
an r-like statement and it

00:22:23,600 --> 00:22:28,080
synthesizes them into into an uh state

00:22:26,880 --> 00:22:32,480
machine

00:22:28,080 --> 00:22:36,240
that's actually quite effective on

00:22:32,480 --> 00:22:38,640
fpga there's the

00:22:36,240 --> 00:22:40,720
streaming component library that has a

00:22:38,640 --> 00:22:43,039
bunch of very useful building blocks

00:22:40,720 --> 00:22:46,000
like fifos like

00:22:43,039 --> 00:22:47,760
with conversion which is called the

00:22:46,000 --> 00:22:50,799
serializer

00:22:47,760 --> 00:22:52,320
register slices to improve performance

00:22:50,799 --> 00:22:55,600
to improve timing performance on the

00:22:52,320 --> 00:22:58,400
fpga etc

00:22:55,600 --> 00:23:00,240
then we have the memory mapped register

00:22:58,400 --> 00:23:01,919
generator which is also

00:23:00,240 --> 00:23:04,559
it saves you a lot of time because you

00:23:01,919 --> 00:23:06,400
can basically give it

00:23:04,559 --> 00:23:08,480
a file with with the fields that you

00:23:06,400 --> 00:23:08,960
want and it generates a component for

00:23:08,480 --> 00:23:12,000
you in

00:23:08,960 --> 00:23:12,559
in vhdl it has an oxy slave interface it

00:23:12,000 --> 00:23:14,720
has

00:23:12,559 --> 00:23:15,919
the register interfaces that you've

00:23:14,720 --> 00:23:19,120
asked for

00:23:15,919 --> 00:23:22,400
and it will generate the documentation

00:23:19,120 --> 00:23:23,280
and in the future we were planning to

00:23:22,400 --> 00:23:25,919
also include

00:23:23,280 --> 00:23:26,400
uh generated c headers because yeah that

00:23:25,919 --> 00:23:28,880
would

00:23:26,400 --> 00:23:31,200
also help writing your device drivers

00:23:28,880 --> 00:23:31,200
for it

00:23:31,520 --> 00:23:35,679
so let's go through a couple of use

00:23:34,640 --> 00:23:39,360
cases that

00:23:35,679 --> 00:23:39,360
we've built using these tools

00:23:39,679 --> 00:23:43,279
and i'd like to show the test system

00:23:42,559 --> 00:23:46,320
that we

00:23:43,279 --> 00:23:48,159
are using for for most of our work it's

00:23:46,320 --> 00:23:51,600
based on an

00:23:48,159 --> 00:23:52,480
open power system from inspire to dual

00:23:51,600 --> 00:23:55,279
socket

00:23:52,480 --> 00:23:56,240
power 9 system with with an alpha data

00:23:55,279 --> 00:24:01,440
card

00:23:56,240 --> 00:24:04,559
that has hbm so i think this is a very

00:24:01,440 --> 00:24:07,919
good setup to do research

00:24:04,559 --> 00:24:13,200
and here are some results for

00:24:07,919 --> 00:24:15,440
our pacquiao decoder and as you can see

00:24:13,200 --> 00:24:16,559
the plain version which is just a bunch

00:24:15,440 --> 00:24:21,120
of ins

00:24:16,559 --> 00:24:23,520
it achieves 12 gigabytes per second

00:24:21,120 --> 00:24:25,440
mostly because we're using the 512 bits

00:24:23,520 --> 00:24:28,480
version of oc excel

00:24:25,440 --> 00:24:31,360
but burrino has been helping me to try

00:24:28,480 --> 00:24:32,720
to get the 1024 version working as well

00:24:31,360 --> 00:24:37,279
so i hope to

00:24:32,720 --> 00:24:39,440
get some update results on this soon

00:24:37,279 --> 00:24:42,320
but actually for the more difficult

00:24:39,440 --> 00:24:44,960
encodings like the delta and the strings

00:24:42,320 --> 00:24:46,559
the advantage of using cpu is is

00:24:44,960 --> 00:24:48,799
actually very clear

00:24:46,559 --> 00:24:48,799
here

00:24:49,600 --> 00:24:54,080
so then for the snappy decompressor i

00:24:52,400 --> 00:24:56,080
basically have

00:24:54,080 --> 00:24:57,200
some results in the table here but the

00:24:56,080 --> 00:25:00,559
most interesting

00:24:57,200 --> 00:25:02,480
part of this is that the

00:25:00,559 --> 00:25:04,880
research utilization is actually very

00:25:02,480 --> 00:25:08,159
modest and you can

00:25:04,880 --> 00:25:10,159
achieve very high performance so in this

00:25:08,159 --> 00:25:12,840
case you only need a couple of

00:25:10,159 --> 00:25:14,080
parallel instances to get the maximum

00:25:12,840 --> 00:25:16,799
bandwidth

00:25:14,080 --> 00:25:19,039
now what we also did is we made a

00:25:16,799 --> 00:25:19,760
wikipedia search demo that searches to

00:25:19,039 --> 00:25:23,600
the entire

00:25:19,760 --> 00:25:26,880
data set text without an index

00:25:23,600 --> 00:25:30,080
and this we basically did by

00:25:26,880 --> 00:25:33,279
parallelizing the workload over

00:25:30,080 --> 00:25:37,039
three times uh five kernels

00:25:33,279 --> 00:25:39,520
um each reading from a ddr bank

00:25:37,039 --> 00:25:41,360
uh and they're being decompressed by our

00:25:39,520 --> 00:25:43,039
snepd compressor and then they're going

00:25:41,360 --> 00:25:46,559
to a mature

00:25:43,039 --> 00:25:47,120
um and the results yeah there's even a

00:25:46,559 --> 00:25:50,720
nice

00:25:47,120 --> 00:25:53,840
gui that's also demo that we've created

00:25:50,720 --> 00:25:54,799
uh and the result is that if you put

00:25:53,840 --> 00:25:57,919
this up against

00:25:54,799 --> 00:26:01,919
a dual socket cpu system

00:25:57,919 --> 00:26:04,480
speed up this even five times

00:26:01,919 --> 00:26:05,679
and i have to mark remark here that of

00:26:04,480 --> 00:26:08,799
course we're not

00:26:05,679 --> 00:26:10,720
purely looking for cpu speedups

00:26:08,799 --> 00:26:12,000
we're actually targeting to build like a

00:26:10,720 --> 00:26:14,720
more holistic

00:26:12,000 --> 00:26:16,000
system where you distribute the workload

00:26:14,720 --> 00:26:17,919
according to

00:26:16,000 --> 00:26:20,400
which computational fabric makes the

00:26:17,919 --> 00:26:23,440
more sense right but it does show that

00:26:20,400 --> 00:26:26,559
the fpga can show a lot of

00:26:23,440 --> 00:26:29,200
benefit here um

00:26:26,559 --> 00:26:30,000
and then the last uh probably most

00:26:29,200 --> 00:26:33,039
interesting

00:26:30,000 --> 00:26:34,720
result i have to keep it for my students

00:26:33,039 --> 00:26:37,200
because i don't want to steal it

00:26:34,720 --> 00:26:39,200
from them so i would like to ask you

00:26:37,200 --> 00:26:40,320
stay tuned for their presentation it's

00:26:39,200 --> 00:26:43,440
going to start at 11

00:26:40,320 --> 00:26:44,080
45 in the same track and they will tell

00:26:43,440 --> 00:26:47,840
you

00:26:44,080 --> 00:26:49,440
all about it so um thank you very much

00:26:47,840 --> 00:26:52,480
for your attention and

00:26:49,440 --> 00:26:55,039
for being in this presentation

00:26:52,480 --> 00:26:56,159
with me today today and if there's any

00:26:55,039 --> 00:26:59,120
questions

00:26:56,159 --> 00:27:00,320
please let me know i'll um i'll also

00:26:59,120 --> 00:27:03,840
join the slack

00:27:00,320 --> 00:27:04,480
later but uh you can raise your hand and

00:27:03,840 --> 00:27:07,840
now

00:27:04,480 --> 00:27:21,840
i should be able to give you to give

00:27:07,840 --> 00:27:21,840
you the floor thank you

00:29:18,159 --> 00:29:24,799
so i'm seeing a question by zaid

00:29:22,480 --> 00:29:26,399
about the speed up that we were seeing

00:29:24,799 --> 00:29:29,919
for the

00:29:26,399 --> 00:29:29,919
parquet to error converter

00:29:30,640 --> 00:29:35,760
i guess i should be able to pull up that

00:29:33,120 --> 00:29:35,760
slide again

00:29:38,320 --> 00:29:43,520
so currently for the delta and the

00:29:41,840 --> 00:29:47,360
strings

00:29:43,520 --> 00:29:49,760
encodings i would say it's it's about 3x

00:29:47,360 --> 00:29:50,480
but i do have to say that both of these

00:29:49,760 --> 00:29:53,200
designs

00:29:50,480 --> 00:29:54,080
we can still improve if we use parallel

00:29:53,200 --> 00:29:56,880
prefix

00:29:54,080 --> 00:29:58,159
some implementation because the current

00:29:56,880 --> 00:30:01,039
implementation

00:29:58,159 --> 00:30:02,399
is difficult to uh to achieve time

00:30:01,039 --> 00:30:05,440
enclosure

00:30:02,399 --> 00:30:08,480
uh so basically the the the width

00:30:05,440 --> 00:30:11,600
of these designs are 256 bits

00:30:08,480 --> 00:30:14,799
and with open copy we can go up 2024

00:30:11,600 --> 00:30:18,320
so in principle if if we can get

00:30:14,799 --> 00:30:21,120
them if we can fix the timing

00:30:18,320 --> 00:30:22,240
this can go all the way up to 20

00:30:21,120 --> 00:30:25,440
gigabytes

00:30:22,240 --> 00:30:26,480
per second approximately in that case it

00:30:25,440 --> 00:30:30,480
would be over

00:30:26,480 --> 00:30:37,840
over 10x yeah but that's

00:30:30,480 --> 00:30:37,840
yeah that's hypothetical

00:30:52,640 --> 00:30:56,880
so i'm seeing a question from uh from

00:30:55,520 --> 00:30:59,919
peter about

00:30:56,880 --> 00:31:03,600
our vision of um of getting

00:30:59,919 --> 00:31:06,799
to multiple fpgas

00:31:03,600 --> 00:31:07,760
yeah so we were working with uh with a

00:31:06,799 --> 00:31:09,600
big road map

00:31:07,760 --> 00:31:11,919
in the group over the past year and i

00:31:09,600 --> 00:31:13,919
think the students responded to that uh

00:31:11,919 --> 00:31:15,600
i think it really resonated with them so

00:31:13,919 --> 00:31:16,399
before we knew we had an army of

00:31:15,600 --> 00:31:19,600
students uh

00:31:16,399 --> 00:31:22,399
working on our on our vision

00:31:19,600 --> 00:31:24,159
for these um yeah for accelerated big

00:31:22,399 --> 00:31:26,480
data systems

00:31:24,159 --> 00:31:27,519
and one of the aspects of this was

00:31:26,480 --> 00:31:29,360
obviously

00:31:27,519 --> 00:31:31,440
instead of focusing on getting these

00:31:29,360 --> 00:31:33,440
kernels on one fpga how to distribute

00:31:31,440 --> 00:31:35,760
them over multiple fpgas

00:31:33,440 --> 00:31:36,559
uh so we did a project to combine it

00:31:35,760 --> 00:31:39,840
with

00:31:36,559 --> 00:31:44,240
desk which is actually distributed

00:31:39,840 --> 00:31:46,480
pink for that we we used

00:31:44,240 --> 00:31:48,080
yeah basically they were pink uh bit

00:31:46,480 --> 00:31:51,519
streams

00:31:48,080 --> 00:31:52,159
but in an ideal world we should be able

00:31:51,519 --> 00:31:55,600
to

00:31:52,159 --> 00:31:59,039
distribute these kernels uh over uh

00:31:55,600 --> 00:32:02,480
over multiple fpgas uh as well

00:31:59,039 --> 00:32:04,320
and then whether it's desk or or whether

00:32:02,480 --> 00:32:07,760
it's a spark or another

00:32:04,320 --> 00:32:10,000
big data analytics framework

00:32:07,760 --> 00:32:13,440
they will have to co cooperate in

00:32:10,000 --> 00:32:16,559
distributing the workload between them

00:32:13,440 --> 00:32:19,519
but basically it um

00:32:16,559 --> 00:32:21,039
i think it's uh it's more or less in the

00:32:19,519 --> 00:32:23,519
same order of magnitude

00:32:21,039 --> 00:32:25,360
in terms of difficulty to distribute it

00:32:23,519 --> 00:32:28,080
over multiple kernels

00:32:25,360 --> 00:32:29,440
uh as distributing it over multiple

00:32:28,080 --> 00:32:32,880
fpgas and even on

00:32:29,440 --> 00:32:35,279
multiple uh machines but there

00:32:32,880 --> 00:32:36,640
there has to be some software support uh

00:32:35,279 --> 00:32:38,960
for it

00:32:36,640 --> 00:32:39,679
and uh yeah we're we're thinking about

00:32:38,960 --> 00:32:41,360
that and

00:32:39,679 --> 00:32:45,840
this is also something we discussed with

00:32:41,360 --> 00:32:45,840
aquas and fabian as well of course

00:32:52,640 --> 00:32:57,760
so i'm seeing a question from alan by

00:32:55,360 --> 00:33:00,880
the way thank you all for the questions

00:32:57,760 --> 00:33:03,039
um the question is how transparent

00:33:00,880 --> 00:33:05,279
is this to a programmer when you're

00:33:03,039 --> 00:33:08,640
using spark

00:33:05,279 --> 00:33:10,880
with with python and

00:33:08,640 --> 00:33:12,399
i have i have to be honest that it's

00:33:10,880 --> 00:33:14,559
kind of the topic of

00:33:12,399 --> 00:33:15,760
acquisition fabian's presentation as

00:33:14,559 --> 00:33:18,880
well so

00:33:15,760 --> 00:33:21,519
i can't go in into too much detail

00:33:18,880 --> 00:33:22,480
but i do have to say that currently

00:33:21,519 --> 00:33:25,760
there is still

00:33:22,480 --> 00:33:29,039
a manual step which is the

00:33:25,760 --> 00:33:32,240
composition language i mentioned that

00:33:29,039 --> 00:33:34,640
that's called tidal but

00:33:32,240 --> 00:33:36,399
this language basically has a one-to-one

00:33:34,640 --> 00:33:39,760
mapping with many of these parallel

00:33:36,399 --> 00:33:43,279
patterns like map and reduce

00:33:39,760 --> 00:33:45,120
so it has all the potential to in the

00:33:43,279 --> 00:33:46,720
end become more or less an intermediate

00:33:45,120 --> 00:33:49,679
representation

00:33:46,720 --> 00:33:50,840
uh yeah also to go towards this

00:33:49,679 --> 00:33:54,480
transparency

00:33:50,840 --> 00:33:57,679
of of not having a programmer intervene

00:33:54,480 --> 00:33:58,159
or fpga designer intervene that being

00:33:57,679 --> 00:34:01,039
said

00:33:58,159 --> 00:34:02,240
of course i still believe that there

00:34:01,039 --> 00:34:05,919
will be

00:34:02,240 --> 00:34:07,679
situations where an actual fpga design

00:34:05,919 --> 00:34:08,800
or a data flow engineer or whatever you

00:34:07,679 --> 00:34:11,599
will call it

00:34:08,800 --> 00:34:12,639
we want to look at optimizing a kernel

00:34:11,599 --> 00:34:16,480
because if you're using

00:34:12,639 --> 00:34:20,399
fpgas there's a lot of opportunity to do

00:34:16,480 --> 00:34:24,159
much more for example using um

00:34:20,399 --> 00:34:26,720
application specific number

00:34:24,159 --> 00:34:28,800
well not number representations but

00:34:26,720 --> 00:34:32,399
precisions right and you analyze the

00:34:28,800 --> 00:34:34,720
precision that every variable needs

00:34:32,399 --> 00:34:36,000
so that you don't have to use 64 or 32

00:34:34,720 --> 00:34:38,480
bits for every

00:34:36,000 --> 00:34:41,119
value that you where you don't need it

00:34:38,480 --> 00:34:43,760
so i think in some cases there

00:34:41,119 --> 00:34:45,119
is still value in having an actual

00:34:43,760 --> 00:34:47,200
engineer

00:34:45,119 --> 00:34:48,159
look at these designs so i do think that

00:34:47,200 --> 00:34:50,720
you

00:34:48,159 --> 00:34:51,679
still want to give the opportunity to

00:34:50,720 --> 00:34:53,839
intervene and

00:34:51,679 --> 00:34:56,399
to create manual designs and to very

00:34:53,839 --> 00:34:59,040
easily integrate these manual designs

00:34:56,399 --> 00:35:00,640
uh and reuse these these manual designs

00:34:59,040 --> 00:35:04,560
for example in ip core

00:35:00,640 --> 00:35:07,359
libraries that were also in the end

00:35:04,560 --> 00:35:08,240
looking towards implementing as well

00:35:07,359 --> 00:35:11,040
yeah

00:35:08,240 --> 00:35:12,079
i think that library based acceleration

00:35:11,040 --> 00:35:15,200
is uh

00:35:12,079 --> 00:35:18,640
is going to be a very important in the

00:35:15,200 --> 00:35:18,640
future in some way or another

00:35:19,839 --> 00:35:26,079
so peter

00:35:22,960 --> 00:35:28,240
is asking about tidal and whether it may

00:35:26,079 --> 00:35:31,200
be useful for asic design

00:35:28,240 --> 00:35:33,359
and whether it can co-exist with oxy or

00:35:31,200 --> 00:35:37,280
be used together with oxy

00:35:33,359 --> 00:35:41,119
um and i believe that both

00:35:37,280 --> 00:35:41,760
answers are yes i think tidal can be

00:35:41,119 --> 00:35:46,000
very

00:35:41,760 --> 00:35:49,359
useful for for asic design

00:35:46,000 --> 00:35:50,079
but advantage is i do think probably

00:35:49,359 --> 00:35:52,400
larger

00:35:50,079 --> 00:35:53,200
for fpgas just because they're more

00:35:52,400 --> 00:35:56,640
dynamic

00:35:53,200 --> 00:35:57,200
right so it allows a software engineer

00:35:56,640 --> 00:35:59,599
to

00:35:57,200 --> 00:36:01,599
change his data structure a little bit

00:35:59,599 --> 00:36:03,760
then generate a new interface for it for

00:36:01,599 --> 00:36:07,119
the fpga very quickly

00:36:03,760 --> 00:36:10,320
which on asic in an asic situation

00:36:07,119 --> 00:36:13,280
will not happen as quickly but

00:36:10,320 --> 00:36:14,079
in the future maybe when multi-project

00:36:13,280 --> 00:36:15,760
wafers

00:36:14,079 --> 00:36:18,079
are going to become more and more

00:36:15,760 --> 00:36:20,800
popular because the price of it is

00:36:18,079 --> 00:36:22,079
slowly dropping and the tools are

00:36:20,800 --> 00:36:25,359
becoming available

00:36:22,079 --> 00:36:26,400
i just saw on the keynote which is very

00:36:25,359 --> 00:36:28,400
cool

00:36:26,400 --> 00:36:30,000
then who knows maybe more and more

00:36:28,400 --> 00:36:32,880
people will start

00:36:30,000 --> 00:36:33,680
designing data flow asics and in that

00:36:32,880 --> 00:36:36,960
case

00:36:33,680 --> 00:36:40,560
of course yeah tidal can be very useful

00:36:36,960 --> 00:36:42,400
for ipcor reused and

00:36:40,560 --> 00:36:44,800
and connecting and different data flow

00:36:42,400 --> 00:36:46,560
components together

00:36:44,800 --> 00:36:48,800
and so combined with it with the

00:36:46,560 --> 00:36:51,599
composition language that

00:36:48,800 --> 00:36:55,839
that accuracy created i think that we're

00:36:51,599 --> 00:36:55,839

YouTube URL: https://www.youtube.com/watch?v=Eca1ca52Nw8


