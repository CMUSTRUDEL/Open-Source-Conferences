Title: Competitive Analysis of the Top Gradient... - Sai Ayachit, Shyam R, Anubhav Singh & Vinayak Patil
Publication date: 2020-09-22
Playlist: OpenPOWER Summit NA 2020
Description: 
	Competitive Analysis of the Top Gradient Boosting Machine Learning Algorithms - Sai Ayachit, Shyam R, Anubhav Singh & Vinayak Patil, The National Institute of Engineering

Speakers: Sai Ayachit, Shyam R, Anubhav Singh, Vinayak Patil

Given the rapid increase in computing power and data-driven approaches to tackle many real-world problems today, ML has become an integral part of many solutions.As per a Kaggle survey in 2019, boosting algorithms are among the top 3 preferred methods used by data scientists. Their popularity is due to their robustness against overfitting, faster training times, ability to handle multimodal data while leaving a small memory footprint. In this paper, we compare four state-of-the-art gradient boosting algorithms XGBoost, CatBoost, LightGBM, and SnapBoost on 4 diverse datasets. We perform this competitive analysis on the IBM PowerAI AC922 server. This platform helps end-users experience faster iterations and training than the standard x86. Finally, we present the accuracy and training times of all the algorithms across the 4 datasets. We perform analysis using two approaches; One with only the baseline algorithms, and the other with systematic Hyperparameter Optimization with HyperOpt.
Captions: 
	00:00:00,160 --> 00:00:03,199
hello people hope you're all safe

00:00:02,080 --> 00:00:06,160
wherever you are

00:00:03,199 --> 00:00:07,919
i am sayachev and this is my team we are

00:00:06,160 --> 00:00:09,920
finally a computer science student

00:00:07,919 --> 00:00:11,599
at the national institute of engineering

00:00:09,920 --> 00:00:13,519
mysore and we were a part

00:00:11,599 --> 00:00:15,440
of the ibm global remote mentorship

00:00:13,519 --> 00:00:17,279
program 2020.

00:00:15,440 --> 00:00:19,279
today we are here to demonstrate our

00:00:17,279 --> 00:00:21,199
work competitive analysis

00:00:19,279 --> 00:00:22,560
of the top gradient boosting machine

00:00:21,199 --> 00:00:24,480
learning algorithms

00:00:22,560 --> 00:00:26,000
which was carried out as a part of the

00:00:24,480 --> 00:00:27,920
grm program

00:00:26,000 --> 00:00:29,760
before we start off we would like to

00:00:27,920 --> 00:00:30,480
acknowledge the constant support and

00:00:29,760 --> 00:00:32,800
efforts

00:00:30,480 --> 00:00:34,079
extended by our project mentor mr

00:00:32,800 --> 00:00:36,079
sangeeth kiryadat

00:00:34,079 --> 00:00:38,160
who is a senior developer at systems

00:00:36,079 --> 00:00:41,600
development lab ibm

00:00:38,160 --> 00:00:43,440
and our faculty mentor mr abhinanda nsp

00:00:41,600 --> 00:00:45,039
who is a professor at the department of

00:00:43,440 --> 00:00:48,719
computer science and engineering

00:00:45,039 --> 00:00:50,320
n9 we would also like to thank ibm india

00:00:48,719 --> 00:00:53,600
for providing us with this amazing

00:00:50,320 --> 00:00:55,440
opportunity through the grm program

00:00:53,600 --> 00:00:57,199
we live in the 21st century where

00:00:55,440 --> 00:00:58,559
machine learning has become an integral

00:00:57,199 --> 00:01:00,480
part of many solutions

00:00:58,559 --> 00:01:02,399
owing to the rapid increase in computing

00:01:00,480 --> 00:01:04,400
power and data driven approaches to

00:01:02,399 --> 00:01:06,159
tackle many real-world problems

00:01:04,400 --> 00:01:08,000
with the advent of competitive machine

00:01:06,159 --> 00:01:10,400
learning we started witnessing

00:01:08,000 --> 00:01:12,799
a surge in highly accurate and fast

00:01:10,400 --> 00:01:15,520
machine learning algorithms

00:01:12,799 --> 00:01:17,520
according to kaggle survey in 2019

00:01:15,520 --> 00:01:19,600
boosting algorithms are among the top

00:01:17,520 --> 00:01:21,360
three preferred methods used by data

00:01:19,600 --> 00:01:23,280
scientists for competitive machine

00:01:21,360 --> 00:01:26,799
learning tasks

00:01:23,280 --> 00:01:28,799
gradient boosted decision trees or gbdt

00:01:26,799 --> 00:01:30,799
is one of the techniques that utilize

00:01:28,799 --> 00:01:32,960
gradient boosting to its advantage

00:01:30,799 --> 00:01:35,680
and has seen a wide range of commercial

00:01:32,960 --> 00:01:38,479
and academic applications

00:01:35,680 --> 00:01:40,000
gradient boosting is similar to adaboost

00:01:38,479 --> 00:01:42,320
in a way that they both use an

00:01:40,000 --> 00:01:43,600
ensemble of decision trees to project a

00:01:42,320 --> 00:01:46,079
target label

00:01:43,600 --> 00:01:46,960
however unlike ada boost the gradient

00:01:46,079 --> 00:01:50,159
boosted trees

00:01:46,960 --> 00:01:53,040
have a depth larger than one so what

00:01:50,159 --> 00:01:53,520
exactly does boosting mean in simple

00:01:53,040 --> 00:01:56,000
words

00:01:53,520 --> 00:01:56,640
it is combining a learning algorithm in

00:01:56,000 --> 00:01:58,640
series

00:01:56,640 --> 00:02:01,360
to achieve a strong learner from many

00:01:58,640 --> 00:02:03,920
sequentially connected weak learners

00:02:01,360 --> 00:02:04,960
in the case of the gbbt algorithm the

00:02:03,920 --> 00:02:07,439
weak learners

00:02:04,960 --> 00:02:09,119
are decision trees where each tree

00:02:07,439 --> 00:02:10,640
attempts to minimize the errors of the

00:02:09,119 --> 00:02:13,200
previous trees

00:02:10,640 --> 00:02:15,040
unlike bagging boosting does not involve

00:02:13,200 --> 00:02:17,760
bootstrap sampling

00:02:15,040 --> 00:02:19,360
every time a new tree is added it fits

00:02:17,760 --> 00:02:20,879
on a modified version of the initial

00:02:19,360 --> 00:02:22,959
data set

00:02:20,879 --> 00:02:24,480
since trees are added sequentially

00:02:22,959 --> 00:02:27,440
boosting algorithms

00:02:24,480 --> 00:02:28,879
learn slowly in statistical learning

00:02:27,440 --> 00:02:32,239
models that learn slowly

00:02:28,879 --> 00:02:33,280
perform better gbts are considered a

00:02:32,239 --> 00:02:35,599
black box model

00:02:33,280 --> 00:02:37,519
and our research makes an attempt to

00:02:35,599 --> 00:02:40,480
help improve its explainability

00:02:37,519 --> 00:02:41,680
across several use cases and to do that

00:02:40,480 --> 00:02:44,000
we perform

00:02:41,680 --> 00:02:46,160
an exhaustive 360 degree comparative

00:02:44,000 --> 00:02:49,120
analysis of four state-of-the-art

00:02:46,160 --> 00:02:49,760
gradient boosting algorithms xgboost cad

00:02:49,120 --> 00:02:52,800
boost

00:02:49,760 --> 00:02:54,959
light gbm and ibm's novel snap boost

00:02:52,800 --> 00:02:56,800
by training and testing them on device

00:02:54,959 --> 00:02:59,920
data sets which included

00:02:56,800 --> 00:03:00,959
numeric categorical image and temporal

00:02:59,920 --> 00:03:04,480
data sets

00:03:00,959 --> 00:03:06,159
while leveraging ibm's power ai ac 922

00:03:04,480 --> 00:03:08,159
cpu

00:03:06,159 --> 00:03:10,239
these algorithms were validated for

00:03:08,159 --> 00:03:12,560
being robust against overfitting

00:03:10,239 --> 00:03:13,920
ability to handle categorical variables

00:03:12,560 --> 00:03:16,239
and multimodal data

00:03:13,920 --> 00:03:18,159
while leaving a small memory footprint

00:03:16,239 --> 00:03:20,560
the analysis was performed using two

00:03:18,159 --> 00:03:22,560
approaches

00:03:20,560 --> 00:03:24,640
the first one was using the baseline

00:03:22,560 --> 00:03:26,239
algorithms

00:03:24,640 --> 00:03:28,560
and the other one was by performing

00:03:26,239 --> 00:03:30,640
systematic hyper parameter optimization

00:03:28,560 --> 00:03:32,480
using the hyper-op framework

00:03:30,640 --> 00:03:34,959
although the hpo process is resource

00:03:32,480 --> 00:03:37,680
intensive the power system architecture

00:03:34,959 --> 00:03:39,680
facilitated lower training times without

00:03:37,680 --> 00:03:42,799
compromising the algorithm's accuracy

00:03:39,680 --> 00:03:42,799
for each of the data set

00:03:43,360 --> 00:03:46,640
as we all know in machine learning for

00:03:45,360 --> 00:03:48,000
evaluating a model

00:03:46,640 --> 00:03:50,400
we should look into the performance of

00:03:48,000 --> 00:03:53,040
the model in terms of both

00:03:50,400 --> 00:03:54,799
times and accuracy we present the

00:03:53,040 --> 00:03:56,239
accuracy scores and training times

00:03:54,799 --> 00:03:59,599
across the four datasets

00:03:56,239 --> 00:04:02,319
for both the approaches mentioned before

00:03:59,599 --> 00:04:04,080
before we get any further i'd like to

00:04:02,319 --> 00:04:06,080
give you a quick introduction on our

00:04:04,080 --> 00:04:08,319
runtime environments we used

00:04:06,080 --> 00:04:10,799
throughout the experiments except for

00:04:08,319 --> 00:04:13,280
cad boost all the algorithms for x

00:04:10,799 --> 00:04:16,560
executed on the data sets on the power

00:04:13,280 --> 00:04:19,759
system ac 922 cpu

00:04:16,560 --> 00:04:22,639
ibm's power system ac 922 delivers

00:04:19,759 --> 00:04:25,280
unprecedented performance for analytics

00:04:22,639 --> 00:04:26,800
artificial intelligence and modern high

00:04:25,280 --> 00:04:29,520
performance computing

00:04:26,800 --> 00:04:31,280
the power ac 922 is engineered to be the

00:04:29,520 --> 00:04:32,000
most powerful training platform

00:04:31,280 --> 00:04:34,639
available

00:04:32,000 --> 00:04:35,600
providing the data and compute intensive

00:04:34,639 --> 00:04:37,199
infrastructure

00:04:35,600 --> 00:04:39,199
needed to deliver faster time to

00:04:37,199 --> 00:04:41,280
insights data scientists get

00:04:39,199 --> 00:04:43,360
get to use their favorite tools without

00:04:41,280 --> 00:04:45,199
sacrificing speed and performance

00:04:43,360 --> 00:04:47,440
while it leaders get proven

00:04:45,199 --> 00:04:50,280
infrastructure to accelerate time

00:04:47,440 --> 00:04:52,000
to value this is it infrastructure

00:04:50,280 --> 00:04:54,880
pre-designed for

00:04:52,000 --> 00:04:56,880
enterprise ai now my colleague sharm

00:04:54,880 --> 00:04:59,040
will describe the datasets we used

00:04:56,880 --> 00:05:00,960
the four algorithms and give you an

00:04:59,040 --> 00:05:04,080
overview of the hyperparameter

00:05:00,960 --> 00:05:05,440
optimization technique we used

00:05:04,080 --> 00:05:07,520
let me start off by giving you the

00:05:05,440 --> 00:05:08,800
details of the datasets that we used for

00:05:07,520 --> 00:05:10,320
our experiments

00:05:08,800 --> 00:05:12,880
all the data sets that we used are

00:05:10,320 --> 00:05:14,479
publicly available we specifically chose

00:05:12,880 --> 00:05:16,320
a broad range of data sets

00:05:14,479 --> 00:05:17,600
to ensure that specific algorithms are

00:05:16,320 --> 00:05:19,039
tested for flexibility

00:05:17,600 --> 00:05:20,639
and adaptiveness to different data

00:05:19,039 --> 00:05:22,960
formats

00:05:20,639 --> 00:05:23,759
our first data set is the numeric data

00:05:22,960 --> 00:05:26,160
set

00:05:23,759 --> 00:05:27,680
which is pokemon numerical it contained

00:05:26,160 --> 00:05:29,600
800 numeric entries

00:05:27,680 --> 00:05:31,360
and 11 features with the objective being

00:05:29,600 --> 00:05:32,639
to predict whether or not a pokemon is

00:05:31,360 --> 00:05:34,800
legendary

00:05:32,639 --> 00:05:36,080
given the values of features such as hp

00:05:34,800 --> 00:05:39,600
attack defense

00:05:36,080 --> 00:05:41,199
special attack and special defense etc

00:05:39,600 --> 00:05:43,520
thirty percent of the data was used for

00:05:41,199 --> 00:05:45,600
testing

00:05:43,520 --> 00:05:47,199
next the categorical data set which was

00:05:45,600 --> 00:05:48,560
pokemon categorical

00:05:47,199 --> 00:05:50,479
it had a similar objective as that of

00:05:48,560 --> 00:05:53,199
the numeric data set but had six

00:05:50,479 --> 00:05:53,919
categorical features and 800 categorical

00:05:53,199 --> 00:05:55,360
entries

00:05:53,919 --> 00:05:57,919
30 percent of the data was used for

00:05:55,360 --> 00:05:59,919
testing here as well

00:05:57,919 --> 00:06:01,600
the image data set that we chose was the

00:05:59,919 --> 00:06:04,880
well-known cipher 10 data set

00:06:01,600 --> 00:06:07,120
which consisted of 60 000 32 by 32 color

00:06:04,880 --> 00:06:10,160
images with 10 classes

00:06:07,120 --> 00:06:11,520
in 6000 images per class it includes 50

00:06:10,160 --> 00:06:15,120
000 training images

00:06:11,520 --> 00:06:16,960
and 10 000 testing images

00:06:15,120 --> 00:06:18,960
our fourth and final data set is the

00:06:16,960 --> 00:06:21,680
temporal data set in other words

00:06:18,960 --> 00:06:23,039
the time series data the data set that

00:06:21,680 --> 00:06:24,400
we chose was the daily minimum

00:06:23,039 --> 00:06:27,280
temperatures at melbourne

00:06:24,400 --> 00:06:29,280
it had 3650 entries comprising of three

00:06:27,280 --> 00:06:30,800
years worth of data

00:06:29,280 --> 00:06:32,639
so essentially we tried to test these

00:06:30,800 --> 00:06:35,680
gbt frameworks on

00:06:32,639 --> 00:06:37,759
as many diverse data sets as possible

00:06:35,680 --> 00:06:39,360
this should help expose the weaknesses

00:06:37,759 --> 00:06:41,440
and identify the strong points of each

00:06:39,360 --> 00:06:43,039
of these algorithms

00:06:41,440 --> 00:06:44,960
well now it's time to talk about our

00:06:43,039 --> 00:06:46,160
competitors the boosting algorithms

00:06:44,960 --> 00:06:47,680
themselves

00:06:46,160 --> 00:06:49,360
as mentioned before we compare some of

00:06:47,680 --> 00:06:52,560
the top gradient boosts and frameworks

00:06:49,360 --> 00:06:53,039
to reiterate extreme boost live gbm cad

00:06:52,560 --> 00:06:55,280
boost

00:06:53,039 --> 00:06:57,199
and snap boost let's examine some of the

00:06:55,280 --> 00:06:58,020
key features of these algorithms one by

00:06:57,199 --> 00:06:59,680
one

00:06:58,020 --> 00:07:02,960
[Music]

00:06:59,680 --> 00:07:04,479
first xt boost xt boost is a machine

00:07:02,960 --> 00:07:04,880
learning algorithm based on decision

00:07:04,479 --> 00:07:07,599
trees

00:07:04,880 --> 00:07:09,440
that uses gradient boosting frameworks

00:07:07,599 --> 00:07:11,360
it was developed as a research

00:07:09,440 --> 00:07:12,720
project at the university of washington

00:07:11,360 --> 00:07:15,360
and has been credited with winning

00:07:12,720 --> 00:07:17,520
numerous kaggle competitions

00:07:15,360 --> 00:07:19,360
the two main standouts of xc boost are

00:07:17,520 --> 00:07:20,479
system optimizations and algorithmic

00:07:19,360 --> 00:07:22,080
enhancements

00:07:20,479 --> 00:07:24,080
as far as system optimizations are

00:07:22,080 --> 00:07:25,840
concerned exuberable supports

00:07:24,080 --> 00:07:27,360
parallelization which is possible

00:07:25,840 --> 00:07:28,880
because merely because

00:07:27,360 --> 00:07:30,639
of the highly interchangeable nature of

00:07:28,880 --> 00:07:31,360
the loops used for building the base

00:07:30,639 --> 00:07:33,520
learners

00:07:31,360 --> 00:07:35,280
the outer loop enumerates the leaf nodes

00:07:33,520 --> 00:07:37,919
of the tree and the second inner loop

00:07:35,280 --> 00:07:40,639
calculates the features

00:07:37,919 --> 00:07:42,800
tree pruning xt boost uses max depth

00:07:40,639 --> 00:07:44,800
parameter as specified

00:07:42,800 --> 00:07:47,520
instead of the criterion first and it

00:07:44,800 --> 00:07:49,440
starts pruning the trees backwards

00:07:47,520 --> 00:07:50,800
xt boost also incorporates several

00:07:49,440 --> 00:07:52,400
hardware optimizations

00:07:50,800 --> 00:07:55,039
accomplished by cache awareness

00:07:52,400 --> 00:07:57,280
buffering and multi-threading

00:07:55,039 --> 00:07:58,960
coming to algorithmic enhancements xcboo

00:07:57,280 --> 00:08:00,800
supports regularization

00:07:58,960 --> 00:08:04,160
sparsity awareness weighted quantile

00:08:00,800 --> 00:08:04,160
sketch and cross validation

00:08:04,319 --> 00:08:08,960
our next algorithm is light gbm like gbm

00:08:07,520 --> 00:08:09,440
stands for light gradient boosting

00:08:08,960 --> 00:08:11,039
machine

00:08:09,440 --> 00:08:13,360
and it was developed by microsoft

00:08:11,039 --> 00:08:15,120
corporation in 2017

00:08:13,360 --> 00:08:16,800
it is a robust distributed and high

00:08:15,120 --> 00:08:18,319
performance gradient boosting framework

00:08:16,800 --> 00:08:20,000
that is used for classification

00:08:18,319 --> 00:08:21,360
ranking and many other such machine

00:08:20,000 --> 00:08:23,199
learning tasks

00:08:21,360 --> 00:08:24,639
the word light goes to its surprisingly

00:08:23,199 --> 00:08:26,400
fast nature

00:08:24,639 --> 00:08:28,639
the algorithm uses a novel technique of

00:08:26,400 --> 00:08:31,280
gradient-based one-side sampling

00:08:28,639 --> 00:08:32,959
in that it keeps all the instances with

00:08:31,280 --> 00:08:33,519
large gradients and performs random

00:08:32,959 --> 00:08:36,320
sampling

00:08:33,519 --> 00:08:38,399
on instances with smaller gradients

00:08:36,320 --> 00:08:40,320
light gbm splits the tree leaf wise

00:08:38,399 --> 00:08:42,880
whereas other algorithm splits the tree

00:08:40,320 --> 00:08:42,880
depth wise

00:08:43,680 --> 00:08:47,040
our next algorithm is catboost catboost

00:08:46,320 --> 00:08:48,560
is a recent

00:08:47,040 --> 00:08:50,880
open source machine learning algorithm

00:08:48,560 --> 00:08:52,320
from yandex it can be easily integrated

00:08:50,880 --> 00:08:54,080
with modern deep learning frameworks

00:08:52,320 --> 00:08:56,959
such as google's tensorflow or

00:08:54,080 --> 00:08:58,800
apple's core ml it performs very well

00:08:56,959 --> 00:09:01,040
with diverse datasets

00:08:58,800 --> 00:09:02,800
and it helps solve a wide range of

00:09:01,040 --> 00:09:04,399
problems that businesses face today

00:09:02,800 --> 00:09:06,080
making it an ideal candidate for our

00:09:04,399 --> 00:09:07,680
analysis

00:09:06,080 --> 00:09:09,440
cat boost is also appreciated for its

00:09:07,680 --> 00:09:12,000
ability to outperform algorithms when it

00:09:09,440 --> 00:09:14,000
comes to handling categorical features

00:09:12,000 --> 00:09:15,760
previous algorithms tackle this issue of

00:09:14,000 --> 00:09:17,279
sparse features by exclusive feature

00:09:15,760 --> 00:09:19,680
bundling

00:09:17,279 --> 00:09:21,680
catboost is also 13 to 16 times faster

00:09:19,680 --> 00:09:25,120
than predictions made by other libraries

00:09:21,680 --> 00:09:25,120
according to the yandex benchmark

00:09:26,080 --> 00:09:30,480
the last algorithm that we examine is

00:09:28,080 --> 00:09:32,080
snapboast snapboost is a boosting

00:09:30,480 --> 00:09:34,839
machine which is available under the

00:09:32,080 --> 00:09:37,360
snapml library developed by ibm research

00:09:34,839 --> 00:09:39,360
zurich snapboost performs the functional

00:09:37,360 --> 00:09:40,480
gradient descent to learn an ensemble of

00:09:39,360 --> 00:09:41,839
decision trees

00:09:40,480 --> 00:09:43,760
the booster machine does not take a

00:09:41,839 --> 00:09:45,440
heterogeneous ensemble of decision trees

00:09:43,760 --> 00:09:47,279
instead it takes a probabilistic

00:09:45,440 --> 00:09:47,680
approach to select the maximum tree

00:09:47,279 --> 00:09:50,320
depth

00:09:47,680 --> 00:09:52,399
at each boosting iteration snapboost can

00:09:50,320 --> 00:09:55,440
be used for a variety of classification

00:09:52,399 --> 00:09:57,360
and regression tasks coming to

00:09:55,440 --> 00:09:59,519
hyperparameter optimization

00:09:57,360 --> 00:10:00,560
hyperparameter optimization or hpo is a

00:09:59,519 --> 00:10:02,399
technique through which one can

00:10:00,560 --> 00:10:03,279
considerably improve the accuracies of

00:10:02,399 --> 00:10:05,040
the algorithms

00:10:03,279 --> 00:10:06,720
by carefully tuning the baseline model's

00:10:05,040 --> 00:10:08,079
hyper parameters

00:10:06,720 --> 00:10:09,760
apart from manual tuning which is

00:10:08,079 --> 00:10:11,519
cumbersome there exists several

00:10:09,760 --> 00:10:13,279
automatic hyper parameter optimization

00:10:11,519 --> 00:10:15,680
frameworks such as grid search cv

00:10:13,279 --> 00:10:16,320
or randomized search cv we use the

00:10:15,680 --> 00:10:18,480
hyperopt

00:10:16,320 --> 00:10:20,480
framework for our experiments hyperx

00:10:18,480 --> 00:10:21,760
utilizes a tree of parsons estimator or

00:10:20,480 --> 00:10:23,360
a tpe approach

00:10:21,760 --> 00:10:25,360
for selecting the optimal hyper

00:10:23,360 --> 00:10:28,560
parameter through a specified number of

00:10:25,360 --> 00:10:29,279
iterations to give insights on the hpo

00:10:28,560 --> 00:10:31,279
process

00:10:29,279 --> 00:10:32,959
in our analysis we provide the necessary

00:10:31,279 --> 00:10:34,640
iteration graphs of hyperopt

00:10:32,959 --> 00:10:36,480
and also provide a comparison of the

00:10:34,640 --> 00:10:36,880
accuracies obtained by the algorithms

00:10:36,480 --> 00:10:40,399
with

00:10:36,880 --> 00:10:42,320
and without using hbo now that we've

00:10:40,399 --> 00:10:44,000
established our experimental methodology

00:10:42,320 --> 00:10:45,680
and had a comprehensive overview of the

00:10:44,000 --> 00:10:47,200
algorithms and the data sets that we use

00:10:45,680 --> 00:10:48,720
for our analysis

00:10:47,200 --> 00:10:50,160
binac will now walk you through the

00:10:48,720 --> 00:10:52,720
results that we obtained from our

00:10:50,160 --> 00:10:55,120
experiments

00:10:52,720 --> 00:10:56,399
as mentioned previously we trained xd

00:10:55,120 --> 00:10:58,640
boost light gbm

00:10:56,399 --> 00:11:02,399
and snapboost with the four datasets on

00:10:58,640 --> 00:11:03,200
the cpu on the ibm power system ac 902

00:11:02,399 --> 00:11:05,279
server

00:11:03,200 --> 00:11:07,040
with and without the usage of hyper opt

00:11:05,279 --> 00:11:08,560
the same was done with cad boost on

00:11:07,040 --> 00:11:10,560
google collab cpu

00:11:08,560 --> 00:11:12,480
we plot the accuracy achieved by the

00:11:10,560 --> 00:11:15,279
algorithm and the training time

00:11:12,480 --> 00:11:16,000
in seconds obtained in all the cases

00:11:15,279 --> 00:11:19,040
hyperopt

00:11:16,000 --> 00:11:20,640
adopts the tree of parson estimator for

00:11:19,040 --> 00:11:22,959
hyper parameter tuning

00:11:20,640 --> 00:11:24,240
for the purpose of illustration of how

00:11:22,959 --> 00:11:26,000
hyper opt works

00:11:24,240 --> 00:11:27,360
we have provided the hyper route

00:11:26,000 --> 00:11:30,640
iteration graphs

00:11:27,360 --> 00:11:31,120
for several hyper parameters these

00:11:30,640 --> 00:11:32,800
graphs

00:11:31,120 --> 00:11:34,560
will enable the reader to better

00:11:32,800 --> 00:11:37,519
understand the range of values through

00:11:34,560 --> 00:11:39,839
which hyperopt iterated for each hp

00:11:37,519 --> 00:11:40,800
it is also helpful in determining the

00:11:39,839 --> 00:11:43,200
optimal value

00:11:40,800 --> 00:11:44,959
of a hp for which the accuracy of the

00:11:43,200 --> 00:11:47,440
model was the highest

00:11:44,959 --> 00:11:49,920
the chosen optimal hp value is

00:11:47,440 --> 00:11:52,000
highlighted with red color in the graphs

00:11:49,920 --> 00:11:54,079
interested viewers can feel free to

00:11:52,000 --> 00:11:56,480
check the graphs i demonstrate here

00:11:54,079 --> 00:11:57,120
on our github repository which will be

00:11:56,480 --> 00:12:00,320
provided

00:11:57,120 --> 00:12:02,160
at the end of our talk firstly we

00:12:00,320 --> 00:12:03,200
examine the performance on the numeric

00:12:02,160 --> 00:12:05,600
data set

00:12:03,200 --> 00:12:07,839
here is the accuracy plot for the

00:12:05,600 --> 00:12:09,839
algorithms on the numeric data set

00:12:07,839 --> 00:12:12,639
without hpo

00:12:09,839 --> 00:12:14,160
snap boost performed very well with 99

00:12:12,639 --> 00:12:17,200
accuracy in predictions

00:12:14,160 --> 00:12:18,000
followed by the cat boost with 98 and

00:12:17,200 --> 00:12:21,200
light gbm

00:12:18,000 --> 00:12:24,480
with 95.83 percentage and xg boost

00:12:21,200 --> 00:12:27,839
followed closely with 95.82 percentage

00:12:24,480 --> 00:12:29,680
which all are good performances without

00:12:27,839 --> 00:12:32,240
any optimizations

00:12:29,680 --> 00:12:34,240
in the case with hpo while cat boost and

00:12:32,240 --> 00:12:35,519
snap boost had more or less the same

00:12:34,240 --> 00:12:37,839
accuracies here

00:12:35,519 --> 00:12:40,800
light gbm and xg boost showed an

00:12:37,839 --> 00:12:43,360
improvement of two to three percentage

00:12:40,800 --> 00:12:44,240
here are the hpo graphs the x-axis

00:12:43,360 --> 00:12:46,399
contains the

00:12:44,240 --> 00:12:49,279
value of the hyper-parameter and the

00:12:46,399 --> 00:12:52,000
y-axis contains the accuracy obtained

00:12:49,279 --> 00:12:52,480
as shown the tuning process was carried

00:12:52,000 --> 00:12:55,760
out

00:12:52,480 --> 00:12:56,639
for several hps for all four algorithms

00:12:55,760 --> 00:12:59,680
individually

00:12:56,639 --> 00:13:01,279
the best suited for our optimal

00:12:59,680 --> 00:13:03,440
combination of hps

00:13:01,279 --> 00:13:05,680
are auto selected by hyper out and we

00:13:03,440 --> 00:13:08,480
have highlighted them in red color

00:13:05,680 --> 00:13:08,880
on top on the top left the selected

00:13:08,480 --> 00:13:11,040
value

00:13:08,880 --> 00:13:12,399
of the hyper parameter is also printed

00:13:11,040 --> 00:13:14,720
for convenience

00:13:12,399 --> 00:13:16,160
now for the training time plots light

00:13:14,720 --> 00:13:18,639
gbm is the fastest

00:13:16,160 --> 00:13:21,120
without hpo and snapboost being the

00:13:18,639 --> 00:13:23,040
fastest in the latter

00:13:21,120 --> 00:13:24,720
next we examine the performance on the

00:13:23,040 --> 00:13:26,560
categorical data set

00:13:24,720 --> 00:13:28,480
here is the accuracy plot for the

00:13:26,560 --> 00:13:30,000
algorithms on the numeric data set

00:13:28,480 --> 00:13:32,480
without hpo

00:13:30,000 --> 00:13:34,639
on the categorical data set cad boost

00:13:32,480 --> 00:13:37,839
outperformed the rest by achieving

00:13:34,639 --> 00:13:39,440
99.5 percent accuracy the second place

00:13:37,839 --> 00:13:42,279
is taken by snap boost

00:13:39,440 --> 00:13:44,079
with 98 percentage while xt boost with

00:13:42,279 --> 00:13:46,839
97.24 percentage

00:13:44,079 --> 00:13:48,560
that narrowly overtakes light gbm with

00:13:46,839 --> 00:13:51,440
96.68 percentage

00:13:48,560 --> 00:13:52,560
giving a tough competition with hpo

00:13:51,440 --> 00:13:54,560
using hyper

00:13:52,560 --> 00:13:56,240
all the algorithms showed an improvement

00:13:54,560 --> 00:13:58,480
in performance snap boost

00:13:56,240 --> 00:14:00,240
improved by one percent while the others

00:13:58,480 --> 00:14:03,199
improved by two percent

00:14:00,240 --> 00:14:04,480
here are the hpo graphs now for the

00:14:03,199 --> 00:14:06,720
training time plots

00:14:04,480 --> 00:14:08,399
in the in both the cases snapboost

00:14:06,720 --> 00:14:10,240
performs extremely well

00:14:08,399 --> 00:14:13,040
especially when coupled with the power

00:14:10,240 --> 00:14:16,720
ai architecture

00:14:13,040 --> 00:14:19,360
now the for temporal data without hpo

00:14:16,720 --> 00:14:20,079
on the temporal data set light gbm beats

00:14:19,360 --> 00:14:22,600
the rest

00:14:20,079 --> 00:14:24,639
and achieves the highest accuracy of

00:14:22,600 --> 00:14:28,399
54.45 percentage

00:14:24,639 --> 00:14:31,360
while catboost achieves a 54.12

00:14:28,399 --> 00:14:32,240
percentage score xg boost fails

00:14:31,360 --> 00:14:34,639
miserably

00:14:32,240 --> 00:14:35,839
with hpo snap boost showed the most

00:14:34,639 --> 00:14:39,040
promising improvement

00:14:35,839 --> 00:14:41,440
by 27 percent increase in accuracy

00:14:39,040 --> 00:14:43,680
while all others showed an improvement

00:14:41,440 --> 00:14:45,199
of around 13 to 16 percentage in the

00:14:43,680 --> 00:14:48,560
accuracy

00:14:45,199 --> 00:14:50,800
once again here are the hpo graphs now

00:14:48,560 --> 00:14:52,480
for the training time plots with snap

00:14:50,800 --> 00:14:56,480
boost once again beating them

00:14:52,480 --> 00:14:58,399
all to the finish in both the cases

00:14:56,480 --> 00:15:00,000
finally we examine the performance on

00:14:58,399 --> 00:15:02,160
the image data set

00:15:00,000 --> 00:15:03,600
observing the accuracy plots for both

00:15:02,160 --> 00:15:06,399
the cases we have

00:15:03,600 --> 00:15:08,079
without hpo on the image data set cad

00:15:06,399 --> 00:15:11,120
boost is the clear winner

00:15:08,079 --> 00:15:13,680
as it achieves an accuracy of 61 percent

00:15:11,120 --> 00:15:15,040
while snap boost not far behind manages

00:15:13,680 --> 00:15:18,240
to achieve a 50

00:15:15,040 --> 00:15:20,399
accuracy light gbm again fails here with

00:15:18,240 --> 00:15:23,440
a mere accuracy of 10

00:15:20,399 --> 00:15:25,360
with hpo using hyper opt xd boost

00:15:23,440 --> 00:15:27,040
showed an improvement of 13 percent

00:15:25,360 --> 00:15:30,240
increase in the accuracy

00:15:27,040 --> 00:15:33,040
while others showed an only marginal

00:15:30,240 --> 00:15:34,560
change the hpo graphs are shown for

00:15:33,040 --> 00:15:36,720
reference again

00:15:34,560 --> 00:15:39,279
and lastly the training time plots

00:15:36,720 --> 00:15:40,320
xgboost and light gbm steal the show

00:15:39,279 --> 00:15:43,040
this time

00:15:40,320 --> 00:15:43,839
in the case of using hpo but

00:15:43,040 --> 00:15:46,320
nevertheless

00:15:43,839 --> 00:15:47,199
snapboost has a considerably good speed

00:15:46,320 --> 00:15:48,880
overall

00:15:47,199 --> 00:15:51,120
so these were the results of our

00:15:48,880 --> 00:15:52,560
analysis now anubhav will summarize the

00:15:51,120 --> 00:15:54,480
results we have seen

00:15:52,560 --> 00:15:56,320
and draw some inferences and insights

00:15:54,480 --> 00:15:58,880
from this experiment so are you guys

00:15:56,320 --> 00:16:00,480
excited to know who has won the war

00:15:58,880 --> 00:16:02,560
let me walk you through the inferences

00:16:00,480 --> 00:16:04,560
of the experiments being performed

00:16:02,560 --> 00:16:06,480
all the algorithms were validated for

00:16:04,560 --> 00:16:08,800
being robust against overfitting

00:16:06,480 --> 00:16:10,560
ability to handle categorical variables

00:16:08,800 --> 00:16:12,240
and multi-modal data while leaving a

00:16:10,560 --> 00:16:13,920
small memory footprint

00:16:12,240 --> 00:16:16,320
our research aids in improving the

00:16:13,920 --> 00:16:17,519
explainability of gb dts across several

00:16:16,320 --> 00:16:19,360
use cases

00:16:17,519 --> 00:16:22,079
on examining the performance of

00:16:19,360 --> 00:16:23,040
algorithms on four diverse data sets we

00:16:22,079 --> 00:16:24,959
infer that the

00:16:23,040 --> 00:16:26,320
most consistent performance in terms of

00:16:24,959 --> 00:16:28,880
accuracy and training

00:16:26,320 --> 00:16:30,720
time was seen in snap boost and xd boost

00:16:28,880 --> 00:16:33,519
across all the data sets

00:16:30,720 --> 00:16:34,880
xd boost was always second or third in

00:16:33,519 --> 00:16:36,880
terms of training time

00:16:34,880 --> 00:16:39,440
and put up a stiff competition with

00:16:36,880 --> 00:16:40,959
other algorithms in terms of accuracy

00:16:39,440 --> 00:16:42,800
cad boost worked very well for

00:16:40,959 --> 00:16:44,000
categorical data and outperformed the

00:16:42,800 --> 00:16:46,079
other algorithms

00:16:44,000 --> 00:16:48,720
in image as well but was the slowest

00:16:46,079 --> 00:16:50,720
algorithm in three out of four cases

00:16:48,720 --> 00:16:52,800
light gbm is very fast in terms of

00:16:50,720 --> 00:16:54,320
training on all data sets exclusive

00:16:52,800 --> 00:16:57,120
excluding image data

00:16:54,320 --> 00:16:58,720
but its training time progress is marred

00:16:57,120 --> 00:17:01,519
by its inconsistency in

00:16:58,720 --> 00:17:03,519
accuracy snapbo showed good performance

00:17:01,519 --> 00:17:05,120
on temporal and numeric data set

00:17:03,519 --> 00:17:07,520
and it is consistent in terms of

00:17:05,120 --> 00:17:09,919
accuracy the huge highlight being that

00:17:07,520 --> 00:17:12,079
snapboost when run on ibm power server

00:17:09,919 --> 00:17:14,640
system ac 922 server

00:17:12,079 --> 00:17:15,760
with hbo is the fastest algorithm in

00:17:14,640 --> 00:17:18,240
terms of training time

00:17:15,760 --> 00:17:19,919
which is then followed by light gbm we

00:17:18,240 --> 00:17:21,760
have observed several

00:17:19,919 --> 00:17:24,000
improvements in accuracies of all the

00:17:21,760 --> 00:17:26,000
algorithms after using hyper op for

00:17:24,000 --> 00:17:27,760
hyper parameter optimization

00:17:26,000 --> 00:17:29,840
it reduced overfitting in some of the

00:17:27,760 --> 00:17:32,160
cases while in others it helped in

00:17:29,840 --> 00:17:33,840
making better fits to the data

00:17:32,160 --> 00:17:36,000
on behalf of the team i would like to

00:17:33,840 --> 00:17:37,760
conclude this talk and also remind the

00:17:36,000 --> 00:17:40,400
viewers that this challenge of building

00:17:37,760 --> 00:17:42,240
a robust gbdt framework that excels in

00:17:40,400 --> 00:17:43,280
all scenarios still continues to be an

00:17:42,240 --> 00:17:45,120
open problem

00:17:43,280 --> 00:17:46,320
despite extensive research in this

00:17:45,120 --> 00:17:49,120
domain

00:17:46,320 --> 00:17:50,640
also we would hope that the research has

00:17:49,120 --> 00:17:52,559
substantially aided the future

00:17:50,640 --> 00:17:53,840
enhancement on ongoing work around the

00:17:52,559 --> 00:17:55,280
gbts

00:17:53,840 --> 00:17:57,679
we would like to thank you all for

00:17:55,280 --> 00:17:58,240
lending your ears feel free to contact

00:17:57,679 --> 00:18:00,480
us for

00:17:58,240 --> 00:18:02,320
any queries regarding the talk speaker

00:18:00,480 --> 00:18:03,919
social media links and github links for

00:18:02,320 --> 00:18:04,559
entire work is provided in the next

00:18:03,919 --> 00:18:06,720
slide

00:18:04,559 --> 00:18:07,840
kindly feel free to contact us for any

00:18:06,720 --> 00:18:12,240
queries or

00:18:07,840 --> 00:18:12,240

YouTube URL: https://www.youtube.com/watch?v=IntaycFrQ6g


