Title: Benchmarking Graphical Models on POWER with Healthcare... - Radha Nagarajan & Arghya Kusum Das
Publication date: 2020-09-21
Playlist: OpenPOWER Summit NA 2020
Description: 
	Benchmarking Graphical Models on POWER with Healthcare Applications - Radha Nagarajan, Marshfield Clinic Research Institute & Arghya Kusum Das, University of Wisconsin-Platteville

Speakers: Arghya Kusum Das, Ph.D., Radha Nagarajan, Ph.D.

Graphical models, in particular, that have the ability to validate hypothesized associations from multivariate data and discover novel, undocumented, causal associations have seen increasing applications in healthcare that has witnessed a surge in digitization and adoption of data-driven and evidence-based approaches to assist in decision making. Structure learning algorithms that form the core of graphical modeling and aim to decipher associations between the variables of interest from the given data fall under three broad categories, namely constraint-based, search and score, and hybrid. These computationally intensive algorithms need parallelization across HPC environments for enhanced performance. In this presentation, we will discuss the performance of established structure learning algorithms across POWER9 systems with and without GPUs across large data sets. Our approach will be open-sourced for enhanced transparency and reproducibility and adoption in education and research.
Captions: 
	00:00:00,640 --> 00:00:04,880
thank you for taking time to attend this

00:00:02,480 --> 00:00:06,960
presentation this presentation is on

00:00:04,880 --> 00:00:09,840
benchmarking graphical models

00:00:06,960 --> 00:00:12,240
on power with healthcare applications

00:00:09,840 --> 00:00:13,599
i'm robert nagarajan director center for

00:00:12,240 --> 00:00:16,160
oral and systemic health

00:00:13,599 --> 00:00:18,240
martial clinical research institute

00:00:16,160 --> 00:00:20,640
marshall clinic health system wisconsin

00:00:18,240 --> 00:00:22,000
and we'll i'll be tag teaming with uh

00:00:20,640 --> 00:00:23,680
argo das

00:00:22,000 --> 00:00:25,840
assistant professor professor of

00:00:23,680 --> 00:00:29,199
computer science university of wisconsin

00:00:25,840 --> 00:00:30,000
blackwell so a brief overview of the

00:00:29,199 --> 00:00:32,160
presentation

00:00:30,000 --> 00:00:33,520
we'll talk about graphical models as to

00:00:32,160 --> 00:00:35,200
what these are

00:00:33,520 --> 00:00:37,280
and why they are pertinent and

00:00:35,200 --> 00:00:39,280
implementation aspects

00:00:37,280 --> 00:00:41,360
and uh we'll discuss some of our

00:00:39,280 --> 00:00:42,320
preliminary results this is a work under

00:00:41,360 --> 00:00:43,920
progress

00:00:42,320 --> 00:00:45,600
and we'll also discuss healthcare

00:00:43,920 --> 00:00:47,840
applications

00:00:45,600 --> 00:00:49,360
so before we jump start into graphical

00:00:47,840 --> 00:00:51,440
models what are graphs

00:00:49,360 --> 00:00:52,719
these are essentially composed of nodes

00:00:51,440 --> 00:00:54,960
and edges

00:00:52,719 --> 00:00:56,480
nodes represent the entities of interest

00:00:54,960 --> 00:00:57,520
and edges represent potential

00:00:56,480 --> 00:01:00,640
associations

00:00:57,520 --> 00:01:02,640
including relationship between the nodes

00:01:00,640 --> 00:01:04,159
so graphical models are essentially

00:01:02,640 --> 00:01:06,400
models that is

00:01:04,159 --> 00:01:08,159
derive these association between the

00:01:06,400 --> 00:01:09,280
entities of interest from their digital

00:01:08,159 --> 00:01:11,360
signatures

00:01:09,280 --> 00:01:12,479
an example of a graph is shown on the

00:01:11,360 --> 00:01:14,479
lower right

00:01:12,479 --> 00:01:16,000
where we have the notes this could be

00:01:14,479 --> 00:01:18,400
subjects

00:01:16,000 --> 00:01:19,680
and edges potential association between

00:01:18,400 --> 00:01:22,080
the subjects

00:01:19,680 --> 00:01:23,280
so on the left we've shown a comparative

00:01:22,080 --> 00:01:24,960
uh figure

00:01:23,280 --> 00:01:27,200
that essentially represents the

00:01:24,960 --> 00:01:29,040
reductionist abstraction

00:01:27,200 --> 00:01:31,280
when each of the entities are considered

00:01:29,040 --> 00:01:32,640
in isolation so this is more or less a

00:01:31,280 --> 00:01:34,640
univariate

00:01:32,640 --> 00:01:36,799
representation where we discuss each of

00:01:34,640 --> 00:01:38,479
these entities in isolation

00:01:36,799 --> 00:01:39,840
and some of their properties might not

00:01:38,479 --> 00:01:41,520
readily become a pattern

00:01:39,840 --> 00:01:44,000
which warrants understanding the

00:01:41,520 --> 00:01:46,399
relationship between them

00:01:44,000 --> 00:01:47,840
so why graphical models as discussed on

00:01:46,399 --> 00:01:49,439
the previous slide they can

00:01:47,840 --> 00:01:51,439
definitely reveal system level

00:01:49,439 --> 00:01:52,880
properties and behavior

00:01:51,439 --> 00:01:54,799
not readily patterned from the

00:01:52,880 --> 00:01:57,280
reductionist representation so

00:01:54,799 --> 00:01:58,960
so to speak in a way the principle of

00:01:57,280 --> 00:02:00,640
superposition fails

00:01:58,960 --> 00:02:02,960
for many of the complex systems in the

00:02:00,640 --> 00:02:04,079
real world and uh system level

00:02:02,960 --> 00:02:05,759
abstractions

00:02:04,079 --> 00:02:07,520
understanding them are especially

00:02:05,759 --> 00:02:08,720
critical in developing targeted

00:02:07,520 --> 00:02:11,360
interventions

00:02:08,720 --> 00:02:13,360
examples include modeling code spread in

00:02:11,360 --> 00:02:13,680
a given community using approaches such

00:02:13,360 --> 00:02:16,319
as

00:02:13,680 --> 00:02:16,720
contact pricing and subsequently using

00:02:16,319 --> 00:02:19,040
these

00:02:16,720 --> 00:02:19,920
models to assess community-based

00:02:19,040 --> 00:02:23,599
intervention

00:02:19,920 --> 00:02:25,680
policies and optimal resource location

00:02:23,599 --> 00:02:27,680
so another example would be modeling the

00:02:25,680 --> 00:02:28,800
signaling mechanism in response to a

00:02:27,680 --> 00:02:31,200
pathogen

00:02:28,800 --> 00:02:34,080
and identifying the cascaded process

00:02:31,200 --> 00:02:37,599
that takes place so astro assistant

00:02:34,080 --> 00:02:41,200
drug drug targeting also these

00:02:37,599 --> 00:02:43,680
models can have the capacity to serve as

00:02:41,200 --> 00:02:44,560
uh in silicon models that can be

00:02:43,680 --> 00:02:46,480
controlled

00:02:44,560 --> 00:02:49,360
and experimented in a cost effective

00:02:46,480 --> 00:02:50,959
manner so this includes posing questions

00:02:49,360 --> 00:02:52,080
to these models that fall under

00:02:50,959 --> 00:02:54,239
inference

00:02:52,080 --> 00:02:55,680
so given the evidence of partial

00:02:54,239 --> 00:02:57,440
information can we

00:02:55,680 --> 00:02:58,959
make a statement on the probability of

00:02:57,440 --> 00:03:01,760
outcomes

00:02:58,959 --> 00:03:03,440
so these associations can reveal also

00:03:01,760 --> 00:03:04,080
causal associations under certain

00:03:03,440 --> 00:03:06,879
implicit

00:03:04,080 --> 00:03:07,840
assumptions but it is important to know

00:03:06,879 --> 00:03:10,959
that we are

00:03:07,840 --> 00:03:13,440
we are kind we are not attempting to

00:03:10,959 --> 00:03:16,159
decipher the true causality

00:03:13,440 --> 00:03:18,319
but drag the causality from the observed

00:03:16,159 --> 00:03:20,959
data so that definitely limitations

00:03:18,319 --> 00:03:22,000
and i wanted to explicitly acknowledge

00:03:20,959 --> 00:03:24,560
that

00:03:22,000 --> 00:03:25,760
so what is the problem what we have is

00:03:24,560 --> 00:03:27,920
data

00:03:25,760 --> 00:03:29,440
across a given set of entities

00:03:27,920 --> 00:03:30,959
multivariate data

00:03:29,440 --> 00:03:32,640
and what we need is a graphical

00:03:30,959 --> 00:03:35,200
structure representing potential

00:03:32,640 --> 00:03:38,640
association and representation

00:03:35,200 --> 00:03:41,200
of relationship between these verbals so

00:03:38,640 --> 00:03:43,519
one can ask why can't we directly

00:03:41,200 --> 00:03:44,000
determine the pairwise dependencies

00:03:43,519 --> 00:03:47,200
using

00:03:44,000 --> 00:03:49,840
symmetric measures such as correlation

00:03:47,200 --> 00:03:51,360
or mutual information and that is

00:03:49,840 --> 00:03:53,120
definitely helpful

00:03:51,360 --> 00:03:55,360
but it is important to note that

00:03:53,120 --> 00:03:57,280
association between a pair of entities

00:03:55,360 --> 00:03:59,680
need not necessarily be direct

00:03:57,280 --> 00:04:01,360
and may be mediated to a third variable

00:03:59,680 --> 00:04:03,120
or a set of variables

00:04:01,360 --> 00:04:05,519
so this emphasizes the need to

00:04:03,120 --> 00:04:07,920
incorporate conditional dependencies

00:04:05,519 --> 00:04:10,000
and simple examples shown at the bottom

00:04:07,920 --> 00:04:11,200
where we have laws of taste and disease

00:04:10,000 --> 00:04:13,599
severity

00:04:11,200 --> 00:04:14,319
as such these are marginally independent

00:04:13,599 --> 00:04:17,120
events

00:04:14,319 --> 00:04:18,799
but they may buy it related when we

00:04:17,120 --> 00:04:19,519
condition them on the fact that the

00:04:18,799 --> 00:04:22,799
subject

00:04:19,519 --> 00:04:22,799
is covered positive

00:04:23,600 --> 00:04:26,800
again there are several approaches that

00:04:25,360 --> 00:04:29,759
have been proposed in literature

00:04:26,800 --> 00:04:31,600
to decipher these graphical abstractions

00:04:29,759 --> 00:04:34,240
uh one of the popular approaches based

00:04:31,600 --> 00:04:37,040
in structural learning that models the

00:04:34,240 --> 00:04:38,479
joint probability distribution across a

00:04:37,040 --> 00:04:40,800
given set of entities

00:04:38,479 --> 00:04:42,479
informed set of variables as a product

00:04:40,800 --> 00:04:43,919
of marginal conditional probability

00:04:42,479 --> 00:04:45,759
distributions

00:04:43,919 --> 00:04:47,759
so incorporating conditional

00:04:45,759 --> 00:04:48,880
dependencies is at the core of these

00:04:47,759 --> 00:04:50,960
approaches

00:04:48,880 --> 00:04:53,120
and the resulting network is easy to

00:04:50,960 --> 00:04:54,960
visualize and is in form of a directed

00:04:53,120 --> 00:04:56,639
acyclic graph

00:04:54,960 --> 00:04:58,880
the probabilistic nature of these

00:04:56,639 --> 00:05:02,560
approaches explicitly

00:04:58,880 --> 00:05:04,960
uh well is useful in modeling potential

00:05:02,560 --> 00:05:08,400
uncertainties in real world data

00:05:04,960 --> 00:05:10,400
which is probably which is um ubiquitous

00:05:08,400 --> 00:05:11,840
so an example of a directed acyclic

00:05:10,400 --> 00:05:14,400
graph is shown

00:05:11,840 --> 00:05:14,960
where we have l and d who do not have

00:05:14,400 --> 00:05:18,960
parents

00:05:14,960 --> 00:05:21,600
and uh and uh affect the

00:05:18,960 --> 00:05:23,759
value of c so the joint probability

00:05:21,600 --> 00:05:24,639
distribution of this graph is given by

00:05:23,759 --> 00:05:26,880
probability of

00:05:24,639 --> 00:05:28,560
l product probability of d and

00:05:26,880 --> 00:05:31,919
probability of c

00:05:28,560 --> 00:05:35,039
uh conditioned on l and d so in general

00:05:31,919 --> 00:05:38,240
you have the equation shown below p of x

00:05:35,039 --> 00:05:39,600
equals product of p of x i by condition

00:05:38,240 --> 00:05:42,240
on pi x i

00:05:39,600 --> 00:05:43,840
where pi x i essentially represents the

00:05:42,240 --> 00:05:45,520
patterns of x i

00:05:43,840 --> 00:05:47,280
it's important to note that there's a

00:05:45,520 --> 00:05:49,360
one-to-one map between the conditional

00:05:47,280 --> 00:05:51,039
probability distributions

00:05:49,360 --> 00:05:53,759
giving us a product about and the

00:05:51,039 --> 00:05:56,319
corresponding graphical structure

00:05:53,759 --> 00:05:58,000
so multivariate data what is it how is

00:05:56,319 --> 00:05:59,840
it how does it look like

00:05:58,000 --> 00:06:01,840
there are two forms of data that one can

00:05:59,840 --> 00:06:03,680
encounter in real world

00:06:01,840 --> 00:06:04,960
first is longitudinal data which is

00:06:03,680 --> 00:06:06,880
essentially a continuous

00:06:04,960 --> 00:06:08,240
process that sample as a function of

00:06:06,880 --> 00:06:10,960
time

00:06:08,240 --> 00:06:13,280
these often is are in the form of time

00:06:10,960 --> 00:06:15,280
series multivariate time series

00:06:13,280 --> 00:06:16,880
but there are certain properties that we

00:06:15,280 --> 00:06:19,440
want to preserve when we

00:06:16,880 --> 00:06:21,360
acquire these time series so as its

00:06:19,440 --> 00:06:24,000
properties do not change as a function

00:06:21,360 --> 00:06:26,160
of time so they're challenging to obtain

00:06:24,000 --> 00:06:27,759
another data that's much more common is

00:06:26,160 --> 00:06:29,759
cross-sectional data

00:06:27,759 --> 00:06:32,080
so as opposed to time series where we

00:06:29,759 --> 00:06:33,919
have explicit temporal information

00:06:32,080 --> 00:06:35,840
in the case of cross-sectional data what

00:06:33,919 --> 00:06:36,800
we have is a snapshot of the temporal

00:06:35,840 --> 00:06:39,120
process

00:06:36,800 --> 00:06:40,800
but think of it as taking a camera and

00:06:39,120 --> 00:06:43,280
taking up snapshots

00:06:40,800 --> 00:06:45,199
multiple snapshot of the process but in

00:06:43,280 --> 00:06:46,800
a given time window that's what you get

00:06:45,199 --> 00:06:48,800
in the case of cross-sectional data with

00:06:46,800 --> 00:06:50,880
replicate measurements

00:06:48,800 --> 00:06:53,199
so many of the properties and other

00:06:50,880 --> 00:06:55,599
aspects can vary in this presentation we

00:06:53,199 --> 00:06:58,800
don't address all these complexities

00:06:55,599 --> 00:07:01,199
we assume that all is well and good

00:06:58,800 --> 00:07:02,800
so here's an example of a data the

00:07:01,199 --> 00:07:03,680
cross-sectional data that we talked

00:07:02,800 --> 00:07:05,440
about

00:07:03,680 --> 00:07:07,680
it's shown on the left and it's a form

00:07:05,440 --> 00:07:09,520
of a matrix where the rows represent the

00:07:07,680 --> 00:07:11,360
independent realizations

00:07:09,520 --> 00:07:13,199
and the columns represent essentially

00:07:11,360 --> 00:07:15,520
the variables of inference

00:07:13,199 --> 00:07:17,360
so we have the data now the question is

00:07:15,520 --> 00:07:20,560
how do we define the association's

00:07:17,360 --> 00:07:22,160
relationships between the entities three

00:07:20,560 --> 00:07:23,840
popular approaches

00:07:22,160 --> 00:07:26,080
for structure learning is constraint

00:07:23,840 --> 00:07:28,160
based approach search and score

00:07:26,080 --> 00:07:29,440
and hybrid that uses a combination of

00:07:28,160 --> 00:07:31,520
the about two

00:07:29,440 --> 00:07:33,039
constraint based approaches essentially

00:07:31,520 --> 00:07:34,560
learn the structure using conditional

00:07:33,039 --> 00:07:36,319
independence tests

00:07:34,560 --> 00:07:38,560
search and score learn the structure

00:07:36,319 --> 00:07:40,080
that best test the data using research

00:07:38,560 --> 00:07:44,000
in conjunction with

00:07:40,080 --> 00:07:45,360
with the chosen scoring criteria

00:07:44,000 --> 00:07:47,120
just to get an idea about the

00:07:45,360 --> 00:07:49,680
computational complexity of these

00:07:47,120 --> 00:07:51,759
uh structural learning algorithms

00:07:49,680 --> 00:07:54,400
exhaustive enumeration of the

00:07:51,759 --> 00:07:54,960
as the number of nodes increases the

00:07:54,400 --> 00:07:57,520
number of

00:07:54,960 --> 00:07:59,599
draft decks directed acyclic graphs

00:07:57,520 --> 00:08:02,879
increase super exponentially

00:07:59,599 --> 00:08:05,440
this more or less challenges

00:08:02,879 --> 00:08:06,720
by using these approaches using

00:08:05,440 --> 00:08:09,759
exhaustive enumeration

00:08:06,720 --> 00:08:12,240
from practical standpoints so

00:08:09,759 --> 00:08:14,879
another aspect that needs to be

00:08:12,240 --> 00:08:16,560
emphasized is the mark of equivalence

00:08:14,879 --> 00:08:18,319
although these structures look

00:08:16,560 --> 00:08:20,000
moderately different based on the

00:08:18,319 --> 00:08:21,360
direction of the arrows it's important

00:08:20,000 --> 00:08:23,039
to note that

00:08:21,360 --> 00:08:24,800
they're probabilistically equal

00:08:23,039 --> 00:08:26,639
structures so one can obtain one

00:08:24,800 --> 00:08:29,360
structure from another

00:08:26,639 --> 00:08:29,919
uh by directly applying the base theory

00:08:29,360 --> 00:08:31,759
so

00:08:29,919 --> 00:08:33,519
what i'm trying to get across in this

00:08:31,759 --> 00:08:36,240
slide is that

00:08:33,519 --> 00:08:38,640
uh although it's even if you're able to

00:08:36,240 --> 00:08:40,479
have exhaustive enumeration

00:08:38,640 --> 00:08:42,240
the structures that can be derived from

00:08:40,479 --> 00:08:42,560
observational healthcare data can only

00:08:42,240 --> 00:08:45,600
be

00:08:42,560 --> 00:08:46,640
up to the marker equivalence class so

00:08:45,600 --> 00:08:48,560
here's a

00:08:46,640 --> 00:08:51,279
brief description of the health clamping

00:08:48,560 --> 00:08:53,279
algorithm so how it starts is

00:08:51,279 --> 00:08:54,959
you assume you have a random seed or

00:08:53,279 --> 00:08:56,240
your initial guess of the graphical

00:08:54,959 --> 00:08:57,680
structure

00:08:56,240 --> 00:09:00,000
then you have to choose a scoring

00:08:57,680 --> 00:09:02,000
criteria and score the structure

00:09:00,000 --> 00:09:04,800
then you make modifications to this

00:09:02,000 --> 00:09:07,839
graph then you recompute the scores

00:09:04,800 --> 00:09:08,480
if the recomputed scores of the modified

00:09:07,839 --> 00:09:10,959
structure

00:09:08,480 --> 00:09:12,399
is greater than appreciably greater than

00:09:10,959 --> 00:09:13,920
what we had before

00:09:12,399 --> 00:09:16,640
then we replace the structure and

00:09:13,920 --> 00:09:17,519
iterate and we stop this and exit the

00:09:16,640 --> 00:09:19,600
loop

00:09:17,519 --> 00:09:21,440
when we have a structure whose scores do

00:09:19,600 --> 00:09:23,680
not change appreciably

00:09:21,440 --> 00:09:24,800
from that of the previous iteration

00:09:23,680 --> 00:09:26,959
there are some theoretical

00:09:24,800 --> 00:09:28,640
considerations for research and recent

00:09:26,959 --> 00:09:30,399
article discusses this

00:09:28,640 --> 00:09:32,720
in the interests of time we won't go

00:09:30,399 --> 00:09:35,440
into any detailed sentence

00:09:32,720 --> 00:09:36,160
so hill climbing approach by its very

00:09:35,440 --> 00:09:38,640
nature

00:09:36,160 --> 00:09:39,600
is the sequential algorithm so the score

00:09:38,640 --> 00:09:41,600
of the present test

00:09:39,600 --> 00:09:43,440
structure g star is generated by

00:09:41,600 --> 00:09:45,600
modifying the previous structure

00:09:43,440 --> 00:09:47,440
however there are opportunities for

00:09:45,600 --> 00:09:49,600
distributing the computation in the

00:09:47,440 --> 00:09:51,360
clamping approach for instance the

00:09:49,600 --> 00:09:52,320
potential structure is a snapshot of

00:09:51,360 --> 00:09:54,959
step forwards

00:09:52,320 --> 00:09:56,800
which is shown there for convenience the

00:09:54,959 --> 00:09:58,560
potential structure is interrogated and

00:09:56,800 --> 00:10:00,560
it can be distributed

00:09:58,560 --> 00:10:02,079
and the bic score for instance one of

00:10:00,560 --> 00:10:05,040
the scores uh

00:10:02,079 --> 00:10:05,360
widely used in basic structural learning

00:10:05,040 --> 00:10:07,680
uh

00:10:05,360 --> 00:10:09,279
it's it's express a functional form it

00:10:07,680 --> 00:10:11,760
represents some of the scores of its

00:10:09,279 --> 00:10:13,680
local structure and can be distributed

00:10:11,760 --> 00:10:16,079
since this is a gradient algorithm

00:10:13,680 --> 00:10:18,720
convergence to local optimum cannot be

00:10:16,079 --> 00:10:20,399
avoided so it might have to be repeated

00:10:18,720 --> 00:10:22,880
multiple times with

00:10:20,399 --> 00:10:23,600
different random initial seats that in

00:10:22,880 --> 00:10:26,240
that case

00:10:23,600 --> 00:10:28,000
that aspect can't be distributed and in

00:10:26,240 --> 00:10:29,839
the case the data doesn't fit into a

00:10:28,000 --> 00:10:31,760
commodity ram

00:10:29,839 --> 00:10:33,440
then the data in addition to the

00:10:31,760 --> 00:10:35,040
computation that we discussed can be

00:10:33,440 --> 00:10:38,640
distributed

00:10:35,040 --> 00:10:42,399
so i'm going to hand this to orgo

00:10:38,640 --> 00:10:45,839
now sure

00:10:42,399 --> 00:10:48,000
thanks radha for the nice introduction

00:10:45,839 --> 00:10:50,560
about the bayesian network and also

00:10:48,000 --> 00:10:52,399
from here i am going to discuss about

00:10:50,560 --> 00:10:54,640
our experience on

00:10:52,399 --> 00:10:55,519
power nine by the way i am argya

00:10:54,640 --> 00:10:57,920
kusumdas

00:10:55,519 --> 00:11:00,399
and i am an assistant professor at

00:10:57,920 --> 00:11:02,480
university of wisconsin platteville

00:11:00,399 --> 00:11:04,959
my research interest is mostly like big

00:11:02,480 --> 00:11:06,160
data analytics and their infrastructure

00:11:04,959 --> 00:11:09,279
and some performance

00:11:06,160 --> 00:11:12,399
related things so

00:11:09,279 --> 00:11:14,399
so here in this uh

00:11:12,399 --> 00:11:16,640
in this presentation i am going to talk

00:11:14,399 --> 00:11:18,880
more about our experience on

00:11:16,640 --> 00:11:20,800
power nine and i will do a little bit of

00:11:18,880 --> 00:11:21,440
comparison between our power nine

00:11:20,800 --> 00:11:25,600
experience

00:11:21,440 --> 00:11:27,920
and x86 experiences so uh the power nine

00:11:25,600 --> 00:11:29,040
server that we have used that is

00:11:27,920 --> 00:11:32,000
actually situated

00:11:29,040 --> 00:11:33,279
in university of wisconsin-platteville

00:11:32,000 --> 00:11:37,040
uh that is uh

00:11:33,279 --> 00:11:37,360
ic922 server uh which has a cpu type

00:11:37,040 --> 00:11:40,880
like

00:11:37,360 --> 00:11:44,320
dd 2.3 power 9 and processor module

00:11:40,880 --> 00:11:47,519
and then it has a 160 virtual

00:11:44,320 --> 00:11:50,959
core so with all the smt's enabled and

00:11:47,519 --> 00:11:55,200
it has like access to 32 uh teams

00:11:50,959 --> 00:11:59,040
and sustained bandwidth of 28.8

00:11:55,200 --> 00:12:02,000
gb we also tested we tested the

00:11:59,040 --> 00:12:02,560
compared the performance of this ic92

00:12:02,000 --> 00:12:06,600
server

00:12:02,560 --> 00:12:09,600
with uh with respect to a hpe proliant

00:12:06,600 --> 00:12:14,000
dl-580 server which

00:12:09,600 --> 00:12:16,720
based intel xeon x series processor

00:12:14,000 --> 00:12:17,360
and it has like a number of cores per

00:12:16,720 --> 00:12:21,040
node uh

00:12:17,360 --> 00:12:23,440
i mean 16. so uh this actually makes a

00:12:21,040 --> 00:12:24,959
huge difference so where intel has only

00:12:23,440 --> 00:12:25,519
the hyper threading like two hyper

00:12:24,959 --> 00:12:27,839
threading

00:12:25,519 --> 00:12:29,600
and most of the times uh they are perf

00:12:27,839 --> 00:12:32,480
they have some kind of performance

00:12:29,600 --> 00:12:33,360
glitches like uh if the data size is

00:12:32,480 --> 00:12:35,200
huge uh

00:12:33,360 --> 00:12:37,760
then hyper threading does not optimize

00:12:35,200 --> 00:12:40,800
the performance and all these things

00:12:37,760 --> 00:12:43,200
they are actually our experiences power

00:12:40,800 --> 00:12:44,560
is uh power performed really great by

00:12:43,200 --> 00:12:46,880
the way uh this may all

00:12:44,560 --> 00:12:47,920
both the machines have like 512

00:12:46,880 --> 00:12:52,880
gigabytes of

00:12:47,920 --> 00:12:55,600
ram uh next slide please rather

00:12:52,880 --> 00:12:56,560
so uh regarding the implementation as

00:12:55,600 --> 00:12:59,600
rather told

00:12:56,560 --> 00:13:02,000
that hill climbing method um so uh

00:12:59,600 --> 00:13:03,440
what we have done is first of all we

00:13:02,000 --> 00:13:05,519
have used like

00:13:03,440 --> 00:13:08,639
hepmus dataset which is publicly

00:13:05,519 --> 00:13:12,480
available and it has almost like 10.5

00:13:08,639 --> 00:13:13,760
million samples and it comprises of 28

00:13:12,480 --> 00:13:17,600
variables

00:13:13,760 --> 00:13:19,760
all the continuous normalized variables

00:13:17,600 --> 00:13:21,680
but we have discretized it into binary

00:13:19,760 --> 00:13:23,040
categorical variable by thresholding

00:13:21,680 --> 00:13:26,720
about their mean

00:13:23,040 --> 00:13:29,760
and um then on these

00:13:26,720 --> 00:13:31,200
10.5 millions records we did all our

00:13:29,760 --> 00:13:34,399
bases the bayesian network

00:13:31,200 --> 00:13:37,440
uh structure learning implementation for

00:13:34,399 --> 00:13:39,680
that initially we have used

00:13:37,440 --> 00:13:42,480
pgm pi libraries which has some

00:13:39,680 --> 00:13:47,120
dependencies on pandas and network x

00:13:42,480 --> 00:13:50,959
and uh here actually we have

00:13:47,120 --> 00:13:52,000
we have just uh the hill climbing method

00:13:50,959 --> 00:13:53,920
actually so we have

00:13:52,000 --> 00:13:55,600
recognized i mean we have just analyzed

00:13:53,920 --> 00:13:58,320
the performance of uh

00:13:55,600 --> 00:14:00,880
the hill climbing algorithm uh offered

00:13:58,320 --> 00:14:04,079
by pgmp we have modified it a little bit

00:14:00,880 --> 00:14:06,000
and then tested the performance so

00:14:04,079 --> 00:14:07,519
ah so rather can you go to the next

00:14:06,000 --> 00:14:11,279
slide please

00:14:07,519 --> 00:14:14,959
so here is the performance so

00:14:11,279 --> 00:14:17,600
not we did not only we not only

00:14:14,959 --> 00:14:19,279
did the performance analysis for only

00:14:17,600 --> 00:14:21,839
the single traded

00:14:19,279 --> 00:14:24,000
rather what we were more interested to

00:14:21,839 --> 00:14:27,279
check the parallel performance because

00:14:24,000 --> 00:14:29,760
there is where our novelty based on

00:14:27,279 --> 00:14:30,959
and there is actually power nine also

00:14:29,760 --> 00:14:34,160
surpasses uh

00:14:30,959 --> 00:14:37,760
intel cluster because uh so

00:14:34,160 --> 00:14:40,000
what we have done is we have distributed

00:14:37,760 --> 00:14:42,800
the 10.5 million data set

00:14:40,000 --> 00:14:44,000
using the desk apis over the nodes of

00:14:42,800 --> 00:14:46,399
many memories

00:14:44,000 --> 00:14:48,639
and then we have written a hadoop

00:14:46,399 --> 00:14:51,760
wrapper on top of the pgm pi's

00:14:48,639 --> 00:14:54,639
hill climbing uh instance so

00:14:51,760 --> 00:14:56,240
this is how we have a parallelized our

00:14:54,639 --> 00:14:59,279
entire algorithm and then

00:14:56,240 --> 00:14:59,800
run on top of our ic922 which offers

00:14:59,279 --> 00:15:03,760
like

00:14:59,800 --> 00:15:07,519
160 virtual cores so the next

00:15:03,760 --> 00:15:10,399
slide rather so uh

00:15:07,519 --> 00:15:11,040
here is the performance result which is

00:15:10,399 --> 00:15:14,000
really

00:15:11,040 --> 00:15:14,639
great and as you can see there we got

00:15:14,000 --> 00:15:18,000
like

00:15:14,639 --> 00:15:21,519
significant uh difference in performance

00:15:18,000 --> 00:15:24,800
in terms of like power 9 and x86

00:15:21,519 --> 00:15:27,199
so the most of the time

00:15:24,800 --> 00:15:27,920
we have received when it is just a

00:15:27,199 --> 00:15:30,480
single

00:15:27,920 --> 00:15:32,720
trade uh then also we have received

00:15:30,480 --> 00:15:34,720
almost like 105 times

00:15:32,720 --> 00:15:35,839
performance difference and as you can

00:15:34,720 --> 00:15:38,000
see the uh

00:15:35,839 --> 00:15:40,399
these are like statistically significant

00:15:38,000 --> 00:15:43,519
difference and you can see the

00:15:40,399 --> 00:15:47,680
p value uh in the bottom of this

00:15:43,519 --> 00:15:50,959
slide and as expected like

00:15:47,680 --> 00:15:51,600
as i mentioned it is like bic scoring

00:15:50,959 --> 00:15:55,040
technique

00:15:51,600 --> 00:15:58,160
takes like very less computational

00:15:55,040 --> 00:16:01,600
time and also we have done

00:15:58,160 --> 00:16:04,720
not only the bic scoring we also

00:16:01,600 --> 00:16:07,440
check the performance for k2 scoring and

00:16:04,720 --> 00:16:11,440
as expected like bic score takes less

00:16:07,440 --> 00:16:14,880
computational time over power 9 and x86

00:16:11,440 --> 00:16:17,199
both but if you compare the performance

00:16:14,880 --> 00:16:18,720
of power 9 and x86 you will see a

00:16:17,199 --> 00:16:22,000
constant like

00:16:18,720 --> 00:16:25,120
1.5 times difference and power nine is

00:16:22,000 --> 00:16:27,920
of course better in this case so can we

00:16:25,120 --> 00:16:28,399
go to the next slide please rather uh

00:16:27,920 --> 00:16:31,279
thanks

00:16:28,399 --> 00:16:32,240
so this is the performance for our

00:16:31,279 --> 00:16:34,560
parallelized

00:16:32,240 --> 00:16:36,160
workload where we have used a hadoop

00:16:34,560 --> 00:16:39,680
wrapper and we have just

00:16:36,160 --> 00:16:41,680
uh done like uh so we

00:16:39,680 --> 00:16:43,839
collected the computational time across

00:16:41,680 --> 00:16:44,800
the five rounds of etmos data with hill

00:16:43,839 --> 00:16:48,399
climbing

00:16:44,800 --> 00:16:50,399
and we did a two sample t test with

00:16:48,399 --> 00:16:52,240
unequal variants as you can see in the

00:16:50,399 --> 00:16:56,560
slide and

00:16:52,240 --> 00:16:59,440
we so and as you can see for the

00:16:56,560 --> 00:16:59,839
first two places i mean that there are

00:16:59,440 --> 00:17:03,199
like

00:16:59,839 --> 00:17:04,880
significantly different uh

00:17:03,199 --> 00:17:06,799
difference between the performance of

00:17:04,880 --> 00:17:10,160
power and x86

00:17:06,799 --> 00:17:13,199
and the best part that i you'd like

00:17:10,160 --> 00:17:16,319
to uh focus on

00:17:13,199 --> 00:17:20,160
that after 32 map tasks in

00:17:16,319 --> 00:17:23,360
hadoop intel actually started the

00:17:20,160 --> 00:17:25,199
x86 processor actually started like um

00:17:23,360 --> 00:17:26,799
stabilizing the performance so we did

00:17:25,199 --> 00:17:29,840
not get more from

00:17:26,799 --> 00:17:31,679
that but since power nine has this smt

00:17:29,840 --> 00:17:34,559
simultaneous multi trading

00:17:31,679 --> 00:17:35,120
these trading so we got like a constant

00:17:34,559 --> 00:17:39,280
linear

00:17:35,120 --> 00:17:42,640
improvement in power in performance

00:17:39,280 --> 00:17:45,679
so this is our experience so i

00:17:42,640 --> 00:17:48,080
think that is it from my site and

00:17:45,679 --> 00:17:49,760
rather if you have anything to add on

00:17:48,080 --> 00:17:53,280
performance you can do that and

00:17:49,760 --> 00:17:55,919
the next part is here thank you orgo

00:17:53,280 --> 00:17:57,840
so oh healthcare so how does this all

00:17:55,919 --> 00:17:59,039
fit within the broad scope of healthcare

00:17:57,840 --> 00:18:01,600
applications

00:17:59,039 --> 00:18:03,440
how does this call medicine nexus so

00:18:01,600 --> 00:18:03,760
digital healthcare data has definitely

00:18:03,440 --> 00:18:06,720
seen

00:18:03,760 --> 00:18:08,559
a explosion of late the number of source

00:18:06,720 --> 00:18:10,160
systems to generate healthcare data and

00:18:08,559 --> 00:18:10,960
obtain healthcare data has grown over

00:18:10,160 --> 00:18:12,559
time

00:18:10,960 --> 00:18:14,559
initially it started off with high-tech

00:18:12,559 --> 00:18:16,640
hacked and mandatory compliance and

00:18:14,559 --> 00:18:19,120
implementation in the united states

00:18:16,640 --> 00:18:20,480
of electronic health records then we had

00:18:19,120 --> 00:18:22,480
claims data

00:18:20,480 --> 00:18:24,320
and then registries a number of

00:18:22,480 --> 00:18:25,919
registries have gone and more recently

00:18:24,320 --> 00:18:28,640
internet of things have come

00:18:25,919 --> 00:18:29,120
to be one of the dominant players from

00:18:28,640 --> 00:18:30,880
where

00:18:29,120 --> 00:18:32,400
data is being retrieved in a constant

00:18:30,880 --> 00:18:35,200
basis so

00:18:32,400 --> 00:18:37,280
uh what data types that we encounter in

00:18:35,200 --> 00:18:39,360
healthcare testings also come

00:18:37,280 --> 00:18:41,600
from in different flavors for instance

00:18:39,360 --> 00:18:42,400
the data could be clinician notes free

00:18:41,600 --> 00:18:45,679
text

00:18:42,400 --> 00:18:48,080
or structured text after a

00:18:45,679 --> 00:18:50,480
certain amount of standardization so you

00:18:48,080 --> 00:18:53,120
have the data in the form of stats or

00:18:50,480 --> 00:18:53,600
images it could be radiographic images

00:18:53,120 --> 00:18:56,160
such as

00:18:53,600 --> 00:18:59,120
x-rays fmris or it could be

00:18:56,160 --> 00:19:02,559
physiological signals such as eeg

00:18:59,120 --> 00:19:05,360
ecg or any such so you have a compendium

00:19:02,559 --> 00:19:07,200
data sets that you encounter in addition

00:19:05,360 --> 00:19:09,200
to that there's a strong emphasis on

00:19:07,200 --> 00:19:12,080
multi-scale profiling

00:19:09,200 --> 00:19:13,039
where we want to see uh interrogate a

00:19:12,080 --> 00:19:14,799
given subject

00:19:13,039 --> 00:19:16,559
from different angles so it could be

00:19:14,799 --> 00:19:19,440
multiple data

00:19:16,559 --> 00:19:21,600
it could be uh behavioral data and so we

00:19:19,440 --> 00:19:23,200
have multiple types of data sets

00:19:21,600 --> 00:19:25,760
in fact that flies under the broad

00:19:23,200 --> 00:19:28,240
umbrella and falls within the objective

00:19:25,760 --> 00:19:30,480
operational medicine and there is high

00:19:28,240 --> 00:19:30,799
triplet data in healthcare such as those

00:19:30,480 --> 00:19:33,280
that

00:19:30,799 --> 00:19:34,160
obtain from high throughput molecular

00:19:33,280 --> 00:19:36,720
assay such as

00:19:34,160 --> 00:19:37,440
next generation sequencing and there is

00:19:36,720 --> 00:19:39,520
also

00:19:37,440 --> 00:19:40,880
significant progress in standardizing

00:19:39,520 --> 00:19:43,120
these data sets

00:19:40,880 --> 00:19:45,200
several initiatives are there in place

00:19:43,120 --> 00:19:46,480
and uh these have resolved results that

00:19:45,200 --> 00:19:48,559
are not only

00:19:46,480 --> 00:19:50,480
a massive data sets in that healthcare

00:19:48,559 --> 00:19:52,160
system or organization

00:19:50,480 --> 00:19:54,400
pooling data sets from multiple

00:19:52,160 --> 00:19:56,400
healthcare systems or organization

00:19:54,400 --> 00:19:57,840
through health information exchange and

00:19:56,400 --> 00:20:00,000
other efforts

00:19:57,840 --> 00:20:02,400
and more recently fast healthcare

00:20:00,000 --> 00:20:05,520
interoperability resources give a fine

00:20:02,400 --> 00:20:08,400
nexus to iots and other devices

00:20:05,520 --> 00:20:09,120
and in exchange of data sets so all

00:20:08,400 --> 00:20:11,600
these things

00:20:09,120 --> 00:20:13,919
culminate and contribute to the massive

00:20:11,600 --> 00:20:16,240
data explosion in healthcare

00:20:13,919 --> 00:20:18,400
and there is also a strong trend and

00:20:16,240 --> 00:20:20,640
increasing analytics adoption

00:20:18,400 --> 00:20:23,280
initially there was always concern about

00:20:20,640 --> 00:20:26,799
the storage and hadoop and other kind of

00:20:23,280 --> 00:20:30,480
big data ecosystem came to the rescue

00:20:26,799 --> 00:20:32,960
and uh but however over time the storage

00:20:30,480 --> 00:20:35,120
problem has more or less been handled

00:20:32,960 --> 00:20:36,240
in some sense but there's a strong

00:20:35,120 --> 00:20:37,840
emphasis on

00:20:36,240 --> 00:20:40,240
trying to retrieve knowledge out of

00:20:37,840 --> 00:20:42,080
these data sets these fly under

00:20:40,240 --> 00:20:43,600
descriptive operative and prescriptive

00:20:42,080 --> 00:20:45,600
analytics so

00:20:43,600 --> 00:20:47,360
the challenge increases from descriptive

00:20:45,600 --> 00:20:48,159
to prescription descriptive is more or

00:20:47,360 --> 00:20:50,880
less trying to

00:20:48,159 --> 00:20:52,000
get summary statistics like querying the

00:20:50,880 --> 00:20:54,480
data to aggregate

00:20:52,000 --> 00:20:56,799
aggregate counts and productive would be

00:20:54,480 --> 00:20:58,480
predicting longitudinal outcomes so five

00:20:56,799 --> 00:21:00,000
minus down the line what would be the

00:20:58,480 --> 00:21:01,520
clinical outcome

00:21:00,000 --> 00:21:03,200
or what would be the operational or

00:21:01,520 --> 00:21:04,880
financial outcome and perspective

00:21:03,200 --> 00:21:06,480
analytics is how do we come up with

00:21:04,880 --> 00:21:08,720
interventions

00:21:06,480 --> 00:21:11,840
so and then there's a strong shift from

00:21:08,720 --> 00:21:13,919
consensus based or agreed upon

00:21:11,840 --> 00:21:15,520
approaches to more evidence-based and

00:21:13,919 --> 00:21:16,080
data different approaches that can

00:21:15,520 --> 00:21:18,400
impact

00:21:16,080 --> 00:21:20,240
outcomes as well as key performance

00:21:18,400 --> 00:21:22,080
indicators in healthcare

00:21:20,240 --> 00:21:23,679
and then definitely and many of you

00:21:22,080 --> 00:21:24,720
would have noted this research in the

00:21:23,679 --> 00:21:26,720
adoption of

00:21:24,720 --> 00:21:28,840
machine learning and artificial

00:21:26,720 --> 00:21:31,840
intelligence approaches that includes

00:21:28,840 --> 00:21:34,320
mining natural language

00:21:31,840 --> 00:21:35,760
such as nlp approaches as well as

00:21:34,320 --> 00:21:39,120
imaging data

00:21:35,760 --> 00:21:41,600
so there's a whole lot of exciting uh

00:21:39,120 --> 00:21:43,200
initiatives that are underway so

00:21:41,600 --> 00:21:44,000
graphical models we started this

00:21:43,200 --> 00:21:45,919
conversation with

00:21:44,000 --> 00:21:47,200
graphical models how does this all tie

00:21:45,919 --> 00:21:48,880
together so

00:21:47,200 --> 00:21:50,880
healthcare applications of graphical

00:21:48,880 --> 00:21:53,679
models include diagnostic reasoning

00:21:50,880 --> 00:21:55,280
prognostic reasoning it can also assist

00:21:53,679 --> 00:21:58,080
in treatment selection as well as

00:21:55,280 --> 00:22:00,080
discovering functional associations

00:21:58,080 --> 00:22:01,679
so healthcare data sets for various

00:22:00,080 --> 00:22:03,440
reasons we

00:22:01,679 --> 00:22:05,840
we have not delved into the delved into

00:22:03,440 --> 00:22:06,720
this in detail but are noisy by their

00:22:05,840 --> 00:22:09,120
very nature

00:22:06,720 --> 00:22:10,080
and multivariate several factors

00:22:09,120 --> 00:22:11,919
attribute to it

00:22:10,080 --> 00:22:14,480
making probabilistic graphical models a

00:22:11,919 --> 00:22:16,559
natural choice to mine them

00:22:14,480 --> 00:22:17,600
and many of the associations between

00:22:16,559 --> 00:22:20,480
these entities

00:22:17,600 --> 00:22:21,679
are unknown so graphical models not only

00:22:20,480 --> 00:22:24,000
have the ability to

00:22:21,679 --> 00:22:25,280
validate what is known in terms of

00:22:24,000 --> 00:22:27,360
association but also

00:22:25,280 --> 00:22:28,400
discover new associations so as to

00:22:27,360 --> 00:22:30,480
assist in

00:22:28,400 --> 00:22:32,799
hypothesis generation as opposed to

00:22:30,480 --> 00:22:34,960
classical hypothesis testing

00:22:32,799 --> 00:22:37,280
and de-suffering this association as

00:22:34,960 --> 00:22:39,679
discussed in the earlier slides

00:22:37,280 --> 00:22:42,000
is an important preliminary step that

00:22:39,679 --> 00:22:44,000
can assist in targeted intervention

00:22:42,000 --> 00:22:45,440
and graphical models fall under machine

00:22:44,000 --> 00:22:46,720
learning as well as artificial

00:22:45,440 --> 00:22:49,120
intelligence

00:22:46,720 --> 00:22:50,159
and uh one of the popular classification

00:22:49,120 --> 00:22:52,480
approach which is

00:22:50,159 --> 00:22:53,600
kind of in some sense a very constrained

00:22:52,480 --> 00:22:56,559
graphical model

00:22:53,600 --> 00:22:56,880
is your naive bayes classifier but then

00:22:56,559 --> 00:22:59,039
uh

00:22:56,880 --> 00:23:00,400
in the more advanced cases where kind of

00:22:59,039 --> 00:23:03,280
builds nexus to ai

00:23:00,400 --> 00:23:05,039
is you can post queries to these models

00:23:03,280 --> 00:23:07,280
and kind of

00:23:05,039 --> 00:23:08,400
estimate probabilities on outcomes based

00:23:07,280 --> 00:23:11,200
on

00:23:08,400 --> 00:23:11,679
partial knowledge or partial evidence

00:23:11,200 --> 00:23:14,960
and

00:23:11,679 --> 00:23:18,080
um the nexus to causality also

00:23:14,960 --> 00:23:20,400
brings it as a as an approach that can

00:23:18,080 --> 00:23:21,360
complement rcds or randomized control

00:23:20,400 --> 00:23:23,600
trials

00:23:21,360 --> 00:23:26,480
randomized control trials are thought of

00:23:23,600 --> 00:23:28,880
as the gold standards and treatment

00:23:26,480 --> 00:23:30,159
recommendations but then again they are

00:23:28,880 --> 00:23:31,840
idealized in this

00:23:30,159 --> 00:23:33,600
in that there's a lot of inclusion

00:23:31,840 --> 00:23:35,919
exclusion criteria

00:23:33,600 --> 00:23:38,559
so they really don't incorporate the

00:23:35,919 --> 00:23:40,400
population specific digital signatures

00:23:38,559 --> 00:23:42,480
in which case graphical models can come

00:23:40,400 --> 00:23:45,760
in handy and definitely

00:23:42,480 --> 00:23:48,320
they are easy easy to visualize and

00:23:45,760 --> 00:23:50,320
easy to interpret as opposed to some of

00:23:48,320 --> 00:23:51,200
the black box algorithms that are out

00:23:50,320 --> 00:23:53,520
there

00:23:51,200 --> 00:23:55,360
and what we need is architectures and

00:23:53,520 --> 00:23:56,000
programming environment that can dive

00:23:55,360 --> 00:23:58,960
into these

00:23:56,000 --> 00:24:00,640
large gamut of data sets so as to derive

00:23:58,960 --> 00:24:02,799
this knowledge from it

00:24:00,640 --> 00:24:04,640
and assistant decision making and this

00:24:02,799 --> 00:24:07,360
could also be real-time clinical

00:24:04,640 --> 00:24:09,679
decision making

00:24:07,360 --> 00:24:11,440
so in summary what we have talked about

00:24:09,679 --> 00:24:13,679
in this presentation and we have covered

00:24:11,440 --> 00:24:14,720
is we talked about graphical models what

00:24:13,679 --> 00:24:17,279
they are

00:24:14,720 --> 00:24:17,760
and what how the computational aspects

00:24:17,279 --> 00:24:20,799
and

00:24:17,760 --> 00:24:23,760
how they are computationally intense

00:24:20,799 --> 00:24:25,279
and uh then we shared some of our

00:24:23,760 --> 00:24:26,880
preliminary findings there's still a

00:24:25,279 --> 00:24:29,760
long way to go

00:24:26,880 --> 00:24:31,120
and we also built nexus to healthcare

00:24:29,760 --> 00:24:33,760
applications

00:24:31,120 --> 00:24:36,000
and uh we'd like to acknowledge some of

00:24:33,760 --> 00:24:38,159
our uh

00:24:36,000 --> 00:24:40,080
the people who are played a critical

00:24:38,159 --> 00:24:41,760
role in this research and contributed

00:24:40,080 --> 00:24:43,919
from various standpoints

00:24:41,760 --> 00:24:46,159
they are listed below and the contact

00:24:43,919 --> 00:24:48,799
information of

00:24:46,159 --> 00:24:54,400
we can be contacted at the email below

00:24:48,799 --> 00:24:54,400

YouTube URL: https://www.youtube.com/watch?v=NmVPe5H8C74


