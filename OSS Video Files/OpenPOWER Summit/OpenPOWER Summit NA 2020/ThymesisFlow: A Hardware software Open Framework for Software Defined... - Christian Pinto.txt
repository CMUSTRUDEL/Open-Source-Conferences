Title: ThymesisFlow: A Hardware software Open Framework for Software Defined... - Christian Pinto
Publication date: 2020-09-21
Playlist: OpenPOWER Summit NA 2020
Description: 
	ThymesisFlow: A Hardware/software Open Framework for Software Defined Disaggregation Based on OpenCAPI - Christian Pinto, IBM Research Europe

Speakers: Christian Pinto

In this talk Dr Pinto will introduce ThymesisFlow, a hardware/software open framework based on OpenCAPI for implementing off-tray peripherals disaggregation via a software defined interconnect. Peripherals can be borrowed from a remote server and interfaced to the local processor, in the same spirit an off-chip peripheral is connected to the processor. ThymesisFlow is based on the OpenCAPI 3.0 specification and relies on a custom MAC that interfaces to a low-latency network. Each ThymesisFlow instance serves as bridge between the local coherence domain of a POWER processor and a remote counterpart, mastering unmodified OpenCAPI transactions towards the correct remote coherent domains based on the target address. During this talk Dr. Pinto will drive the audience through the hardware/software design of a main system memory disaggregation prototype using ThymesisFlow, its full stack evaluation on a set of cloud workloads, as well as a live demonstration on two IBM AC922 POWER9 servers.
Captions: 
	00:00:00,240 --> 00:00:03,840
good morning or good afternoon everybody

00:00:02,480 --> 00:00:05,920
depending on

00:00:03,840 --> 00:00:08,080
where you are i'm i'm christian pinto

00:00:05,920 --> 00:00:11,280
from ibm research europe in

00:00:08,080 --> 00:00:14,080
dublin and today i will

00:00:11,280 --> 00:00:15,040
let's introduce you to team assist flow

00:00:14,080 --> 00:00:17,119
that is our

00:00:15,040 --> 00:00:18,160
open source hardware software framework

00:00:17,119 --> 00:00:20,480
for uh

00:00:18,160 --> 00:00:22,000
software defined disaggregation that

00:00:20,480 --> 00:00:24,640
that is based on

00:00:22,000 --> 00:00:26,560
on open copy so if you are familiar with

00:00:24,640 --> 00:00:30,080
the the open power summit

00:00:26,560 --> 00:00:30,800
you might also be familiar with the team

00:00:30,080 --> 00:00:33,360
assist flow

00:00:30,800 --> 00:00:35,200
as well because last year there was a an

00:00:33,360 --> 00:00:38,399
extensive

00:00:35,200 --> 00:00:41,520
representation of the hardware design

00:00:38,399 --> 00:00:44,160
so today i will just quickly go

00:00:41,520 --> 00:00:45,520
through a few details of the design that

00:00:44,160 --> 00:00:46,960
you need to know for

00:00:45,520 --> 00:00:50,239
for the rest of the presentation but i

00:00:46,960 --> 00:00:53,360
want mostly i was i mostly want to

00:00:50,239 --> 00:00:55,520
uh let's say concentrate on the

00:00:53,360 --> 00:00:57,199
software stock that we have built and

00:00:55,520 --> 00:00:58,079
then the evaluation that we have done

00:00:57,199 --> 00:01:01,280
using this

00:00:58,079 --> 00:01:03,039
this software start so let's start from

00:01:01,280 --> 00:01:05,040
the beginning okay so what is this

00:01:03,039 --> 00:01:07,119
software defined memory disaggregation

00:01:05,040 --> 00:01:09,040
if we you've been hearing a lot about

00:01:07,119 --> 00:01:11,439
today about disaggregation and

00:01:09,040 --> 00:01:12,320
composing cloud also the p10 talk that

00:01:11,439 --> 00:01:15,200
was great

00:01:12,320 --> 00:01:17,200
right so essentially when we say

00:01:15,200 --> 00:01:18,640
software-defined memory disaggregation

00:01:17,200 --> 00:01:22,159
is that in a classical

00:01:18,640 --> 00:01:24,320
system you have uh in the data center

00:01:22,159 --> 00:01:26,560
for instance you have a series of boxes

00:01:24,320 --> 00:01:27,680
each box has cpu memory network

00:01:26,560 --> 00:01:30,240
interfaces

00:01:27,680 --> 00:01:32,000
and the application operating system run

00:01:30,240 --> 00:01:34,320
on each of these boxes

00:01:32,000 --> 00:01:35,360
thinking that the peripherals that they

00:01:34,320 --> 00:01:37,439
use

00:01:35,360 --> 00:01:38,960
are physically on those in those boxes

00:01:37,439 --> 00:01:41,840
and they are actually in these boxes

00:01:38,960 --> 00:01:44,159
right in a software disaggregated setup

00:01:41,840 --> 00:01:46,880
that you see on the right

00:01:44,159 --> 00:01:47,520
the application and operating system are

00:01:46,880 --> 00:01:50,399
running

00:01:47,520 --> 00:01:52,720
on a kind of abstraction right we call

00:01:50,399 --> 00:01:55,360
it software-defined compute node

00:01:52,720 --> 00:01:56,960
and it is essentially an abstraction

00:01:55,360 --> 00:01:59,040
that we create in software that

00:01:56,960 --> 00:02:00,880
fools the operating system and the

00:01:59,040 --> 00:02:03,439
application of thinking that

00:02:00,880 --> 00:02:04,320
as usual all the peripherals on are on

00:02:03,439 --> 00:02:06,079
the same node

00:02:04,320 --> 00:02:07,680
so in this case the the os thinks that

00:02:06,079 --> 00:02:08,560
there will be one cpu and two memories

00:02:07,680 --> 00:02:11,680
on the same note

00:02:08,560 --> 00:02:14,720
well actually a part of these

00:02:11,680 --> 00:02:16,400
peripherals are located physically on

00:02:14,720 --> 00:02:18,879
another machine

00:02:16,400 --> 00:02:20,879
so they are disaggregated and this is

00:02:18,879 --> 00:02:21,280
made possible because all the machines

00:02:20,879 --> 00:02:23,680
that

00:02:21,280 --> 00:02:24,560
participate in this disaggregation

00:02:23,680 --> 00:02:26,800
system

00:02:24,560 --> 00:02:29,120
are connected by uh the custom

00:02:26,800 --> 00:02:32,400
disaggregation fabric

00:02:29,120 --> 00:02:34,800
so if you think if if your application

00:02:32,400 --> 00:02:37,519
needs more memory than what you have on

00:02:34,800 --> 00:02:37,920
a node in a classic setup what you would

00:02:37,519 --> 00:02:39,760
do

00:02:37,920 --> 00:02:40,959
is essentially you would scale out on

00:02:39,760 --> 00:02:43,360
two nodes so

00:02:40,959 --> 00:02:44,640
you run two instances of the application

00:02:43,360 --> 00:02:47,680
these instances

00:02:44,640 --> 00:02:50,319
uh interact and synchronize by a network

00:02:47,680 --> 00:02:50,800
and then you you use the memory of both

00:02:50,319 --> 00:02:52,239
nodes

00:02:50,800 --> 00:02:53,920
and you're happy because now you have

00:02:52,239 --> 00:02:55,680
enough node which is more than the

00:02:53,920 --> 00:02:58,480
memory available in one node

00:02:55,680 --> 00:02:59,680
but what can happen is that as you can

00:02:58,480 --> 00:03:04,159
see on the left

00:02:59,680 --> 00:03:06,080
there is cpu3 but this cpu is jail these

00:03:04,159 --> 00:03:07,120
cpu resources are jailed because the

00:03:06,080 --> 00:03:09,360
memory

00:03:07,120 --> 00:03:10,159
is all completely used in that node so

00:03:09,360 --> 00:03:12,560
you cannot run

00:03:10,159 --> 00:03:15,040
any meaningful workload on that cpu so

00:03:12,560 --> 00:03:17,280
that cpu resources are simply wasted

00:03:15,040 --> 00:03:18,080
while on the right what you can do is

00:03:17,280 --> 00:03:21,280
that

00:03:18,080 --> 00:03:23,440
you can select how to compose

00:03:21,280 --> 00:03:24,879
the software-defined compute node in a

00:03:23,440 --> 00:03:26,159
way that for instance reduces

00:03:24,879 --> 00:03:28,159
fragmentation right

00:03:26,159 --> 00:03:29,840
so for instance in this case the result

00:03:28,159 --> 00:03:31,680
would be that

00:03:29,840 --> 00:03:33,599
you still have the two nodes and you're

00:03:31,680 --> 00:03:34,799
still using the resources on the two

00:03:33,599 --> 00:03:36,640
nodes

00:03:34,799 --> 00:03:38,560
but the sources that are left are not

00:03:36,640 --> 00:03:40,239
jailed because you have a full cpu

00:03:38,560 --> 00:03:41,599
and a meaningful amount of memory for

00:03:40,239 --> 00:03:45,360
you running

00:03:41,599 --> 00:03:49,040
further workloads in general

00:03:45,360 --> 00:03:51,680
let's say why why would you do it right

00:03:49,040 --> 00:03:52,640
today today we have seen many many

00:03:51,680 --> 00:03:55,840
presentations

00:03:52,640 --> 00:03:57,360
mentioning this this topic but one one

00:03:55,840 --> 00:04:00,400
main reason is that the way

00:03:57,360 --> 00:04:02,319
data centers are built today lead to low

00:04:00,400 --> 00:04:04,560
resource utilization and jailed

00:04:02,319 --> 00:04:06,480
resources in general this is because the

00:04:04,560 --> 00:04:08,239
way how memory and cpu resources are

00:04:06,480 --> 00:04:10,799
consumed have uh

00:04:08,239 --> 00:04:11,360
orders of magnitude gap in between so

00:04:10,799 --> 00:04:13,120
it's

00:04:11,360 --> 00:04:15,519
nearly impossible for you to have the

00:04:13,120 --> 00:04:16,079
perfect mix of machines in your data

00:04:15,519 --> 00:04:18,000
center

00:04:16,079 --> 00:04:19,519
so that you can serve any possible

00:04:18,000 --> 00:04:21,919
workload so you will end up

00:04:19,519 --> 00:04:23,680
like over provisioning and then you will

00:04:21,919 --> 00:04:26,560
have wasted resources

00:04:23,680 --> 00:04:28,479
and this will lead to a high tco and low

00:04:26,560 --> 00:04:30,160
energy efficiency of your system

00:04:28,479 --> 00:04:32,560
while with this aggregation what you can

00:04:30,160 --> 00:04:35,919
do essentially is that you can

00:04:32,560 --> 00:04:38,960
mix and match resources across nodes

00:04:35,919 --> 00:04:41,199
and you can create the perfect a

00:04:38,960 --> 00:04:42,800
software defined node that is tailored

00:04:41,199 --> 00:04:46,639
around your uh

00:04:42,800 --> 00:04:48,880
your application uh so what we have done

00:04:46,639 --> 00:04:50,000
in ibm research is that we have built

00:04:48,880 --> 00:04:52,880
themes flow

00:04:50,000 --> 00:04:54,240
that is a prototype framework completely

00:04:52,880 --> 00:04:57,520
open source

00:04:54,240 --> 00:05:00,000
that enables uh memory disaggregation

00:04:57,520 --> 00:05:00,639
using open capital event attachment and

00:05:00,000 --> 00:05:02,880
this this

00:05:00,639 --> 00:05:03,759
aggregation is completely software

00:05:02,880 --> 00:05:05,680
defined

00:05:03,759 --> 00:05:07,440
and the result is totally transparent

00:05:05,680 --> 00:05:09,360
from the software point of view so

00:05:07,440 --> 00:05:10,800
neither the os nor the applications

00:05:09,360 --> 00:05:14,320
running on the systems

00:05:10,800 --> 00:05:16,320
we know will notice that the memory is

00:05:14,320 --> 00:05:18,639
part of the memory is actually remote

00:05:16,320 --> 00:05:21,440
from from another machine

00:05:18,639 --> 00:05:22,000
so as i just said it's open source so

00:05:21,440 --> 00:05:24,479
you can

00:05:22,000 --> 00:05:25,280
you can download the full very load

00:05:24,479 --> 00:05:27,759
sources

00:05:25,280 --> 00:05:28,560
and also the full software stock it's

00:05:27,759 --> 00:05:30,320
there so

00:05:28,560 --> 00:05:32,080
download it give it a go send us your

00:05:30,320 --> 00:05:34,560
feedback

00:05:32,080 --> 00:05:35,840
okay so as i said i will just quickly go

00:05:34,560 --> 00:05:36,720
through the design just a couple of

00:05:35,840 --> 00:05:40,479
slides to

00:05:36,720 --> 00:05:41,280
get you started so this is a very high

00:05:40,479 --> 00:05:44,400
level view

00:05:41,280 --> 00:05:47,440
of our prototype so essentially

00:05:44,400 --> 00:05:49,440
what we are doing is in a nutshell is

00:05:47,440 --> 00:05:53,039
that we are bridging

00:05:49,440 --> 00:05:55,520
the open copy n1 mode on one machine

00:05:53,039 --> 00:05:56,160
with the open copy c1 mode on the other

00:05:55,520 --> 00:05:58,720
machine

00:05:56,160 --> 00:05:59,199
now the open copy in one mode also

00:05:58,720 --> 00:06:03,360
called

00:05:59,199 --> 00:06:06,319
lpc lowest point of coherence makes it

00:06:03,360 --> 00:06:09,600
that the and open copy accelerate an

00:06:06,319 --> 00:06:12,880
open copy device is materialized

00:06:09,600 --> 00:06:14,720
as just a windows a window in the

00:06:12,880 --> 00:06:16,240
physical address space of the machine

00:06:14,720 --> 00:06:18,160
and we use this mode

00:06:16,240 --> 00:06:20,240
on what we call the compute node so the

00:06:18,160 --> 00:06:22,880
node that is borrowing memory from

00:06:20,240 --> 00:06:23,840
from a remote machine so for this memory

00:06:22,880 --> 00:06:26,000
the open copy

00:06:23,840 --> 00:06:27,919
device will just be a window in the

00:06:26,000 --> 00:06:29,120
other space like if it was any other

00:06:27,919 --> 00:06:30,639
beam

00:06:29,120 --> 00:06:32,639
on the other side instead which is the

00:06:30,639 --> 00:06:35,520
compute node the memory node so the

00:06:32,639 --> 00:06:38,319
memory the node that is actually lending

00:06:35,520 --> 00:06:41,199
its memory to someone else will use the

00:06:38,319 --> 00:06:42,000
c1 or accelerator mode of open copy that

00:06:41,199 --> 00:06:43,919
allows

00:06:42,000 --> 00:06:45,199
an open copy device to master

00:06:43,919 --> 00:06:48,319
transactions on the bus

00:06:45,199 --> 00:06:50,160
in a coherent way what we do

00:06:48,319 --> 00:06:51,680
essentially what happens in in in all

00:06:50,160 --> 00:06:54,800
these infrastructure is that

00:06:51,680 --> 00:06:57,280
whenever on the compute side we each

00:06:54,800 --> 00:06:59,120
a memory operation is issued by the

00:06:57,280 --> 00:07:00,479
processor this is translated into an

00:06:59,120 --> 00:07:02,160
open copy transaction

00:07:00,479 --> 00:07:04,160
this transaction flows through our

00:07:02,160 --> 00:07:06,880
design and it is

00:07:04,160 --> 00:07:08,319
replayed at the destination node as an

00:07:06,880 --> 00:07:10,479
open capital transaction

00:07:08,319 --> 00:07:11,919
almost unchanged and seemed almost

00:07:10,479 --> 00:07:15,360
unchanged because

00:07:11,919 --> 00:07:18,160
we do just to transformations one is we

00:07:15,360 --> 00:07:19,680
we change the op codes because the c1

00:07:18,160 --> 00:07:21,039
and then one mode of open copy use

00:07:19,680 --> 00:07:24,160
different op codes

00:07:21,039 --> 00:07:25,360
for the similar operation and we also we

00:07:24,160 --> 00:07:27,520
patch

00:07:25,360 --> 00:07:30,319
the the address and i'll show you in the

00:07:27,520 --> 00:07:33,599
next slide why we do that

00:07:30,319 --> 00:07:36,800
so once you exit the cpu

00:07:33,599 --> 00:07:38,720
the the counterparts of our design are

00:07:36,800 --> 00:07:42,639
connected using a secret

00:07:38,720 --> 00:07:45,520
network and we use 100 gigabit service

00:07:42,639 --> 00:07:46,319
so any every every single team assist

00:07:45,520 --> 00:07:50,319
flow link

00:07:46,319 --> 00:07:52,960
between two machines uh avails of

00:07:50,319 --> 00:07:54,400
100 gigabits per second bandwidth and

00:07:52,960 --> 00:07:56,840
then we have an

00:07:54,400 --> 00:07:59,360
out-of-bound control plane that

00:07:56,840 --> 00:08:02,160
configures all the actors in this

00:07:59,360 --> 00:08:03,840
in this picture so configures the our

00:08:02,160 --> 00:08:05,599
custom logic at both the computer and

00:08:03,840 --> 00:08:08,000
the memory side and also if you have a

00:08:05,599 --> 00:08:11,280
secret network it would even configure

00:08:08,000 --> 00:08:13,280
the switch so our design is

00:08:11,280 --> 00:08:16,319
right right now implemented on the alpha

00:08:13,280 --> 00:08:17,280
data 9v3 fpga i will talk about this a

00:08:16,319 --> 00:08:20,160
bit later

00:08:17,280 --> 00:08:21,759
in the presentation i have mentioned

00:08:20,160 --> 00:08:25,120
that we

00:08:21,759 --> 00:08:27,440
patch the address so why we do that

00:08:25,120 --> 00:08:28,720
because as i was saying on the on the

00:08:27,440 --> 00:08:31,759
compute node

00:08:28,720 --> 00:08:32,800
the the accelerator is using this m1

00:08:31,759 --> 00:08:35,120
mode

00:08:32,800 --> 00:08:36,479
meaning that the opengl device uses the

00:08:35,120 --> 00:08:37,200
one mode meaning that we see the

00:08:36,479 --> 00:08:40,719
opencopy

00:08:37,200 --> 00:08:43,440
device as a window in the physical

00:08:40,719 --> 00:08:43,839
address space of the compute node also

00:08:43,440 --> 00:08:46,320
called

00:08:43,839 --> 00:08:48,000
the real address space so every time the

00:08:46,320 --> 00:08:49,600
compute node issues an open copy

00:08:48,000 --> 00:08:50,320
transaction towards this aggregated

00:08:49,600 --> 00:08:53,839
memory

00:08:50,320 --> 00:08:54,720
this will be using a physical address

00:08:53,839 --> 00:08:57,120
however

00:08:54,720 --> 00:08:59,120
on the on the memory side we are not

00:08:57,120 --> 00:09:02,240
actually

00:08:59,120 --> 00:09:04,160
lending physical memory to someone but

00:09:02,240 --> 00:09:05,680
rather there is a user space process

00:09:04,160 --> 00:09:07,600
that is allocating

00:09:05,680 --> 00:09:10,080
some memory so in visual memory doing a

00:09:07,600 --> 00:09:11,920
malloc allocating some memory and makes

00:09:10,080 --> 00:09:14,800
it available

00:09:11,920 --> 00:09:16,560
for a remote machine to access via our

00:09:14,800 --> 00:09:19,279
open copy accelerator

00:09:16,560 --> 00:09:21,360
so essentially at configuration time

00:09:19,279 --> 00:09:24,640
using the out-of-band control plane

00:09:21,360 --> 00:09:27,760
we configure a table

00:09:24,640 --> 00:09:31,440
in our design where we have one

00:09:27,760 --> 00:09:33,920
offset for each team assist flow

00:09:31,440 --> 00:09:36,000
memory section and attempts this flow

00:09:33,920 --> 00:09:38,640
memory section is one terabyte

00:09:36,000 --> 00:09:39,600
big meaning that every a physical

00:09:38,640 --> 00:09:41,279
address

00:09:39,600 --> 00:09:43,120
every transaction that has a physical

00:09:41,279 --> 00:09:44,880
address belonging to the same one

00:09:43,120 --> 00:09:46,560
terabyte section will

00:09:44,880 --> 00:09:47,920
receive the same conversion the

00:09:46,560 --> 00:09:50,240
conversion is simply we take the

00:09:47,920 --> 00:09:52,399
effective address coming from the power

00:09:50,240 --> 00:09:54,399
from the compute node and we are the

00:09:52,399 --> 00:09:56,320
offset sorry the real address we are the

00:09:54,399 --> 00:09:57,360
offset and this translates into the

00:09:56,320 --> 00:09:59,120
effective address

00:09:57,360 --> 00:10:00,640
at the destination side so once the

00:09:59,120 --> 00:10:03,279
translation is done

00:10:00,640 --> 00:10:04,000
the new address of the open targeted by

00:10:03,279 --> 00:10:05,839
the open copy

00:10:04,000 --> 00:10:08,800
transaction is actually a valid vital

00:10:05,839 --> 00:10:11,680
address at the destination

00:10:08,800 --> 00:10:13,120
so this is how far we go with the design

00:10:11,680 --> 00:10:14,720
of the hardware because i want to

00:10:13,120 --> 00:10:17,760
talk about the software but these were

00:10:14,720 --> 00:10:19,760
some concepts that are needed for

00:10:17,760 --> 00:10:22,399
for everybody to understand what we what

00:10:19,760 --> 00:10:25,519
we'll be doing next

00:10:22,399 --> 00:10:29,279
so we have built a full

00:10:25,519 --> 00:10:31,200
uh software stack on top of libo cxl

00:10:29,279 --> 00:10:33,360
that is the official open copy library

00:10:31,200 --> 00:10:35,279
to implement user space drivers

00:10:33,360 --> 00:10:38,000
so the first layer we have implemented

00:10:35,279 --> 00:10:41,440
is called lib team assist flow

00:10:38,000 --> 00:10:42,880
and this team assist flow exports a set

00:10:41,440 --> 00:10:46,000
of helpers

00:10:42,880 --> 00:10:47,680
that allow attachment and detachment of

00:10:46,000 --> 00:10:49,519
disaggregated memory from both the

00:10:47,680 --> 00:10:52,399
computer memory node

00:10:49,519 --> 00:10:53,680
and also give access to some hardware

00:10:52,399 --> 00:10:55,920
counters that we have

00:10:53,680 --> 00:10:59,839
in our design these counters can be used

00:10:55,920 --> 00:11:01,760
for monitoring latency or throughput of

00:10:59,839 --> 00:11:03,680
data going through through the design

00:11:01,760 --> 00:11:07,200
and in the network between the agents

00:11:03,680 --> 00:11:10,800
between the tms's flow cards

00:11:07,200 --> 00:11:11,360
and on top of that the last layer of our

00:11:10,800 --> 00:11:13,519
stock

00:11:11,360 --> 00:11:15,519
is this team assist flow agent so the

00:11:13,519 --> 00:11:18,079
team assist flow agent

00:11:15,519 --> 00:11:18,720
is a process that we envision that is

00:11:18,079 --> 00:11:21,760
running

00:11:18,720 --> 00:11:22,320
every machine that uses steam assist

00:11:21,760 --> 00:11:23,839
flow

00:11:22,320 --> 00:11:25,600
and it is essentially the point of

00:11:23,839 --> 00:11:28,320
interaction between either the

00:11:25,600 --> 00:11:30,560
orchestrator

00:11:28,320 --> 00:11:32,079
flow or the user if the user wants to

00:11:30,560 --> 00:11:32,880
manually configure the system and the

00:11:32,079 --> 00:11:34,399
images flow

00:11:32,880 --> 00:11:36,079
so team message flow hardware here in

00:11:34,399 --> 00:11:38,240
the picture is called the tvs4

00:11:36,079 --> 00:11:40,480
acceleration function unit

00:11:38,240 --> 00:11:42,560
so let's let's see what is the complete

00:11:40,480 --> 00:11:46,000
flow for memory to be attached

00:11:42,560 --> 00:11:48,000
from the surface software perspective so

00:11:46,000 --> 00:11:50,079
it all begins from the memory node so

00:11:48,000 --> 00:11:52,320
the orchestrator of the user or the user

00:11:50,079 --> 00:11:53,279
asks the team assist flow agent attach

00:11:52,320 --> 00:11:56,720
100 gigabyte

00:11:53,279 --> 00:11:59,600
memory this results in the agent

00:11:56,720 --> 00:12:01,440
allocating these 100 gigabytes of memory

00:11:59,600 --> 00:12:04,720
and making them accessible

00:12:01,440 --> 00:12:06,399
via the team assist flow design so our

00:12:04,720 --> 00:12:09,360
custom design

00:12:06,399 --> 00:12:11,279
how is this and sorry when this is done

00:12:09,360 --> 00:12:13,040
the agent returns the effective address

00:12:11,279 --> 00:12:16,399
that we will use later on

00:12:13,040 --> 00:12:18,399
so what happens if we have a look at the

00:12:16,399 --> 00:12:20,240
memory map so on the right you see the

00:12:18,399 --> 00:12:22,320
effective address space of the phoenix's

00:12:20,240 --> 00:12:25,839
flow agent so the visual memory map

00:12:22,320 --> 00:12:29,200
whenever we ask the agent please lend

00:12:25,839 --> 00:12:30,959
100 gigabyte to another machine

00:12:29,200 --> 00:12:33,040
the result of this operation is the

00:12:30,959 --> 00:12:35,200
agent allocating

00:12:33,040 --> 00:12:37,279
this much memory so the memory that is

00:12:35,200 --> 00:12:39,839
requested this allocation is uh

00:12:37,279 --> 00:12:40,800
just as simple as a malloc and this

00:12:39,839 --> 00:12:43,279
memory is

00:12:40,800 --> 00:12:44,800
for the time being pinned to avoid page

00:12:43,279 --> 00:12:45,920
faults even though some page faults are

00:12:44,800 --> 00:12:48,959
still there because of

00:12:45,920 --> 00:12:51,519
a huge space transparent huge basis

00:12:48,959 --> 00:12:52,160
and then the the once the memories are

00:12:51,519 --> 00:12:57,360
located

00:12:52,160 --> 00:12:59,440
the the the teammates flow agent

00:12:57,360 --> 00:13:01,120
using lead team as is flow and libos

00:12:59,440 --> 00:13:04,160
excel will share the

00:13:01,120 --> 00:13:06,720
its own process address uh a process

00:13:04,160 --> 00:13:08,240
address identifier process process of

00:13:06,720 --> 00:13:11,360
the space identified

00:13:08,240 --> 00:13:14,320
with the team as flow accelerator

00:13:11,360 --> 00:13:14,880
once the accelerator in c1 mode knows

00:13:14,320 --> 00:13:18,399
the

00:13:14,880 --> 00:13:19,839
psid of the process it will be able of

00:13:18,399 --> 00:13:22,320
mastering

00:13:19,839 --> 00:13:23,760
memory transactions towards the just

00:13:22,320 --> 00:13:25,600
allocated buffer without the

00:13:23,760 --> 00:13:27,680
intervention of the cpu in a custom unit

00:13:25,600 --> 00:13:29,920
way

00:13:27,680 --> 00:13:31,440
then this the vital address the base

00:13:29,920 --> 00:13:33,519
visual address of these buffers

00:13:31,440 --> 00:13:35,120
is is sent back to the user of the

00:13:33,519 --> 00:13:37,839
orchestrator so now the

00:13:35,120 --> 00:13:38,399
compute the memory node is configured

00:13:37,839 --> 00:13:41,360
and we go

00:13:38,399 --> 00:13:43,120
configuring the compute node so again

00:13:41,360 --> 00:13:43,920
the orchestrator of the user interact

00:13:43,120 --> 00:13:47,600
with the

00:13:43,920 --> 00:13:49,920
team assist flow agent asking attach 100

00:13:47,600 --> 00:13:51,760
gigabyte of memory to this machine

00:13:49,920 --> 00:13:53,360
and this is the effective address at the

00:13:51,760 --> 00:13:55,120
remote endpoint

00:13:53,360 --> 00:13:56,480
so now the team is flow engine the first

00:13:55,120 --> 00:13:58,639
thing that is done it

00:13:56,480 --> 00:14:00,639
it will configure the hardware and

00:13:58,639 --> 00:14:02,959
essentially it will establish the link

00:14:00,639 --> 00:14:04,320
with the remote counterpart so after

00:14:02,959 --> 00:14:06,880
this is done

00:14:04,320 --> 00:14:07,680
the memory is physically connected to

00:14:06,880 --> 00:14:09,680
the machine

00:14:07,680 --> 00:14:11,199
but not yet visible to any application

00:14:09,680 --> 00:14:13,600
or operating system

00:14:11,199 --> 00:14:14,480
so the next step is then to actually hot

00:14:13,600 --> 00:14:16,560
plug

00:14:14,480 --> 00:14:18,720
this memory to the running operating

00:14:16,560 --> 00:14:20,639
system that is linux

00:14:18,720 --> 00:14:23,360
and then what the result is that they

00:14:20,639 --> 00:14:25,519
did after the hot plug linus will see

00:14:23,360 --> 00:14:29,360
some more extra memory like if you go to

00:14:25,519 --> 00:14:29,360
the machine and hot plug some new things

00:14:29,440 --> 00:14:35,440
again if you have a look at the real

00:14:32,639 --> 00:14:36,160
address so the physical address uh

00:14:35,440 --> 00:14:38,639
memory map

00:14:36,160 --> 00:14:39,600
of the physical memory map of the

00:14:38,639 --> 00:14:42,160
compute nodes

00:14:39,600 --> 00:14:44,000
the the compute node normally you would

00:14:42,160 --> 00:14:47,519
see on the machines that we use the

00:14:44,000 --> 00:14:49,120
ac 922 our nine servers you would see

00:14:47,519 --> 00:14:52,000
two numa nodes

00:14:49,120 --> 00:14:53,519
each numa node has of course a local cpu

00:14:52,000 --> 00:14:56,560
and some local memory

00:14:53,519 --> 00:14:58,079
so whenever we attach some disaggregated

00:14:56,560 --> 00:15:01,040
memory that is being

00:14:58,079 --> 00:15:02,320
borrowed by another node the result is

00:15:01,040 --> 00:15:05,680
that this memory is

00:15:02,320 --> 00:15:08,720
hot plugged into a new pneuma node

00:15:05,680 --> 00:15:10,160
that is a cpu less number and memory

00:15:08,720 --> 00:15:11,760
less essentially when we boot the

00:15:10,160 --> 00:15:13,440
machine

00:15:11,760 --> 00:15:15,680
we have slightly modified the firmware

00:15:13,440 --> 00:15:17,760
so that it creates this uh

00:15:15,680 --> 00:15:18,880
memory less and cpu less pneuma node

00:15:17,760 --> 00:15:21,760
that we

00:15:18,880 --> 00:15:22,800
for for let's say for later usage so

00:15:21,760 --> 00:15:26,560
essentially

00:15:22,800 --> 00:15:28,560
the the the agent will configure

00:15:26,560 --> 00:15:30,320
tms's flow with the effective address

00:15:28,560 --> 00:15:30,880
received so it will compute the offset

00:15:30,320 --> 00:15:33,440
and we

00:15:30,880 --> 00:15:34,480
will write it in the table for patching

00:15:33,440 --> 00:15:38,240
the

00:15:34,480 --> 00:15:40,079
the addresses and then we hot plug

00:15:38,240 --> 00:15:42,160
those physical addresses that are

00:15:40,079 --> 00:15:45,920
assigned to the open copy device

00:15:42,160 --> 00:15:49,199
as memory to linux this hot plugging

00:15:45,920 --> 00:15:51,360
uses the uh let's say default hot plug

00:15:49,199 --> 00:15:53,360
functionality that is available in linux

00:15:51,360 --> 00:15:54,399
we haven't modified that it's a plain

00:15:53,360 --> 00:15:55,680
hot plug

00:15:54,399 --> 00:15:58,240
and the only thing that we do is that we

00:15:55,680 --> 00:16:00,079
hot plug it as zone movable

00:15:58,240 --> 00:16:01,839
which means that the kernel cannot

00:16:00,079 --> 00:16:02,959
cannot allocate permanent data

00:16:01,839 --> 00:16:04,800
structures in it

00:16:02,959 --> 00:16:06,560
because this would prevent us hot

00:16:04,800 --> 00:16:08,000
unplugging so we could not essentially

00:16:06,560 --> 00:16:10,480
if the kernel allocates something

00:16:08,000 --> 00:16:11,360
that we cannot detach that is segregated

00:16:10,480 --> 00:16:12,800
memory anymore

00:16:11,360 --> 00:16:14,480
while we want to keep this dynamic

00:16:12,800 --> 00:16:17,279
behavior uh at

00:16:14,480 --> 00:16:18,720
all stages and one one thing that might

00:16:17,279 --> 00:16:22,000
be interesting to know is that

00:16:18,720 --> 00:16:24,399
the base address of this uh hardware

00:16:22,000 --> 00:16:25,600
of this physical memory physical area

00:16:24,399 --> 00:16:28,079
that we hot plug

00:16:25,600 --> 00:16:29,120
is actually decided at good time by the

00:16:28,079 --> 00:16:31,920
ski boot firmware

00:16:29,120 --> 00:16:33,040
it's essentially the the window of

00:16:31,920 --> 00:16:35,279
addresses that is

00:16:33,040 --> 00:16:36,639
associated to the lpc device for the

00:16:35,279 --> 00:16:38,720
lowest point of periods

00:16:36,639 --> 00:16:40,639
so to the open copy device they they

00:16:38,720 --> 00:16:43,839
associate four terabytes of uh

00:16:40,639 --> 00:16:45,839
of physical memory map

00:16:43,839 --> 00:16:48,959
meaning that each since we use one

00:16:45,839 --> 00:16:52,320
terabyte sections each open copy device

00:16:48,959 --> 00:16:54,320
with images flow could handle a maximum

00:16:52,320 --> 00:16:57,199
of four section

00:16:54,320 --> 00:16:58,959
at the time and once this is done so

00:16:57,199 --> 00:17:00,000
once the odd plug is successful this

00:16:58,959 --> 00:17:03,360
memory

00:17:00,000 --> 00:17:04,000
is accessible from any application in

00:17:03,360 --> 00:17:07,520
the operating

00:17:04,000 --> 00:17:09,919
system automatically without any

00:17:07,520 --> 00:17:11,760
further intervention so essentially what

00:17:09,919 --> 00:17:13,520
we do is that we extend the beat

00:17:11,760 --> 00:17:15,120
the kind of extend a bit the memory

00:17:13,520 --> 00:17:15,839
model of linux so normally you would

00:17:15,120 --> 00:17:18,319
have

00:17:15,839 --> 00:17:20,240
sparse man that sees memory divided in

00:17:18,319 --> 00:17:23,280
memory sections and then except

00:17:20,240 --> 00:17:24,400
each section is composed of page frames

00:17:23,280 --> 00:17:27,280
and then these page frames would

00:17:24,400 --> 00:17:28,960
normally be mapped to physical addresses

00:17:27,280 --> 00:17:31,039
that correspond to where you have your

00:17:28,960 --> 00:17:32,320
memory map your dims your memory

00:17:31,039 --> 00:17:35,120
controller

00:17:32,320 --> 00:17:35,600
in this case instead the page frames are

00:17:35,120 --> 00:17:39,039
mapped

00:17:35,600 --> 00:17:41,280
to that that

00:17:39,039 --> 00:17:43,840
window of addresses that is associated

00:17:41,280 --> 00:17:47,280
to the open copy device

00:17:43,840 --> 00:17:49,200
so we create then

00:17:47,280 --> 00:17:51,200
let's say we divide these page frames in

00:17:49,200 --> 00:17:54,480
one terabyte sections

00:17:51,200 --> 00:17:56,799
and each section is linearly uh mapped

00:17:54,480 --> 00:17:58,400
to a remote destination meaning that one

00:17:56,799 --> 00:18:00,799
section can only be served

00:17:58,400 --> 00:18:01,600
by a remote machine but one remote

00:18:00,799 --> 00:18:03,600
machine could be

00:18:01,600 --> 00:18:05,919
potentially serving multiple dimensions

00:18:03,600 --> 00:18:08,559
flow memory sections so

00:18:05,919 --> 00:18:08,960
the fact that the the memory attachment

00:18:08,559 --> 00:18:11,679
is

00:18:08,960 --> 00:18:13,600
completely transparent helped us a lot

00:18:11,679 --> 00:18:14,720
in running evaluation and software

00:18:13,600 --> 00:18:17,120
because

00:18:14,720 --> 00:18:19,120
technically you actually you don't need

00:18:17,120 --> 00:18:20,320
to not even recompile your software

00:18:19,120 --> 00:18:21,919
right you just run it

00:18:20,320 --> 00:18:23,520
and you benefit from from this

00:18:21,919 --> 00:18:26,640
aggregated memory so

00:18:23,520 --> 00:18:28,720
now i will show you

00:18:26,640 --> 00:18:30,880
quickly the evaluation some applications

00:18:28,720 --> 00:18:32,960
that we have executed on our prototype

00:18:30,880 --> 00:18:34,640
and all these results that you see here

00:18:32,960 --> 00:18:36,960
are going to appear at micro

00:18:34,640 --> 00:18:38,480
2020 so if you want a deeper explanation

00:18:36,960 --> 00:18:40,400
of it just join us

00:18:38,480 --> 00:18:42,320
at micro in a couple of weeks in one

00:18:40,400 --> 00:18:44,640
month from now

00:18:42,320 --> 00:18:47,360
so we have used the three cloud

00:18:44,640 --> 00:18:50,080
workloads in memory database

00:18:47,360 --> 00:18:51,520
caching and elasticsearch and also we

00:18:50,080 --> 00:18:53,520
run stream

00:18:51,520 --> 00:18:56,000
to see what is the sustained boundary

00:18:53,520 --> 00:19:00,080
the maximum boundary that we can measure

00:18:56,000 --> 00:19:00,080
and we we

00:19:00,240 --> 00:19:04,080
compare two configurations that are one

00:19:02,320 --> 00:19:04,960
is the local versus aggregated meaning

00:19:04,080 --> 00:19:07,120
that

00:19:04,960 --> 00:19:08,480
an application is running on a node it

00:19:07,120 --> 00:19:10,160
either use

00:19:08,480 --> 00:19:11,760
it either uses a local memory or

00:19:10,160 --> 00:19:14,400
disaggregated memory

00:19:11,760 --> 00:19:15,440
and the scale out versus scale up so

00:19:14,400 --> 00:19:17,120
scale out means that

00:19:15,440 --> 00:19:18,960
i need more memory than what is

00:19:17,120 --> 00:19:20,240
available in the node and i get it by

00:19:18,960 --> 00:19:22,240
running a second instance of the

00:19:20,240 --> 00:19:25,280
application on another node

00:19:22,240 --> 00:19:26,960
or scale up which is i still need more

00:19:25,280 --> 00:19:28,880
memory than what is available to the

00:19:26,960 --> 00:19:30,799
node but instead of using the cpu of the

00:19:28,880 --> 00:19:32,480
other node i just borrowed the memory

00:19:30,799 --> 00:19:34,559
using team assist flow this means that

00:19:32,480 --> 00:19:36,400
scale up uses half

00:19:34,559 --> 00:19:38,559
of the course of scale out so this is to

00:19:36,400 --> 00:19:41,039
be taken into consideration

00:19:38,559 --> 00:19:43,120
into account let's start with the

00:19:41,039 --> 00:19:46,880
experimental setup as said we use

00:19:43,120 --> 00:19:49,280
two ac 922 servers

00:19:46,880 --> 00:19:50,960
our design is implemented on the alpha

00:19:49,280 --> 00:19:54,000
data 9v3 cards

00:19:50,960 --> 00:19:58,320
that are open copies open copy enabled

00:19:54,000 --> 00:20:00,880
alter scale plus fpgas and the network

00:19:58,320 --> 00:20:02,559
that is uh a secret network in the

00:20:00,880 --> 00:20:03,919
initial picture in this case is a

00:20:02,559 --> 00:20:05,919
point-to-point connection between the

00:20:03,919 --> 00:20:06,559
cards and we have 100 gigabits per

00:20:05,919 --> 00:20:08,880
second

00:20:06,559 --> 00:20:10,240
between the two machines for memory for

00:20:08,880 --> 00:20:12,559
disaggregate memory only right this is

00:20:10,240 --> 00:20:15,360
not networking traffic

00:20:12,559 --> 00:20:16,960
so stream let's first see what is the

00:20:15,360 --> 00:20:17,919
maximum bandwidth that an application

00:20:16,960 --> 00:20:20,080
can perceive

00:20:17,919 --> 00:20:22,320
when using this aggregated memory and

00:20:20,080 --> 00:20:24,640
with with with surprise

00:20:22,320 --> 00:20:26,640
we see that we can actually reach the

00:20:24,640 --> 00:20:27,280
maximum bandwidth for team assist flow

00:20:26,640 --> 00:20:29,440
which is

00:20:27,280 --> 00:20:31,280
12.5 gigabytes per second the 100

00:20:29,440 --> 00:20:33,600
megabits per second

00:20:31,280 --> 00:20:35,200
and this test was using a stream array

00:20:33,600 --> 00:20:38,320
big enough for

00:20:35,200 --> 00:20:39,919
for being bigger than any cache that we

00:20:38,320 --> 00:20:43,679
have in the system

00:20:39,919 --> 00:20:45,600
and i i also have this dotted line on

00:20:43,679 --> 00:20:47,600
the top that is just

00:20:45,600 --> 00:20:51,039
to remember you that the local memory on

00:20:47,600 --> 00:20:53,039
a p9 has a much higher memory bandwidth

00:20:51,039 --> 00:20:54,720
just to remember that there is at least

00:20:53,039 --> 00:20:55,679
one order of magnitude difference

00:20:54,720 --> 00:20:58,720
between

00:20:55,679 --> 00:21:01,600
disaggregated and local memory and one

00:20:58,720 --> 00:21:03,360
one other metric that i haven't i don't

00:21:01,600 --> 00:21:05,360
have a chat for it but i can tell you is

00:21:03,360 --> 00:21:08,000
that the latency round trick latency

00:21:05,360 --> 00:21:08,960
for a memory access is of 900

00:21:08,000 --> 00:21:11,919
nanoseconds

00:21:08,960 --> 00:21:13,760
so will this will this is this

00:21:11,919 --> 00:21:14,960
performance good enough for running any

00:21:13,760 --> 00:21:16,240
meaningful workload

00:21:14,960 --> 00:21:18,159
this is the question that we asked

00:21:16,240 --> 00:21:20,880
ourselves and we

00:21:18,159 --> 00:21:22,880
we have a kind of an answer with a few

00:21:20,880 --> 00:21:26,480
benchmarks so first benchmark

00:21:22,880 --> 00:21:26,880
a votive in memory database uh we run

00:21:26,480 --> 00:21:29,840
volt

00:21:26,880 --> 00:21:31,360
b so you see the three configuration

00:21:29,840 --> 00:21:33,679
local memory scale out and single

00:21:31,360 --> 00:21:36,480
disaggregated is team assist flow

00:21:33,679 --> 00:21:38,559
and i'm here i'm showing the throughput

00:21:36,480 --> 00:21:39,280
of these benchmarks the clients for what

00:21:38,559 --> 00:21:42,559
to be

00:21:39,280 --> 00:21:44,240
uh those uh available in the white csb

00:21:42,559 --> 00:21:47,360
the yahoo cloud service

00:21:44,240 --> 00:21:48,960
benchmark suite the first thing that we

00:21:47,360 --> 00:21:51,679
see that we have measured that

00:21:48,960 --> 00:21:52,960
at most we see a nine percent throughput

00:21:51,679 --> 00:21:56,159
degradation

00:21:52,960 --> 00:21:57,120
when using this aggregated memory even

00:21:56,159 --> 00:22:01,039
across the other

00:21:57,120 --> 00:22:03,650
benchmarks that are not shown here

00:22:01,039 --> 00:22:04,960
it's uh it's uh

00:22:03,650 --> 00:22:08,000
[Music]

00:22:04,960 --> 00:22:09,120
it's it's maximum nine percent and also

00:22:08,000 --> 00:22:11,919
another thing that we have

00:22:09,120 --> 00:22:13,760
noticed is that the scale out as very

00:22:11,919 --> 00:22:14,480
similar performance to this aggregated

00:22:13,760 --> 00:22:17,039
memory

00:22:14,480 --> 00:22:18,240
but it is using twice of the course so

00:22:17,039 --> 00:22:20,400
this meaning that

00:22:18,240 --> 00:22:22,400
this means that when you are in scale

00:22:20,400 --> 00:22:24,080
out there is some remote coordination

00:22:22,400 --> 00:22:28,240
for queries to happen right if you have

00:22:24,080 --> 00:22:30,080
a join across partitions of the database

00:22:28,240 --> 00:22:31,840
that this this remote coordination will

00:22:30,080 --> 00:22:33,840
add we'll add an overhead

00:22:31,840 --> 00:22:34,880
that is comparable to what you would you

00:22:33,840 --> 00:22:38,799
would get with

00:22:34,880 --> 00:22:40,240
with remote memory and and therefore

00:22:38,799 --> 00:22:41,679
this means that with team assist flow

00:22:40,240 --> 00:22:42,480
you can get the similar similar

00:22:41,679 --> 00:22:44,159
performance

00:22:42,480 --> 00:22:45,840
by just using half of the course and

00:22:44,159 --> 00:22:47,679
consider this is a prototype system

00:22:45,840 --> 00:22:48,720
right this is not not even close to what

00:22:47,679 --> 00:22:52,240
you will get with

00:22:48,720 --> 00:22:54,000
memory inception on v10

00:22:52,240 --> 00:22:57,600
the second benchmark we have is

00:22:54,000 --> 00:23:01,039
memcached so here you see the cdf of the

00:22:57,600 --> 00:23:04,640
get transactions latency

00:23:01,039 --> 00:23:08,080
and again 90 of the transactions

00:23:04,640 --> 00:23:11,039
only have seven percent delay

00:23:08,080 --> 00:23:12,640
and even more interesting is that scale

00:23:11,039 --> 00:23:16,640
out is actually

00:23:12,640 --> 00:23:18,720
slower than than the disaggregated case

00:23:16,640 --> 00:23:19,840
and this is because when you use this

00:23:18,720 --> 00:23:21,679
when you use mentally

00:23:19,840 --> 00:23:24,240
in disaggregate in uh let's say

00:23:21,679 --> 00:23:26,960
replicated uh

00:23:24,240 --> 00:23:28,480
deployment you need to have a proxy in

00:23:26,960 --> 00:23:30,320
front of all the replicas

00:23:28,480 --> 00:23:32,960
because the proxy then will decide which

00:23:30,320 --> 00:23:35,760
server is serving the specific request

00:23:32,960 --> 00:23:37,840
while with the memory disaggregation

00:23:35,760 --> 00:23:38,559
actually can expand in memory so we can

00:23:37,840 --> 00:23:42,000
have a

00:23:38,559 --> 00:23:44,400
bigger key value store so much

00:23:42,000 --> 00:23:46,080
much more data that we can host but we

00:23:44,400 --> 00:23:48,840
don't need to have a proxy in between

00:23:46,080 --> 00:23:51,200
so we don't need the the delay of a

00:23:48,840 --> 00:23:54,320
proxy

00:23:51,200 --> 00:23:56,080
so so far so good uh

00:23:54,320 --> 00:23:57,679
but it's not good for every application

00:23:56,080 --> 00:23:59,520
right so we have seen for instance that

00:23:57,679 --> 00:24:02,320
for elastic search

00:23:59,520 --> 00:24:02,880
is not is not as good and mostly because

00:24:02,320 --> 00:24:05,120
of the

00:24:02,880 --> 00:24:06,000
way how elastic search works which is a

00:24:05,120 --> 00:24:08,640
non-sql

00:24:06,000 --> 00:24:09,679
kind of let's call it database it's not

00:24:08,640 --> 00:24:12,640
database

00:24:09,679 --> 00:24:13,039
uh where let's say when you do a query

00:24:12,640 --> 00:24:16,240
you

00:24:13,039 --> 00:24:16,960
often go through most of the data rather

00:24:16,240 --> 00:24:19,600
than doing

00:24:16,960 --> 00:24:21,039
doing complex joints between distributed

00:24:19,600 --> 00:24:24,799
partitions this means that

00:24:21,039 --> 00:24:26,880
if you have more cores available

00:24:24,799 --> 00:24:28,400
you have more horsepower for going

00:24:26,880 --> 00:24:30,400
through the records right

00:24:28,400 --> 00:24:32,080
and in fact it's visible that the scale

00:24:30,400 --> 00:24:34,960
out is always performing

00:24:32,080 --> 00:24:36,559
better than the disaggregated place at

00:24:34,960 --> 00:24:40,559
least of 45 percent

00:24:36,559 --> 00:24:43,440
we have measured and

00:24:40,559 --> 00:24:45,360
the the other interesting thing is that

00:24:43,440 --> 00:24:47,200
for some cases the scale out is even

00:24:45,360 --> 00:24:50,080
faster than the local memory case

00:24:47,200 --> 00:24:51,279
because for instance in the rtq in the

00:24:50,080 --> 00:24:54,400
rtq

00:24:51,279 --> 00:24:55,919
challenge as they are called they they

00:24:54,400 --> 00:24:58,880
search all the answers

00:24:55,919 --> 00:25:00,000
in stock overflow that feature a random

00:24:58,880 --> 00:25:02,400
generated tag so this

00:25:00,000 --> 00:25:03,120
means that you are likely going through

00:25:02,400 --> 00:25:06,400
most

00:25:03,120 --> 00:25:08,480
of the answers in in in the database so

00:25:06,400 --> 00:25:11,840
the more course you have the more

00:25:08,480 --> 00:25:13,360
data you can check in in parallel so for

00:25:11,840 --> 00:25:15,440
elastic session

00:25:13,360 --> 00:25:16,960
we have seen that memory disaggregation

00:25:15,440 --> 00:25:20,000
is not really offering

00:25:16,960 --> 00:25:21,039
this is not as appealing as in the other

00:25:20,000 --> 00:25:23,520
cases

00:25:21,039 --> 00:25:24,159
but for some use cases it might still be

00:25:23,520 --> 00:25:26,559
uh

00:25:24,159 --> 00:25:27,279
it might still be let's say appealing in

00:25:26,559 --> 00:25:29,200
the sense that

00:25:27,279 --> 00:25:30,720
depends on what kind of trade-offs you

00:25:29,200 --> 00:25:31,440
are looking for right you might be

00:25:30,720 --> 00:25:34,799
looking for

00:25:31,440 --> 00:25:36,320
less for cheaper deployment

00:25:34,799 --> 00:25:38,320
and you can live with the lower

00:25:36,320 --> 00:25:41,360
performance

00:25:38,320 --> 00:25:42,159
so now i'll just go to a quick demo to

00:25:41,360 --> 00:25:45,200
show you

00:25:42,159 --> 00:25:48,960
this is actually uh this is actually

00:25:45,200 --> 00:25:51,039
working so uh we have built an

00:25:48,960 --> 00:25:54,320
orchestration framework

00:25:51,039 --> 00:25:57,360
the which is a web interface

00:25:54,320 --> 00:25:59,520
through which we can uh let's say model

00:25:57,360 --> 00:26:00,640
the the network the disaggregation

00:25:59,520 --> 00:26:03,520
fabric between

00:26:00,640 --> 00:26:05,360
machines that that uh that participate

00:26:03,520 --> 00:26:07,200
with this disaggregated system

00:26:05,360 --> 00:26:09,200
in this case you see four machines each

00:26:07,200 --> 00:26:10,400
machine has two ports because each fpga

00:26:09,200 --> 00:26:12,000
has two ports

00:26:10,400 --> 00:26:13,919
you see four machines but for the time

00:26:12,000 --> 00:26:16,640
being only two of these machines

00:26:13,919 --> 00:26:17,919
puerto ri and capo basso are connected

00:26:16,640 --> 00:26:19,679
to each other the others are

00:26:17,919 --> 00:26:22,080
not connected at the moment we're doing

00:26:19,679 --> 00:26:24,640
some other experiments

00:26:22,080 --> 00:26:26,320
these essentially what you can do is

00:26:24,640 --> 00:26:27,840
that you can select a node

00:26:26,320 --> 00:26:30,240
and you see what is available on the

00:26:27,840 --> 00:26:33,840
node via this orchestrator you can ask

00:26:30,240 --> 00:26:35,279
attach 100 more 100 gigabit more to this

00:26:33,840 --> 00:26:36,880
uh

00:26:35,279 --> 00:26:39,360
to this uh to this machine the

00:26:36,880 --> 00:26:43,760
orchestrator will find the perfect body

00:26:39,360 --> 00:26:46,640
machine that can borrow you this uh

00:26:43,760 --> 00:26:47,120
sorry that can lend you this this memory

00:26:46,640 --> 00:26:49,120
for uh

00:26:47,120 --> 00:26:52,000
at the moment as i said it's it's can

00:26:49,120 --> 00:26:55,360
only be this machine capable now

00:26:52,000 --> 00:26:56,240
if we make it a bit more nerdy i can

00:26:55,360 --> 00:26:57,600
show you so

00:26:56,240 --> 00:26:59,360
these are determined the two machine

00:26:57,600 --> 00:27:02,520
compobas so it's going to be the memory

00:26:59,360 --> 00:27:06,480
donald so as you see the machine has

00:27:02,520 --> 00:27:07,919
573 gigabytes of ram available and

00:27:06,480 --> 00:27:09,760
almost none is used

00:27:07,919 --> 00:27:11,679
and this one on the left is instead that

00:27:09,760 --> 00:27:14,000
they compute the compute node

00:27:11,679 --> 00:27:15,520
that has the same situation 500 gigs of

00:27:14,000 --> 00:27:19,760
memory so now

00:27:15,520 --> 00:27:22,480
i was talking about a numa before

00:27:19,760 --> 00:27:23,200
and if you see this machine has two numa

00:27:22,480 --> 00:27:25,200
nodes

00:27:23,200 --> 00:27:26,960
with memory that is numa node zero and

00:27:25,200 --> 00:27:29,039
one node eight but there is this newer

00:27:26,960 --> 00:27:31,679
node 16 here that does not six

00:27:29,039 --> 00:27:34,000
no cpu but also no memory this is where

00:27:31,679 --> 00:27:37,039
we are going to attach

00:27:34,000 --> 00:27:39,520
the disaggregated memory so

00:27:37,039 --> 00:27:42,080
we go back on the orchestrator and we

00:27:39,520 --> 00:27:44,880
just ask the orchestrator to attach

00:27:42,080 --> 00:27:46,080
100 gigabytes of memory to this machine

00:27:44,880 --> 00:27:49,200
so the orchestrator

00:27:46,080 --> 00:27:49,840
quickly found who is the body so while

00:27:49,200 --> 00:27:51,760
the

00:27:49,840 --> 00:27:53,360
the the line is orange what is happening

00:27:51,760 --> 00:27:54,000
is that the orchestrator is contacting

00:27:53,360 --> 00:27:58,640
the teammates

00:27:54,000 --> 00:28:00,559
flow agent on the memory donor

00:27:58,640 --> 00:28:02,320
asking for allocating the memory getting

00:28:00,559 --> 00:28:04,480
the effective address and then

00:28:02,320 --> 00:28:05,919
going to the to the compute node

00:28:04,480 --> 00:28:07,440
configuring the hardware and hot

00:28:05,919 --> 00:28:09,840
plugging in the memory so

00:28:07,440 --> 00:28:11,440
once the link is green this means that

00:28:09,840 --> 00:28:12,320
the memory was allocated and you see

00:28:11,440 --> 00:28:14,799
here immediately

00:28:12,320 --> 00:28:15,760
that the node now has 600 gigabytes of

00:28:14,799 --> 00:28:18,640
memory and then

00:28:15,760 --> 00:28:19,520
the memory went up but if you don't

00:28:18,640 --> 00:28:23,039
believe me i

00:28:19,520 --> 00:28:24,799
show you here that if you now see

00:28:23,039 --> 00:28:26,080
the memory available at the compute node

00:28:24,799 --> 00:28:30,559
side it

00:28:26,080 --> 00:28:32,240
magically went up to 600 gigabytes

00:28:30,559 --> 00:28:34,080
and the thing the interesting thing is

00:28:32,240 --> 00:28:36,320
that if you see the memory donor it's

00:28:34,080 --> 00:28:37,840
not that the memory available

00:28:36,320 --> 00:28:39,679
went down right because we're not

00:28:37,840 --> 00:28:40,720
actually removing the memory physically

00:28:39,679 --> 00:28:42,720
from the machine

00:28:40,720 --> 00:28:44,559
we're just reserving it with the user

00:28:42,720 --> 00:28:47,520
space process so the memory

00:28:44,559 --> 00:28:48,880
was allocated by by processing made

00:28:47,520 --> 00:28:52,000
available now

00:28:48,880 --> 00:28:55,279
if i go here and i run

00:28:52,000 --> 00:28:58,640
stream so this is stream running

00:28:55,279 --> 00:29:00,159
on the new one of zero you see that the

00:28:58,640 --> 00:29:03,120
performance is uh

00:29:00,159 --> 00:29:04,159
something around the 140 gigabytes per

00:29:03,120 --> 00:29:06,720
second

00:29:04,159 --> 00:29:07,279
but by just changing the the numa

00:29:06,720 --> 00:29:09,520
binding

00:29:07,279 --> 00:29:11,440
of this application now i'm running

00:29:09,520 --> 00:29:11,760
stream on this aggregated memory so you

00:29:11,440 --> 00:29:14,799
see

00:29:11,760 --> 00:29:17,840
the bandwidth is roughly 12 point

00:29:14,799 --> 00:29:19,520
something close to the the 100 gigabit

00:29:17,840 --> 00:29:21,679
per second that you

00:29:19,520 --> 00:29:23,919
that you could expect from uh from this

00:29:21,679 --> 00:29:25,279
design so as you see i i didn't change

00:29:23,919 --> 00:29:27,679
the application i didn't do anything i

00:29:25,279 --> 00:29:29,760
just said use this numa node for uh for

00:29:27,679 --> 00:29:31,520
memory allocation

00:29:29,760 --> 00:29:33,120
and then if you want you can even detach

00:29:31,520 --> 00:29:33,679
so you just go to this interface and

00:29:33,120 --> 00:29:36,399
detach

00:29:33,679 --> 00:29:38,320
the disaggregated memory and the the

00:29:36,399 --> 00:29:42,159
operation is inverse so the memory is

00:29:38,320 --> 00:29:42,960
hot unplugged and and freed on the other

00:29:42,159 --> 00:29:46,399
machine

00:29:42,960 --> 00:29:50,000
so i'll just i'll just conclude

00:29:46,399 --> 00:29:52,000
that i have shown you teammas flow that

00:29:50,000 --> 00:29:54,399
is the first of its kind open framework

00:29:52,000 --> 00:29:55,840
for peripherals disaggregation

00:29:54,399 --> 00:29:57,440
and the software stock enables

00:29:55,840 --> 00:30:00,399
completely transparent

00:29:57,440 --> 00:30:01,600
software controlled memory attachment

00:30:00,399 --> 00:30:03,520
and

00:30:01,600 --> 00:30:05,279
from from this evaluation it looks like

00:30:03,520 --> 00:30:08,559
it can be an appealing technology

00:30:05,279 --> 00:30:10,799
for uh for some for some applications

00:30:08,559 --> 00:30:12,080
so this concludes my talk if you have

00:30:10,799 --> 00:30:16,640
any questions i'll

00:30:12,080 --> 00:30:16,640
i'm happy to to take them

00:30:16,880 --> 00:30:23,279
there is actually uh

00:30:20,159 --> 00:30:27,440
there is actually a question from uh

00:30:23,279 --> 00:30:30,880
from paul uh so yes if we used

00:30:27,440 --> 00:30:34,799
two uh two memory links

00:30:30,880 --> 00:30:37,760
we would we would we would get

00:30:34,799 --> 00:30:38,960
the sorry the bandwidth would go up we

00:30:37,760 --> 00:30:41,039
have done this

00:30:38,960 --> 00:30:42,320
it's just it's still experimental it's

00:30:41,039 --> 00:30:44,399
called bonding

00:30:42,320 --> 00:30:47,120
but it doesn't go at 200 gigabits per

00:30:44,399 --> 00:30:50,080
second we have seen a 330

00:30:47,120 --> 00:30:50,720
improvement and this was because at the

00:30:50,080 --> 00:30:54,480
at the

00:30:50,720 --> 00:30:57,919
memory side we we don't use the 256

00:30:54,480 --> 00:31:01,519
uh bytes open copy transactions so we

00:30:57,919 --> 00:31:03,919
we are not capable of uh let's say

00:31:01,519 --> 00:31:06,720
using the full bandwidth that copy gives

00:31:03,919 --> 00:31:09,840
us because our design uses the 128

00:31:06,720 --> 00:31:12,960
bytes transaction because since the

00:31:09,840 --> 00:31:16,080
open copy transactions are generated as

00:31:12,960 --> 00:31:17,200
a result of um of a cache miss

00:31:16,080 --> 00:31:20,080
essentially right

00:31:17,200 --> 00:31:21,120
uh the the l3 cache miss the transaction

00:31:20,080 --> 00:31:25,679
will always be as

00:31:21,120 --> 00:31:28,559
big as the as the cache line

00:31:25,679 --> 00:31:29,120
uh then there is another question from

00:31:28,559 --> 00:31:31,440
port

00:31:29,120 --> 00:31:33,279
so we how do we map the open copy

00:31:31,440 --> 00:31:34,559
transactions to the ethernet frames we

00:31:33,279 --> 00:31:36,159
do not map

00:31:34,559 --> 00:31:37,919
uh open copy transactions to the

00:31:36,159 --> 00:31:41,519
internet frames uh

00:31:37,919 --> 00:31:43,840
if you see if you see if i show you

00:31:41,519 --> 00:31:45,519
if you see the the first slide here what

00:31:43,840 --> 00:31:49,679
happens is that

00:31:45,519 --> 00:31:52,720
the open copy transaction flows through

00:31:49,679 --> 00:31:56,240
inside our design and what we do

00:31:52,720 --> 00:32:00,640
is that uh the teams's flow logic

00:31:56,240 --> 00:32:03,519
is just patching the address and

00:32:00,640 --> 00:32:05,440
in the in the op code but then but then

00:32:03,519 --> 00:32:07,440
the network

00:32:05,440 --> 00:32:09,200
there is no transport here right the

00:32:07,440 --> 00:32:11,360
there is no internet

00:32:09,200 --> 00:32:13,519
we we just use the physical line for

00:32:11,360 --> 00:32:14,399
transmitting open copy transactions to

00:32:13,519 --> 00:32:16,559
the other side

00:32:14,399 --> 00:32:18,480
so what is flowing here we have a

00:32:16,559 --> 00:32:22,080
completely custom

00:32:18,480 --> 00:32:24,080
llc where we where we provide the

00:32:22,080 --> 00:32:27,279
reliable channel and we also

00:32:24,080 --> 00:32:28,000
have credits to avoid the overflowing of

00:32:27,279 --> 00:32:29,760
the other side

00:32:28,000 --> 00:32:31,600
but it is uh let's say it's all

00:32:29,760 --> 00:32:35,039
happening at the data link

00:32:31,600 --> 00:32:35,039
layer there is no protocol

00:32:37,679 --> 00:32:41,519
the memory yes so another another

00:32:40,080 --> 00:32:44,320
question from paul

00:32:41,519 --> 00:32:45,440
yes the memory is cacheable from the

00:32:44,320 --> 00:32:48,559
compute side in the

00:32:45,440 --> 00:32:50,559
yes because because for for the system

00:32:48,559 --> 00:32:52,720
is just uh

00:32:50,559 --> 00:32:54,159
just some memory so it goes through the

00:32:52,720 --> 00:32:57,440
cache yes

00:32:54,159 --> 00:33:00,320
uh so this this open let's say

00:32:57,440 --> 00:33:00,880
many many discussions right at some at

00:33:00,320 --> 00:33:02,559
some

00:33:00,880 --> 00:33:04,640
some points if you have if you have

00:33:02,559 --> 00:33:05,760
multiple nodes accessing the same remote

00:33:04,640 --> 00:33:07,440
memory

00:33:05,760 --> 00:33:09,279
then you might need to flash your caches

00:33:07,440 --> 00:33:11,519
before uh

00:33:09,279 --> 00:33:13,360
before the other node accesses to make

00:33:11,519 --> 00:33:15,919
sure that the data is in there but yes

00:33:13,360 --> 00:33:15,919
it's cacheable

00:33:18,640 --> 00:33:25,200
and so i don't see

00:33:23,200 --> 00:33:27,440
i don't see any other question here and

00:33:25,200 --> 00:33:27,440
uh

00:33:28,480 --> 00:33:32,320
no i don't see any other questions or

00:33:30,799 --> 00:33:35,760
hands that are raised

00:33:32,320 --> 00:33:36,480
so if if anyone has any other question

00:33:35,760 --> 00:33:39,840
just

00:33:36,480 --> 00:33:39,840
ask or otherwise i will be

00:33:43,519 --> 00:33:48,240
so yeah um another question from paul

00:33:46,159 --> 00:33:48,640
how could more than one be connected yes

00:33:48,240 --> 00:33:50,960
so

00:33:48,640 --> 00:33:52,240
we have it's something that i haven't

00:33:50,960 --> 00:33:55,200
discussed here

00:33:52,240 --> 00:33:57,600
we have uh besides the patching the

00:33:55,200 --> 00:34:00,880
address we have a network identifier

00:33:57,600 --> 00:34:03,600
that we use for selecting which circuit

00:34:00,880 --> 00:34:05,120
should these the transactions belonging

00:34:03,600 --> 00:34:08,159
to this section

00:34:05,120 --> 00:34:11,520
this forward the two this means that

00:34:08,159 --> 00:34:15,119
if you had if you put the

00:34:11,520 --> 00:34:18,000
circuit switch in between the fpgas

00:34:15,119 --> 00:34:18,800
this switch could be used for creating

00:34:18,000 --> 00:34:21,919
circuits

00:34:18,800 --> 00:34:23,919
between between multiple machines but

00:34:21,919 --> 00:34:25,599
definitely for instance right now even

00:34:23,919 --> 00:34:28,639
if you had a switch

00:34:25,599 --> 00:34:31,119
one node with one fpga

00:34:28,639 --> 00:34:32,320
could be connected only to two other

00:34:31,119 --> 00:34:35,760
remote machines

00:34:32,320 --> 00:34:39,760
simply because these

00:34:35,760 --> 00:34:43,280
the fpgas only have two qsfp

00:34:39,760 --> 00:34:45,630
cages so two links two secrets

00:34:43,280 --> 00:34:47,040
we were also considering

00:34:45,630 --> 00:34:49,760
[Music]

00:34:47,040 --> 00:34:51,599
working with the packet network but we

00:34:49,760 --> 00:34:52,240
we haven't we haven't yet done that work

00:34:51,599 --> 00:34:55,119
but this is

00:34:52,240 --> 00:34:56,560
this is something definitely interesting

00:34:55,119 --> 00:34:58,290
because this would

00:34:56,560 --> 00:34:59,599
would help us in uh

00:34:58,290 --> 00:35:01,280
[Music]

00:34:59,599 --> 00:35:02,640
definitely connected to much more

00:35:01,280 --> 00:35:04,320
machines like to have a much more

00:35:02,640 --> 00:35:08,000
complex

00:35:04,320 --> 00:35:08,000
complex fabric topology

00:35:14,839 --> 00:35:20,560
oh

00:35:17,119 --> 00:35:23,760
oh sorry there were there uh

00:35:20,560 --> 00:35:24,880
there were more questions yeah so one

00:35:23,760 --> 00:35:28,000
thing is

00:35:24,880 --> 00:35:30,160
so routing routing switching is done

00:35:28,000 --> 00:35:31,920
at the team assist flow logic so the

00:35:30,160 --> 00:35:33,839
team is flow logic is

00:35:31,920 --> 00:35:36,320
using the network identifier that i have

00:35:33,839 --> 00:35:38,560
mentioned is doing the actual routing

00:35:36,320 --> 00:35:39,920
the switching for the time there is no

00:35:38,560 --> 00:35:41,440
switching if you put the switch in

00:35:39,920 --> 00:35:42,480
between there would be some switching

00:35:41,440 --> 00:35:46,480
but only the routing

00:35:42,480 --> 00:35:49,599
is done by our logic

00:35:46,480 --> 00:35:50,079
potentially yes something like gen z for

00:35:49,599 --> 00:35:53,520
instance

00:35:50,079 --> 00:35:55,200
could be used as a transport for going

00:35:53,520 --> 00:35:59,839
outside of the node right

00:35:55,200 --> 00:35:59,839
it it is a possibility you you could use

00:36:00,800 --> 00:36:06,400
you you could use the open copy between

00:36:04,560 --> 00:36:09,760
the fpga

00:36:06,400 --> 00:36:09,760
now the fpga and the

00:36:10,000 --> 00:36:14,000
in the cpu and then have something like

00:36:11,920 --> 00:36:15,280
gen z as a transport for going between

00:36:14,000 --> 00:36:18,400
the nodes

00:36:15,280 --> 00:36:20,480
uh so uh the the session is ending now

00:36:18,400 --> 00:36:21,119
in two minutes so we have to wrap up but

00:36:20,480 --> 00:36:22,480
i will be

00:36:21,119 --> 00:36:25,200
i will be in the slot channel if there

00:36:22,480 --> 00:36:25,680
are more uh more questions or also there

00:36:25,200 --> 00:36:28,880
is my

00:36:25,680 --> 00:36:30,640
my email so if anybody wants to keep the

00:36:28,880 --> 00:36:31,680
discussion going i will i will be more

00:36:30,640 --> 00:36:34,560
than happy

00:36:31,680 --> 00:36:35,359
in doing that so thanks uh very much

00:36:34,560 --> 00:36:38,960
everybody for

00:36:35,359 --> 00:36:38,960

YouTube URL: https://www.youtube.com/watch?v=4N5G_Am6U-M


