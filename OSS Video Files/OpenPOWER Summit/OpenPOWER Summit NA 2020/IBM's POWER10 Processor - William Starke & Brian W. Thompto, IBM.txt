Title: IBM's POWER10 Processor - William Starke & Brian W. Thompto, IBM
Publication date: 2020-09-25
Playlist: OpenPOWER Summit NA 2020
Description: 
	IBM's POWER10 Processor - William Starke & Brian W. Thompto, IBM

Speakers: William Starke, Brian W. Thompto

POWER10 is IBM's next generation POWER micro-processor focused on enterprise computing. This talk will describe many of the new innovations and capabilities of POWER10 that build upon the strengths of recent generations and provide flexibility for the enterprise cloud.  POWER10 is fabricated using Samsung 7nm technology. It features a new core microarchitecture focused on energy efficiency, thread strength, increased SIMD execution capabilities, and instruction set enhancements targeted toward AI optimization. It also provides a substantial data plane bandwidth increase coupled with next generation OpenCAPI accelerator attach and Open Memory Interface (OMI) capabilities. A modular packaging architecture enables a broad range of deployments optimized for socket throughput, thread strength, cost, and scale. The PowerAXON high bandwidth, low latency, multi-protocol link architecture combines with numerous scaling architecture enhancements to provide robust large system characteristics and with scale-out system architectures to provide disaggregated memory clustering capabilities.
Captions: 
	00:00:00,160 --> 00:00:04,160
hello i'm bill starkey the chief

00:00:02,159 --> 00:00:05,759
architect of ibm's power 10

00:00:04,160 --> 00:00:07,680
microprocessor

00:00:05,759 --> 00:00:10,639
today i'm going to take you through the

00:00:07,680 --> 00:00:12,320
system level details of power 10.

00:00:10,639 --> 00:00:14,000
then i'll hand it off to our chief core

00:00:12,320 --> 00:00:15,440
architect brian tomto

00:00:14,000 --> 00:00:17,119
and he will unveil the core's

00:00:15,440 --> 00:00:20,080
capabilities

00:00:17,119 --> 00:00:21,039
but first a little context over the 10

00:00:20,080 --> 00:00:23,359
years shown here

00:00:21,039 --> 00:00:25,279
and many more before that power has been

00:00:23,359 --> 00:00:26,880
fundamentally about the enterprise and

00:00:25,279 --> 00:00:28,960
about extreme scale

00:00:26,880 --> 00:00:30,400
this isn't some consumer-grade piece of

00:00:28,960 --> 00:00:32,000
high-tech jewelry

00:00:30,400 --> 00:00:34,399
it's not under the hood of a beautiful

00:00:32,000 --> 00:00:35,760
new automobile it's the building block

00:00:34,399 --> 00:00:36,960
for the world's most powerful

00:00:35,760 --> 00:00:39,440
supercomputers

00:00:36,960 --> 00:00:40,480
applied to science to defeating covet

00:00:39,440 --> 00:00:42,719
19.

00:00:40,480 --> 00:00:45,440
even more importantly it powers the

00:00:42,719 --> 00:00:48,960
systems that enable our world to operate

00:00:45,440 --> 00:00:52,559
financial systems commerce healthcare

00:00:48,960 --> 00:00:54,160
governments power 10 is built for the

00:00:52,559 --> 00:00:55,840
enterprise of the future

00:00:54,160 --> 00:00:58,239
one that is mobilized by cloud

00:00:55,840 --> 00:00:59,600
capabilities and made smarter by ai

00:00:58,239 --> 00:01:02,320
technologies

00:00:59,600 --> 00:01:03,920
a bulletproof fortress of security and

00:01:02,320 --> 00:01:06,400
resiliency

00:01:03,920 --> 00:01:08,479
we have first hardware back in our labs

00:01:06,400 --> 00:01:10,479
and we are making excellent progress

00:01:08,479 --> 00:01:12,080
toward testing the chip and the first

00:01:10,479 --> 00:01:14,400
wave of systems

00:01:12,080 --> 00:01:17,759
we are on track to delivering power 10

00:01:14,400 --> 00:01:20,479
systems a little over a year from now

00:01:17,759 --> 00:01:21,360
today we'll talk about power 10's data

00:01:20,479 --> 00:01:23,439
plane

00:01:21,360 --> 00:01:26,320
built upon bandwidth enabling

00:01:23,439 --> 00:01:27,840
multi-faceted connectivity and scale

00:01:26,320 --> 00:01:30,960
about the enterprise strengths of the

00:01:27,840 --> 00:01:34,240
core about new security capabilities

00:01:30,960 --> 00:01:36,880
co-optimized across the stack about

00:01:34,240 --> 00:01:38,799
from the ground up re-architecting for

00:01:36,880 --> 00:01:41,360
energy efficiency

00:01:38,799 --> 00:01:42,880
and finally about the maturing ai

00:01:41,360 --> 00:01:45,520
landscape

00:01:42,880 --> 00:01:46,079
just as we hope for our children ai

00:01:45,520 --> 00:01:48,799
models

00:01:46,079 --> 00:01:51,200
grow up they graduate from training and

00:01:48,799 --> 00:01:53,439
get jobs in the real world

00:01:51,200 --> 00:01:54,640
acceleration needs to get pushed into

00:01:53,439 --> 00:01:57,200
the processor core

00:01:54,640 --> 00:01:59,439
infusing ai directly into the enterprise

00:01:57,200 --> 00:02:01,840
workflows

00:01:59,439 --> 00:02:03,439
so here is the chip a beautiful photo

00:02:01,840 --> 00:02:04,560
thanks to our partners at samsung

00:02:03,439 --> 00:02:07,520
foundry

00:02:04,560 --> 00:02:09,200
18 billion transistors in samsung's 7

00:02:07,520 --> 00:02:11,039
nanometer technology

00:02:09,200 --> 00:02:12,400
and with a few ibm tricks baked in

00:02:11,039 --> 00:02:15,200
courtesy of our rich

00:02:12,400 --> 00:02:17,040
research and development collaboration

00:02:15,200 --> 00:02:18,560
two packaging options but i'll get into

00:02:17,040 --> 00:02:21,440
that in a moment

00:02:18,560 --> 00:02:22,239
we also have two variants of our core a

00:02:21,440 --> 00:02:24,560
smaller

00:02:22,239 --> 00:02:26,800
four threaded version and more powerful

00:02:24,560 --> 00:02:28,560
eight threaded enterprise version

00:02:26,800 --> 00:02:31,519
today we'll talk about the enterprise

00:02:28,560 --> 00:02:33,840
core we offer up to 15 per chip

00:02:31,519 --> 00:02:36,800
each with two megabytes of l2 cache

00:02:33,840 --> 00:02:38,239
backed by a robust 120 megabytes of l3

00:02:36,800 --> 00:02:40,160
at the chip level

00:02:38,239 --> 00:02:41,840
those with a keen eye will notice 16

00:02:40,160 --> 00:02:43,519
physical cores on the chip

00:02:41,840 --> 00:02:45,360
we tend to get high demand for our

00:02:43,519 --> 00:02:47,599
highest core count offerings

00:02:45,360 --> 00:02:50,560
having a built-in spare vastly improves

00:02:47,599 --> 00:02:52,640
the economics of servicing those demands

00:02:50,560 --> 00:02:54,400
we feed these powerful hungry cores with

00:02:52,640 --> 00:02:55,680
a massive data plane interface to the

00:02:54,400 --> 00:02:57,280
outside world

00:02:55,680 --> 00:02:59,280
the chip perimeter is filled with high

00:02:57,280 --> 00:03:01,760
bandwidth low energy fis

00:02:59,280 --> 00:03:03,200
comprising our omi and power axon

00:03:01,760 --> 00:03:06,239
signaling infrastructure

00:03:03,200 --> 00:03:08,560
as well as our pci gen 5 io attach but

00:03:06,239 --> 00:03:10,560
more on these later

00:03:08,560 --> 00:03:13,200
power 10 comes in two packaging form

00:03:10,560 --> 00:03:15,519
factors the single chip module maximizes

00:03:13,200 --> 00:03:18,159
energy and data bandwidth per core

00:03:15,519 --> 00:03:19,360
it also provides topological flexibility

00:03:18,159 --> 00:03:21,840
for systems ranging

00:03:19,360 --> 00:03:24,080
all the way up to 16 gluelessly

00:03:21,840 --> 00:03:26,159
connected power 10 chips

00:03:24,080 --> 00:03:27,360
the dual chip module maximizes dense

00:03:26,159 --> 00:03:30,000
compute throughput

00:03:27,360 --> 00:03:30,959
and i o connectivity by compromising

00:03:30,000 --> 00:03:32,560
energy per core

00:03:30,959 --> 00:03:34,959
and memory attach per core while

00:03:32,560 --> 00:03:36,400
limiting smp topologies to one to four

00:03:34,959 --> 00:03:38,799
sockets

00:03:36,400 --> 00:03:40,720
when people talk about cloud they often

00:03:38,799 --> 00:03:41,840
think just about two and four socket

00:03:40,720 --> 00:03:44,239
systems

00:03:41,840 --> 00:03:46,400
with our power 10 dual chip module we

00:03:44,239 --> 00:03:49,040
gain impressive strength there

00:03:46,400 --> 00:03:50,799
but cloud isn't only about force fitting

00:03:49,040 --> 00:03:53,439
into those form factors

00:03:50,799 --> 00:03:55,040
by optimizing our 16 socket big iron

00:03:53,439 --> 00:03:58,000
systems for cloud as well

00:03:55,040 --> 00:04:01,840
we enable scale and elasticity far

00:03:58,000 --> 00:04:01,840
beyond what any small system can provide

00:04:02,159 --> 00:04:06,799
last year we introduced the power axon

00:04:04,959 --> 00:04:09,120
and open memory interfaces which are

00:04:06,799 --> 00:04:12,560
foundational to power 10.

00:04:09,120 --> 00:04:15,120
built upon high bandwidth low latency

00:04:12,560 --> 00:04:17,359
low energy certis technology they run

00:04:15,120 --> 00:04:19,120
high lane counts at up to 32 giga

00:04:17,359 --> 00:04:20,160
transfers per second using straight

00:04:19,120 --> 00:04:23,600
forward packaging

00:04:20,160 --> 00:04:25,600
150 micron bumps the result is a

00:04:23,600 --> 00:04:26,880
combined two terabytes per second per

00:04:25,600 --> 00:04:29,120
power tent chip

00:04:26,880 --> 00:04:30,960
with optimized placement for module and

00:04:29,120 --> 00:04:32,800
system planar simplicity

00:04:30,960 --> 00:04:34,880
more economical than expensive micro

00:04:32,800 --> 00:04:37,360
bump packaging for achieving bandwidth

00:04:34,880 --> 00:04:40,639
and more flexible enabling plugability

00:04:37,360 --> 00:04:42,560
and even cabling for distance

00:04:40,639 --> 00:04:43,840
these are both highly flexible

00:04:42,560 --> 00:04:45,680
interfaces but

00:04:43,840 --> 00:04:47,280
first let's look at how they are used

00:04:45,680 --> 00:04:50,800
for more conventional purposes

00:04:47,280 --> 00:04:54,000
yet provide unconventional value add

00:04:50,800 --> 00:04:57,040
in power 10 power axon is used for

00:04:54,000 --> 00:04:59,040
all chip to chip smp connectivity

00:04:57,040 --> 00:05:01,840
from small two socket systems to the

00:04:59,040 --> 00:05:02,800
largest 16 socket robustly scalable

00:05:01,840 --> 00:05:06,000
systems

00:05:02,800 --> 00:05:06,639
no asics or pneuma controllers just 16

00:05:06,000 --> 00:05:09,199
power 10

00:05:06,639 --> 00:05:11,280
chips a whole lot of bandwidth and

00:05:09,199 --> 00:05:13,280
several decades of coherence transport

00:05:11,280 --> 00:05:15,680
and protocol innovation

00:05:13,280 --> 00:05:17,199
so what will power 10 add for the large

00:05:15,680 --> 00:05:19,840
enterprise space

00:05:17,199 --> 00:05:21,120
we've built in compute capacity growth a

00:05:19,840 --> 00:05:23,680
stronger thread

00:05:21,120 --> 00:05:24,720
more bandwidth and several new scaling

00:05:23,680 --> 00:05:27,919
capabilities

00:05:24,720 --> 00:05:28,560
compared to an already strong power e980

00:05:27,919 --> 00:05:30,800
system

00:05:28,560 --> 00:05:33,039
power 10 big iron raises the ceiling

00:05:30,800 --> 00:05:35,280
substantially higher

00:05:33,039 --> 00:05:37,600
omi memory which we introduced here last

00:05:35,280 --> 00:05:39,120
year is the grandchild of the centaur

00:05:37,600 --> 00:05:41,039
memory we unveiled here several years

00:05:39,120 --> 00:05:44,080
ago along with power8

00:05:41,039 --> 00:05:46,800
it is technology agnostic so it can

00:05:44,080 --> 00:05:50,400
attach to any memory media with an omi

00:05:46,800 --> 00:05:51,680
compliant buffer since ddr4dram

00:05:50,400 --> 00:05:54,000
is still the bread and butter of the

00:05:51,680 --> 00:05:57,360
main memory tier power 10 will launch

00:05:54,000 --> 00:05:58,160
with the omi ddr4 buffer chip built by

00:05:57,360 --> 00:06:00,319
microchip

00:05:58,160 --> 00:06:02,400
that we introduced here last year the

00:06:00,319 --> 00:06:05,039
cost and bandwidth efficiencies enable

00:06:02,400 --> 00:06:06,479
up to 410 gigabytes per second of peak

00:06:05,039 --> 00:06:09,440
ddr4 bandwidth

00:06:06,479 --> 00:06:10,560
per power 10 chip that's at a chip level

00:06:09,440 --> 00:06:12,880
not a system level

00:06:10,560 --> 00:06:14,319
while incurring less than 10 nanoseconds

00:06:12,880 --> 00:06:16,880
of extra latency

00:06:14,319 --> 00:06:18,479
they've built an impressive solution and

00:06:16,880 --> 00:06:21,440
the nice thing about omi

00:06:18,479 --> 00:06:24,319
when ddr5 is ready we can simply replace

00:06:21,440 --> 00:06:27,919
ddr4 dimms with the er5 dimms

00:06:24,319 --> 00:06:30,000
no new system no new processor chip

00:06:27,919 --> 00:06:31,680
looking at broader possibilities

00:06:30,000 --> 00:06:34,240
consider a gpu

00:06:31,680 --> 00:06:36,800
rigidly packaged with hbm memory on an

00:06:34,240 --> 00:06:39,840
expensive micro bump module

00:06:36,800 --> 00:06:41,199
using a gddr buffer in a standard omi

00:06:39,840 --> 00:06:43,759
dim form factor

00:06:41,199 --> 00:06:44,639
you can push 800 gigabytes per second

00:06:43,759 --> 00:06:47,360
sustained

00:06:44,639 --> 00:06:48,000
without the rigidity or cost not quite

00:06:47,360 --> 00:06:50,880
the bandwidth

00:06:48,000 --> 00:06:52,639
hbm can push into a gpu but highly

00:06:50,880 --> 00:06:54,080
differentiated for a general purpose

00:06:52,639 --> 00:06:56,479
processor

00:06:54,080 --> 00:06:58,479
or at the other end of the spectrum you

00:06:56,479 --> 00:07:00,479
could put a high capacity persistent

00:06:58,479 --> 00:07:03,599
storage class memory into a standard

00:07:00,479 --> 00:07:05,520
omi dim form factor might be a nice way

00:07:03,599 --> 00:07:10,080
to start filling up power 10's

00:07:05,520 --> 00:07:13,599
two petabyte physical address range

00:07:10,080 --> 00:07:15,759
power 10 also supports open capi an open

00:07:13,599 --> 00:07:17,520
asymmetric protocol for coherently

00:07:15,759 --> 00:07:20,479
attaching compute accelerators

00:07:17,520 --> 00:07:22,880
memory devices networking and storage

00:07:20,479 --> 00:07:24,880
which first showed up in power9

00:07:22,880 --> 00:07:27,680
the memory piece of the protocol like

00:07:24,880 --> 00:07:28,479
omi is well suited to attaching storage

00:07:27,680 --> 00:07:31,120
class memory

00:07:28,479 --> 00:07:34,160
but in a device slot or cabled external

00:07:31,120 --> 00:07:36,639
enclosure instead of a dimm slot

00:07:34,160 --> 00:07:37,840
power 10 introduces a bold new member to

00:07:36,639 --> 00:07:41,120
the power axon

00:07:37,840 --> 00:07:43,759
family memory inception

00:07:41,120 --> 00:07:46,000
as the world evolves toward cloud the

00:07:43,759 --> 00:07:48,080
economic promise of disaggregated

00:07:46,000 --> 00:07:49,680
infrastructure has been a tantalizing

00:07:48,080 --> 00:07:51,599
proposition

00:07:49,680 --> 00:07:53,440
marketing chartwear describing rack

00:07:51,599 --> 00:07:56,319
scale architectures

00:07:53,440 --> 00:07:58,800
and iconic names like the machine

00:07:56,319 --> 00:08:00,879
inspire the dream of extending resource

00:07:58,800 --> 00:08:04,080
sharing beyond the server

00:08:00,879 --> 00:08:05,840
power 10 takes a large step by enabling

00:08:04,080 --> 00:08:08,319
systems to directly

00:08:05,840 --> 00:08:09,120
share each other's main memory full

00:08:08,319 --> 00:08:11,280
hardware

00:08:09,120 --> 00:08:14,720
load store access to the other guy's

00:08:11,280 --> 00:08:18,160
memory as if it was in your own computer

00:08:14,720 --> 00:08:19,440
latency roughly only 50 to 100

00:08:18,160 --> 00:08:22,080
nanoseconds worse

00:08:19,440 --> 00:08:22,879
than a remote socket in your own system

00:08:22,080 --> 00:08:26,160
good enough

00:08:22,879 --> 00:08:29,440
to still be used as main to your memory

00:08:26,160 --> 00:08:30,240
how might this be used well consider a

00:08:29,440 --> 00:08:32,640
pod

00:08:30,240 --> 00:08:34,479
of eight two socket servers each with

00:08:32,640 --> 00:08:36,320
eight terabytes of memory

00:08:34,479 --> 00:08:38,159
imagine three premium workloads each

00:08:36,320 --> 00:08:38,719
consuming the compute resources of a

00:08:38,159 --> 00:08:41,200
server

00:08:38,719 --> 00:08:42,240
but with differing memory requirements

00:08:41,200 --> 00:08:43,919
while workload a

00:08:42,240 --> 00:08:45,360
could run happily without a memory

00:08:43,919 --> 00:08:48,320
clustering capability

00:08:45,360 --> 00:08:50,240
look at workloads b and c they both need

00:08:48,320 --> 00:08:52,160
three times more memory than a single

00:08:50,240 --> 00:08:54,240
box can provide

00:08:52,160 --> 00:08:55,760
by borrowing from their neighbors their

00:08:54,240 --> 00:08:58,000
needs are met

00:08:55,760 --> 00:08:59,839
fundamentally we can cut memory costs

00:08:58,000 --> 00:09:02,080
since we don't need to configure each

00:08:59,839 --> 00:09:04,720
server to meet spike demand

00:09:02,080 --> 00:09:07,519
but this is a small example with power

00:09:04,720 --> 00:09:10,800
10's two petabyte physical address range

00:09:07,519 --> 00:09:12,800
we can think big how about instead of

00:09:10,800 --> 00:09:15,120
clustering eight small systems

00:09:12,800 --> 00:09:16,560
why not use the biggest baddest 16

00:09:15,120 --> 00:09:18,640
socket systems

00:09:16,560 --> 00:09:20,720
and why stop at eight of them why not

00:09:18,640 --> 00:09:22,720
pull together all two petabytes worth of

00:09:20,720 --> 00:09:25,920
shareable capacity

00:09:22,720 --> 00:09:28,720
or consider a hub and spoke topology

00:09:25,920 --> 00:09:29,839
maybe a large system maxed out on memory

00:09:28,720 --> 00:09:32,800
and several small

00:09:29,839 --> 00:09:33,760
dense inexpensive servers without any

00:09:32,800 --> 00:09:36,160
memory at all

00:09:33,760 --> 00:09:38,560
borrowing from the mothership there are

00:09:36,160 --> 00:09:39,920
lots of possibilities

00:09:38,560 --> 00:09:42,000
instead of just sharing with your

00:09:39,920 --> 00:09:43,200
neighbor why not build messaging

00:09:42,000 --> 00:09:45,200
software

00:09:43,200 --> 00:09:46,480
write to the memory that lives across

00:09:45,200 --> 00:09:49,279
the data center

00:09:46,480 --> 00:09:50,640
by re-translating addresses at every hop

00:09:49,279 --> 00:09:53,360
using page tables

00:09:50,640 --> 00:09:55,760
as routing tables using two petabyte

00:09:53,360 --> 00:09:57,920
addressability for routing ids

00:09:55,760 --> 00:09:58,959
and incorporating robust virtual channel

00:09:57,920 --> 00:10:01,920
management

00:09:58,959 --> 00:10:04,560
a fully hardware managed end-to-end

00:10:01,920 --> 00:10:07,839
messaging capability is possible

00:10:04,560 --> 00:10:11,519
thousands of nodes flexible topologies

00:10:07,839 --> 00:10:14,640
robust bisection bandwidth low latency

00:10:11,519 --> 00:10:17,360
cables only no extra gear

00:10:14,640 --> 00:10:18,640
memory inception for pod level memory

00:10:17,360 --> 00:10:20,480
resource pooling

00:10:18,640 --> 00:10:22,800
or for clustering up to thousands of

00:10:20,480 --> 00:10:25,200
nodes without any extra gear

00:10:22,800 --> 00:10:26,560
power 10 opens the door to memory

00:10:25,200 --> 00:10:28,480
disaggregation

00:10:26,560 --> 00:10:30,560
transforming cloud infrastructure

00:10:28,480 --> 00:10:33,279
economics

00:10:30,560 --> 00:10:35,279
last but certainly not least power 10

00:10:33,279 --> 00:10:36,800
incorporates high bandwidth benefits of

00:10:35,279 --> 00:10:39,519
pci gen 5

00:10:36,800 --> 00:10:42,079
and the breadth of the pci ecosystem up

00:10:39,519 --> 00:10:44,399
to 64 lanes per socket in the dual chip

00:10:42,079 --> 00:10:46,880
module offering

00:10:44,399 --> 00:10:49,440
before handing this over i want to share

00:10:46,880 --> 00:10:51,839
a performance analysis of a power 10

00:10:49,440 --> 00:10:54,079
dual socket system compared to its

00:10:51,839 --> 00:10:58,240
direct power 9 predecessor

00:10:54,079 --> 00:11:02,160
a power 9 s924 system

00:10:58,240 --> 00:11:04,560
3x performance relative to power 9.

00:11:02,160 --> 00:11:05,440
that is an outsized gain for one

00:11:04,560 --> 00:11:08,079
generation

00:11:05,440 --> 00:11:08,720
our strongest in over a decade and for

00:11:08,079 --> 00:11:11,760
memory

00:11:08,720 --> 00:11:14,399
we see the positive impact of omi

00:11:11,760 --> 00:11:15,519
brian our chief core architect will walk

00:11:14,399 --> 00:11:18,160
through the ipc

00:11:15,519 --> 00:11:18,640
and energy efficiency details underneath

00:11:18,160 --> 00:11:20,959
those

00:11:18,640 --> 00:11:23,040
integer commercial enterprise and

00:11:20,959 --> 00:11:24,800
floating point gains

00:11:23,040 --> 00:11:27,040
and just wait until he shows you the

00:11:24,800 --> 00:11:30,800
larger gains for ai inference

00:11:27,040 --> 00:11:32,959
brian take it away thank you bill

00:11:30,800 --> 00:11:35,040
i'm brian tomto and i'm excited to share

00:11:32,959 --> 00:11:37,440
with you many of the innovations

00:11:35,040 --> 00:11:38,880
and capabilities of our power 10

00:11:37,440 --> 00:11:41,440
processor core

00:11:38,880 --> 00:11:42,079
that help make power 10 ideally suited

00:11:41,440 --> 00:11:44,079
for mission

00:11:42,079 --> 00:11:47,120
critical applications and the

00:11:44,079 --> 00:11:49,279
opportunities of the enterprise cloud

00:11:47,120 --> 00:11:50,880
the power 10 core was focused first and

00:11:49,279 --> 00:11:51,680
foremost with providing gains and

00:11:50,880 --> 00:11:53,279
performance

00:11:51,680 --> 00:11:55,279
while at the same time reducing the

00:11:53,279 --> 00:11:56,880
energy consumption required

00:11:55,279 --> 00:11:58,959
this resulted in a new micro

00:11:56,880 --> 00:12:00,560
architecture that delivers extraordinary

00:11:58,959 --> 00:12:01,839
generational gains in computational

00:12:00,560 --> 00:12:04,320
efficiency

00:12:01,839 --> 00:12:06,000
compared with power 9 the power 10 core

00:12:04,320 --> 00:12:07,279
delivers an average of 30 percent

00:12:06,000 --> 00:12:09,920
performance improvement

00:12:07,279 --> 00:12:11,360
across a range of workloads and 20 gains

00:12:09,920 --> 00:12:13,360
for single thread

00:12:11,360 --> 00:12:14,480
combined with a ground-up focus on

00:12:13,360 --> 00:12:16,800
energy efficiency

00:12:14,480 --> 00:12:19,040
the power 10 core provides a 2.6 x

00:12:16,800 --> 00:12:20,720
improvement in performance per watt

00:12:19,040 --> 00:12:22,240
at the socket level these efficiency

00:12:20,720 --> 00:12:23,839
gains are further amplified

00:12:22,240 --> 00:12:26,320
as we're able to pack in significantly

00:12:23,839 --> 00:12:27,760
more cores in our dcm socket

00:12:26,320 --> 00:12:29,680
and operate at a more efficient

00:12:27,760 --> 00:12:30,320
operating point delivering 3x

00:12:29,680 --> 00:12:33,120
improvements

00:12:30,320 --> 00:12:34,880
in performance and efficiency using the

00:12:33,120 --> 00:12:35,760
same modular building block concepts as

00:12:34,880 --> 00:12:37,519
power 9

00:12:35,760 --> 00:12:39,839
the power 10 core provides deployment

00:12:37,519 --> 00:12:41,120
flexibility supporting up to 8 threads

00:12:39,839 --> 00:12:43,120
and providing for a second chip

00:12:41,120 --> 00:12:45,839
variation with 4 threads per core

00:12:43,120 --> 00:12:47,200
and twice as many cores that is up to 30

00:12:45,839 --> 00:12:50,399
big smta cores

00:12:47,200 --> 00:12:52,800
or 60 s t 4 cores per socket a second

00:12:50,399 --> 00:12:55,120
major focus for the new power 10 design

00:12:52,800 --> 00:12:56,720
was to supercharge the processing core

00:12:55,120 --> 00:12:58,639
to accelerate ai

00:12:56,720 --> 00:13:00,399
enabling infusion into the enterprise

00:12:58,639 --> 00:13:02,320
applications

00:13:00,399 --> 00:13:04,160
as seen in the core dye photo these

00:13:02,320 --> 00:13:05,360
capabilities were built into every layer

00:13:04,160 --> 00:13:07,920
of the design

00:13:05,360 --> 00:13:09,360
cache load store cmd and the

00:13:07,920 --> 00:13:11,360
introduction of a matrix math

00:13:09,360 --> 00:13:13,440
accelerator providing 4x throughput

00:13:11,360 --> 00:13:14,880
gains versus power 9.

00:13:13,440 --> 00:13:16,480
we'll go into each of these themes a

00:13:14,880 --> 00:13:18,880
little deeper in the remainder of this

00:13:16,480 --> 00:13:20,399
presentation

00:13:18,880 --> 00:13:22,480
as depicted on the right side of this

00:13:20,399 --> 00:13:24,399
diagram the modular power 10 core

00:13:22,480 --> 00:13:26,480
provides enterprise flexibility

00:13:24,399 --> 00:13:28,480
and supports from one to eight active

00:13:26,480 --> 00:13:29,519
threads with an optimized balance of

00:13:28,480 --> 00:13:31,920
resource sharing

00:13:29,519 --> 00:13:33,839
isolation and efficiency this

00:13:31,920 --> 00:13:36,000
flexibility extends to supporting a

00:13:33,839 --> 00:13:39,519
world-class set of software stacks

00:13:36,000 --> 00:13:41,040
including aix ibm i enterprise linux

00:13:39,519 --> 00:13:43,199
and container-based stacks including

00:13:41,040 --> 00:13:45,199
openshift all designed to run

00:13:43,199 --> 00:13:47,360
side-by-side in the same socket with the

00:13:45,199 --> 00:13:49,440
powervm hypervisor

00:13:47,360 --> 00:13:51,360
power 10 extends this flexibility by

00:13:49,440 --> 00:13:53,600
supporting the kvm hypervisor

00:13:51,360 --> 00:13:55,760
running on top of power vm as a high

00:13:53,600 --> 00:13:57,440
performance nested hypervisor

00:13:55,760 --> 00:13:59,440
this opens the door to additional

00:13:57,440 --> 00:14:02,720
deployment flexibility for kvm

00:13:59,440 --> 00:14:04,639
and supports enhanced security

00:14:02,720 --> 00:14:06,320
enabling a host of new capabilities in

00:14:04,639 --> 00:14:08,639
the processor core and system

00:14:06,320 --> 00:14:10,320
powerten implements the power isa

00:14:08,639 --> 00:14:12,320
version 3.1

00:14:10,320 --> 00:14:13,839
the new isa was also contributed to the

00:14:12,320 --> 00:14:15,839
open power foundation

00:14:13,839 --> 00:14:17,279
and extends power's commitment to open

00:14:15,839 --> 00:14:20,000
communities and supporting

00:14:17,279 --> 00:14:22,639
an open development ecosystem the new

00:14:20,000 --> 00:14:24,720
isa supports 64-bit prefix instructions

00:14:22,639 --> 00:14:27,040
in a risk-friendly way in addition to

00:14:24,720 --> 00:14:29,040
classic 32-bit instruction forms

00:14:27,040 --> 00:14:30,959
this adds new capabilities to existing

00:14:29,040 --> 00:14:31,920
instructions such as pc relative

00:14:30,959 --> 00:14:33,839
addressing

00:14:31,920 --> 00:14:36,320
as well as providing a rich new op code

00:14:33,839 --> 00:14:38,000
space ready for future expansion

00:14:36,320 --> 00:14:39,760
power 10 supports over 200 new

00:14:38,000 --> 00:14:40,480
instructions including 32 byte loads and

00:14:39,760 --> 00:14:42,480
stores

00:14:40,480 --> 00:14:44,880
and new ai accelerating instructions and

00:14:42,480 --> 00:14:46,480
data types it also adds a number of

00:14:44,880 --> 00:14:48,399
advanced system capabilities

00:14:46,480 --> 00:14:50,320
designed for the connected cloud with

00:14:48,399 --> 00:14:51,680
enablement for the swiss army knife of

00:14:50,320 --> 00:14:53,680
power 10 connectivity

00:14:51,680 --> 00:14:55,839
that bill showed us including memory

00:14:53,680 --> 00:14:57,040
inception advanced storage optimizations

00:14:55,839 --> 00:15:00,320
and memory tiers

00:14:57,040 --> 00:15:02,480
as well as enhanced security measures

00:15:00,320 --> 00:15:03,760
enterprise-grade security requires

00:15:02,480 --> 00:15:06,160
delivering on security

00:15:03,760 --> 00:15:07,920
across the software stacks the power 10

00:15:06,160 --> 00:15:09,680
processor builds on a strong foundation

00:15:07,920 --> 00:15:11,600
of security and isolation

00:15:09,680 --> 00:15:13,519
co-optimized from the hardware through

00:15:11,600 --> 00:15:15,600
each secure virtualization layer

00:15:13,519 --> 00:15:17,440
and the application layers as depicted

00:15:15,600 --> 00:15:19,279
in the right side of this chart

00:15:17,440 --> 00:15:20,720
confidential computing capabilities are

00:15:19,279 --> 00:15:22,320
enhanced in power 10

00:15:20,720 --> 00:15:24,800
enabling the deployment of secure

00:15:22,320 --> 00:15:26,560
containers in the enterprise cloud

00:15:24,800 --> 00:15:28,959
in this co-engineered approach between

00:15:26,560 --> 00:15:30,800
hardware and the power vm hypervisor

00:15:28,959 --> 00:15:32,160
secure containers are isolated and

00:15:30,800 --> 00:15:33,120
protected from threads in the

00:15:32,160 --> 00:15:35,519
application

00:15:33,120 --> 00:15:36,639
and virtualization layers allowing power

00:15:35,519 --> 00:15:38,240
10 to extend

00:15:36,639 --> 00:15:40,079
the traditional level of platform

00:15:38,240 --> 00:15:42,880
security to the latest

00:15:40,079 --> 00:15:44,160
cloud oriented software and frameworks

00:15:42,880 --> 00:15:46,079
power 10 also

00:15:44,160 --> 00:15:48,560
includes full and transparent memory

00:15:46,079 --> 00:15:50,800
encryption providing additional security

00:15:48,560 --> 00:15:52,720
in the cloud data center

00:15:50,800 --> 00:15:54,800
applications on power 10 can also take

00:15:52,720 --> 00:15:57,440
advantage of higher performance

00:15:54,800 --> 00:15:58,959
core cryptography and active management

00:15:57,440 --> 00:16:00,959
measures in the processor core

00:15:58,959 --> 00:16:04,320
to provide enhanced performance or

00:16:00,959 --> 00:16:06,160
retaining robust side channel avoidance

00:16:04,320 --> 00:16:08,079
now let's take a deeper look at the

00:16:06,160 --> 00:16:10,399
performance and efficiency foundations

00:16:08,079 --> 00:16:11,759
of the power tank core

00:16:10,399 --> 00:16:13,360
depicted in this chart is a block

00:16:11,759 --> 00:16:15,360
diagram showing the high level core

00:16:13,360 --> 00:16:17,279
microarchitecture for power 10.

00:16:15,360 --> 00:16:18,480
the resources shown support between one

00:16:17,279 --> 00:16:21,120
and four threads

00:16:18,480 --> 00:16:23,040
one half of the smta core instructions

00:16:21,120 --> 00:16:25,279
are fetched from the l2 cache

00:16:23,040 --> 00:16:27,600
the lower right up through the predecode

00:16:25,279 --> 00:16:29,360
and into the level 1 instruction cache

00:16:27,600 --> 00:16:31,440
power 10 then decodes and dispatches

00:16:29,360 --> 00:16:33,199
instructions to the execution slices

00:16:31,440 --> 00:16:35,759
and up to 8 instructions per thread and

00:16:33,199 --> 00:16:37,759
16 press mta core

00:16:35,759 --> 00:16:40,160
the execution slices on power 10 are

00:16:37,759 --> 00:16:42,639
each double the width of power 9

00:16:40,160 --> 00:16:43,519
each able to do 128 bit cindy operation

00:16:42,639 --> 00:16:45,680
per cycle

00:16:43,519 --> 00:16:47,279
and each with fixed float permute and

00:16:45,680 --> 00:16:49,040
crypto capabilities

00:16:47,279 --> 00:16:50,399
the core also includes matrix math

00:16:49,040 --> 00:16:52,800
acceleration engines

00:16:50,399 --> 00:16:54,959
providing a 4x growth in flops and ai

00:16:52,800 --> 00:16:57,360
data type support

00:16:54,959 --> 00:16:58,800
power 10 supports ex significantly

00:16:57,360 --> 00:17:01,040
larger working sites

00:16:58,800 --> 00:17:02,800
including four times the l2 capacity

00:17:01,040 --> 00:17:05,919
four times the tlb capacity

00:17:02,800 --> 00:17:07,520
and a 50 larger instruction cache power

00:17:05,919 --> 00:17:08,319
10 also supports deeper instruction

00:17:07,520 --> 00:17:10,319
windows

00:17:08,319 --> 00:17:13,360
larger queues with up to 1 000

00:17:10,319 --> 00:17:16,720
instructions in flight per smt8 core

00:17:13,360 --> 00:17:18,480
data latency access was also reduced l1

00:17:16,720 --> 00:17:20,079
data store forwarding is performed

00:17:18,480 --> 00:17:22,400
with the nominal load to use latency of

00:17:20,079 --> 00:17:24,400
four cycles eliminating two cycles and

00:17:22,400 --> 00:17:27,039
associated pipeline disruptions

00:17:24,400 --> 00:17:27,919
access to l2 l3 and tlb were also

00:17:27,039 --> 00:17:29,919
reduced

00:17:27,919 --> 00:17:32,080
two cycles less for l2 cache and eight

00:17:29,919 --> 00:17:33,919
cycles less for the l3

00:17:32,080 --> 00:17:35,679
for branches we doubled the bht

00:17:33,919 --> 00:17:36,400
predictor and have included new tag

00:17:35,679 --> 00:17:39,280
predictors

00:17:36,400 --> 00:17:41,280
for both indirect and relative branches

00:17:39,280 --> 00:17:42,880
we also made a significant change to

00:17:41,280 --> 00:17:44,400
brandt's execution

00:17:42,880 --> 00:17:46,960
merging the branch pipeline into the

00:17:44,400 --> 00:17:48,400
execution slices and targets into the

00:17:46,960 --> 00:17:50,320
main register files

00:17:48,400 --> 00:17:52,160
this reduces latency and improves

00:17:50,320 --> 00:17:53,919
efficiency

00:17:52,160 --> 00:17:56,240
we leverage the new prefix instruction

00:17:53,919 --> 00:17:58,160
detection circuitry in the predecode

00:17:56,240 --> 00:17:59,360
to also identify instruction fusion

00:17:58,160 --> 00:18:01,280
opportunities

00:17:59,360 --> 00:18:03,200
power 10 supports a wide array of fusion

00:18:01,280 --> 00:18:04,160
sequences including scalar and cmd

00:18:03,200 --> 00:18:06,240
instructions

00:18:04,160 --> 00:18:07,919
eliminating dependencies as well as

00:18:06,240 --> 00:18:09,200
supporting fusion between consecutive

00:18:07,919 --> 00:18:10,960
loads or stores

00:18:09,200 --> 00:18:12,720
utilizing each of the wider load and

00:18:10,960 --> 00:18:15,200
store pipes to handle two instructions

00:18:12,720 --> 00:18:15,919
at once combined with store gathering in

00:18:15,200 --> 00:18:18,400
the core

00:18:15,919 --> 00:18:21,120
each door port to the l2 can drain up to

00:18:18,400 --> 00:18:23,600
four stores per cycle

00:18:21,120 --> 00:18:25,679
so a lot of changes for performance but

00:18:23,600 --> 00:18:27,440
to accomplish our goals for power 10

00:18:25,679 --> 00:18:28,960
we had to redesign from day one for

00:18:27,440 --> 00:18:30,720
efficiency

00:18:28,960 --> 00:18:33,039
this included deployment of enhanced

00:18:30,720 --> 00:18:35,679
tools physical design techniques

00:18:33,039 --> 00:18:37,679
and innovations in the microarchitecture

00:18:35,679 --> 00:18:38,400
each design element was re-examined to

00:18:37,679 --> 00:18:40,840
improve

00:18:38,400 --> 00:18:42,160
clock gating and reduce switching

00:18:40,840 --> 00:18:44,080
capacitance

00:18:42,160 --> 00:18:45,520
many of the improvements for performance

00:18:44,080 --> 00:18:48,080
also paid double dividends and

00:18:45,520 --> 00:18:49,440
efficiency by reducing wasted work

00:18:48,080 --> 00:18:51,200
in addition we benefited from

00:18:49,440 --> 00:18:54,240
redesigning major structures

00:18:51,200 --> 00:18:57,039
such as queues and register files with a

00:18:54,240 --> 00:18:58,960
focus on port reduction

00:18:57,039 --> 00:19:00,559
that included changing our level 1 data

00:18:58,960 --> 00:19:03,200
and instruction cache structures

00:19:00,559 --> 00:19:04,960
to be ea tagged this allows instructions

00:19:03,200 --> 00:19:07,200
to flow through the pipeline without the

00:19:04,960 --> 00:19:09,600
need to perform address translation

00:19:07,200 --> 00:19:10,880
the e-wrap for example is only accessed

00:19:09,600 --> 00:19:12,960
on a cache miss

00:19:10,880 --> 00:19:16,720
and tracking structures took advantage

00:19:12,960 --> 00:19:18,480
of simpler tagging to improve efficiency

00:19:16,720 --> 00:19:20,160
all of this redesign and innovation

00:19:18,480 --> 00:19:21,200
result in significant improvements in

00:19:20,160 --> 00:19:23,679
both performance

00:19:21,200 --> 00:19:24,640
and performance per watt the power 10

00:19:23,679 --> 00:19:27,919
core achieves

00:19:24,640 --> 00:19:29,600
30 improvement in ipc while consuming

00:19:27,919 --> 00:19:32,000
one half the power

00:19:29,600 --> 00:19:33,919
at 2.6 x improvement in core efficiency

00:19:32,000 --> 00:19:36,400
compared with power 9.

00:19:33,919 --> 00:19:38,320
at the socket level this adds up to an

00:19:36,400 --> 00:19:39,679
aggregate of 3x improvement at a more

00:19:38,320 --> 00:19:41,919
efficient operating point

00:19:39,679 --> 00:19:44,880
and as bill showed also translates into

00:19:41,919 --> 00:19:44,880
3x performance

00:19:45,039 --> 00:19:49,280
the power 10 core supports the seamless

00:19:47,600 --> 00:19:50,160
integration of ai in the enterprise

00:19:49,280 --> 00:19:53,200
cloud

00:19:50,160 --> 00:19:55,360
our ai focus is partly about the compute

00:19:53,200 --> 00:19:56,720
but it's also about the data machine

00:19:55,360 --> 00:19:58,400
learning algorithms depend on the

00:19:56,720 --> 00:20:00,559
ability to move and prepare data

00:19:58,400 --> 00:20:02,320
and feed computations effectively so

00:20:00,559 --> 00:20:04,320
power 10 also focused on something a

00:20:02,320 --> 00:20:07,200
little more traditional to computing

00:20:04,320 --> 00:20:09,120
providing robust bytes per flop moving

00:20:07,200 --> 00:20:11,280
from power 9 to power 10

00:20:09,120 --> 00:20:12,799
we doubled the ability to bring in data

00:20:11,280 --> 00:20:15,200
from every level of cache

00:20:12,799 --> 00:20:15,919
and from memory to the compute engines

00:20:15,200 --> 00:20:18,400
that is

00:20:15,919 --> 00:20:20,400
2x from the l1 cache enabled by wider

00:20:18,400 --> 00:20:22,960
load and store instructions and fusion

00:20:20,400 --> 00:20:23,520
up to four 32 byte loads or two 32 byte

00:20:22,960 --> 00:20:26,720
stores

00:20:23,520 --> 00:20:28,559
per cycle presented core as

00:20:26,720 --> 00:20:30,720
ai inference goes to work in the

00:20:28,559 --> 00:20:33,039
enterprise cloud individual cores can

00:20:30,720 --> 00:20:35,760
also be central to data ingestion

00:20:33,039 --> 00:20:38,000
or for feeding transactional inference

00:20:35,760 --> 00:20:39,039
so we plumbed in 2x bandwidth from

00:20:38,000 --> 00:20:41,200
memory

00:20:39,039 --> 00:20:43,679
through to the cores and we can sustain

00:20:41,200 --> 00:20:46,320
a rate of 120 gigabytes per second

00:20:43,679 --> 00:20:47,280
out of a 256 gigabyte per second peak

00:20:46,320 --> 00:20:50,320
read plus right

00:20:47,280 --> 00:20:52,799
with a single smta core that's

00:20:50,320 --> 00:20:55,280
2x on the bite side of our story now

00:20:52,799 --> 00:20:57,679
let's look at the flops

00:20:55,280 --> 00:20:58,640
we doubled our cindy engines to 8 press

00:20:57,679 --> 00:21:00,720
and t8 core

00:20:58,640 --> 00:21:02,159
to provide bandwidth match compute and

00:21:00,720 --> 00:21:03,679
tackle data preparation

00:21:02,159 --> 00:21:06,159
as well as a host of machine learning

00:21:03,679 --> 00:21:08,080
algorithms we did this while staying

00:21:06,159 --> 00:21:09,520
within our existing 128-bit cindy

00:21:08,080 --> 00:21:11,600
register architecture

00:21:09,520 --> 00:21:13,360
providing flexibility to software while

00:21:11,600 --> 00:21:15,039
boosting the performance in existing

00:21:13,360 --> 00:21:16,799
binaries

00:21:15,039 --> 00:21:18,080
we also focused on algorithms that were

00:21:16,799 --> 00:21:20,240
hungry for flops

00:21:18,080 --> 00:21:21,440
such as the matrix map utilized in deep

00:21:20,240 --> 00:21:23,440
learning

00:21:21,440 --> 00:21:26,000
every core has built-in matrix math

00:21:23,440 --> 00:21:28,480
acceleration that efficiently perform

00:21:26,000 --> 00:21:30,480
matrix outer product operations these

00:21:28,480 --> 00:21:32,640
operations were optimized across a

00:21:30,480 --> 00:21:34,320
wide range of data types recognizing

00:21:32,640 --> 00:21:35,919
that various precisions can be best

00:21:34,320 --> 00:21:36,799
suited for specific machine learning

00:21:35,919 --> 00:21:39,200
algorithms

00:21:36,799 --> 00:21:40,000
we included very broad support double

00:21:39,200 --> 00:21:42,320
precision

00:21:40,000 --> 00:21:44,640
single precision two flavors of half

00:21:42,320 --> 00:21:46,960
precision including both i triple e

00:21:44,640 --> 00:21:48,240
and b float 16 as well as reduced

00:21:46,960 --> 00:21:51,440
precision integer

00:21:48,240 --> 00:21:53,280
16 8 and 4 bit the result

00:21:51,440 --> 00:21:55,200
is 64 flaps per cycle of double

00:21:53,280 --> 00:21:57,440
precision and up to 1k

00:21:55,200 --> 00:22:00,159
ops per cycle of reduced precision math

00:21:57,440 --> 00:22:02,000
per smta core

00:22:00,159 --> 00:22:03,360
these operations were tailor-made to be

00:22:02,000 --> 00:22:04,159
efficient while applying to machine

00:22:03,360 --> 00:22:05,600
learning

00:22:04,159 --> 00:22:07,440
the reduced precision operations

00:22:05,600 --> 00:22:09,520
accumulate into 32-bit results

00:22:07,440 --> 00:22:12,559
preserving precision and reducing

00:22:09,520 --> 00:22:14,240
algorithmic overhead you can see one of

00:22:12,559 --> 00:22:15,280
these mma operations depicted in the

00:22:14,240 --> 00:22:17,039
upper right

00:22:15,280 --> 00:22:20,159
this efficient matrix math that is

00:22:17,039 --> 00:22:22,799
performed is based on just 128-bit

00:22:20,159 --> 00:22:26,880
operands two of them and accumulate into

00:22:22,799 --> 00:22:28,480
a 512-bit result register per smt8 core

00:22:26,880 --> 00:22:29,679
we can do four of these operations per

00:22:28,480 --> 00:22:31,520
cycle

00:22:29,679 --> 00:22:33,039
just as for the main cmd engines the new

00:22:31,520 --> 00:22:34,559
matrix math acceleration

00:22:33,039 --> 00:22:36,880
doesn't require any new architected

00:22:34,559 --> 00:22:38,640
register state and minimizes software

00:22:36,880 --> 00:22:41,039
ecosystem disruption

00:22:38,640 --> 00:22:42,960
it also enables performance for machine

00:22:41,039 --> 00:22:43,760
learning frameworks and applications to

00:22:42,960 --> 00:22:45,520
be employed

00:22:43,760 --> 00:22:46,799
by simply updating libraries in many

00:22:45,520 --> 00:22:48,240
cases

00:22:46,799 --> 00:22:50,960
re-referencing the dye photo in the

00:22:48,240 --> 00:22:52,480
lower right all of that matrix math

00:22:50,960 --> 00:22:54,799
happens in the blue rectangle on the

00:22:52,480 --> 00:22:56,400
right side with a purpose-built

00:22:54,799 --> 00:22:59,120
structure that leverages

00:22:56,400 --> 00:23:00,640
data reuse efficiency the accumulator

00:22:59,120 --> 00:23:01,600
result data is stored local to the

00:23:00,640 --> 00:23:03,679
computation

00:23:01,600 --> 00:23:04,880
while opera operands are streamed in

00:23:03,679 --> 00:23:08,000
from the cache hierarchy

00:23:04,880 --> 00:23:09,919
for 2x efficiency versus traditional 7d

00:23:08,000 --> 00:23:11,120
these operations provide 4x plus

00:23:09,919 --> 00:23:14,080
throughput per core

00:23:11,120 --> 00:23:14,799
and also significantly reduce latency a

00:23:14,080 --> 00:23:16,480
3 times

00:23:14,799 --> 00:23:18,480
inference latency reduction for a single

00:23:16,480 --> 00:23:20,480
thread using single precision

00:23:18,480 --> 00:23:22,799
and a 6 times reduction in latency for

00:23:20,480 --> 00:23:25,120
integer 8.

00:23:22,799 --> 00:23:26,559
at the socket level you get 10 times the

00:23:25,120 --> 00:23:28,480
performance per socket

00:23:26,559 --> 00:23:30,080
for double and single precision and

00:23:28,480 --> 00:23:33,360
using reduced precision

00:23:30,080 --> 00:23:36,080
b float 16 is sped up to over 15x and

00:23:33,360 --> 00:23:37,440
8 inference sped up to over 20x versus

00:23:36,080 --> 00:23:40,480
power 9.

00:23:37,440 --> 00:23:42,640
this gives power 10 the ai horsepower to

00:23:40,480 --> 00:23:45,279
integrate ai intelligence in enterprise

00:23:42,640 --> 00:23:45,279
applications

00:23:45,520 --> 00:23:49,279
now let's review how the main technology

00:23:48,000 --> 00:23:50,960
themes of power 10

00:23:49,279 --> 00:23:53,360
provide extraordinary capability for the

00:23:50,960 --> 00:23:56,400
enterprise cloud whether on-prem

00:23:53,360 --> 00:23:58,720
public or hybrid a robust data plane

00:23:56,400 --> 00:23:59,520
enables the most resilient scalable s p

00:23:58,720 --> 00:24:01,679
servers

00:23:59,520 --> 00:24:03,120
and provides unparalleled flexibility

00:24:01,679 --> 00:24:05,279
for deploying workloads

00:24:03,120 --> 00:24:06,320
in the enterprise clouds of today and

00:24:05,279 --> 00:24:08,159
tomorrow

00:24:06,320 --> 00:24:10,000
a new enterprise core provides

00:24:08,159 --> 00:24:10,559
significant performance and efficiency

00:24:10,000 --> 00:24:12,799
gains

00:24:10,559 --> 00:24:15,120
and was co-optimized to run flexibly

00:24:12,799 --> 00:24:17,840
with world-class software stacks

00:24:15,120 --> 00:24:19,679
security was engineered across the stack

00:24:17,840 --> 00:24:20,880
to provide enterprise asset protection

00:24:19,679 --> 00:24:22,480
to the latest class

00:24:20,880 --> 00:24:25,279
of cloud deployment scenarios and

00:24:22,480 --> 00:24:26,000
software and extreme generational gains

00:24:25,279 --> 00:24:28,480
of 3x

00:24:26,000 --> 00:24:30,080
performance and efficiency improve

00:24:28,480 --> 00:24:32,400
computational capacity

00:24:30,080 --> 00:24:33,120
across a broad range of workloads

00:24:32,400 --> 00:24:35,679
finally

00:24:33,120 --> 00:24:36,400
in core accelerated support for ai

00:24:35,679 --> 00:24:39,919
inference

00:24:36,400 --> 00:24:42,400
provides 10 to 20x generational speedups

00:24:39,919 --> 00:24:44,480
to power ai infusion into enterprise

00:24:42,400 --> 00:24:46,960
application workflows

00:24:44,480 --> 00:24:47,760
these demonstrate how powertime was

00:24:46,960 --> 00:24:53,360
truly built

00:24:47,760 --> 00:24:53,360

YouTube URL: https://www.youtube.com/watch?v=27VRdI2BGWg


