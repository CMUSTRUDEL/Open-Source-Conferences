Title: This presentation will explain why OpenPOWER is the ideal platform for in-memory data analytics
Publication date: 2016-04-29
Playlist: OpenPOWER Summit 2016
Description: 
	OpenPOWER Summit 2016
Presented by Lorena Pesantez and Raj Krishnamurth of IBM
Captions: 
	00:00:00,000 --> 00:00:04,950
hi everybody so yes I'm Lorena and I'm

00:00:03,240 --> 00:00:08,400
gonna be presenting work that was done

00:00:04,950 --> 00:00:10,139
by Randy Swanberg and others in

00:00:08,400 --> 00:00:14,340
different places on the open bar

00:00:10,139 --> 00:00:15,870
platform so I have some notices there

00:00:14,340 --> 00:00:19,289
that those of you who download the

00:00:15,870 --> 00:00:20,850
charts can look at so first I assume

00:00:19,289 --> 00:00:24,230
most of you are familiar with spark but

00:00:20,850 --> 00:00:27,869
if you're not at a very high level is a

00:00:24,230 --> 00:00:31,260
big data and analytics platform it's

00:00:27,869 --> 00:00:33,989
open source of course so it combines you

00:00:31,260 --> 00:00:35,670
know you can have SQL streaming machine

00:00:33,989 --> 00:00:38,340
learning and graph only the same

00:00:35,670 --> 00:00:39,420
infrastructure and you can work with all

00:00:38,340 --> 00:00:43,070
the different light with a bunch of

00:00:39,420 --> 00:00:46,079
different languages Java Python SQL are

00:00:43,070 --> 00:00:49,200
and it's the really cool thing about it

00:00:46,079 --> 00:00:51,390
it's in memory so that makes it perform

00:00:49,200 --> 00:00:55,860
really well so we have a graph here

00:00:51,390 --> 00:00:59,070
comparing it to Hadoop so runtime hires

00:00:55,860 --> 00:01:01,260
were so 110 seconds for had to open the

00:00:59,070 --> 00:01:03,660
line for sparks almost invisible so that

00:01:01,260 --> 00:01:07,740
shows how just how how much better it

00:01:03,660 --> 00:01:10,470
performs compared to spar to Hadoop so

00:01:07,740 --> 00:01:13,170
that's about spark so I'm about to show

00:01:10,470 --> 00:01:15,869
you some results on the open bar

00:01:13,170 --> 00:01:18,450
platform and this just outlines about

00:01:15,869 --> 00:01:21,990
the experiments where we use the spark

00:01:18,450 --> 00:01:24,479
bench if a few different these are the

00:01:21,990 --> 00:01:27,990
three families of workloads that we use

00:01:24,479 --> 00:01:29,070
of machine learning SQL and graph three

00:01:27,990 --> 00:01:31,950
different workloads on each of those

00:01:29,070 --> 00:01:35,090
areas and the hardware we use were was

00:01:31,950 --> 00:01:39,540
seven note clusters of Intel Haswell

00:01:35,090 --> 00:01:47,810
2620 and our scale out power 8 system

00:01:39,540 --> 00:01:47,810
with 10 cores so here hear the results

00:01:48,020 --> 00:01:56,159
but that's that chart speaks for itself

00:01:50,549 --> 00:01:59,640
but so on average were 1.7 1.7 times the

00:01:56,159 --> 00:02:03,990
the Intel performance and you can see on

00:01:59,640 --> 00:02:07,140
each so we can say between like 1.3 x +

00:02:03,990 --> 00:02:10,920
2 and a half X what the inter-cluster

00:02:07,140 --> 00:02:15,590
did and this is without using any of the

00:02:10,920 --> 00:02:15,590
additional open power innovation

00:02:17,960 --> 00:02:25,500
so on the next chart we have a price

00:02:21,600 --> 00:02:29,340
performance and again this sort of

00:02:25,500 --> 00:02:31,140
speaks for itself but so you can look at

00:02:29,340 --> 00:02:33,870
it from two angles you can spend thirty

00:02:31,140 --> 00:02:35,220
three percent less on infrastructure and

00:02:33,870 --> 00:02:37,190
get the same performance or you can

00:02:35,220 --> 00:02:40,170
spend the same and get fifty percent

00:02:37,190 --> 00:02:50,780
more performance more throughput out of

00:02:40,170 --> 00:02:56,790
your cluster so why why do we do so well

00:02:50,780 --> 00:02:58,620
here's some of the reasons our stream

00:02:56,790 --> 00:03:00,600
and SQL benefit of course from my heart

00:02:58,620 --> 00:03:04,290
for my Chi thread density and

00:03:00,600 --> 00:03:06,570
concurrency so we have smt eight on 10

00:03:04,290 --> 00:03:09,290
or up to 12 cores on our on our scale

00:03:06,570 --> 00:03:12,960
out systems and scale up systems as well

00:03:09,290 --> 00:03:15,240
really powerful threads so we can

00:03:12,960 --> 00:03:17,310
process lots of Prayer lots of

00:03:15,240 --> 00:03:21,810
parallelism versus multiple things at a

00:03:17,310 --> 00:03:23,220
time and different stages at a time and

00:03:21,810 --> 00:03:26,370
then of course we have large caches

00:03:23,220 --> 00:03:30,060
memory and and lots of memory bandwidth

00:03:26,370 --> 00:03:34,400
available it's close to 200 gigabytes

00:03:30,060 --> 00:03:34,400
per second on this particular system

00:03:35,090 --> 00:03:41,790
which of course it's it's huge for

00:03:37,320 --> 00:03:43,560
machine machine learning and then that

00:03:41,790 --> 00:03:45,540
means that your pipeline your core is

00:03:43,560 --> 00:03:47,340
spending more time doing work been

00:03:45,540 --> 00:03:50,970
waiting for data to come from the memory

00:03:47,340 --> 00:03:53,010
subsystem and then the graph workload

00:03:50,970 --> 00:03:55,350
also benefits from both of those the

00:03:53,010 --> 00:04:00,270
large caches memory bandwidth and the

00:03:55,350 --> 00:04:01,770
potent thread and core we have and of

00:04:00,270 --> 00:04:09,150
course we have flexibility to go from

00:04:01,770 --> 00:04:12,780
smt single thread smt to a 70 478 that's

00:04:09,150 --> 00:04:13,920
that okay so if you like what you saw

00:04:12,780 --> 00:04:17,280
you're going to be really excited about

00:04:13,920 --> 00:04:19,320
the next couple of slides so this is

00:04:17,280 --> 00:04:21,150
what happens when we start playing with

00:04:19,320 --> 00:04:24,690
some of the open power innovation with

00:04:21,150 --> 00:04:27,270
this workload so I'll start with the

00:04:24,690 --> 00:04:30,449
left with your left here

00:04:27,270 --> 00:04:33,539
so 4 times the memo reduction or equal

00:04:30,449 --> 00:04:35,940
performance so this means here the

00:04:33,539 --> 00:04:38,039
y-axis higher is worse this is runtime

00:04:35,940 --> 00:04:41,400
so lower is better so we're showing that

00:04:38,039 --> 00:04:43,860
when we add Cathy flash like about 2

00:04:41,400 --> 00:04:45,419
gigabytes of memory you get the same

00:04:43,860 --> 00:04:48,389
performance that you get with discs on

00:04:45,419 --> 00:04:52,409
like eight gigabytes so that's where the

00:04:48,389 --> 00:04:55,349
forks comes from and so that that means

00:04:52,409 --> 00:04:59,009
a lot of money savings if you care about

00:04:55,349 --> 00:05:02,970
that because flash is a lot cheaper than

00:04:59,009 --> 00:05:06,270
memory and then the other exciting thing

00:05:02,970 --> 00:05:08,729
is enabling RDMA for this workload and

00:05:06,270 --> 00:05:11,990
here are just three sample queries that

00:05:08,729 --> 00:05:15,870
we have showed thirty percent better

00:05:11,990 --> 00:05:19,110
performance at the same time lower lower

00:05:15,870 --> 00:05:22,590
utilization lower memory footprint these

00:05:19,110 --> 00:05:25,289
are all great things and and this

00:05:22,590 --> 00:05:28,680
doesn't even use the copy technology

00:05:25,289 --> 00:05:31,560
this is just on the mellanox technology

00:05:28,680 --> 00:05:33,300
now so there's work being done to to add

00:05:31,560 --> 00:05:41,430
in copying to make this even more

00:05:33,300 --> 00:05:44,370
exciting okay this one also serves

00:05:41,430 --> 00:05:47,039
speaks for itself so once we add GPUs to

00:05:44,370 --> 00:05:48,479
this this is by the way an example of

00:05:47,039 --> 00:05:53,550
this is an adverse drug reaction

00:05:48,479 --> 00:05:54,990
prediction built on spark and it's

00:05:53,550 --> 00:05:57,210
pretty clear there so now we're looking

00:05:54,990 --> 00:06:00,659
at throughput so higher is better and we

00:05:57,210 --> 00:06:02,340
see the example in the red box we're

00:06:00,659 --> 00:06:04,610
done many many times better with the

00:06:02,340 --> 00:06:07,380
ones we enable GPU acceleration and

00:06:04,610 --> 00:06:13,050
overall it's like four times better for

00:06:07,380 --> 00:06:15,840
the for the total yeah so 25 x speed up

00:06:13,050 --> 00:06:20,520
on that one stage of the learn building

00:06:15,840 --> 00:06:22,229
model and again this is this is using

00:06:20,520 --> 00:06:24,259
very basic technology doesn't include

00:06:22,229 --> 00:06:27,659
the envy link which was put into our

00:06:24,259 --> 00:06:30,930
next generation platform so you can

00:06:27,659 --> 00:06:33,860
expect this to get get much better in

00:06:30,930 --> 00:06:33,860
the future in the near future

00:06:35,610 --> 00:06:44,530
so to summarize you know spark is very

00:06:41,020 --> 00:06:46,539
powerful disruptive technology open

00:06:44,530 --> 00:06:51,430
source so we expect to see lots of

00:06:46,539 --> 00:06:52,870
growth in the near future and we've

00:06:51,430 --> 00:06:56,770
shown that it does really well on the up

00:06:52,870 --> 00:06:59,050
empower platform and there's even more

00:06:56,770 --> 00:07:02,409
innovation coming in using the new

00:06:59,050 --> 00:07:06,150
technologies such as copy and then be

00:07:02,409 --> 00:07:06,150

YouTube URL: https://www.youtube.com/watch?v=chbTAPr06Mg


