Title: OpenPOWER Summit 2016 - IBM POWER8 HPC System Accelerates Genomics Analysis with SMT8 Multithreading
Publication date: 2016-04-29
Playlist: OpenPOWER Summit 2016
Description: 
	Presented by Richard Platania (for Seung-Jong Park) of LSU
Captions: 
	00:00:00,260 --> 00:00:05,790
alright good afternoon or I guess almost

00:00:03,149 --> 00:00:08,429
good evening to everyone here today my

00:00:05,790 --> 00:00:10,980
name is Richard pathania the name is he

00:00:08,429 --> 00:00:12,929
up there dr. Park that is my advisor I'm

00:00:10,980 --> 00:00:15,630
a PhD student at Louisiana State

00:00:12,929 --> 00:00:17,820
University with this talk I kind of want

00:00:15,630 --> 00:00:19,770
to change gears a little bit because my

00:00:17,820 --> 00:00:21,900
research revolves mostly around

00:00:19,770 --> 00:00:24,810
developing frameworks and applications

00:00:21,900 --> 00:00:27,720
for large-scale scientific applications

00:00:24,810 --> 00:00:29,460
using big data technologies and running

00:00:27,720 --> 00:00:33,510
these over a cloud or other distributed

00:00:29,460 --> 00:00:35,670
cyber infrastructures so earlier you

00:00:33,510 --> 00:00:37,350
heard a lot about the in depths of all

00:00:35,670 --> 00:00:39,510
these technologies such as Kathy and

00:00:37,350 --> 00:00:41,129
powerade and a little bit about the

00:00:39,510 --> 00:00:42,629
benchmarks so instead I'm going to give

00:00:41,129 --> 00:00:44,850
a little bit of an overview on the

00:00:42,629 --> 00:00:47,489
performance using these technologies for

00:00:44,850 --> 00:00:50,190
this large-scale scientific application

00:00:47,489 --> 00:00:54,180
particularly in this case for genome

00:00:50,190 --> 00:00:56,520
assembly so there's three main things I

00:00:54,180 --> 00:00:57,809
want to talk about here the first is I'm

00:00:56,520 --> 00:00:59,850
just going to briefly introduce the

00:00:57,809 --> 00:01:01,829
genome assembly problem for any

00:00:59,850 --> 00:01:04,260
biologist the audience I apologize ahead

00:01:01,829 --> 00:01:06,210
of times if I oversimplifying the

00:01:04,260 --> 00:01:09,390
explanation I'm a computer scientist

00:01:06,210 --> 00:01:11,760
though so I tend to do that secondly I

00:01:09,390 --> 00:01:13,560
want to talk about the two phases of the

00:01:11,760 --> 00:01:15,210
genome assembly process that we have

00:01:13,560 --> 00:01:16,979
implemented here and the two different

00:01:15,210 --> 00:01:19,140
ways that we have accelerated them the

00:01:16,979 --> 00:01:22,280
first is going to be accelerating the

00:01:19,140 --> 00:01:24,869
graph construction process using power 8

00:01:22,280 --> 00:01:26,159
now the will get a little bit more into

00:01:24,869 --> 00:01:28,799
what the graph construction process is

00:01:26,159 --> 00:01:30,090
in a second year second we want to see

00:01:28,799 --> 00:01:32,220
how we can accelerate the graph

00:01:30,090 --> 00:01:37,650
simplification process with the capi

00:01:32,220 --> 00:01:40,320
flash so just an overall simplified view

00:01:37,650 --> 00:01:43,290
of the genome assembly problem so we

00:01:40,320 --> 00:01:45,840
have a bunch of clone genome samples and

00:01:43,290 --> 00:01:47,850
we want we what the scientists will do

00:01:45,840 --> 00:01:48,960
before they give them to us is it'll

00:01:47,850 --> 00:01:51,180
just break them up and do a bunch of

00:01:48,960 --> 00:01:52,829
smaller fragments so now it's our job to

00:01:51,180 --> 00:01:54,689
be able to take those small fragments

00:01:52,829 --> 00:01:57,119
and somehow be able to piece them back

00:01:54,689 --> 00:02:01,280
together in the correct order to be able

00:01:57,119 --> 00:02:03,270
to form a constructed genome again so

00:02:01,280 --> 00:02:04,950
firstly we could see what this is going

00:02:03,270 --> 00:02:06,630
to be a very computationally intensive

00:02:04,950 --> 00:02:08,970
endeavor trying to find the correct

00:02:06,630 --> 00:02:13,069
ordering of up to millions of these

00:02:08,970 --> 00:02:13,069
different short fragments is not easy

00:02:15,040 --> 00:02:18,860
so a few problems we can run into with

00:02:17,690 --> 00:02:21,379
this like I just said it's very

00:02:18,860 --> 00:02:23,569
computationally intensive but we also

00:02:21,379 --> 00:02:25,519
need to consider that the new the

00:02:23,569 --> 00:02:27,319
next-generation sequencing technologies

00:02:25,519 --> 00:02:29,629
that have recently been developed such

00:02:27,319 --> 00:02:31,640
as alumina things like that they are

00:02:29,629 --> 00:02:34,099
outpacing Moore's Law I know we've

00:02:31,640 --> 00:02:37,250
talked about Morris lot plenty today so

00:02:34,099 --> 00:02:39,680
it's becoming so cheap to be able to do

00:02:37,250 --> 00:02:42,590
DNA sequencing that they're having a

00:02:39,680 --> 00:02:43,970
throughput on all these sequences at an

00:02:42,590 --> 00:02:46,540
unprecedented rate where people

00:02:43,970 --> 00:02:48,650
especially biologists can't handle

00:02:46,540 --> 00:02:51,109
processing all this data so that's

00:02:48,650 --> 00:02:52,760
really turn to other large-scale

00:02:51,109 --> 00:02:54,140
applications people who develop these or

00:02:52,760 --> 00:02:57,560
scale applications to be able to

00:02:54,140 --> 00:03:00,549
assemble these all these Reed's or all

00:02:57,560 --> 00:03:03,709
these pieces of DNA back into the genome

00:03:00,549 --> 00:03:05,420
so I don't the problem is in order to do

00:03:03,709 --> 00:03:07,790
this we need some software with extreme

00:03:05,420 --> 00:03:09,560
scalability it's not necessarily easy to

00:03:07,790 --> 00:03:12,019
develop especially if you're just the

00:03:09,560 --> 00:03:15,530
biologist not the biologists cannot code

00:03:12,019 --> 00:03:17,090
but yeah getting these soft words with

00:03:15,530 --> 00:03:20,480
extreme to scalability it can be very

00:03:17,090 --> 00:03:21,799
difficult and the third is we need to

00:03:20,480 --> 00:03:24,560
have this hardware that's going to give

00:03:21,799 --> 00:03:26,599
us what we need and for the applications

00:03:24,560 --> 00:03:29,209
such as more compute cycles we need

00:03:26,599 --> 00:03:31,720
extreme IO performance and we need huge

00:03:29,209 --> 00:03:33,859
storage space because all these

00:03:31,720 --> 00:03:35,840
next-generation sequencing technologies

00:03:33,859 --> 00:03:38,090
are producing such a large amount of

00:03:35,840 --> 00:03:41,000
data on the scale of terabytes at this

00:03:38,090 --> 00:03:42,950
point making this also a very data

00:03:41,000 --> 00:03:47,030
intensive endeavor in addition to

00:03:42,950 --> 00:03:50,690
computationally intensive endeavor so

00:03:47,030 --> 00:03:54,169
our initial solution we had here was for

00:03:50,690 --> 00:03:55,639
the graph building process we use Hadoop

00:03:54,169 --> 00:03:58,099
MapReduce very common Big Data

00:03:55,639 --> 00:04:00,230
technology in order to implement the

00:03:58,099 --> 00:04:01,910
first phase of the genome assembly

00:04:00,230 --> 00:04:04,519
process so what we do here is we

00:04:01,910 --> 00:04:06,919
basically have these fragments which you

00:04:04,519 --> 00:04:09,109
see on the very left that we're given

00:04:06,919 --> 00:04:11,870
the biologists will give to us our job

00:04:09,109 --> 00:04:13,970
is to break those up into a little bit

00:04:11,870 --> 00:04:15,590
smaller fragments called k-mers and

00:04:13,970 --> 00:04:18,109
that's what we'll do in the first map

00:04:15,590 --> 00:04:20,449
phase from there we're just going to

00:04:18,109 --> 00:04:23,780
combine all these different cameras that

00:04:20,449 --> 00:04:25,130
have the same value into in the reduced

00:04:23,780 --> 00:04:26,660
phase to be able to get

00:04:25,130 --> 00:04:28,580
all the different edges that would

00:04:26,660 --> 00:04:30,650
produce Lee graph so overall this is a

00:04:28,580 --> 00:04:33,170
very embarrassing Lee parallel process

00:04:30,650 --> 00:04:35,210
each of these reads can be processed on

00:04:33,170 --> 00:04:36,710
its own without any dependent on any of

00:04:35,210 --> 00:04:39,230
the other reads so that's why the

00:04:36,710 --> 00:04:41,150
MapReduce paradigm is very very great

00:04:39,230 --> 00:04:47,090
for this great for a batch processing

00:04:41,150 --> 00:04:48,770
data intensive jobs so how do we want to

00:04:47,090 --> 00:04:50,600
speed this up the first thing we did is

00:04:48,770 --> 00:04:53,060
the folks the IBM folks at poughkeepsie

00:04:50,600 --> 00:04:55,850
new york we're kind enough to lend us a

00:04:53,060 --> 00:04:57,830
40 node cluster there with their power 8

00:04:55,850 --> 00:04:59,870
servers so we wanted to see how we could

00:04:57,830 --> 00:05:02,960
take our Hadoop based implementation of

00:04:59,870 --> 00:05:04,940
the scrap construction and run it on top

00:05:02,960 --> 00:05:10,400
of the power rate technologies to see

00:05:04,940 --> 00:05:11,240
what kind of speed up wouldn't get so we

00:05:10,400 --> 00:05:13,700
wanted to compare it to something

00:05:11,240 --> 00:05:18,620
obviously at LSU we have our super Mike

00:05:13,700 --> 00:05:21,650
to cluster and at that cluster it's we

00:05:18,620 --> 00:05:23,630
wanted to compare it to the Intel CPUs

00:05:21,650 --> 00:05:26,600
that we had there so in the Poughkeepsie

00:05:23,630 --> 00:05:28,910
cluster we have to 10 core power 8 CPUs

00:05:26,600 --> 00:05:31,700
and at the super Mike cluster we have to

00:05:28,910 --> 00:05:33,260
eight core intel cpus so obviously we

00:05:31,700 --> 00:05:35,870
see we have a few more cores with the

00:05:33,260 --> 00:05:38,690
power 8 but you'll also notice that

00:05:35,870 --> 00:05:41,210
we're only using 40 nodes of the pic

00:05:38,690 --> 00:05:42,860
ypsi cluster we're using up to 120 at

00:05:41,210 --> 00:05:44,930
the super my cluster so still the

00:05:42,860 --> 00:05:48,530
overall number of cores are way higher

00:05:44,930 --> 00:05:50,300
when using the super my cluster now

00:05:48,530 --> 00:05:51,890
another thing to note out of this is

00:05:50,300 --> 00:05:53,990
we're gonna be able to take advantage of

00:05:51,890 --> 00:05:55,610
the SMT that power 8 will give us all

00:05:53,990 --> 00:05:58,220
the way up to eight so we're going to be

00:05:55,610 --> 00:05:59,660
able to configure Hadoop to use many

00:05:58,220 --> 00:06:01,910
more virtual cores than it normally

00:05:59,660 --> 00:06:04,010
would so on the super mech cluster each

00:06:01,910 --> 00:06:05,840
node has 16 cores we can only take

00:06:04,010 --> 00:06:07,670
advantage of 16 virtual cores for

00:06:05,840 --> 00:06:09,680
running Hadoop here if we use the power

00:06:07,670 --> 00:06:15,860
8 we can take advantage of all the way

00:06:09,680 --> 00:06:17,300
up to 160 if we're using 8 smt there's

00:06:15,860 --> 00:06:19,250
three different data sets we use for

00:06:17,300 --> 00:06:21,590
these experiments the very first is just

00:06:19,250 --> 00:06:24,470
a smaller data set only 12 gigabytes in

00:06:21,590 --> 00:06:26,480
size rice genome we also use a bumblebee

00:06:24,470 --> 00:06:28,790
genome data set which is 90 gigabytes

00:06:26,480 --> 00:06:30,620
and the final data set we used was a

00:06:28,790 --> 00:06:32,840
meta-genome data set which is a

00:06:30,620 --> 00:06:34,820
staggering 3.2 terabytes in this case

00:06:32,840 --> 00:06:37,340
now you may notice there are two

00:06:34,820 --> 00:06:38,540
additional columns here not just the

00:06:37,340 --> 00:06:40,580
input size we

00:06:38,540 --> 00:06:42,860
shuffle data size and the output size

00:06:40,580 --> 00:06:45,140
the output size just gives us the actual

00:06:42,860 --> 00:06:47,390
size of the graph once we're done the

00:06:45,140 --> 00:06:49,280
shovel data size that is the

00:06:47,390 --> 00:06:51,770
intermediate size of the data when we're

00:06:49,280 --> 00:06:53,810
going through the MapReduce phases so

00:06:51,770 --> 00:06:55,670
between map and reduce there's a shuffle

00:06:53,810 --> 00:06:57,680
phase in which we're sending data across

00:06:55,670 --> 00:06:59,930
the network and combining all the

00:06:57,680 --> 00:07:01,280
results so we could see we go with the

00:06:59,930 --> 00:07:03,200
meta-genome from three point two

00:07:01,280 --> 00:07:04,520
terabytes all the way up to eight point

00:07:03,200 --> 00:07:08,060
or I'm sorry oh we have to 20 terabytes

00:07:04,520 --> 00:07:10,040
of intermediate data this is a huge

00:07:08,060 --> 00:07:12,320
amount that needs to be processed so

00:07:10,040 --> 00:07:14,420
we're hoping that using these powerade

00:07:12,320 --> 00:07:16,250
technologies especially with the SMT

00:07:14,420 --> 00:07:20,990
we'd be able to speed up processing all

00:07:16,250 --> 00:07:23,270
of this data so just some basic Hadoop

00:07:20,990 --> 00:07:25,640
configurations not that exciting to look

00:07:23,270 --> 00:07:27,500
at these again the main thing to take

00:07:25,640 --> 00:07:29,330
out of this is we see with the IBM

00:07:27,500 --> 00:07:32,060
power8 we're able to configure to use up

00:07:29,330 --> 00:07:37,100
to 120 virtual cores per node instead of

00:07:32,060 --> 00:07:38,750
just 16 with the super my cluster so the

00:07:37,100 --> 00:07:41,570
first test we are in Tehran was just

00:07:38,750 --> 00:07:44,060
testing the scalability of smt with

00:07:41,570 --> 00:07:46,070
Hadoop so for this we just set up a two

00:07:44,060 --> 00:07:49,910
nodes out of the 40 nodes epic ypsi and

00:07:46,070 --> 00:07:51,890
we kept increasing the SMT just using

00:07:49,910 --> 00:07:53,480
the small rice genome data set and what

00:07:51,890 --> 00:07:56,030
we found is we got pretty great

00:07:53,480 --> 00:07:59,030
scalability results out of this not

00:07:56,030 --> 00:08:02,180
completely linear but almost almost on

00:07:59,030 --> 00:08:03,650
par with that so for the rest of the

00:08:02,180 --> 00:08:05,390
experiments we obviously are going to

00:08:03,650 --> 00:08:07,400
end up using that smta to take advantage

00:08:05,390 --> 00:08:09,260
of that so that's already we're seeing

00:08:07,400 --> 00:08:13,550
an acceleration there with our genome

00:08:09,260 --> 00:08:16,160
assembly so next we wanted to compare

00:08:13,550 --> 00:08:19,460
the overall execution times using the

00:08:16,160 --> 00:08:21,710
Arak cluster at LSU superbike as well as

00:08:19,460 --> 00:08:23,720
the pickup see cluster here so for the

00:08:21,710 --> 00:08:27,050
12 gigabyte rice genome data set we

00:08:23,720 --> 00:08:29,690
found that we were able to achieve 7.5

00:08:27,050 --> 00:08:32,060
times performance improvement per node

00:08:29,690 --> 00:08:35,120
using the poughkeepsie clusters with the

00:08:32,060 --> 00:08:36,919
powerade servers that is a lot I mean we

00:08:35,120 --> 00:08:39,680
can already see using six super Mike

00:08:36,919 --> 00:08:43,070
nodes compared to to power 8 nodes the

00:08:39,680 --> 00:08:44,780
time is less than cut in half and if we

00:08:43,070 --> 00:08:47,580
compare it on the level of performance

00:08:44,780 --> 00:08:49,500
per server we get a 7.5 times increase

00:08:47,580 --> 00:08:51,360
so that's great in all we see that for a

00:08:49,500 --> 00:08:52,920
12 gigabyte data set we want to keep

00:08:51,360 --> 00:08:55,290
going we want to see how we can do for

00:08:52,920 --> 00:08:57,030
the actual big data because we're

00:08:55,290 --> 00:09:00,030
considering big data 12 gigabytes is not

00:08:57,030 --> 00:09:03,120
not really anything so we move up next

00:09:00,030 --> 00:09:05,610
to the bumble gene be genome so this one

00:09:03,120 --> 00:09:07,650
was 90 gigabytes and had a graph size of

00:09:05,610 --> 00:09:09,630
I believe it was a little over 90

00:09:07,650 --> 00:09:11,460
gigabytes in size here we see the same

00:09:09,630 --> 00:09:13,350
result in this case though we end up

00:09:11,460 --> 00:09:15,930
using all 120 nodes of the super my

00:09:13,350 --> 00:09:17,880
cluster and we use 40 nodes of the kitty

00:09:15,930 --> 00:09:21,420
cluster so there was three times the

00:09:17,880 --> 00:09:22,860
nodes at super Mike we can't get close

00:09:21,420 --> 00:09:26,160
to the power 8 performance we're seeing

00:09:22,860 --> 00:09:28,590
here IBM power8 with 40 nodes is less

00:09:26,160 --> 00:09:30,810
than half the time and if we look at it

00:09:28,590 --> 00:09:32,610
in terms of per server performance we

00:09:30,810 --> 00:09:36,120
end up getting again at 7.5 times

00:09:32,610 --> 00:09:38,400
improvement now moving on to most

00:09:36,120 --> 00:09:40,470
important one the meta-genome data set

00:09:38,400 --> 00:09:42,390
so this one was 3.2 terabytes in size

00:09:40,470 --> 00:09:44,580
the intermediate data size was 20

00:09:42,390 --> 00:09:48,750
terabytes and the final graph output

00:09:44,580 --> 00:09:51,570
size was 8.6 terabytes again we used 120

00:09:48,750 --> 00:09:54,090
nodes on our super my cluster and 40

00:09:51,570 --> 00:09:55,890
nodes of the IBM power8 what we found

00:09:54,090 --> 00:09:57,690
here is we ended up getting those nine

00:09:55,890 --> 00:10:00,420
times improvement in terms of

00:09:57,690 --> 00:10:02,280
performance per server that is great

00:10:00,420 --> 00:10:06,210
that's great for us because I mean this

00:10:02,280 --> 00:10:09,030
already takes 6.5 hours on the gypsy

00:10:06,210 --> 00:10:11,070
cluster the IBM power8 cluster so I'm

00:10:09,030 --> 00:10:12,690
super my cluster it took us way too long

00:10:11,070 --> 00:10:15,210
to run these experiments and we needed

00:10:12,690 --> 00:10:17,040
120 nodes to be able to do this so this

00:10:15,210 --> 00:10:20,430
has really enabled us to be able to run

00:10:17,040 --> 00:10:24,690
more of these experiments much more much

00:10:20,430 --> 00:10:27,060
more quickly so next we want to get into

00:10:24,690 --> 00:10:28,650
the actual so we have these graphs for

00:10:27,060 --> 00:10:30,150
the genome but that doesn't really mean

00:10:28,650 --> 00:10:32,010
much to us we need to actually get

00:10:30,150 --> 00:10:33,900
pieces of the genome out of it that we

00:10:32,010 --> 00:10:35,400
can give back to the biologists so very

00:10:33,900 --> 00:10:37,230
simplified explanation of what we need

00:10:35,400 --> 00:10:39,960
to do here is we need to take these

00:10:37,230 --> 00:10:42,780
graphs and sort of just Traverse them to

00:10:39,960 --> 00:10:44,850
get linear chains of the genome which we

00:10:42,780 --> 00:10:47,690
can refer to as context oh they're just

00:10:44,850 --> 00:10:50,100
larger pieces of the DNA sequence

00:10:47,690 --> 00:10:51,930
another way to view this process is if

00:10:50,100 --> 00:10:53,970
we just have a giant graph imagine we

00:10:51,930 --> 00:10:56,960
just want to squash it down into as many

00:10:53,970 --> 00:10:59,580
linear chains as we can I'll put those

00:10:56,960 --> 00:11:01,350
speeding up this process we couldn't

00:10:59,580 --> 00:11:03,510
directly use map of how to mapper to

00:11:01,350 --> 00:11:06,960
is because as we can see here if we need

00:11:03,510 --> 00:11:08,820
to traverse a graph like this each data

00:11:06,960 --> 00:11:10,620
point is very dependent on each other so

00:11:08,820 --> 00:11:12,180
using batch processing like something

00:11:10,620 --> 00:11:13,950
with MapReduce doesn't work out so well

00:11:12,180 --> 00:11:16,110
so instead we look towards using

00:11:13,950 --> 00:11:19,230
distributed key value store to be able

00:11:16,110 --> 00:11:21,090
to scale out and solve this problem now

00:11:19,230 --> 00:11:25,200
when we wanted to speed it up even more

00:11:21,090 --> 00:11:27,230
we took it to IBM again and we took

00:11:25,200 --> 00:11:31,590
advantage of their Cappy Cappy

00:11:27,230 --> 00:11:33,240
accelerated flash so here what we're

00:11:31,590 --> 00:11:36,390
going to do is we actually take Retta

00:11:33,240 --> 00:11:39,330
slabs we used their software on top of a

00:11:36,390 --> 00:11:41,670
single for now just a single powerade

00:11:39,330 --> 00:11:43,830
node with 20 cores and we're able to run

00:11:41,670 --> 00:11:48,390
this graph simplification process just

00:11:43,830 --> 00:11:52,170
one node and only 7.5 hours with 500

00:11:48,390 --> 00:11:53,880
gigabytes of graph data 7.5 hours is not

00:11:52,170 --> 00:11:55,650
our ideal time but the point is here

00:11:53,880 --> 00:11:57,840
we're able to do this on just a single

00:11:55,650 --> 00:12:00,240
node which is nowhere near possible with

00:11:57,840 --> 00:12:01,590
our previous resources obviously we get

00:12:00,240 --> 00:12:04,320
a little bit of a trade-off here using

00:12:01,590 --> 00:12:06,210
the capi accelerated flash as main

00:12:04,320 --> 00:12:07,500
memory and main memory in this case we

00:12:06,210 --> 00:12:09,240
all know it's not actually main memory

00:12:07,500 --> 00:12:11,520
but it's a very good trade-off here

00:12:09,240 --> 00:12:13,950
because this is enabling us to do it on

00:12:11,520 --> 00:12:15,420
single node and in the future we hope to

00:12:13,950 --> 00:12:18,410
do it actually with multiple nodes to

00:12:15,420 --> 00:12:20,220
show just a very good increase on a

00:12:18,410 --> 00:12:24,540
acceleration with the grab some flow

00:12:20,220 --> 00:12:25,740
graph simplification so to kind of start

00:12:24,540 --> 00:12:27,990
closing up a little bit I just want to

00:12:25,740 --> 00:12:29,010
talk about where we're going towards in

00:12:27,990 --> 00:12:31,830
the future what we're planning on

00:12:29,010 --> 00:12:34,380
working on so the main challenges we've

00:12:31,830 --> 00:12:37,230
run into are the graph building process

00:12:34,380 --> 00:12:39,390
is so expensive that dominates the

00:12:37,230 --> 00:12:41,760
entire pipeline of genome assembly for

00:12:39,390 --> 00:12:43,820
us and from what we've seen other genome

00:12:41,760 --> 00:12:47,190
assemblers have this exact same problem

00:12:43,820 --> 00:12:48,780
the graph building phase not only takes

00:12:47,190 --> 00:12:52,980
the most resources but it also takes the

00:12:48,780 --> 00:12:54,840
most time so we had two solutions that

00:12:52,980 --> 00:12:56,640
we came up with initially we could

00:12:54,840 --> 00:12:58,320
either use a single machine and scale

00:12:56,640 --> 00:12:59,580
that up with a ton of memory although

00:12:58,320 --> 00:13:01,080
it's difficult that's not widely

00:12:59,580 --> 00:13:02,790
available especially if we're dealing

00:13:01,080 --> 00:13:04,770
with the meta-genome we don't

00:13:02,790 --> 00:13:08,190
necessarily have 20 terabytes of memory

00:13:04,770 --> 00:13:09,720
at a single node the other one is to run

00:13:08,190 --> 00:13:12,330
on a cluster and continue scaling out

00:13:09,720 --> 00:13:13,440
but as we can see we reach the limit

00:13:12,330 --> 00:13:14,170
with our hundred twenty nodes on the

00:13:13,440 --> 00:13:16,269
super my clothes

00:13:14,170 --> 00:13:17,620
sir we can't keep scaling out with that

00:13:16,269 --> 00:13:20,769
because we can't keep getting more

00:13:17,620 --> 00:13:23,800
resources getting over 120 nodes and a

00:13:20,769 --> 00:13:25,540
single cluster is not quite so easy so

00:13:23,800 --> 00:13:27,639
instead we decided to use this Cappy

00:13:25,540 --> 00:13:33,190
accelerated flash to also implement our

00:13:27,639 --> 00:13:34,810
graph building phase so this is a very

00:13:33,190 --> 00:13:36,910
imp regress phase here but basically

00:13:34,810 --> 00:13:39,250
what we want to be doing is instead of

00:13:36,910 --> 00:13:40,810
having the MapReduce functionality we

00:13:39,250 --> 00:13:42,940
have the same functionality we had in

00:13:40,810 --> 00:13:46,449
the map and then we're also going to

00:13:42,940 --> 00:13:50,850
access the copy to do all of the sorting

00:13:46,449 --> 00:13:53,560
and storing of all the graph information

00:13:50,850 --> 00:13:56,529
so there's some initial results we got

00:13:53,560 --> 00:13:58,779
from that we ran the 8090 gigabyte or

00:13:56,529 --> 00:14:01,600
sorry 85 gigabyte bumblebee genome data

00:13:58,779 --> 00:14:04,269
set on an eight node cluster that we

00:14:01,600 --> 00:14:06,279
have at LSU and we also ran it on a

00:14:04,269 --> 00:14:09,730
single power right node that's

00:14:06,279 --> 00:14:12,310
accelerated with cappy flash both both

00:14:09,730 --> 00:14:14,290
clusters have 20 cores per node and what

00:14:12,310 --> 00:14:15,760
we ended up finding is it took a little

00:14:14,290 --> 00:14:17,949
bit longer using the capi accelerated

00:14:15,760 --> 00:14:21,190
flash server but in terms of speed up

00:14:17,949 --> 00:14:24,070
per node we ended up receiving 3.5 times

00:14:21,190 --> 00:14:26,110
increase in performance so in the future

00:14:24,070 --> 00:14:28,240
if we're able to get more nodes with

00:14:26,110 --> 00:14:29,980
access to this cappy flash storage it's

00:14:28,240 --> 00:14:32,890
obviously going to show a huge

00:14:29,980 --> 00:14:36,180
acceleration with doing the grab some

00:14:32,890 --> 00:14:38,079
graph building with the cabbie flash and

00:14:36,180 --> 00:14:40,529
that's all I have for today any

00:14:38,079 --> 00:14:40,529

YouTube URL: https://www.youtube.com/watch?v=KeuBmo5ibUo


