Title: OpenPOWER Summit 2016 - PGI Accelerator Fortran C C++ Compilers for OpenPOWER+Tesla
Publication date: 2016-04-29
Playlist: OpenPOWER Summit 2016
Description: 
	Presented by Doug Miles of PGI
Captions: 
	00:00:00,030 --> 00:00:06,690
okay I am Doug miles with the PGI

00:00:03,449 --> 00:00:09,210
compilers and tools group at Nvidia we

00:00:06,690 --> 00:00:10,110
developed Fortran C and C++ compilers

00:00:09,210 --> 00:00:12,300
for the high-performance computing

00:00:10,110 --> 00:00:15,890
market today those compilers are

00:00:12,300 --> 00:00:17,820
available on x86 CPUs and NVIDIA GPUs a

00:00:15,890 --> 00:00:21,720
little over a year ago we started

00:00:17,820 --> 00:00:23,189
retargeting them to open power CPUs and

00:00:21,720 --> 00:00:25,830
I'm going to give you an update on our

00:00:23,189 --> 00:00:29,340
progress there we will have a beta

00:00:25,830 --> 00:00:30,960
release in May and I'm going to review

00:00:29,340 --> 00:00:33,480
what we hope to bring to the software

00:00:30,960 --> 00:00:36,690
development ecosystem and give some

00:00:33,480 --> 00:00:39,180
early performance results so high

00:00:36,690 --> 00:00:41,190
performance computing is expanding as

00:00:39,180 --> 00:00:43,590
you've seen this week deep learning is

00:00:41,190 --> 00:00:46,829
really driving that expansion the part

00:00:43,590 --> 00:00:48,329
of the market that we cater to is the

00:00:46,829 --> 00:00:51,059
classic high performance computing

00:00:48,329 --> 00:00:54,989
market engineering and science sites

00:00:51,059 --> 00:00:58,250
that do weather forecasting climate

00:00:54,989 --> 00:01:01,410
modeling structural analysis and

00:00:58,250 --> 00:01:04,500
automotive crash testing higher energy

00:01:01,410 --> 00:01:06,450
physics classic HPC science and

00:01:04,500 --> 00:01:09,390
engineering applications and and the key

00:01:06,450 --> 00:01:12,330
factor about these applications is a lot

00:01:09,390 --> 00:01:13,740
of them are proprietary or research

00:01:12,330 --> 00:01:16,140
community applications that are

00:01:13,740 --> 00:01:19,110
continually evolving they're used in

00:01:16,140 --> 00:01:20,640
source form at these sites which modify

00:01:19,110 --> 00:01:23,369
them in various ways based on the

00:01:20,640 --> 00:01:25,770
science they're doing and so they need

00:01:23,369 --> 00:01:27,000
to be compiled over and over and over

00:01:25,770 --> 00:01:29,970
again they're primarily written in

00:01:27,000 --> 00:01:33,240
Fortran C and C++ and there they need to

00:01:29,970 --> 00:01:35,970
be parallelized to run across hundreds

00:01:33,240 --> 00:01:39,479
or thousands or more processors using

00:01:35,970 --> 00:01:41,820
OpenMP or MPI and more and more and more

00:01:39,479 --> 00:01:46,350
they're being accelerated on GPUs using

00:01:41,820 --> 00:01:49,829
CUDA or open ACC so this is the the

00:01:46,350 --> 00:01:53,700
market space that that we are aiming at

00:01:49,829 --> 00:01:56,460
who are these customers in PGI's case

00:01:53,700 --> 00:02:00,770
it's nearly every US Department of

00:01:56,460 --> 00:02:04,920
Energy Laboratory DoD computing Center

00:02:00,770 --> 00:02:07,429
NASA cites NSF computing centers

00:02:04,920 --> 00:02:10,160
pharmaceutical companies oil companies

00:02:07,429 --> 00:02:13,900
and most of the international

00:02:10,160 --> 00:02:17,290
equivalents and so they they

00:02:13,900 --> 00:02:20,110
are the leaders in HPC and a lot of them

00:02:17,290 --> 00:02:21,579
drive HPC forward you've heard Fernanda

00:02:20,110 --> 00:02:24,790
talk earlier about the work that Oak

00:02:21,579 --> 00:02:25,989
Ridge is doing and in fact Oak Ridge and

00:02:24,790 --> 00:02:28,060
the Lawrence Livermore National

00:02:25,989 --> 00:02:30,040
Laboratory are the lead customers for

00:02:28,060 --> 00:02:32,709
PGI's open power compilers they have

00:02:30,040 --> 00:02:37,120
them today and are giving us feedback

00:02:32,709 --> 00:02:39,519
and are really driving us forward to to

00:02:37,120 --> 00:02:42,609
support that processor in a in a

00:02:39,519 --> 00:02:46,840
comprehensive way and why do these

00:02:42,609 --> 00:02:51,849
customers use HPC compilers or need HPC

00:02:46,840 --> 00:02:54,700
compilers the answer is performance this

00:02:51,849 --> 00:02:57,730
is a look at the spec OMP benchmarks

00:02:54,700 --> 00:03:01,030
which is a set of 14 science and

00:02:57,730 --> 00:03:03,220
engineering applications in this case we

00:03:01,030 --> 00:03:06,040
have production compilers on x86 so this

00:03:03,220 --> 00:03:09,790
is these benchmarks the average

00:03:06,040 --> 00:03:14,620
performance running across all 32 cores

00:03:09,790 --> 00:03:16,540
of a dual socket Haswell system and and

00:03:14,620 --> 00:03:19,260
the performance is normalized to GCC

00:03:16,540 --> 00:03:22,510
which is the infrastructure compilers

00:03:19,260 --> 00:03:25,090
GCC G Fortran g+ plus the infrastructure

00:03:22,510 --> 00:03:27,760
compilers for linux they're on every

00:03:25,090 --> 00:03:33,250
system so why do people use compilers

00:03:27,760 --> 00:03:35,079
from PGI or IBM or Intel or Cray the

00:03:33,250 --> 00:03:37,030
reason is there's there is in a lot of

00:03:35,079 --> 00:03:39,669
applications of performance premium and

00:03:37,030 --> 00:03:41,859
you'll see here that the Intel compilers

00:03:39,669 --> 00:03:45,069
which are obviously very highly tuned

00:03:41,859 --> 00:03:47,979
for Haswell deliver an average about 50

00:03:45,069 --> 00:03:50,799
percent higher performance than GCC

00:03:47,979 --> 00:03:53,290
across those 14 benchmarks and the PGI

00:03:50,799 --> 00:03:57,010
compilers are a little bit behind Intel

00:03:53,290 --> 00:04:00,010
but competitive and and this is an

00:03:57,010 --> 00:04:02,169
absolute requirement for an HPC compiler

00:04:00,010 --> 00:04:06,780
if you've spent millions or tens of

00:04:02,169 --> 00:04:10,209
millions of dollars on an HPC system and

00:04:06,780 --> 00:04:11,949
your application is of this profile well

00:04:10,209 --> 00:04:15,400
you're leaving a lot of performance on

00:04:11,949 --> 00:04:18,130
the floor gfortran might be the the most

00:04:15,400 --> 00:04:21,729
expensive piece of software that you you

00:04:18,130 --> 00:04:24,969
never bought but as we've seen while

00:04:21,729 --> 00:04:27,460
this is a requirement it's no longer

00:04:24,969 --> 00:04:29,259
enough more and more systems are Jeep

00:04:27,460 --> 00:04:33,210
you accelerated Jensen mentioned in his

00:04:29,259 --> 00:04:37,690
keynote yesterday that that now 97% of

00:04:33,210 --> 00:04:43,800
HPC systems include some GPUs in them

00:04:37,690 --> 00:04:46,090
and in the last top 500 list over 20% of

00:04:43,800 --> 00:04:48,759
the systems on that list

00:04:46,090 --> 00:04:52,690
derived their compute power primarily

00:04:48,759 --> 00:04:55,270
from GPUs and so GPUs are becoming more

00:04:52,690 --> 00:04:57,330
and more pervasive and a little over

00:04:55,270 --> 00:05:00,910
four years ago

00:04:57,330 --> 00:05:03,780
PGI together with Nvidia and a few other

00:05:00,910 --> 00:05:05,620
compiler vendors and several

00:05:03,780 --> 00:05:09,310
representatives from the user community

00:05:05,620 --> 00:05:13,210
defined open a cc which is a directive

00:05:09,310 --> 00:05:16,509
based model for parallelization of codes

00:05:13,210 --> 00:05:17,770
and really it's designed in a

00:05:16,509 --> 00:05:20,979
streamlined way so that you can

00:05:17,770 --> 00:05:23,440
parallelize code for offloading to a GPU

00:05:20,979 --> 00:05:27,970
but those same directives can be used to

00:05:23,440 --> 00:05:29,979
parallelize the code for a CPU and again

00:05:27,970 --> 00:05:35,080
we have this this capability available

00:05:29,979 --> 00:05:36,699
today it is a user driven standard every

00:05:35,080 --> 00:05:39,130
one of the applications you see they're

00:05:36,699 --> 00:05:42,099
listed on the the right-hand side of the

00:05:39,130 --> 00:05:44,800
screen let your left hand side is has

00:05:42,099 --> 00:05:46,810
contributed to improvements in

00:05:44,800 --> 00:05:49,479
performance or features in the compiler

00:05:46,810 --> 00:05:51,729
and several of them the ports were

00:05:49,479 --> 00:05:55,060
started at the hackathons that that

00:05:51,729 --> 00:05:57,580
Fernanda referenced in the talk that she

00:05:55,060 --> 00:06:02,889
kicked off early today and and the

00:05:57,580 --> 00:06:05,110
benefits of open ACC are portability so

00:06:02,889 --> 00:06:08,440
because these are directives pragmas and

00:06:05,110 --> 00:06:10,150
C or C++ directive comments in Fortran

00:06:08,440 --> 00:06:15,430
the code remains a hundred percent

00:06:10,150 --> 00:06:17,520
portable to any compiler or system so so

00:06:15,430 --> 00:06:21,430
you have the portability benefits and

00:06:17,520 --> 00:06:23,500
productivity wise relative to CUDA or

00:06:21,430 --> 00:06:27,190
OpenCL or a lower level programming

00:06:23,500 --> 00:06:31,659
model there is a huge benefit mark covet

00:06:27,190 --> 00:06:34,930
Noah in a talk he gave yesterday talked

00:06:31,659 --> 00:06:36,580
about porting some Fortran code to a GPU

00:06:34,930 --> 00:06:40,419
last week and he was able to move eight

00:06:36,580 --> 00:06:41,270
hundred lines of code from the CPU to a

00:06:40,419 --> 00:06:43,730
GPU

00:06:41,270 --> 00:06:45,590
in less than 10 minutes and if you've

00:06:43,730 --> 00:06:47,450
ever written a CUDA program or an open

00:06:45,590 --> 00:06:49,610
CL program you'll know that that you

00:06:47,450 --> 00:06:52,100
just can't do that and so the the

00:06:49,610 --> 00:06:56,690
productivity benefits of it are huge and

00:06:52,100 --> 00:06:59,420
and our goal is that eventually it's no

00:06:56,690 --> 00:07:02,420
more difficult to parallelize code for

00:06:59,420 --> 00:07:04,720
GPU than it is for a multi-core CPU and

00:07:02,420 --> 00:07:09,710
we're getting ever closer to that goal

00:07:04,720 --> 00:07:13,070
as we progress here and so all of this

00:07:09,710 --> 00:07:15,950
technology we are retargeting to open

00:07:13,070 --> 00:07:18,110
power and as I said earlier we're within

00:07:15,950 --> 00:07:20,210
about six weeks of the first beta

00:07:18,110 --> 00:07:24,580
release we've had several alpha releases

00:07:20,210 --> 00:07:27,170
to to both Oak Ridge and Livermore and

00:07:24,580 --> 00:07:30,170
all of the features that we support on

00:07:27,170 --> 00:07:34,970
Linux x86 today will be supported on

00:07:30,170 --> 00:07:37,450
Linux open power so Fortran C C++ open

00:07:34,970 --> 00:07:41,510
MP programming open ACC programming

00:07:37,450 --> 00:07:42,860
support for CUDA and Fortran all of this

00:07:41,510 --> 00:07:45,320
is being integrated with the code

00:07:42,860 --> 00:07:48,020
generator and LLVM code generator

00:07:45,320 --> 00:07:52,040
developed by IBM and the open source

00:07:48,020 --> 00:07:53,750
community and we're on schedule 2 to

00:07:52,040 --> 00:07:56,990
have the beta release in May and a

00:07:53,750 --> 00:08:01,100
production release in November and and

00:07:56,990 --> 00:08:04,040
the goal here is very simple if you have

00:08:01,100 --> 00:08:08,240
an application that that runs today on

00:08:04,040 --> 00:08:10,730
Linux x86 and is GPU enabled using open

00:08:08,240 --> 00:08:14,150
ACC or CUDA Fortran you'll be able to

00:08:10,730 --> 00:08:17,480
recompile and run it on a Linux open

00:08:14,150 --> 00:08:21,530
power system and there are a few caveats

00:08:17,480 --> 00:08:24,380
if you're using SSE specific intrinsic

00:08:21,530 --> 00:08:28,400
functions or certain Azzam statements

00:08:24,380 --> 00:08:30,530
well there are barriers there but most

00:08:28,400 --> 00:08:34,790
applications do not use those features

00:08:30,530 --> 00:08:39,050
and and in that case you will be able to

00:08:34,790 --> 00:08:41,570
recompile and run from x86 to power and

00:08:39,050 --> 00:08:43,550
and an obvious question is does it work

00:08:41,570 --> 00:08:45,350
how well does it work and we're far

00:08:43,550 --> 00:08:49,550
enough along that we're building and

00:08:45,350 --> 00:08:51,440
running a lot of real applications wrf

00:08:49,550 --> 00:08:53,760
the weather research and forecast model

00:08:51,440 --> 00:08:58,350
is an 800,000 line applique

00:08:53,760 --> 00:09:04,530
is the most widely used weather code in

00:08:58,350 --> 00:09:07,350
the world and and you can take that code

00:09:04,530 --> 00:09:11,850
you can put all eight hundred thousand

00:09:07,350 --> 00:09:14,070
lines in one directory have an x86

00:09:11,850 --> 00:09:15,840
directory and a window open there and

00:09:14,070 --> 00:09:19,260
open power directory and a window open

00:09:15,840 --> 00:09:21,120
there with links to that make file type

00:09:19,260 --> 00:09:24,570
make in each of those directories and

00:09:21,120 --> 00:09:25,140
build the whole application same make

00:09:24,570 --> 00:09:28,050
file

00:09:25,140 --> 00:09:31,980
same compiler options same source code

00:09:28,050 --> 00:09:37,290
no changes and produce executables for

00:09:31,980 --> 00:09:41,700
as well on the x86 system and open power

00:09:37,290 --> 00:09:45,390
on the power system and and that version

00:09:41,700 --> 00:09:48,480
of the code will run over 2x faster than

00:09:45,390 --> 00:09:51,450
G Fortran compiled code on Haswell and

00:09:48,480 --> 00:09:54,720
almost 2x faster on the open power and

00:09:51,450 --> 00:09:57,870
and we're already running millions and

00:09:54,720 --> 00:10:02,460
millions of lines of code every night in

00:09:57,870 --> 00:10:07,290
this mode and and this is what we we

00:10:02,460 --> 00:10:10,230
bring to to the ecosystem we want you to

00:10:07,290 --> 00:10:13,020
be able to easily move your x86 code to

00:10:10,230 --> 00:10:15,480
open power with all of the GPU

00:10:13,020 --> 00:10:18,000
enablement that you have and when you

00:10:15,480 --> 00:10:20,520
move to open power with env link you've

00:10:18,000 --> 00:10:21,960
heard a lot about that this week from a

00:10:20,520 --> 00:10:26,490
compiler standpoint that is a

00:10:21,960 --> 00:10:30,150
game-changer that allows us to think of

00:10:26,490 --> 00:10:31,740
data movement between CPU memory and GPU

00:10:30,150 --> 00:10:33,980
memory as an optimization so the

00:10:31,740 --> 00:10:37,890
programmer can just focus on

00:10:33,980 --> 00:10:39,810
parallelization let the system move data

00:10:37,890 --> 00:10:43,050
between host memory and device memory

00:10:39,810 --> 00:10:45,060
and yes you may need to you probably

00:10:43,050 --> 00:10:47,070
will need to in some cases go in and

00:10:45,060 --> 00:10:49,230
insert directives to optimize that data

00:10:47,070 --> 00:10:52,440
movement but again it gets us one step

00:10:49,230 --> 00:10:54,930
closer to the day when you can compile

00:10:52,440 --> 00:10:59,010
any code for execution on the CPU or the

00:10:54,930 --> 00:11:01,410
GPU and just execute it on the processor

00:10:59,010 --> 00:11:05,640
that's most appropriate for for that

00:11:01,410 --> 00:11:07,020
part of your problem so and and you

00:11:05,640 --> 00:11:11,910
heard earlier goo

00:11:07,020 --> 00:11:13,500
is moving all of their infrastructure to

00:11:11,910 --> 00:11:17,880
target open power to enable their

00:11:13,500 --> 00:11:21,450
applications to run on either x86 or

00:11:17,880 --> 00:11:23,700
open power and and our goal is for all

00:11:21,450 --> 00:11:26,100
of those customers that use the saw

00:11:23,700 --> 00:11:28,380
listed earlier if you're a one-off

00:11:26,100 --> 00:11:30,330
science scientist or engineer working on

00:11:28,380 --> 00:11:32,940
a code and you want that same capability

00:11:30,330 --> 00:11:36,090
to be able to run on either x86 or open

00:11:32,940 --> 00:11:38,700
power to move your code these compilers

00:11:36,090 --> 00:11:40,650
will enable you to do that

00:11:38,700 --> 00:11:42,660
if you're an OEM or system builder we

00:11:40,650 --> 00:11:45,060
want to engage with you we want to make

00:11:42,660 --> 00:11:47,700
it easy for those customers on your

00:11:45,060 --> 00:11:51,150
prospect list to be able to do that same

00:11:47,700 --> 00:11:53,280
thing so with that I will stop and and

00:11:51,150 --> 00:11:55,400
happy to answer any questions if there

00:11:53,280 --> 00:11:55,400

YouTube URL: https://www.youtube.com/watch?v=7KoV6NDzUrk


