Title: OpenPOWER Summit 2016 - Machine and Deep Learning on Power Systems
Publication date: 2016-04-29
Playlist: OpenPOWER Summit 2016
Description: 
	Presented by Dr. Ruchir Puri of IBM
Captions: 
	00:00:00,439 --> 00:00:06,750
all right I think for those of you who

00:00:04,859 --> 00:00:08,400
know me know that I don't want to keep

00:00:06,750 --> 00:00:10,349
the microphone too close to my mouth

00:00:08,400 --> 00:00:14,009
because I'm too loud in general so it

00:00:10,349 --> 00:00:17,010
tends to overdrive the microphone but

00:00:14,009 --> 00:00:19,140
thanks for coming I think I wanted to

00:00:17,010 --> 00:00:22,080
probably your note is actually quite a

00:00:19,140 --> 00:00:24,210
bit this conference has become more

00:00:22,080 --> 00:00:28,140
about deep learning than than GPUs in

00:00:24,210 --> 00:00:31,199
itself so it has evolved a whole lot and

00:00:28,140 --> 00:00:34,230
I think our focus in open power

00:00:31,199 --> 00:00:36,690
community is also to leverage that

00:00:34,230 --> 00:00:38,850
energy and that mind share out there and

00:00:36,690 --> 00:00:41,250
really put together solutions that are

00:00:38,850 --> 00:00:43,500
very compelling from the point of view

00:00:41,250 --> 00:00:46,770
of big data so one question I think a

00:00:43,500 --> 00:00:49,770
good question to keep in mind is why is

00:00:46,770 --> 00:00:50,070
open why is deep learning where it is

00:00:49,770 --> 00:00:52,469
today

00:00:50,070 --> 00:00:54,899
people neural networks were around for a

00:00:52,469 --> 00:00:58,170
long time I remember actually when I was

00:00:54,899 --> 00:01:00,420
doing my PhD long time back we had my

00:00:58,170 --> 00:01:02,910
colleagues doing their thesis on neural

00:01:00,420 --> 00:01:04,619
networks as well so what what actually

00:01:02,910 --> 00:01:06,570
came together that that gave rise to

00:01:04,619 --> 00:01:08,610
this phenomenon that we are seeing now

00:01:06,570 --> 00:01:10,890
and I think you can look at it and say

00:01:08,610 --> 00:01:15,450
there were two things that happen two

00:01:10,890 --> 00:01:17,310
major things that happened significantly

00:01:15,450 --> 00:01:21,119
I think the algorithms the fundamental

00:01:17,310 --> 00:01:23,820
algorithms are similar I won't say

00:01:21,119 --> 00:01:26,119
exactly the same but similar so gradient

00:01:23,820 --> 00:01:28,680
descent everybody knows it I think

00:01:26,119 --> 00:01:30,509
probably one could argue that it's one

00:01:28,680 --> 00:01:31,950
of the simple algorithms out there there

00:01:30,509 --> 00:01:33,780
are lot more complex algorithms but

00:01:31,950 --> 00:01:36,869
gradient descent is one of the simple

00:01:33,780 --> 00:01:38,640
algorithms out there two things came

00:01:36,869 --> 00:01:42,780
together to give rise to this phenomenon

00:01:38,640 --> 00:01:45,810
and I call data and the second one is

00:01:42,780 --> 00:01:48,360
compute actually so the compute grew to

00:01:45,810 --> 00:01:51,930
a point where it can manage large amount

00:01:48,360 --> 00:01:54,600
of data thereby giving us insights that

00:01:51,930 --> 00:01:56,460
we didn't have before so previously

00:01:54,600 --> 00:01:58,590
either we didn't have very large amount

00:01:56,460 --> 00:02:00,479
of data so we couldn't derive the

00:01:58,590 --> 00:02:02,040
insights and we didn't have the compute

00:02:00,479 --> 00:02:03,479
to match it as well so those are two

00:02:02,040 --> 00:02:06,240
things I could argue which are

00:02:03,479 --> 00:02:09,000
underpinnings of this phenomenon of deep

00:02:06,240 --> 00:02:10,410
learning that we are seeing today so

00:02:09,000 --> 00:02:13,200
basically I think I'm going to take you

00:02:10,410 --> 00:02:15,810
through what we've been focused on

00:02:13,200 --> 00:02:18,750
kind of things that you will see to

00:02:15,810 --> 00:02:21,840
start come from the point of releases on

00:02:18,750 --> 00:02:23,790
open power as well because many people

00:02:21,840 --> 00:02:25,320
may be wondering lot of these deep

00:02:23,790 --> 00:02:27,960
learning packages and machine learning

00:02:25,320 --> 00:02:30,690
packages that are out there they run on

00:02:27,960 --> 00:02:33,090
obviously the standard platform on Intel

00:02:30,690 --> 00:02:34,590
what's going on with power so I'll take

00:02:33,090 --> 00:02:37,290
you through what what we've been working

00:02:34,590 --> 00:02:38,940
on and what our releases will be very

00:02:37,290 --> 00:02:42,200
soon and you will see more actually

00:02:38,940 --> 00:02:44,880
press releases about it as we go forward

00:02:42,200 --> 00:02:48,870
okay so I think from the point of view

00:02:44,880 --> 00:02:52,500
of the backbone of of open power systems

00:02:48,870 --> 00:02:56,100
is the power processor itself which is

00:02:52,500 --> 00:02:58,500
an enhanced microarchitecture a large

00:02:56,100 --> 00:03:01,500
amount of execution bandwidth SMT eight

00:02:58,500 --> 00:03:04,410
transactional memory vector scalar units

00:03:01,500 --> 00:03:06,480
awesim D and high-performance integer

00:03:04,410 --> 00:03:08,970
and floating-point vector processor it's

00:03:06,480 --> 00:03:11,250
really optimized for data rich

00:03:08,970 --> 00:03:13,140
applications this was really a processor

00:03:11,250 --> 00:03:16,520
that was built for big data actually

00:03:13,140 --> 00:03:16,520
very high bandwidths and big data

00:03:17,180 --> 00:03:24,180
so overall the combined i/o bandwidth

00:03:21,090 --> 00:03:26,070
that you see is around 7.6 tear-up it's

00:03:24,180 --> 00:03:28,799
a second which is massive amount of i/o

00:03:26,070 --> 00:03:30,959
bandwidth so it's really big bandwidth

00:03:28,799 --> 00:03:33,150
that is it to match the big data

00:03:30,959 --> 00:03:35,970
requirements putting it all together

00:03:33,150 --> 00:03:39,390
with the memory links on and off node

00:03:35,970 --> 00:03:40,980
SMP links as well as PCIe as I said it

00:03:39,390 --> 00:03:46,380
runs at around seven point six terabytes

00:03:40,980 --> 00:03:49,709
of chip i/o bandwidth so from the point

00:03:46,380 --> 00:03:52,890
of view of the the processor itself it's

00:03:49,709 --> 00:03:55,320
a general purpose CPU design obviously

00:03:52,890 --> 00:03:59,130
in any general-purpose processor there

00:03:55,320 --> 00:04:01,769
are many competing requirements and so

00:03:59,130 --> 00:04:04,320
you're trying to make one size fits all

00:04:01,769 --> 00:04:05,970
and as all of us know one size doesn't

00:04:04,320 --> 00:04:07,980
fit all but that's since it's the

00:04:05,970 --> 00:04:10,680
general-purpose processor that's what we

00:04:07,980 --> 00:04:13,650
are basically building there is a branch

00:04:10,680 --> 00:04:15,739
control flow dominated code codes with

00:04:13,650 --> 00:04:18,570
unpredictable data access patterns

00:04:15,739 --> 00:04:19,919
operating system code multiple separate

00:04:18,570 --> 00:04:22,990
applications that could be running

00:04:19,919 --> 00:04:24,610
multiple VMs concurrently running

00:04:22,990 --> 00:04:27,160
which results in relatively low

00:04:24,610 --> 00:04:30,280
efficiency for any one particular metric

00:04:27,160 --> 00:04:31,090
I could argue so flops per area or

00:04:30,280 --> 00:04:33,400
integer offs

00:04:31,090 --> 00:04:37,000
per area and so on so what we really are

00:04:33,400 --> 00:04:39,030
trying to do is we basically are let me

00:04:37,000 --> 00:04:43,419
just sort of forward this a little bit

00:04:39,030 --> 00:04:46,509
so I think our vision goes in open power

00:04:43,419 --> 00:04:49,900
to really adapt openness and provide

00:04:46,509 --> 00:04:52,150
interfaces whereby whether it is capi or

00:04:49,900 --> 00:04:55,810
whether it is n feeling whereby we can

00:04:52,150 --> 00:04:58,240
adapt accelerators so if you got

00:04:55,810 --> 00:05:01,300
computer intensive jobs we can offload

00:04:58,240 --> 00:05:02,380
that to for example a GPU if you got

00:05:01,300 --> 00:05:04,509
actually certain other applications

00:05:02,380 --> 00:05:07,360
where the transactions or other ones

00:05:04,509 --> 00:05:09,490
which can utilize at PGA's we can offer

00:05:07,360 --> 00:05:13,660
that offload that through copy to an

00:05:09,490 --> 00:05:16,630
FPGA as well so for GPU acceleration for

00:05:13,660 --> 00:05:19,750
example coming up with then we link we

00:05:16,630 --> 00:05:23,169
support up to 18 GPUs or more we have

00:05:19,750 --> 00:05:26,560
tested this with up to 24 devices so 12

00:05:23,169 --> 00:05:29,500
K eighties we exploit the IBM design for

00:05:26,560 --> 00:05:32,199
big data it's I think the key is that we

00:05:29,500 --> 00:05:34,599
are able to address it there's a very

00:05:32,199 --> 00:05:36,580
large addressable space which enables

00:05:34,599 --> 00:05:38,500
rich accelerator configurations which I

00:05:36,580 --> 00:05:40,240
think the some of the other designs are

00:05:38,500 --> 00:05:42,280
not able to do so our address space is

00:05:40,240 --> 00:05:44,229
very very large thereby we are able to

00:05:42,280 --> 00:05:47,289
actually incorporate large amount of

00:05:44,229 --> 00:05:49,330
GPUs in the chassis it's 1 terabytes of

00:05:47,289 --> 00:05:50,860
address space per PCIe host interface

00:05:49,330 --> 00:05:53,680
which is larger than anybody available

00:05:50,860 --> 00:05:55,930
out there standard let little-endian

00:05:53,680 --> 00:05:59,110
Linux and video drivers are available

00:05:55,930 --> 00:06:00,820
and it's supposed cuda 7.5 it's

00:05:59,110 --> 00:06:02,620
available now I think we are at their

00:06:00,820 --> 00:06:05,889
systems that you see around that are

00:06:02,620 --> 00:06:09,310
coming up so from the point of view of

00:06:05,889 --> 00:06:12,159
power and and machine and deep learning

00:06:09,310 --> 00:06:14,530
software stack so I think it's important

00:06:12,159 --> 00:06:16,870
to keep in mind that it's great to build

00:06:14,530 --> 00:06:19,120
hardware but what makes or breaks the

00:06:16,870 --> 00:06:21,610
the product is really the software stack

00:06:19,120 --> 00:06:23,650
and what we really are doing is that

00:06:21,610 --> 00:06:25,930
there are several frameworks we are

00:06:23,650 --> 00:06:27,880
supporting so we are not in obviously

00:06:25,930 --> 00:06:30,220
the business of selecting what

00:06:27,880 --> 00:06:32,710
frameworks you run I think what you run

00:06:30,220 --> 00:06:34,120
is really your job you decide what you

00:06:32,710 --> 00:06:35,139
run for your deep learning or your

00:06:34,120 --> 00:06:37,719
machine learning Africa

00:06:35,139 --> 00:06:40,030
Asians we are in business are supporting

00:06:37,719 --> 00:06:41,770
all of these frameworks the major ones

00:06:40,030 --> 00:06:44,409
so cafe from Berkeley which is very

00:06:41,770 --> 00:06:48,610
suitable for machine for vision

00:06:44,409 --> 00:06:50,740
applications torch which is a a a face

00:06:48,610 --> 00:06:53,919
book open source software available

00:06:50,740 --> 00:06:56,830
which actually runs Lua and through did

00:06:53,919 --> 00:06:59,289
process and it's very flexible very

00:06:56,830 --> 00:07:02,949
popular for text and and speech now

00:06:59,289 --> 00:07:07,300
openly increasingly speech now piano

00:07:02,949 --> 00:07:08,949
which is more sort of more popular from

00:07:07,300 --> 00:07:10,659
the point of view of research usage

00:07:08,949 --> 00:07:13,569
actually I've seen but and also speech

00:07:10,659 --> 00:07:16,120
applications tensorflow which there were

00:07:13,569 --> 00:07:18,279
a couple of talks here in the in the GTC

00:07:16,120 --> 00:07:21,370
on tensorflow as well which is from

00:07:18,279 --> 00:07:24,099
Google that will be supported CNT K from

00:07:21,370 --> 00:07:26,979
Microsoft will be supported as well and

00:07:24,099 --> 00:07:28,629
DL for J which a clearance poppins

00:07:26,979 --> 00:07:30,639
distribution will be supported as well

00:07:28,629 --> 00:07:33,009
so you will see that it's a very wide

00:07:30,639 --> 00:07:35,740
amount of support for the software stack

00:07:33,009 --> 00:07:37,870
that's coming up working on power and

00:07:35,740 --> 00:07:39,669
for these you will start seeing very

00:07:37,870 --> 00:07:42,250
compelling numbers from performance

00:07:39,669 --> 00:07:44,830
point of view with large amount of GPUs

00:07:42,250 --> 00:07:47,139
available to you in terms of the library

00:07:44,830 --> 00:07:49,870
layer so obviously all of this runs

00:07:47,139 --> 00:07:53,349
underneath a library layer there's

00:07:49,870 --> 00:07:56,349
obviously ku blast and an impeccable

00:07:53,349 --> 00:07:59,229
- you see UDN n is supported and we are

00:07:56,349 --> 00:08:02,319
also working with Nirvana office which

00:07:59,229 --> 00:08:04,719
is actually a FPGA based vendor for

00:08:02,319 --> 00:08:06,639
doing training and inferencing working

00:08:04,719 --> 00:08:08,979
with Xilinx and the Xilinx blocks will

00:08:06,639 --> 00:08:10,539
be supported as well on the on the

00:08:08,979 --> 00:08:13,659
hardware stack through software

00:08:10,539 --> 00:08:15,909
libraries internally IBM we are also

00:08:13,659 --> 00:08:17,469
working at what is what we call as DL SL

00:08:15,909 --> 00:08:19,779
which is a deep learning system level

00:08:17,469 --> 00:08:21,879
library that we'll be able to exploit NV

00:08:19,779 --> 00:08:24,669
link as we go along for several of these

00:08:21,879 --> 00:08:27,460
applications so as we go through our our

00:08:24,669 --> 00:08:29,710
evolution of and we link based systems

00:08:27,460 --> 00:08:31,509
come along you will also start seeing as

00:08:29,710 --> 00:08:33,909
oft where library that we'll be able to

00:08:31,509 --> 00:08:35,979
utilize the NV link based systems for

00:08:33,909 --> 00:08:37,690
several of your applications and will

00:08:35,979 --> 00:08:39,640
become part and parcel of our

00:08:37,690 --> 00:08:43,740
lower-level libraries which we will

00:08:39,640 --> 00:08:43,740
release in open source as well

00:08:43,940 --> 00:08:49,610
so from the point of view of power GPU

00:08:46,520 --> 00:08:51,440
acceleration there's a CUDA programming

00:08:49,610 --> 00:08:54,470
environment supported under

00:08:51,440 --> 00:08:56,690
little-endian Linux GPU obviously as a

00:08:54,470 --> 00:08:58,580
compute accelerator offload dominated

00:08:56,690 --> 00:09:01,340
computer intensive application portion

00:08:58,580 --> 00:09:04,610
to the GPU and advances in GPU

00:09:01,340 --> 00:09:06,440
performance and programming Universal

00:09:04,610 --> 00:09:08,870
virtual addressing and unified memory

00:09:06,440 --> 00:09:11,600
will be coming on board as well

00:09:08,870 --> 00:09:13,640
ongoing collaboration to to co-op to my

00:09:11,600 --> 00:09:15,710
system so we actually are within

00:09:13,640 --> 00:09:19,550
research for example we are working on

00:09:15,710 --> 00:09:21,760
systems to to to experiment with

00:09:19,550 --> 00:09:24,830
Chelsey's that have very high GPU count

00:09:21,760 --> 00:09:27,380
with with very high bandwidth available

00:09:24,830 --> 00:09:30,260
between the the power processor and the

00:09:27,380 --> 00:09:33,530
sea and the GPUs that are deployed with

00:09:30,260 --> 00:09:38,780
PCIe in particular and running with NB

00:09:33,530 --> 00:09:41,390
link as well so so really I think from a

00:09:38,780 --> 00:09:42,700
programming point of view we will be

00:09:41,390 --> 00:09:44,810
supporting a very heterogeneous

00:09:42,700 --> 00:09:46,220
environment and several of the companies

00:09:44,810 --> 00:09:48,710
that you see out there are working

00:09:46,220 --> 00:09:51,080
towards that as well obviously there's

00:09:48,710 --> 00:09:53,240
the power processor itself you can

00:09:51,080 --> 00:09:57,500
actually you could probably write it in

00:09:53,240 --> 00:09:59,000
C++ Java then obviously from FPGA point

00:09:57,500 --> 00:10:02,420
of view you may like to write it in

00:09:59,000 --> 00:10:05,630
OpenCL system C or VHDL obviously

00:10:02,420 --> 00:10:08,360
several of these I think the at least in

00:10:05,630 --> 00:10:10,970
mind the make or break for several of

00:10:08,360 --> 00:10:15,500
these things will be how easy it is to

00:10:10,970 --> 00:10:18,260
use a particular library and I think

00:10:15,500 --> 00:10:20,780
I've discussions with the Xilinx team

00:10:18,260 --> 00:10:23,390
for example and if as a software

00:10:20,780 --> 00:10:25,490
developer if anyone told me that I have

00:10:23,390 --> 00:10:28,640
to so if I'm in a meeting and someone

00:10:25,490 --> 00:10:30,980
told me RTL I'm bad because I got

00:10:28,640 --> 00:10:33,020
nothing to do with RTL unless somebody

00:10:30,980 --> 00:10:34,790
can make it highly consumable from a

00:10:33,020 --> 00:10:36,640
software point of view and I think

00:10:34,790 --> 00:10:39,230
that's where NVIDIA has done a great job

00:10:36,640 --> 00:10:41,420
making sure that the that the

00:10:39,230 --> 00:10:43,310
intricacies of hardware are actually

00:10:41,420 --> 00:10:45,380
hidden from you by enabling things like

00:10:43,310 --> 00:10:48,230
qu+ and CU DNN which really were

00:10:45,380 --> 00:10:50,330
underpinnings of of this sort of tsunami

00:10:48,230 --> 00:10:52,400
that is taking us through for the deep

00:10:50,330 --> 00:10:53,939
learning space and I think from the fpga

00:10:52,400 --> 00:10:55,649
point of view we need to do something

00:10:53,939 --> 00:10:58,769
very similar and we are working with

00:10:55,649 --> 00:11:00,359
Xilinx and office for that and I think

00:10:58,769 --> 00:11:04,079
from the point of view of obviously the

00:11:00,359 --> 00:11:06,959
the nvidia part programming environment

00:11:04,079 --> 00:11:09,449
as well as programming based on some of

00:11:06,959 --> 00:11:15,479
the key linear algebra constructs which

00:11:09,449 --> 00:11:16,799
are supported in Kulas and CU DNN so i

00:11:15,479 --> 00:11:17,999
think from the portability and

00:11:16,799 --> 00:11:20,159
optimization in heterogeneous

00:11:17,999 --> 00:11:24,119
environment point of view obviously

00:11:20,159 --> 00:11:26,009
there's GPU enablement FPGA interfaces

00:11:24,119 --> 00:11:27,659
and configuration and potentially other

00:11:26,009 --> 00:11:29,369
accelerators which would be coming on

00:11:27,659 --> 00:11:30,869
board as well which can be attached

00:11:29,369 --> 00:11:33,119
through either than we link or

00:11:30,869 --> 00:11:35,639
specifically PCIe obviously you can

00:11:33,119 --> 00:11:37,589
connect it through PCIe there's a

00:11:35,639 --> 00:11:39,569
library layer I talked about earlier on

00:11:37,589 --> 00:11:41,669
and there's a sort of couple of

00:11:39,569 --> 00:11:43,739
libraries we are working on as well to

00:11:41,669 --> 00:11:46,769
take advantage of NV link in particular

00:11:43,739 --> 00:11:49,289
and I think then right on top of that

00:11:46,769 --> 00:11:51,929
other applications which you could write

00:11:49,289 --> 00:11:54,319
it in actually the the Watson

00:11:51,929 --> 00:11:56,099
development cloud or other actually

00:11:54,319 --> 00:11:58,529
applications that you are writing

00:11:56,099 --> 00:12:00,689
yourself in application environment and

00:11:58,529 --> 00:12:02,369
I think right in in the middle of this

00:12:00,689 --> 00:12:05,249
I'm showing this cognitive middleware

00:12:02,369 --> 00:12:07,549
that we are calling which basically is

00:12:05,249 --> 00:12:10,079
the layer that that hooks up the

00:12:07,549 --> 00:12:13,649
lower-level libraries to the application

00:12:10,079 --> 00:12:16,919
development so in terms of what is

00:12:13,649 --> 00:12:21,029
supported today which will be coming

00:12:16,919 --> 00:12:23,399
from in a release very soon the deep

00:12:21,029 --> 00:12:26,220
learning framework Cafe has already been

00:12:23,399 --> 00:12:26,579
ported included in first release of

00:12:26,220 --> 00:12:28,799
power

00:12:26,579 --> 00:12:32,099
Emel TL distribution which will be

00:12:28,799 --> 00:12:34,049
coming very soon torch which is also

00:12:32,099 --> 00:12:36,929
being ported and will also be included

00:12:34,049 --> 00:12:39,479
in the first release piano and digits 3

00:12:36,929 --> 00:12:42,329
interface so digits three is the Nvidia

00:12:39,479 --> 00:12:46,109
box which is an interface as well by the

00:12:42,329 --> 00:12:48,029
way which is actually supporting sort of

00:12:46,109 --> 00:12:49,919
a graphical interface which is able to

00:12:48,029 --> 00:12:53,429
support various interfaces that will be

00:12:49,919 --> 00:12:56,309
also supported on power in the open

00:12:53,429 --> 00:12:59,669
power ecosystem and tensorflow

00:12:56,309 --> 00:13:01,919
RTL for J and C NT K will be coming very

00:12:59,669 --> 00:13:03,359
soon they're being ported but I think

00:13:01,919 --> 00:13:06,060
they will be supported in the upcoming

00:13:03,359 --> 00:13:09,030
releases as we go on

00:13:06,060 --> 00:13:10,350
I think that's what I wanted to talk

00:13:09,030 --> 00:13:12,330
about I'll be happy to take questions

00:13:10,350 --> 00:13:14,010
from you but I think we are gonna have

00:13:12,330 --> 00:13:15,630
some pretty exciting time with lot of

00:13:14,010 --> 00:13:17,670
these packages coming on board which are

00:13:15,630 --> 00:13:19,770
highly optimized on the over power open

00:13:17,670 --> 00:13:22,470
power ecosystem and the libraries that

00:13:19,770 --> 00:13:27,140
we are building with respect to the envy

00:13:22,470 --> 00:13:27,140

YouTube URL: https://www.youtube.com/watch?v=e9FxVjJMT2o


