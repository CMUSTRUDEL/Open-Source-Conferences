Title: OpenPOWER Summit 2016 - Energy Efficient Transparent Library Acceleration with CAPI
Publication date: 2016-04-29
Playlist: OpenPOWER Summit 2016
Description: 
	Presented by Heiner Giefers of IBM
Captions: 
	00:00:00,000 --> 00:00:05,640
okay here we are so yeah my presentation

00:00:02,939 --> 00:00:09,030
is about a case study we did that shows

00:00:05,640 --> 00:00:11,540
how we can leverage cappy to build

00:00:09,030 --> 00:00:14,610
energy efficient accelerators and how we

00:00:11,540 --> 00:00:17,990
make these accelerators available as in

00:00:14,610 --> 00:00:20,490
form of libraries to the end user so

00:00:17,990 --> 00:00:22,439
when we look at the roadmap towards

00:00:20,490 --> 00:00:24,390
energy efficient data centers we see

00:00:22,439 --> 00:00:27,570
that in the past we spend a lot of

00:00:24,390 --> 00:00:31,199
effort to reduce the overheads right so

00:00:27,570 --> 00:00:35,040
we we did efficient cooling efficient

00:00:31,199 --> 00:00:36,660
power supplies to optimize the data

00:00:35,040 --> 00:00:39,809
center energy as a whole but this

00:00:36,660 --> 00:00:41,940
doesn't help a lot to really make the

00:00:39,809 --> 00:00:45,149
work more efficient that is computed on

00:00:41,940 --> 00:00:47,250
the nodes so in the far future we expect

00:00:45,149 --> 00:00:50,219
a lot of gains from from new device

00:00:47,250 --> 00:00:53,940
technologies from even new processing

00:00:50,219 --> 00:00:57,260
paradigms but today the tool of choice

00:00:53,940 --> 00:01:00,510
we have to improve energy efficiency is

00:00:57,260 --> 00:01:03,660
accelerators and that can be massively

00:01:00,510 --> 00:01:05,280
parallel accelerators like like GPUs or

00:01:03,660 --> 00:01:09,710
accelerators where you can put your

00:01:05,280 --> 00:01:12,689
specialized logic like FPGAs so the

00:01:09,710 --> 00:01:15,689
problem we have right now is with some

00:01:12,689 --> 00:01:18,479
FPGAs is that they are still not as

00:01:15,689 --> 00:01:22,110
programmable as CPUs right so it's it's

00:01:18,479 --> 00:01:24,840
a it's difficult to put your application

00:01:22,110 --> 00:01:29,640
on in fpgas and there are two ways to

00:01:24,840 --> 00:01:32,270
optimize this process so one option is

00:01:29,640 --> 00:01:34,979
to use high level synthesis flows and

00:01:32,270 --> 00:01:36,780
that's a message where you have where

00:01:34,979 --> 00:01:39,720
you can write your application in the

00:01:36,780 --> 00:01:41,340
sea level language and user tools to

00:01:39,720 --> 00:01:43,619
compile the specification down into an

00:01:41,340 --> 00:01:46,500
fpga bit stream and that improves

00:01:43,619 --> 00:01:48,930
productivity a lot but you still have to

00:01:46,500 --> 00:01:51,840
have an idea about the architecture of

00:01:48,930 --> 00:01:54,000
your chip and you have to paralyze your

00:01:51,840 --> 00:01:57,060
application by hand so and this will

00:01:54,000 --> 00:02:00,600
probably not help to embrace a vast

00:01:57,060 --> 00:02:02,549
amount of the programmers community so

00:02:00,600 --> 00:02:05,880
here we think that library acceleration

00:02:02,549 --> 00:02:09,860
is is a better option and the idea is to

00:02:05,880 --> 00:02:12,360
build up an optimized library or

00:02:09,860 --> 00:02:13,070
accelerated library that the user can

00:02:12,360 --> 00:02:15,210
use

00:02:13,070 --> 00:02:19,050
just as the software version of the

00:02:15,210 --> 00:02:21,420
library well looking at what what

00:02:19,050 --> 00:02:23,490
libraries are around there is some there

00:02:21,420 --> 00:02:27,120
is a vast amount of fairly standard

00:02:23,490 --> 00:02:30,060
libraries from from dense and sparse

00:02:27,120 --> 00:02:32,730
linear algebra image processing machine

00:02:30,060 --> 00:02:37,580
learning and regular expression matching

00:02:32,730 --> 00:02:40,380
and in this presentation we look at FFTs

00:02:37,580 --> 00:02:44,940
fast Fourier transforms and especially

00:02:40,380 --> 00:02:49,530
at the FF to W library that is fairly

00:02:44,940 --> 00:02:52,860
standard in the open source domain okay

00:02:49,530 --> 00:02:56,490
so a quick recap what what is an fft so

00:02:52,860 --> 00:02:59,130
the FFT basically transform a signal

00:02:56,490 --> 00:03:00,540
from the time domain into the frequency

00:02:59,130 --> 00:03:03,600
domain so you get the spectrum of the

00:03:00,540 --> 00:03:06,270
signal and it's used in many many

00:03:03,600 --> 00:03:08,280
applications so not only signal

00:03:06,270 --> 00:03:12,500
processing but also data compression

00:03:08,280 --> 00:03:16,920
machine learning and hpc and we have

00:03:12,500 --> 00:03:21,480
some fairly standard libraries to use

00:03:16,920 --> 00:03:28,110
most prominent fftw MKL and andy ssl for

00:03:21,480 --> 00:03:31,500
IBM systems so looking at how an FF II

00:03:28,110 --> 00:03:33,420
is executed in software we see the the

00:03:31,500 --> 00:03:35,910
FFT is basically a recursive algorithm

00:03:33,420 --> 00:03:38,270
so you can compute a bigger problem by

00:03:35,910 --> 00:03:43,200
decomposing it into smaller problems

00:03:38,270 --> 00:03:45,720
like here and if we expand a 1d FFT like

00:03:43,200 --> 00:03:49,890
this we see a certain data flow graph

00:03:45,720 --> 00:03:52,470
and we have local computations or local

00:03:49,890 --> 00:03:56,460
access patterns in some parts of the FFT

00:03:52,470 --> 00:03:58,260
and more global access patterns in other

00:03:56,460 --> 00:04:01,320
phases of the FFT and this is the

00:03:58,260 --> 00:04:03,840
problem this is a problem for four cpus

00:04:01,320 --> 00:04:05,400
because this kind of strident excesses

00:04:03,840 --> 00:04:10,740
don't map very well to the memory

00:04:05,400 --> 00:04:18,540
architecture of modern CPUs okay so but

00:04:10,740 --> 00:04:21,630
how does GPUs CPUs now execute FFTs well

00:04:18,540 --> 00:04:24,820
in an fftw for example you have kernels

00:04:21,630 --> 00:04:27,820
that are optimized

00:04:24,820 --> 00:04:30,670
and compute a very tiny FFT problem and

00:04:27,820 --> 00:04:34,410
now the problem is to find for your for

00:04:30,670 --> 00:04:37,510
your bigger FFT a good schedule yeah so

00:04:34,410 --> 00:04:40,660
how do you how do you find a good

00:04:37,510 --> 00:04:42,880
sequence of these code let's so that you

00:04:40,660 --> 00:04:45,850
find most of your data in your in your

00:04:42,880 --> 00:04:48,130
cash you can estimate the schedule or

00:04:45,850 --> 00:04:52,240
you can learn it by computing many many

00:04:48,130 --> 00:04:56,320
iterations over the same over the same

00:04:52,240 --> 00:05:00,370
problem the approach we follow on the

00:04:56,320 --> 00:05:04,930
FPGA is quite quite different here so we

00:05:00,370 --> 00:05:07,840
also have hand-tuned blocks so the

00:05:04,930 --> 00:05:10,320
compute units over here and and these

00:05:07,840 --> 00:05:15,070
compute units solve a tiny FFT problem

00:05:10,320 --> 00:05:17,380
and after each compute unit we have a

00:05:15,070 --> 00:05:19,540
shuffle unit that does the shuffling of

00:05:17,380 --> 00:05:22,870
the data so this removes the strides and

00:05:19,540 --> 00:05:26,980
ensures that you find the proper samples

00:05:22,870 --> 00:05:30,460
at the next compute compute stage and

00:05:26,980 --> 00:05:32,710
the fft we did for example has a

00:05:30,460 --> 00:05:35,590
pipeline depths of about six thousand

00:05:32,710 --> 00:05:38,380
cycles so it's a very very deep pipeline

00:05:35,590 --> 00:05:41,980
and we can have samples of multiple FFTs

00:05:38,380 --> 00:05:46,180
problem or f50s in the same pipeline at

00:05:41,980 --> 00:05:48,610
the same at the same time and well now

00:05:46,180 --> 00:05:52,480
we simply type through our data in a

00:05:48,610 --> 00:05:55,570
linear fashion and grab our results of

00:05:52,480 --> 00:06:00,130
the problem at the end in an ordered

00:05:55,570 --> 00:06:02,350
fashion okay so this is how the

00:06:00,130 --> 00:06:05,830
architecture of the of the FFT

00:06:02,350 --> 00:06:08,050
accelerator basically looks and I now

00:06:05,830 --> 00:06:13,180
want to show you how we integrate this

00:06:08,050 --> 00:06:16,120
into into a library so we have we came

00:06:13,180 --> 00:06:20,290
up with with different versions of the

00:06:16,120 --> 00:06:24,390
FFT library one is simply the fftw here

00:06:20,290 --> 00:06:29,100
running on a powerade cpu we have done

00:06:24,390 --> 00:06:33,120
another one that is using the PCIe a

00:06:29,100 --> 00:06:37,460
protocol and a nam CL drivers tech and

00:06:33,120 --> 00:06:41,090
finally our copy enabled fft

00:06:37,460 --> 00:06:44,419
and actually both of these versions are

00:06:41,090 --> 00:06:47,870
running on the very same hardware in the

00:06:44,419 --> 00:06:51,740
end okay so now the user can make use of

00:06:47,870 --> 00:06:56,090
this accelerated libraries just by by

00:06:51,740 --> 00:07:00,949
calling to the interface but this is not

00:06:56,090 --> 00:07:02,630
not the optimal way to use it so we came

00:07:00,949 --> 00:07:08,620
up with a little interpose a library

00:07:02,630 --> 00:07:13,669
that intercepts calls to the to the fftw

00:07:08,620 --> 00:07:16,099
library and in case we have or we can

00:07:13,669 --> 00:07:20,320
accelerate this computation we just

00:07:16,099 --> 00:07:23,780
remap the call dynamically to an FPGA

00:07:20,320 --> 00:07:26,210
okay so it's completely transparent to

00:07:23,780 --> 00:07:32,150
the user it doesn't even see that there

00:07:26,210 --> 00:07:35,270
is an an accelerator in this system yeah

00:07:32,150 --> 00:07:39,919
so what what can we do with that so in

00:07:35,270 --> 00:07:43,280
order to find out if a problem works

00:07:39,919 --> 00:07:44,419
well on the FPGA we can at runtime trace

00:07:43,280 --> 00:07:47,710
the performance and the power

00:07:44,419 --> 00:07:50,599
consumption of the system to figure out

00:07:47,710 --> 00:07:54,740
what is the optimal platform for a

00:07:50,599 --> 00:07:56,449
certain computation yeah and we can we

00:07:54,740 --> 00:07:59,330
can use this information in our inter

00:07:56,449 --> 00:08:01,849
posle library here to remap the calls to

00:07:59,330 --> 00:08:05,570
the most optimal a platform in the

00:08:01,849 --> 00:08:10,190
system okay what does it bring that in

00:08:05,570 --> 00:08:14,659
the end so we were we were able to to

00:08:10,190 --> 00:08:17,360
use this this library in in tools that

00:08:14,659 --> 00:08:19,820
link to the fftw so we tried it for

00:08:17,360 --> 00:08:23,240
example with new radio which is a signal

00:08:19,820 --> 00:08:24,710
processing tool box in linux and we

00:08:23,240 --> 00:08:27,740
could accelerate this application

00:08:24,710 --> 00:08:29,990
without changing any line of code so no

00:08:27,740 --> 00:08:32,719
code changes to the application the

00:08:29,990 --> 00:08:36,820
interposing does the job and remaps the

00:08:32,719 --> 00:08:36,820
calls to the fpga

00:08:37,370 --> 00:08:43,170
ok so this keppi accelerated FFT is

00:08:41,100 --> 00:08:45,870
running on super on the super vessel

00:08:43,170 --> 00:08:49,470
developer cloud you can check it out on

00:08:45,870 --> 00:08:54,810
the web and try it if you want it's

00:08:49,470 --> 00:08:57,690
available there a few things how this

00:08:54,810 --> 00:09:02,210
works under the hood so whenever we get

00:08:57,690 --> 00:09:06,000
an F FTW the command to plan a new FFT

00:09:02,210 --> 00:09:08,820
we recheck Oh does this structure that's

00:09:06,000 --> 00:09:12,990
this problem map to our accelerator and

00:09:08,820 --> 00:09:16,530
if yes we mark this this plan as

00:09:12,990 --> 00:09:19,770
accelerate able and any subsequent

00:09:16,530 --> 00:09:23,760
execution of an FFT will then be mapped

00:09:19,770 --> 00:09:26,850
to the accelerator ok so we can always

00:09:23,760 --> 00:09:30,270
fall back to software in terms of if the

00:09:26,850 --> 00:09:37,350
FPGA is not available or if we have if

00:09:30,270 --> 00:09:41,730
the queues are full for example ok a few

00:09:37,350 --> 00:09:45,540
results so the first the first thing we

00:09:41,730 --> 00:09:48,000
looked at was what's the latency of this

00:09:45,540 --> 00:09:51,630
accelerator because if we do this kind

00:09:48,000 --> 00:09:54,750
of transparent intersection of library

00:09:51,630 --> 00:09:57,690
calls we need accelerators that are have

00:09:54,750 --> 00:10:00,380
low latency react very fast in the

00:09:57,690 --> 00:10:04,890
system and here we could show that

00:10:00,380 --> 00:10:09,230
although the fft pipeline is computing

00:10:04,890 --> 00:10:12,090
faster than a core the little overhead

00:10:09,230 --> 00:10:14,190
edit a little edit a little overhead to

00:10:12,090 --> 00:10:18,230
the cpu runtime so we are not quite as

00:10:14,190 --> 00:10:21,870
fast but it's approximately the same as

00:10:18,230 --> 00:10:24,660
we do on the on the cpu compared to an

00:10:21,870 --> 00:10:29,070
pci express accelerated version our

00:10:24,660 --> 00:10:32,250
accelerator is far more by far better we

00:10:29,070 --> 00:10:34,350
see here that the opencl version that we

00:10:32,250 --> 00:10:37,440
use spends most of the time just copying

00:10:34,350 --> 00:10:41,520
data back and forth preparing data into

00:10:37,440 --> 00:10:46,260
buffers and this overhead makes a pci

00:10:41,520 --> 00:10:48,339
inversion just not some compatible 22 to

00:10:46,260 --> 00:10:50,889
the copy one

00:10:48,339 --> 00:10:54,129
but the accelerator was not really

00:10:50,889 --> 00:10:59,370
designed for executing single FFTs it's

00:10:54,129 --> 00:11:01,689
more for for throughput and here we show

00:10:59,370 --> 00:11:04,629
what happens when we increase the bad

00:11:01,689 --> 00:11:07,629
guys of the FFT so that means we have

00:11:04,629 --> 00:11:09,819
multiple FFTs in a row to process on the

00:11:07,629 --> 00:11:14,410
accelerator and here we can play out the

00:11:09,819 --> 00:11:18,610
strength of the of the FPGA so the power

00:11:14,410 --> 00:11:21,089
8 baseline is given in red here and the

00:11:18,610 --> 00:11:23,439
brownish curve over here shows an

00:11:21,089 --> 00:11:26,439
accelerator that really follows the

00:11:23,439 --> 00:11:28,480
execution model of ccp you that is you

00:11:26,439 --> 00:11:31,120
can submit a job wait until the job is

00:11:28,480 --> 00:11:33,639
completed and then pick up the next one

00:11:31,120 --> 00:11:36,579
on the fpga and this is performing as

00:11:33,639 --> 00:11:38,769
good as one core if you want so if we

00:11:36,579 --> 00:11:41,019
can have multiple fft s in the same

00:11:38,769 --> 00:11:44,589
pipeline as you would have in the normal

00:11:41,019 --> 00:11:47,170
use case we are by a factor of two to

00:11:44,589 --> 00:11:51,519
three better compared to compared to 1

00:11:47,170 --> 00:11:54,610
cor in this case but even more important

00:11:51,519 --> 00:11:57,100
than just the performance is the power

00:11:54,610 --> 00:12:02,889
consumption and here we did an

00:11:57,100 --> 00:12:06,759
experiment and we set up on a larger

00:12:02,889 --> 00:12:09,189
problem so we generated like a gigabyte

00:12:06,759 --> 00:12:12,160
of input samples and computed the fft

00:12:09,189 --> 00:12:14,589
over this large set of data in in a row

00:12:12,160 --> 00:12:18,309
and looked at the power consumption on

00:12:14,589 --> 00:12:20,709
the cpu and on the fpga we did this with

00:12:18,309 --> 00:12:24,790
with the a mr. tool that was announced

00:12:20,709 --> 00:12:27,040
yesterday and here in this in this

00:12:24,790 --> 00:12:29,110
window we can see the total power

00:12:27,040 --> 00:12:32,589
consumption of the note of the of the

00:12:29,110 --> 00:12:35,170
entire power 8 node and with you if we

00:12:32,589 --> 00:12:37,449
use a multi-threaded FFT that uses all

00:12:35,170 --> 00:12:39,069
the threats available in the system for

00:12:37,449 --> 00:12:44,050
the computation the power ramps up by

00:12:39,069 --> 00:12:47,110
almost 200 watts if we look at the time

00:12:44,050 --> 00:12:48,730
spent on the trace where the eff fpga is

00:12:47,110 --> 00:12:50,980
active you cannot even see that there is

00:12:48,730 --> 00:12:53,769
computation going on it's it's more

00:12:50,980 --> 00:12:55,899
looking like like noise over here so

00:12:53,769 --> 00:12:59,139
when we when we zoom into this problem a

00:12:55,899 --> 00:13:01,370
bit deeper and look what's going on the

00:12:59,139 --> 00:13:06,440
on the pci express power rail he

00:13:01,370 --> 00:13:09,200
we see that the additional power that is

00:13:06,440 --> 00:13:12,920
drawn from the pcie rails when the FPGA

00:13:09,200 --> 00:13:14,960
is active is about 3 to 4 watts so this

00:13:12,920 --> 00:13:17,600
is the only power you need to compute

00:13:14,960 --> 00:13:19,790
the the FFT on the FPGA the rest is just

00:13:17,600 --> 00:13:23,240
a static power that the card consumes

00:13:19,790 --> 00:13:25,600
even when being idle okay but this is

00:13:23,240 --> 00:13:27,700
not the only parts we have another

00:13:25,600 --> 00:13:29,779
activity going on in the memories

00:13:27,700 --> 00:13:33,290
because we are fetching the data out of

00:13:29,779 --> 00:13:36,830
the hosts memory and this is shown over

00:13:33,290 --> 00:13:38,839
here so again we see when all threads

00:13:36,830 --> 00:13:41,270
are active this causes a lot of activity

00:13:38,839 --> 00:13:44,630
on your memory channels when we have

00:13:41,270 --> 00:13:48,110
only the FPGA making this very regular

00:13:44,630 --> 00:13:51,589
memory accesses the power consumption

00:13:48,110 --> 00:13:55,910
the memories is only a fraction so to

00:13:51,589 --> 00:13:59,390
put this in some numbers when we when we

00:13:55,910 --> 00:14:01,850
use one core with fftw we get to a power

00:13:59,390 --> 00:14:04,880
efficiency of about point2 gigaflops per

00:14:01,850 --> 00:14:09,650
for what we can increase the efficiency

00:14:04,880 --> 00:14:13,010
a bit when we when we go parallel when

00:14:09,650 --> 00:14:15,880
we use multiple threats and paralyze the

00:14:13,010 --> 00:14:18,800
computation but again this is about a

00:14:15,880 --> 00:14:21,620
point three gigaflops per watt when we

00:14:18,800 --> 00:14:25,720
used the AF you on the FPGA the

00:14:21,620 --> 00:14:29,900
efficiency increases or a magnitude and

00:14:25,720 --> 00:14:33,700
we can achieve almost a performance of a

00:14:29,900 --> 00:14:37,130
parallel version on the on the power 8

00:14:33,700 --> 00:14:40,040
okay so why was Cappy important in this

00:14:37,130 --> 00:14:42,890
case we really needed the coherent

00:14:40,040 --> 00:14:45,050
virtual memory addressing for for

00:14:42,890 --> 00:14:49,880
offloading this fine grained compute

00:14:45,050 --> 00:14:51,800
asuna to an accelerator a pci pci a

00:14:49,880 --> 00:14:55,160
driver stack wouldn't wouldn't allow us

00:14:51,800 --> 00:14:56,900
to do wouldn't allow to do this and this

00:14:55,160 --> 00:15:00,470
enables us to show really the energy

00:14:56,900 --> 00:15:02,120
efficiency of fpga computing and the fft

00:15:00,470 --> 00:15:04,580
is not the only thing we did so we have

00:15:02,120 --> 00:15:06,260
similar accelerators for regular

00:15:04,580 --> 00:15:08,839
expression matching and sparse linear

00:15:06,260 --> 00:15:10,579
algebra and we try to combine these

00:15:08,839 --> 00:15:15,120
solutions now for doing machine learning

00:15:10,579 --> 00:15:17,279
on fpga um that was my last slide

00:15:15,120 --> 00:15:21,140
you for mum for your attention and if

00:15:17,279 --> 00:15:21,140

YouTube URL: https://www.youtube.com/watch?v=gHpe6n5tTOY


