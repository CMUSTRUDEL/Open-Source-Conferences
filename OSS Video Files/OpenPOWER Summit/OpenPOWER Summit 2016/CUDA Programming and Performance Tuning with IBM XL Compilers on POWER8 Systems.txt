Title: CUDA Programming and Performance Tuning with IBM XL Compilers on POWER8 Systems
Publication date: 2016-04-29
Playlist: OpenPOWER Summit 2016
Description: 
	OpenPOWER Summit 2016
Presented by Shereen Ghobrial &  Istvan Reguly of IBM
Captions: 
	00:00:00,000 --> 00:00:04,950
so we'll make it a quick presentation

00:00:02,159 --> 00:00:07,680
I'll give you a quick summary of who I

00:00:04,950 --> 00:00:09,780
do you need to use XL compilers and open

00:00:07,680 --> 00:00:12,030
power then we'll have first-hand

00:00:09,780 --> 00:00:15,660
experience with the professor istafan

00:00:12,030 --> 00:00:18,869
from PP you see you and hungry and he

00:00:15,660 --> 00:00:20,880
will share some phase results as that he

00:00:18,869 --> 00:00:24,210
did it in association with Oxford a

00:00:20,880 --> 00:00:34,079
learning center so you will hear it from

00:00:24,210 --> 00:00:36,450
the experts so first iBM has invested

00:00:34,079 --> 00:00:39,809
long has long experience with compilers

00:00:36,450 --> 00:00:44,640
not only on on Linux power open power

00:00:39,809 --> 00:00:49,020
but on AIX in 0s etc and our experience

00:00:44,640 --> 00:00:53,520
for those compilers go across languages

00:00:49,020 --> 00:00:56,610
Fortran and C++ but what makes the

00:00:53,520 --> 00:00:59,340
compiler unique from excel compiler is

00:00:56,610 --> 00:01:01,859
the long history that we have developed

00:00:59,340 --> 00:01:04,379
to really get the best breed of

00:01:01,859 --> 00:01:07,350
technology optimization technology that

00:01:04,379 --> 00:01:08,729
we have in our back end most importantly

00:01:07,350 --> 00:01:10,650
I will highlight the interprocedural

00:01:08,729 --> 00:01:13,229
analysis and the profile directed

00:01:10,650 --> 00:01:15,900
feedback optimizations that you will not

00:01:13,229 --> 00:01:19,619
find it as mature for power architecture

00:01:15,900 --> 00:01:21,299
as you have it in the compiler one

00:01:19,619 --> 00:01:23,280
important thing of course that makes us

00:01:21,299 --> 00:01:25,170
very very unique is the full

00:01:23,280 --> 00:01:27,600
exploitation of the hardware we work

00:01:25,170 --> 00:01:31,079
very closely with the hardware designers

00:01:27,600 --> 00:01:34,880
for powerade power7 and upcoming power 9

00:01:31,079 --> 00:01:39,210
so we know exactly how the processor is

00:01:34,880 --> 00:01:40,950
acting in many cases we suggest changes

00:01:39,210 --> 00:01:43,950
to the processor or new instructions

00:01:40,950 --> 00:01:46,170
that will make things go faster so if

00:01:43,950 --> 00:01:48,509
you're really looking for somebody who

00:01:46,170 --> 00:01:51,720
know exactly about power that's the XL

00:01:48,509 --> 00:01:54,360
compiler from language standard

00:01:51,720 --> 00:01:58,009
compliance we are compliant with all the

00:01:54,360 --> 00:02:00,420
latest and greatest for C++ and Fortran

00:01:58,009 --> 00:02:02,189
one thing I would like to highlight and

00:02:00,420 --> 00:02:05,270
it's very important for people working

00:02:02,189 --> 00:02:07,049
on Linux is the fact that we are really

00:02:05,270 --> 00:02:10,739
dancing with the open source community

00:02:07,049 --> 00:02:13,260
if you can use that expression because

00:02:10,739 --> 00:02:13,650
the XL compiler is binary compatible

00:02:13,260 --> 00:02:15,629
with g

00:02:13,650 --> 00:02:16,769
see so if you compile part of the

00:02:15,629 --> 00:02:19,260
libraries or part of your application

00:02:16,769 --> 00:02:20,459
which you see you can easily compile the

00:02:19,260 --> 00:02:22,829
rest with Excel and get the best

00:02:20,459 --> 00:02:25,769
performance if you are migrating from

00:02:22,829 --> 00:02:28,439
another platform to power and you're

00:02:25,769 --> 00:02:33,180
coming from GCC all what you need just

00:02:28,439 --> 00:02:35,459
change the calling compiler command but

00:02:33,180 --> 00:02:37,680
the options the source file etc will

00:02:35,459 --> 00:02:41,939
stay the same so migration is very

00:02:37,680 --> 00:02:46,019
smooth finally these are a couple of

00:02:41,939 --> 00:02:48,079
performance tuning hints usually when

00:02:46,019 --> 00:02:50,489
you start it's important to identify

00:02:48,079 --> 00:02:53,700
where are the bottlenecks that you need

00:02:50,489 --> 00:02:57,269
to tackle for your program whether it's

00:02:53,700 --> 00:03:00,090
specific loop specific modules and for

00:02:57,269 --> 00:03:02,579
that you can use different tools that

00:03:00,090 --> 00:03:07,139
are available on the Linux like perv Oh

00:03:02,579 --> 00:03:09,629
p / GG prof etc then you can use the

00:03:07,139 --> 00:03:12,750
compiler to make you smarter by by

00:03:09,629 --> 00:03:15,720
really generating optimization report to

00:03:12,750 --> 00:03:17,879
tell you oh I was able to optimize this

00:03:15,720 --> 00:03:20,040
loop or I managed to find Cindy

00:03:17,879 --> 00:03:22,799
opportunity in this loop or this loop

00:03:20,040 --> 00:03:24,930
was not able to send eyes and it will

00:03:22,799 --> 00:03:26,250
give you some hints how you can make it

00:03:24,930 --> 00:03:28,769
better so you can change your source

00:03:26,250 --> 00:03:31,650
code to give more hints to the compiler

00:03:28,769 --> 00:03:33,750
for example aliasing information you say

00:03:31,650 --> 00:03:35,540
these variables will never overlap so

00:03:33,750 --> 00:03:38,879
the compiler can use this opportunity to

00:03:35,540 --> 00:03:41,819
give better performance typically we

00:03:38,879 --> 00:03:46,229
suggest people start with 02 03 if you

00:03:41,819 --> 00:03:48,629
are a analytic workload or

00:03:46,229 --> 00:03:52,590
floating-point workload then 03 is good

00:03:48,629 --> 00:03:54,720
with Q hot if you are more commercial a

00:03:52,590 --> 00:04:02,099
kind of off-the-shelf application then

00:03:54,720 --> 00:04:05,069
it's better to use 02 or 02 IPA in some

00:04:02,099 --> 00:04:10,079
cases it is important to use math

00:04:05,069 --> 00:04:13,709
libraries if you are hpc workload and if

00:04:10,079 --> 00:04:15,120
you are really keen about accuracy some

00:04:13,709 --> 00:04:17,400
optimizations will be more aggressive

00:04:15,120 --> 00:04:20,250
and overcome that but there are

00:04:17,400 --> 00:04:22,770
information and the compiler or hints

00:04:20,250 --> 00:04:24,659
that it can give it information that to

00:04:22,770 --> 00:04:27,240
have it strictness in terms of

00:04:24,659 --> 00:04:30,120
floating-point now we are at open

00:04:27,240 --> 00:04:32,520
Power Summit and as part of that

00:04:30,120 --> 00:04:34,440
community I'm glad to announce for the

00:04:32,520 --> 00:04:36,990
first time we will introduce a free

00:04:34,440 --> 00:04:39,330
compiler on the linux platform so

00:04:36,990 --> 00:04:42,389
starting this week we announced that it

00:04:39,330 --> 00:04:44,400
will be available in June it's it's a

00:04:42,389 --> 00:04:47,280
free of charge compiler there are no

00:04:44,400 --> 00:04:50,550
limit as many users can use it as you

00:04:47,280 --> 00:04:52,319
can you can use it for production use if

00:04:50,550 --> 00:04:55,910
you are really interested in service

00:04:52,319 --> 00:04:58,410
then you need to buy the paid compiler

00:04:55,910 --> 00:05:01,919
so with that I would like to leave you

00:04:58,410 --> 00:05:05,659
with three important messages before I

00:05:01,919 --> 00:05:08,729
pass it to Professor Steven first the

00:05:05,659 --> 00:05:10,860
compiler provide easy migration to open

00:05:08,729 --> 00:05:14,550
power platform from any other platform

00:05:10,860 --> 00:05:17,009
the other one is it we know that the

00:05:14,550 --> 00:05:19,699
hardware so we provide the industrial

00:05:17,009 --> 00:05:22,370
eating performance on power processors

00:05:19,699 --> 00:05:25,080
and last one we provide world-class

00:05:22,370 --> 00:05:31,849
support with that I'll pass it to

00:05:25,080 --> 00:05:35,310
professor's Japan Thank You Sharon so

00:05:31,849 --> 00:05:38,130
over the past year or so I had the good

00:05:35,310 --> 00:05:41,400
fortune of working with the IBM compiler

00:05:38,130 --> 00:05:44,250
dream since the very early compilers to

00:05:41,400 --> 00:05:48,300
really try to nail down high performance

00:05:44,250 --> 00:05:51,710
computing performance on the power 8 so

00:05:48,300 --> 00:05:56,810
I basically have a set of benchmarks

00:05:51,710 --> 00:05:59,580
compute and bandwidth intensive that are

00:05:56,810 --> 00:06:03,690
basically common patterns in high

00:05:59,580 --> 00:06:06,180
performance computing and to begin with

00:06:03,690 --> 00:06:09,030
this is basically the number one

00:06:06,180 --> 00:06:10,530
benchmark anyone runs in a system this

00:06:09,030 --> 00:06:16,860
is the stream benchmark that measures

00:06:10,530 --> 00:06:20,340
bandwidth to the ddr4 memory and you can

00:06:16,860 --> 00:06:24,870
see that we have two socket tan-tan core

00:06:20,340 --> 00:06:27,509
machine and or the maximum bandwidth we

00:06:24,870 --> 00:06:29,699
we can achieve is somewhere around 300

00:06:27,509 --> 00:06:31,800
gigabytes a second which is pretty

00:06:29,699 --> 00:06:35,460
impressive especially when compared to

00:06:31,800 --> 00:06:37,169
the competition but one thing to note

00:06:35,460 --> 00:06:39,660
there actually is you really need a

00:06:37,169 --> 00:06:43,020
considerable amount of data

00:06:39,660 --> 00:06:45,240
about gigabyte and a half per race or a

00:06:43,020 --> 00:06:48,450
total of four and a half gigabytes being

00:06:45,240 --> 00:06:51,660
streamed in from memory and then out to

00:06:48,450 --> 00:06:54,990
actually get there and in addition to

00:06:51,660 --> 00:06:58,560
that on power 8 you can have up to four

00:06:54,990 --> 00:07:01,200
physical threads on each core so from

00:06:58,560 --> 00:07:03,210
single threaded mode to a 72 four and

00:07:01,200 --> 00:07:05,940
eight and the hardware can actually

00:07:03,210 --> 00:07:08,670
dynamically switch between them but the

00:07:05,940 --> 00:07:11,460
point is again performance depends

00:07:08,670 --> 00:07:15,870
somewhat on how many threads you have

00:07:11,460 --> 00:07:20,730
per core at higher smt settings you you

00:07:15,870 --> 00:07:25,290
cannot really reach that memory

00:07:20,730 --> 00:07:29,850
bandwidth and also we have very nice

00:07:25,290 --> 00:07:33,000
caching and with about what is that Arab

00:07:29,850 --> 00:07:37,710
over a terabyte per second over this to

00:07:33,000 --> 00:07:40,250
socket machine so we can sort of expect

00:07:37,710 --> 00:07:43,200
now that for scientific applications

00:07:40,250 --> 00:07:46,560
that I really bandwidth-intensive this

00:07:43,200 --> 00:07:49,110
is sort of the maximum we can achieve we

00:07:46,560 --> 00:07:52,710
cannot really go beyond what this simple

00:07:49,110 --> 00:07:55,980
benchmark shows us the other side of the

00:07:52,710 --> 00:07:59,220
coin is of course compute and we have

00:07:55,980 --> 00:08:01,740
tried the essl library and specifically

00:07:59,220 --> 00:08:05,010
the single end up position general

00:08:01,740 --> 00:08:07,590
matrix-matrix multiply benchmarks and

00:08:05,010 --> 00:08:11,340
again looked at performance at different

00:08:07,590 --> 00:08:13,980
smt settings and we see that in single

00:08:11,340 --> 00:08:16,610
precision you get almost teraflop per

00:08:13,980 --> 00:08:20,280
second and in double precision about 500

00:08:16,610 --> 00:08:25,020
and again this is fairly similar to

00:08:20,280 --> 00:08:27,960
similar offerings from Intel to x 10

00:08:25,020 --> 00:08:32,190
core machine but we should note that you

00:08:27,960 --> 00:08:37,380
know intel has 256 bit vectors the power

00:08:32,190 --> 00:08:40,800
it has 128 bit vector registers but it

00:08:37,380 --> 00:08:44,190
does run at a higher clock and again we

00:08:40,800 --> 00:08:46,290
see that we only get this big

00:08:44,190 --> 00:08:49,920
performance at lower

00:08:46,290 --> 00:08:54,720
settings so our first benchmark is a

00:08:49,920 --> 00:08:57,290
black shows a 1d benchmark it's some

00:08:54,720 --> 00:09:02,000
very primitive form of a financial

00:08:57,290 --> 00:09:06,569
computation and we evaluate simple a

00:09:02,000 --> 00:09:09,839
scalar code which is the continuous

00:09:06,569 --> 00:09:13,410
lines and then we actually developed

00:09:09,839 --> 00:09:17,519
some manually vectorized code using alt

00:09:13,410 --> 00:09:21,660
evac which basically is a wrapper for

00:09:17,519 --> 00:09:24,810
the 128-bit vector registers and we see

00:09:21,660 --> 00:09:27,420
here that we no longer get this big

00:09:24,810 --> 00:09:29,880
difference between smt settings but

00:09:27,420 --> 00:09:33,800
still the best performance you can get

00:09:29,880 --> 00:09:36,360
at lower SMP settings and it is actually

00:09:33,800 --> 00:09:38,490
even though it is a compute heavy

00:09:36,360 --> 00:09:41,069
benchmark it's still very competitive

00:09:38,490 --> 00:09:46,380
with Intel numbers and on the implicit

00:09:41,069 --> 00:09:48,810
case which is this bottom ones which

00:09:46,380 --> 00:09:51,480
does not vectorize very valid a very

00:09:48,810 --> 00:09:53,880
sequential operation in solving a

00:09:51,480 --> 00:09:57,420
tridiagonal set of equations using the

00:09:53,880 --> 00:10:00,690
thermos algorithm there it even

00:09:57,420 --> 00:10:02,970
outperforms the Intel machines because

00:10:00,690 --> 00:10:08,029
there is really the clock rate that

00:10:02,970 --> 00:10:11,519
drives performance looking at another

00:10:08,029 --> 00:10:13,170
benchmark this time this is a structured

00:10:11,519 --> 00:10:16,709
mesh partial differential equation

00:10:13,170 --> 00:10:19,410
solver it's called code cloverleaf 3d

00:10:16,709 --> 00:10:21,990
it's in the Montego sweet this is a

00:10:19,410 --> 00:10:24,779
primarily bandwidth limited application

00:10:21,990 --> 00:10:29,370
and it it consists of about 50 different

00:10:24,779 --> 00:10:33,180
finite difference like stencil

00:10:29,370 --> 00:10:37,110
computation loops and here we also

00:10:33,180 --> 00:10:41,639
compare performance of pure MPI with NPI

00:10:37,110 --> 00:10:45,329
plus OpenMP this is mostly in Fortran

00:10:41,639 --> 00:10:50,399
actually and we can see that again we

00:10:45,329 --> 00:10:53,069
get best performance around smt 2 or smt

00:10:50,399 --> 00:10:57,120
for and actually if we get a sustained

00:10:53,069 --> 00:10:58,110
bandwidth of on average of 160 gigabytes

00:10:57,120 --> 00:11:00,779
a second

00:10:58,110 --> 00:11:04,890
which is pretty good on on simpler

00:11:00,779 --> 00:11:07,950
kernels we get 271 more computationally

00:11:04,890 --> 00:11:10,980
complex and more latency bound kernels

00:11:07,950 --> 00:11:14,209
we only get seven gigabytes a second but

00:11:10,980 --> 00:11:18,029
on average it's 160 and this is actually

00:11:14,209 --> 00:11:22,709
one and a half times faster almost than

00:11:18,029 --> 00:11:25,920
a k40 dpu let alone an intel cpu that's

00:11:22,709 --> 00:11:28,140
another factor of two there so this is a

00:11:25,920 --> 00:11:30,420
very nice-looking application this is a

00:11:28,140 --> 00:11:32,940
structured mesh stencil computations

00:11:30,420 --> 00:11:36,930
this actually vectorized is very well so

00:11:32,940 --> 00:11:39,390
the IBM compilers went and vectorized i

00:11:36,930 --> 00:11:41,690
think about ninety percent of the

00:11:39,390 --> 00:11:44,970
computational looks in this application

00:11:41,690 --> 00:11:47,390
the other example would be an

00:11:44,970 --> 00:11:50,760
unstructured mesh finite volume method

00:11:47,390 --> 00:11:53,010
application where computations are much

00:11:50,760 --> 00:11:56,329
more irregular you no longer have that

00:11:53,010 --> 00:11:59,060
nice structured memory access patterns

00:11:56,329 --> 00:12:03,449
the compiler can no longer really

00:11:59,060 --> 00:12:07,740
automatically vectorize any of this but

00:12:03,449 --> 00:12:11,730
even then we see fairly good performance

00:12:07,740 --> 00:12:14,070
although more with mpi vid OpenMP for

00:12:11,730 --> 00:12:16,620
some reason performance just goes up and

00:12:14,070 --> 00:12:20,250
it's significantly worse than just pure

00:12:16,620 --> 00:12:22,640
MPI I mean seeing a performance

00:12:20,250 --> 00:12:25,740
difference or our performance

00:12:22,640 --> 00:12:28,230
improvement when going from MPI OpenMP

00:12:25,740 --> 00:12:30,690
to just pure MPI we have seen on other

00:12:28,230 --> 00:12:36,089
platforms as well not quite to this

00:12:30,690 --> 00:12:39,120
extent though so again we we are a

00:12:36,089 --> 00:12:43,019
little slower than in the structured

00:12:39,120 --> 00:12:46,980
mesh applications but still fairly good

00:12:43,019 --> 00:12:50,160
and whereas the previous example was a

00:12:46,980 --> 00:12:53,699
very small application we have also

00:12:50,160 --> 00:12:56,240
tested this on the production CFD

00:12:53,699 --> 00:13:01,440
application at rolls-royce called Hydra

00:12:56,240 --> 00:13:04,470
which actually compiled almost you know

00:13:01,440 --> 00:13:07,589
without any changes just tweaking a few

00:13:04,470 --> 00:13:10,920
compiler flags and again we ran similar

00:13:07,589 --> 00:13:12,080
tests and we see that the power 8 ended

00:13:10,920 --> 00:13:15,890
up being about one

00:13:12,080 --> 00:13:20,060
half acts of what an Intel machine can

00:13:15,890 --> 00:13:21,890
do but in this case the GPU is a bit

00:13:20,060 --> 00:13:23,780
faster this is a much more complicated

00:13:21,890 --> 00:13:25,850
application and again this is

00:13:23,780 --> 00:13:31,120
unstructured mesh so it doesn't

00:13:25,850 --> 00:13:34,430
vectorize as well so that's it about the

00:13:31,120 --> 00:13:37,940
benchmarks the last thing I wanted to

00:13:34,430 --> 00:13:41,990
mention is IBM has put out this

00:13:37,940 --> 00:13:45,080
developer challenge where you can

00:13:41,990 --> 00:13:48,590
compete in in two tracks the open road

00:13:45,080 --> 00:13:53,570
test and the spark rally I guess you can

00:13:48,590 --> 00:13:55,930
read all the details in that link so

00:13:53,570 --> 00:13:55,930

YouTube URL: https://www.youtube.com/watch?v=ZiqHXIeMFw8


