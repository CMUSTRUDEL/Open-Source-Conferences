Title: OpenPOWER Summit 2016 - Power-Efficient Machine Learning on POWER systems using FPGA Acceleration
Publication date: 2016-04-29
Playlist: OpenPOWER Summit 2016
Description: 
	Presented by Ralph Wittig of Xilinix
Captions: 
	00:00:00,319 --> 00:00:05,009
agoura good afternoon my name is Ralph

00:00:03,449 --> 00:00:06,750
Witek I'm here from Xilinx I'm actually

00:00:05,009 --> 00:00:09,120
quite excited to be here we've been

00:00:06,750 --> 00:00:12,929
collaborating with IBM for several years

00:00:09,120 --> 00:00:15,240
now and bringing open power to the

00:00:12,929 --> 00:00:18,000
market open power is all about

00:00:15,240 --> 00:00:20,990
acceleration and we have a great

00:00:18,000 --> 00:00:23,400
accelerator story with chef PJ's on this

00:00:20,990 --> 00:00:24,539
great platform and I'm also excited to

00:00:23,400 --> 00:00:26,400
be here because this conference is

00:00:24,539 --> 00:00:27,750
really as it turns out all about machine

00:00:26,400 --> 00:00:29,730
learning this was the topic we chose

00:00:27,750 --> 00:00:33,260
well before we were aware of what the

00:00:29,730 --> 00:00:35,520
main theme was of his greater GTC

00:00:33,260 --> 00:00:38,700
conference and so I'd like to really

00:00:35,520 --> 00:00:40,469
talk about today about how we use FPGAs

00:00:38,700 --> 00:00:44,850
to accelerate machine learning on the

00:00:40,469 --> 00:00:46,110
great power platform when I was a

00:00:44,850 --> 00:00:48,000
student in Toronto

00:00:46,110 --> 00:00:49,379
there was some crazy guys in the AI

00:00:48,000 --> 00:00:51,559
department trying to build neural

00:00:49,379 --> 00:00:53,670
networks and I was sort of admired their

00:00:51,559 --> 00:00:55,980
passion for this technology back then

00:00:53,670 --> 00:00:57,530
was completely not feasible but it was a

00:00:55,980 --> 00:01:02,850
lot more compute available these days

00:00:57,530 --> 00:01:05,909
and and great trained reference so image

00:01:02,850 --> 00:01:07,380
sets and other sample sets this

00:01:05,909 --> 00:01:10,670
technology's really come together what

00:01:07,380 --> 00:01:10,670
you see in the blue dots here is really

00:01:11,060 --> 00:01:18,150
networks or not networks but this is the

00:01:14,130 --> 00:01:21,689
image net competition where you're

00:01:18,150 --> 00:01:24,900
trying to beat others predicting the in

00:01:21,689 --> 00:01:26,790
identifying images and and you see that

00:01:24,900 --> 00:01:28,350
the blue dots these were all non AI or

00:01:26,790 --> 00:01:29,939
non neural network based technologies

00:01:28,350 --> 00:01:32,729
and suddenly into in 2012

00:01:29,939 --> 00:01:36,270
Alex Khrushchev skiiers Network Alex net

00:01:32,729 --> 00:01:38,040
showed up and anatomy beat the all the

00:01:36,270 --> 00:01:39,630
blue dots probably quite by quite a

00:01:38,040 --> 00:01:41,790
margin and then you look at the next

00:01:39,630 --> 00:01:43,619
year the following year there's more

00:01:41,790 --> 00:01:45,500
green dots than blue dots on the scale

00:01:43,619 --> 00:01:47,579
and then after well there's no more

00:01:45,500 --> 00:01:48,119
non-ai meffert's being used at all

00:01:47,579 --> 00:01:49,290
anymore

00:01:48,119 --> 00:01:52,380
well it's actually kind of nice is that

00:01:49,290 --> 00:01:55,049
in 2015 Microsoft actually came up with

00:01:52,380 --> 00:01:57,240
a network that's actually more accurate

00:01:55,049 --> 00:01:59,780
than what the human but humans can

00:01:57,240 --> 00:02:03,420
predict and so essentially in summary

00:01:59,780 --> 00:02:05,579
CNN's by far outperform what non-ai

00:02:03,420 --> 00:02:07,890
methods can deliver and in the meantime

00:02:05,579 --> 00:02:09,780
CNN's have essentially delivered better

00:02:07,890 --> 00:02:11,670
accuracy than humans can right so this

00:02:09,780 --> 00:02:13,560
is applied to image search image

00:02:11,670 --> 00:02:13,980
detection but it really applies to other

00:02:13,560 --> 00:02:17,459
areas

00:02:13,980 --> 00:02:20,190
well when you're running Siri or so an

00:02:17,459 --> 00:02:22,290
iPhone or an Android speech recognition

00:02:20,190 --> 00:02:25,890
that's all AI methods based in the mean

00:02:22,290 --> 00:02:27,420
time so how do these things work very

00:02:25,890 --> 00:02:30,030
brief I mean you probably attended many

00:02:27,420 --> 00:02:31,799
other sessions you have feature

00:02:30,030 --> 00:02:33,120
extraction layers and then

00:02:31,799 --> 00:02:34,170
classification layers in the feature

00:02:33,120 --> 00:02:36,330
extraction there's a bunch of

00:02:34,170 --> 00:02:38,849
convolution operations that we draw out

00:02:36,330 --> 00:02:41,519
different ways of looking at with input

00:02:38,849 --> 00:02:43,290
image has in terms of sharp edges or

00:02:41,519 --> 00:02:44,730
diagonal stripes or whatever not and

00:02:43,290 --> 00:02:47,069
then the different features are

00:02:44,730 --> 00:02:49,170
extracted from from from the image and

00:02:47,069 --> 00:02:51,150
then from that information that you have

00:02:49,170 --> 00:02:52,769
you pull it back together and classify

00:02:51,150 --> 00:02:54,720
which based on what you've really seen

00:02:52,769 --> 00:02:56,430
and then the end you find out or is it a

00:02:54,720 --> 00:02:58,739
road sign or is it something else is it

00:02:56,430 --> 00:02:59,129
a speed limit sign or is it something

00:02:58,739 --> 00:03:01,829
else

00:02:59,129 --> 00:03:03,209
is it 60 kilometers an hour or is it

00:03:01,829 --> 00:03:06,060
whatever so that's what the

00:03:03,209 --> 00:03:08,280
classification stages really do looking

00:03:06,060 --> 00:03:10,440
at Alex Ned again this this network

00:03:08,280 --> 00:03:14,310
model is representative of a deeper

00:03:10,440 --> 00:03:15,810
convolution based network it has six or

00:03:14,310 --> 00:03:18,359
seven convolutional layers up front and

00:03:15,810 --> 00:03:19,500
then three dense layers in the end what

00:03:18,359 --> 00:03:21,840
is it sort of highlight what the

00:03:19,500 --> 00:03:23,880
computational operations are because

00:03:21,840 --> 00:03:26,160
there's a lot of information in them

00:03:23,880 --> 00:03:27,180
that we can exploit in terms of form

00:03:26,160 --> 00:03:30,180
when it comes to implementing these

00:03:27,180 --> 00:03:32,970
networks looking at one of the later

00:03:30,180 --> 00:03:36,540
convolutional layers where a 13 by 13 by

00:03:32,970 --> 00:03:39,810
384 volume is convolved with a 3 by 3

00:03:36,540 --> 00:03:43,380
filter into another 13 by 13 by in this

00:03:39,810 --> 00:03:46,889
case 256 layer deep volume so the

00:03:43,380 --> 00:03:48,900
compute that takes place is you have the

00:03:46,889 --> 00:03:54,049
input volume you take you a filter sized

00:03:48,900 --> 00:03:57,180
image sub region at 3 by 3 volume it's

00:03:54,049 --> 00:03:59,310
384 layers deep and you convolve it with

00:03:57,180 --> 00:04:02,430
the weights volume that's also a 3 by 3

00:03:59,310 --> 00:04:05,250
in size and 384 entries deep what you do

00:04:02,430 --> 00:04:08,040
is you multiply every element in the

00:04:05,250 --> 00:04:09,810
feature map sub volume times every

00:04:08,040 --> 00:04:12,480
element times the corresponding element

00:04:09,810 --> 00:04:13,919
inside the weights volume so there's

00:04:12,480 --> 00:04:15,840
quite a few multiplications above just

00:04:13,919 --> 00:04:19,169
to produce one upper pixel right and

00:04:15,840 --> 00:04:21,750
then you advance your sub feature map by

00:04:19,169 --> 00:04:24,300
one element or by whatever the stride is

00:04:21,750 --> 00:04:26,099
say the stride is one multiplied by the

00:04:24,300 --> 00:04:27,740
same weight set the same weight set is

00:04:26,099 --> 00:04:30,210
shared across all the pixels on the same

00:04:27,740 --> 00:04:33,570
right and so on the fourth bring it

00:04:30,210 --> 00:04:36,300
across go back to the beginning but Mach

00:04:33,570 --> 00:04:37,740
one level down and then do the

00:04:36,300 --> 00:04:39,570
multiplication but again the same way it

00:04:37,740 --> 00:04:40,800
said until you're fully done and you'll

00:04:39,570 --> 00:04:43,140
notice that there is a quite a bit of

00:04:40,800 --> 00:04:45,810
overlap so the weights are reused but if

00:04:43,140 --> 00:04:48,000
I was doing back between a subvolume and

00:04:45,810 --> 00:04:50,520
its neighbor there's also in this case

00:04:48,000 --> 00:04:51,810
for a 3x3 a two-thirds overlap so

00:04:50,520 --> 00:04:53,820
there's a lot of data we use and there's

00:04:51,810 --> 00:04:55,080
certainly even more weights we use and

00:04:53,820 --> 00:04:58,140
that's what you really have to exploit

00:04:55,080 --> 00:05:00,450
when implementing networks so go across

00:04:58,140 --> 00:05:02,910
the row next row all the way across

00:05:00,450 --> 00:05:06,630
build your first plane then jump back to

00:05:02,910 --> 00:05:08,880
the same sub volume but now go to a

00:05:06,630 --> 00:05:10,260
different weight set and that brings you

00:05:08,880 --> 00:05:11,310
the first picks of the second plane and

00:05:10,260 --> 00:05:14,820
so on so forth

00:05:11,310 --> 00:05:15,960
until you build one plane after the

00:05:14,820 --> 00:05:17,670
other until the entire volume is

00:05:15,960 --> 00:05:19,380
generated so you can see there's a lot

00:05:17,670 --> 00:05:21,870
of compute and there's a lot of data

00:05:19,380 --> 00:05:24,000
views over here so for the fully

00:05:21,870 --> 00:05:26,010
connected layers they are not based on

00:05:24,000 --> 00:05:28,380
convolutions or these massive reductions

00:05:26,010 --> 00:05:30,590
but they based on essentially a matrix

00:05:28,380 --> 00:05:35,930
times vector multiplication so you have

00:05:30,590 --> 00:05:38,700
an input row back to the you have a

00:05:35,930 --> 00:05:42,000
dense layer that's basically a vector

00:05:38,700 --> 00:05:44,610
and every element in in one layer

00:05:42,000 --> 00:05:46,800
contributes to the to every element in

00:05:44,610 --> 00:05:52,680
the in the next layer right so you

00:05:46,800 --> 00:05:55,080
basically have you have say an element

00:05:52,680 --> 00:05:58,440
in the blue layer is a function of every

00:05:55,080 --> 00:06:00,210
element in the previous orange layer so

00:05:58,440 --> 00:06:02,370
you need a lot of weights and there's

00:06:00,210 --> 00:06:03,510
not a lot of beings over here and so if

00:06:02,370 --> 00:06:06,530
you look at the properties of these

00:06:03,510 --> 00:06:08,850
networks and there's nice work done by

00:06:06,530 --> 00:06:11,070
multiple universities that I'd like to

00:06:08,850 --> 00:06:12,780
really bring your attention to today the

00:06:11,070 --> 00:06:16,080
team at Tsinghua actually done sort of

00:06:12,780 --> 00:06:17,910
nice characteristics summary the

00:06:16,080 --> 00:06:20,310
convolutional layers are fully dominated

00:06:17,910 --> 00:06:21,540
by compute well the dense layers or the

00:06:20,310 --> 00:06:23,010
fully connected layers are really

00:06:21,540 --> 00:06:25,500
dominated by

00:06:23,010 --> 00:06:28,740
weights or memory memory weight

00:06:25,500 --> 00:06:29,940
transfers or memory bandwidth and and so

00:06:28,740 --> 00:06:34,169
this is what we have to really keep in

00:06:29,940 --> 00:06:38,660
mind so looking at me this this accuracy

00:06:34,169 --> 00:06:40,739
comment so why machines have become more

00:06:38,660 --> 00:06:43,800
accurate

00:06:40,739 --> 00:06:46,529
than what humans can deliver in image

00:06:43,800 --> 00:06:49,759
recognition for example machines are by

00:06:46,529 --> 00:06:52,229
far not as efficient as well you mean

00:06:49,759 --> 00:06:55,349
what a human brain is so this is a

00:06:52,229 --> 00:06:59,459
somewhat older slide from about 2012

00:06:55,349 --> 00:07:02,009
this is a IBM Watson computer compared

00:06:59,459 --> 00:07:03,689
to a human brain human brain has given

00:07:02,009 --> 00:07:05,429
take 20 watts and whether it has ten

00:07:03,689 --> 00:07:08,279
petaflop so over 20 pedo flops of

00:07:05,429 --> 00:07:09,539
compute is to be debated but compare

00:07:08,279 --> 00:07:12,330
that to the to the Watson computer

00:07:09,539 --> 00:07:15,509
you're off by six orders of magnitude

00:07:12,330 --> 00:07:17,550
inefficiency and now we've had of course

00:07:15,509 --> 00:07:20,399
a generation or two or more law since

00:07:17,550 --> 00:07:23,069
2012 but the point is that the machines

00:07:20,399 --> 00:07:25,079
are still far less efficient than humans

00:07:23,069 --> 00:07:26,069
still holds and so we have a long way to

00:07:25,079 --> 00:07:28,529
go if you're wondering when our

00:07:26,069 --> 00:07:31,409
machine's gonna take over in terms of

00:07:28,529 --> 00:07:33,449
intelligence we have a way to go okay so

00:07:31,409 --> 00:07:35,909
this should bring this home to your kids

00:07:33,449 --> 00:07:37,069
or your spouse's and say this is all

00:07:35,909 --> 00:07:40,559
just fine

00:07:37,069 --> 00:07:43,379
so looking at the cost of compute you

00:07:40,559 --> 00:07:45,479
see that custom compute is really

00:07:43,379 --> 00:07:46,740
touching external memory is very

00:07:45,479 --> 00:07:48,300
expensive you don't want to do that too

00:07:46,740 --> 00:07:50,819
often and if you have weights that you

00:07:48,300 --> 00:07:52,649
can be used bring them in once cache

00:07:50,819 --> 00:07:55,379
them for use in multiple times that's a

00:07:52,649 --> 00:07:57,360
real key to performance and then in

00:07:55,379 --> 00:07:59,009
terms of looking at multiplication

00:07:57,360 --> 00:08:00,240
operator these are a lot of

00:07:59,009 --> 00:08:02,490
multiplication operations come you know

00:08:00,240 --> 00:08:04,050
these networks try not to use

00:08:02,490 --> 00:08:05,550
floating-point but try to be in fixed

00:08:04,050 --> 00:08:08,069
point and try not to be in 32 bits but

00:08:05,550 --> 00:08:10,619
try to be in 16 bits or possibly even

00:08:08,069 --> 00:08:13,739
eight bits but any question as how much

00:08:10,619 --> 00:08:15,989
accuracy is enough right so but the

00:08:13,739 --> 00:08:18,419
takeaway is staying on chip memories as

00:08:15,989 --> 00:08:19,829
much as you can so it's about flexible

00:08:18,419 --> 00:08:22,619
unguent memory structures to match the

00:08:19,829 --> 00:08:24,509
problem use smaller multipliers right

00:08:22,619 --> 00:08:26,309
and ideally stay in fixed point don't

00:08:24,509 --> 00:08:29,519
waste bits on on dynamic ranges in

00:08:26,309 --> 00:08:32,519
floating point so there's other tricks

00:08:29,519 --> 00:08:35,189
and this is still fairly new there is a

00:08:32,519 --> 00:08:37,949
great work Kong in academia and build

00:08:35,189 --> 00:08:40,259
Ally both at Nvidia and also really cost

00:08:37,949 --> 00:08:41,789
upon it as a professor at Stanford his

00:08:40,259 --> 00:08:45,149
team has done great work they've

00:08:41,789 --> 00:08:46,230
actually looked at how much information

00:08:45,149 --> 00:08:48,889
you really need to carry in these

00:08:46,230 --> 00:08:51,000
networks and they've come up with

00:08:48,889 --> 00:08:52,920
results that are quite interesting you

00:08:51,000 --> 00:08:53,670
can actually take a network you say sort

00:08:52,920 --> 00:08:55,830
of a

00:08:53,670 --> 00:08:57,800
densely connected fully symmetrical

00:08:55,830 --> 00:09:00,390
network you can look at which of these

00:08:57,800 --> 00:09:03,090
synapses which really weight

00:09:00,390 --> 00:09:06,690
contributing to the picture of the next

00:09:03,090 --> 00:09:08,550
layer which of these synapses has a

00:09:06,690 --> 00:09:10,410
significant weight on it and the ones

00:09:08,550 --> 00:09:12,840
that are lightly weighted might as well

00:09:10,410 --> 00:09:14,970
just prune away trim away then take the

00:09:12,840 --> 00:09:17,850
remaining network and retrain it right

00:09:14,970 --> 00:09:19,980
and they got quite amazing results first

00:09:17,850 --> 00:09:22,200
of all they said we can reduce the

00:09:19,980 --> 00:09:24,960
number of weights which is these

00:09:22,200 --> 00:09:25,770
synapses by a factor of nine that's

00:09:24,960 --> 00:09:27,930
significant

00:09:25,770 --> 00:09:30,180
remember it's all about keep saving

00:09:27,930 --> 00:09:32,220
energy not going touching external

00:09:30,180 --> 00:09:35,360
memory too much so changing our weights

00:09:32,220 --> 00:09:37,950
brings you significant energy reduction

00:09:35,360 --> 00:09:40,290
and then they stood this was even more

00:09:37,950 --> 00:09:42,810
interesting that even when you trim this

00:09:40,290 --> 00:09:45,510
many weights you hardly have any loss in

00:09:42,810 --> 00:09:46,800
accuracy so that was the most surprising

00:09:45,510 --> 00:09:48,360
when I first saw this work that was most

00:09:46,800 --> 00:09:50,100
surprising to me but this is huge

00:09:48,360 --> 00:09:52,080
because for these fully connected nails

00:09:50,100 --> 00:09:54,540
which is all about memory transfers that

00:09:52,080 --> 00:09:56,850
big savings can be had if you just prune

00:09:54,540 --> 00:09:59,970
the network compress the remainder and

00:09:56,850 --> 00:10:04,400
then uncompressed when you come back to

00:09:59,970 --> 00:10:07,500
the unzipped computer so for precision

00:10:04,400 --> 00:10:09,330
the same team is Stanford indicated that

00:10:07,500 --> 00:10:12,680
you don't have to initially use 32-bit

00:10:09,330 --> 00:10:15,360
floating point but 16-bit integer is

00:10:12,680 --> 00:10:18,090
perfectly good enough so much lower

00:10:15,360 --> 00:10:21,410
energy and hardly any loss in accuracy

00:10:18,090 --> 00:10:24,150
they predicted that for 8-bit integer

00:10:21,410 --> 00:10:25,800
but you'd see a loss in accuracy but in

00:10:24,150 --> 00:10:29,040
the meantime there's more work by the

00:10:25,800 --> 00:10:30,960
team back at Tsinghua that sort of said

00:10:29,040 --> 00:10:33,000
you can actually even get away with

00:10:30,960 --> 00:10:36,710
eight bits and both for data bits as

00:10:33,000 --> 00:10:40,320
well as for weight bits if you perform

00:10:36,710 --> 00:10:43,440
dynamic quantization so Yuri quantized

00:10:40,320 --> 00:10:45,780
after each layer so dynamic they mean a

00:10:43,440 --> 00:10:50,280
fixed quantization but it's unique only

00:10:45,780 --> 00:10:53,760
it on a per day basis so versus if you

00:10:50,280 --> 00:10:55,830
chop with the same quantization at each

00:10:53,760 --> 00:10:58,350
layer you do see a significant loss in

00:10:55,830 --> 00:11:00,720
accuracy but if you customize your

00:10:58,350 --> 00:11:03,000
quantization the precision required on

00:11:00,720 --> 00:11:04,440
each on a layer by layer basis you get

00:11:03,000 --> 00:11:05,730
away with eight bits as well possibly

00:11:04,440 --> 00:11:07,260
even four bits so this is actually

00:11:05,730 --> 00:11:10,870
getting quite interesting

00:11:07,260 --> 00:11:13,000
so in summary fixed-point is definitely

00:11:10,870 --> 00:11:15,550
sufficient for the inference part of the

00:11:13,000 --> 00:11:18,010
ford passes of these neural networks in

00:11:15,550 --> 00:11:19,660
60 and possibly even in Dade there's no

00:11:18,010 --> 00:11:22,660
significant loss in accuracy for less

00:11:19,660 --> 00:11:25,780
precision used drawing with into eight

00:11:22,660 --> 00:11:27,610
versus floating-point 32 you can save

00:11:25,780 --> 00:11:29,980
yourself quite a bit of energy because

00:11:27,610 --> 00:11:31,480
of multiplier doing integer

00:11:29,980 --> 00:11:32,890
multiplication is simpler than

00:11:31,480 --> 00:11:36,340
floating-point and likewise fewer bits

00:11:32,890 --> 00:11:40,450
means extra energy savings and if you

00:11:36,340 --> 00:11:43,390
pack data as in eight verses in 32 or

00:11:40,450 --> 00:11:45,400
fp3 to use 1/4 of the bits and that

00:11:43,390 --> 00:11:49,630
gives you a new memory energy savings as

00:11:45,400 --> 00:11:51,520
well so what we really do with these

00:11:49,630 --> 00:11:53,050
networks and see we take the model that

00:11:51,520 --> 00:11:56,200
the computer scientists that really come

00:11:53,050 --> 00:11:59,800
up with we prune them tomahtoes weights

00:11:56,200 --> 00:12:01,960
that have no contribution we train the

00:11:59,800 --> 00:12:04,270
network based on existing the remaining

00:12:01,960 --> 00:12:05,920
weights and then figure out what

00:12:04,270 --> 00:12:09,460
quantization we have to apply after you

00:12:05,920 --> 00:12:13,060
layer and go from floating point to

00:12:09,460 --> 00:12:15,340
fixed point okay and then now let's look

00:12:13,060 --> 00:12:17,370
at so that was techniques that you can

00:12:15,340 --> 00:12:19,390
apply independent of which

00:12:17,370 --> 00:12:22,960
implementation or processor architecture

00:12:19,390 --> 00:12:26,140
use independent of CPU or GPU or FPGA or

00:12:22,960 --> 00:12:27,070
DSP now just look at how we exploit some

00:12:26,140 --> 00:12:30,850
of these properties of these networks

00:12:27,070 --> 00:12:33,550
using FPGAs so here's an example of a

00:12:30,850 --> 00:12:35,890
accelerated card this card is qualified

00:12:33,550 --> 00:12:38,500
for open power it's available it's

00:12:35,890 --> 00:12:40,470
actually for my science partner Alex an

00:12:38,500 --> 00:12:44,920
IBM partner company called alpha data

00:12:40,470 --> 00:12:49,270
the open power capi interface has been

00:12:44,920 --> 00:12:53,650
implemented on this pci card the device

00:12:49,270 --> 00:12:55,930
on there has 5500 DSP blocks that run up

00:12:53,650 --> 00:13:00,570
to 500 megahertz so this part would get

00:12:55,930 --> 00:13:03,250
up to 5.5 ter ups of integer 16

00:13:00,570 --> 00:13:06,790
performance the DSP blocks on FPGA

00:13:03,250 --> 00:13:10,240
optimized for in 16 operation and you're

00:13:06,790 --> 00:13:12,400
sitting in a small so a half-height

00:13:10,240 --> 00:13:15,930
half-length was low profile form factor

00:13:12,400 --> 00:13:19,390
thermal envelope so less than 75 Watts

00:13:15,930 --> 00:13:19,699
an FPGA itself is this regular array to

00:13:19,390 --> 00:13:23,239
dementia

00:13:19,699 --> 00:13:26,209
array of lookup table of resources or

00:13:23,239 --> 00:13:29,259
configurable logic blocks DSPs is

00:13:26,209 --> 00:13:32,029
essentially 18 bit term a 16-bit integer

00:13:29,259 --> 00:13:34,730
multiplied accumulates as well as

00:13:32,029 --> 00:13:36,949
embedded rams alright and what's really

00:13:34,730 --> 00:13:39,980
nice about this architecture is that you

00:13:36,949 --> 00:13:41,839
can do the bit level processing right

00:13:39,980 --> 00:13:43,489
next to where the memories are that

00:13:41,839 --> 00:13:45,859
actually hold the data so you're very

00:13:43,489 --> 00:13:47,660
doing close to memory compute and at the

00:13:45,859 --> 00:13:49,879
same time this is all strung together

00:13:47,660 --> 00:13:51,439
with an interconnect that allows you to

00:13:49,879 --> 00:13:54,619
broadcast which essentially is about

00:13:51,439 --> 00:13:55,939
weight reuse so you can exploit a lot of

00:13:54,619 --> 00:13:59,029
these properties of these networks with

00:13:55,939 --> 00:14:02,749
these FPGA architectures at the heart of

00:13:59,029 --> 00:14:05,149
the math is an 18-bit my reach shown as

00:14:02,749 --> 00:14:08,359
a 16-bit multiplier block over here that

00:14:05,149 --> 00:14:10,609
is followed by 48 bit accumulator and

00:14:08,359 --> 00:14:12,199
then you have a custom quantization

00:14:10,609 --> 00:14:14,720
element where you can trim it back down

00:14:12,199 --> 00:14:16,249
from 40 8-bits to 16-bits or even less

00:14:14,720 --> 00:14:18,739
and you can figure out where you want to

00:14:16,249 --> 00:14:20,149
place your decimal dot right and before

00:14:18,739 --> 00:14:23,419
and after that you've got memory

00:14:20,149 --> 00:14:27,169
elements these unship brands that can

00:14:23,419 --> 00:14:29,179
store data as in for in date in 1632

00:14:27,169 --> 00:14:31,609
floating-point or not so you can

00:14:29,179 --> 00:14:34,609
basically figure out what precision you

00:14:31,609 --> 00:14:36,559
want to use for the weights and data

00:14:34,609 --> 00:14:38,689
sets that you store in on chip Rams and

00:14:36,559 --> 00:14:41,569
then you can decide if you're running in

00:14:38,689 --> 00:14:43,720
16 bits multiplier or if you're using as

00:14:41,569 --> 00:14:45,529
an 8-bit multiplier you don't get to

00:14:43,720 --> 00:14:47,419
fracture there multiple you don't get to

00:14:45,529 --> 00:14:49,639
use two to 8-bit multipliers for every

00:14:47,419 --> 00:14:51,529
element you only get one 16-bit or one

00:14:49,639 --> 00:14:53,709
8-bit multiplier but if you're using 8

00:14:51,529 --> 00:14:57,489
bits you're running a lower energy cost

00:14:53,709 --> 00:15:00,980
okay so now we're building up these

00:14:57,489 --> 00:15:02,569
operations we have the at the heart of

00:15:00,980 --> 00:15:05,029
the architecture we're building a

00:15:02,569 --> 00:15:06,609
convolve err say we build a 3 by 3 con

00:15:05,029 --> 00:15:09,049
Volvo I showed you the example of the

00:15:06,609 --> 00:15:11,389
the network layer that wanted a 3 by 3

00:15:09,049 --> 00:15:13,879
filter size so a 3 by 3 we convolve ER

00:15:11,389 --> 00:15:16,699
and over there we take a data buffer of

00:15:13,879 --> 00:15:20,119
3 by 3 elements and weight buffer of 3

00:15:16,699 --> 00:15:21,859
by 3 elements and we feed a 3 by 3

00:15:20,119 --> 00:15:24,429
multiplier array in parallel right so

00:15:21,859 --> 00:15:27,709
we're basically doing not just

00:15:24,429 --> 00:15:30,019
sequential it's really spatial data

00:15:27,709 --> 00:15:31,879
parallel processing and what's nice over

00:15:30,019 --> 00:15:32,960
here is remember we saw that there was

00:15:31,879 --> 00:15:36,020
an overlap in the data

00:15:32,960 --> 00:15:38,180
the feature map subvolume from one pixel

00:15:36,020 --> 00:15:38,960
to the next pixel and it was an overlap

00:15:38,180 --> 00:15:40,970
by two-thirds

00:15:38,960 --> 00:15:42,530
you can actually exploit this feature by

00:15:40,970 --> 00:15:44,510
just sort of streaming and data through

00:15:42,530 --> 00:15:46,640
these data buffers pushing in one new

00:15:44,510 --> 00:15:48,680
element every cycle one falls off and

00:15:46,640 --> 00:15:51,500
you haven't we use a data pair longer

00:15:48,680 --> 00:15:53,240
use of 8 over 9 right so it's quite a

00:15:51,500 --> 00:15:55,130
bit of data reuse and then likewise for

00:15:53,240 --> 00:15:57,020
the weight buffer you you use the

00:15:55,130 --> 00:15:58,850
weights quite a bit but in the meantime

00:15:57,020 --> 00:16:00,260
while using current set of weights you

00:15:58,850 --> 00:16:01,670
you're you're bringing in the next set

00:16:00,260 --> 00:16:03,530
of where it's already using ping pong

00:16:01,670 --> 00:16:06,470
buffering what's really nice is that you

00:16:03,530 --> 00:16:08,930
can schedule the data push to these on

00:16:06,470 --> 00:16:10,910
chip ramps to arrive just in time you

00:16:08,930 --> 00:16:13,120
don't have to wait for memory misses and

00:16:10,910 --> 00:16:16,790
so on the data is there when you need it

00:16:13,120 --> 00:16:20,000
ok so then you take some of these

00:16:16,790 --> 00:16:21,470
control these controllers notice it's

00:16:20,000 --> 00:16:23,540
block called see over here this is a

00:16:21,470 --> 00:16:25,370
slide and you build a bunch of these

00:16:23,540 --> 00:16:26,840
controllers in parallel all right and

00:16:25,370 --> 00:16:29,450
then you feed them from the same input

00:16:26,840 --> 00:16:31,580
buffers and go into a common output

00:16:29,450 --> 00:16:33,140
buffer and in between you have this

00:16:31,580 --> 00:16:35,060
adder tree or this reduction tree

00:16:33,140 --> 00:16:37,070
because convolution is about massive

00:16:35,060 --> 00:16:41,330
reduction you're getting one pixel out

00:16:37,070 --> 00:16:44,300
for a massive input volume so in with

00:16:41,330 --> 00:16:46,250
these input buffers we can using the on

00:16:44,300 --> 00:16:48,500
chip interconnect on the FPGA s we can

00:16:46,250 --> 00:16:49,910
broadcast the weights and have the same

00:16:48,500 --> 00:16:52,280
way to be used by multiple controller

00:16:49,910 --> 00:16:55,760
engines so offloading the pressure on an

00:16:52,280 --> 00:16:56,960
off chip in an off chip memories ok and

00:16:55,760 --> 00:16:58,310
then we have as I said earlier the

00:16:56,960 --> 00:17:00,260
custom quantization block and we can

00:16:58,310 --> 00:17:02,210
quantize differently on each layer of

00:17:00,260 --> 00:17:03,500
the network okay so that's one

00:17:02,210 --> 00:17:05,810
processing element built out of multiple

00:17:03,500 --> 00:17:07,430
convolve our tiles and then we have

00:17:05,810 --> 00:17:11,720
multiple processing elements tied

00:17:07,430 --> 00:17:13,610
together into the overall neural network

00:17:11,720 --> 00:17:15,470
processor architectures implement on the

00:17:13,610 --> 00:17:19,190
FPGA and over there we can then have

00:17:15,470 --> 00:17:21,230
dmas prefetching data from from the big

00:17:19,190 --> 00:17:23,320
store the external memory store and we

00:17:21,230 --> 00:17:29,050
can then decompress the data on the fly

00:17:23,320 --> 00:17:29,050
in case we actually had prune networks

00:17:29,980 --> 00:17:34,490
and so all while we're doing this

00:17:32,870 --> 00:17:36,680
there's ping-pong buffering going on at

00:17:34,490 --> 00:17:38,900
this layer of the NAAFI of the processor

00:17:36,680 --> 00:17:41,450
as well so we're really exploiting

00:17:38,900 --> 00:17:43,340
multiple PPEs inside of people have

00:17:41,450 --> 00:17:45,320
multiple controllers and within a cotton

00:17:43,340 --> 00:17:47,750
ball what we have data parallelism

00:17:45,320 --> 00:17:50,810
the size of your filter to be cut it

00:17:47,750 --> 00:17:53,240
quite a facing architecture that tiles

00:17:50,810 --> 00:17:57,110
well we have this semi static dataflow

00:17:53,240 --> 00:17:58,790
semi static as in it's different so the

00:17:57,110 --> 00:18:00,860
the network topology is different for

00:17:58,790 --> 00:18:03,770
each layer but we know what the topology

00:18:00,860 --> 00:18:05,810
is up front and we can then push the

00:18:03,770 --> 00:18:08,750
relevant data sets into the computer

00:18:05,810 --> 00:18:10,670
resources just in time and while we're

00:18:08,750 --> 00:18:12,320
doing this we essentially doing

00:18:10,670 --> 00:18:17,060
broadcasting of weights and so there's

00:18:12,320 --> 00:18:18,560
about memory we use going on so but what

00:18:17,060 --> 00:18:20,630
we find is most efficient now bringing

00:18:18,560 --> 00:18:23,120
back to the main theme of this

00:18:20,630 --> 00:18:25,700
conference of your open power open power

00:18:23,120 --> 00:18:29,110
has a great coprocessor interface open

00:18:25,700 --> 00:18:33,430
power capi allows really the sharing of

00:18:29,110 --> 00:18:36,110
processors host memory as virtual memory

00:18:33,430 --> 00:18:40,280
visible to the accelerator right so we

00:18:36,110 --> 00:18:42,980
have system-wide shared memory we have

00:18:40,280 --> 00:18:44,480
system-wide coherency we don't have to

00:18:42,980 --> 00:18:48,080
flash cache there and we spend any time

00:18:44,480 --> 00:18:49,610
flushing caches and there's actually a

00:18:48,080 --> 00:18:51,800
low latency control channel involved

00:18:49,610 --> 00:18:55,040
where we can then synchronize and what

00:18:51,800 --> 00:18:57,280
to work on next what we find is very

00:18:55,040 --> 00:18:59,630
useful over here is this peer processing

00:18:57,280 --> 00:19:01,520
memory model and this peer processing

00:18:59,630 --> 00:19:04,340
synchronization model because in the end

00:19:01,520 --> 00:19:05,630
you have a processor doing some work at

00:19:04,340 --> 00:19:08,510
the imaan extreme the processor just

00:19:05,630 --> 00:19:10,510
coordinates what the FPGA processes but

00:19:08,510 --> 00:19:12,470
you can have a much more finer grain

00:19:10,510 --> 00:19:14,510
division of work you can run the

00:19:12,470 --> 00:19:16,400
convolution of the computation the

00:19:14,510 --> 00:19:18,230
computationally intense layers on the

00:19:16,400 --> 00:19:21,650
FPGA you can run the memory intensive

00:19:18,230 --> 00:19:22,790
layers on the CPU alright it's all just

00:19:21,650 --> 00:19:24,710
with pointer passing back and forth

00:19:22,790 --> 00:19:27,770
there's no data that needs to be copied

00:19:24,710 --> 00:19:31,370
and pushed around that's what old power

00:19:27,770 --> 00:19:33,620
and copy really enable and so we then

00:19:31,370 --> 00:19:34,970
run the main frameworks cafe or

00:19:33,620 --> 00:19:37,760
tensorflow or whatever not you're

00:19:34,970 --> 00:19:41,600
capturing your neural networks in on the

00:19:37,760 --> 00:19:43,670
CPU the CPU loads the model the CPU that

00:19:41,600 --> 00:19:45,830
calls these accelerated libraries

00:19:43,670 --> 00:19:47,660
they'll be running on the FPGA s you

00:19:45,830 --> 00:19:48,830
don't really design these architectures

00:19:47,660 --> 00:19:50,420
that I showed on the previous slides

00:19:48,830 --> 00:19:52,100
yourself there's libraries for all this

00:19:50,420 --> 00:19:53,540
and there's certainly a lot of effort

00:19:52,100 --> 00:19:54,920
going on to these libraries we have

00:19:53,540 --> 00:19:56,310
multiple different library partners are

00:19:54,920 --> 00:19:58,980
mentioning one over here

00:19:56,310 --> 00:20:01,170
company called Elvis and then on the

00:19:58,980 --> 00:20:02,940
FPGA here on the DLN kernels but that's

00:20:01,170 --> 00:20:05,130
just the convolution or the convolution

00:20:02,940 --> 00:20:09,470
in the fully connected layers is is

00:20:05,130 --> 00:20:13,140
opportunistic choice it's privatized so

00:20:09,470 --> 00:20:14,730
with the FPGA you're getting honestly

00:20:13,140 --> 00:20:16,800
the highest super performance I think

00:20:14,730 --> 00:20:19,350
for absolute throughput GPUs are still

00:20:16,800 --> 00:20:22,110
great if you're looking for low latency

00:20:19,350 --> 00:20:24,870
that says one processing that's what the

00:20:22,110 --> 00:20:28,130
fpw shines right and so you're getting

00:20:24,870 --> 00:20:31,800
quite a bit of compute performance in a

00:20:28,130 --> 00:20:36,630
low profile card which is also a low

00:20:31,800 --> 00:20:38,160
profile TDP alt accelerating the machine

00:20:36,630 --> 00:20:42,930
learning applications on power eight

00:20:38,160 --> 00:20:46,200
systems quite nicely so in summary the

00:20:42,930 --> 00:20:48,980
FPGA is this architecture that allows

00:20:46,200 --> 00:20:51,510
you to customize you see in a data flow

00:20:48,980 --> 00:20:54,750
to the computer resources present and

00:20:51,510 --> 00:20:56,790
get exploit all the concurrency present

00:20:54,750 --> 00:21:00,300
the parallelism present the weights

00:20:56,790 --> 00:21:04,410
reuse etc etc but really this is enabled

00:21:00,300 --> 00:21:06,780
using this power eight capi peer

00:21:04,410 --> 00:21:09,150
processor model well we don't have to

00:21:06,780 --> 00:21:11,100
push data back and forth we just send

00:21:09,150 --> 00:21:14,780
pointers and the data flow just works

00:21:11,100 --> 00:21:17,730
out from there and finally you can find

00:21:14,780 --> 00:21:19,320
efficient libraries for convolutional

00:21:17,730 --> 00:21:21,120
net current convolutional neural

00:21:19,320 --> 00:21:23,660
networks for FPGA is from the islands

00:21:21,120 --> 00:21:27,260
partners so thanks for your time and

00:21:23,660 --> 00:21:27,260

YouTube URL: https://www.youtube.com/watch?v=Yi80P5U2YAE


