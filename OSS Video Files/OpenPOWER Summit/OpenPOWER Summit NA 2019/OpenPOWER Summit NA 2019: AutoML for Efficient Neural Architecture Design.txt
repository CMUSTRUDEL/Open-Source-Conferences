Title: OpenPOWER Summit NA 2019: AutoML for Efficient Neural Architecture Design
Publication date: 2019-08-20
Playlist: OpenPOWER Summit NA 2019
Description: 
	Presented by Ligeng Zhu, MIT

Deep learning has drawn growing attention in recent years. The impressive performance comes at scale of computational power, which makes it hard to execute especially on mobile devices. Besides, designing a good and efficient neural-net requires a lot of engineering efforts, let alone the following platform/hardware specific deployment. To reduce the human-bandwidth, we propose ProxylessNAS -- an efficient framework that automatically specializes neural architecture for different hardwares. With 74.5% top-1 accuracy, the measured latency of ProxylessNAS is 1.8x faster than MobileNet-v2, a widely used human design for mobile vision.
Captions: 
	00:00:00,199 --> 00:00:04,830
okay hi everyone it's my pleasure to be

00:00:04,259 --> 00:00:07,230
here

00:00:04,830 --> 00:00:09,269
so I'm Lee Ganju so currently a research

00:00:07,230 --> 00:00:16,619
assistant from MIT Hana

00:00:09,269 --> 00:00:19,320
so today I'm going to present our okay

00:00:16,619 --> 00:00:21,810
today I'm going to present our recent

00:00:19,320 --> 00:00:25,439
work out email for official EU

00:00:21,810 --> 00:00:28,220
architecture design so the planning has

00:00:25,439 --> 00:00:32,070
made a great process in recent years and

00:00:28,220 --> 00:00:34,680
along with the process so is dinner side

00:00:32,070 --> 00:00:37,500
and a computation size as well as the

00:00:34,680 --> 00:00:39,840
model devs has grown directly so for

00:00:37,500 --> 00:00:43,579
last century a simple matter of only

00:00:39,840 --> 00:00:47,730
five years network now it has become to

00:00:43,579 --> 00:00:50,190
152 layers even this kind of network has

00:00:47,730 --> 00:00:52,340
outperformed human and certain

00:00:50,190 --> 00:00:55,199
benchmarks and has very impressive

00:00:52,340 --> 00:00:57,870
applications in various of domains

00:00:55,199 --> 00:01:01,170
it's become really hard for human to

00:00:57,870 --> 00:01:03,239
deploy it as well as desired consider if

00:01:01,170 --> 00:01:06,000
every layer may have like for example

00:01:03,239 --> 00:01:10,650
ten possible combinations and a time to

00:01:06,000 --> 00:01:14,100
the times of 100 155 is not some space

00:01:10,650 --> 00:01:17,189
can be explored by human experts use

00:01:14,100 --> 00:01:19,740
mean time so such a network who makes a

00:01:17,189 --> 00:01:24,060
specialization is very hard so

00:01:19,740 --> 00:01:25,890
previously people used a simple single

00:01:24,060 --> 00:01:28,470
efficient neural network for our

00:01:25,890 --> 00:01:30,659
platform and our data size because

00:01:28,470 --> 00:01:33,180
designing an efficient neural network

00:01:30,659 --> 00:01:36,180
takes a lot of engineer efforts and a

00:01:33,180 --> 00:01:38,970
lot of trial and errors but we notice

00:01:36,180 --> 00:01:41,579
that actually different hollow words has

00:01:38,970 --> 00:01:44,310
different preference so for example CPU

00:01:41,579 --> 00:01:46,740
has how frequency but not good as at a

00:01:44,310 --> 00:01:48,930
power station and a GPU is very good at

00:01:46,740 --> 00:01:52,710
the power stations but has a lower clock

00:01:48,930 --> 00:01:56,040
speed so such a difference well it's not

00:01:52,710 --> 00:01:58,920
a very well studied in the corona scene

00:01:56,040 --> 00:02:00,960
design domain so currently we will sink

00:01:58,920 --> 00:02:03,719
isatis so machine learning wants to a

00:02:00,960 --> 00:02:05,719
generalization so one models once

00:02:03,719 --> 00:02:09,420
designed that can be deployed everywhere

00:02:05,719 --> 00:02:12,690
every devices but actually Hardware

00:02:09,420 --> 00:02:13,860
requires a specialization so need us aha

00:02:12,690 --> 00:02:16,530
see you

00:02:13,860 --> 00:02:18,660
especially design and fit into some

00:02:16,530 --> 00:02:22,170
properties also its own hardware

00:02:18,660 --> 00:02:24,120
so I generalize the model to handle

00:02:22,170 --> 00:02:27,360
specialized the hardware is actually not

00:02:24,120 --> 00:02:31,110
ideal so you can see we have many

00:02:27,360 --> 00:02:34,800
different platform so the FPGA is GPU

00:02:31,110 --> 00:02:36,660
CPU SOC and many many etc and we also

00:02:34,800 --> 00:02:39,840
have kind of different data set like

00:02:36,660 --> 00:02:43,830
this see far image not places anymore

00:02:39,840 --> 00:02:47,130
are so if you for our each scenario and

00:02:43,830 --> 00:02:50,430
each Hardware you do a specialization

00:02:47,130 --> 00:02:53,459
design then it's not that horrible you

00:02:50,430 --> 00:02:56,430
need this kind of combinations you don't

00:02:53,459 --> 00:02:59,040
have some human bandwidth to explore so

00:02:56,430 --> 00:03:02,370
we what do we want is we want to move

00:02:59,040 --> 00:03:04,590
with a manually aside from some manual

00:03:02,370 --> 00:03:06,810
designed to automatically send less than

00:03:04,590 --> 00:03:09,450
neural network designed by some machine

00:03:06,810 --> 00:03:13,260
learning algorithms instant up group of

00:03:09,450 --> 00:03:15,900
engineers many trial and arrows yeah so

00:03:13,260 --> 00:03:19,470
this is our proposed the bell of heaven

00:03:15,900 --> 00:03:22,220
so previously we only give them a one

00:03:19,470 --> 00:03:23,340
good network like the rest net

00:03:22,220 --> 00:03:26,310
inceptions

00:03:23,340 --> 00:03:29,640
dance that also move on that for various

00:03:26,310 --> 00:03:32,700
platforms and now we want to have our

00:03:29,640 --> 00:03:35,130
albums and ultimately designs for one

00:03:32,700 --> 00:03:37,560
platform and one thing has said in a one

00:03:35,130 --> 00:03:40,590
scenario so such as specializations

00:03:37,560 --> 00:03:42,920
cannot only reduce the effort of four

00:03:40,590 --> 00:03:46,110
designer network but also allows our

00:03:42,920 --> 00:03:49,860
ultimately design network to runs much

00:03:46,110 --> 00:03:53,190
faster and another benefit of this

00:03:49,860 --> 00:03:55,200
automation is it along snow expert to

00:03:53,190 --> 00:03:57,239
benefit is growing of the machine

00:03:55,200 --> 00:03:59,190
learning Allyson's so currently is the

00:03:57,239 --> 00:04:01,040
senior network is kind of like a

00:03:59,190 --> 00:04:03,930
privilege for machine learning

00:04:01,040 --> 00:04:05,760
researchers and big companies since like

00:04:03,930 --> 00:04:08,130
so many companies does not have some

00:04:05,760 --> 00:04:11,160
anyways develop a team to develop their

00:04:08,130 --> 00:04:13,170
own network but we search her design

00:04:11,160 --> 00:04:15,299
automation you just needed to press the

00:04:13,170 --> 00:04:18,959
button and you can do a specialization

00:04:15,299 --> 00:04:21,600
for your own user case so this allows no

00:04:18,959 --> 00:04:28,099
expert to efficiently design your

00:04:21,600 --> 00:04:28,099
network and wrongs faster on CR Omaha

00:04:28,130 --> 00:04:34,860
okay ah yeah here is the background and

00:04:31,470 --> 00:04:36,840
now we are going to a massive apart so

00:04:34,860 --> 00:04:38,580
conversional knows yeah

00:04:36,840 --> 00:04:42,510
so you're accessed search has been

00:04:38,580 --> 00:04:45,570
expelled for a while but I stated before

00:04:42,510 --> 00:04:47,220
so people want ultimately design because

00:04:45,570 --> 00:04:49,940
this thing is human design is

00:04:47,220 --> 00:04:52,500
time-consuming and expensive

00:04:49,940 --> 00:04:54,840
conventional NASA so made some progress

00:04:52,500 --> 00:04:58,050
on the performance but still very

00:04:54,840 --> 00:05:00,120
expensive like the first for example not

00:04:58,050 --> 00:05:03,210
Schneider proposed by Google in a

00:05:00,120 --> 00:05:05,310
request 48,000 to few hours which means

00:05:03,210 --> 00:05:07,949
if you only have one ship here in a

00:05:05,310 --> 00:05:10,830
required to run one year yeah so

00:05:07,949 --> 00:05:12,840
actually if maybe even more expensive us

00:05:10,830 --> 00:05:15,000
than you hire a group or researcher to

00:05:12,840 --> 00:05:17,550
design at well for you and there are

00:05:15,000 --> 00:05:18,330
some improvements like one famous wine

00:05:17,550 --> 00:05:20,789
stars

00:05:18,330 --> 00:05:23,550
so that's greatly reduced such a few

00:05:20,789 --> 00:05:26,370
hours but however it'll requires a much

00:05:23,550 --> 00:05:28,320
larger memories so check your memory

00:05:26,370 --> 00:05:30,750
unlike the CPU one you may have like

00:05:28,320 --> 00:05:33,630
hundreds of gigabytes or even one

00:05:30,750 --> 00:05:36,720
terabyte so currently even the most of

00:05:33,630 --> 00:05:39,599
the fancy GPUs can only have 32 memory

00:05:36,720 --> 00:05:42,360
chip chip here like so we 100 however

00:05:39,599 --> 00:05:45,150
the thoughts if you want to do a human

00:05:42,360 --> 00:05:47,849
ever fully design so it will require a

00:05:45,150 --> 00:05:50,780
100 gigabyte of memory which it exceeds

00:05:47,849 --> 00:05:55,080
the limits of most modern GPUs and

00:05:50,780 --> 00:05:58,979
because also expensive on a few hours or

00:05:55,080 --> 00:06:03,720
GPU memory so most of us work previously

00:05:58,979 --> 00:06:07,289
are based on the proxy so proxy is has

00:06:03,720 --> 00:06:11,190
several common ways so one common way is

00:06:07,289 --> 00:06:13,199
okay so we may not search on your final

00:06:11,190 --> 00:06:15,090
dinner cell because your final dinners

00:06:13,199 --> 00:06:18,150
that maybe fail may be very large like

00:06:15,090 --> 00:06:20,400
image that it has millions of images so

00:06:18,150 --> 00:06:22,500
instead they do not search on millions

00:06:20,400 --> 00:06:24,990
of images they just search maybe on a

00:06:22,500 --> 00:06:27,090
small they are selling Civitan so which

00:06:24,990 --> 00:06:29,820
only consists all 450 cells in the

00:06:27,090 --> 00:06:32,610
images so it can greatly accelerate as a

00:06:29,820 --> 00:06:35,820
process or maybe you say to now searches

00:06:32,610 --> 00:06:38,460
a whole architecture so just a search a

00:06:35,820 --> 00:06:39,780
part of the architecture and the Builder

00:06:38,460 --> 00:06:43,860
differnet walkable

00:06:39,780 --> 00:06:46,620
in similar years and another way is okay

00:06:43,860 --> 00:06:49,200
maybe we do not need a true training

00:06:46,620 --> 00:06:51,000
hunky on converge because you want to

00:06:49,200 --> 00:06:53,370
treat a deep inner network to converge

00:06:51,000 --> 00:06:56,370
and usually take like hundreds of epochs

00:06:53,370 --> 00:06:59,070
and the you can be trained on five or

00:06:56,370 --> 00:06:59,790
tiny posts and to see which one is the

00:06:59,070 --> 00:07:03,060
best

00:06:59,790 --> 00:07:05,430
okay so such proxies well indeed uh give

00:07:03,060 --> 00:07:11,040
some improvements on the efficiency but

00:07:05,430 --> 00:07:13,800
it does not really help or it introduced

00:07:11,040 --> 00:07:16,980
some new problems so some limitations of

00:07:13,800 --> 00:07:18,840
proxy so is about the sub optimal so for

00:07:16,980 --> 00:07:20,640
example if you search on a small data

00:07:18,840 --> 00:07:23,520
set and the scent transfer to a large

00:07:20,640 --> 00:07:25,830
one which means many properties of the

00:07:23,520 --> 00:07:27,960
large data center is not considered so

00:07:25,830 --> 00:07:30,390
you are optimizing for the small data

00:07:27,960 --> 00:07:33,150
set instead of the large one and such

00:07:30,390 --> 00:07:35,940
gap may reduce the final performance and

00:07:33,150 --> 00:07:38,520
the one you search for some small

00:07:35,940 --> 00:07:41,310
attackers understand repeating yet

00:07:38,520 --> 00:07:43,680
actually your architect researcher loses

00:07:41,310 --> 00:07:47,760
the diversity your architecture is what

00:07:43,680 --> 00:07:50,340
just a series of repeating blocks not

00:07:47,760 --> 00:07:54,090
like specialized specialized each layer

00:07:50,340 --> 00:07:57,180
for your target scenario and there's a

00:07:54,090 --> 00:08:00,000
few epochs is also brings some new

00:07:57,180 --> 00:08:02,729
problem like for example when you do the

00:08:00,000 --> 00:08:04,919
training settings so models with fewer

00:08:02,729 --> 00:08:07,320
parameters usually learns larger if you

00:08:04,919 --> 00:08:11,070
do not consider with a decay so this

00:08:07,320 --> 00:08:14,039
also kind of sub optimal makes this such

00:08:11,070 --> 00:08:15,750
a proxy it's not idea but I wish is

00:08:14,039 --> 00:08:19,229
necessary because otherwise it's too

00:08:15,750 --> 00:08:21,570
costly and also such a proxy means you

00:08:19,229 --> 00:08:23,970
cannot direct optimize for your tunnel

00:08:21,570 --> 00:08:26,910
hardware so currently most previous

00:08:23,970 --> 00:08:28,979
workers they only consider like accuracy

00:08:26,910 --> 00:08:31,530
as only evaluation magic a fourth-year

00:08:28,979 --> 00:08:33,810
search yeah so our work

00:08:31,530 --> 00:08:37,289
aim to solve these problems is called

00:08:33,810 --> 00:08:39,960
proxies nos so Louie one is okay

00:08:37,289 --> 00:08:42,690
conventionally NASA relies on the proxy

00:08:39,960 --> 00:08:45,330
tasks and we want to remove the proxy so

00:08:42,690 --> 00:08:49,230
we just we why we call the proxy days

00:08:45,330 --> 00:08:52,200
and if we remove the proxies so the cost

00:08:49,230 --> 00:08:53,100
becomes gross regularly so we want to

00:08:52,200 --> 00:08:56,970
make our

00:08:53,100 --> 00:08:59,399
Walker efficient so our girl is directly

00:08:56,970 --> 00:09:01,680
yeah we saw proxy learn architecture on

00:08:59,399 --> 00:09:04,800
the target hearts and at our hardware

00:09:01,680 --> 00:09:07,529
well I'll well allows all blockings to

00:09:04,800 --> 00:09:08,670
have different structures and in a very

00:09:07,529 --> 00:09:13,649
efficient manner

00:09:08,670 --> 00:09:16,050
yeah so in general we will actually the

00:09:13,649 --> 00:09:19,079
by two ways so first is reducing the

00:09:16,050 --> 00:09:21,329
cost of nos to the regular level of our

00:09:19,079 --> 00:09:23,970
training so which means in now you to a

00:09:21,329 --> 00:09:27,209
search is the cost of search our

00:09:23,970 --> 00:09:29,370
official model is as cheap as done on

00:09:27,209 --> 00:09:32,220
one network and the second one is

00:09:29,370 --> 00:09:35,060
cooperating hardware feedbacks into our

00:09:32,220 --> 00:09:37,529
search process so search search

00:09:35,060 --> 00:09:42,959
architecture can be customized

00:09:37,529 --> 00:09:45,420
we are targeted hardware ok so yeah so

00:09:42,959 --> 00:09:48,329
the first part is to make an ass more

00:09:45,420 --> 00:09:50,670
efficient so unlike many big-name

00:09:48,329 --> 00:09:54,720
industries which has a lot of Engineers

00:09:50,670 --> 00:09:57,930
Eloheinu clusters and a lot of huge few

00:09:54,720 --> 00:09:58,649
parts yeah we do not have such a power

00:09:57,930 --> 00:10:01,259
yeah

00:09:58,649 --> 00:10:04,500
so we only have to thing about small

00:10:01,259 --> 00:10:08,279
items yeah but luckily we find an

00:10:04,500 --> 00:10:11,759
efficient Allison so our met model our

00:10:08,279 --> 00:10:14,519
messes in spouts of France combination

00:10:11,759 --> 00:10:17,880
of monochrome compression and a new

00:10:14,519 --> 00:10:20,970
architecture search so we the GPO ours

00:10:17,880 --> 00:10:23,540
bimodal compression yeah and we still

00:10:20,970 --> 00:10:28,529
the GPU memory is by binarization and

00:10:23,540 --> 00:10:30,779
here is our details of our Alliston so

00:10:28,529 --> 00:10:34,319
first we formulate reformulate as an ass

00:10:30,779 --> 00:10:36,839
program not Allison so conventionally so

00:10:34,319 --> 00:10:38,970
say Susan a search is based on the

00:10:36,839 --> 00:10:41,790
reinforcement learning every time this

00:10:38,970 --> 00:10:43,769
sample our network yeah and the trainer

00:10:41,790 --> 00:10:45,660
from scratch gets the feed back into the

00:10:43,769 --> 00:10:48,870
reinforced agent and it performs the

00:10:45,660 --> 00:10:51,060
next step yeah and in our work will

00:10:48,870 --> 00:10:53,220
reformulate her into a pruning past

00:10:51,060 --> 00:10:57,449
level pruning problem so which means the

00:10:53,220 --> 00:10:59,730
first leg showing this figure we have a

00:10:57,449 --> 00:11:02,970
large network which consists of many

00:10:59,730 --> 00:11:05,939
possible prices so

00:11:02,970 --> 00:11:06,930
adds to search searching stage so each

00:11:05,939 --> 00:11:10,110
passage has

00:11:06,930 --> 00:11:13,770
it's architectural parameters so as each

00:11:10,110 --> 00:11:15,990
searching stage we train boasts of ways

00:11:13,770 --> 00:11:20,040
and also its architectural parameters

00:11:15,990 --> 00:11:22,620
yeah and after we finish the searching

00:11:20,040 --> 00:11:24,990
stage only surpass with higher still

00:11:22,620 --> 00:11:28,080
have architecture parameters will be

00:11:24,990 --> 00:11:30,450
preserved so after searching we can

00:11:28,080 --> 00:11:33,839
prove all other passes so we can get a

00:11:30,450 --> 00:11:36,570
stream network so by doing so we simply

00:11:33,839 --> 00:11:39,990
find us to a single training process all

00:11:36,570 --> 00:11:42,410
over parameterize the network so no

00:11:39,990 --> 00:11:45,390
meter controls no reinforce know

00:11:42,410 --> 00:11:48,000
multiple samplings so we only needed to

00:11:45,390 --> 00:11:51,270
trigger one join the network so it makes

00:11:48,000 --> 00:11:54,470
our safer the chip shows hours by 200

00:11:51,270 --> 00:11:58,890
times and next that we are going to do

00:11:54,470 --> 00:12:02,940
and yes here is like if we do it in a

00:11:58,890 --> 00:12:04,830
naive way so memory access so memory

00:12:02,940 --> 00:12:06,960
footprint is actually this gross as

00:12:04,830 --> 00:12:09,390
linearly as your candidate size which

00:12:06,960 --> 00:12:11,490
means if you have tank and 8 you may

00:12:09,390 --> 00:12:14,490
have like 10 times the larger memory if

00:12:11,490 --> 00:12:16,770
you have 110 days so you will have what

00:12:14,490 --> 00:12:19,080
an enhanced memory so which limits the

00:12:16,770 --> 00:12:21,750
total search space is not what we want

00:12:19,080 --> 00:12:24,810
so next we are going to save the GPU

00:12:21,750 --> 00:12:26,970
memory so to save the GPU memory

00:12:24,810 --> 00:12:29,279
so we binarize the architecture

00:12:26,970 --> 00:12:32,459
parameters and allow only one pass

00:12:29,279 --> 00:12:34,080
activations to be stored in the GPU

00:12:32,459 --> 00:12:38,550
during the forward and backward

00:12:34,080 --> 00:12:42,089
training's so by doing so no we'd reduce

00:12:38,550 --> 00:12:44,839
the memory footprint for Owen 201 and

00:12:42,089 --> 00:12:48,740
there's architectural parameters is not

00:12:44,839 --> 00:12:51,180
update can it's not updated by the

00:12:48,740 --> 00:12:54,000
gradient descent who can do it you know

00:12:51,180 --> 00:12:56,790
reinforcement learning and then we can

00:12:54,000 --> 00:12:59,100
reduce a memory footprint to a constant

00:12:56,790 --> 00:13:02,670
factor so we can perform search on a

00:12:59,100 --> 00:13:06,300
large search space and then by doing so

00:13:02,670 --> 00:13:09,120
we solves both a few hours to few hours

00:13:06,300 --> 00:13:13,290
and GPU memory issues and next we are

00:13:09,120 --> 00:13:16,410
going to do is to include a hardware

00:13:13,290 --> 00:13:19,050
feedback into our consideration so to

00:13:16,410 --> 00:13:20,130
performance however aware your archive

00:13:19,050 --> 00:13:23,120
search

00:13:20,130 --> 00:13:26,310
we needed to cancel agencies phones a

00:13:23,120 --> 00:13:29,820
target platform so in our experiment we

00:13:26,310 --> 00:13:33,030
focus on CPU GPU and a mobile CPU but

00:13:29,820 --> 00:13:34,980
you know if we directly mentions the

00:13:33,030 --> 00:13:38,700
target devices is very slow so for

00:13:34,980 --> 00:13:41,130
example inference network is usually 300

00:13:38,700 --> 00:13:43,290
milliseconds and together and precise

00:13:41,130 --> 00:13:46,680
number you need a to do like a at least

00:13:43,290 --> 00:13:49,380
20 times one to get an average so which

00:13:46,680 --> 00:13:52,440
means to get a one latency you need like

00:13:49,380 --> 00:13:55,710
six second so which was significantly

00:13:52,440 --> 00:13:58,200
slow your searching process so to avoid

00:13:55,710 --> 00:14:01,290
it we noticed one interesting yes

00:13:58,200 --> 00:14:03,090
because neural network actually so every

00:14:01,290 --> 00:14:06,630
layers of computations a memory access

00:14:03,090 --> 00:14:08,970
is very heavy and we can take a

00:14:06,630 --> 00:14:11,520
combination of every layer or every

00:14:08,970 --> 00:14:14,220
block as the independent process because

00:14:11,520 --> 00:14:17,580
every time you know we are fresh also

00:14:14,220 --> 00:14:21,390
caches so the computations of previous

00:14:17,580 --> 00:14:22,500
layer is not that important to this

00:14:21,390 --> 00:14:25,310
layer yeah

00:14:22,500 --> 00:14:27,450
so by doing so we can take hers in every

00:14:25,310 --> 00:14:29,670
Computing's of every layer as an

00:14:27,450 --> 00:14:33,120
independent process thus we can build a

00:14:29,670 --> 00:14:35,730
low latency lookup table to quickly

00:14:33,120 --> 00:14:39,810
estimate the latency and as you can see

00:14:35,730 --> 00:14:42,600
on the right so so y-axis is real agency

00:14:39,810 --> 00:14:44,640
and the x-axis is our estimated latency

00:14:42,600 --> 00:14:47,130
use the latency don't have table so say

00:14:44,640 --> 00:14:48,380
to actually match is quite weird which

00:14:47,130 --> 00:14:52,830
means the our latency

00:14:48,380 --> 00:14:56,360
lookup table is not good estimations so

00:14:52,830 --> 00:14:59,490
we use the latency good table as our

00:14:56,360 --> 00:15:02,370
economical way as an l2 negative as as

00:14:59,490 --> 00:15:04,140
we said so our words the team do not

00:15:02,370 --> 00:15:06,780
have many fancy

00:15:04,140 --> 00:15:09,000
hardware's and our result is quite

00:15:06,780 --> 00:15:12,480
limited so we have to do everything in

00:15:09,000 --> 00:15:16,880
an efficient manner so and so we can

00:15:12,480 --> 00:15:19,920
send optimized new architecture by using

00:15:16,880 --> 00:15:25,260
gradients or reinforced in both way and

00:15:19,920 --> 00:15:27,840
here is a search history of our

00:15:25,260 --> 00:15:30,290
architectures and you can see the first

00:15:27,840 --> 00:15:30,290
is

00:15:31,240 --> 00:15:37,150
so first first one is a certain process

00:15:33,970 --> 00:15:39,100
of flower-mobile model yes specialized

00:15:37,150 --> 00:15:41,070
stuff for the mobile CP CP oh and

00:15:39,100 --> 00:15:44,620
there's a second is specialized of some

00:15:41,070 --> 00:15:47,320
pad that's all secure and it's a third

00:15:44,620 --> 00:15:49,330
one in specialized of course that's how

00:15:47,320 --> 00:15:51,250
much you you so for the mobile phone

00:15:49,330 --> 00:15:53,950
where user pick someone and for the

00:15:51,250 --> 00:15:56,170
Scipio is the CEO and what you feel

00:15:53,950 --> 00:16:10,960
values will be 100 and here is this

00:15:56,170 --> 00:16:13,030
surgery's animation and so so blue one

00:16:10,960 --> 00:16:15,310
means the kernel is three by three and

00:16:13,030 --> 00:16:17,770
the yellow one is Colonel five by five

00:16:15,310 --> 00:16:20,080
and by the wise cannot seven by seven

00:16:17,770 --> 00:16:22,180
and there's always respect to the

00:16:20,080 --> 00:16:24,550
expansion ratio so how many channels

00:16:22,180 --> 00:16:27,160
inside each one so you can see during

00:16:24,550 --> 00:16:29,620
the search process so actually the

00:16:27,160 --> 00:16:32,350
vader's especially for the GPU one you

00:16:29,620 --> 00:16:36,040
become more shallow shallow and

00:16:32,350 --> 00:16:38,860
emotional because GPU has more paracin

00:16:36,040 --> 00:16:41,080
so it prefers a shallow networker but

00:16:38,860 --> 00:16:44,020
with large kernels so it can better

00:16:41,080 --> 00:16:45,940
paralyzed and of course the mobile cpu

00:16:44,020 --> 00:16:51,150
and the desk austin he'll say both

00:16:45,940 --> 00:16:54,700
choose are they both choose are slim but

00:16:51,150 --> 00:16:55,780
deep your architecture because they have

00:16:54,700 --> 00:16:58,630
less paradism

00:16:55,780 --> 00:17:01,330
yes and so one thing we have note here

00:16:58,630 --> 00:17:02,080
is very interesting is also the kernel

00:17:01,330 --> 00:17:04,390
choices

00:17:02,080 --> 00:17:07,330
come on size choices so if in a long

00:17:04,390 --> 00:17:10,390
time people believed so 3x3 Chronos is

00:17:07,330 --> 00:17:13,540
better because the three volcanoes to

00:17:10,390 --> 00:17:16,060
3x3 Cano's has the same receptive field

00:17:13,540 --> 00:17:18,280
but the last floors there are five by

00:17:16,060 --> 00:17:20,080
five colors but we know here is that

00:17:18,280 --> 00:17:23,320
actually in many platform like

00:17:20,080 --> 00:17:26,080
especially the mobile CPU and desktop

00:17:23,320 --> 00:17:27,940
said here even it has less flops but

00:17:26,080 --> 00:17:30,370
it's a given that who career caused you

00:17:27,940 --> 00:17:34,410
to storm water activations so it's

00:17:30,370 --> 00:17:37,150
actress lower in the real inference so

00:17:34,410 --> 00:17:39,460
choosing a large kernel and there

00:17:37,150 --> 00:17:42,450
reduces the net who adapts sometimes

00:17:39,460 --> 00:17:46,540
makes you on network even more efficient

00:17:42,450 --> 00:17:47,680
and here is our accuracies on the Steve

00:17:46,540 --> 00:17:49,780
attend dinner set

00:17:47,680 --> 00:17:52,150
so in the first section salsa dance

00:17:49,780 --> 00:17:54,550
night primate Shake Shack and the

00:17:52,150 --> 00:17:56,440
premiere tonight that process D so

00:17:54,550 --> 00:17:58,690
system models are designed about human

00:17:56,440 --> 00:18:03,370
experts and the you can see see I choose

00:17:58,690 --> 00:18:06,130
arrows above two point three one yeah

00:18:03,370 --> 00:18:08,440
and the single section is conventional

00:18:06,130 --> 00:18:10,630
mast air wisdom see try to search orange

00:18:08,440 --> 00:18:13,720
and you can see see achieve better

00:18:10,630 --> 00:18:16,720
performance and humanism and the last

00:18:13,720 --> 00:18:20,320
one you know proxies are and proxies G

00:18:16,720 --> 00:18:23,200
is our proposed mass the R stands for

00:18:20,320 --> 00:18:26,440
search by reinforce and the G stands for

00:18:23,200 --> 00:18:29,230
service agreement and you can see we our

00:18:26,440 --> 00:18:31,840
search the absence is much faster and

00:18:29,230 --> 00:18:35,260
previous it's much better than previous

00:18:31,840 --> 00:18:37,510
message and more notif Elise

00:18:35,260 --> 00:18:39,490
so even compared with the commercial

00:18:37,510 --> 00:18:42,820
Nast metaphor examples I'll move on that

00:18:39,490 --> 00:18:45,070
so the last one is a second reception so

00:18:42,820 --> 00:18:47,110
you know achieves a similar accuracy but

00:18:45,070 --> 00:18:50,290
you know use seven times the more

00:18:47,110 --> 00:18:52,810
practice than our network yeah which

00:18:50,290 --> 00:18:56,380
shows the power and effectiveness of our

00:18:52,810 --> 00:18:59,500
absence and next is our results answer

00:18:56,380 --> 00:19:01,690
image metadata said mobile platform so

00:18:59,500 --> 00:19:04,510
we choose a mobile at the b2 which is

00:19:01,690 --> 00:19:07,060
currently industry standard and our

00:19:04,510 --> 00:19:09,310
baseline to compare with you can see in

00:19:07,060 --> 00:19:11,530
different latency settings our models

00:19:09,310 --> 00:19:15,700
consistently outperform all mobile data

00:19:11,530 --> 00:19:19,930
v2 and the to achieve a crystal ever are

00:19:15,700 --> 00:19:23,380
wrong something for 0.7 0.8 yeah

00:19:19,930 --> 00:19:25,530
our network is a 1.8 times faster sense

00:19:23,380 --> 00:19:31,090
a move on either be - yes

00:19:25,530 --> 00:19:35,380
so alright and here is some variations

00:19:31,090 --> 00:19:38,980
Oh Mobile mobile efficiency neurons in

00:19:35,380 --> 00:19:42,700
mobile so for Samoa and v1 and v2 yeah

00:19:38,980 --> 00:19:47,880
so you have the top actresses up almost

00:19:42,700 --> 00:19:51,010
70 and no waste latency is 110 and 75

00:19:47,880 --> 00:19:53,470
respectively and the 14 last part

00:19:51,010 --> 00:19:55,390
so say improves the accuracy about

00:19:53,470 --> 00:19:56,020
greeting margin but however the

00:19:55,390 --> 00:19:57,400
latencies

00:19:56,020 --> 00:19:58,990
also grows across the state on all

00:19:57,400 --> 00:20:02,670
accounts consider how well feed about

00:19:58,990 --> 00:20:06,010
during the search yeah and also know his

00:20:02,670 --> 00:20:07,690
last column of the mass network in a

00:20:06,010 --> 00:20:10,660
request

00:20:07,690 --> 00:20:13,360
well ridiculous to few hours which is

00:20:10,660 --> 00:20:16,120
available for research team to reproduce

00:20:13,360 --> 00:20:21,760
and follows so this work are all done by

00:20:16,120 --> 00:20:24,190
Google and our work yeah you can see ok

00:20:21,760 --> 00:20:26,110
so we have various settings and then in

00:20:24,190 --> 00:20:29,560
all these settings that we achieve will

00:20:26,110 --> 00:20:31,450
our ecosystem is as good and some in

00:20:29,560 --> 00:20:34,630
some settings even better than

00:20:31,450 --> 00:20:36,640
Convention on us and as our work

00:20:34,630 --> 00:20:39,280
consider how do we feel about the during

00:20:36,640 --> 00:20:42,340
the searching process so our network is

00:20:39,280 --> 00:20:44,590
also runs much faster as fast as the

00:20:42,340 --> 00:20:47,590
original momentum of each you and the

00:20:44,590 --> 00:20:50,230
mostly not only so our search cost is

00:20:47,590 --> 00:20:52,030
just the two hundred rupee hours is at

00:20:50,230 --> 00:20:54,090
the same level of a training on your

00:20:52,030 --> 00:20:56,830
network which makes our work

00:20:54,090 --> 00:21:02,860
reproducible and affordable by other

00:20:56,830 --> 00:21:06,880
teams here's our results on the GPU

00:21:02,860 --> 00:21:08,830
platform so we measure human needs and

00:21:06,880 --> 00:21:13,930
models previous math works and our

00:21:08,830 --> 00:21:16,720
models on C V 150 oh yeah so you can see

00:21:13,930 --> 00:21:19,630
on this one so our models has the best

00:21:16,720 --> 00:21:22,690
accuracy is well runs the fastest are on

00:21:19,630 --> 00:21:24,640
some very 100 GPU and even come here

00:21:22,690 --> 00:21:27,010
with Google reason and last network

00:21:24,640 --> 00:21:29,620
instead of omocha see also takes a

00:21:27,010 --> 00:21:31,480
hardware feedback to consider the

00:21:29,620 --> 00:21:34,630
hardware feedback during the search our

00:21:31,480 --> 00:21:36,850
work is still 20% faster than see ours

00:21:34,630 --> 00:21:42,550
one well has one point one to point

00:21:36,850 --> 00:21:45,850
across the improvement so yeah stop from

00:21:42,550 --> 00:21:48,430
here so what we did is first we want to

00:21:45,850 --> 00:21:52,080
efficiently search our model so

00:21:48,430 --> 00:21:55,030
previously training our network also

00:21:52,080 --> 00:21:57,070
y-axis is a tassel log level so

00:21:55,030 --> 00:22:01,540
previously training on that who is

00:21:57,070 --> 00:22:03,970
usually I run 100 you yours yeah so but

00:22:01,540 --> 00:22:06,070
if you want to ultimately process use

00:22:03,970 --> 00:22:09,080
the conventional approach it'll require

00:22:06,070 --> 00:22:11,660
more than 10,000 GPRS

00:22:09,080 --> 00:22:14,780
yeah which is unaffordable for most of

00:22:11,660 --> 00:22:17,240
teams and we reduce it about 200 times

00:22:14,780 --> 00:22:23,140
to the regular level of no more training

00:22:17,240 --> 00:22:27,950
yeah and also so save such a few hours

00:22:23,140 --> 00:22:34,370
and also we search and efficient models

00:22:27,950 --> 00:22:37,160
yeah so you can see on the top is is the

00:22:34,370 --> 00:22:39,380
conventional Nast so y axis is the

00:22:37,160 --> 00:22:45,440
accuracy and the accesses is the flops

00:22:39,380 --> 00:22:47,060
so even they have better accuracy sorry

00:22:45,440 --> 00:22:49,400
I made a mistake

00:22:47,060 --> 00:22:51,890
so the y axis is the latency so you can

00:22:49,400 --> 00:22:54,530
see even professionals say take the

00:22:51,890 --> 00:22:56,510
flocks as some evaluation metric so say

00:22:54,530 --> 00:22:58,070
have a very small flops but I actually

00:22:56,510 --> 00:22:59,510
see or not how do we are friendly I

00:22:58,070 --> 00:23:02,030
don't see how do where it's very

00:22:59,510 --> 00:23:05,990
actually very slow in our network so

00:23:02,030 --> 00:23:08,960
even ours flops a very similar we want

00:23:05,990 --> 00:23:12,110
much faster on the desired hardware yeah

00:23:08,960 --> 00:23:15,290
and one more interesting we have god

00:23:12,110 --> 00:23:18,080
Francis work is some inspirations France

00:23:15,290 --> 00:23:20,030
have acted with the Sun so two points I

00:23:18,080 --> 00:23:24,320
have mentioned before is this a large

00:23:20,030 --> 00:23:27,380
kernel and also see tabs so right so we

00:23:24,320 --> 00:23:30,170
always think large Chronos is not good

00:23:27,380 --> 00:23:32,540
yeah because it has the better flops but

00:23:30,170 --> 00:23:35,360
a large kernel weighs shallow Network

00:23:32,540 --> 00:23:38,960
actually the erosion network were much

00:23:35,360 --> 00:23:41,270
faster yeah um various platform you can

00:23:38,960 --> 00:23:44,000
see in conventional works the only use 3

00:23:41,270 --> 00:23:48,050
by 3 but in orange roughy search see

00:23:44,000 --> 00:23:50,960
well even for story is the mobile secure

00:23:48,050 --> 00:23:54,620
they choose many large kernels yeah try

00:23:50,960 --> 00:23:57,140
to improve the speed yeah and also in

00:23:54,620 --> 00:24:00,230
the chip you have formed such a shallow

00:23:57,140 --> 00:24:02,630
and the dance design is first to be

00:24:00,230 --> 00:24:05,180
discussed because previously design

00:24:02,630 --> 00:24:06,920
network is too expensive so people want

00:24:05,180 --> 00:24:09,440
to design network can be run everywhere

00:24:06,920 --> 00:24:11,120
they do not consider supreme property

00:24:09,440 --> 00:24:13,610
and I said do not specialize for each

00:24:11,120 --> 00:24:16,220
hardware such as point how big an order

00:24:13,610 --> 00:24:20,240
for a long time and we hope it raises

00:24:16,220 --> 00:24:22,350
awareness for people and as expected so

00:24:20,240 --> 00:24:25,400
if you search for so them

00:24:22,350 --> 00:24:28,169
form it work it if you low hanging fruit

00:24:25,400 --> 00:24:31,289
improvements on stage platform enhance

00:24:28,169 --> 00:24:33,840
the model search for GPU is once focused

00:24:31,289 --> 00:24:36,539
on Sergio Sergio for six here is the one

00:24:33,840 --> 00:24:39,419
facet of alpha CPU and the reform about

00:24:36,539 --> 00:24:41,460
is faster on my analysis mobile and

00:24:39,419 --> 00:24:44,010
enhances our performance actually very

00:24:41,460 --> 00:24:46,860
close so which means give us a very

00:24:44,010 --> 00:24:48,860
interesting feedback so sometimes if

00:24:46,860 --> 00:24:51,660
you're treating our core network hounds

00:24:48,860 --> 00:24:53,130
GPU yeah and then now you want to

00:24:51,660 --> 00:24:55,020
deployed it to the mobile and I you

00:24:53,130 --> 00:24:57,480
think it's not enough fast enough

00:24:55,020 --> 00:25:00,830
actually you do need you do not need to

00:24:57,480 --> 00:25:03,870
like to change your inference code or

00:25:00,830 --> 00:25:05,909
try to compress tomato faster you're

00:25:03,870 --> 00:25:09,740
just in your true specialized yet and

00:25:05,909 --> 00:25:12,330
you can get us a free improvement and

00:25:09,740 --> 00:25:14,610
based on this work we also made some

00:25:12,330 --> 00:25:17,880
achievements under the sound automation

00:25:14,610 --> 00:25:21,480
yeah so recently we have used a process

00:25:17,880 --> 00:25:23,850
not to once first place in so visual we

00:25:21,480 --> 00:25:29,789
have word challenging so if you are so

00:25:23,850 --> 00:25:30,960
so challenging is image too so it's kind

00:25:29,789 --> 00:25:33,000
of queue what a week huh

00:25:30,960 --> 00:25:35,039
so just like we use you see really use

00:25:33,000 --> 00:25:37,320
hey Siri and they you should a response

00:25:35,039 --> 00:25:39,809
as fast as possible so train Paulo is

00:25:37,320 --> 00:25:42,120
the model size is very limited as the

00:25:39,809 --> 00:25:43,980
peak memory usage is also limited and as

00:25:42,120 --> 00:25:46,049
a phlox it's also limited so

00:25:43,980 --> 00:25:48,900
conventionally people may like manual a

00:25:46,049 --> 00:25:51,600
tumor settings all trials are social

00:25:48,900 --> 00:25:53,940
settings for this one but in our work we

00:25:51,600 --> 00:25:55,770
just feedings is hollow feedbacks and it

00:25:53,940 --> 00:25:58,500
can automatically search a feasible

00:25:55,770 --> 00:26:01,350
solution for us and there's no surprise

00:25:58,500 --> 00:26:03,750
me over here our automatic search one is

00:26:01,350 --> 00:26:05,340
much better than human tuna and now we

00:26:03,750 --> 00:26:07,530
also want to see the place in the

00:26:05,340 --> 00:26:10,620
classification track of the low-power

00:26:07,530 --> 00:26:13,500
computation so it's kind of try to do

00:26:10,620 --> 00:26:15,929
image classifications within 13

00:26:13,500 --> 00:26:18,539
milliseconds latency on pixel two phones

00:26:15,929 --> 00:26:20,940
and again we just put those a Halliwell

00:26:18,539 --> 00:26:23,429
feedback into it and it'll automatically

00:26:20,940 --> 00:26:25,289
search a feasible solution for us yeah

00:26:23,429 --> 00:26:27,809
so this shows the power of these

00:26:25,289 --> 00:26:31,650
animations so this automation not only

00:26:27,809 --> 00:26:33,990
allows non-experts to enjoy the benefit

00:26:31,650 --> 00:26:36,210
of deep learning but also in many cases

00:26:33,990 --> 00:26:43,470
they can outperform human

00:26:36,210 --> 00:26:45,809
experts so yeah as is even is inside the

00:26:43,470 --> 00:26:48,570
Salinas foundation and we all work has

00:26:45,809 --> 00:26:50,759
based on many open with open source

00:26:48,570 --> 00:26:53,100
stores will also love open source so our

00:26:50,759 --> 00:26:55,590
code are now released on that github

00:26:53,100 --> 00:27:00,769
with friction aways and the searching

00:26:55,590 --> 00:27:13,860
codes yeah you're welcome to start yeah

00:27:00,769 --> 00:27:17,720
any question here yes I do me like some

00:27:13,860 --> 00:27:17,720
language moodle Alice p.m. or arms

00:27:18,450 --> 00:27:25,379
yes so we are currently working on yet

00:27:22,080 --> 00:27:27,990
yes so but the system way is so

00:27:25,379 --> 00:27:30,360
prominent so we are not set of familiar

00:27:27,990 --> 00:27:34,169
with an LP task so this projects they're

00:27:30,360 --> 00:27:35,940
ongoing but on some simple tasks we have

00:27:34,169 --> 00:27:44,009
shows that this information is also

00:27:35,940 --> 00:27:47,369
practical sorry what which is the API

00:27:44,009 --> 00:27:49,499
sorry what language is the API what a

00:27:47,369 --> 00:27:52,190
language model we are using yeah so

00:27:49,499 --> 00:27:55,230
currently we are trying to start with

00:27:52,190 --> 00:27:59,460
sequence to synchronous translation yeah

00:27:55,230 --> 00:28:03,529
so we are try to like the machine only

00:27:59,460 --> 00:28:06,210
option to automatically search our code

00:28:03,529 --> 00:28:09,710
sequence to go into sequence model yeah

00:28:06,210 --> 00:28:09,710
but this is the ongoing

00:28:12,210 --> 00:28:23,639
any more questions okay so I'm going on

00:28:19,830 --> 00:28:26,820
so from France and so we have talked

00:28:23,639 --> 00:28:29,970
about how to use machine learnings to

00:28:26,820 --> 00:28:37,649
search and efficient models but you know

00:28:29,970 --> 00:28:41,190
is oh you mean my search process is

00:28:37,649 --> 00:28:43,350
secure bunion on manual Panetta I think

00:28:41,190 --> 00:28:45,720
neither will be cheap Hugh ponded yeah

00:28:43,350 --> 00:28:47,639
if we have more computational sausage oh

00:28:45,720 --> 00:28:50,129
sorry you need to search the model or

00:28:47,639 --> 00:28:53,399
searching process the process the

00:28:50,129 --> 00:29:02,220
process is as message if you bonded not

00:28:53,399 --> 00:29:09,659
memory not memory so far yeah okay any

00:29:02,220 --> 00:29:11,460
more questions okay so right we so we

00:29:09,659 --> 00:29:13,350
just talked a lot about how to use

00:29:11,460 --> 00:29:17,879
machine learning to search and efficient

00:29:13,350 --> 00:29:20,669
models but you know for deployments so

00:29:17,879 --> 00:29:22,769
just searching on model it's not enough

00:29:20,669 --> 00:29:26,009
so usually after you design go to model

00:29:22,769 --> 00:29:28,649
you also need to compress it understand

00:29:26,009 --> 00:29:31,369
you may need to quantize it to make the

00:29:28,649 --> 00:29:34,830
actual runs faster on your hardware and

00:29:31,369 --> 00:29:37,559
yeah so we also do these three steps or

00:29:34,830 --> 00:29:39,539
who works so so we first a use of

00:29:37,559 --> 00:29:40,320
proxies not to search an efficient on

00:29:39,539 --> 00:29:42,720
your network

00:29:40,320 --> 00:29:45,570
Sam we use the AMC which stands for

00:29:42,720 --> 00:29:47,369
alpha ml for model compression and also

00:29:45,570 --> 00:29:50,460
we use a hug Halliwell aware

00:29:47,369 --> 00:29:53,669
organization to all fully automate his

00:29:50,460 --> 00:29:55,919
process so with these techniques you can

00:29:53,669 --> 00:29:58,200
just press one button and let the

00:29:55,919 --> 00:30:00,840
machine learning outcomes to do every

00:29:58,200 --> 00:30:03,389
step for you and Alice Walker now has

00:30:00,840 --> 00:30:07,230
been advocated by accepted to conference

00:30:03,389 --> 00:30:11,600
and not open sourced and the next I'm

00:30:07,230 --> 00:30:11,600
going to introduce about EMC and Hart

00:30:16,539 --> 00:30:23,709
yeah so after you have our model you're

00:30:21,820 --> 00:30:27,489
always near to do pruning and the

00:30:23,709 --> 00:30:32,169
pruning is you have different levers so

00:30:27,489 --> 00:30:34,779
forth of fun whenever yeah you achieve

00:30:32,169 --> 00:30:37,809
the best the sparsity but either usually

00:30:34,779 --> 00:30:42,399
nice specialized the hardware to support

00:30:37,809 --> 00:30:44,289
it to run faster so for the best one it

00:30:42,399 --> 00:30:47,139
which is the feel flavor pruning so

00:30:44,289 --> 00:30:50,799
which is most of hardware friendly but

00:30:47,139 --> 00:30:53,440
also Spacely is somehow limited so

00:30:50,799 --> 00:30:56,559
conventionally we you have a customer

00:30:53,440 --> 00:30:59,619
you have a model and you can let your

00:30:56,559 --> 00:31:02,649
engineer teams to work for it but if you

00:30:59,619 --> 00:31:05,619
have more models or more tasker's saying

00:31:02,649 --> 00:31:09,039
your work just bonded by the human

00:31:05,619 --> 00:31:13,029
bandwidth so our girls use auto email to

00:31:09,039 --> 00:31:15,249
process automated process and allows to

00:31:13,029 --> 00:31:17,190
achieve human-level and even outperform

00:31:15,249 --> 00:31:21,999
it

00:31:17,190 --> 00:31:24,339
so the process here is based on the

00:31:21,999 --> 00:31:26,949
reinforcement learning so it's kind of

00:31:24,339 --> 00:31:30,579
like a human process but just a human

00:31:26,949 --> 00:31:33,129
agent is replaced by the REO agent so we

00:31:30,579 --> 00:31:37,389
have a network and we set the pruning

00:31:33,129 --> 00:31:39,849
levers and for each each times that we

00:31:37,389 --> 00:31:41,879
sample and actions which is how to prune

00:31:39,849 --> 00:31:45,549
the network and the same we quickly

00:31:41,879 --> 00:31:47,889
calibrate it and the galaxy accuracy so

00:31:45,549 --> 00:31:50,679
by doing so can gather so desire the

00:31:47,889 --> 00:31:53,619
flocks and accuracy we can fit it into

00:31:50,679 --> 00:31:57,759
into agent and get our and update and

00:31:53,619 --> 00:32:00,039
you can see i'm the pruning agent so

00:31:57,759 --> 00:32:06,129
it's actually fun some interesting

00:32:00,039 --> 00:32:06,579
patterns so for this one is so is this

00:32:06,129 --> 00:32:10,359
one

00:32:06,579 --> 00:32:13,449
it shows magnitude all for how many ways

00:32:10,359 --> 00:32:15,219
and the total how how many announcer

00:32:13,449 --> 00:32:17,709
always and always and pruning

00:32:15,219 --> 00:32:20,139
intersections so you can see our agent

00:32:17,709 --> 00:32:22,149
ultimately learns that one by one confer

00:32:20,139 --> 00:32:24,249
it has less redundancy and it should it

00:32:22,149 --> 00:32:27,159
be less food and a three by three

00:32:24,249 --> 00:32:28,600
solutions ace has the most redundancy so

00:32:27,159 --> 00:32:32,320
you can see many peaks

00:32:28,600 --> 00:32:34,030
in many peaks in CSI so which means

00:32:32,320 --> 00:32:38,350
these people stand for the 3x3

00:32:34,030 --> 00:32:41,890
convolutional kernels yes and right and

00:32:38,350 --> 00:32:46,780
you can see our MC so here's proud of so

00:32:41,890 --> 00:32:49,060
as x axis which means flaws and the y

00:32:46,780 --> 00:32:51,460
axis and remains to say accuracy so

00:32:49,060 --> 00:32:53,770
conventionally if you prune about human

00:32:51,460 --> 00:32:58,270
expert which is a right adult only

00:32:53,770 --> 00:33:01,750
achieved some settler limit to the

00:32:58,270 --> 00:33:05,320
limited trials and the MZ is much higher

00:33:01,750 --> 00:33:07,990
accuracy and accuracy and absorbs racial

00:33:05,320 --> 00:33:10,630
sense of human trial and you can see

00:33:07,990 --> 00:33:14,740
also we can use the final inference

00:33:10,630 --> 00:33:16,810
latency as our target as our reward so

00:33:14,740 --> 00:33:21,220
by doing so our work will grow much

00:33:16,810 --> 00:33:23,860
faster than previous human designs so

00:33:21,220 --> 00:33:29,230
here's a detail numbers so for the model

00:33:23,860 --> 00:33:32,350
another v1 1.0 with settings so Intel

00:33:29,230 --> 00:33:35,860
chief was seven point six accuracy well

00:33:32,350 --> 00:33:40,780
runs to 120 milliseconds on C Samsung

00:33:35,860 --> 00:33:43,510
and we can prune either by 50% of the

00:33:40,780 --> 00:33:45,790
flops and it'll reduce the latency about

00:33:43,510 --> 00:33:50,340
guida margin while sir accuracy is

00:33:45,790 --> 00:33:53,350
almost no changed we also were ruined by

00:33:50,340 --> 00:33:56,260
50% of the inference time and you can

00:33:53,350 --> 00:33:59,350
see it's a crisis so it is literally

00:33:56,260 --> 00:34:02,500
dropped Buster acceptable so it's really

00:33:59,350 --> 00:34:05,320
once 2 times faster since original one

00:34:02,500 --> 00:34:08,140
so come how to manually schedule so we

00:34:05,320 --> 00:34:10,840
setting which has a similar similar

00:34:08,140 --> 00:34:13,830
latency so accuracy is drawn by a large

00:34:10,840 --> 00:34:16,390
margin and the our method is

00:34:13,830 --> 00:34:20,230
significantly also performs the manual

00:34:16,390 --> 00:34:23,380
tuning and here's the comparisons of the

00:34:20,230 --> 00:34:25,750
previous one whose previous human

00:34:23,380 --> 00:34:28,630
experts with which is professor has

00:34:25,750 --> 00:34:31,390
thesis so you can see previous the x-ray

00:34:28,630 --> 00:34:33,760
tries very highly to prune the network

00:34:31,390 --> 00:34:36,250
and for each one you choose a different

00:34:33,760 --> 00:34:38,680
stretch hold and the has to be very

00:34:36,250 --> 00:34:42,200
careful as well as a crazy world well

00:34:38,680 --> 00:34:46,280
dropped so here is the pruning ratios of

00:34:42,200 --> 00:34:49,280
conclusion layer and it has the MC fun

00:34:46,280 --> 00:34:52,129
are better scheduling and how many how

00:34:49,280 --> 00:34:55,339
many are should be pruned in every block

00:34:52,129 --> 00:34:59,390
and axe racer pruning ratio is three

00:34:55,339 --> 00:35:01,970
point four times is improved from three

00:34:59,390 --> 00:35:04,849
point four which is previous status of

00:35:01,970 --> 00:35:09,800
sod from humid spread to five times the

00:35:04,849 --> 00:35:12,500
compression ratio yeah and right so with

00:35:09,800 --> 00:35:15,140
automated process we not only makes this

00:35:12,500 --> 00:35:21,530
process much simpler but also better

00:35:15,140 --> 00:35:25,400
than previous human expert and next one

00:35:21,530 --> 00:35:32,270
is about how do we our conversation yeah

00:35:25,400 --> 00:35:34,820
so our conversation has many useful case

00:35:32,270 --> 00:35:37,339
because currently team on audience and

00:35:34,820 --> 00:35:40,250
so knowing that call is very robust

00:35:37,339 --> 00:35:43,280
you know you do not always need a single

00:35:40,250 --> 00:35:45,800
flow to precision to handle yet so you

00:35:43,280 --> 00:35:48,140
can use multiple positions or maybe mix

00:35:45,800 --> 00:35:51,050
the Precision's but this brings a

00:35:48,140 --> 00:35:53,480
challenge to design space so for example

00:35:51,050 --> 00:35:55,760
you have only Asian and you have so will

00:35:53,480 --> 00:35:58,400
can be one to eight bit and then also

00:35:55,760 --> 00:36:01,760
the activation happy one to eight bit so

00:35:58,400 --> 00:36:04,569
the total design space it can be 64 to

00:36:01,760 --> 00:36:07,960
the times of n so this cannot be

00:36:04,569 --> 00:36:11,480
exploited by humans so we need our ml to

00:36:07,960 --> 00:36:13,339
do this task and the task is also

00:36:11,480 --> 00:36:16,490
designer by whom

00:36:13,339 --> 00:36:19,760
reinforcement learning and so key point

00:36:16,490 --> 00:36:20,990
here is we use a special magic to

00:36:19,760 --> 00:36:23,540
preserve the accuracy

00:36:20,990 --> 00:36:27,109
so we when we do the reinforcement knee

00:36:23,540 --> 00:36:30,440
we are used as designer reward functions

00:36:27,109 --> 00:36:32,990
and the reward from the latency is a

00:36:30,440 --> 00:36:35,240
latency quantization - Brooklyn see

00:36:32,990 --> 00:36:36,859
France original we caused the

00:36:35,240 --> 00:36:39,800
improvement and the difference between

00:36:36,859 --> 00:36:42,170
contest model and original model and the

00:36:39,800 --> 00:36:44,660
force and Christian one is the lead is

00:36:42,170 --> 00:36:47,150
accuracy from context version and the

00:36:44,660 --> 00:36:50,839
original version to pre-service accuracy

00:36:47,150 --> 00:36:52,539
we set as a relative weight also one has

00:36:50,839 --> 00:36:54,910
an accuracy

00:36:52,539 --> 00:36:57,099
very high to ensure we to market a

00:36:54,910 --> 00:36:59,709
performance job doing some conversations

00:36:57,099 --> 00:37:02,529
and as you can see these results are

00:36:59,709 --> 00:37:05,589
from different FPGA board

00:37:02,529 --> 00:37:07,689
yeah cuz currently no public how do we

00:37:05,589 --> 00:37:10,449
are all free really supposed to mix the

00:37:07,689 --> 00:37:12,880
precision and can see we also observe

00:37:10,449 --> 00:37:15,189
similar trends in the proxies nas which

00:37:12,880 --> 00:37:17,709
means you quantize for us in the

00:37:15,189 --> 00:37:22,419
hardware and the inner ones for instance

00:37:17,709 --> 00:37:26,679
is hardware and here is a comparison of

00:37:22,419 --> 00:37:30,069
previous work so the blue dot is France

00:37:26,679 --> 00:37:32,589
fixed 8-bit conversation from previous

00:37:30,069 --> 00:37:35,529
state-of-the-art results and this yellow

00:37:32,589 --> 00:37:37,900
dot is our results on the fax via the

00:37:35,529 --> 00:37:41,829
conversations so so why access is

00:37:37,900 --> 00:37:44,529
latency and x-axis is latency and y-axis

00:37:41,829 --> 00:37:46,779
is top one accuracy in post settings we

00:37:44,529 --> 00:37:53,169
get the pattern latency and accuracy

00:37:46,779 --> 00:37:57,609
ratios okay and yeah so process we do

00:37:53,169 --> 00:38:04,529
the whole process and we make this fully

00:37:57,609 --> 00:38:04,529
automatic and right thank you

00:38:04,800 --> 00:38:09,489

YouTube URL: https://www.youtube.com/watch?v=nBLDRT2DcCs


