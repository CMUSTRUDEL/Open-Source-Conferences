Title: OpenPOWER Summit NA 2019: CAPI OpenCAPI Accelerated PostgreSQL
Publication date: 2019-08-20
Playlist: OpenPOWER Summit NA 2019
Description: 
	Presented by Peng Fei Gou, IBM

Heterogeneous computing, one of the most crucial technology to OpenPOWER systems, is extended by cache-coherent accelerator interfaces, i.e., CAPI/OpenCAPI. Database, critical workload in data-analytics, AI, etc., is key to the success of OpenPOWER ecosystem. In this proposal, a CAPI/OpenCAPI and FPGA based solution is described to accelerate database queries. In particular, an architecture with multithreading software and multi-engine FPGA logic is presented, with detailed analysis on how CAPI/OpenCAPIâ€™s advantages of latency, bandwidth and simplified software stack can be leveraged to accelerate database. The accelerated database is based on the popular open source PostgreSQL. Query operators including regular expression matching and other general queries are implemented in FPGA logic, with 10x to 2x overall throughput improvement compared with state-of-the-art CPU version.
Captions: 
	00:00:00,030 --> 00:00:07,589
okay so thank you Peter

00:00:03,149 --> 00:00:10,410
so today I'm so grateful that o-p-s

00:00:07,589 --> 00:00:13,889
gives me the opportunity to present our

00:00:10,410 --> 00:00:17,250
latest work um Kathy and open can be

00:00:13,889 --> 00:00:25,550
accelerated to Postgres so I'm Pompey

00:00:17,250 --> 00:00:29,460
from IBM China system lab okay so so

00:00:25,550 --> 00:00:32,340
let's give some very brief introduction

00:00:29,460 --> 00:00:35,250
to worries Postgres so Postgres

00:00:32,340 --> 00:00:37,410
according to their own website it's one

00:00:35,250 --> 00:00:40,260
of the most world's most advanced

00:00:37,410 --> 00:00:43,170
open-source relational database so from

00:00:40,260 --> 00:00:46,260
the database popularity ranking we can

00:00:43,170 --> 00:00:49,020
see that the Postgres is ranked as

00:00:46,260 --> 00:00:52,399
number force so from number one to

00:00:49,020 --> 00:00:55,739
number three it's three enterprise

00:00:52,399 --> 00:00:57,960
databases so this means Postgres should

00:00:55,739 --> 00:01:01,020
be very popular in various areas

00:00:57,960 --> 00:01:03,059
including maybe croute enterprise and

00:01:01,020 --> 00:01:09,659
the research so that's why we choose

00:01:03,059 --> 00:01:11,549
Postgres as our target okay so let's

00:01:09,659 --> 00:01:13,860
give you know some very high-level view

00:01:11,549 --> 00:01:16,470
of what we have done so currently we are

00:01:13,860 --> 00:01:18,869
able to accelerate a Postgres with

00:01:16,470 --> 00:01:21,540
regular expression matching and for

00:01:18,869 --> 00:01:24,600
users for the hell and the users we have

00:01:21,540 --> 00:01:26,659
to interface for for them to use our

00:01:24,600 --> 00:01:29,490
acceleration the first one is a

00:01:26,659 --> 00:01:33,930
user-defined function which is the UDF

00:01:29,490 --> 00:01:36,270
we create a new function in this name

00:01:33,930 --> 00:01:38,130
and if the user is going to use our

00:01:36,270 --> 00:01:41,880
observations they need to call this

00:01:38,130 --> 00:01:45,060
function in the sequel language script

00:01:41,880 --> 00:01:46,920
so which means you know most of the

00:01:45,060 --> 00:01:49,200
users would not be very happy with this

00:01:46,920 --> 00:01:51,180
because you know they it requires them

00:01:49,200 --> 00:01:54,659
to change their changes our script

00:01:51,180 --> 00:01:57,180
changes our codes so that's why we have

00:01:54,659 --> 00:02:00,530
in the other way which is implemented as

00:01:57,180 --> 00:02:03,500
Postgres hooks and plugins and this

00:02:00,530 --> 00:02:07,140
enables the user to use the standard

00:02:03,500 --> 00:02:09,390
echo language in this form with where

00:02:07,140 --> 00:02:12,900
Kraus and I regular expression matching

00:02:09,390 --> 00:02:13,450
operator then the users are okay to use

00:02:12,900 --> 00:02:16,900
our

00:02:13,450 --> 00:02:19,269
salvation so this is exactly what we are

00:02:16,900 --> 00:02:26,170
trying to present to the end the users

00:02:19,269 --> 00:02:28,800
to use our acceleration okay so next let

00:02:26,170 --> 00:02:31,120
me give some very high-level you know

00:02:28,800 --> 00:02:33,849
introduction about the overall

00:02:31,120 --> 00:02:36,610
architecture for both software and

00:02:33,849 --> 00:02:38,319
hardware so from this screen we can see

00:02:36,610 --> 00:02:39,130
that on the left side it's the host

00:02:38,319 --> 00:02:42,519
memory

00:02:39,130 --> 00:02:44,739
there are various data structures inside

00:02:42,519 --> 00:02:47,230
the house of memory and for the fun of

00:02:44,739 --> 00:02:50,769
orange boxes they are the Postgres

00:02:47,230 --> 00:02:54,069
internal data structures for pages in

00:02:50,769 --> 00:02:57,220
Postgres every iteration or every table

00:02:54,069 --> 00:03:00,430
is comprised of a large amount of pages

00:02:57,220 --> 00:03:04,510
and those pages will be loaded from

00:03:00,430 --> 00:03:07,900
storage to the buffer cache on the host

00:03:04,510 --> 00:03:10,840
of memory and then we have multiple

00:03:07,900 --> 00:03:13,769
threats which will work which will be

00:03:10,840 --> 00:03:17,380
working in parallel to process those

00:03:13,769 --> 00:03:20,530
pages and then for each threat they will

00:03:17,380 --> 00:03:24,010
they are going to process a part of

00:03:20,530 --> 00:03:25,120
those pages and then they will find the

00:03:24,010 --> 00:03:28,150
columns

00:03:25,120 --> 00:03:31,480
Vala will be matched with the expression

00:03:28,150 --> 00:03:34,269
and furniture columns and copy that into

00:03:31,480 --> 00:03:36,850
another memory buffer in userspace we

00:03:34,269 --> 00:03:38,290
call it packet buffer and for every

00:03:36,850 --> 00:03:41,049
thread that they have a packet buffer

00:03:38,290 --> 00:03:44,319
and for every packet buffer they will

00:03:41,049 --> 00:03:47,170
finally be touched by the regular

00:03:44,319 --> 00:03:50,560
expression engine on the FPGA we are

00:03:47,170 --> 00:03:53,260
happy and open happy by virtual address

00:03:50,560 --> 00:03:55,630
pointers so that that is the advantage

00:03:53,260 --> 00:03:59,680
of of carry and open carry

00:03:55,630 --> 00:04:02,799
so those regular expression engines they

00:03:59,680 --> 00:04:06,160
are connected with an ax I interconnect

00:04:02,799 --> 00:04:10,840
and they will talk with the host memory

00:04:06,160 --> 00:04:13,750
we buy a bridge which is a oxide to

00:04:10,840 --> 00:04:17,380
carry or open carry bridge for cappy's

00:04:13,750 --> 00:04:21,519
ax I to PSL foreign cabbie it's a xi2

00:04:17,380 --> 00:04:23,680
open happy Terrax so by using this ax I

00:04:21,519 --> 00:04:25,670
interconnect and in this bridge the

00:04:23,680 --> 00:04:29,690
regular expression engine they are able

00:04:25,670 --> 00:04:31,360
fetch the packets that is estranged that

00:04:29,690 --> 00:04:34,550
I will be matched with the wrecker

00:04:31,360 --> 00:04:36,890
expression and they will fetch the

00:04:34,550 --> 00:04:39,110
contents of this baffle choose a

00:04:36,890 --> 00:04:41,930
hardware and then they will perform the

00:04:39,110 --> 00:04:43,850
regular expression matching and when

00:04:41,930 --> 00:04:47,660
they finished their job they will write

00:04:43,850 --> 00:04:50,210
back the result also with by by using

00:04:47,660 --> 00:04:52,670
the AXA interconnect right back that

00:04:50,210 --> 00:04:56,030
into the buffer we call it a result

00:04:52,670 --> 00:04:58,660
buffer and then the software will is

00:04:56,030 --> 00:05:02,480
going to use this result buffer and

00:04:58,660 --> 00:05:04,460
present it to the Postgres upper level

00:05:02,480 --> 00:05:07,460
software to end the user so that the

00:05:04,460 --> 00:05:09,890
user can finally see the result of the

00:05:07,460 --> 00:05:12,830
of the regular expression matching

00:05:09,890 --> 00:05:17,810
result and the on the hardware we are

00:05:12,830 --> 00:05:20,390
with this this structure allows not only

00:05:17,810 --> 00:05:24,080
in the regular expression engine engines

00:05:20,390 --> 00:05:26,210
they allow other type of engines like in

00:05:24,080 --> 00:05:29,120
a general car engine or any other types

00:05:26,210 --> 00:05:31,430
such as hash drawing or aggregation but

00:05:29,120 --> 00:05:33,980
those engines ask you under under under

00:05:31,430 --> 00:05:37,370
construction in progress and we also

00:05:33,980 --> 00:05:40,580
have a plan to to have a harder job

00:05:37,370 --> 00:05:43,400
manager on the on the FPGA so that the

00:05:40,580 --> 00:05:46,330
job scheduling will have less overhead

00:05:43,400 --> 00:05:48,710
but this component is also in progress

00:05:46,330 --> 00:05:51,320
currently all the engines they are

00:05:48,710 --> 00:05:55,580
controlled by software directly with

00:05:51,320 --> 00:05:58,280
emmawho access okay so this is the

00:05:55,580 --> 00:06:00,950
overall architecture and then let's take

00:05:58,280 --> 00:06:03,560
a look and and and some of the details

00:06:00,950 --> 00:06:06,470
of our regular expression engine so this

00:06:03,560 --> 00:06:12,380
engine is written by by our team from

00:06:06,470 --> 00:06:16,220
scratch in HDL in RTL it is in comprised

00:06:12,380 --> 00:06:19,850
of multiple pipelines each pair on each

00:06:16,220 --> 00:06:24,320
pipeline is able to process one stream

00:06:19,850 --> 00:06:26,390
and one time and inside that pie plan we

00:06:24,320 --> 00:06:29,960
have we also have multiple processing

00:06:26,390 --> 00:06:32,360
units which allows the engine to match

00:06:29,960 --> 00:06:34,760
while stirring with multiple regular

00:06:32,360 --> 00:06:37,280
expression patterns and at the same time

00:06:34,760 --> 00:06:38,909
so which means they have two dimensions

00:06:37,280 --> 00:06:41,589
of parallel

00:06:38,909 --> 00:06:43,719
preparation and then the whole

00:06:41,589 --> 00:06:45,669
architecture is configurable for post

00:06:43,719 --> 00:06:48,669
hardware and software so for Hardware

00:06:45,669 --> 00:06:51,279
during the image building phase we are

00:06:48,669 --> 00:06:53,740
able to defend the number of pipelines

00:06:51,279 --> 00:06:56,619
we are able to defend the number of

00:06:53,740 --> 00:07:00,969
processing units and during wrong time

00:06:56,619 --> 00:07:03,550
the vehicle expression pattern is also

00:07:00,969 --> 00:07:07,809
configurable by using hardware and the

00:07:03,550 --> 00:07:09,879
wrong time okay so that's those two

00:07:07,809 --> 00:07:10,599
pages I introduced some of the details

00:07:09,879 --> 00:07:13,509
of the hardware

00:07:10,599 --> 00:07:15,759
then let's get take a look and what we

00:07:13,509 --> 00:07:19,360
have done for the software so for the

00:07:15,759 --> 00:07:22,869
software we are basically you know

00:07:19,360 --> 00:07:26,289
building our our host software on top of

00:07:22,869 --> 00:07:30,009
Postgres hook and plug-in flow so for

00:07:26,289 --> 00:07:32,830
posters cook and protein flow it is

00:07:30,009 --> 00:07:36,009
basically the customer pass customer

00:07:32,830 --> 00:07:39,809
skin and custom execution measures there

00:07:36,009 --> 00:07:43,419
are four very important hooks to to

00:07:39,809 --> 00:07:46,509
implement our our acceleration those are

00:07:43,419 --> 00:07:49,839
the four hooks are picking scan execute

00:07:46,509 --> 00:07:53,050
skin and the skin and explain scan so in

00:07:49,839 --> 00:07:55,389
begin scan you know this is where the

00:07:53,050 --> 00:07:57,759
most of the interaction between software

00:07:55,389 --> 00:08:00,699
and the hardware happens we will prepare

00:07:57,759 --> 00:08:03,669
the internal data structures for for the

00:08:00,699 --> 00:08:06,899
whole skin work and then we will extract

00:08:03,669 --> 00:08:10,539
the regular expression pattern from the

00:08:06,899 --> 00:08:13,180
from a psycho psycho language given by

00:08:10,539 --> 00:08:16,539
user and then we'll start the multi

00:08:13,180 --> 00:08:19,569
threading worker to enable the software

00:08:16,539 --> 00:08:22,119
talk with hardware to get the result of

00:08:19,569 --> 00:08:26,080
hardware and to process every details

00:08:22,119 --> 00:08:28,990
and then after the whole work finished

00:08:26,080 --> 00:08:32,439
by the hardware we will go to the XQ

00:08:28,990 --> 00:08:35,199
scan in SQL hook the results will be

00:08:32,439 --> 00:08:37,750
harvest so the results will be processed

00:08:35,199 --> 00:08:40,060
by the software and the tapos which

00:08:37,750 --> 00:08:42,610
means the rows in the in the tables in

00:08:40,060 --> 00:08:46,380
the relations will be reconstructed and

00:08:42,610 --> 00:08:50,529
then this result will be returned to the

00:08:46,380 --> 00:08:51,970
Postgres so called sequel internal data

00:08:50,529 --> 00:08:54,130
structures so that the

00:08:51,970 --> 00:08:56,650
and the user will fader with out of

00:08:54,130 --> 00:09:01,690
their regular expression matching and

00:08:56,650 --> 00:09:04,600
then the third hook is the end scan hook

00:09:01,690 --> 00:09:07,150
in this hook we will do most of the

00:09:04,600 --> 00:09:10,330
clean-up jobs which means you know every

00:09:07,150 --> 00:09:13,030
temporary buffers allocated during the

00:09:10,330 --> 00:09:16,330
whole scan will be freed in this stage

00:09:13,030 --> 00:09:19,600
and we also have another explained scan

00:09:16,330 --> 00:09:22,690
so in this x-ray scan we will defend the

00:09:19,600 --> 00:09:25,900
how the users will look when they call

00:09:22,690 --> 00:09:30,520
the explained explained statement in the

00:09:25,900 --> 00:09:33,370
postcode Seco client console so this is

00:09:30,520 --> 00:09:35,950
the basic high-level structure of how we

00:09:33,370 --> 00:09:40,930
implement our software so here we

00:09:35,950 --> 00:09:42,090
talking about when when we when we make

00:09:40,930 --> 00:09:45,100
the software and the hardware

00:09:42,090 --> 00:09:47,440
communicate with each other I'm talking

00:09:45,100 --> 00:09:50,670
about the multi-threading worker so what

00:09:47,440 --> 00:09:54,610
is that so this is a very low-level

00:09:50,670 --> 00:09:58,230
software that enables the software

00:09:54,610 --> 00:10:00,760
talking with hardware in a in a very

00:09:58,230 --> 00:10:03,340
parallel way we defend the worker

00:10:00,760 --> 00:10:07,150
defense thread and different jobs

00:10:03,340 --> 00:10:09,070
so for each post crafts skin we will

00:10:07,150 --> 00:10:10,870
have a one worker that allows the

00:10:09,070 --> 00:10:13,240
software to communicate with the

00:10:10,870 --> 00:10:16,300
hardware and in each worker we have a

00:10:13,240 --> 00:10:19,090
multiple threat and for every threat

00:10:16,300 --> 00:10:22,330
will be working in parallel and in every

00:10:19,090 --> 00:10:25,000
stress we will have multiple jobs so for

00:10:22,330 --> 00:10:28,060
each job they will act they will be

00:10:25,000 --> 00:10:29,950
executed in sequential it sequentially

00:10:28,060 --> 00:10:32,380
so which means you know a threads they

00:10:29,950 --> 00:10:35,170
are parallel but within the threads the

00:10:32,380 --> 00:10:37,720
jobs will be execute execute you know is

00:10:35,170 --> 00:10:40,300
in sequential the reason is that you

00:10:37,720 --> 00:10:43,360
know because for every thread while they

00:10:40,300 --> 00:10:46,210
are going to process it fixed the amount

00:10:43,360 --> 00:10:49,480
of database well that will be processed

00:10:46,210 --> 00:10:51,460
is fixed so which means if you have very

00:10:49,480 --> 00:10:53,800
large table then for every thread that

00:10:51,460 --> 00:10:56,260
you you may need to allocate a very

00:10:53,800 --> 00:11:00,280
large packet buffer which you will have

00:10:56,260 --> 00:11:02,650
you know tremendous overhead of when you

00:11:00,280 --> 00:11:05,270
D allocated this memory buffer so to

00:11:02,650 --> 00:11:07,970
reduce the size of this buffer when

00:11:05,270 --> 00:11:11,120
to defend multiple jobs those jobs we

00:11:07,970 --> 00:11:13,340
use this packet buffer so they are the

00:11:11,120 --> 00:11:15,770
size of the pack but packet buffer will

00:11:13,340 --> 00:11:19,400
be smaller so I will give some more

00:11:15,770 --> 00:11:21,200
details in the later stage when I when I

00:11:19,400 --> 00:11:23,870
explain the performance I will show you

00:11:21,200 --> 00:11:26,300
how the number of jobs will impact the

00:11:23,870 --> 00:11:31,120
overall performance especially for the

00:11:26,300 --> 00:11:34,190
cleanup phase ok so this is what I have

00:11:31,120 --> 00:11:36,440
this is further for the overview of the

00:11:34,190 --> 00:11:38,960
details of a hardware and software so

00:11:36,440 --> 00:11:41,360
let me show you a short video to you to

00:11:38,960 --> 00:11:43,990
say - so that you can feel that while we

00:11:41,360 --> 00:11:48,010
have done what you can do is our

00:11:43,990 --> 00:11:54,320
acceleration ok this is the video that

00:11:48,010 --> 00:11:58,850
is recorded by my screen I'm sorry hold

00:11:54,320 --> 00:12:03,760
on let me show you where is my pointer

00:11:58,850 --> 00:12:03,760
here we go

00:12:04,380 --> 00:12:12,060
okay this is a cut of my screen is the

00:12:08,910 --> 00:12:14,580
Postgres circle console this is the

00:12:12,060 --> 00:12:17,520
table I use the to to perform the test

00:12:14,580 --> 00:12:20,670
it has to the schema they have two

00:12:17,520 --> 00:12:23,280
columns one for packet the other for ID

00:12:20,670 --> 00:12:25,800
and then we will load the hook the

00:12:23,280 --> 00:12:29,550
library that enables kepi and we will

00:12:25,800 --> 00:12:31,470
set the parameters and for for enable

00:12:29,550 --> 00:12:33,840
copy mode to defend the number of

00:12:31,470 --> 00:12:35,550
threads we will were going to use to

00:12:33,840 --> 00:12:38,640
defend the number of jobs we are going

00:12:35,550 --> 00:12:40,830
to use and then after all the parameters

00:12:38,640 --> 00:12:45,450
we said we are going to call the

00:12:40,830 --> 00:12:48,360
standard psycho language with regular

00:12:45,450 --> 00:12:51,570
expression operator and the through that

00:12:48,360 --> 00:12:54,330
without 128 rows are matched with this

00:12:51,570 --> 00:12:58,260
operation and then we will call explain

00:12:54,330 --> 00:13:00,360
analyze to show show the real skin

00:12:58,260 --> 00:13:03,000
custom scheme paths that we use for this

00:13:00,360 --> 00:13:06,270
for this operation and the execution

00:13:03,000 --> 00:13:10,500
time will also be shown with this one -

00:13:06,270 --> 00:13:13,590
you know other to compare the difference

00:13:10,500 --> 00:13:16,860
between CPU and capi we here we will

00:13:13,590 --> 00:13:20,130
disable the kepi mode to run impure CPU

00:13:16,860 --> 00:13:24,120
modes so they are using the same psycho

00:13:20,130 --> 00:13:26,520
language and the readout is exactly the

00:13:24,120 --> 00:13:28,830
same and when you run the explain

00:13:26,520 --> 00:13:32,220
analyze it will say that they are using

00:13:28,830 --> 00:13:35,280
CPU power worker and the and the wrong

00:13:32,220 --> 00:13:38,880
time is much you know slower so I will

00:13:35,280 --> 00:13:45,290
give some more details answer on the

00:13:38,880 --> 00:13:45,290
performance evaluation okay

00:13:46,350 --> 00:13:54,380
okay let's go back to to the tour charts

00:13:50,090 --> 00:13:57,800
so I just show the video of how our

00:13:54,380 --> 00:14:01,860
observation will be looking like in the

00:13:57,800 --> 00:14:04,320
in the real server so let's give some

00:14:01,860 --> 00:14:06,330
more details about the performance

00:14:04,320 --> 00:14:08,070
evaluation so - to measure the

00:14:06,330 --> 00:14:11,220
performance that we have our environment

00:14:08,070 --> 00:14:13,920
setup so we use the poly I server two

00:14:11,220 --> 00:14:18,030
sockets every socket have 22 course and

00:14:13,920 --> 00:14:20,880
we have 512 gigabytes memory and for the

00:14:18,030 --> 00:14:24,990
FPGA card we use a zerlings of a uni p

00:14:20,880 --> 00:14:28,410
FP g card and for the kepi we are pay

00:14:24,990 --> 00:14:31,370
our of our work based on kappachu and

00:14:28,410 --> 00:14:34,920
the snap - were using snap - as our

00:14:31,370 --> 00:14:37,860
development development framework this

00:14:34,920 --> 00:14:40,740
also allows us to to move to open KP

00:14:37,860 --> 00:14:43,440
with minimal effort because we have a

00:14:40,740 --> 00:14:47,130
open heavy version snap which is called

00:14:43,440 --> 00:14:49,080
a open cab acceleration framework Luo

00:14:47,130 --> 00:14:53,130
Yan will present details for that

00:14:49,080 --> 00:14:56,880
framework later and then for the for the

00:14:53,130 --> 00:15:00,000
engines we use 8 8 engines each engine

00:14:56,880 --> 00:15:02,520
has 16 pipe packet pipelines which means

00:15:00,000 --> 00:15:05,220
each energy is able to process 16

00:15:02,520 --> 00:15:07,860
strange and at the same time and the the

00:15:05,220 --> 00:15:11,970
fpga logic is Ronnie and to croak

00:15:07,860 --> 00:15:15,930
promisee of 225 megahertz and for post

00:15:11,970 --> 00:15:18,840
cursor for Postgres we have different

00:15:15,930 --> 00:15:22,890
parameter set we are running with

00:15:18,840 --> 00:15:25,680
Postgres version 11 point 2 and then for

00:15:22,890 --> 00:15:28,110
the share the buffers will use to two

00:15:25,680 --> 00:15:30,240
sets foking about a new banking about to

00:15:28,110 --> 00:15:31,080
see the difference to see if they have

00:15:30,240 --> 00:15:33,360
any comeback

00:15:31,080 --> 00:15:35,790
impacted to the performance and there

00:15:33,360 --> 00:15:36,510
are also other other parameters set for

00:15:35,790 --> 00:15:39,780
Postgres

00:15:36,510 --> 00:15:42,060
and for the queries we use the standards

00:15:39,780 --> 00:15:45,780
in those Seco language I just you know

00:15:42,060 --> 00:15:48,270
third in the previous rides and for the

00:15:45,780 --> 00:15:50,850
tables to be tested that we have some

00:15:48,270 --> 00:15:54,210
synthetic tables and for each table we

00:15:50,850 --> 00:15:57,210
have two columns one for string and the

00:15:54,210 --> 00:15:59,220
other for ID and the faulty besides we

00:15:57,210 --> 00:16:00,210
have you know different number of table

00:15:59,220 --> 00:16:02,790
tables

00:16:00,210 --> 00:16:06,570
different a number of Joe's for four

00:16:02,790 --> 00:16:10,770
different data points okay here is the

00:16:06,570 --> 00:16:13,920
results of compare our acceleration with

00:16:10,770 --> 00:16:14,610
CPU version we have four data points to

00:16:13,920 --> 00:16:17,490
4kp

00:16:14,610 --> 00:16:19,410
the other two for CPU with buffer cache

00:16:17,490 --> 00:16:20,850
to run gigabytes and a Giga

00:16:19,410 --> 00:16:23,360
bytes respectively

00:16:20,850 --> 00:16:26,930
so from this picture we can see that

00:16:23,360 --> 00:16:29,550
compared with CPU our acceleration or

00:16:26,930 --> 00:16:32,720
character region we have is going to

00:16:29,550 --> 00:16:35,610
have 5 times 2 to 10 times speed up

00:16:32,720 --> 00:16:38,640
compared with the CPU so when the table

00:16:35,610 --> 00:16:40,410
table is larger the speed-up dropped

00:16:38,640 --> 00:16:43,500
from ten times to five times

00:16:40,410 --> 00:16:45,899
that is because you know when the table

00:16:43,500 --> 00:16:49,380
is larger then the CPU versions are

00:16:45,899 --> 00:16:52,589
going to use a few parallel workers so

00:16:49,380 --> 00:16:54,990
that stupid apps let's a CPU version but

00:16:52,589 --> 00:16:57,750
we still have five times advantage and

00:16:54,990 --> 00:17:01,560
the winds up when the table is very

00:16:57,750 --> 00:17:04,199
small we also have some job for the

00:17:01,560 --> 00:17:08,699
stupid app that is because the table is

00:17:04,199 --> 00:17:11,130
too small so the overhead so the

00:17:08,699 --> 00:17:13,949
overhead of the of the engine

00:17:11,130 --> 00:17:16,640
configuration will have some constant we

00:17:13,949 --> 00:17:17,790
have impact to the overall performance

00:17:16,640 --> 00:17:19,760
okay

00:17:17,790 --> 00:17:24,420
this is the overall performance

00:17:19,760 --> 00:17:27,900
comparison with CPU and our acceleration

00:17:24,420 --> 00:17:30,559
and then we also have some measurement

00:17:27,900 --> 00:17:33,420
that you see how the number of threads

00:17:30,559 --> 00:17:36,120
impacts the overall performance

00:17:33,420 --> 00:17:39,780
this picture shows that when the number

00:17:36,120 --> 00:17:42,950
of threads changed from one to eight the

00:17:39,780 --> 00:17:46,470
overall query time drops significantly

00:17:42,950 --> 00:17:49,140
so it's very good to see that in a way

00:17:46,470 --> 00:17:53,100
if we have more threads and that means

00:17:49,140 --> 00:17:56,010
more more works will be pirate

00:17:53,100 --> 00:17:59,460
especially when we when we process the

00:17:56,010 --> 00:18:02,960
packet buffer and then we also have some

00:17:59,460 --> 00:18:07,020
teacher breakdown for each for each

00:18:02,960 --> 00:18:09,950
query we break the whole query time to

00:18:07,020 --> 00:18:12,780
four parts the first part is the packet

00:18:09,950 --> 00:18:13,860
preparation this means that you know for

00:18:12,780 --> 00:18:16,799
the software

00:18:13,860 --> 00:18:18,720
we'll make copy that the software will

00:18:16,799 --> 00:18:21,690
process the Postgres internal data

00:18:18,720 --> 00:18:24,690
structures process the pages then copy

00:18:21,690 --> 00:18:27,029
the columns from the internal data

00:18:24,690 --> 00:18:29,279
structure to the packet buffer this time

00:18:27,029 --> 00:18:32,250
is measured as packed the preparation in

00:18:29,279 --> 00:18:34,350
green and then in the blue for the blue

00:18:32,250 --> 00:18:37,140
part it's a hardware round which means

00:18:34,350 --> 00:18:40,200
the regular expression engine become

00:18:37,140 --> 00:18:43,080
figures and to read the little packets

00:18:40,200 --> 00:18:45,059
and to process the the whole package to

00:18:43,080 --> 00:18:47,340
perform the regular expression matching

00:18:45,059 --> 00:18:49,169
and to read back the readout there is a

00:18:47,340 --> 00:18:51,600
hard way around and then for the yellow

00:18:49,169 --> 00:18:53,850
part it's the clean up and for the

00:18:51,600 --> 00:18:58,080
orange part it's all the other other

00:18:53,850 --> 00:19:02,610
times consumed by for example the two

00:18:58,080 --> 00:19:05,549
relationship to readership are all or

00:19:02,610 --> 00:19:07,889
something others and then in this you

00:19:05,549 --> 00:19:10,620
know measurement we can see that as the

00:19:07,889 --> 00:19:13,169
number of threads increase the time

00:19:10,620 --> 00:19:15,539
consumed on the packet preparation and

00:19:13,169 --> 00:19:18,690
the hardware on decreased very

00:19:15,539 --> 00:19:22,350
significant significantly so that's why

00:19:18,690 --> 00:19:24,480
the overall crime reduced you know very

00:19:22,350 --> 00:19:28,470
significantly when the number of threads

00:19:24,480 --> 00:19:30,960
increased so for each data point I think

00:19:28,470 --> 00:19:34,049
the say the number of jobs for each data

00:19:30,960 --> 00:19:38,159
point we we use the optimal number of

00:19:34,049 --> 00:19:40,409
jobs for each data point then why we use

00:19:38,159 --> 00:19:42,990
you know what why I say I use the

00:19:40,409 --> 00:19:46,649
optimal number of jobs that is because

00:19:42,990 --> 00:19:49,380
you know in the in the in the next child

00:19:46,649 --> 00:19:51,870
will say that if we have the fixed

00:19:49,380 --> 00:19:54,539
number of threads which is a distress if

00:19:51,870 --> 00:19:59,190
we change the number of jobs from one to

00:19:54,539 --> 00:20:01,889
ten we will see that the breakdown is

00:19:59,190 --> 00:20:03,899
virtually for the cleanup time which is

00:20:01,889 --> 00:20:07,649
the allocation of shared memory buffer

00:20:03,899 --> 00:20:11,669
this will job very significantly so this

00:20:07,649 --> 00:20:14,250
means that because you know the overall

00:20:11,669 --> 00:20:16,529
wrong time overall carry time for a

00:20:14,250 --> 00:20:19,980
fixed number of threads is almost the

00:20:16,529 --> 00:20:22,620
same especially for Hardware wrong and

00:20:19,980 --> 00:20:25,440
for packet preparation so if you cost

00:20:22,620 --> 00:20:27,440
too much time on the clean up you will

00:20:25,440 --> 00:20:31,789
have performance joking

00:20:27,440 --> 00:20:34,220
so the the philosophy behind this is

00:20:31,789 --> 00:20:37,730
that we need to have more jobs so that

00:20:34,220 --> 00:20:39,769
the shadow power used to buy each job is

00:20:37,730 --> 00:20:42,080
very small we only need to allocate a

00:20:39,769 --> 00:20:43,460
very a very small amount of shadow

00:20:42,080 --> 00:20:46,220
buffer so that the one with the

00:20:43,460 --> 00:20:50,450
allocated this buffer the cost will be

00:20:46,220 --> 00:20:53,389
reduced okay so this is why we that's

00:20:50,450 --> 00:20:56,059
why we choose to use them multiple jobs

00:20:53,389 --> 00:21:00,980
and that you allow the job to share one

00:20:56,059 --> 00:21:07,039
very small shell buffer okay so let's go

00:21:00,980 --> 00:21:10,250
to the next page so so actually we also

00:21:07,039 --> 00:21:13,909
have some other other worker items in

00:21:10,250 --> 00:21:18,019
progress for example we are working on a

00:21:13,909 --> 00:21:20,570
solution you know not not not use the

00:21:18,019 --> 00:21:22,879
packet buffer but we are working on a

00:21:20,570 --> 00:21:25,549
solution to allow the regular expression

00:21:22,879 --> 00:21:28,399
engine or allow anyway I'm doing on the

00:21:25,549 --> 00:21:32,299
FPGA to access the Postgres internal

00:21:28,399 --> 00:21:34,370
page directory with the page pointers in

00:21:32,299 --> 00:21:37,340
virtual address so this will save the

00:21:34,370 --> 00:21:39,919
amount of time when you copy the the

00:21:37,340 --> 00:21:43,159
internal data structure to the packet

00:21:39,919 --> 00:21:45,980
buffer this is what we can benefit from

00:21:43,159 --> 00:21:49,820
Kathy and open Kathy but this is still

00:21:45,980 --> 00:21:51,889
in progress okay and then the other the

00:21:49,820 --> 00:21:55,879
other work item is the job manager well

00:21:51,889 --> 00:21:59,840
it's in progress it will reduce the cost

00:21:55,879 --> 00:22:02,929
of job configuration so if you configure

00:21:59,840 --> 00:22:06,379
the job you need to have a large amount

00:22:02,929 --> 00:22:08,480
of i/o accesses so those will have

00:22:06,379 --> 00:22:11,570
impact to overall performance if you

00:22:08,480 --> 00:22:15,679
have too many jobs so but if we have a

00:22:11,570 --> 00:22:19,129
hardware job manager that will have you

00:22:15,679 --> 00:22:22,399
know benefit for for you know reducing

00:22:19,129 --> 00:22:24,830
the the configuration overhead because

00:22:22,399 --> 00:22:27,289
the hundreds of manager they will

00:22:24,830 --> 00:22:30,830
configure the engines they're actually

00:22:27,289 --> 00:22:33,370
without mmm our access and we also have

00:22:30,830 --> 00:22:38,389
other types of work items including

00:22:33,370 --> 00:22:40,000
developing other types of engines such

00:22:38,389 --> 00:22:42,490
as the general car engine

00:22:40,000 --> 00:22:44,889
or hash join engines but all those

00:22:42,490 --> 00:22:50,049
engines are still under progress I

00:22:44,889 --> 00:22:52,629
improve s okay this is a summary of what

00:22:50,049 --> 00:22:55,899
we are going to present maybe in the

00:22:52,629 --> 00:22:59,980
next few months including more

00:22:55,899 --> 00:23:02,860
performance and analysis Harbor job

00:22:59,980 --> 00:23:06,269
manager and they use page pointers

00:23:02,860 --> 00:23:09,070
directory other types of engines and

00:23:06,269 --> 00:23:12,340
adoption to open carry I believe

00:23:09,070 --> 00:23:15,490
adoption to open happy move from capital

00:23:12,340 --> 00:23:18,129
to open carry should be very easy if of

00:23:15,490 --> 00:23:20,559
our work based on snap or open heavy

00:23:18,129 --> 00:23:22,179
acceleration firm work and we are also

00:23:20,559 --> 00:23:24,850
seeking opportunities working with the

00:23:22,179 --> 00:23:27,399
clients or partners for other types of

00:23:24,850 --> 00:23:29,710
databases database because currently we

00:23:27,399 --> 00:23:31,980
are only we only have the solution for

00:23:29,710 --> 00:23:35,279
Postgres there should be other

00:23:31,980 --> 00:23:38,379
requirements for may be more ready be on

00:23:35,279 --> 00:23:41,590
any other MongoDB or any other type of

00:23:38,379 --> 00:23:44,950
database if the customers or partners we

00:23:41,590 --> 00:23:48,519
are interested in and the list here we

00:23:44,950 --> 00:23:51,009
goes to the conclusion so our demo shows

00:23:48,519 --> 00:23:52,899
that Postgres and Kefi they can work

00:23:51,009 --> 00:23:57,039
well especially for the Recker

00:23:52,899 --> 00:24:02,320
expression matching and it it is a very

00:23:57,039 --> 00:24:05,980
variable demo or showcase that needs to

00:24:02,320 --> 00:24:08,669
be paid more attention if we want you

00:24:05,980 --> 00:24:12,809
okay once you have a more sophisticated

00:24:08,669 --> 00:24:18,090
solutions or products in the future and

00:24:12,809 --> 00:24:21,279
also we are you know because we are a

00:24:18,090 --> 00:24:23,230
demo team where showcased team we most

00:24:21,279 --> 00:24:25,809
of our engineers are from hardware

00:24:23,230 --> 00:24:29,259
domain we are chip engineers so we're

00:24:25,809 --> 00:24:30,909
not very familiar with the higher for

00:24:29,259 --> 00:24:34,539
the solute of all applications for the

00:24:30,909 --> 00:24:37,990
software's so we learn that from scratch

00:24:34,539 --> 00:24:40,539
by you know googling but you know if

00:24:37,990 --> 00:24:43,809
there's experts from database errors

00:24:40,539 --> 00:24:47,590
they can join us to to chill the kind of

00:24:43,809 --> 00:24:51,990
details I believe we are able to present

00:24:47,590 --> 00:24:55,110
more powerful more production level

00:24:51,990 --> 00:24:59,020
solutions in the few

00:24:55,110 --> 00:25:01,440
so that's all I want to present any

00:24:59,020 --> 00:25:01,440
questions

00:25:01,600 --> 00:25:06,180
[Applause]

00:25:09,320 --> 00:25:12,430
[Music]

00:25:14,570 --> 00:25:20,140
[Music]

00:25:25,940 --> 00:25:31,050
okay the question is that you know in

00:25:28,950 --> 00:25:33,870
the performance broke down which part

00:25:31,050 --> 00:25:39,000
stands for the configuration overhead

00:25:33,870 --> 00:25:42,330
okay so actually the this breakdown

00:25:39,000 --> 00:25:46,200
with all the configuration overheads are

00:25:42,330 --> 00:25:48,690
included in the blue part yeah so this

00:25:46,200 --> 00:25:51,450
blue part includes post configuration

00:25:48,690 --> 00:25:53,640
and the runtime for her for hardware so

00:25:51,450 --> 00:25:56,490
we didn't you know pretty too much

00:25:53,640 --> 00:25:58,860
details because you know I'm not sure if

00:25:56,490 --> 00:26:02,370
this is the best situation to present so

00:25:58,860 --> 00:26:06,300
many technical details yeah but but

00:26:02,370 --> 00:26:09,320
maybe I maybe I think I know this year

00:26:06,300 --> 00:26:12,150
we will have a paper take a new paper to

00:26:09,320 --> 00:26:13,890
be ready and if you want I can send you

00:26:12,150 --> 00:26:15,960
a copy of that there will be more

00:26:13,890 --> 00:26:25,840
detailed analysis and the performance

00:26:15,960 --> 00:26:32,860
perked up okay

00:26:25,840 --> 00:26:37,240
me an idea yeah so yeah I i Peters

00:26:32,860 --> 00:26:39,840
questions as you know country the this

00:26:37,240 --> 00:26:42,940
chart shows a latency you know

00:26:39,840 --> 00:26:45,340
performance we don't have a throughput

00:26:42,940 --> 00:26:47,800
performance the reason is that too many

00:26:45,340 --> 00:26:51,400
less reports we need to have multiple

00:26:47,800 --> 00:26:54,540
clients work access the server database

00:26:51,400 --> 00:26:59,320
and at the same time so right now our

00:26:54,540 --> 00:27:02,260
hardware engine it's still you know we

00:26:59,320 --> 00:27:04,690
still need some extra work to make those

00:27:02,260 --> 00:27:07,260
engines to handle requests from

00:27:04,690 --> 00:27:09,910
different process from different

00:27:07,260 --> 00:27:12,040
different queries at the same time but

00:27:09,910 --> 00:27:15,400
this will be ready very soon maybe in

00:27:12,040 --> 00:27:17,980
September right now that the number is

00:27:15,400 --> 00:27:22,140
not a very stable we are still working

00:27:17,980 --> 00:27:22,140
on data to present the stupid number

00:27:32,400 --> 00:27:35,819

YouTube URL: https://www.youtube.com/watch?v=iJTy9oYuNEY


