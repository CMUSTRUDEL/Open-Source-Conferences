Title: OpenPOWER Summit NA 2019: New Innovations in Power and Performance Management
Publication date: 2019-08-28
Playlist: OpenPOWER Summit NA 2019
Description: 
	Presented by Todd Rosedahl, IBM 

This talk will provide an overview of the existing power measurement, power management, and performance-boosting capabilities of the POWER architecture. Various levels of the hardware/firmware and software stack will be described. Then, it will focus on a new data collection method enabled by OpenBMC that provides a rich data set including performance metrics, temperatures, and power consumption. This new technology has been deployed at scale at ORNL with excellent results. Additionally, a newly emerging power/performance-optimizing application, called GEOPM, will be described.
Captions: 
	00:00:00,240 --> 00:00:06,270
thank us seven Tadros doll I've worked

00:00:03,179 --> 00:00:09,269
at IBM for 25 years it seems like

00:00:06,270 --> 00:00:11,700
forever and all in power our thermal

00:00:09,269 --> 00:00:14,009
management it's been it's been fun to

00:00:11,700 --> 00:00:15,230
see some of the the industry shift it's

00:00:14,009 --> 00:00:17,609
been fun to be a part of the open power

00:00:15,230 --> 00:00:19,020
and we're get to work with a lot of

00:00:17,609 --> 00:00:22,410
other companies and you can see up here

00:00:19,020 --> 00:00:24,840
that we have it this pitch was from me

00:00:22,410 --> 00:00:26,640
from Justin Taylor or Taylor from IBM

00:00:24,840 --> 00:00:28,410
and then a guy from Intel so we are

00:00:26,640 --> 00:00:30,390
collaborating with them not just an open

00:00:28,410 --> 00:00:31,650
VM see but on a lot of power management

00:00:30,390 --> 00:00:34,649
things which is which has been

00:00:31,650 --> 00:00:36,329
interesting so I'm going to structure

00:00:34,649 --> 00:00:38,820
this talk is I'll give you a little

00:00:36,329 --> 00:00:40,140
overview on how the power systems do

00:00:38,820 --> 00:00:41,399
power and thermal management from a

00:00:40,140 --> 00:00:43,020
hardware and then I'll show you the

00:00:41,399 --> 00:00:44,820
software second a little bit maybe you

00:00:43,020 --> 00:00:46,590
most of you are familiar with that and

00:00:44,820 --> 00:00:48,149
it'll be brief well I'm going to talk

00:00:46,590 --> 00:00:50,280
about workload optimized frequency it's

00:00:48,149 --> 00:00:52,680
a new way that we are maximizing the

00:00:50,280 --> 00:00:55,620
performance of our systems there are two

00:00:52,680 --> 00:00:57,360
new telemetry collection techniques

00:00:55,620 --> 00:00:59,039
wanted one in band one out of and I'll

00:00:57,360 --> 00:01:01,890
explain that and how those are how those

00:00:59,039 --> 00:01:03,930
are used I both of them were came out of

00:01:01,890 --> 00:01:05,760
the work with Oakridge where they wanted

00:01:03,930 --> 00:01:07,020
a couple of different ways to collect a

00:01:05,760 --> 00:01:09,119
bunch of data from their systems for

00:01:07,020 --> 00:01:10,680
various reasons and then something new

00:01:09,119 --> 00:01:13,380
that we're working on is this HPC power

00:01:10,680 --> 00:01:17,580
stack missus power as in energy it's not

00:01:13,380 --> 00:01:22,530
power as in our power chip stack just to

00:01:17,580 --> 00:01:26,729
be clear this is so you guys have

00:01:22,530 --> 00:01:29,460
probably seen a peanut and processor

00:01:26,729 --> 00:01:31,820
yeah really cool I like carrying that

00:01:29,460 --> 00:01:35,460
around because I'm a little geeky and

00:01:31,820 --> 00:01:37,290
this right here represents this module

00:01:35,460 --> 00:01:39,119
so it has this is a block diamond so

00:01:37,290 --> 00:01:41,420
that's 24 cores most of the cores you

00:01:39,119 --> 00:01:43,710
running the actual customer workloads on

00:01:41,420 --> 00:01:45,450
other people were showing this as well

00:01:43,710 --> 00:01:48,060
then there's this on ship controller

00:01:45,450 --> 00:01:49,710
complex and that is a completely

00:01:48,060 --> 00:01:51,479
separate processor it does not run

00:01:49,710 --> 00:01:53,610
customer jobs much like the SB if you

00:01:51,479 --> 00:01:55,200
just watched IMATS SP talk that's

00:01:53,610 --> 00:01:58,049
another little processor this one is a

00:01:55,200 --> 00:01:59,759
405 has its own Ram has its own

00:01:58,049 --> 00:02:02,670
general-purpose engines and then it has

00:01:59,759 --> 00:02:05,280
connections one connection is up on the

00:02:02,670 --> 00:02:08,369
power bus which gives it access to all

00:02:05,280 --> 00:02:10,140
kinds of interesting data inside the

00:02:08,369 --> 00:02:12,060
system so they can go and read core

00:02:10,140 --> 00:02:13,319
temperatures utilizations memory

00:02:12,060 --> 00:02:13,680
temperatures and utilizations it can

00:02:13,319 --> 00:02:15,659
talk

00:02:13,680 --> 00:02:17,430
up in domain memory uses that to talk to

00:02:15,659 --> 00:02:18,780
opal which is the open power abstraction

00:02:17,430 --> 00:02:21,900
layer there's communication that goes

00:02:18,780 --> 00:02:24,689
back and forth between the OCC and opal

00:02:21,900 --> 00:02:26,099
and and its job in life is has

00:02:24,689 --> 00:02:27,120
traditionally been power and thermal

00:02:26,099 --> 00:02:28,470
management let's make sure the system

00:02:27,120 --> 00:02:29,879
that stays under a certain power limit

00:02:28,470 --> 00:02:32,069
let's make sure that everything stays

00:02:29,879 --> 00:02:34,319
thermally safe and at the same time we

00:02:32,069 --> 00:02:37,530
want to maximize the the performance of

00:02:34,319 --> 00:02:40,500
the system there's some special hardware

00:02:37,530 --> 00:02:41,609
down here that the OCC reads and some of

00:02:40,500 --> 00:02:43,920
these interfaces are a little bit out of

00:02:41,609 --> 00:02:46,049
date they're 38 but it reads this this

00:02:43,920 --> 00:02:48,750
hardware and therefore it gets the

00:02:46,049 --> 00:02:50,250
voltage and the power on every every

00:02:48,750 --> 00:02:51,930
rail in the system so like you can tell

00:02:50,250 --> 00:02:54,840
fan power it's not just it's not a

00:02:51,930 --> 00:02:55,769
psychic power it has a variety of access

00:02:54,840 --> 00:02:57,900
to a variety of different power levels

00:02:55,769 --> 00:02:59,720
and it can do some voltage adjustment

00:02:57,900 --> 00:03:02,670
I'll talk about how that works and then

00:02:59,720 --> 00:03:06,060
there's a connection here new for p9 for

00:03:02,670 --> 00:03:07,230
the BMC two into the OCC if you've seen

00:03:06,060 --> 00:03:10,109
it if you saw I'm at stock it goes

00:03:07,230 --> 00:03:11,639
through the self boot engine and talks

00:03:10,109 --> 00:03:13,680
into the O's to see it and this is used

00:03:11,639 --> 00:03:15,510
to collect all kinds of data so they the

00:03:13,680 --> 00:03:17,549
OCC because it has access to this data

00:03:15,510 --> 00:03:19,620
it presents it down and BMC polls

00:03:17,549 --> 00:03:21,389
actually pulls it and gets his data and

00:03:19,620 --> 00:03:23,940
then he could be can pass it out the a

00:03:21,389 --> 00:03:26,879
variety of means out-of-band and that's

00:03:23,940 --> 00:03:29,579
that telemetry that I was mentioning so

00:03:26,879 --> 00:03:33,090
that's the hardware block diagram over

00:03:29,579 --> 00:03:35,099
you mostly I show this that because it's

00:03:33,090 --> 00:03:37,620
kind of a pat on the back because we as

00:03:35,099 --> 00:03:39,150
a community because when I started

00:03:37,620 --> 00:03:41,069
giving the stock a lot of these things

00:03:39,150 --> 00:03:42,359
were not green and green means this is

00:03:41,069 --> 00:03:44,459
open source and now you can see

00:03:42,359 --> 00:03:46,199
everything as open source and on chip

00:03:44,459 --> 00:03:47,970
controller its output engine of course

00:03:46,199 --> 00:03:50,760
host boot hopeful all the way up and now

00:03:47,970 --> 00:03:51,510
that with the open BMC the BMC as well

00:03:50,760 --> 00:03:53,780
as open source

00:03:51,510 --> 00:03:56,190
I just put next cat here as a

00:03:53,780 --> 00:03:58,639
representation of some system management

00:03:56,190 --> 00:04:01,319
entity there are many such open source

00:03:58,639 --> 00:04:04,079
admins but that's that's one that we are

00:04:01,319 --> 00:04:08,430
actually using it on the AC 922 it at

00:04:04,079 --> 00:04:09,900
Oakridge alright so that was a brief

00:04:08,430 --> 00:04:12,449
overview and we're going to talk about

00:04:09,900 --> 00:04:14,639
this workload optimized frequency this

00:04:12,449 --> 00:04:16,409
is kind of a complicated topic

00:04:14,639 --> 00:04:17,669
potentially we'll see how we do so

00:04:16,409 --> 00:04:19,470
normally when you are going to design a

00:04:17,669 --> 00:04:21,419
system you design it to a certain

00:04:19,470 --> 00:04:23,760
thermal design point that means a

00:04:21,419 --> 00:04:27,150
wattage that your socket is going to is

00:04:23,760 --> 00:04:28,440
going to consume then you base every

00:04:27,150 --> 00:04:30,150
else around that so you'd have to have

00:04:28,440 --> 00:04:32,790
already heatsink and have to fight fans

00:04:30,150 --> 00:04:35,250
and cooling to cool that much power and

00:04:32,790 --> 00:04:38,250
how he traditionally did this as we

00:04:35,250 --> 00:04:39,840
would take a take a socket and we would

00:04:38,250 --> 00:04:43,169
run the max or the presses we run the

00:04:39,840 --> 00:04:44,880
maximum workload on it and at maximum

00:04:43,169 --> 00:04:47,850
conditions ambient conditions and we'd

00:04:44,880 --> 00:04:49,169
say what it at what frequency now can I

00:04:47,850 --> 00:04:51,090
run what frequency in voltage actually

00:04:49,169 --> 00:04:52,919
lower the frequency in voltage the the

00:04:51,090 --> 00:04:54,389
less power you're going to consume what

00:04:52,919 --> 00:04:56,460
frequency and voltage can I run at to

00:04:54,389 --> 00:04:57,900
hit that thermal design point we picked

00:04:56,460 --> 00:04:59,490
the maximum or we could find it we would

00:04:57,900 --> 00:05:01,350
call it nominal and that's the frequency

00:04:59,490 --> 00:05:03,360
you would we would ship the system and

00:05:01,350 --> 00:05:05,160
well we started realizing that most

00:05:03,360 --> 00:05:07,020
customers are not running in such an

00:05:05,160 --> 00:05:09,600
environment right there there's room

00:05:07,020 --> 00:05:13,110
there to go to a higher frequency and

00:05:09,600 --> 00:05:15,539
use more use more power and and yet

00:05:13,110 --> 00:05:17,220
still stay underneath this you still

00:05:15,539 --> 00:05:20,400
keep the system safe but with under that

00:05:17,220 --> 00:05:21,539
TDP that was and we would we did that in

00:05:20,400 --> 00:05:24,990
the past and that was our life sort of

00:05:21,539 --> 00:05:26,550
our legacy turbo now for the for p9 who

00:05:24,990 --> 00:05:29,270
started taking advantage of of even more

00:05:26,550 --> 00:05:31,710
things so we realized that most of these

00:05:29,270 --> 00:05:33,330
customers do not run a maximum workload

00:05:31,710 --> 00:05:35,789
they are most usually they are running

00:05:33,330 --> 00:05:38,160
some 40 you know 30 40 % workload

00:05:35,789 --> 00:05:40,080
sometimes the cores are off so in those

00:05:38,160 --> 00:05:41,910
cases you have a lot of extra power in

00:05:40,080 --> 00:05:44,669
your socket you can raise that voltage

00:05:41,910 --> 00:05:46,110
and frequency up then to what we're

00:05:44,669 --> 00:05:47,520
calling you this wolf max for you can

00:05:46,110 --> 00:05:50,370
see and still be under that thermal

00:05:47,520 --> 00:05:51,900
design point that's what we're doing

00:05:50,370 --> 00:05:53,520
that means that the frequency is going

00:05:51,900 --> 00:05:56,550
to the maximum freaky the system runs

00:05:53,520 --> 00:06:02,190
that is going to be based on on workload

00:05:56,550 --> 00:06:03,810
and a active core account but it's still

00:06:02,190 --> 00:06:05,610
we still maintain the part-to-part

00:06:03,810 --> 00:06:07,590
determinism so we factor off the leakage

00:06:05,610 --> 00:06:09,300
current that you that you have between

00:06:07,590 --> 00:06:12,630
parts to make sure that if you would run

00:06:09,300 --> 00:06:14,250
the same workload on the same system or

00:06:12,630 --> 00:06:15,479
if your this workload on this system and

00:06:14,250 --> 00:06:16,650
the same exact workload on that system

00:06:15,479 --> 00:06:20,460
the same configuration you're going to

00:06:16,650 --> 00:06:23,820
get the same result there's another way

00:06:20,460 --> 00:06:26,460
to look at it we have a table this is

00:06:23,820 --> 00:06:28,740
not really how it works in p9 we don't

00:06:26,460 --> 00:06:31,020
we have many many tables but this is

00:06:28,740 --> 00:06:32,639
conceptually a way to think about it so

00:06:31,020 --> 00:06:35,880
on this axis you've got the power

00:06:32,639 --> 00:06:38,070
consumption essentially the non non DC

00:06:35,880 --> 00:06:39,599
power consumption when you're out of max

00:06:38,070 --> 00:06:40,310
of your maximum workload you're running

00:06:39,599 --> 00:06:41,990
the max

00:06:40,310 --> 00:06:43,400
excellent power consumption if all of

00:06:41,990 --> 00:06:46,430
your cores are active honestly they're

00:06:43,400 --> 00:06:47,960
you know active core counts so if you

00:06:46,430 --> 00:06:50,570
had a ten core part which of course this

00:06:47,960 --> 00:06:52,910
is old we have bigger than beer course

00:06:50,570 --> 00:06:54,410
accounts than that now but if you have a

00:06:52,910 --> 00:06:55,670
10 cores active and you're running at

00:06:54,410 --> 00:06:58,640
max workload you can be at this two

00:06:55,670 --> 00:07:02,750
point eight to seven that would be your

00:06:58,640 --> 00:07:04,220
maximum frequency then as if but then if

00:07:02,750 --> 00:07:05,810
you would turn Coors off you can see

00:07:04,220 --> 00:07:07,520
even though at that maximum workload you

00:07:05,810 --> 00:07:10,520
can increase that frequency up so the

00:07:07,520 --> 00:07:11,990
unship controller is determining hey I'm

00:07:10,520 --> 00:07:13,190
here reading a bunch of things saying

00:07:11,990 --> 00:07:15,350
here's my maximum workload

00:07:13,190 --> 00:07:17,090
I'm the cores that are on so I can I can

00:07:15,350 --> 00:07:18,980
essentially set the maximum frequency to

00:07:17,090 --> 00:07:21,680
three point five I can raise up the the

00:07:18,980 --> 00:07:23,570
max frequency also when the workload

00:07:21,680 --> 00:07:25,910
drops if you run a they have 40%

00:07:23,570 --> 00:07:27,410
workload even with all ten cores on you

00:07:25,910 --> 00:07:29,150
can see we can additionally we can boost

00:07:27,410 --> 00:07:32,030
that frequency up can take advantage of

00:07:29,150 --> 00:07:35,240
the power Headroom that exists because

00:07:32,030 --> 00:07:38,360
the workload is not max and the cork

00:07:35,240 --> 00:07:41,960
belt may not be mags it's at making

00:07:38,360 --> 00:07:43,990
sense it's all good here's how here's

00:07:41,960 --> 00:07:46,190
how the wolf algorithm works inside the

00:07:43,990 --> 00:07:47,600
yeah so one thing if you have any

00:07:46,190 --> 00:07:49,190
questions I'd like to just keep it

00:07:47,600 --> 00:07:50,600
interactive it's more fun so if you if

00:07:49,190 --> 00:07:54,080
you have something to ask I would

00:07:50,600 --> 00:07:57,170
appreciate it here's here's how we do

00:07:54,080 --> 00:07:59,000
the vo she does the voting back so we we

00:07:57,170 --> 00:08:00,410
are still the OCC is up reading that

00:07:59,000 --> 00:08:02,420
power measurement hardware for 500

00:08:00,410 --> 00:08:04,190
microseconds we're comparing it to some

00:08:02,420 --> 00:08:05,960
limit there's only some some power limit

00:08:04,190 --> 00:08:07,460
that that no it has and voting it and

00:08:05,960 --> 00:08:08,750
putting in here say what is the maximum

00:08:07,460 --> 00:08:11,660
frequency voltage I could get from a

00:08:08,750 --> 00:08:13,250
power perspective similarly it's not

00:08:11,660 --> 00:08:14,420
reading all these temperatures taken

00:08:13,250 --> 00:08:16,850
reading temperatures every 4

00:08:14,420 --> 00:08:18,680
milliseconds from memory and and

00:08:16,850 --> 00:08:20,210
processors putting that into the voting

00:08:18,680 --> 00:08:21,620
box and saying here's the maximum

00:08:20,210 --> 00:08:24,080
frequency that I think the system could

00:08:21,620 --> 00:08:25,520
run it from a thermal perspective then

00:08:24,080 --> 00:08:26,930
there's this wolf algorithm running it

00:08:25,520 --> 00:08:29,090
runs every 4 milliseconds as well

00:08:26,930 --> 00:08:30,950
reading all kinds of information and

00:08:29,090 --> 00:08:33,290
deciding here's the maximum frequency I

00:08:30,950 --> 00:08:35,690
think we could run at from a wolf yeah

00:08:33,290 --> 00:08:37,220
wolf clip perspective don't you take

00:08:35,690 --> 00:08:39,860
them you know the lowest of lows goes

00:08:37,220 --> 00:08:41,300
into this maximum P state and and just

00:08:39,860 --> 00:08:43,310
note that that's not the P state that

00:08:41,300 --> 00:08:44,900
the system is going to run at the OCC is

00:08:43,310 --> 00:08:46,460
setting a clip and setting the maximum

00:08:44,900 --> 00:08:47,870
frequency you're going to run at the

00:08:46,460 --> 00:08:48,400
actual frequency is still just like on

00:08:47,870 --> 00:08:51,230
any other

00:08:48,400 --> 00:08:52,940
x86 system running Linux is set by Linux

00:08:51,230 --> 00:08:53,270
Linux comes in and says here's the the

00:08:52,940 --> 00:08:54,620
PC

00:08:53,270 --> 00:08:56,750
I want to run a so if you're running an

00:08:54,620 --> 00:08:59,000
on-demand governor they might vote for a

00:08:56,750 --> 00:09:00,740
lower peace date than the one that the

00:08:59,000 --> 00:09:02,870
FCC says that's fine then that sets the

00:09:00,740 --> 00:09:04,730
frequency if they asked for that if they

00:09:02,870 --> 00:09:06,710
have the the performance governor around

00:09:04,730 --> 00:09:08,900
they asked for the maximum P State that

00:09:06,710 --> 00:09:11,180
means that they're not going to get it

00:09:08,900 --> 00:09:12,830
right they might get it but it might

00:09:11,180 --> 00:09:15,200
potentially be clipped either by wolf or

00:09:12,830 --> 00:09:19,190
some one of these other other reasons

00:09:15,200 --> 00:09:24,790
one of the other loops so that is the

00:09:19,190 --> 00:09:24,790
voting bugs what's that

00:09:27,920 --> 00:09:36,389
well finish the well it doesn't get that

00:09:33,839 --> 00:09:37,680
information excision its own algorithms

00:09:36,389 --> 00:09:39,810
to figure out what do I think the

00:09:37,680 --> 00:09:41,009
utilization in the system is so I keep

00:09:39,810 --> 00:09:42,930
that on our system for this on-demand

00:09:41,009 --> 00:09:45,629
governor it's looking out and seeing if

00:09:42,930 --> 00:09:47,160
I'm less than 80% utilized from from its

00:09:45,629 --> 00:09:49,439
perspective nothing to do with the OCC

00:09:47,160 --> 00:09:51,839
then it's gonna it's going to lower the

00:09:49,439 --> 00:09:53,180
frequency down and ask say hey give me a

00:09:51,839 --> 00:09:56,129
lower frequency which would which would

00:09:53,180 --> 00:09:57,990
be actuated yes it's two totally

00:09:56,129 --> 00:09:59,970
independent systems running and trying

00:09:57,990 --> 00:10:01,319
to figure out what the frequency you

00:09:59,970 --> 00:10:02,879
know they're saying here's the frequency

00:10:01,319 --> 00:10:04,259
should be from their perspective and I'm

00:10:02,879 --> 00:10:05,610
saying here's the maximum it could be

00:10:04,259 --> 00:10:08,120
from a power thermal in Wolfe

00:10:05,610 --> 00:10:08,120
perspective

00:10:12,199 --> 00:10:19,559
ya know we're probably maybe maybe not

00:10:18,119 --> 00:10:21,809
the best term we've always call it of

00:10:19,559 --> 00:10:23,039
voting backs it's really just we of

00:10:21,809 --> 00:10:25,379
these two whichever one is the lowest

00:10:23,039 --> 00:10:28,109
wins and then here

00:10:25,379 --> 00:10:29,039
similarly I'll the lowest B state is the

00:10:28,109 --> 00:10:34,609
one that's going to be put here eyes

00:10:29,039 --> 00:10:34,609
into the maximum p state that yeah

00:10:41,490 --> 00:10:46,420
does Walter mean it's all open source so

00:10:44,500 --> 00:10:52,180
it there is control but you need to does

00:10:46,420 --> 00:10:55,450
do you does a user have control this is

00:10:52,180 --> 00:10:57,340
the system designer has control of how

00:10:55,450 --> 00:10:59,500
though FL ger them runs yeah and I mean

00:10:57,340 --> 00:11:01,810
so if you go back here it's really a

00:10:59,500 --> 00:11:03,610
series of many tables and those tables

00:11:01,810 --> 00:11:06,580
are generated when we build the system

00:11:03,610 --> 00:11:07,780
it's it's a per chip table so how so

00:11:06,580 --> 00:11:10,420
exactly what's gonna be picked here is

00:11:07,780 --> 00:11:12,250
based on many many discuss many

00:11:10,420 --> 00:11:13,600
discussions and an algorithm to say all

00:11:12,250 --> 00:11:15,280
right what frequency can we really run

00:11:13,600 --> 00:11:16,810
at if if we're in this particular

00:11:15,280 --> 00:11:19,150
condition that goes into that table but

00:11:16,810 --> 00:11:21,780
think it's it's loaded it's in the flash

00:11:19,150 --> 00:11:27,820
and then they OCC uses that table yeah

00:11:21,780 --> 00:11:29,860
yep let's move under this telemetry so

00:11:27,820 --> 00:11:31,810
there's there are two schools of thought

00:11:29,860 --> 00:11:33,160
here with data collection there's in

00:11:31,810 --> 00:11:35,140
band and out of and both of them have

00:11:33,160 --> 00:11:38,050
have use uses and both of them have

00:11:35,140 --> 00:11:39,490
drawbacks so one method is you go hidden

00:11:38,050 --> 00:11:41,830
ban when I say in band what I mean is

00:11:39,490 --> 00:11:43,960
you're using the actual processes that

00:11:41,830 --> 00:11:46,720
customer would run on to collect this

00:11:43,960 --> 00:11:47,940
data so that affects your system

00:11:46,720 --> 00:11:49,630
performance because you're using

00:11:47,940 --> 00:11:52,750
resources that the customer would want

00:11:49,630 --> 00:11:55,090
to use the good thing about that though

00:11:52,750 --> 00:11:57,130
is it it is really easy to profile for a

00:11:55,090 --> 00:11:58,360
job so you might do I want to start

00:11:57,130 --> 00:11:59,530
measuring here start measuring there

00:11:58,360 --> 00:12:02,160
inside of a job and you can do it in

00:11:59,530 --> 00:12:08,560
band extremely fast so that's that's a

00:12:02,160 --> 00:12:09,580
benefit the out-of-band is is also okay

00:12:08,560 --> 00:12:11,110
the good thing about the auto band is

00:12:09,580 --> 00:12:12,850
you're not affecting again you're not

00:12:11,110 --> 00:12:13,900
affecting your system or your system

00:12:12,850 --> 00:12:15,520
workload at all

00:12:13,900 --> 00:12:17,260
you're just you're measuring from the

00:12:15,520 --> 00:12:18,640
signed shift controller or from the BMC

00:12:17,260 --> 00:12:20,140
and gathering the data and you're not

00:12:18,640 --> 00:12:23,770
affecting the running jobs that's a

00:12:20,140 --> 00:12:25,720
really really beneficial aspect of

00:12:23,770 --> 00:12:27,820
out-of-band and the other thing is you

00:12:25,720 --> 00:12:29,290
when you go out a band if you really

00:12:27,820 --> 00:12:30,700
need this data all the time when you're

00:12:29,290 --> 00:12:31,900
OS could crash and you wouldn't have the

00:12:30,700 --> 00:12:33,370
data so if you're using for something

00:12:31,900 --> 00:12:33,820
critical you would need to want to go on

00:12:33,370 --> 00:12:36,430
a bin

00:12:33,820 --> 00:12:39,070
problem with on a band is it it's it's

00:12:36,430 --> 00:12:40,930
traditionally really really slow right

00:12:39,070 --> 00:12:42,910
and you have a lot of network problems

00:12:40,930 --> 00:12:44,770
it doesn't scale whereas this certainly

00:12:42,910 --> 00:12:45,680
scales well but this it doesn't scale up

00:12:44,770 --> 00:12:48,560
into it

00:12:45,680 --> 00:12:50,420
you know 300 400 nodes and you mean

00:12:48,560 --> 00:12:51,290
nobody likes IPMI right think I'll slow

00:12:50,420 --> 00:12:52,790
that is that's one way people

00:12:51,290 --> 00:12:55,029
traditionally do it you go and you grab

00:12:52,790 --> 00:12:57,529
some power some thermal information

00:12:55,029 --> 00:12:59,209
that's no good you know we moved to rest

00:12:57,529 --> 00:13:02,779
on open BMC and now we're moving that to

00:12:59,209 --> 00:13:06,680
red fish that's that it's better but it

00:13:02,779 --> 00:13:08,269
has similar scale issues actually now

00:13:06,680 --> 00:13:10,339
and there there's something that this is

00:13:08,269 --> 00:13:11,269
not new this is this is um I wanna I

00:13:10,339 --> 00:13:12,860
need to mention though is something

00:13:11,269 --> 00:13:15,139
called aim stir it is an open source

00:13:12,860 --> 00:13:17,660
program and it runs up here on this

00:13:15,139 --> 00:13:19,699
laptop and you can use that to tunnel

00:13:17,660 --> 00:13:22,550
all the way through the BMC into the OCC

00:13:19,699 --> 00:13:24,800
and and collect all this really detailed

00:13:22,550 --> 00:13:26,660
sensor information really fast so that's

00:13:24,800 --> 00:13:28,490
a I have a I have a chart on that that

00:13:26,660 --> 00:13:30,800
I'll talk about but that's good for pro

00:13:28,490 --> 00:13:34,189
telling profiling sections of jobs and

00:13:30,800 --> 00:13:36,350
the new thing so this in bandpass is new

00:13:34,189 --> 00:13:40,339
and I'll talk about that and often then

00:13:36,350 --> 00:13:42,730
this crass D function is new this is a

00:13:40,339 --> 00:13:45,499
it's there's a web socket that gets

00:13:42,730 --> 00:13:47,509
subscribed to and the BMC just pumps

00:13:45,499 --> 00:13:48,680
this data out we streamlined the data

00:13:47,509 --> 00:13:50,750
down so it's just a couple bytes per

00:13:48,680 --> 00:13:51,920
sensor and if there's a change in a

00:13:50,750 --> 00:13:53,509
sensor is the only time it gets

00:13:51,920 --> 00:13:56,990
straightened out so once the second the

00:13:53,509 --> 00:14:00,769
BMC will pass its data back to a service

00:13:56,990 --> 00:14:02,899
up here and that is been that's been the

00:14:00,769 --> 00:14:05,930
answer I guess so far for okra just

00:14:02,899 --> 00:14:09,529
worked very well it scales and I'll show

00:14:05,930 --> 00:14:12,009
you a demo on that hopefully so first

00:14:09,529 --> 00:14:14,720
let's talk about these in-band sensors

00:14:12,009 --> 00:14:17,089
so there are 40 different types of

00:14:14,720 --> 00:14:19,879
sensors over 400 total they're all

00:14:17,089 --> 00:14:22,339
time-stamped but what kind of cool about

00:14:19,879 --> 00:14:23,420
this is the OCC and the OS and the GPU I

00:14:22,339 --> 00:14:24,920
don't show that you've used here but

00:14:23,420 --> 00:14:26,449
there's an out-of-band interface from

00:14:24,920 --> 00:14:28,790
the OCC to the GPUs where we are

00:14:26,449 --> 00:14:30,800
measuring data from them all of those

00:14:28,790 --> 00:14:32,779
are timestamp at the same time stamp and

00:14:30,800 --> 00:14:34,220
we made the Nvidia guys use the OS

00:14:32,779 --> 00:14:36,230
timestamp as well so if they collect

00:14:34,220 --> 00:14:38,300
data in band from the from the GPUs it

00:14:36,230 --> 00:14:39,589
can all be correlated so that's I mean

00:14:38,300 --> 00:14:40,939
you got a hat you got to know that right

00:14:39,589 --> 00:14:42,290
the you have to know when the state is

00:14:40,939 --> 00:14:45,249
collecting or in order to line it all up

00:14:42,290 --> 00:14:48,350
and have it make sense there is an

00:14:45,249 --> 00:14:50,209
accumulator and an update tag here so

00:14:48,350 --> 00:14:51,949
you can get energy so you can read hey

00:14:50,209 --> 00:14:52,350
how much power did I use here and then

00:14:51,949 --> 00:14:54,060
just

00:14:52,350 --> 00:14:55,860
how much power to use here in time and

00:14:54,060 --> 00:14:57,030
then you know how long that has been and

00:14:55,860 --> 00:14:59,460
you can calculate it get an energy

00:14:57,030 --> 00:15:02,400
calculation there's mins and maxes that

00:14:59,460 --> 00:15:03,990
are supported the so so the OCC who can

00:15:02,400 --> 00:15:06,720
will say what's my minute max is a

00:15:03,990 --> 00:15:08,190
temperature or voltage or power over a

00:15:06,720 --> 00:15:10,530
certain time period and then you can

00:15:08,190 --> 00:15:13,500
come down and clear that and all of this

00:15:10,530 --> 00:15:16,080
has been put in to the standard Linux LM

00:15:13,500 --> 00:15:17,670
sensor utility so this is this is new I

00:15:16,080 --> 00:15:19,470
don't know how many people are using it

00:15:17,670 --> 00:15:21,750
but if you load that LM sensor utility

00:15:19,470 --> 00:15:24,090
you can from user space go in and and

00:15:21,750 --> 00:15:26,450
grab all of that information and use it

00:15:24,090 --> 00:15:29,520
for whatever whatever purpose you need

00:15:26,450 --> 00:15:30,540
so we push every 10 milliseconds we're

00:15:29,520 --> 00:15:31,620
pushing some of these up and some we

00:15:30,540 --> 00:15:33,330
push up every 10 that are really

00:15:31,620 --> 00:15:34,440
important but we make sure that all of

00:15:33,330 --> 00:15:36,150
them get updated every hundred

00:15:34,440 --> 00:15:38,340
milliseconds so that's really not that

00:15:36,150 --> 00:15:39,930
fast right I mean it it's it's not as

00:15:38,340 --> 00:15:41,550
it's not as good as I'm going to show

00:15:39,930 --> 00:15:45,270
you that aimster is but that's that's

00:15:41,550 --> 00:15:47,300
decent for profiling you know some jobs

00:15:45,270 --> 00:15:49,320
and if you want a longer period of time

00:15:47,300 --> 00:15:52,200
certainly if you're doing something over

00:15:49,320 --> 00:15:54,410
seconds large jobs this is a decent way

00:15:52,200 --> 00:15:54,410
to go

00:15:59,810 --> 00:16:05,610
Alissa's name student's been around a

00:16:01,620 --> 00:16:07,920
long time it is open source and people

00:16:05,610 --> 00:16:10,680
use it again to to profile particulars

00:16:07,920 --> 00:16:11,160
as it grabs the data and it graphs it

00:16:10,680 --> 00:16:14,040
for you

00:16:11,160 --> 00:16:16,770
so you what you do is you tell the OCC

00:16:14,040 --> 00:16:18,360
hey I want you to I want you to measure

00:16:16,770 --> 00:16:19,920
something and you can get every single

00:16:18,360 --> 00:16:21,540
measurement so we read every 500

00:16:19,920 --> 00:16:23,250
microseconds we're reading those powers

00:16:21,540 --> 00:16:24,600
we can get all of those in a buffer and

00:16:23,250 --> 00:16:26,640
then you come back and grab that data

00:16:24,600 --> 00:16:29,220
out of that buffer and then you can you

00:16:26,640 --> 00:16:31,550
can display it or store it away use it

00:16:29,220 --> 00:16:35,490
for some other some other purpose it's

00:16:31,550 --> 00:16:37,680
it's pretty handy but again it's kind of

00:16:35,490 --> 00:16:40,860
kind of for a shorter shorter period of

00:16:37,680 --> 00:16:42,810
time like just a you know a second or

00:16:40,860 --> 00:16:48,060
two it's not gonna just be streaming

00:16:42,810 --> 00:16:50,340
Dave data for you but this new crass D

00:16:48,060 --> 00:16:53,550
service stands for cluster Rass service

00:16:50,340 --> 00:16:55,050
daemon this will do that again this is

00:16:53,550 --> 00:16:56,940
the same picture now of your if your

00:16:55,050 --> 00:16:59,190
compute no acute nope compute no at BMC

00:16:56,940 --> 00:17:01,440
OCC and everything the BMC is is

00:16:59,190 --> 00:17:03,060
gathering these these sensors maybe it's

00:17:01,440 --> 00:17:04,800
from the LCC maybe it's there gathering

00:17:03,060 --> 00:17:07,589
on their own like from fans voltage

00:17:04,800 --> 00:17:10,020
regulators and if that if that value

00:17:07,589 --> 00:17:13,410
changes then every second it would it

00:17:10,020 --> 00:17:14,850
will push that data out to crash D crash

00:17:13,410 --> 00:17:17,130
D then ok well I'll give you the use

00:17:14,850 --> 00:17:19,370
case for this those Oakridge said what I

00:17:17,130 --> 00:17:21,780
want to do is I want to be able to

00:17:19,370 --> 00:17:23,490
control my pumps layout they have a lot

00:17:21,780 --> 00:17:24,990
of chillers and pumps on these chillers

00:17:23,490 --> 00:17:27,180
and they want to have all this power

00:17:24,990 --> 00:17:29,670
information slain could be could develop

00:17:27,180 --> 00:17:31,650
fancy new more efficient ways to control

00:17:29,670 --> 00:17:34,590
the pumps that's really why we did this

00:17:31,650 --> 00:17:35,910
in the first place so we push the state

00:17:34,590 --> 00:17:38,220
up and the crash D service and allows

00:17:35,910 --> 00:17:40,260
other users to register for that data

00:17:38,220 --> 00:17:41,550
and so that's what they do at Oakridge

00:17:40,260 --> 00:17:43,560
they have another know that registers

00:17:41,550 --> 00:17:46,830
for that data and then they their their

00:17:43,560 --> 00:17:48,990
graphing it and figuring out what how to

00:17:46,830 --> 00:17:51,990
control their pumps and using it someone

00:17:48,990 --> 00:17:54,180
is a proxy for utilization as well then

00:17:51,990 --> 00:17:55,620
the Craske service as well we haven't

00:17:54,180 --> 00:17:58,170
done got this done yet we're working on

00:17:55,620 --> 00:18:00,630
it we plan to push this data into this

00:17:58,170 --> 00:18:02,700
big data store later at that Oakridge so

00:18:00,630 --> 00:18:04,560
it will have a history of all that all

00:18:02,700 --> 00:18:06,600
of that data we can mine it later

00:18:04,560 --> 00:18:08,220
and you'll notice CSM that's called

00:18:06,600 --> 00:18:10,530
classic sense for clustered Systems

00:18:08,220 --> 00:18:11,309
Manager that's running at Oakridge here

00:18:10,530 --> 00:18:12,840
that we push

00:18:11,309 --> 00:18:14,340
grab a bunch of that data in band this

00:18:12,840 --> 00:18:15,960
is the in band path and also that's

00:18:14,340 --> 00:18:17,129
getting put in big data store so we

00:18:15,960 --> 00:18:20,970
should have a really good rich set of

00:18:17,129 --> 00:18:23,639
data there at at these National Labs now

00:18:20,970 --> 00:18:27,809
one thing the Lladro guys you guys were

00:18:23,639 --> 00:18:30,809
asking does this also do air logs and it

00:18:27,809 --> 00:18:33,029
does and how that works is if the BMC

00:18:30,809 --> 00:18:35,700
has an air log that something broke a

00:18:33,029 --> 00:18:38,370
fan power supply it will then push that

00:18:35,700 --> 00:18:40,169
data you'll send an alert and the crafty

00:18:38,370 --> 00:18:44,759
service will come down and grab the the

00:18:40,169 --> 00:18:47,460
log then inside of here there's a like a

00:18:44,759 --> 00:18:49,440
parser at a policy table and so the

00:18:47,460 --> 00:18:50,970
crassius service will will parse it and

00:18:49,440 --> 00:18:54,029
figure out what is really wrong because

00:18:50,970 --> 00:18:56,279
you'd said it's difficult to figure out

00:18:54,029 --> 00:18:59,129
what the what shrew is is bad right if

00:18:56,279 --> 00:19:01,080
the CPU it's dim crafty does that for

00:18:59,129 --> 00:19:03,539
you so it'll it'll figure out what the

00:19:01,080 --> 00:19:04,860
fru-u-- is and then it figures out via

00:19:03,539 --> 00:19:05,369
this policy table what should I do about

00:19:04,860 --> 00:19:07,409
that

00:19:05,369 --> 00:19:09,840
so at Oakridge they want to do a couple

00:19:07,409 --> 00:19:11,279
things maybe they want to take this take

00:19:09,840 --> 00:19:12,570
this node knock it out they don't want

00:19:11,279 --> 00:19:14,940
they do they want to take it offline

00:19:12,570 --> 00:19:16,559
maybe they want to say let's keep

00:19:14,940 --> 00:19:18,360
running that and then next service

00:19:16,559 --> 00:19:20,850
window will or maybe they want to power

00:19:18,360 --> 00:19:22,950
it down so there's a bunch of policies

00:19:20,850 --> 00:19:24,330
here that you can set to say where does

00:19:22,950 --> 00:19:26,399
this information go or should it go to

00:19:24,330 --> 00:19:30,360
the sysadmin should I go to the service

00:19:26,399 --> 00:19:31,529
guy and that is against policy table so

00:19:30,360 --> 00:19:33,450
if you want and this is all open source

00:19:31,529 --> 00:19:34,769
so if you wanted to take it and run

00:19:33,450 --> 00:19:36,659
crash D and a service node in one of

00:19:34,769 --> 00:19:42,019
your one of your clusters it would you

00:19:36,659 --> 00:19:42,019
would do that for you and decode it so

00:19:45,720 --> 00:19:51,550
no totally different so aimster is a

00:19:49,900 --> 00:19:53,800
masseur is actually pulled to go back to

00:19:51,550 --> 00:19:55,800
this so I'm sure is sitting here and

00:19:53,800 --> 00:19:58,090
passing something of the BMC and then

00:19:55,800 --> 00:20:00,040
saying hey do you gap you have my data

00:19:58,090 --> 00:20:01,660
so it's all poles interfaces all three

00:20:00,040 --> 00:20:03,400
of Louis this the new thing about this

00:20:01,660 --> 00:20:05,650
crafty is it's a website and it's pushed

00:20:03,400 --> 00:20:07,660
so the BMC is just push in that data

00:20:05,650 --> 00:20:10,750
which is why it's so much faster and we

00:20:07,660 --> 00:20:12,280
we were able to scale this up to three

00:20:10,750 --> 00:20:13,720
hundred nodes so you have one service

00:20:12,280 --> 00:20:15,670
node to three hundred nodes and it can

00:20:13,720 --> 00:20:17,800
continually stream from all 300 of these

00:20:15,670 --> 00:20:27,610
nodes and that and have minimal impact

00:20:17,800 --> 00:20:29,700
on the on the network this know that so

00:20:27,610 --> 00:20:31,990
this web socket is not a standardized

00:20:29,700 --> 00:20:33,760
web socket standard but the daily goes

00:20:31,990 --> 00:20:37,450
over the web socket is not standardized

00:20:33,760 --> 00:20:39,250
the on aim stir it used to be I PMI we

00:20:37,450 --> 00:20:40,870
changed it to a rest-based command it's

00:20:39,250 --> 00:20:42,250
not it's also not a standard command

00:20:40,870 --> 00:20:44,080
it's rest but it's our proprietary

00:20:42,250 --> 00:20:46,150
rescue mode and we're going to move that

00:20:44,080 --> 00:20:51,190
to the redfish eventually once we get

00:20:46,150 --> 00:20:53,940
there so you guys any other other

00:20:51,190 --> 00:20:53,940
questions on this

00:21:00,510 --> 00:21:10,059
take it yeah there's this purple stuff

00:21:07,419 --> 00:21:12,669
here yeah that's so that's that's the in

00:21:10,059 --> 00:21:14,020
band I was talking about so so the OCC

00:21:12,669 --> 00:21:16,690
is pushing that those up to the hole so

00:21:14,020 --> 00:21:18,190
ask CSM is using the LM sensor command

00:21:16,690 --> 00:21:19,720
essentially actually give them a special

00:21:18,190 --> 00:21:21,850
one but uses that's those same

00:21:19,720 --> 00:21:23,200
interfaces to grab that data that's the

00:21:21,850 --> 00:21:27,490
in band stuff and then they're pushing

00:21:23,200 --> 00:21:29,590
it into big sometimes sometimes it's the

00:21:27,490 --> 00:21:31,780
same that we are duplicating some of it

00:21:29,590 --> 00:21:33,610
but sometimes it's it's not because we

00:21:31,780 --> 00:21:35,320
don't present the same information on

00:21:33,610 --> 00:21:39,039
both of these interfaces just because we

00:21:35,320 --> 00:21:40,150
we don't have or we don't we have we

00:21:39,039 --> 00:21:42,159
have kind of have a bottleneck here

00:21:40,150 --> 00:21:43,929
between the OCC and the BMC so we're not

00:21:42,159 --> 00:21:46,059
passing all the information that the OCC

00:21:43,929 --> 00:21:48,340
has does not get passed to the BMC it's

00:21:46,059 --> 00:21:50,380
a limited set it's mostly power and

00:21:48,340 --> 00:21:52,240
thermal it's not utilization data for

00:21:50,380 --> 00:21:54,010
instance but from here we don't have any

00:21:52,240 --> 00:21:56,380
we don't have as much of a limit so from

00:21:54,010 --> 00:21:59,010
from from the LCC up in the main memory

00:21:56,380 --> 00:22:01,419
there we put utilization information

00:21:59,010 --> 00:22:02,559
like throttle counts there's a whole

00:22:01,419 --> 00:22:04,510
bunch of things information about the

00:22:02,559 --> 00:22:07,210
health of your system that goes up this

00:22:04,510 --> 00:22:09,760
way and we want to enhance this or

00:22:07,210 --> 00:22:10,780
figure out a way to you know like next

00:22:09,760 --> 00:22:12,580
steps would be getting figure out a way

00:22:10,780 --> 00:22:13,960
to configure there so the user can say

00:22:12,580 --> 00:22:15,970
what is the data that I actually want to

00:22:13,960 --> 00:22:17,980
see on that on that interface but we

00:22:15,970 --> 00:22:19,809
don't have that done yet but so it's

00:22:17,980 --> 00:22:22,630
someone tell you so again sometimes it

00:22:19,809 --> 00:22:26,880
is duplicated but not all of it

00:22:22,630 --> 00:22:26,880
something that's different yeah

00:22:29,500 --> 00:22:31,530
Oh

00:22:34,240 --> 00:22:37,920
the whole series

00:22:38,539 --> 00:22:44,570
yeah no current yeah currently you would

00:22:42,079 --> 00:22:46,489
a if you wanted to run crass D on your

00:22:44,570 --> 00:22:48,289
actual processor that you're running in

00:22:46,489 --> 00:22:49,699
yeah you would have to go loop out a

00:22:48,289 --> 00:22:54,219
band today you would have to go out

00:22:49,699 --> 00:22:54,219
Ethernet back in there's no internal

00:23:01,089 --> 00:23:07,159
what we do I mean we do it tend to make

00:23:05,029 --> 00:23:09,409
you move this WebSocket to something

00:23:07,159 --> 00:23:11,059
like red fish hey we want to put into

00:23:09,409 --> 00:23:13,849
the redfish standard some way to push

00:23:11,059 --> 00:23:16,129
this data and and the guys will create

00:23:13,849 --> 00:23:17,149
so we've kind of work on this as a whole

00:23:16,129 --> 00:23:19,909
consortium which I'll talk about a

00:23:17,149 --> 00:23:22,489
minute he says you can push it and using

00:23:19,909 --> 00:23:25,069
redfish and what he what they do is they

00:23:22,489 --> 00:23:26,959
encode that breakfast is JSON so it's

00:23:25,069 --> 00:23:28,909
really verbose right so what they do is

00:23:26,959 --> 00:23:30,649
they encrypt it if you will pack it and

00:23:28,909 --> 00:23:31,699
then unpack it at the other side so

00:23:30,649 --> 00:23:34,789
we're looking into that we don't really

00:23:31,699 --> 00:23:36,409
want this to be a proprietary interface

00:23:34,789 --> 00:23:38,269
we just did it cuz if you know you just

00:23:36,409 --> 00:23:40,129
started an okra bread fish wasn't ready

00:23:38,269 --> 00:23:42,139
we didn't have anything else but it also

00:23:40,129 --> 00:23:43,609
does need to be really streamlined I

00:23:42,139 --> 00:23:46,279
mean it's like wait if we were gonna

00:23:43,609 --> 00:23:50,449
scale us to 300 plus nodes I mean you

00:23:46,279 --> 00:23:52,969
can't be sending like a string of of you

00:23:50,449 --> 00:23:54,829
know 200 bytes to sit to get one piece

00:23:52,969 --> 00:23:58,190
of information through ours it's gonna

00:23:54,829 --> 00:23:59,719
crash the network so we're looking at

00:23:58,190 --> 00:24:04,519
that and if we if we did that when we go

00:23:59,719 --> 00:24:06,229
in band to redfish as well see it

00:24:04,519 --> 00:24:10,519
so within band redfish you have to have

00:24:06,229 --> 00:24:12,109
the whole HTTP server and we as we go

00:24:10,519 --> 00:24:14,419
forward into the next generation we're

00:24:12,109 --> 00:24:16,879
not intending to support even that type

00:24:14,419 --> 00:24:20,749
of an interface we're going mctv and PL

00:24:16,879 --> 00:24:22,669
DM over MC TP to talk from the BMC to

00:24:20,749 --> 00:24:25,089
the hosts so they probably won't do

00:24:22,669 --> 00:24:25,089
right fish

00:24:27,370 --> 00:24:45,760
okay now do I have an actual actual what

00:24:38,900 --> 00:24:45,760
happened there okay

00:24:48,180 --> 00:24:50,990
that run

00:25:01,629 --> 00:25:08,889
so what I got to do you know so I can't

00:25:07,249 --> 00:25:14,389
I'm displaying but it's not showing up

00:25:08,889 --> 00:25:17,929
I'm student what if I do extend or

00:25:14,389 --> 00:25:32,659
something oh you want to see that

00:25:17,929 --> 00:25:44,049
hanging yeah she oh no I lost my cursor

00:25:32,659 --> 00:25:44,049
- this is awful sorry about that guys

00:25:54,309 --> 00:25:57,090
okay

00:25:58,050 --> 00:26:05,730
there we go it's not really showing much

00:25:59,850 --> 00:26:08,730
in tourism is it moving okay so this is

00:26:05,730 --> 00:26:11,130
that what the upper looks like and this

00:26:08,730 --> 00:26:12,780
is not at at Oakridge they did it just

00:26:11,130 --> 00:26:14,400
slightly differently but this is how

00:26:12,780 --> 00:26:16,560
we're doing it on our own system so this

00:26:14,400 --> 00:26:17,310
is really running on a on one of our 89

00:26:16,560 --> 00:26:19,380
22s

00:26:17,310 --> 00:26:20,970
so you can see that you know the fan tax

00:26:19,380 --> 00:26:23,160
there's not a lot going on I'd hope to

00:26:20,970 --> 00:26:24,570
pick a note that had some some good

00:26:23,160 --> 00:26:27,420
action going on up at this is us would

00:26:24,570 --> 00:26:28,950
be sitting at idle but you've got their

00:26:27,420 --> 00:26:30,750
dual rotor fans for instance and one

00:26:28,950 --> 00:26:34,530
runs a little faster than the other so

00:26:30,750 --> 00:26:37,250
you can see that your fan tach is pretty

00:26:34,530 --> 00:26:40,380
stable but separated by a few uh few

00:26:37,250 --> 00:26:42,090
hundred rpm you've got your power supply

00:26:40,380 --> 00:26:43,800
powers and then I talked about the

00:26:42,090 --> 00:26:46,440
powers and all those rails and you can

00:26:43,800 --> 00:26:48,600
zoom in on this and isolate it down to a

00:26:46,440 --> 00:26:50,280
tour you can see better what the what

00:26:48,600 --> 00:26:51,540
the actual numbers are you get

00:26:50,280 --> 00:26:52,890
temperatures look like look at all those

00:26:51,540 --> 00:26:54,990
temperatures you get your dims and your

00:26:52,890 --> 00:26:56,970
CPU sockets and all of your temps and

00:26:54,990 --> 00:26:58,620
when you when you watch this you'll see

00:26:56,970 --> 00:27:00,270
that the power will go up and all your

00:26:58,620 --> 00:27:02,340
temps will go up and the power going on

00:27:00,270 --> 00:27:07,430
temps will go down and you can store

00:27:02,340 --> 00:27:11,520
this away in you know file and and

00:27:07,430 --> 00:27:15,300
analyze it later so it's yeah I think

00:27:11,520 --> 00:27:17,390
it's pretty cool now let's get back to

00:27:15,300 --> 00:27:24,530
the other presentation

00:27:17,390 --> 00:27:24,530
Lynch okay just drag this over here

00:27:26,420 --> 00:27:38,810
can I go all right perfect

00:27:33,430 --> 00:27:41,630
all right next is a new initiative out

00:27:38,810 --> 00:27:44,090
there called HPC power stack and again

00:27:41,630 --> 00:27:47,480
it's its power as in energy it's a it's

00:27:44,090 --> 00:27:51,650
an industry-wide and global consortium

00:27:47,480 --> 00:27:55,280
of people so it's it's IBM its Intel HP

00:27:51,650 --> 00:27:58,430
Cray arm AMD and then a whole bunch of

00:27:55,280 --> 00:28:01,790
these centers like you know I don't

00:27:58,430 --> 00:28:04,430
Barcelona supercomputing and then in

00:28:01,790 --> 00:28:05,900
universities so lrz and a bunch of

00:28:04,430 --> 00:28:07,640
universities and we're all getting

00:28:05,900 --> 00:28:08,960
together week twice a year and we run

00:28:07,640 --> 00:28:11,090
these workgroups and we're trying to

00:28:08,960 --> 00:28:13,880
solve the problems that are gonna be

00:28:11,090 --> 00:28:15,070
coming with excess scale and so that we

00:28:13,880 --> 00:28:17,840
think that there's going to be a

00:28:15,070 --> 00:28:20,840
significant power problem when you start

00:28:17,840 --> 00:28:23,090
scaling up to such a such a large system

00:28:20,840 --> 00:28:25,760
so maybe like after coral that you know

00:28:23,090 --> 00:28:27,860
the next the next system and they're

00:28:25,760 --> 00:28:29,600
there so the motivation here is you've

00:28:27,860 --> 00:28:31,370
got you have power constraints due to

00:28:29,600 --> 00:28:34,670
sometimes external factors like at

00:28:31,370 --> 00:28:35,540
Sandia they don't have enough they don't

00:28:34,670 --> 00:28:37,370
think they have even to have enough

00:28:35,540 --> 00:28:39,710
power so they're gonna have to be some

00:28:37,370 --> 00:28:41,540
power limits set for the for the cluster

00:28:39,710 --> 00:28:43,130
and we think about these clusters you

00:28:41,540 --> 00:28:45,890
think about you have CPUs and GPUs and

00:28:43,130 --> 00:28:48,290
how do you distribute this power in the

00:28:45,890 --> 00:28:49,700
best way to get it is as efficient as

00:28:48,290 --> 00:28:51,860
you can you want to maximize the

00:28:49,700 --> 00:28:53,060
sufficiency of your entire cluster so

00:28:51,860 --> 00:28:54,380
it's going to take interaction with you

00:28:53,060 --> 00:28:55,790
in all these various aspects of the

00:28:54,380 --> 00:28:58,850
stack and I'll show you that the picture

00:28:55,790 --> 00:29:00,260
here at the end there also you know our

00:28:58,850 --> 00:29:01,280
government mandates say you can't use

00:29:00,260 --> 00:29:03,470
them more than a certain amount of power

00:29:01,280 --> 00:29:05,480
there's there's there are times when

00:29:03,470 --> 00:29:08,420
power is really expensive

00:29:05,480 --> 00:29:09,920
so in like at Livermore their periods of

00:29:08,420 --> 00:29:11,120
time where they if they use more than

00:29:09,920 --> 00:29:13,700
that much power they're gonna completely

00:29:11,120 --> 00:29:15,800
blow their their budget for the year and

00:29:13,700 --> 00:29:17,690
there's power is very expensive at times

00:29:15,800 --> 00:29:19,220
so you have to be able to set a power

00:29:17,690 --> 00:29:21,430
limit and you still wanna operate as

00:29:19,220 --> 00:29:23,390
efficiently as possible under that limit

00:29:21,430 --> 00:29:26,030
we want to be on the predictive power

00:29:23,390 --> 00:29:27,830
consumption of the future systems large

00:29:26,030 --> 00:29:29,570
systems so if you know it if you can

00:29:27,830 --> 00:29:32,450
tell here's how much power I'm actually

00:29:29,570 --> 00:29:34,100
using on a CPU or GPU you can use that

00:29:32,450 --> 00:29:35,240
to predict and we build the next system

00:29:34,100 --> 00:29:36,590
what kind of workloads are we going to

00:29:35,240 --> 00:29:37,080
be running and therefore what is our

00:29:36,590 --> 00:29:37,860
power cancer

00:29:37,080 --> 00:29:39,840
you're gonna look like which because

00:29:37,860 --> 00:29:41,400
going to help the labs and anybody

00:29:39,840 --> 00:29:43,530
that's building a data center provision

00:29:41,400 --> 00:29:45,360
that Center correctly and that

00:29:43,530 --> 00:29:48,720
over-provision it which is a big problem

00:29:45,360 --> 00:29:50,160
today and then one other one there's

00:29:48,720 --> 00:29:51,990
this education the validation of end

00:29:50,160 --> 00:29:54,600
users some people when you write a set

00:29:51,990 --> 00:29:56,070
of code it may be very power efficient

00:29:54,600 --> 00:29:57,900
or maybe not and we want to at least be

00:29:56,070 --> 00:29:59,940
able to tell whoever wrote that that

00:29:57,900 --> 00:30:01,530
that really wasn't wasn't a very power

00:29:59,940 --> 00:30:03,420
efficient chunk of code so that they can

00:30:01,530 --> 00:30:04,560
make changes and we're talking about why

00:30:03,420 --> 00:30:07,740
we're gonna how are we going to

00:30:04,560 --> 00:30:12,630
incentivize them in some way and maybe

00:30:07,740 --> 00:30:13,830
maybe we will will do that so we can

00:30:12,630 --> 00:30:15,210
open fist this if the impacts of

00:30:13,830 --> 00:30:17,670
exceeding a power limit if you think

00:30:15,210 --> 00:30:19,290
about you know what's gonna happen there

00:30:17,670 --> 00:30:20,790
at first you might just you might not be

00:30:19,290 --> 00:30:24,840
as efficient as you want to be so you

00:30:20,790 --> 00:30:26,280
may be maybe your well I guess that's

00:30:24,840 --> 00:30:27,690
self-explanatory maybe you came to run

00:30:26,280 --> 00:30:30,210
the jobs you don't have enough power and

00:30:27,690 --> 00:30:31,830
maybe unexpected cost I talked about and

00:30:30,210 --> 00:30:32,790
as you get down here now you get the

00:30:31,830 --> 00:30:33,240
point or maybe you're gonna flip a

00:30:32,790 --> 00:30:34,950
breaker

00:30:33,240 --> 00:30:37,530
maybe you're gonna physically harm the

00:30:34,950 --> 00:30:39,000
system and these down here I really in

00:30:37,530 --> 00:30:41,640
my mind are gonna be have to be done by

00:30:39,000 --> 00:30:43,350
like the OCC on the node some of these

00:30:41,640 --> 00:30:45,540
as you go up the stack can be done by a

00:30:43,350 --> 00:30:47,430
higher level layer of software that can

00:30:45,540 --> 00:30:48,540
do a better job of making sure

00:30:47,430 --> 00:30:51,600
everything's efficient across the

00:30:48,540 --> 00:30:54,060
cluster and but not not necessarily keep

00:30:51,600 --> 00:30:57,200
it under the limit if it's a hard limit

00:30:54,060 --> 00:30:57,200
like a like the bottom two

00:30:58,570 --> 00:31:04,840
but this is a picture of some real data

00:31:01,600 --> 00:31:09,160
or it's from Lu Zhi and Q really quite a

00:31:04,840 --> 00:31:10,570
while ago can you notice that there's at

00:31:09,160 --> 00:31:13,600
the beginning this isn't months here on

00:31:10,570 --> 00:31:16,480
the on the bottom so in February 2013

00:31:13,600 --> 00:31:19,090
the in February March April they use a

00:31:16,480 --> 00:31:21,490
lot of power and then after that they

00:31:19,090 --> 00:31:26,650
really didn't use that much so why you

00:31:21,490 --> 00:31:31,140
know why is that why right away what's

00:31:26,650 --> 00:31:33,220
that I actually don't know where this is

00:31:31,140 --> 00:31:36,640
no I don't know this isn't this is

00:31:33,220 --> 00:31:37,960
somewhere the u.s. is blue-jean q so why

00:31:36,640 --> 00:31:40,800
for the first three months I were using

00:31:37,960 --> 00:31:40,800
this much power and then

00:31:45,070 --> 00:31:48,670
you're running Linpack is exactly why

00:31:47,410 --> 00:31:50,770
you're running Linpack you got to get

00:31:48,670 --> 00:31:53,530
that score you got to get on the on the

00:31:50,770 --> 00:31:55,180
sit down the supercomputing top 500 so

00:31:53,530 --> 00:31:57,130
you're on Lynn Peck do you run Lynn pack

00:31:55,180 --> 00:31:58,570
the rest of the time you never run one

00:31:57,130 --> 00:32:00,430
packet you know clean packs dumb

00:31:58,570 --> 00:32:01,690
so what other people who are running

00:32:00,430 --> 00:32:03,970
their jobs and this is how much power

00:32:01,690 --> 00:32:06,100
you you actually are consuming so you

00:32:03,970 --> 00:32:07,450
they provisioned this blue jean queue

00:32:06,100 --> 00:32:10,140
for this much so all that extra

00:32:07,450 --> 00:32:14,170
infrastructure all your all your extra

00:32:10,140 --> 00:32:16,570
cooling and regulation and it was all

00:32:14,170 --> 00:32:19,720
done for this this power level when

00:32:16,570 --> 00:32:23,080
really it was just unused unused paid

00:32:19,720 --> 00:32:24,430
for and and so they learned a lesson

00:32:23,080 --> 00:32:26,410
from that or we're trying to learn a

00:32:24,430 --> 00:32:28,810
lesson from that so what you'd really

00:32:26,410 --> 00:32:30,970
want to do is you want to just provision

00:32:28,810 --> 00:32:32,710
for this but now you can't run a link

00:32:30,970 --> 00:32:33,700
back right how do you run Linpack if

00:32:32,710 --> 00:32:37,360
you're just going to provision for this

00:32:33,700 --> 00:32:39,070
so what we're thinking is that you would

00:32:37,360 --> 00:32:40,570
be able to you might have to provision

00:32:39,070 --> 00:32:42,430
for this what I'd like to do is actually

00:32:40,570 --> 00:32:44,320
change this so that you don't have to

00:32:42,430 --> 00:32:46,060
run one second over the whole cluster

00:32:44,320 --> 00:32:47,770
and maybe you can predict it instead of

00:32:46,060 --> 00:32:49,420
instead of having to run it but I don't

00:32:47,770 --> 00:32:51,130
know if that's gonna happen but but what

00:32:49,420 --> 00:32:52,660
what you could do is you can have this

00:32:51,130 --> 00:32:53,980
provision for this much power but then

00:32:52,660 --> 00:32:55,780
know that you're gonna you're gonna

00:32:53,980 --> 00:32:57,430
shift that power to some other system

00:32:55,780 --> 00:32:59,530
some other clusters some other area

00:32:57,430 --> 00:33:01,450
inside you your data center but no

00:32:59,530 --> 00:33:02,680
matter what you're gonna need if you've

00:33:01,450 --> 00:33:04,900
only provision for this much power

00:33:02,680 --> 00:33:06,400
you're gonna need here to enforce these

00:33:04,900 --> 00:33:08,440
power gap so there's going there going

00:33:06,400 --> 00:33:12,990
to be power limits set that are going to

00:33:08,440 --> 00:33:12,990
affect the performance of the system

00:33:16,000 --> 00:33:19,060
and here's another so that's one

00:33:17,560 --> 00:33:20,950
motivation that's why that's cuz that's

00:33:19,060 --> 00:33:22,570
why we're doing this and there you saw

00:33:20,950 --> 00:33:25,540
many other motivations but this one is

00:33:22,570 --> 00:33:28,240
really interesting to me so this is an

00:33:25,540 --> 00:33:29,740
IPC so instructions per cycle here and

00:33:28,240 --> 00:33:31,300
this is the Sandy Bridge there Intel

00:33:29,740 --> 00:33:33,370
Intel process there's the Sandy Bridge

00:33:31,300 --> 00:33:35,920
and next is you know the Broadwell so

00:33:33,370 --> 00:33:38,650
going up in generations and these on

00:33:35,920 --> 00:33:39,850
here on the x-axis is power limits so

00:33:38,650 --> 00:33:41,680
when you're not power limit when you

00:33:39,850 --> 00:33:43,720
were not power limit at all on Sandy

00:33:41,680 --> 00:33:47,380
Bridge your performance was almost

00:33:43,720 --> 00:33:48,490
identical socket the socket when you hat

00:33:47,380 --> 00:33:51,040
were under a power limit it was

00:33:48,490 --> 00:33:52,330
different there was a variation in the

00:33:51,040 --> 00:33:54,520
amount of power because again you know

00:33:52,330 --> 00:33:56,110
one socket is a good one because it has

00:33:54,520 --> 00:33:58,390
low leakage and then you have one that

00:33:56,110 --> 00:34:00,190
has high leakage and so you you have

00:33:58,390 --> 00:34:02,260
differences in the power consumption at

00:34:00,190 --> 00:34:04,870
performance levels and as soon as you

00:34:02,260 --> 00:34:05,950
expose a soon as you put a power cap on

00:34:04,870 --> 00:34:08,470
that you're going to expose that

00:34:05,950 --> 00:34:10,330
performance variability but now go

00:34:08,470 --> 00:34:12,220
forward as we shrink this technology

00:34:10,330 --> 00:34:15,690
before it abroad well now it's crazy

00:34:12,220 --> 00:34:18,310
right I mean that's like significantly

00:34:15,690 --> 00:34:19,660
different performance and even when you

00:34:18,310 --> 00:34:20,800
aren't under a power limit which is that

00:34:19,660 --> 00:34:23,440
which is there you still have

00:34:20,800 --> 00:34:25,660
manufacturing variability and until does

00:34:23,440 --> 00:34:27,460
expose this so in their processors they

00:34:25,660 --> 00:34:29,830
just they say they're gonna run in some

00:34:27,460 --> 00:34:31,750
range and they do not worry about the

00:34:29,830 --> 00:34:34,630
fact that there there's non-determinism

00:34:31,750 --> 00:34:37,540
they let the business in my

00:34:34,630 --> 00:34:38,830
understanding they let HP e or whoever's

00:34:37,540 --> 00:34:40,210
building the system worried about how do

00:34:38,830 --> 00:34:44,470
they tune that system to avoid that

00:34:40,210 --> 00:34:47,350
variability so what I would like to do

00:34:44,470 --> 00:34:49,450
is okay so so what so what are we gonna

00:34:47,350 --> 00:34:51,280
do here so you would need to know that

00:34:49,450 --> 00:34:54,310
the power performance of each socket and

00:34:51,280 --> 00:34:56,290
then you'd be able to to and you know

00:34:54,310 --> 00:34:58,150
give it give it if you have a bad socket

00:34:56,290 --> 00:34:59,380
you would give it more power if you have

00:34:58,150 --> 00:35:01,350
good sake you would give it less power

00:34:59,380 --> 00:35:03,250
and that you keep that way you keep your

00:35:01,350 --> 00:35:05,440
system running as efficiently as you

00:35:03,250 --> 00:35:06,460
possibly can so you need you need to

00:35:05,440 --> 00:35:08,110
somehow know the power and performance

00:35:06,460 --> 00:35:09,490
profile the Sacketts and there was just

00:35:08,110 --> 00:35:11,200
some research done and a paper published

00:35:09,490 --> 00:35:13,660
it this sort of proved that if you do it

00:35:11,200 --> 00:35:15,520
this do that you will you will do better

00:35:13,660 --> 00:35:17,080
than any of these other algorithms the

00:35:15,520 --> 00:35:18,370
sunfish using an average power camp if

00:35:17,080 --> 00:35:19,900
you do that you're gonna you're going to

00:35:18,370 --> 00:35:22,120
cause this performance variability

00:35:19,900 --> 00:35:24,550
between these two particular sockets and

00:35:22,120 --> 00:35:27,130
you're gonna have you jitter and if you

00:35:24,550 --> 00:35:28,390
think about a big HPC system this is all

00:35:27,130 --> 00:35:29,619
about the last manhole

00:35:28,390 --> 00:35:30,910
right so you're running you're running

00:35:29,619 --> 00:35:32,500
an application everybody runs for a

00:35:30,910 --> 00:35:35,289
while and you stop and then you run for

00:35:32,500 --> 00:35:37,000
a while you stop and then they all have

00:35:35,289 --> 00:35:38,559
to wait for everybody to finish so

00:35:37,000 --> 00:35:40,930
whoever is the let the slowest notice

00:35:38,559 --> 00:35:42,640
essentially sets the speed of that of

00:35:40,930 --> 00:35:45,220
how fast that job processes it and

00:35:42,640 --> 00:35:48,430
executes so what you would want to do is

00:35:45,220 --> 00:35:50,349
maybe move slow knowns into one spot and

00:35:48,430 --> 00:35:51,910
run some running all the slow ones

00:35:50,349 --> 00:35:53,619
together from different Saboteur the

00:35:51,910 --> 00:35:57,579
fast ones and be able to maximize the

00:35:53,619 --> 00:36:00,549
efficiency that's that's one idea here

00:35:57,579 --> 00:36:03,460
is so this is a histogram so how many

00:36:00,549 --> 00:36:04,990
systems are running at that particular

00:36:03,460 --> 00:36:07,359
frequency so the spread here is about

00:36:04,990 --> 00:36:10,180
five hundred thirty five megahertz so it

00:36:07,359 --> 00:36:11,829
this is just saying if you if you just

00:36:10,180 --> 00:36:13,930
let them run you're gonna have this kind

00:36:11,829 --> 00:36:15,789
of variability between your nodes and

00:36:13,930 --> 00:36:17,799
this does happen this happens on GPUs

00:36:15,789 --> 00:36:21,760
especially have these GPUs of a wide

00:36:17,799 --> 00:36:24,490
variety of performance levels and

00:36:21,760 --> 00:36:26,859
powerful and powerful for performance so

00:36:24,490 --> 00:36:29,200
at at Oakridge and these places they

00:36:26,859 --> 00:36:30,640
have to take the sum of these nodes that

00:36:29,200 --> 00:36:32,559
are slow ones and kick them out

00:36:30,640 --> 00:36:34,660
essentially maybe they have slow GPUs so

00:36:32,559 --> 00:36:36,880
they they do have to have to worry about

00:36:34,660 --> 00:36:37,809
this so I don't think this problem is

00:36:36,880 --> 00:36:39,730
going away they're gonna have this

00:36:37,809 --> 00:36:40,930
manufacturing variability so I think we

00:36:39,730 --> 00:36:42,910
need a way to solve it and make

00:36:40,930 --> 00:36:46,150
efficient clusters and by the way this

00:36:42,910 --> 00:36:48,519
doesn't just apply to HPC I mean I think

00:36:46,150 --> 00:36:50,529
the prod the problem is is the same in a

00:36:48,519 --> 00:36:55,509
cloud in a massive cloud you're gonna

00:36:50,529 --> 00:36:58,000
have the same types of problems so what

00:36:55,509 --> 00:37:00,099
is gonna solve this or we're thinking uh

00:36:58,000 --> 00:37:03,700
solve it something called G OPM this is

00:37:00,099 --> 00:37:09,190
an open source again open source set of

00:37:03,700 --> 00:37:10,509
code and its job is to to do this

00:37:09,190 --> 00:37:12,970
balancing or at least help with the

00:37:10,509 --> 00:37:14,589
balancing it Intel originally did it

00:37:12,970 --> 00:37:17,380
they wrote it we have ported it to power

00:37:14,589 --> 00:37:20,920
so it is running on our our power

00:37:17,380 --> 00:37:22,480
systems that's at Livermore and we're

00:37:20,920 --> 00:37:24,400
experimenting with with how to make this

00:37:22,480 --> 00:37:26,380
work so this is one thing this HPC power

00:37:24,400 --> 00:37:28,900
stack is trying to do is we're trying to

00:37:26,380 --> 00:37:30,789
like use G OPM figure out some

00:37:28,900 --> 00:37:31,269
algorithms to be able to to solve this

00:37:30,789 --> 00:37:33,640
problem

00:37:31,269 --> 00:37:36,460
well G OPM would run if you if you look

00:37:33,640 --> 00:37:38,019
at think of the AC 922 and the sock and

00:37:36,460 --> 00:37:40,000
I showed you right it would run on one

00:37:38,019 --> 00:37:40,910
of those cores and what we do at

00:37:40,000 --> 00:37:42,829
Oakridge is we

00:37:40,910 --> 00:37:44,059
we actually do use core isolation so

00:37:42,829 --> 00:37:45,380
you'll take a couple of course anyway

00:37:44,059 --> 00:37:46,579
off and they are just doing

00:37:45,380 --> 00:37:47,779
administrative tasks they aren't running

00:37:46,579 --> 00:37:49,430
customer workloads

00:37:47,779 --> 00:37:52,099
so you would run this on one of those

00:37:49,430 --> 00:37:54,440
isolated cores and that would and it

00:37:52,099 --> 00:37:58,279
would collect power and performance

00:37:54,440 --> 00:38:00,049
information then from the OCC and they

00:37:58,279 --> 00:38:01,880
would it would bill and all I see I see

00:38:00,049 --> 00:38:03,769
the job that's running I see its

00:38:01,880 --> 00:38:07,779
performance characteristics it's a hot

00:38:03,769 --> 00:38:11,599
job and I can't I also see that this

00:38:07,779 --> 00:38:13,190
socket is a good one or a bad one and it

00:38:11,599 --> 00:38:15,019
can do this characterization and then be

00:38:13,190 --> 00:38:17,119
able to balance and it's hierarchical

00:38:15,019 --> 00:38:19,309
it's scales so you would have one of

00:38:17,119 --> 00:38:22,519
those cars running that instance of GOP

00:38:19,309 --> 00:38:24,589
m and then the other car would or the

00:38:22,519 --> 00:38:27,440
other socket would also be running an

00:38:24,589 --> 00:38:30,200
instance then there would be an instance

00:38:27,440 --> 00:38:32,390
up here at this level also running on

00:38:30,200 --> 00:38:34,759
that same another instance of GOP I'm

00:38:32,390 --> 00:38:36,980
also running on that same car so that in

00:38:34,759 --> 00:38:38,180
that way you'll have one master G OPM

00:38:36,980 --> 00:38:42,589
instance running somewhere that has a

00:38:38,180 --> 00:38:47,720
view of the entire system just make any

00:38:42,589 --> 00:38:49,160
sense yeah and here's how we're gonna we

00:38:47,720 --> 00:38:51,049
are going to include it there's two

00:38:49,160 --> 00:38:52,400
different ways we could run we could

00:38:51,049 --> 00:38:55,099
change the colonel up with the GOP M

00:38:52,400 --> 00:38:56,990
server in here or a direct interface to

00:38:55,099 --> 00:38:58,700
Opel this is really what I think we want

00:38:56,990 --> 00:39:01,279
to go that's not done yet so we would

00:38:58,700 --> 00:39:04,549
GOP M would be running analytics guest L

00:39:01,279 --> 00:39:05,930
bar run talks to Opel and then it gets

00:39:04,549 --> 00:39:13,819
all this information then from the the

00:39:05,930 --> 00:39:16,579
OCC I'm good almost done then so this is

00:39:13,819 --> 00:39:18,740
the outcome of the of the HPC power

00:39:16,579 --> 00:39:20,690
stack well there's many more outcomes

00:39:18,740 --> 00:39:22,220
but this is the big picture of how all

00:39:20,690 --> 00:39:24,440
the stuff fits together every time we

00:39:22,220 --> 00:39:28,250
meet I think we had two more lines going

00:39:24,440 --> 00:39:29,660
from one spot to another but we you can

00:39:28,250 --> 00:39:33,490
see that you would have to have policies

00:39:29,660 --> 00:39:35,750
in so that the admin would have to know

00:39:33,490 --> 00:39:36,950
you know what it like what am what did

00:39:35,750 --> 00:39:38,299
what am I trying we're trying to limit

00:39:36,950 --> 00:39:40,250
my power no because I'm gonna in a

00:39:38,299 --> 00:39:43,190
situation where I don't where I'm gonna

00:39:40,250 --> 00:39:45,529
be charged more so there's those kinds

00:39:43,190 --> 00:39:46,400
of policies and then you have you're

00:39:45,529 --> 00:39:48,019
gonna have to have maybe some

00:39:46,400 --> 00:39:49,970
information from your mechanical system

00:39:48,019 --> 00:39:52,670
that says hey you know I lost a chiller

00:39:49,970 --> 00:39:53,769
or whatever and now I need to set a

00:39:52,670 --> 00:39:56,109
lower

00:39:53,769 --> 00:39:58,359
this is the GOP mo is pointing out and

00:39:56,109 --> 00:39:59,769
it's talking here like in band so it

00:39:58,359 --> 00:40:01,059
would talk to the OCC and get that

00:39:59,769 --> 00:40:03,849
information but there needs to be then

00:40:01,059 --> 00:40:06,430
some interfaces back to the jobscheduler

00:40:03,849 --> 00:40:08,380
job schedule resource manager to know

00:40:06,430 --> 00:40:10,660
hey I'm running this particular job G

00:40:08,380 --> 00:40:13,329
OPM says it took this much this much

00:40:10,660 --> 00:40:14,380
energy and and the and then you would

00:40:13,329 --> 00:40:15,999
need to like store that away so

00:40:14,380 --> 00:40:17,979
therefore the next time you run that job

00:40:15,999 --> 00:40:20,200
you would know how much energy that it

00:40:17,979 --> 00:40:22,569
is going to take and now you can fit

00:40:20,200 --> 00:40:24,609
these jobs in in the most efficient way

00:40:22,569 --> 00:40:26,229
into your system is you know that the

00:40:24,609 --> 00:40:29,410
the energy they're going to take and

00:40:26,229 --> 00:40:32,640
that you can fit under that limit in the

00:40:29,410 --> 00:40:34,359
most efficient fashion anyway this is

00:40:32,640 --> 00:40:36,309
something where can I get it's open

00:40:34,359 --> 00:40:38,470
source you guys can go look at it and

00:40:36,309 --> 00:40:43,599
play with it and help us help us solve

00:40:38,470 --> 00:40:46,229
this problem that's it anybody have any

00:40:43,599 --> 00:40:46,229
other questions

00:41:06,040 --> 00:41:10,880
so GPUs have their own throttling so

00:41:09,380 --> 00:41:12,230
they're running just like the OCC is

00:41:10,880 --> 00:41:13,730
running they're running and they're

00:41:12,230 --> 00:41:16,400
they're doing their own temperature

00:41:13,730 --> 00:41:18,230
control they're doing their own if you

00:41:16,400 --> 00:41:19,910
put it in a mode for energy efficiency

00:41:18,230 --> 00:41:21,560
they're lowering their voltage and

00:41:19,910 --> 00:41:24,530
frequency as well when they're not being

00:41:21,560 --> 00:41:26,510
used but in addition to that we we have

00:41:24,530 --> 00:41:29,930
an out-of-band interface this is an RS

00:41:26,510 --> 00:41:32,060
XM to connected GPUs NVIDIA GPUs on the

00:41:29,930 --> 00:41:34,700
AC 922 we have a special on abandoner

00:41:32,060 --> 00:41:36,800
fish from the OCC to the GPUs that I had

00:41:34,700 --> 00:41:38,290
them add and on that interface I can set

00:41:36,800 --> 00:41:41,480
power limits and I can get temperatures

00:41:38,290 --> 00:41:46,240
so we are the OCC is doing some active

00:41:41,480 --> 00:41:48,950
balancing between the the CPU and GPU it

00:41:46,240 --> 00:41:50,270
um I'd have to have to give you more

00:41:48,950 --> 00:41:52,100
details on how it works but there is

00:41:50,270 --> 00:41:53,840
definitely an interface there where we

00:41:52,100 --> 00:41:55,760
can set up our limit so when you if you

00:41:53,840 --> 00:41:56,840
would lose a power supply say and all

00:41:55,760 --> 00:41:57,290
the sudden you have to cut your power in

00:41:56,840 --> 00:41:59,480
half

00:41:57,290 --> 00:42:01,250
the OCC says well I'm gonna lower my

00:41:59,480 --> 00:42:03,830
power and then we give a new power limit

00:42:01,250 --> 00:42:06,160
over to the GPU is based on whatever the

00:42:03,830 --> 00:42:08,240
system whatever the system power

00:42:06,160 --> 00:42:09,920
requirement is and then they lower the

00:42:08,240 --> 00:42:12,760
GPUs lower their power and your

00:42:09,920 --> 00:42:12,760
underneath that limit

00:42:15,579 --> 00:42:20,839
GOP m3 deficient

00:42:19,039 --> 00:42:25,210
I need to see you have to combine

00:42:20,839 --> 00:42:25,210
economic or the processors

00:42:26,300 --> 00:42:32,610
and exactly the memory subsystem

00:42:30,200 --> 00:42:34,770
absolutely the G opium is not just going

00:42:32,610 --> 00:42:35,970
to talk to the to the LCC it's going to

00:42:34,770 --> 00:42:37,350
have to get information about all those

00:42:35,970 --> 00:42:39,210
different components so it's able to

00:42:37,350 --> 00:42:41,040
balance so at one level we'd want to be

00:42:39,210 --> 00:42:42,600
balancing on the node one instance of G

00:42:41,040 --> 00:42:44,070
OPM but then you of course you would

00:42:42,600 --> 00:42:47,760
also want the higher level instances of

00:42:44,070 --> 00:42:50,580
GOP I'm balancing among the nodes and it

00:42:47,760 --> 00:42:53,040
on up the pajama yes the GOP n was going

00:42:50,580 --> 00:42:56,070
to have to have GPU power memory power

00:42:53,040 --> 00:42:57,180
CPU power maybe maybe even i/o power and

00:42:56,070 --> 00:42:58,770
that's that's the things we're trying to

00:42:57,180 --> 00:43:01,350
figure out this workgroup is okay what's

00:42:58,770 --> 00:43:04,740
the data we need to pass and and who

00:43:01,350 --> 00:43:05,670
know who should have that data and and

00:43:04,740 --> 00:43:07,770
then what are they going to do with it

00:43:05,670 --> 00:43:12,450
but we aren't since we're using arm

00:43:07,770 --> 00:43:14,160
Intel AMD this this G opium has plugins

00:43:12,450 --> 00:43:16,350
so we're not trying to say this is

00:43:14,160 --> 00:43:18,600
exactly the same data that's retrieved

00:43:16,350 --> 00:43:20,310
in the same way we're gonna say they'll

00:43:18,600 --> 00:43:22,320
be a plug-in to get it to get this data

00:43:20,310 --> 00:43:24,060
from the OCC different you use rattle

00:43:22,320 --> 00:43:24,990
for instance I'm Intel to get the data

00:43:24,060 --> 00:43:26,850
from an Intel chip

00:43:24,990 --> 00:43:29,730
it'll be they'll be different in your

00:43:26,850 --> 00:43:32,630
interfaces down here but want to

00:43:29,730 --> 00:43:32,630
accomplish the same thing

00:43:44,569 --> 00:43:51,150
what's the downside of not being able to

00:43:47,150 --> 00:43:53,759
but we we are good we can't from the

00:43:51,150 --> 00:44:02,609
myeloma Center you can get the sensor

00:43:53,759 --> 00:44:07,200
data to the yeah let's go back to the my

00:44:02,609 --> 00:44:08,900
in band out a venture somewhere right

00:44:07,200 --> 00:44:13,009
here oops

00:44:08,900 --> 00:44:13,009
yeah so let's see so what's the question

00:44:16,520 --> 00:44:22,609
yeah they're a bunch of senses sensor

00:44:19,470 --> 00:44:22,609
informations available this way

00:44:34,360 --> 00:44:39,500
yeah so you're talking what what if so

00:44:37,790 --> 00:44:41,900
going forward in this new world of

00:44:39,500 --> 00:44:44,930
security we do have this problem I'm

00:44:41,900 --> 00:44:47,810
pointed out that in userspace if you go

00:44:44,930 --> 00:44:49,670
here this actually runs in the standard

00:44:47,810 --> 00:44:52,970
element sensors windows utility in user

00:44:49,670 --> 00:44:54,860
space but you may not want that utility

00:44:52,970 --> 00:44:56,690
to have access to the power information

00:44:54,860 --> 00:44:58,640
on a socket on which it is not running

00:44:56,690 --> 00:45:00,980
you could you can certainly infer

00:44:58,640 --> 00:45:03,560
workloads and various things using that

00:45:00,980 --> 00:45:06,320
using that so I think you're pointing

00:45:03,560 --> 00:45:11,630
out we we need to restrict that

00:45:06,320 --> 00:45:13,750
potentially it will not be made

00:45:11,630 --> 00:45:13,750
available

00:45:20,529 --> 00:45:25,519
well if you make that information not

00:45:23,239 --> 00:45:29,900
available it will it would then not

00:45:25,519 --> 00:45:33,109
allow things like G OPM to get that

00:45:29,900 --> 00:45:37,940
information so we need a solution to

00:45:33,109 --> 00:45:40,999
that problem I mean because yeah I mean

00:45:37,940 --> 00:45:42,349
we I went into one of the topics we

00:45:40,999 --> 00:45:43,999
talked about at the station PC power

00:45:42,349 --> 00:45:46,160
status yeah but I mean we can't just

00:45:43,999 --> 00:45:47,839
expose all this data how do you and we

00:45:46,160 --> 00:45:50,299
don't have a solution to that so maybe

00:45:47,839 --> 00:45:51,499
we should have you come to the next

00:45:50,299 --> 00:45:52,910
meeting or we should talk about a

00:45:51,499 --> 00:45:56,119
proposal that we would have that would

00:45:52,910 --> 00:45:57,739
work for us you know that's it's a it's

00:45:56,119 --> 00:46:00,589
good to have everybody in there talking

00:45:57,739 --> 00:46:01,880
but additionally you want to just make

00:46:00,589 --> 00:46:05,799
sure that it's a solution that can work

00:46:01,880 --> 00:46:05,799

YouTube URL: https://www.youtube.com/watch?v=pqhI1rVwayQ


