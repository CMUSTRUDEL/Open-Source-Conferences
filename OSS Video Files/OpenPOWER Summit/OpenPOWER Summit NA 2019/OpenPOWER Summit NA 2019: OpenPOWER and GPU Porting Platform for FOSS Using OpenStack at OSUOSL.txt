Title: OpenPOWER Summit NA 2019: OpenPOWER and GPU Porting Platform for FOSS Using OpenStack at OSUOSL
Publication date: 2019-08-28
Playlist: OpenPOWER Summit NA 2019
Description: 
	Presented by Lance Albertson, OSU Open Source Lab

The Oregon State University Open Source Lab (OSUOSL) works closely with Free and Open Source Software (FOSS) projects to provide them with a stable environment to improve their OpenPOWER porting needs. Since 2011, we’ve deployed and managed an OpenStack cluster powered by POWER9 and POWER8) used by over 100 projects. In this talk we will describe the technical challenges we have faced from setting up the initial cluster, to managing new users’ needs, tooling involved for managing the system, upgrading OpenStack and its components and other related topics. In addition, we’ll discuss a continuous integration platform we deployed which includes access to Nvidia GPU hardware along with a few other new services that are upcoming.
Captions: 
	00:00:00,030 --> 00:00:05,670
no I guess I'll go ahead and get started

00:00:02,659 --> 00:00:07,560
all right everybody welcome to open

00:00:05,670 --> 00:00:10,290
power pointing platform for free and

00:00:07,560 --> 00:00:14,309
open source software using OpenStack at

00:00:10,290 --> 00:00:15,389
the OSU open source lab here's kind of a

00:00:14,309 --> 00:00:19,770
summary of what I'll be talking about

00:00:15,389 --> 00:00:22,500
today how we created it kind of what we

00:00:19,770 --> 00:00:24,420
looked into with what the projects need

00:00:22,500 --> 00:00:25,920
for porting and also talked about the

00:00:24,420 --> 00:00:28,349
continuous integration platform that we

00:00:25,920 --> 00:00:31,380
created a little about a little bit

00:00:28,349 --> 00:00:33,840
about Who I am I'm the director of the

00:00:31,380 --> 00:00:37,260
lab I started the open source lab back

00:00:33,840 --> 00:00:39,180
in 2007 as a sysm in was promoted in

00:00:37,260 --> 00:00:41,670
2011 to director and I managed about

00:00:39,180 --> 00:00:43,350
seven to ten undergraduate students I've

00:00:41,670 --> 00:00:46,829
been active in the open source community

00:00:43,350 --> 00:00:48,210
for many years original image into then

00:00:46,829 --> 00:00:49,590
with a project called Gannett II and

00:00:48,210 --> 00:00:54,510
more recently I've been involved with

00:00:49,590 --> 00:00:55,800
chef and OpenStack chef quite a bit so

00:00:54,510 --> 00:00:58,949
first things first what is the open

00:00:55,800 --> 00:01:00,780
source lab and what services we provide

00:00:58,949 --> 00:01:01,859
so first thing I'd like to mention is

00:01:00,780 --> 00:01:04,199
we're basically an infrastructure

00:01:01,859 --> 00:01:06,180
hosting provider you can kind of think

00:01:04,199 --> 00:01:09,720
of as a colocation hosting provider for

00:01:06,180 --> 00:01:10,650
large or medium open source projects but

00:01:09,720 --> 00:01:12,720
that really means a lot

00:01:10,650 --> 00:01:14,220
wide gamut of things I could just means

00:01:12,720 --> 00:01:17,310
difficult location that could mean

00:01:14,220 --> 00:01:19,500
private cloud hosting it could mean our

00:01:17,310 --> 00:01:22,020
popular mirror which is really popular

00:01:19,500 --> 00:01:23,670
or we also can manage the hosting of a

00:01:22,020 --> 00:01:25,049
project that doesn't really have much

00:01:23,670 --> 00:01:26,100
system and experience and they just want

00:01:25,049 --> 00:01:28,200
to have their site and all their

00:01:26,100 --> 00:01:30,450
applications they need working and so

00:01:28,200 --> 00:01:31,650
forth the other half of it is is that

00:01:30,450 --> 00:01:33,180
we're an exponential learning

00:01:31,650 --> 00:01:35,939
environment for undergraduate students

00:01:33,180 --> 00:01:38,130
so we hire students and they work for us

00:01:35,939 --> 00:01:40,650
and we give them hands-on experience

00:01:38,130 --> 00:01:42,899
doing production stuff going that we

00:01:40,650 --> 00:01:44,250
have they work really close with the

00:01:42,899 --> 00:01:46,470
open source projects we treat them as

00:01:44,250 --> 00:01:48,540
our customers and they set up and they

00:01:46,470 --> 00:01:51,600
manage the services that communicate

00:01:48,540 --> 00:01:53,939
with them we try to teach them new and

00:01:51,600 --> 00:01:56,189
current dev up practices technologies

00:01:53,939 --> 00:01:58,290
the best that we can as we as long as we

00:01:56,189 --> 00:02:00,509
can keep up and many of our alumni have

00:01:58,290 --> 00:02:02,280
gone and done amazing things in the in

00:02:00,509 --> 00:02:06,000
the world probably our most impactful is

00:02:02,280 --> 00:02:07,890
Alex Povey and Brandon Phillips who both

00:02:06,000 --> 00:02:11,760
co-founded core OS which is now under

00:02:07,890 --> 00:02:12,620
was acquired by Red Hat and but an idea

00:02:11,760 --> 00:02:15,260
master

00:02:12,620 --> 00:02:18,230
but but yeah we have lots of people out

00:02:15,260 --> 00:02:19,790
there currently I'm the one full timer

00:02:18,230 --> 00:02:23,870
and I have eight undergraduate students

00:02:19,790 --> 00:02:25,450
that changes throughout the year so what

00:02:23,870 --> 00:02:28,400
do we provide specifically to open power

00:02:25,450 --> 00:02:30,620
we provide that neutral hosting provider

00:02:28,400 --> 00:02:33,590
for projects that have access to that

00:02:30,620 --> 00:02:35,510
platform what we strive to do is create

00:02:33,590 --> 00:02:38,030
a stable and flexible development

00:02:35,510 --> 00:02:41,840
environment for projects to be able to

00:02:38,030 --> 00:02:44,120
port their software to open power we

00:02:41,840 --> 00:02:46,549
work with IBM to architect and maintain

00:02:44,120 --> 00:02:49,430
the physical infrastructure and work

00:02:46,549 --> 00:02:52,549
with IBM as we as we run into issue and

00:02:49,430 --> 00:02:58,010
we also enable the access to power an

00:02:52,549 --> 00:03:02,959
GPU resources to all these projects we

00:02:58,010 --> 00:03:04,730
also work on computing resources to test

00:03:02,959 --> 00:03:07,040
large-scale testing and porting we also

00:03:04,730 --> 00:03:09,440
have lots of continuous integration

00:03:07,040 --> 00:03:11,150
backends for open power and we also

00:03:09,440 --> 00:03:15,049
provide software build computing

00:03:11,150 --> 00:03:17,780
resources for downstream users we've had

00:03:15,049 --> 00:03:19,970
quite a history with power and porting I

00:03:17,780 --> 00:03:22,010
think it started back in 2005 actually

00:03:19,970 --> 00:03:24,109
before I started at the lab with a

00:03:22,010 --> 00:03:25,970
single power v server with shell access

00:03:24,109 --> 00:03:27,590
and we only had a few projects using it

00:03:25,970 --> 00:03:29,359
we really didn't have a lot of

00:03:27,590 --> 00:03:31,549
collaboration with IBM at the time they

00:03:29,359 --> 00:03:35,000
just say here's a machine go do stuff

00:03:31,549 --> 00:03:37,040
with it and by the late 2000s we started

00:03:35,000 --> 00:03:40,400
hosting a lot more system so we at first

00:03:37,040 --> 00:03:42,440
we had six power seven servers we would

00:03:40,400 --> 00:03:44,959
do all the grunge work with getting the

00:03:42,440 --> 00:03:46,609
L par set up and giving access to the

00:03:44,959 --> 00:03:51,290
projects and deal with all the hardware

00:03:46,609 --> 00:03:54,079
headaches and that was great it moved

00:03:51,290 --> 00:03:55,700
forward we also hosted some dedicated

00:03:54,079 --> 00:03:58,389
hardware for the GCC compiled form

00:03:55,700 --> 00:04:01,700
project and a few other projects as well

00:03:58,389 --> 00:04:03,950
then in 2011 we really bumped up a lot

00:04:01,700 --> 00:04:06,200
of what we were doing with power we

00:04:03,950 --> 00:04:09,669
started working on the next generation

00:04:06,200 --> 00:04:11,870
with power eight and figuring out how to

00:04:09,669 --> 00:04:14,359
do what we did on the power of seven

00:04:11,870 --> 00:04:16,190
servers but do a better job of it make

00:04:14,359 --> 00:04:18,560
it more flexible and that was utilizing

00:04:16,190 --> 00:04:20,930
OpenStack which was really big at the

00:04:18,560 --> 00:04:22,940
time getting up and up and up so we

00:04:20,930 --> 00:04:25,400
actually started out with the

00:04:22,940 --> 00:04:26,090
pre-development power 7 plus machines

00:04:25,400 --> 00:04:27,920
that had

00:04:26,090 --> 00:04:30,800
Opel firmware that made it think it was

00:04:27,920 --> 00:04:32,990
power eight which was fun and buggy but

00:04:30,800 --> 00:04:35,120
it got us started this is before power

00:04:32,990 --> 00:04:36,290
eight system even got in I think we

00:04:35,120 --> 00:04:39,110
actually got one of the first powering

00:04:36,290 --> 00:04:40,250
this machines at the time but I got us

00:04:39,110 --> 00:04:41,570
to the point where we can develop and

00:04:40,250 --> 00:04:45,620
get the stuff going and figuring out

00:04:41,570 --> 00:04:48,020
what we needed to do once we got that up

00:04:45,620 --> 00:04:49,880
and going we quickly replaced those

00:04:48,020 --> 00:04:52,700
power 7 plus machines with power eight

00:04:49,880 --> 00:04:54,290
and then eventually power nine and we

00:04:52,700 --> 00:04:56,630
currently host about a hundred projects

00:04:54,290 --> 00:05:02,900
on the on nine different kind of

00:04:56,630 --> 00:05:04,190
machines that we have right now so when

00:05:02,900 --> 00:05:05,750
we talk about porting what do we really

00:05:04,190 --> 00:05:08,240
need what a Foss projects really need

00:05:05,750 --> 00:05:09,500
and the thing about the our experience

00:05:08,240 --> 00:05:11,060
at the open source lab was we have

00:05:09,500 --> 00:05:13,640
really great relationship with the

00:05:11,060 --> 00:05:15,110
projects we know what their needs are we

00:05:13,640 --> 00:05:16,790
know each project has a different need

00:05:15,110 --> 00:05:18,890
and what they what they may or may not

00:05:16,790 --> 00:05:20,870
want so one thing we want to make sure

00:05:18,890 --> 00:05:24,380
we do is provide easy access to the

00:05:20,870 --> 00:05:25,910
computing resources they need whether

00:05:24,380 --> 00:05:28,100
that means development architectures

00:05:25,910 --> 00:05:29,780
troubleshooting we also provide the

00:05:28,100 --> 00:05:32,479
continuous integration testing and also

00:05:29,780 --> 00:05:34,310
if they want to integrate that actually

00:05:32,479 --> 00:05:37,370
in their own CI built system they have

00:05:34,310 --> 00:05:38,720
and also a part of that also is putting

00:05:37,370 --> 00:05:41,330
in their binary builds into their

00:05:38,720 --> 00:05:42,680
pipeline as well the other thing is that

00:05:41,330 --> 00:05:44,810
we want to support as many operating

00:05:42,680 --> 00:05:47,450
systems and platforms and versions that

00:05:44,810 --> 00:05:50,150
we can so we do that within the best

00:05:47,450 --> 00:05:51,590
reason we can and just basically giving

00:05:50,150 --> 00:05:53,450
them enough raw resources to do what

00:05:51,590 --> 00:05:55,130
they need to do so giving them enough

00:05:53,450 --> 00:05:57,200
disk space and performance for doing

00:05:55,130 --> 00:05:59,870
testing and building providing enough

00:05:57,200 --> 00:06:01,580
RAM to being able to do the types of

00:05:59,870 --> 00:06:03,710
builds and testing that they need and

00:06:01,580 --> 00:06:06,560
the the last part depending on the

00:06:03,710 --> 00:06:07,940
project having an API to be able to

00:06:06,560 --> 00:06:09,890
drive spinning up and spinning down

00:06:07,940 --> 00:06:11,900
instances for automated testing can be

00:06:09,890 --> 00:06:13,940
really important not other project means

00:06:11,900 --> 00:06:16,160
that but it can be really useful for

00:06:13,940 --> 00:06:17,539
doing multi node testing for example and

00:06:16,160 --> 00:06:20,780
if you wanted to test between two

00:06:17,539 --> 00:06:24,050
different nodes doing stuff so we knew

00:06:20,780 --> 00:06:27,200
that going in so what we provide is the

00:06:24,050 --> 00:06:29,000
OpenStack power cluster we provide

00:06:27,200 --> 00:06:31,520
compute networking block storage and

00:06:29,000 --> 00:06:33,400
orchestration we have pre-built images

00:06:31,520 --> 00:06:34,870
that we provide we

00:06:33,400 --> 00:06:39,100
currently have about two sent to West

00:06:34,870 --> 00:06:42,070
Fedora Debian and opens Tuesday for

00:06:39,100 --> 00:06:43,990
access we provide API access and to pull

00:06:42,070 --> 00:06:45,190
or we can deploy the VM for them if they

00:06:43,990 --> 00:06:47,410
don't want to deal with the API access

00:06:45,190 --> 00:06:48,940
and that also means giving them the web

00:06:47,410 --> 00:06:50,350
GUI interface so if they want to get on

00:06:48,940 --> 00:06:52,210
the console and troubleshoot things

00:06:50,350 --> 00:06:54,090
maybe do some kernel development they

00:06:52,210 --> 00:06:56,979
could pin it could potentially do that

00:06:54,090 --> 00:06:59,169
we do have some larger projects that

00:06:56,979 --> 00:07:00,580
have dedicated bare-metal access to

00:06:59,169 --> 00:07:02,949
their own systems that are outside of

00:07:00,580 --> 00:07:05,949
this cluster so that includes the GCC

00:07:02,949 --> 00:07:07,600
compiled form project debian alpine and

00:07:05,949 --> 00:07:10,300
freebsd has their own systems as well

00:07:07,600 --> 00:07:11,919
right now and that kind of comes with

00:07:10,300 --> 00:07:15,039
the collaboration we have with IBM and

00:07:11,919 --> 00:07:17,169
allocating those systems we basically

00:07:15,039 --> 00:07:20,800
try to make it as flexible as possible

00:07:17,169 --> 00:07:24,060
for these projects on the flip side of

00:07:20,800 --> 00:07:26,740
it we deal with all the hardware support

00:07:24,060 --> 00:07:28,660
so that all they worry about is being

00:07:26,740 --> 00:07:30,580
able to SSH into an instance of being

00:07:28,660 --> 00:07:32,440
able to do what they need to do for

00:07:30,580 --> 00:07:34,870
those for those projects that have the

00:07:32,440 --> 00:07:36,820
dedicated bare metal access we make sure

00:07:34,870 --> 00:07:38,440
that the machines probably has all the

00:07:36,820 --> 00:07:40,479
firmware updated everything's configured

00:07:38,440 --> 00:07:41,770
properly we know how to configure the

00:07:40,479 --> 00:07:43,360
raid on it which can be a little weird

00:07:41,770 --> 00:07:46,120
for some of these projects coming from

00:07:43,360 --> 00:07:47,680
an x86 world we just have all that

00:07:46,120 --> 00:07:50,289
experience over the years of knowing how

00:07:47,680 --> 00:07:51,580
we can give advice to these projects and

00:07:50,289 --> 00:07:53,260
then when we get into a mind we always

00:07:51,580 --> 00:07:54,909
can reach out to IBM engineering support

00:07:53,260 --> 00:07:58,270
to kind of figure out why are we running

00:07:54,909 --> 00:08:00,580
into this issue I remember early early

00:07:58,270 --> 00:08:02,169
on we actually found an opal firmer

00:08:00,580 --> 00:08:04,120
above that they hadn't seen out on the

00:08:02,169 --> 00:08:07,050
wild yet and we got got them to fix that

00:08:04,120 --> 00:08:09,940
that was kind of fun

00:08:07,050 --> 00:08:11,260
so here's a quick synopsis of the kinds

00:08:09,940 --> 00:08:14,110
of projects that are using this platform

00:08:11,260 --> 00:08:16,599
right now doctors using it right now to

00:08:14,110 --> 00:08:18,310
build all of their PowerPC little-endian

00:08:16,599 --> 00:08:20,620
docker images are built on this platform

00:08:18,310 --> 00:08:21,610
Travis CI is doing some beta testing

00:08:20,620 --> 00:08:24,130
I've been working with Gerrit quite a

00:08:21,610 --> 00:08:25,449
bit on that some of the jobs that go

00:08:24,130 --> 00:08:27,970
through Travis CI now go through our

00:08:25,449 --> 00:08:31,449
system and then a whole bunch of other

00:08:27,970 --> 00:08:33,240
projects that we provide resources for

00:08:31,449 --> 00:08:36,279
whether it's just part of their CI

00:08:33,240 --> 00:08:39,159
development testing you name it it's in

00:08:36,279 --> 00:08:41,620
there so it's quite a bit of projects

00:08:39,159 --> 00:08:43,719
that we encompass and it goes and grows

00:08:41,620 --> 00:08:44,800
and grows and grows and when I went on a

00:08:43,719 --> 00:08:46,270
show was I was looking through the

00:08:44,800 --> 00:08:49,450
number of tickets that we've gotten over

00:08:46,270 --> 00:08:50,830
year starting about 2015 when I could go

00:08:49,450 --> 00:08:54,790
back that far our ticketing system to

00:08:50,830 --> 00:08:56,220
kind of see what was going on you can

00:08:54,790 --> 00:09:00,310
see the growth that happened between

00:08:56,220 --> 00:09:02,290
2015 and 2017 I'm not really worried

00:09:00,310 --> 00:09:04,180
about keeping that going up and up and

00:09:02,290 --> 00:09:05,260
up every year but you can kind of see

00:09:04,180 --> 00:09:06,640
where we had this peak and now we're

00:09:05,260 --> 00:09:08,890
just kind of maintaining and it can go

00:09:06,640 --> 00:09:10,960
up and down based off of you know

00:09:08,890 --> 00:09:14,860
word-of-mouth people knowing about it

00:09:10,960 --> 00:09:15,310
and knowing that we're we this access is

00:09:14,860 --> 00:09:19,260
here

00:09:15,310 --> 00:09:23,680
so as of 2019 we have I think about 25

00:09:19,260 --> 00:09:26,830
projects that we've brought on and all

00:09:23,680 --> 00:09:29,050
of our students actually do the work of

00:09:26,830 --> 00:09:31,000
getting projects access so they we have

00:09:29,050 --> 00:09:33,700
a process where what they fill out this

00:09:31,000 --> 00:09:35,740
form and then they we spent up the VM or

00:09:33,700 --> 00:09:37,930
we get them the access and we go through

00:09:35,740 --> 00:09:41,410
then so the prod our students do all of

00:09:37,930 --> 00:09:42,850
that now the part that I really enjoy

00:09:41,410 --> 00:09:44,230
doing is a technical side and that's

00:09:42,850 --> 00:09:45,250
actually how did we create this platform

00:09:44,230 --> 00:09:46,930
in the first place

00:09:45,250 --> 00:09:48,760
so I'm going to talk about our

00:09:46,930 --> 00:09:51,160
architecture and design the challenges

00:09:48,760 --> 00:09:52,630
we made scalability challenges and kind

00:09:51,160 --> 00:09:55,540
of some upcoming changes we're gonna be

00:09:52,630 --> 00:09:57,550
dealing with so currently we're running

00:09:55,540 --> 00:10:02,590
the Queens release where two major

00:09:57,550 --> 00:10:05,110
releases behind right now I did a lot of

00:10:02,590 --> 00:10:08,740
upgrading of the of our OpenStack in the

00:10:05,110 --> 00:10:11,830
past year to get it up to where we need

00:10:08,740 --> 00:10:13,660
to be we have the following services

00:10:11,830 --> 00:10:15,700
enable which is what most projects need

00:10:13,660 --> 00:10:18,400
we may add to that as we need as we see

00:10:15,700 --> 00:10:20,410
fit at the lab we standardize on sent to

00:10:18,400 --> 00:10:22,240
Wes and so we're currently using system

00:10:20,410 --> 00:10:23,110
to s7 as our host operating system on

00:10:22,240 --> 00:10:24,790
the hypervisors

00:10:23,110 --> 00:10:27,280
and we use chef as our configuration

00:10:24,790 --> 00:10:31,840
management tool and so that's how we

00:10:27,280 --> 00:10:33,460
manage all of these not to give to you

00:10:31,840 --> 00:10:35,410
too much chef jargon but we have a

00:10:33,460 --> 00:10:39,460
wrapper cookbook that's publicly visible

00:10:35,410 --> 00:10:43,240
on github that shows it's still working

00:10:39,460 --> 00:10:45,640
okay that shows kind of how we have

00:10:43,240 --> 00:10:48,430
these things kept configured and we that

00:10:45,640 --> 00:10:51,100
actually pulls in upstream cookbooks

00:10:48,430 --> 00:10:53,560
that the OpenStack community and I'm a

00:10:51,100 --> 00:10:55,600
part of maintain and how we can get

00:10:53,560 --> 00:10:57,370
figure all of that another change that

00:10:55,600 --> 00:10:58,100
we made was we're actually running a

00:10:57,370 --> 00:11:00,650
mainline

00:10:58,100 --> 00:11:02,300
LTS Colonel on all the hypervisor nodes

00:11:00,650 --> 00:11:04,400
that's just to ensure that we get the

00:11:02,300 --> 00:11:05,750
latest and greatest boost and things

00:11:04,400 --> 00:11:07,130
that we need out of the kernel and run

00:11:05,750 --> 00:11:08,740
on these systems we were running we were

00:11:07,130 --> 00:11:11,720
starting to run into some issues where

00:11:08,740 --> 00:11:13,280
the kernel was coming from cinta Wes was

00:11:11,720 --> 00:11:14,600
having a few itching issues and we

00:11:13,280 --> 00:11:16,610
wanted to make sure we got ahead of that

00:11:14,600 --> 00:11:18,290
so of course running our own mainline

00:11:16,610 --> 00:11:21,260
has its own set of issues sometimes that

00:11:18,290 --> 00:11:22,940
we've run into but also helps us kind of

00:11:21,260 --> 00:11:24,140
make sure that you know if we catch

00:11:22,940 --> 00:11:26,660
anything we can kind of get it fixed

00:11:24,140 --> 00:11:29,330
more extreme more quickly another thing

00:11:26,660 --> 00:11:31,520
that we wanted to do is we can't build

00:11:29,330 --> 00:11:33,890
this cluster to be for testing

00:11:31,520 --> 00:11:35,270
performance we've had several projects

00:11:33,890 --> 00:11:37,910
come and like hey we want to be able to

00:11:35,270 --> 00:11:40,250
see how fast this runs comparing on x86

00:11:37,910 --> 00:11:42,470
and was like well you could do it but

00:11:40,250 --> 00:11:43,910
you can't you really you're on a shared

00:11:42,470 --> 00:11:46,130
resource and depending on what you're

00:11:43,910 --> 00:11:48,080
doing whether it's IO bound or whatnot

00:11:46,130 --> 00:11:49,460
you're going to be contending against a

00:11:48,080 --> 00:11:53,330
bunch of other projects so we can't

00:11:49,460 --> 00:11:55,190
guarantee what it is what's going on my

00:11:53,330 --> 00:11:56,470
counterpart Chris he does all that kind

00:11:55,190 --> 00:11:58,310
of stuff where he can test it and

00:11:56,470 --> 00:11:59,840
actually see what's going on I'm not

00:11:58,310 --> 00:12:01,550
wanting to do that I just want to make

00:11:59,840 --> 00:12:04,070
sure that the system works the best that

00:12:01,550 --> 00:12:05,570
we can at the best effort but I can't

00:12:04,070 --> 00:12:08,570
guarantee it's gonna be great for

00:12:05,570 --> 00:12:11,780
testing performance so that's kind of

00:12:08,570 --> 00:12:15,380
another part of our design currently our

00:12:11,780 --> 00:12:17,270
hardware we have six power eight

00:12:15,380 --> 00:12:20,720
machines and four powering up power nine

00:12:17,270 --> 00:12:23,390
nodes they all have 512 gigs of ram that

00:12:20,720 --> 00:12:25,460
was something initially we only have 256

00:12:23,390 --> 00:12:26,900
gigs that wasn't enough because projects

00:12:25,460 --> 00:12:29,360
really liked having RAM on their other

00:12:26,900 --> 00:12:31,790
VMs and thanks to a donation from

00:12:29,360 --> 00:12:34,540
Mellanox we had a 40 gigabit switch that

00:12:31,790 --> 00:12:38,300
we interconnect all of those nodes with

00:12:34,540 --> 00:12:40,730
more recently we got five storage nodes

00:12:38,300 --> 00:12:42,500
of a power rate based to act as a safe

00:12:40,730 --> 00:12:46,070
storage cluster we're currently running

00:12:42,500 --> 00:12:48,320
the mimic release and it has almost 300

00:12:46,070 --> 00:12:50,360
terabytes of raw disk storage and we're

00:12:48,320 --> 00:12:52,580
currently only using about 20 percent

00:12:50,360 --> 00:12:54,650
capacity of that of that cluster so

00:12:52,580 --> 00:12:56,750
we're doing pretty good on the capacity

00:12:54,650 --> 00:12:59,060
side and then we just have one

00:12:56,750 --> 00:13:00,890
controller node that manages all the

00:12:59,060 --> 00:13:04,000
various API services between all of

00:13:00,890 --> 00:13:04,000
those all those nodes

00:13:04,650 --> 00:13:08,860
so the backend storage was something

00:13:07,240 --> 00:13:11,470
that one of the challenges we ran into

00:13:08,860 --> 00:13:13,840
early on when we first started doing it

00:13:11,470 --> 00:13:16,150
we just put it in local storage we had

00:13:13,840 --> 00:13:19,270
the system set up with a raid 5 with

00:13:16,150 --> 00:13:23,080
spinning to SAS disks using LVM and I

00:13:19,270 --> 00:13:24,670
scuzzy as a way of connecting but the

00:13:23,080 --> 00:13:26,290
problem we ran into is if we needed to

00:13:24,670 --> 00:13:27,880
do any maintenance on the hypervisor as

00:13:26,290 --> 00:13:30,910
rebooting the node would cause many

00:13:27,880 --> 00:13:33,550
issues so depending on I couldn't do

00:13:30,910 --> 00:13:38,320
live migrations and if I rebooted a node

00:13:33,550 --> 00:13:39,910
that had the storage was there visible

00:13:38,320 --> 00:13:41,200
but then it went to another machine it

00:13:39,910 --> 00:13:42,880
was just everything would just not work

00:13:41,200 --> 00:13:45,040
and then we also ran into a lot of i/o

00:13:42,880 --> 00:13:47,320
contention and capacity issues we

00:13:45,040 --> 00:13:48,460
couldn't scale the storage up that we

00:13:47,320 --> 00:13:52,900
wanted to be able to do because it was

00:13:48,460 --> 00:13:56,620
based on those nodes so in 2017 I really

00:13:52,900 --> 00:13:58,030
pushed IBM to get us some push so we can

00:13:56,620 --> 00:13:59,890
move start using stuff which is a

00:13:58,030 --> 00:14:02,770
first-class storage citizen now in

00:13:59,890 --> 00:14:06,400
OpenStack and so luckily they were able

00:14:02,770 --> 00:14:07,990
to donate those storage machines and I

00:14:06,400 --> 00:14:10,030
was able to get that deployed early on

00:14:07,990 --> 00:14:13,240
and migrated everybody over from the

00:14:10,030 --> 00:14:16,420
local storage over to a soft storage in

00:14:13,240 --> 00:14:18,670
April of 2019 after we did that we saw a

00:14:16,420 --> 00:14:20,470
huge performance boost and the other

00:14:18,670 --> 00:14:23,170
benefit was as we were able to do live

00:14:20,470 --> 00:14:24,760
migrations of the VMS and especially

00:14:23,170 --> 00:14:27,760
since we're doing it on the 40 gigabit

00:14:24,760 --> 00:14:29,590
network it's really fit really fast so

00:14:27,760 --> 00:14:32,290
that way we could you know deal with

00:14:29,590 --> 00:14:33,880
upgrades of the we could take a system

00:14:32,290 --> 00:14:35,380
completely down but before we do that we

00:14:33,880 --> 00:14:37,080
can migrate all the instances off of

00:14:35,380 --> 00:14:39,430
that onto the remaining nodes we have

00:14:37,080 --> 00:14:41,430
reboot the node do-over we whatever we

00:14:39,430 --> 00:14:45,250
need to do with it and bring it back up

00:14:41,430 --> 00:14:47,680
that made it a lot better and then also

00:14:45,250 --> 00:14:50,200
it made scaling our storage a lot easier

00:14:47,680 --> 00:14:52,090
so now if we ever for whatever reason

00:14:50,200 --> 00:14:54,670
run out of storage we can buy another

00:14:52,090 --> 00:14:56,620
server or buy bigger disks and just put

00:14:54,670 --> 00:14:58,750
it in there and with the way stuff works

00:14:56,620 --> 00:15:00,430
it's really easy to do I can reboot

00:14:58,750 --> 00:15:02,350
those nodes as long as they do it

00:15:00,430 --> 00:15:05,590
correctly and the storage doesn't go

00:15:02,350 --> 00:15:07,030
down so that was really our biggest

00:15:05,590 --> 00:15:09,100
change we've done in the past year was

00:15:07,030 --> 00:15:11,790
getting stuff working and tuning it

00:15:09,100 --> 00:15:11,790
fine-tuning it

00:15:12,440 --> 00:15:19,339
so here's some hypervisor usage that we

00:15:14,569 --> 00:15:20,810
have so we've been using power E for

00:15:19,339 --> 00:15:22,519
quite a while so is quite a bit more

00:15:20,810 --> 00:15:25,160
power eight VMs now than there are par

00:15:22,519 --> 00:15:27,170
on EIN so on average I'd say we got

00:15:25,160 --> 00:15:30,680
about 45 virtual machines running per

00:15:27,170 --> 00:15:32,930
hypervisor node and this is showing in

00:15:30,680 --> 00:15:36,920
the last since about February since I

00:15:32,930 --> 00:15:39,019
started tracking all this information so

00:15:36,920 --> 00:15:40,459
this is showing the power nine was the

00:15:39,019 --> 00:15:44,649
power eight and this is the total

00:15:40,459 --> 00:15:48,079
between both of them what's going on so

00:15:44,649 --> 00:15:49,670
since we moved over to SEF the power

00:15:48,079 --> 00:15:52,040
nine nodes are actually just one you

00:15:49,670 --> 00:15:54,199
based nodes they don't have a lot of

00:15:52,040 --> 00:15:55,490
storage they don't need it and it makes

00:15:54,199 --> 00:15:58,060
it a lot easier we could we could pack a

00:15:55,490 --> 00:16:00,560
lot more power and I notes as we need to

00:15:58,060 --> 00:16:01,910
so we only have about ten VMs per node

00:16:00,560 --> 00:16:03,050
running on that but that's hopefully

00:16:01,910 --> 00:16:04,610
gonna be changing more and more people

00:16:03,050 --> 00:16:07,790
are going to want to do do things on

00:16:04,610 --> 00:16:10,970
power now here's some snapshots from our

00:16:07,790 --> 00:16:13,009
our dashboard so this shows over the

00:16:10,970 --> 00:16:14,870
last seven days the cluster loads so

00:16:13,009 --> 00:16:18,769
this is showing actual CPU usage across

00:16:14,870 --> 00:16:22,069
all the hypervisors the actual load on

00:16:18,769 --> 00:16:23,480
the machine this capacity the cluster

00:16:22,069 --> 00:16:25,160
memory uses so you can kind of see our

00:16:23,480 --> 00:16:26,660
limiting factor right now is mostly in

00:16:25,160 --> 00:16:31,009
the RAM so we kind of have to keep ahead

00:16:26,660 --> 00:16:33,500
of that we have 265 VMs you can see

00:16:31,009 --> 00:16:34,819
power nine power eight VMs we have a

00:16:33,500 --> 00:16:39,980
hundred and seven projects right now

00:16:34,819 --> 00:16:42,410
registered that are using this and yeah

00:16:39,980 --> 00:16:45,290
it's going really well here's another

00:16:42,410 --> 00:16:46,910
graph set of graphs that I have that's

00:16:45,290 --> 00:16:49,550
the same one I showed before but this is

00:16:46,910 --> 00:16:50,569
showing all of the VMS let's see here

00:16:49,550 --> 00:16:53,329
trying to think of some other

00:16:50,569 --> 00:16:56,059
interesting things here you can see the

00:16:53,329 --> 00:16:57,769
disk throughput throughput on the stuff

00:16:56,059 --> 00:16:59,389
cluster there's lots of writes going on

00:16:57,769 --> 00:17:01,699
I don't know if you can see it from here

00:16:59,389 --> 00:17:03,470
but it's doing about a hundred megabits

00:17:01,699 --> 00:17:07,640
megabytes per second on a lot of the

00:17:03,470 --> 00:17:10,959
writes and then this kind of shows you

00:17:07,640 --> 00:17:15,439
the overview of the whole all of the

00:17:10,959 --> 00:17:16,579
power all of the hypervisor nodes the

00:17:15,439 --> 00:17:19,400
naming scheme doesn't quite make sense

00:17:16,579 --> 00:17:22,339
because I read you we renamed some

00:17:19,400 --> 00:17:24,740
things at some point but this is a power

00:17:22,339 --> 00:17:29,720
eight power eight power eight power

00:17:24,740 --> 00:17:31,580
power eight power eight nine this is a

00:17:29,720 --> 00:17:33,020
development machine I use and I

00:17:31,580 --> 00:17:34,730
sometimes pull it in but that's power

00:17:33,020 --> 00:17:37,760
eight and that's not that's nine and

00:17:34,730 --> 00:17:39,440
nine so this is showing CPU utilization

00:17:37,760 --> 00:17:41,090
so you can see my power eight systems

00:17:39,440 --> 00:17:42,290
are getting pretty pretty hammered

00:17:41,090 --> 00:17:43,910
pretty well and I'm trying to get more

00:17:42,290 --> 00:17:45,740
people to use the power nine systems

00:17:43,910 --> 00:17:47,870
because they're faster and they have

00:17:45,740 --> 00:17:49,760
more capacity right now and you can see

00:17:47,870 --> 00:17:51,650
our our memory usage on the systems and

00:17:49,760 --> 00:17:53,179
then you can see a little bit of the the

00:17:51,650 --> 00:17:54,590
network usage that our that's happening

00:17:53,179 --> 00:17:57,020
down there a lot of that network users

00:17:54,590 --> 00:17:58,220
is just coming from Seth itself if I

00:17:57,020 --> 00:18:00,350
dive down in there you could actually

00:17:58,220 --> 00:18:02,630
see how much actual public Internet

00:18:00,350 --> 00:18:04,160
traffic that's coming off of it so the

00:18:02,630 --> 00:18:07,550
systems are being used quite quite much

00:18:04,160 --> 00:18:09,020
quite a lot this is a dashboard we use

00:18:07,550 --> 00:18:11,030
on stuff to kind of see what's going on

00:18:09,020 --> 00:18:14,630
so we can see how much cluster i/o is

00:18:11,030 --> 00:18:17,660
happening we can see the latencies of

00:18:14,630 --> 00:18:19,370
the various hard drives and we can also

00:18:17,660 --> 00:18:21,950
see the capacity and how everything is

00:18:19,370 --> 00:18:23,770
going so it really it's really helpful

00:18:21,950 --> 00:18:25,610
for diagnosing issues there's a lot more

00:18:23,770 --> 00:18:27,590
graphs that we have in there that I

00:18:25,610 --> 00:18:31,550
can't show you but there's quite a bit

00:18:27,590 --> 00:18:33,470
of stuff so some of the initial

00:18:31,550 --> 00:18:39,740
challenges we ran into since we started

00:18:33,470 --> 00:18:41,120
so early on with power power 8 you know

00:18:39,740 --> 00:18:43,040
there was very limited support for us to

00:18:41,120 --> 00:18:45,440
start out with so an initial initially

00:18:43,040 --> 00:18:47,960
sent to us didn't have any power pc

00:18:45,440 --> 00:18:49,490
little-endian support at all so we

00:18:47,960 --> 00:18:51,170
actually started out with fedora back it

00:18:49,490 --> 00:18:54,080
back in the day and then we switched to

00:18:51,170 --> 00:18:57,080
CentOS back then chef didn't actually

00:18:54,080 --> 00:18:58,550
have a build for power quite yet so we

00:18:57,080 --> 00:19:01,160
had to build our own they now have that

00:18:58,550 --> 00:19:04,730
and that a lot of the binaries that we

00:19:01,160 --> 00:19:06,580
use especially from the OpenStack

00:19:04,730 --> 00:19:09,410
binaries from sent to Wes called our do

00:19:06,580 --> 00:19:12,170
didn't have PowerPC little ending they

00:19:09,410 --> 00:19:13,790
still don't the good news is most of

00:19:12,170 --> 00:19:15,470
them are just Python packages so it

00:19:13,790 --> 00:19:18,530
doesn't matter but there's a few that

00:19:15,470 --> 00:19:20,360
are excuse me dependent that are

00:19:18,530 --> 00:19:23,420
actually binary and all we do is we just

00:19:20,360 --> 00:19:25,580
pull the source rpm from the repository

00:19:23,420 --> 00:19:26,990
we build it on PowerPC and then we put

00:19:25,580 --> 00:19:29,809
it in our own repository and just

00:19:26,990 --> 00:19:35,090
configure the know to use that when we

00:19:29,809 --> 00:19:36,980
move to SEF sent to us and epple and all

00:19:35,090 --> 00:19:39,049
that actually has a build for it but we

00:19:36,980 --> 00:19:40,759
wanted to track upstream

00:19:39,049 --> 00:19:43,249
and upstream stuff didn't actually have

00:19:40,759 --> 00:19:44,479
powerpc little-endian at images i

00:19:43,249 --> 00:19:46,219
reached out to them but they weren't

00:19:44,479 --> 00:19:47,809
that interested in doing that although

00:19:46,219 --> 00:19:51,409
since their Red Hat company maybe

00:19:47,809 --> 00:19:53,569
that'll change so I actually build those

00:19:51,409 --> 00:19:56,569
ourselves as well I'm using the source

00:19:53,569 --> 00:19:57,279
rpm with a few minor modifications to

00:19:56,569 --> 00:19:59,629
get that going

00:19:57,279 --> 00:20:02,419
we also had some initial hypervisor

00:19:59,629 --> 00:20:04,219
problems there was bugs in Q mu and

00:20:02,419 --> 00:20:06,349
libvirt that we had early on but we

00:20:04,219 --> 00:20:08,389
don't have any more and then we also had

00:20:06,349 --> 00:20:13,309
some issues on parry only with a

00:20:08,389 --> 00:20:16,749
contiguous memory allocation where the

00:20:13,309 --> 00:20:18,739
way power8 works with KVM it allocates

00:20:16,749 --> 00:20:20,689
contiguously the amount of memory you

00:20:18,739 --> 00:20:22,759
want to have for doing wide migrations

00:20:20,689 --> 00:20:24,919
if we got to a point where we had enough

00:20:22,759 --> 00:20:26,839
physical memory on the system but we

00:20:24,919 --> 00:20:28,669
didn't have enough contiguous memory to

00:20:26,839 --> 00:20:31,339
add another VM and so it would stop

00:20:28,669 --> 00:20:33,889
adding VMs so we had to make a kernel

00:20:31,339 --> 00:20:36,169
adjustment to kind of do that and we

00:20:33,889 --> 00:20:37,669
still hit that from now now and again

00:20:36,169 --> 00:20:40,519
but it hasn't been that big of a deal

00:20:37,669 --> 00:20:43,249
and then in all of this we had to learn

00:20:40,519 --> 00:20:45,709
OpenStack and then seth which was a lot

00:20:43,249 --> 00:20:48,259
of work and a lot of up taking but our

00:20:45,709 --> 00:20:49,789
students really have a lot of fun

00:20:48,259 --> 00:20:52,339
learning about how all this stuff works

00:20:49,789 --> 00:20:57,709
in the backend and how it all kind of

00:20:52,339 --> 00:20:59,719
fits together some other things when we

00:20:57,709 --> 00:21:01,759
wanted to build guest images we use a

00:20:59,719 --> 00:21:04,959
tool called Packer and at the time

00:21:01,759 --> 00:21:09,769
golang hadn't been built on PowerPC and

00:21:04,959 --> 00:21:11,599
Packer was is built with it go and so we

00:21:09,769 --> 00:21:14,539
had to build our images manually and

00:21:11,599 --> 00:21:16,129
then finally actually once we got going

00:21:14,539 --> 00:21:17,679
on our cluster and they started building

00:21:16,129 --> 00:21:20,149
it we could actually finally build

00:21:17,679 --> 00:21:23,389
Packer and now I think Packer actually

00:21:20,149 --> 00:21:26,149
has its own build of PowerPC so that's

00:21:23,389 --> 00:21:29,959
been great all of our templates on how

00:21:26,149 --> 00:21:32,149
we create our images are in that github

00:21:29,959 --> 00:21:34,459
repository it also includes our x86

00:21:32,149 --> 00:21:37,129
stuff as well but it's at least a good

00:21:34,459 --> 00:21:38,749
starting place for you and the other

00:21:37,129 --> 00:21:42,709
challenge we have is just doing upgrades

00:21:38,749 --> 00:21:47,029
of the software so OpenStack has a six

00:21:42,709 --> 00:21:49,720
month release cycle cadence and they add

00:21:47,029 --> 00:21:53,410
a lot of features and have adds a lot of

00:21:49,720 --> 00:21:55,750
a lot of bug fixes as well and early on

00:21:53,410 --> 00:21:59,710
OpenStack was not really good at doing

00:21:55,750 --> 00:22:01,600
opening in-place upgrades we actually

00:21:59,710 --> 00:22:03,370
through the process we used Nova

00:22:01,600 --> 00:22:05,050
networking which got deprecated into a

00:22:03,370 --> 00:22:07,630
different with Neutron networking and

00:22:05,050 --> 00:22:09,070
there really was no way to migrate that

00:22:07,630 --> 00:22:10,990
in place and so we actually had to

00:22:09,070 --> 00:22:13,420
create two separate clusters at the same

00:22:10,990 --> 00:22:15,340
time and migrate people over when we did

00:22:13,420 --> 00:22:19,510
that thankfully we never had to do that

00:22:15,340 --> 00:22:20,800
again but we had to come up of way of

00:22:19,510 --> 00:22:23,710
doing in-place upgrades without

00:22:20,800 --> 00:22:26,200
impacting the customers and so we've

00:22:23,710 --> 00:22:28,780
been able to do in place upgrades fairly

00:22:26,200 --> 00:22:30,850
well since I think Mitaka so everything

00:22:28,780 --> 00:22:35,800
is alphabetical order so we went from M

00:22:30,850 --> 00:22:37,960
to Q and that's been a big big headache

00:22:35,800 --> 00:22:40,000
when we got the stuff we had that as

00:22:37,960 --> 00:22:44,950
well we just we've done our one one time

00:22:40,000 --> 00:22:48,070
migration from there we go

00:22:44,950 --> 00:22:51,940
well one time aggregation from let's see

00:22:48,070 --> 00:22:53,050
luma knows to mimic and that was pretty

00:22:51,940 --> 00:22:56,710
easy that was a lot easier than

00:22:53,050 --> 00:22:58,090
OpenStack with us but we just had to do

00:22:56,710 --> 00:22:59,380
a rolling upgrade on all the various

00:22:58,090 --> 00:23:01,690
things and and everything in the backend

00:22:59,380 --> 00:23:03,070
worked so we wanted to make sure that

00:23:01,690 --> 00:23:04,540
our users have the ability of not

00:23:03,070 --> 00:23:07,600
knowing an upgrade was happening and

00:23:04,540 --> 00:23:09,190
that their vm's were still running the

00:23:07,600 --> 00:23:10,540
only thing that got impacted during the

00:23:09,190 --> 00:23:12,340
upgrade window is they couldn't create

00:23:10,540 --> 00:23:15,390
new VMs because that black plane was

00:23:12,340 --> 00:23:18,460
down while we were doing that upgrade so

00:23:15,390 --> 00:23:19,510
that was a challenge and it still is a

00:23:18,460 --> 00:23:23,410
challenge but it's getting better each

00:23:19,510 --> 00:23:25,690
time we upgrade so another thing that we

00:23:23,410 --> 00:23:28,120
did over the past few years is built in

00:23:25,690 --> 00:23:31,510
some continuous integration this is

00:23:28,120 --> 00:23:36,280
basically a Jenkins instance that we

00:23:31,510 --> 00:23:37,840
manage that ties into open power this is

00:23:36,280 --> 00:23:39,940
really intended for projects that don't

00:23:37,840 --> 00:23:41,440
already have CI and they just need to

00:23:39,940 --> 00:23:43,870
run some simple tests or doing some

00:23:41,440 --> 00:23:46,180
simple builds a lot of these are

00:23:43,870 --> 00:23:49,990
actually internal IBM projects to work

00:23:46,180 --> 00:23:51,130
on it but it works really well so what

00:23:49,990 --> 00:23:53,710
we've done is is we've integrated

00:23:51,130 --> 00:23:55,450
Jenkins into with github with OAuth

00:23:53,710 --> 00:23:57,040
access so you just give us your github

00:23:55,450 --> 00:23:58,480
username and then we add you to the

00:23:57,040 --> 00:24:02,060
authentication you can do what you need

00:23:58,480 --> 00:24:03,800
to do we have four

00:24:02,060 --> 00:24:05,990
vm's that are actually running on this

00:24:03,800 --> 00:24:06,770
OpenStack cluster to our power 8 to our

00:24:05,990 --> 00:24:10,690
power 9

00:24:06,770 --> 00:24:13,490
we have docker setup on it and how

00:24:10,690 --> 00:24:15,620
Jenkins is set up is that you create a

00:24:13,490 --> 00:24:18,460
job and the job runs inside of a docker

00:24:15,620 --> 00:24:21,490
container on those on those nodes and

00:24:18,460 --> 00:24:23,930
then it stops and goes on from there and

00:24:21,490 --> 00:24:25,520
so some of the projects that are using

00:24:23,930 --> 00:24:31,490
it currently are tensorflow PI torch

00:24:25,520 --> 00:24:32,960
envoy and onyx so yeah it's been we've

00:24:31,490 --> 00:24:34,130
gone through a couple of iterations and

00:24:32,960 --> 00:24:36,470
how we have this deployed there were

00:24:34,130 --> 00:24:38,450
some Jenkins plugins that we used

00:24:36,470 --> 00:24:40,880
initially - didn't quite work and we

00:24:38,450 --> 00:24:43,700
redesign that a little bit here's a

00:24:40,880 --> 00:24:47,060
snapshot of the usage of some of the we

00:24:43,700 --> 00:24:48,770
have these 4 VM so docker 1 and dr2 this

00:24:47,060 --> 00:24:52,220
column is the power 8 and this is the

00:24:48,770 --> 00:24:54,590
power 9 the original way we had this set

00:24:52,220 --> 00:24:56,270
up it didn't really balance the load

00:24:54,590 --> 00:24:58,100
very well and the change we made more

00:24:56,270 --> 00:25:00,800
recently seems to do that however I

00:24:58,100 --> 00:25:03,110
don't know why dr3 hasn't gotten many

00:25:00,800 --> 00:25:06,440
jobs in the last 24 hours but it's not

00:25:03,110 --> 00:25:07,790
that bad so this this shows what's kind

00:25:06,440 --> 00:25:10,220
of going on we've had issues with this

00:25:07,790 --> 00:25:14,870
capacity we had one I think it was PI

00:25:10,220 --> 00:25:17,090
torch or I can remember who all of a

00:25:14,870 --> 00:25:19,520
sudden these these notes I think have

00:25:17,090 --> 00:25:21,080
200 gig of disk capacity now I started

00:25:19,520 --> 00:25:22,520
with a hundred and we have one job

00:25:21,080 --> 00:25:24,440
completely fill that up and I just

00:25:22,520 --> 00:25:25,940
emailed them and be like hey something's

00:25:24,440 --> 00:25:27,920
not right we didn't have this problem

00:25:25,940 --> 00:25:29,090
before they're like oh shoot yeah

00:25:27,920 --> 00:25:32,960
something is not right we got to fix

00:25:29,090 --> 00:25:37,220
that job so something else that we did

00:25:32,960 --> 00:25:39,230
was as we worked with the CG RB to get

00:25:37,220 --> 00:25:41,450
access to one of their systems so we

00:25:39,230 --> 00:25:45,620
could actually integrate with CG but so

00:25:41,450 --> 00:25:47,900
that we could do CI on GPUs so this is

00:25:45,620 --> 00:25:50,330
just doing development or CI usage not

00:25:47,900 --> 00:25:53,420
performance per se so the hardware stack

00:25:50,330 --> 00:25:57,020
is is we have a loaned minsky from the

00:25:53,420 --> 00:25:58,250
CG RB that has two P 100 cards in it we

00:25:57,020 --> 00:26:00,320
don't need the latest and greatest we

00:25:58,250 --> 00:26:02,600
just need something that has the GPU so

00:26:00,320 --> 00:26:05,890
they can do tests with it these systems

00:26:02,600 --> 00:26:08,900
have an nvme on it so we have docker

00:26:05,890 --> 00:26:12,410
connected mounted on to that so we get

00:26:08,900 --> 00:26:14,260
really good IO and we're using Nvidia

00:26:12,410 --> 00:26:16,600
docker so that we can

00:26:14,260 --> 00:26:19,360
do you know several different types of

00:26:16,600 --> 00:26:23,260
platform platforms on there as you need

00:26:19,360 --> 00:26:25,299
all on that same GPU system so we've

00:26:23,260 --> 00:26:29,830
integrated it into the power CGI Jenkins

00:26:25,299 --> 00:26:33,400
instance it's working really well we

00:26:29,830 --> 00:26:37,270
initially did this through the CG arby's

00:26:33,400 --> 00:26:39,549
queuing system through their sge son of

00:26:37,270 --> 00:26:41,830
grid engine system and it kind of worked

00:26:39,549 --> 00:26:44,590
but it was really finicky and it really

00:26:41,830 --> 00:26:47,200
didn't work that the the the integration

00:26:44,590 --> 00:26:50,590
for the user experience did not work

00:26:47,200 --> 00:26:52,270
really well so I worked with them as

00:26:50,590 --> 00:26:53,530
like can we just have root access on

00:26:52,270 --> 00:26:55,480
this machine and just manage it like we

00:26:53,530 --> 00:26:57,160
normally would and thankfully they were

00:26:55,480 --> 00:26:59,500
able to do that and just that's just

00:26:57,160 --> 00:27:02,470
what we ended up doing this is the

00:26:59,500 --> 00:27:04,660
specific plugin that we use for projects

00:27:02,470 --> 00:27:07,299
to be able to use so basically this node

00:27:04,660 --> 00:27:09,610
is a jinkin slave and then when you go

00:27:07,299 --> 00:27:11,470
through the the Jenkins you can say

00:27:09,610 --> 00:27:13,390
build this as a docker image and then we

00:27:11,470 --> 00:27:16,630
have a label set which goes to this node

00:27:13,390 --> 00:27:22,179
and then it runs here's a graph that we

00:27:16,630 --> 00:27:24,610
have for showing the GPU usage on this

00:27:22,179 --> 00:27:27,429
machine so this is the power usage of

00:27:24,610 --> 00:27:29,799
the GPUs that are running how the

00:27:27,429 --> 00:27:32,410
temperature of the of the GPUs and then

00:27:29,799 --> 00:27:34,780
this is actually showing the usage of it

00:27:32,410 --> 00:27:35,950
so you can see when each job runs I

00:27:34,780 --> 00:27:37,660
think there's only a couple jobs that

00:27:35,950 --> 00:27:39,760
are running on it currently well you can

00:27:37,660 --> 00:27:42,370
see that it's running and how much

00:27:39,760 --> 00:27:46,299
memory is using it's not using a lot but

00:27:42,370 --> 00:27:48,760
it's really really good so some upcoming

00:27:46,299 --> 00:27:50,350
changes that we're looking into one big

00:27:48,760 --> 00:27:53,200
project that I'm working towards is

00:27:50,350 --> 00:27:56,950
getting GPU integrated more into

00:27:53,200 --> 00:28:01,150
OpenStack so my goal there is to be able

00:27:56,950 --> 00:28:03,190
to give projects access to have a more

00:28:01,150 --> 00:28:08,350
interactive access to GPUs instead of

00:28:03,190 --> 00:28:10,299
just through CI and I discovered I think

00:28:08,350 --> 00:28:11,799
actually the last open power so a group

00:28:10,299 --> 00:28:14,110
of people from the open side resume

00:28:11,799 --> 00:28:15,910
project was here talking about it but

00:28:14,110 --> 00:28:18,940
there's a project called OpenStack zoon

00:28:15,910 --> 00:28:20,710
which enables it treats containers more

00:28:18,940 --> 00:28:22,210
like vm's instead of dealing with

00:28:20,710 --> 00:28:24,730
orchestration which is a completely

00:28:22,210 --> 00:28:26,710
separate project and OpenStack it'll

00:28:24,730 --> 00:28:27,260
it's great for having like setting up a

00:28:26,710 --> 00:28:29,360
bit

00:28:27,260 --> 00:28:31,460
a container to do a specific thing and

00:28:29,360 --> 00:28:34,669
since we can use Invidia docker

00:28:31,460 --> 00:28:37,160
to interface with the GPUs in theory we

00:28:34,669 --> 00:28:40,040
should be able to create a docker image

00:28:37,160 --> 00:28:41,960
that has a seiche on it that you can

00:28:40,040 --> 00:28:43,640
just SSH to it and you have access to

00:28:41,960 --> 00:28:47,059
the GPU to do the testing and things

00:28:43,640 --> 00:28:49,429
that you need to do so we're not quite

00:28:47,059 --> 00:28:50,809
there yet we need to get a new - a newer

00:28:49,429 --> 00:28:53,059
version of OpenStack

00:28:50,809 --> 00:28:54,410
which includes the support we need on

00:28:53,059 --> 00:28:57,980
the networking set to do that properly

00:28:54,410 --> 00:28:59,210
but I did do an initial test on the

00:28:57,980 --> 00:29:00,860
current version we have we need to get

00:28:59,210 --> 00:29:03,020
the Stein before we can really get to

00:29:00,860 --> 00:29:04,700
using this unfortunately so we have

00:29:03,020 --> 00:29:07,669
where two releases behind to get to

00:29:04,700 --> 00:29:11,240
there so I'm gonna be really excited

00:29:07,669 --> 00:29:13,280
about that once I get that going also

00:29:11,240 --> 00:29:16,760
just continuing upgrade to the latest

00:29:13,280 --> 00:29:18,590
software releases so actually the chef

00:29:16,760 --> 00:29:20,570
OpenStack or the open section chef

00:29:18,590 --> 00:29:22,340
community is a little behind on getting

00:29:20,570 --> 00:29:24,350
their their cookbooks ready for Rocky

00:29:22,340 --> 00:29:26,000
and I've been working with some of the

00:29:24,350 --> 00:29:27,890
upstream guys on getting that ready and

00:29:26,000 --> 00:29:28,970
that should be going soon and then

00:29:27,890 --> 00:29:31,400
shortly after that we'll work on getting

00:29:28,970 --> 00:29:33,020
Stein going and then we'll get over I

00:29:31,400 --> 00:29:35,900
think I can't remember what the t

00:29:33,020 --> 00:29:38,510
release is called but we'll probably get

00:29:35,900 --> 00:29:39,890
to that at some point but I'm hoping you

00:29:38,510 --> 00:29:42,650
at least be no more than one release

00:29:39,890 --> 00:29:45,290
behind to kind of keep things going

00:29:42,650 --> 00:29:47,660
the Nautilus release came out in April

00:29:45,290 --> 00:29:49,970
and so we're gonna try and upgrade the

00:29:47,660 --> 00:29:51,500
10 August soon it should be a fairly

00:29:49,970 --> 00:29:54,110
minimal upgrade but that gives us a few

00:29:51,500 --> 00:29:55,700
little bit more capability of doing

00:29:54,110 --> 00:29:57,650
things and then whenever sent to us

00:29:55,700 --> 00:29:59,960
eight gets released which I'm closely

00:29:57,650 --> 00:30:01,610
watching I think they're really close to

00:29:59,960 --> 00:30:03,830
releasing will probably start testing

00:30:01,610 --> 00:30:05,900
that and and preparing our underlying

00:30:03,830 --> 00:30:06,950
infrastructure to use synthase so that

00:30:05,900 --> 00:30:09,230
we can get on the latest and greatest

00:30:06,950 --> 00:30:10,610
that are probably a fun project to get

00:30:09,230 --> 00:30:13,419
going because there's probably gonna be

00:30:10,610 --> 00:30:16,520
some weird issues going on with that so

00:30:13,419 --> 00:30:18,650
with that do you have any questions and

00:30:16,520 --> 00:30:21,350
if you want here are the URLs to get

00:30:18,650 --> 00:30:24,700
access to the OpenStack environment and

00:30:21,350 --> 00:30:24,700
then the power CI environment

00:30:31,520 --> 00:30:34,599
[Music]

00:30:34,780 --> 00:30:43,730
No yeah we we have been looking at

00:30:41,870 --> 00:30:46,040
there's a project called ironic that

00:30:43,730 --> 00:30:48,350
OpenStack has that allows you to do that

00:30:46,040 --> 00:30:50,450
the scale we're at we're not spinning up

00:30:48,350 --> 00:30:53,120
machines a lot that often so we're not

00:30:50,450 --> 00:30:56,960
needing that but I have been interested

00:30:53,120 --> 00:30:59,600
in using ironic as a way to provide more

00:30:56,960 --> 00:31:01,280
power mental access to systems so if

00:30:59,600 --> 00:31:03,050
projects want to be able to get access

00:31:01,280 --> 00:31:04,580
to a system ironic will talk to the BMC

00:31:03,050 --> 00:31:06,280
to get it set up and they can do that

00:31:04,580 --> 00:31:09,410
and then we can repurpose that machine

00:31:06,280 --> 00:31:13,130
but we do all the BMC stuff me annually

00:31:09,410 --> 00:31:16,550
out of and as we as we want to do it we

00:31:13,130 --> 00:31:18,080
connect to the the serial over land and

00:31:16,550 --> 00:31:20,540
we run all those commands we do all that

00:31:18,080 --> 00:31:23,410
remotely so that we can run that in

00:31:20,540 --> 00:31:23,410
manage those systems

00:31:31,010 --> 00:31:35,340
we do to a certain point

00:31:33,750 --> 00:31:39,210
that's something I'm continuing to work

00:31:35,340 --> 00:31:41,580
on there's actually OpenStack has its

00:31:39,210 --> 00:31:42,870
own vlogging mechanism that I've been

00:31:41,580 --> 00:31:44,850
trying to get going

00:31:42,870 --> 00:31:47,730
and it's not currently working right now

00:31:44,850 --> 00:31:49,590
which is frustrating me but like raw

00:31:47,730 --> 00:31:52,410
logs we do keep those for quite a while

00:31:49,590 --> 00:31:55,200
but like those graphs you see we're

00:31:52,410 --> 00:31:56,670
keeping those we deployed Prometheus and

00:31:55,200 --> 00:31:57,929
core fauna in the past year to kind of

00:31:56,670 --> 00:32:00,840
get a better visibility of what's going

00:31:57,929 --> 00:32:02,160
on in there and so that that enabled us

00:32:00,840 --> 00:32:05,460
to kind of see what's going on

00:32:02,160 --> 00:32:07,980
so OpenStack has some of that it

00:32:05,460 --> 00:32:09,390
features as well yeah we're trying to do

00:32:07,980 --> 00:32:11,190
a better job of tracking that is there

00:32:09,390 --> 00:32:13,580
specific metrics that you're interested

00:32:11,190 --> 00:32:13,580
in

00:32:16,470 --> 00:32:19,799
[Music]

00:32:20,820 --> 00:32:25,820
[Laughter]

00:32:23,200 --> 00:32:27,350
yeah one big project I've been wanting

00:32:25,820 --> 00:32:30,799
to do with our infrastructure is getting

00:32:27,350 --> 00:32:32,450
a centralized like Elks that kind of set

00:32:30,799 --> 00:32:33,679
up in our inner environment but we've

00:32:32,450 --> 00:32:36,409
never really got around to that we just

00:32:33,679 --> 00:32:38,480
have a standard centralized log server

00:32:36,409 --> 00:32:40,190
for all of our self which is at least

00:32:38,480 --> 00:32:43,299
good for archiving but not really good

00:32:40,190 --> 00:32:43,299
for searching per se so

00:32:47,020 --> 00:33:22,679
any other questions correct that's

00:33:05,350 --> 00:33:27,490
correct yeah so it's called Zune and

00:33:22,679 --> 00:33:31,980
it's it basically interacts with docker

00:33:27,490 --> 00:33:34,690
and it uses same set of technology that

00:33:31,980 --> 00:33:36,429
Nova uses for connecting to the console

00:33:34,690 --> 00:33:38,800
and so forth but they're doing it to the

00:33:36,429 --> 00:33:45,160
docker console to get connect to get

00:33:38,800 --> 00:33:47,290
access to that it has there's a

00:33:45,160 --> 00:33:48,670
networking layer that they're still

00:33:47,290 --> 00:33:49,990
working on which is why I need to wait

00:33:48,670 --> 00:33:52,360
to get to the later release because

00:33:49,990 --> 00:33:54,130
they've worked out a lot of it most of

00:33:52,360 --> 00:33:56,800
the upstream contributors are in Japan

00:33:54,130 --> 00:33:58,000
and I think they work in a bank or

00:33:56,800 --> 00:33:59,620
something to doing a lot of GPU

00:33:58,000 --> 00:34:02,350
development stuff and that's why they're

00:33:59,620 --> 00:34:04,300
interested in doing this but it instead

00:34:02,350 --> 00:34:06,190
of like setting up a kubernetes cluster

00:34:04,300 --> 00:34:07,750
it'll set up individual docker

00:34:06,190 --> 00:34:10,510
containers it'll manage all of that

00:34:07,750 --> 00:34:11,590
it'll treat it like a OpenStack object

00:34:10,510 --> 00:34:14,409
inside of OpenStack

00:34:11,590 --> 00:34:18,940
which has all of its benefits it can has

00:34:14,409 --> 00:34:20,530
a horizon GUI interface as well they do

00:34:18,940 --> 00:34:22,090
have a console part like I mentioned

00:34:20,530 --> 00:34:23,200
that I haven't quite got to work but I

00:34:22,090 --> 00:34:25,600
think it's because I'm using an older

00:34:23,200 --> 00:34:29,470
version of it I think it was initially

00:34:25,600 --> 00:34:32,980
released in not rocky but Pike was the

00:34:29,470 --> 00:34:36,600
first release it came out with so it's

00:34:32,980 --> 00:34:36,600
still a pretty early early project

00:34:44,360 --> 00:34:53,330
I wouldn't be able to I only have a

00:34:51,290 --> 00:34:55,310
limited number of GPU notes so if I do

00:34:53,330 --> 00:34:57,200
that then I have one user it has access

00:34:55,310 --> 00:34:59,360
to one whole machine and I wanted to

00:34:57,200 --> 00:35:00,890
over subscribe that machine and so I

00:34:59,360 --> 00:35:02,840
could do that with something like this

00:35:00,890 --> 00:35:06,070
where I wanted to have access where

00:35:02,840 --> 00:35:08,150
multiple docker images could be running

00:35:06,070 --> 00:35:11,090
multiple at the same time that could

00:35:08,150 --> 00:35:13,340
also access the GPUs at the same time

00:35:11,090 --> 00:35:15,290
and I wanted to over subscribe kind of

00:35:13,340 --> 00:35:25,070
like what I've been doing with vm's as

00:35:15,290 --> 00:35:26,930
well I think you can that's something

00:35:25,070 --> 00:35:28,220
else I've been trying to find testing to

00:35:26,930 --> 00:35:29,600
see if I could do that but it seems like

00:35:28,220 --> 00:35:32,270
you can because we've had multiple jobs

00:35:29,600 --> 00:35:34,430
running on the GPU system sometimes and

00:35:32,270 --> 00:35:36,530
that's been working because I think they

00:35:34,430 --> 00:35:38,150
have some Larry Shem that works with

00:35:36,530 --> 00:35:39,770
that since it's it's basically

00:35:38,150 --> 00:35:43,540
containers on the bare metal machines

00:35:39,770 --> 00:35:43,540
it's just set up differently

00:35:48,740 --> 00:35:53,839
I've got a little bit more time there's

00:35:50,570 --> 00:35:55,790
any other questions all right

00:35:53,839 --> 00:35:59,109
well with that thank you for coming out

00:35:55,790 --> 00:35:59,109

YouTube URL: https://www.youtube.com/watch?v=VIYa_6qXFAw


