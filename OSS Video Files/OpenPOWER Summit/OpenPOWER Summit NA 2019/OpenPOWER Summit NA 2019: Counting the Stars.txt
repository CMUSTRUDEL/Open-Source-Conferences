Title: OpenPOWER Summit NA 2019: Counting the Stars
Publication date: 2019-08-28
Playlist: OpenPOWER Summit NA 2019
Description: 
	Presented by Jez Wain, Atos 

There are 200 billion stars in the Milky Way. The European Space Agency's Gaia project has mapped about 1% of them. The stars now need to be classified in much the same way as we classify animals and plants. Atos working with the University of Geneva has investigated the use of machine learning to help with this classification problem. Jez's talk describes the Gaia project before presenting the approach taken to construct and optimize a neural network capable of classifying the different star types. The techniques used are applicable to many different supervised machine learning problems. The talk will conclude with the initial results and a look towards future work.
Captions: 
	00:00:00,500 --> 00:00:05,670
good afternoon so just waiting for

00:00:03,240 --> 00:00:08,010
Mattos company in French company but I'm

00:00:05,670 --> 00:00:11,190
now working in Austin managing

00:00:08,010 --> 00:00:13,610
relationship to Dawson and IBM one of my

00:00:11,190 --> 00:00:16,080
tasks here is to look at neural networks

00:00:13,610 --> 00:00:18,420
and machine learning artificial

00:00:16,080 --> 00:00:20,070
intelligence on the power platform and

00:00:18,420 --> 00:00:22,230
one of the interesting projects which I

00:00:20,070 --> 00:00:24,930
have had the watching to work on has

00:00:22,230 --> 00:00:30,320
been with the European Space Agency who

00:00:24,930 --> 00:00:34,079
in in 2013 launched the Gaia satellite

00:00:30,320 --> 00:00:35,579
into space on top of a Soyuz rocket and

00:00:34,079 --> 00:00:37,410
they put it into what they call the

00:00:35,579 --> 00:00:41,399
Lagrangian the second Lagrange point

00:00:37,410 --> 00:00:44,640
which is 1 million kilometers million

00:00:41,399 --> 00:00:46,620
miles from from Earth and most didn't

00:00:44,640 --> 00:00:49,260
say in the shadow of the Moon the Sun so

00:00:46,620 --> 00:00:51,600
the lagrange second Lagrange point is a

00:00:49,260 --> 00:00:53,219
perfect balance between the central

00:00:51,600 --> 00:00:56,670
force of the rotation of orbits around

00:00:53,219 --> 00:00:59,370
the Sun and the gravity gravity

00:00:56,670 --> 00:01:01,320
attraction of the earth in the moon

00:00:59,370 --> 00:01:03,660
itself there were three LaGrant sin the

00:01:01,320 --> 00:01:06,960
around the in our solar system so the

00:01:03,660 --> 00:01:12,140
object of this this is the satellite

00:01:06,960 --> 00:01:14,520
it contains a 900 million pixel camera

00:01:12,140 --> 00:01:18,330
so this is not your Grandad's digital

00:01:14,520 --> 00:01:21,990
camera and it takes a picture of about

00:01:18,330 --> 00:01:24,119
about 2 million stars every hour it's

00:01:21,990 --> 00:01:27,479
rotating around and the angular accuracy

00:01:24,119 --> 00:01:29,490
of the camera is measured in microwatts

00:01:27,479 --> 00:01:31,829
seconds which is 10 micro arc seconds

00:01:29,490 --> 00:01:34,140
and that is equivalent of it like the

00:01:31,829 --> 00:01:34,680
angle of a quarter would make on the

00:01:34,140 --> 00:01:37,380
moon

00:01:34,680 --> 00:01:41,570
seen from Earth you know it's really

00:01:37,380 --> 00:01:44,220
really fine resolution of the camera and

00:01:41,570 --> 00:01:45,689
so this thing is rotating in space at my

00:01:44,220 --> 00:01:47,509
garage too and it's just sweeping it's

00:01:45,689 --> 00:01:49,920
periodically across the sky and

00:01:47,509 --> 00:01:53,430
precession means we sweep past different

00:01:49,920 --> 00:01:54,930
parts of the of the sky every time it

00:01:53,430 --> 00:01:56,340
goes round so this is a really cool

00:01:54,930 --> 00:01:57,600
pointing there are not many projects

00:01:56,340 --> 00:01:59,219
that can say we've got a satellite a

00:01:57,600 --> 00:02:00,600
million miles from Earth so it to be

00:01:59,219 --> 00:02:01,950
working you know I'm really happy to be

00:02:00,600 --> 00:02:03,420
working with these guys working with

00:02:01,950 --> 00:02:05,210
scientists and astronomers it's a

00:02:03,420 --> 00:02:07,730
fantastic project to be working with

00:02:05,210 --> 00:02:10,009
and the objective is is to bring the

00:02:07,730 --> 00:02:14,270
most accurate map of the Milky Way our

00:02:10,009 --> 00:02:19,100
own galaxy to date the current state of

00:02:14,270 --> 00:02:21,080
the art is this this is 1996 it's about

00:02:19,100 --> 00:02:23,959
a hundred thousand stars with an

00:02:21,080 --> 00:02:28,610
accurate model and about another two

00:02:23,959 --> 00:02:29,810
million which less precision about what

00:02:28,610 --> 00:02:31,760
we've got in them and so this is the

00:02:29,810 --> 00:02:34,190
quartz they've you know we have it's a

00:02:31,760 --> 00:02:39,470
huge difference you know going from a

00:02:34,190 --> 00:02:41,630
couple of million to 1.7 billion there

00:02:39,470 --> 00:02:44,030
were data released - which was in April

00:02:41,630 --> 00:02:48,320
of last year we announced we had

00:02:44,030 --> 00:02:52,489
identified 1.7 billion stars in the

00:02:48,320 --> 00:02:54,800
Milky Way okay this is just one percent

00:02:52,489 --> 00:02:57,050
of the stars in the Milky Way that all

00:02:54,800 --> 00:02:59,239
the rest are to faint so if this we put

00:02:57,050 --> 00:03:03,590
in a book this is book one there will be

00:02:59,239 --> 00:03:05,360
99 other books to get to the end of it

00:03:03,590 --> 00:03:08,150
to get all the stars in our galaxy it's

00:03:05,360 --> 00:03:10,160
a huge numbers that we just have a hard

00:03:08,150 --> 00:03:13,220
time understanding and this is a

00:03:10,160 --> 00:03:14,480
synthetic map of it's not actually a

00:03:13,220 --> 00:03:15,680
camera you know we don't get pictures

00:03:14,480 --> 00:03:17,299
out of this thing it just measures the

00:03:15,680 --> 00:03:20,390
light which is measure light so this is

00:03:17,299 --> 00:03:22,660
a synthetic image of a galaxy these are

00:03:20,390 --> 00:03:26,450
small and the Large Magellanic Clouds

00:03:22,660 --> 00:03:28,640
which a - Artu - near our nearest for

00:03:26,450 --> 00:03:31,459
two of our near neighbors which are

00:03:28,640 --> 00:03:35,209
about tenth of the size of the Milky Way

00:03:31,459 --> 00:03:39,290
so their own way away and you're here

00:03:35,209 --> 00:03:41,090
this is where this is where they're

00:03:39,290 --> 00:03:44,420
roughly where there's the solar system

00:03:41,090 --> 00:03:47,480
the saw was our saw was Sonny's turning

00:03:44,420 --> 00:03:49,130
around hitting him and the team I'm

00:03:47,480 --> 00:03:51,560
working within with which is a team in

00:03:49,130 --> 00:03:53,420
Geneva Switzerland they specialize in

00:03:51,560 --> 00:03:54,709
what's called variable stars and

00:03:53,420 --> 00:03:56,390
variable stars are very important

00:03:54,709 --> 00:03:57,920
because we use these to measure

00:03:56,390 --> 00:04:00,019
intergalactic distance especially

00:03:57,920 --> 00:04:01,790
special distances because the the

00:04:00,019 --> 00:04:04,130
cepheid's which are listed here one

00:04:01,790 --> 00:04:07,190
particular type of star when they've

00:04:04,130 --> 00:04:09,049
given us with a given mass they brought

00:04:07,190 --> 00:04:10,640
the shine with a known brightness so

00:04:09,049 --> 00:04:12,470
they become a standard candle we can see

00:04:10,640 --> 00:04:14,480
if it's this bright we know how heavy it

00:04:12,470 --> 00:04:15,449
is we know how far away it is and with

00:04:14,480 --> 00:04:18,449
that we can

00:04:15,449 --> 00:04:22,330
distances so there are several types of

00:04:18,449 --> 00:04:24,669
of pulsating stars and ones which are

00:04:22,330 --> 00:04:28,840
intrinsically solving the change size

00:04:24,669 --> 00:04:30,850
and so we're talking here about stars

00:04:28,840 --> 00:04:34,000
which is several tens or hundreds of

00:04:30,850 --> 00:04:36,990
times the mass of our own Sun which can

00:04:34,000 --> 00:04:39,930
double in size in space for a few days

00:04:36,990 --> 00:04:42,790
it's speeds which we just cannot imagine

00:04:39,930 --> 00:04:44,050
what you know these pulsating things and

00:04:42,790 --> 00:04:46,389
then we have extrinsic ones you know you

00:04:44,050 --> 00:04:48,100
can imagine in our planet or some dust

00:04:46,389 --> 00:04:49,360
going in front of a star it's going to

00:04:48,100 --> 00:04:51,880
change its brightness or you have two

00:04:49,360 --> 00:04:54,010
two stars of pulsar two stars rotating

00:04:51,880 --> 00:04:55,360
around each other the light varies so

00:04:54,010 --> 00:04:57,280
we'd see these variable stars and we

00:04:55,360 --> 00:04:59,860
would get this sort of graph if the

00:04:57,280 --> 00:05:00,910
brightness of the stars and there's a

00:04:59,860 --> 00:05:03,340
whole class of these things this is just

00:05:00,910 --> 00:05:06,220
the variable ones this is the variables

00:05:03,340 --> 00:05:08,320
with the non variable ones that there's

00:05:06,220 --> 00:05:10,750
another tree so the idea here is with

00:05:08,320 --> 00:05:12,729
1.7 billion we have to classify the

00:05:10,750 --> 00:05:14,919
stars and to say whether it's a Cepheid

00:05:12,729 --> 00:05:17,949
or an hourly array or a different type

00:05:14,919 --> 00:05:20,280
and so this is the the project yeah they

00:05:17,949 --> 00:05:23,740
give us we're going to try and identify

00:05:20,280 --> 00:05:24,849
of all this 1.7 billion but the ones

00:05:23,740 --> 00:05:27,250
that are variable we're gonna try and

00:05:24,849 --> 00:05:29,289
classify them and say what sort of star

00:05:27,250 --> 00:05:35,080
they are and maybe we'll find some stars

00:05:29,289 --> 00:05:37,960
of a new type this is the hope so the

00:05:35,080 --> 00:05:40,200
data we get from the satellite is we get

00:05:37,960 --> 00:05:44,500
about hundred fifty attributes per star

00:05:40,200 --> 00:05:48,220
and we get the color its brightness its

00:05:44,500 --> 00:05:51,280
position is speed its radial and radial

00:05:48,220 --> 00:05:52,900
speed and its speed towards us the

00:05:51,280 --> 00:05:54,789
parallax and then we get some other

00:05:52,900 --> 00:05:57,220
compound attributes which is the red -

00:05:54,789 --> 00:06:00,190
the blue which is a very important data

00:05:57,220 --> 00:06:03,130
point in astronomy for identifying stars

00:06:00,190 --> 00:06:05,139
then we get some other data around the

00:06:03,130 --> 00:06:07,270
observation themselves so like number of

00:06:05,139 --> 00:06:09,070
times we've seen this star its name

00:06:07,270 --> 00:06:11,740
which is just a number and then we have

00:06:09,070 --> 00:06:14,650
some stats based on some of these things

00:06:11,740 --> 00:06:17,320
based on the mean the skewness how

00:06:14,650 --> 00:06:19,330
twisted the shape of the data is the

00:06:17,320 --> 00:06:21,550
kurtosis is the tail and which tail

00:06:19,330 --> 00:06:23,890
there is on the data signal to noise and

00:06:21,550 --> 00:06:25,330
these things are all you've got various

00:06:23,890 --> 00:06:26,909
variations on a theme we have them

00:06:25,330 --> 00:06:30,639
waited in the

00:06:26,909 --> 00:06:34,030
and so a little bit of repetition in our

00:06:30,639 --> 00:06:36,939
data so it's a sort of thing the data we

00:06:34,030 --> 00:06:40,090
get a huge excess at Excel table you

00:06:36,939 --> 00:06:42,729
know it's a 150 columns wide and several

00:06:40,090 --> 00:06:44,439
hundred thousand stars long yeah so it's

00:06:42,729 --> 00:06:45,849
just a big table I'm not allowed to show

00:06:44,439 --> 00:06:48,969
you the real data this is all made up

00:06:45,849 --> 00:06:51,610
data so you can't the data collector is

00:06:48,969 --> 00:06:53,050
is still confidential this is just made

00:06:51,610 --> 00:06:54,189
up but this is the type you know it's

00:06:53,050 --> 00:06:57,430
just like that that's exactly the sort

00:06:54,189 --> 00:06:59,139
of stuff we get and what does they tilt

00:06:57,430 --> 00:07:01,719
like so I took the data straight out of

00:06:59,139 --> 00:07:04,509
the we got from from the ESA and I

00:07:01,719 --> 00:07:05,919
plotted it out just the variable stars

00:07:04,509 --> 00:07:07,659
we have here so this is they are early

00:07:05,919 --> 00:07:10,029
early the cepheid's the things that look

00:07:07,659 --> 00:07:11,740
like out some eclipsing stars and I just

00:07:10,029 --> 00:07:14,770
put the left right Ascension and

00:07:11,740 --> 00:07:16,599
declination and you just pop them out

00:07:14,770 --> 00:07:19,240
and it gets this and what do we have

00:07:16,599 --> 00:07:21,849
here we have the large and the small the

00:07:19,240 --> 00:07:24,639
general Tanz so it looks like the data

00:07:21,849 --> 00:07:29,020
we've got is pretty healthy yeah so it

00:07:24,639 --> 00:07:31,389
looks like reasonable stuff in here if

00:07:29,020 --> 00:07:34,169
we put some other stuff this is more

00:07:31,389 --> 00:07:37,779
astronomical stuff so we can see that

00:07:34,169 --> 00:07:40,349
here we have the blue - the red BP is a

00:07:37,779 --> 00:07:42,639
blue photometric red photomeatery

00:07:40,349 --> 00:07:43,960
plotted so plotted against the

00:07:42,639 --> 00:07:46,270
brightness jeez the brightness of the

00:07:43,960 --> 00:07:49,289
star and we can see one of the things

00:07:46,270 --> 00:07:52,539
interesting things about this data is

00:07:49,289 --> 00:07:55,379
these data is that we're trying to find

00:07:52,539 --> 00:07:58,150
the lines that we can put in this graph

00:07:55,379 --> 00:08:00,400
we could draw around saying this is a

00:07:58,150 --> 00:08:02,050
Cepheid this is an hourly rate this is

00:08:00,400 --> 00:08:05,830
the whole point of machine learning is

00:08:02,050 --> 00:08:07,810
to identify where we can draw a line to

00:08:05,830 --> 00:08:10,419
say we think this is a this type of

00:08:07,810 --> 00:08:12,189
style or that type of stuff and so this

00:08:10,419 --> 00:08:12,699
is for all the stars that are listed

00:08:12,189 --> 00:08:15,900
here

00:08:12,699 --> 00:08:15,900
this is just for

00:08:16,690 --> 00:08:20,570
this is just for the RR Lyrae there's

00:08:19,310 --> 00:08:22,160
different types of therefore there are

00:08:20,570 --> 00:08:24,380
four different types of I'll here you at

00:08:22,160 --> 00:08:26,750
least and this is the point-to-point

00:08:24,380 --> 00:08:28,370
slope of the peak-to-peak slope of the

00:08:26,750 --> 00:08:30,170
the variation of this and we've seen the

00:08:28,370 --> 00:08:31,970
Stars versus the the brightness and we

00:08:30,170 --> 00:08:33,740
can see some flat so we can't do

00:08:31,970 --> 00:08:35,570
anything with these but the all these

00:08:33,740 --> 00:08:37,250
stars here you know you say if the star

00:08:35,570 --> 00:08:40,280
is in this space on this graph it is

00:08:37,250 --> 00:08:41,720
probably one of the star of this type

00:08:40,280 --> 00:08:42,950
this is exactly the sort of thing we're

00:08:41,720 --> 00:08:46,310
trying to do with machine learning we're

00:08:42,950 --> 00:08:48,110
trying to identify how you know what

00:08:46,310 --> 00:08:49,490
what are the bits of the graphs of all

00:08:48,110 --> 00:08:51,770
these different graphs we get to the

00:08:49,490 --> 00:08:53,090
hundred and fifty data points allow us

00:08:51,770 --> 00:08:57,470
to identify which star

00:08:53,090 --> 00:08:59,450
we've got and sort of traditionally this

00:08:57,470 --> 00:09:01,820
has been done using statistical methods

00:08:59,450 --> 00:09:03,920
you know the guys are programming still

00:09:01,820 --> 00:09:05,120
method so logistic and logical

00:09:03,920 --> 00:09:08,420
regression and then loosely sort

00:09:05,120 --> 00:09:10,700
techniques state vector support vector

00:09:08,420 --> 00:09:13,370
machine machines or nearest neighbor

00:09:10,700 --> 00:09:14,960
k-means these sort of techniques to do

00:09:13,370 --> 00:09:17,680
this and we wanted to see if the machine

00:09:14,960 --> 00:09:21,380
learning would add a little bit of

00:09:17,680 --> 00:09:23,480
detail to what they're working on we

00:09:21,380 --> 00:09:26,570
needed to clean some of the data because

00:09:23,480 --> 00:09:29,660
there are certain other things that we

00:09:26,570 --> 00:09:31,910
don't need so the number of observations

00:09:29,660 --> 00:09:33,680
that we make of a star is purely

00:09:31,910 --> 00:09:35,060
coincidental where there's where they

00:09:33,680 --> 00:09:37,040
said where we parked the satellite it's

00:09:35,060 --> 00:09:39,140
not deterministic then obviously the

00:09:37,040 --> 00:09:41,450
name of the star it doesn't help that's

00:09:39,140 --> 00:09:44,900
just us so we we take those out and a

00:09:41,450 --> 00:09:48,710
couple of others at the moment we can't

00:09:44,900 --> 00:09:50,930
handle non missing data this is

00:09:48,710 --> 00:09:52,820
something we all get in the future but

00:09:50,930 --> 00:09:55,550
we can't handle missing data machine

00:09:52,820 --> 00:09:58,670
learning algorithms don't work with us

00:09:55,550 --> 00:10:00,950
in data generally so we have to remove

00:09:58,670 --> 00:10:03,410
any any roles that contain a missing

00:10:00,950 --> 00:10:05,570
value we also have to make sure that sum

00:10:03,410 --> 00:10:07,850
of all the values in numeric yeah it

00:10:05,570 --> 00:10:09,440
just turns out this data we get some of

00:10:07,850 --> 00:10:11,570
the values interpreted strings and we

00:10:09,440 --> 00:10:13,580
have to turn them into that and then the

00:10:11,570 --> 00:10:16,190
other most important point is that we

00:10:13,580 --> 00:10:20,240
normalized machine learning routines

00:10:16,190 --> 00:10:22,880
that we use using neural networks don't

00:10:20,240 --> 00:10:25,670
behave very nicely if the magnitude of

00:10:22,880 --> 00:10:28,280
the values is massively different so we

00:10:25,670 --> 00:10:29,720
try and bring all the values into the

00:10:28,280 --> 00:10:31,790
same scale so we read dimension

00:10:29,720 --> 00:10:33,769
every single one of the columns into

00:10:31,790 --> 00:10:36,139
let's say between North everything is

00:10:33,769 --> 00:10:37,009
valued between Norton 100 or minus 100

00:10:36,139 --> 00:10:39,439
and plus 100

00:10:37,009 --> 00:10:42,110
we just rescale everything because we're

00:10:39,439 --> 00:10:44,149
working from 1 million to a million in

00:10:42,110 --> 00:10:46,339
terms of the numbers were working with

00:10:44,149 --> 00:10:48,949
and in order for neural network to work

00:10:46,339 --> 00:10:51,769
we need to do this operation and these

00:10:48,949 --> 00:10:53,209
are difficult these are typical sorts of

00:10:51,769 --> 00:10:56,360
things you have to do with any machine

00:10:53,209 --> 00:10:57,560
learning project and this was the

00:10:56,360 --> 00:10:59,290
interesting part of the project there is

00:10:57,560 --> 00:11:01,879
one thing to go and get the book and

00:10:59,290 --> 00:11:03,709
copy the examples and do the thing and

00:11:01,879 --> 00:11:05,420
it works when you put your hands dirty

00:11:03,709 --> 00:11:08,449
on a project like this you'll learn a

00:11:05,420 --> 00:11:10,459
whole stuff little cover in the book and

00:11:08,449 --> 00:11:11,569
so the idea is what you know will it

00:11:10,459 --> 00:11:13,490
work now we're going to kick the tires

00:11:11,569 --> 00:11:14,600
and you know nobody never tried this

00:11:13,490 --> 00:11:17,029
there's very little work that's been

00:11:14,600 --> 00:11:18,829
done in this space until now Google had

00:11:17,029 --> 00:11:20,899
done some work about detecting

00:11:18,829 --> 00:11:23,269
exoplanets and they published a paper on

00:11:20,899 --> 00:11:25,670
on this but there was really nobody

00:11:23,269 --> 00:11:27,860
doing this sort of work when we started

00:11:25,670 --> 00:11:29,449
and so we had no idea whether we're

00:11:27,860 --> 00:11:34,009
going to have something that worked or

00:11:29,449 --> 00:11:35,839
not so the idea was that we were going

00:11:34,009 --> 00:11:38,029
to give it a go so the idea just a quick

00:11:35,839 --> 00:11:40,309
recap on the all networks the way this

00:11:38,029 --> 00:11:43,129
works is we give it a supervised

00:11:40,309 --> 00:11:44,019
learning we give it a data set we know

00:11:43,129 --> 00:11:46,790
the answers to

00:11:44,019 --> 00:11:48,680
we so would say this is this this data

00:11:46,790 --> 00:11:52,490
represents a Cepheid this data reference

00:11:48,680 --> 00:11:54,110
and our leary that sort of thing and we

00:11:52,490 --> 00:11:55,879
put it in here then the neural network

00:11:54,110 --> 00:11:58,790
changes its weights and its biases all

00:11:55,879 --> 00:12:00,879
on its own using this mechanism here and

00:11:58,790 --> 00:12:03,589
we see you know is the network

00:12:00,879 --> 00:12:06,620
predicting what it should be predicting

00:12:03,589 --> 00:12:08,410
ok and then we get this thing in what

00:12:06,620 --> 00:12:11,269
this network in the middle which is

00:12:08,410 --> 00:12:13,250
traditionally called a model and this

00:12:11,269 --> 00:12:15,410
becomes our application yeah so we this

00:12:13,250 --> 00:12:17,059
is our you know the point isn't to Train

00:12:15,410 --> 00:12:19,579
the point is to have the network and the

00:12:17,059 --> 00:12:22,100
model and the application building and

00:12:19,579 --> 00:12:25,040
we give it the new data and it's going

00:12:22,100 --> 00:12:28,370
to predict we have probability that this

00:12:25,040 --> 00:12:30,800
is a Cepheid or cat or a dog or

00:12:28,370 --> 00:12:33,379
cancerous or not cancerous you know it's

00:12:30,800 --> 00:12:37,160
exactly that sort of those sorts of

00:12:33,379 --> 00:12:39,680
things we're trying to do here so that's

00:12:37,160 --> 00:12:41,930
that's the idea and so

00:12:39,680 --> 00:12:44,170
basically the approach we took is you

00:12:41,930 --> 00:12:46,579
load the data which was a serious

00:12:44,170 --> 00:12:49,339
monstrous excel table which is a CSV

00:12:46,579 --> 00:12:50,629
format into a what's called panda's data

00:12:49,339 --> 00:12:53,059
frame which I'm going to mention a

00:12:50,629 --> 00:12:54,920
minute and then we delete all those

00:12:53,059 --> 00:12:57,800
columns we don't need you know the

00:12:54,920 --> 00:12:59,420
number of observations a name we

00:12:57,800 --> 00:13:02,869
normalize the values to between minus

00:12:59,420 --> 00:13:08,629
100 and 100 then we taken a very

00:13:02,869 --> 00:13:12,019
important step we take a sample of let's

00:13:08,629 --> 00:13:14,119
say 20% of the stars or 15% but 15% of

00:13:12,019 --> 00:13:15,769
every class you know if you think some

00:13:14,119 --> 00:13:18,769
classes are very few examples of in it

00:13:15,769 --> 00:13:20,869
you don't take 15% then in your test

00:13:18,769 --> 00:13:22,610
data it could be that you don't get any

00:13:20,869 --> 00:13:24,079
example in any examples of those so

00:13:22,610 --> 00:13:28,459
we're very careful about the type of

00:13:24,079 --> 00:13:33,259
sampling we make and then we're going to

00:13:28,459 --> 00:13:38,139
build a simple Network and see see what

00:13:33,259 --> 00:13:41,389
I see what happens all this we used on

00:13:38,139 --> 00:13:44,269
8922 using power III which is now called

00:13:41,389 --> 00:13:47,089
Watson machine learning accelerator very

00:13:44,269 --> 00:13:49,429
power I thought was a great name there's

00:13:47,089 --> 00:13:52,249
a real shame we change and it comes with

00:13:49,429 --> 00:13:56,889
is libraries pandas is the tool that we

00:13:52,249 --> 00:13:59,149
use to read exhale check cell s files

00:13:56,889 --> 00:14:03,220
CSV files those sorts of things and it

00:13:59,149 --> 00:14:06,529
produces a a table in memory of Python a

00:14:03,220 --> 00:14:08,990
table numpy is nupoc numerical routines

00:14:06,529 --> 00:14:10,009
for matrix multiplication and that sort

00:14:08,990 --> 00:14:12,259
of thing

00:14:10,009 --> 00:14:14,929
Kiera's is a framework which sits above

00:14:12,259 --> 00:14:17,029
tensorflow which allows us to very

00:14:14,929 --> 00:14:18,259
simply build a neural network in

00:14:17,029 --> 00:14:18,829
nationally a little bit of code to show

00:14:18,259 --> 00:14:20,839
you how it works

00:14:18,829 --> 00:14:24,170
and scikit-learn it provides some other

00:14:20,839 --> 00:14:27,410
tools and numerical methods which we

00:14:24,170 --> 00:14:30,319
rely on to all what's really nice about

00:14:27,410 --> 00:14:33,019
this though is once you have AI power I

00:14:30,319 --> 00:14:33,410
installed it's all there and it just

00:14:33,019 --> 00:14:38,149
works

00:14:33,410 --> 00:14:40,369
it is I think you know I think iBM has

00:14:38,149 --> 00:14:41,559
done a really great job of Packingtown

00:14:40,369 --> 00:14:43,519
stuff up

00:14:41,559 --> 00:14:45,589
everybody I've shown this to as a

00:14:43,519 --> 00:14:47,929
product always the first common techni

00:14:45,589 --> 00:14:48,890
make is easy to use yeah I mean it's

00:14:47,929 --> 00:14:50,930
fast

00:14:48,890 --> 00:14:52,940
but above all it's easy to use

00:14:50,930 --> 00:14:54,200
everybody's that's compared I'm not

00:14:52,940 --> 00:14:55,610
going to list the competitors but

00:14:54,200 --> 00:14:57,470
everybody knows what the competitors are

00:14:55,610 --> 00:15:00,710
the big differentiator of this platform

00:14:57,470 --> 00:15:01,910
is it's really simple to use and this

00:15:00,710 --> 00:15:04,040
stuff is hard yeah this stuff is

00:15:01,910 --> 00:15:08,050
complicated and having this having it

00:15:04,040 --> 00:15:10,700
installed like this is a real real plus

00:15:08,050 --> 00:15:12,410
so this is a code and this is Nancy's

00:15:10,700 --> 00:15:14,630
really house employees I just wanted to

00:15:12,410 --> 00:15:16,910
show how simple making making the neural

00:15:14,630 --> 00:15:17,810
network is with these libraries so the

00:15:16,910 --> 00:15:19,640
first two lines

00:15:17,810 --> 00:15:21,620
I read the features that's the hundred

00:15:19,640 --> 00:15:25,160
and fifty attribute and then I have the

00:15:21,620 --> 00:15:26,810
labels this RR Lyrae Cepheid the type of

00:15:25,160 --> 00:15:28,820
the class of star and I label I load

00:15:26,810 --> 00:15:30,320
those into two panda data frames and

00:15:28,820 --> 00:15:30,590
it's just a simple call that's as simple

00:15:30,320 --> 00:15:33,290
as that

00:15:30,590 --> 00:15:35,780
read the CSV unless it I missed this

00:15:33,290 --> 00:15:37,550
thing it's just one line but I didn't

00:15:35,780 --> 00:15:41,090
really go into it and then here's where

00:15:37,550 --> 00:15:44,300
I split I saved 20% off to make sure

00:15:41,090 --> 00:15:46,280
that my model I test my model you know I

00:15:44,300 --> 00:15:49,100
trained it with 80% of the data and then

00:15:46,280 --> 00:15:51,530
20% a test and see if it really works

00:15:49,100 --> 00:15:52,880
with data the model has never seen and

00:15:51,530 --> 00:15:54,590
this is where I just do a strain test

00:15:52,880 --> 00:15:57,380
split and we see the test size here is

00:15:54,590 --> 00:15:59,000
20% and I initialize the random State

00:15:57,380 --> 00:16:02,000
this is important if anybody wants to

00:15:59,000 --> 00:16:03,620
try and follow along is if you don't

00:16:02,000 --> 00:16:07,100
initialize the random state when you're

00:16:03,620 --> 00:16:08,720
doing this split then it means every

00:16:07,100 --> 00:16:10,190
time you run it you're going to get a

00:16:08,720 --> 00:16:13,970
different sample and if your network

00:16:10,190 --> 00:16:15,890
behaves better at one time you don't

00:16:13,970 --> 00:16:17,480
know it's because your date has changed

00:16:15,890 --> 00:16:19,580
or because your actual network is better

00:16:17,480 --> 00:16:22,010
so it's very important that when you're

00:16:19,580 --> 00:16:24,170
doing this stuff that you you're trained

00:16:22,010 --> 00:16:25,700
to split prints and presented with the

00:16:24,170 --> 00:16:27,290
same data when you're doing a test you

00:16:25,700 --> 00:16:28,010
can't have all the everything moving

00:16:27,290 --> 00:16:30,680
underneath you

00:16:28,010 --> 00:16:33,410
it's got to be constantly here's a

00:16:30,680 --> 00:16:35,120
scaling you know three lines three lines

00:16:33,410 --> 00:16:37,070
of code you know you think it's gonna be

00:16:35,120 --> 00:16:39,130
tough it's really straightforward to do

00:16:37,070 --> 00:16:40,880
this stuff no it's not hardly

00:16:39,130 --> 00:16:43,130
unfortunately much of this stuff is

00:16:40,880 --> 00:16:45,860
written by mathematicians and for us to

00:16:43,130 --> 00:16:47,510
implement it you know IT guys it's a bit

00:16:45,860 --> 00:16:48,740
hard to get our head round you know it's

00:16:47,510 --> 00:16:50,420
talking about partial differential

00:16:48,740 --> 00:16:51,880
equations and things we'd used to do at

00:16:50,420 --> 00:16:55,329
school and you know

00:16:51,880 --> 00:16:58,360
is it about and once you but once you

00:16:55,329 --> 00:16:59,230
get to the real core of the stuff it's

00:16:58,360 --> 00:17:01,360
not that hard

00:16:59,230 --> 00:17:02,850
yeah it's not that hard so this is the

00:17:01,360 --> 00:17:05,679
essentially the code the whole program

00:17:02,850 --> 00:17:06,819
for my test program was probably less

00:17:05,679 --> 00:17:09,189
than 50 lines of code

00:17:06,819 --> 00:17:13,299
it's an plus the imports for the

00:17:09,189 --> 00:17:14,799
libraries but it's not very much and

00:17:13,299 --> 00:17:17,919
then here's here we how I built the

00:17:14,799 --> 00:17:19,419
model since this is Kuras we create the

00:17:17,919 --> 00:17:20,410
model we say it's just his quencher and

00:17:19,419 --> 00:17:23,679
what were just means I'm putting one

00:17:20,410 --> 00:17:25,569
layer after the next I create one dense

00:17:23,679 --> 00:17:28,179
and this first time it's going to create

00:17:25,569 --> 00:17:29,679
an input layer and a hidden layer which

00:17:28,179 --> 00:17:32,350
I'll talk to in a minute another hidden

00:17:29,679 --> 00:17:34,450
layer and an output layer and these are

00:17:32,350 --> 00:17:37,570
just the magical incantations you've got

00:17:34,450 --> 00:17:40,870
to put on the code and then we just

00:17:37,570 --> 00:17:43,539
compile it so now I have a network yeah

00:17:40,870 --> 00:17:46,690
and that work ready to Train and that's

00:17:43,539 --> 00:17:48,880
just really four lines of code and this

00:17:46,690 --> 00:17:50,980
has given me this this is basic

00:17:48,880 --> 00:17:54,580
architecture of my model this is a 150

00:17:50,980 --> 00:17:56,169
attributes going into my model in the

00:17:54,580 --> 00:17:58,510
input layer I have two hidden layers and

00:17:56,169 --> 00:18:00,669
then I have my four star classes that I

00:17:58,510 --> 00:18:03,130
was testing at the time so Cepheid RR

00:18:00,669 --> 00:18:04,270
Lyrae some like whatever it was there

00:18:03,130 --> 00:18:05,679
those classes that come out to the end

00:18:04,270 --> 00:18:08,950
and that's the basic architecture of my

00:18:05,679 --> 00:18:12,850
model and so once I have that I'm gonna

00:18:08,950 --> 00:18:14,169
train it so I give it the the pandas

00:18:12,850 --> 00:18:16,630
array that I was created Earl you know

00:18:14,169 --> 00:18:19,030
that with all the with all my attributes

00:18:16,630 --> 00:18:20,770
I give it the answers which is the you

00:18:19,030 --> 00:18:23,380
know the classes for that and it just

00:18:20,770 --> 00:18:25,360
goes away four thousand times it puts

00:18:23,380 --> 00:18:28,990
all the weights up and it spits out a

00:18:25,360 --> 00:18:31,720
model at the end and this is really fast

00:18:28,990 --> 00:18:33,220
on a power system this is you know I've

00:18:31,720 --> 00:18:35,470
talked this to the end but this makes a

00:18:33,220 --> 00:18:40,720
huge difference this is fast this is you

00:18:35,470 --> 00:18:44,320
know a minute it's really quick it gives

00:18:40,720 --> 00:18:49,120
us the model we can evaluate to say how

00:18:44,320 --> 00:18:53,020
did you do and and that's it that's it

00:18:49,120 --> 00:18:53,620
and piece of code and we came out first

00:18:53,020 --> 00:18:57,419
go

00:18:53,620 --> 00:19:00,820
the first go to two two hidden layers

00:18:57,419 --> 00:19:03,030
80% accuracy accuracy is a is a measure

00:19:00,820 --> 00:19:03,030
of

00:19:03,990 --> 00:19:07,810
how the models behaved and there's

00:19:05,980 --> 00:19:09,910
several types of measurement of how well

00:19:07,810 --> 00:19:11,680
your models behaving so it's the true

00:19:09,910 --> 00:19:13,540
positives and the true negatives over

00:19:11,680 --> 00:19:17,380
the total of the things were estimating

00:19:13,540 --> 00:19:20,140
and this is it within you know a few

00:19:17,380 --> 00:19:22,540
percent of the best results that the

00:19:20,140 --> 00:19:25,330
guys in Geneva were getting with Ellen

00:19:22,540 --> 00:19:27,550
Europe linear models logistic models so

00:19:25,330 --> 00:19:29,200
we we're onto a winner you know I think

00:19:27,550 --> 00:19:32,320
we're really onto a winner with this we

00:19:29,200 --> 00:19:35,230
you know first go we so we think let's

00:19:32,320 --> 00:19:38,890
let's make a little more effort and see

00:19:35,230 --> 00:19:40,810
what happens so how do we refine it the

00:19:38,890 --> 00:19:42,280
first problem we encountered was we

00:19:40,810 --> 00:19:43,990
think we just let the model run a bit

00:19:42,280 --> 00:19:45,880
more and we run into this problem of

00:19:43,990 --> 00:19:49,000
overfitting so here we have a few data

00:19:45,880 --> 00:19:50,890
points which I think most of us can see

00:19:49,000 --> 00:19:52,360
is a pretty straight line you know

00:19:50,890 --> 00:19:54,040
that's that's the that's the line one

00:19:52,360 --> 00:19:56,320
yeah it's the equation of the line on

00:19:54,040 --> 00:19:58,390
top it's one type of style underneath

00:19:56,320 --> 00:20:00,910
this is another type of style when you

00:19:58,390 --> 00:20:02,950
and your model too much you get this it

00:20:00,910 --> 00:20:04,330
tries to minimize the error and you'll

00:20:02,950 --> 00:20:06,430
put the line through all the points and

00:20:04,330 --> 00:20:09,670
yeah something funky like this and what

00:20:06,430 --> 00:20:12,820
you see is that the the accuracy for

00:20:09,670 --> 00:20:14,860
your training data gets great and then

00:20:12,820 --> 00:20:16,240
all of a sudden for your test data it

00:20:14,860 --> 00:20:18,340
starts to diverge that's getting worse

00:20:16,240 --> 00:20:20,620
and that's the sign of overfitting and

00:20:18,340 --> 00:20:22,150
there's one really easy technique I

00:20:20,620 --> 00:20:24,310
don't understand how it works it's a

00:20:22,150 --> 00:20:27,030
complete mystery you know that is you

00:20:24,310 --> 00:20:29,320
put in a layer that just throws 50% away

00:20:27,030 --> 00:20:31,350
50% of the results you just saw him away

00:20:29,320 --> 00:20:33,460
or twenty percent or eighty percent and

00:20:31,350 --> 00:20:36,640
for some magical reason either I don't

00:20:33,460 --> 00:20:37,960
understand the math but it works and it

00:20:36,640 --> 00:20:40,950
stops the overfitting there are other

00:20:37,960 --> 00:20:43,660
techniques but this one's an easy one

00:20:40,950 --> 00:20:45,490
and so we have all these parameters we

00:20:43,660 --> 00:20:48,040
have all these parameters you know in

00:20:45,490 --> 00:20:49,450
the network you know what's the shake

00:20:48,040 --> 00:20:53,470
how many hidden layers do I need how

00:20:49,450 --> 00:20:57,190
wide should my my layers each be you

00:20:53,470 --> 00:20:59,380
know which is the activation function at

00:20:57,190 --> 00:21:03,970
the end of every output of every neuron

00:20:59,380 --> 00:21:05,110
we try and limit the value slightly that

00:21:03,970 --> 00:21:08,380
comes out we can either limited between

00:21:05,110 --> 00:21:10,990
0 and 1 or minus 1 and 1 well we tighten

00:21:08,380 --> 00:21:12,700
it keeps the network stable so we have

00:21:10,990 --> 00:21:13,200
this activation function that yeah out

00:21:12,700 --> 00:21:16,019
of each

00:21:13,200 --> 00:21:18,240
we could modulate this is learning rate

00:21:16,019 --> 00:21:20,279
we try and minimize as we're trying to

00:21:18,240 --> 00:21:22,409
get to the best solution we take a step

00:21:20,279 --> 00:21:24,299
forward down down the slope to minimize

00:21:22,409 --> 00:21:27,000
the error and this step can be a big

00:21:24,299 --> 00:21:29,610
step and we by risk of going past the

00:21:27,000 --> 00:21:32,669
the optimal or small step and it takes

00:21:29,610 --> 00:21:34,470
ages to get there there's certain you

00:21:32,669 --> 00:21:35,850
know I talked about accuracy decision

00:21:34,470 --> 00:21:38,250
the different ways of measuring the

00:21:35,850 --> 00:21:41,070
success of the network which optimizer

00:21:38,250 --> 00:21:42,870
we use do we do I put one star in at the

00:21:41,070 --> 00:21:44,010
time to put all my stars in the time and

00:21:42,870 --> 00:21:45,210
then then do when it work

00:21:44,010 --> 00:21:48,960
there's a whole bunch of different

00:21:45,210 --> 00:21:51,929
things we can tune the network during

00:21:48,960 --> 00:21:54,809
the training phase so here I have seven

00:21:51,929 --> 00:21:57,029
you know there's seven things and these

00:21:54,809 --> 00:21:58,470
are just ordinary buttons you everybody

00:21:57,029 --> 00:22:01,549
has to decide what they're going to put

00:21:58,470 --> 00:22:03,990
in these things so if we've got seven

00:22:01,549 --> 00:22:06,899
things we can change and if you just

00:22:03,990 --> 00:22:09,990
gives maybe ten values for each one of

00:22:06,899 --> 00:22:13,799
those that's ten million ten million

00:22:09,990 --> 00:22:15,799
combinations you have to test it's just

00:22:13,799 --> 00:22:19,110
huge the number of things you have to do

00:22:15,799 --> 00:22:20,730
you know it is a you know we're not

00:22:19,110 --> 00:22:22,590
really an official industry you know we

00:22:20,730 --> 00:22:23,760
waste a lot of time and a lot of time

00:22:22,590 --> 00:22:26,730
sitting idle and doing things like

00:22:23,760 --> 00:22:29,010
machine learning takes the cake here is

00:22:26,730 --> 00:22:30,600
really really inefficient way of doing

00:22:29,010 --> 00:22:33,559
things because the way things are done

00:22:30,600 --> 00:22:36,269
so in order to find the best network

00:22:33,559 --> 00:22:37,860
that I want even ignoring you know I

00:22:36,269 --> 00:22:41,880
decide that my network it's going to be

00:22:37,860 --> 00:22:43,649
five deep and 150 wide even for a one

00:22:41,880 --> 00:22:46,049
architecture I still have 10 million

00:22:43,649 --> 00:22:48,630
things to test you know with just 10 if

00:22:46,049 --> 00:22:53,269
I had 20 obviously getting way way worse

00:22:48,630 --> 00:22:55,559
and so this is why having a fast machine

00:22:53,269 --> 00:22:59,639
makes all the difference if you can get

00:22:55,559 --> 00:23:01,710
your batch to run in five seconds or 10

00:22:59,639 --> 00:23:02,789
seconds yeah you can run ten million you

00:23:01,710 --> 00:23:06,090
know you go out you leaving for the

00:23:02,789 --> 00:23:09,419
weekend and come back and you're okay if

00:23:06,090 --> 00:23:13,409
you if it takes two hours yeah it's a

00:23:09,419 --> 00:23:15,419
non-starter yeah so this is why fast

00:23:13,409 --> 00:23:17,039
really is important in this in this

00:23:15,419 --> 00:23:20,700
world and having the accelerators it is

00:23:17,039 --> 00:23:23,309
a really really big deal and the the for

00:23:20,700 --> 00:23:25,080
GPUs you getting in the AC 922 - he's a

00:23:23,309 --> 00:23:25,779
really great it's a fantastic box it

00:23:25,080 --> 00:23:28,960
really isn't

00:23:25,779 --> 00:23:31,179
they want to work with so how do we do

00:23:28,960 --> 00:23:32,950
this you know can it takes a you know

00:23:31,179 --> 00:23:34,719
even even with a fast box doing 10

00:23:32,950 --> 00:23:38,340
million and then for every networking

00:23:34,719 --> 00:23:41,489
architecture takes a while and so

00:23:38,340 --> 00:23:45,219
scikit-learn provides some tools which

00:23:41,489 --> 00:23:47,679
to automate now you don't change it by

00:23:45,219 --> 00:23:49,059
hand and and then run it and then come

00:23:47,679 --> 00:23:50,649
back again to its later and change

00:23:49,059 --> 00:23:53,859
something there's some tools to automate

00:23:50,649 --> 00:23:55,629
these this thing and the first one we

00:23:53,859 --> 00:23:57,159
use is a randomized search so we just

00:23:55,629 --> 00:23:58,269
give it the list of the variables we

00:23:57,159 --> 00:24:00,849
want and it's going to choose a certain

00:23:58,269 --> 00:24:02,109
number of them and the second one is a

00:24:00,849 --> 00:24:05,019
systematic search with a grid search

00:24:02,109 --> 00:24:06,999
which will do all the 10 million so our

00:24:05,019 --> 00:24:09,190
approach was to wrap the model that I

00:24:06,999 --> 00:24:11,739
produced initially so that scikit-learn

00:24:09,190 --> 00:24:14,349
can use this in completely automated we

00:24:11,739 --> 00:24:16,269
did a random search initially the best

00:24:14,349 --> 00:24:18,249
best two results we got on the random

00:24:16,269 --> 00:24:20,109
search we did a grid search locally

00:24:18,249 --> 00:24:22,749
around those two low points and we chose

00:24:20,109 --> 00:24:24,249
the network which was optimized for that

00:24:22,749 --> 00:24:26,320
and it's really quite simple to do this

00:24:24,249 --> 00:24:29,649
stuff you know you're passing a

00:24:26,320 --> 00:24:31,450
dictionary so it's a key/value thing and

00:24:29,649 --> 00:24:33,909
the key is the name of the parameter we

00:24:31,450 --> 00:24:36,009
want to modify and the dig and the value

00:24:33,909 --> 00:24:37,719
of that against that key is that is the

00:24:36,009 --> 00:24:41,049
list of values we want to test so it

00:24:37,719 --> 00:24:42,309
could be you know the learning rate we

00:24:41,049 --> 00:24:43,779
could say in the learning rate the key

00:24:42,309 --> 00:24:47,200
is learning rate and the value is the

00:24:43,779 --> 00:24:51,729
list from point zero zero one to two

00:24:47,200 --> 00:24:53,349
point zero one that's all thing you just

00:24:51,729 --> 00:24:54,999
passing you've pass it in and you go

00:24:53,349 --> 00:24:56,710
home in the evening you come back the

00:24:54,999 --> 00:24:57,009
next day and see what it's see what it

00:24:56,710 --> 00:24:59,950
says

00:24:57,009 --> 00:25:01,389
and so we use the random search to

00:24:59,950 --> 00:25:05,639
quickly find a minima and then we did a

00:25:01,389 --> 00:25:05,639
grid search around around the minimo and

00:25:05,729 --> 00:25:09,129
we've got some starting some great

00:25:07,599 --> 00:25:10,029
results but before that I just talked a

00:25:09,129 --> 00:25:13,029
little bit about this on the other

00:25:10,029 --> 00:25:14,349
optimization we made first so as I said

00:25:13,029 --> 00:25:16,239
that we get a lot of statistics about

00:25:14,349 --> 00:25:17,900
statistics so we had this thing about

00:25:16,239 --> 00:25:19,760
kurtosis

00:25:17,900 --> 00:25:21,920
Taylan in the number of the attributes

00:25:19,760 --> 00:25:23,420
and they you know you had them

00:25:21,920 --> 00:25:25,310
standardized read them biased and

00:25:23,420 --> 00:25:28,400
weighted and unweighted these were all

00:25:25,310 --> 00:25:29,990
essentially the same value just shifted

00:25:28,400 --> 00:25:32,630
a little bit left to right or scaled a

00:25:29,990 --> 00:25:34,340
bit left or right so it was Acorah

00:25:32,630 --> 00:25:37,520
Latian between a certain number of the

00:25:34,340 --> 00:25:38,990
columns that we have in in the data and

00:25:37,520 --> 00:25:41,300
there was this first skewness and there

00:25:38,990 --> 00:25:44,540
was this for the means and you know the

00:25:41,300 --> 00:25:47,660
loads of this stuff so what we can do

00:25:44,540 --> 00:25:49,490
the psychic learn tool set and allows us

00:25:47,660 --> 00:25:51,230
to identify eight columns that are

00:25:49,490 --> 00:25:53,930
correlated and we could say anything

00:25:51,230 --> 00:25:55,820
that's correlated higher than 95% we

00:25:53,930 --> 00:25:57,410
just keep one of them and the rest we

00:25:55,820 --> 00:26:00,100
throw away and this is important because

00:25:57,410 --> 00:26:03,710
all this stuff is matrix multiplication

00:26:00,100 --> 00:26:06,440
and matrix multiplication is oh and

00:26:03,710 --> 00:26:09,080
cubed you double the size of the matrix

00:26:06,440 --> 00:26:11,870
the time of execution it goes up by

00:26:09,080 --> 00:26:15,080
eight yeah so keeping the matrix size

00:26:11,870 --> 00:26:17,570
down it helps on enormous in the space I

00:26:15,080 --> 00:26:19,010
mean we have 150 parameters people who

00:26:17,570 --> 00:26:20,510
are doing images they're talking about

00:26:19,010 --> 00:26:22,670
thousands or tens of thousands of

00:26:20,510 --> 00:26:24,740
actually groups for the image so we're

00:26:22,670 --> 00:26:25,940
already not very big but this really

00:26:24,740 --> 00:26:31,100
doesn't make a difference if you want to

00:26:25,940 --> 00:26:32,870
loop through a big search so the next

00:26:31,100 --> 00:26:34,880
step we made was remove the core idea

00:26:32,870 --> 00:26:37,880
features and then there's a second

00:26:34,880 --> 00:26:40,700
technique which scikit-learn provides

00:26:37,880 --> 00:26:41,870
which is primary component analysis PCA

00:26:40,700 --> 00:26:45,470
I won't go into the details but

00:26:41,870 --> 00:26:46,820
essentially you merge a large number of

00:26:45,470 --> 00:26:49,870
columns into a little smaller number of

00:26:46,820 --> 00:26:52,430
columns keeping the same variance okay's

00:26:49,870 --> 00:26:55,910
mathematics I frankly I don't understand

00:26:52,430 --> 00:26:57,890
but it works it works reasonably well so

00:26:55,910 --> 00:26:59,720
and it's just one call again you using

00:26:57,890 --> 00:27:04,340
PCA it's really straightforward to use

00:26:59,720 --> 00:27:05,510
in there in the in the Python code and

00:27:04,340 --> 00:27:09,590
this is what we got we've got an

00:27:05,510 --> 00:27:11,330
accuracy of 94% out of this oh this box

00:27:09,590 --> 00:27:13,850
and this is what we call a confusion

00:27:11,330 --> 00:27:15,410
matrix based on the percentages so the

00:27:13,850 --> 00:27:17,510
idea this is this is what we predict

00:27:15,410 --> 00:27:19,580
this is what it is this was so this was

00:27:17,510 --> 00:27:21,950
a speak speak see this is a Cepheid this

00:27:19,580 --> 00:27:24,889
is a mirror this is our

00:27:21,950 --> 00:27:26,659
one of the classes and these are all in

00:27:24,889 --> 00:27:27,679
percentages so the idea is this is what

00:27:26,659 --> 00:27:29,659
the true was and this what we were

00:27:27,679 --> 00:27:32,240
predicted so the idea is that we get 100

00:27:29,659 --> 00:27:34,490
down this axis here and zero everywhere

00:27:32,240 --> 00:27:37,789
else so you can see some how the model

00:27:34,490 --> 00:27:39,740
is behaving and this is against the test

00:27:37,789 --> 00:27:41,269
data this is not against the training

00:27:39,740 --> 00:27:44,240
data this is against our test data and

00:27:41,269 --> 00:27:46,010
you can see we got a we've got some

00:27:44,240 --> 00:27:47,990
improvements to make here we can maybe

00:27:46,010 --> 00:27:50,210
refine the model a little bit further on

00:27:47,990 --> 00:27:54,529
but you know here we're doing really

00:27:50,210 --> 00:27:56,809
really great you know and this is this

00:27:54,529 --> 00:28:01,549
is at least as good as the traditional

00:27:56,809 --> 00:28:09,409
methods have shown to be capable so it's

00:28:01,549 --> 00:28:12,669
a very very promising start as I said we

00:28:09,409 --> 00:28:14,779
did the timely execution diminishes with

00:28:12,669 --> 00:28:16,610
depend on technique with you so this is

00:28:14,779 --> 00:28:19,880
removing the correlated columns see

00:28:16,610 --> 00:28:23,169
we're at the best result we got 94% and

00:28:19,880 --> 00:28:26,809
we got a 15% reduction in time the PCA

00:28:23,169 --> 00:28:29,840
we got a much worse result it's same

00:28:26,809 --> 00:28:33,529
time so we abandoned the PCA approach

00:28:29,840 --> 00:28:35,210
and we now concentrate on that on on

00:28:33,529 --> 00:28:37,279
testing you know do we do use

00:28:35,210 --> 00:28:38,059
ninety-five percent correlation 99

00:28:37,279 --> 00:28:40,039
percent correlation

00:28:38,059 --> 00:28:41,389
all of this stuff is black magic you

00:28:40,039 --> 00:28:46,000
know there's no real science behind it

00:28:41,389 --> 00:28:48,860
is all guesstimate some just intuition

00:28:46,000 --> 00:28:51,559
so we've got a great result you know we

00:28:48,860 --> 00:28:53,299
started to talk to to Geneva again about

00:28:51,559 --> 00:28:55,250
about results and what we can do next

00:28:53,299 --> 00:28:58,820
and this is the things that these are

00:28:55,250 --> 00:29:00,769
things are coming up we used 150

00:28:58,820 --> 00:29:04,730
attributes the satellite also measures

00:29:00,769 --> 00:29:07,700
the the weather the weather going from

00:29:04,730 --> 00:29:10,880
measuring from red to blue 60 color

00:29:07,700 --> 00:29:13,909
bands and how these color bands change

00:29:10,880 --> 00:29:16,130
over time so rather than just being we

00:29:13,909 --> 00:29:18,139
just got dating it's here it's going

00:29:16,130 --> 00:29:19,490
this fast it's this bright and that's

00:29:18,139 --> 00:29:21,500
just happened to be the time it measured

00:29:19,490 --> 00:29:24,529
it you know we should got here we've got

00:29:21,500 --> 00:29:26,000
things we get it get a graph of the

00:29:24,529 --> 00:29:28,460
shape of how things are changing over

00:29:26,000 --> 00:29:30,860
time as measured by the satellite and so

00:29:28,460 --> 00:29:32,120
we can do you know you can imagine we

00:29:30,860 --> 00:29:34,039
look at the shape of the curve and say

00:29:32,120 --> 00:29:35,150
the shape is characteristic of a Cepheid

00:29:34,039 --> 00:29:37,360
or its characteristic

00:29:35,150 --> 00:29:41,210
Laoghaire and so we're going to try and

00:29:37,360 --> 00:29:43,070
use recurrent neural networks which is a

00:29:41,210 --> 00:29:45,410
technique which is used everybody uses

00:29:43,070 --> 00:29:47,630
it it's the technique you use on your on

00:29:45,410 --> 00:29:49,070
your telephone for a text prediction you

00:29:47,630 --> 00:29:50,300
know when you look at what's gone in the

00:29:49,070 --> 00:29:53,030
past you can guess what's going to

00:29:50,300 --> 00:29:54,260
happen in the future there aren't that

00:29:53,030 --> 00:29:56,809
many applicants so it's used for

00:29:54,260 --> 00:29:58,010
prediction there aren't that many

00:29:56,809 --> 00:30:00,350
applications that use it for

00:29:58,010 --> 00:30:02,360
classification so we again we we I

00:30:00,350 --> 00:30:04,580
haven't found much everything you know

00:30:02,360 --> 00:30:06,350
there isn't much science that I've which

00:30:04,580 --> 00:30:07,340
I'm familiar using this for

00:30:06,350 --> 00:30:09,200
classification the way we're going to

00:30:07,340 --> 00:30:11,780
use it so maybe it won't work yeah but

00:30:09,200 --> 00:30:13,100
this is our strategies to to look at it

00:30:11,780 --> 00:30:14,510
you know if the worst comes to worst we

00:30:13,100 --> 00:30:16,700
just Clinton the graph and give it to an

00:30:14,510 --> 00:30:19,490
image recognition thing and say is it in

00:30:16,700 --> 00:30:20,750
our area or not and then we have to deal

00:30:19,490 --> 00:30:22,940
with the missing data so there are lots

00:30:20,750 --> 00:30:25,610
of techniques here we could just take an

00:30:22,940 --> 00:30:28,580
average or we can you know there's some

00:30:25,610 --> 00:30:30,470
things we can do with then we need real

00:30:28,580 --> 00:30:33,860
help from the scientists you know that

00:30:30,470 --> 00:30:37,059
we can't I'm an IT guy I can't tell you

00:30:33,860 --> 00:30:39,110
what number has to go in the hole so

00:30:37,059 --> 00:30:41,350
we're going to need some help with that

00:30:39,110 --> 00:30:43,520
so this is the work we're gonna do and

00:30:41,350 --> 00:30:45,740
I'm quite confident that you know we're

00:30:43,520 --> 00:30:50,330
going to get we're gonna improve on this

00:30:45,740 --> 00:30:53,390
so 95% the takeaways for this was that

00:30:50,330 --> 00:30:55,220
as I said having a fast and fast server

00:30:53,390 --> 00:30:57,290
and makes a huge difference he's not

00:30:55,220 --> 00:31:00,500
just fast it's just fast on the way it

00:30:57,290 --> 00:31:02,330
runs but also fasting in the way we use

00:31:00,500 --> 00:31:06,050
it you know it's not hard to write this

00:31:02,330 --> 00:31:08,780
program paradise' installed it is really

00:31:06,050 --> 00:31:10,130
straightforward to use and anybody who's

00:31:08,780 --> 00:31:12,110
a little bit familiar with Python and

00:31:10,130 --> 00:31:15,830
the numerical libraries can make a use

00:31:12,110 --> 00:31:18,020
of it this was in the second line we can

00:31:15,830 --> 00:31:19,880
have DDL so this distributed deep deep

00:31:18,020 --> 00:31:23,860
learning is available off-the-shelf

00:31:19,880 --> 00:31:26,840
across the four GPU so we can spread the

00:31:23,860 --> 00:31:28,280
learning process across for cheap use to

00:31:26,840 --> 00:31:29,750
further excite I have actually tried

00:31:28,280 --> 00:31:33,020
this this is another piece of future

00:31:29,750 --> 00:31:34,790
work we have to do as you saw the code

00:31:33,020 --> 00:31:37,010
is really straightforward to build a

00:31:34,790 --> 00:31:38,809
neural network it's not that hard with

00:31:37,010 --> 00:31:40,850
kiosk URIs are really fantastic library

00:31:38,809 --> 00:31:42,260
especially compared to tensorflow

00:31:40,850 --> 00:31:44,750
tensorflow which is the underlying

00:31:42,260 --> 00:31:46,360
library is much more as an assembler

00:31:44,750 --> 00:31:51,770
style

00:31:46,360 --> 00:31:54,110
building your own network things key Rus

00:31:51,770 --> 00:31:56,000
is relative straightforward and you know

00:31:54,110 --> 00:31:57,550
the really big takeaway is that you know

00:31:56,000 --> 00:31:59,840
we've got a great result neural networks

00:31:57,550 --> 00:32:03,140
fantastic approach to doing this work

00:31:59,840 --> 00:32:05,330
and I would really like to talk to a ESA

00:32:03,140 --> 00:32:07,130
to talk about doing it with the other

00:32:05,330 --> 00:32:08,470
stars because I think we've got you know

00:32:07,130 --> 00:32:10,520
we have the variable stars which

00:32:08,470 --> 00:32:12,920
represent a certain percentage of the

00:32:10,520 --> 00:32:15,800
population but the bulk of the

00:32:12,920 --> 00:32:18,860
population of the stars are non

00:32:15,800 --> 00:32:22,790
variables I think we can probably do

00:32:18,860 --> 00:32:25,630
something there too so that's so it's

00:32:22,790 --> 00:32:25,630
rank questions

00:32:30,550 --> 00:32:34,580
they are extremely instant you know

00:32:32,990 --> 00:32:37,310
they're very enthusiastic about it

00:32:34,580 --> 00:32:38,420
they've tried a little bit with I don't

00:32:37,310 --> 00:32:40,870
people are familiar with the h2o

00:32:38,420 --> 00:32:43,490
framework which is a Java framework

00:32:40,870 --> 00:32:45,110
they've tried a little bit this mostly

00:32:43,490 --> 00:32:48,920
numerical routines rather than machine

00:32:45,110 --> 00:32:51,050
learning but they were very pleased with

00:32:48,920 --> 00:32:52,610
the with our initial results yeah then

00:32:51,050 --> 00:32:55,760
we get a little bit of traction inside

00:32:52,610 --> 00:32:57,500
ESA even that you know we were coming

00:32:55,760 --> 00:32:59,900
just on the gaia project which is this

00:32:57,500 --> 00:33:01,640
satellite but even with the ESA is

00:32:59,900 --> 00:33:03,740
globally there's a little bit of

00:33:01,640 --> 00:33:08,000
awareness about what the work that we've

00:33:03,740 --> 00:33:10,430
done here and some interest so you know

00:33:08,000 --> 00:33:13,190
I'm a little bit I'm hopeful you know

00:33:10,430 --> 00:33:15,710
when nobody's signed and a lot of line

00:33:13,190 --> 00:33:17,890
just yet but I'm hopeful that we can we

00:33:15,710 --> 00:33:20,960
showing some promising results and

00:33:17,890 --> 00:33:22,550
they're a great team you know working

00:33:20,960 --> 00:33:24,590
with his scientists you know it takes us

00:33:22,550 --> 00:33:26,030
out of our normal job you know you I'm

00:33:24,590 --> 00:33:28,490
usually working I guess like lots of

00:33:26,030 --> 00:33:29,990
people with the Insurance Commission and

00:33:28,490 --> 00:33:31,640
you know and banks and government

00:33:29,990 --> 00:33:33,020
buildings and go and see these guys you

00:33:31,640 --> 00:33:34,610
know it's a breath of fresh air it's

00:33:33,020 --> 00:33:36,890
fantastic working with these guys always

00:33:34,610 --> 00:33:39,260
very open keen to and always pushing

00:33:36,890 --> 00:33:40,760
it's been fantastic but and they're very

00:33:39,260 --> 00:33:41,600
enthusiastic about our initial results

00:33:40,760 --> 00:33:44,030
here and they're going to provide us

00:33:41,600 --> 00:33:46,460
with more data unfortunately the data we

00:33:44,030 --> 00:33:49,280
have is it is not yet released so that's

00:33:46,460 --> 00:33:50,480
why I can't show you the real numbers

00:33:49,280 --> 00:33:53,360
but the

00:33:50,480 --> 00:33:56,480
you need to see to understand what we've

00:33:53,360 --> 00:34:06,730
done but yeah we're in fairly good shape

00:33:56,480 --> 00:34:10,580
here I think so

00:34:06,730 --> 00:34:12,020
so the very first the very crispy now

00:34:10,580 --> 00:34:14,540
yeah it's nothing the first the first

00:34:12,020 --> 00:34:18,050
set of stars we got we had 6,000 stars

00:34:14,540 --> 00:34:20,359
which is of which you know we had

00:34:18,050 --> 00:34:21,919
missing data from from about 500 and

00:34:20,359 --> 00:34:24,260
then I kept 20% so that was another

00:34:21,919 --> 00:34:26,119
thousand so we've trained on about 5,000

00:34:24,260 --> 00:34:28,700
stars four and a half thousand stars and

00:34:26,119 --> 00:34:29,720
the latest group of data we're gonna get

00:34:28,700 --> 00:34:33,260
is going to be in the order of three

00:34:29,720 --> 00:34:34,970
hundred thousand stars so it's not you

00:34:33,260 --> 00:34:37,970
know we're a long way from 1.7 billion

00:34:34,970 --> 00:34:39,830
yeah but I'm confident that when the

00:34:37,970 --> 00:34:44,679
rest of the data come we're going to be

00:34:39,830 --> 00:34:50,230
in we could be able to absorb it yes I

00:34:44,679 --> 00:34:50,230
know no no please

00:34:51,260 --> 00:34:54,469
[Music]

00:34:57,550 --> 00:35:05,270
okay so let's okay so the European Space

00:35:01,400 --> 00:35:10,400
Agency sent a satellite Gaea into space

00:35:05,270 --> 00:35:12,320
in 2013 to create a map of our own

00:35:10,400 --> 00:35:16,760
galaxy the Milky Way it's the most

00:35:12,320 --> 00:35:20,170
accurate map we got of our galaxy and in

00:35:16,760 --> 00:35:23,660
the last data release they identified

00:35:20,170 --> 00:35:24,770
1.7 billion light sources and I included

00:35:23,660 --> 00:35:26,330
some of the things that are floating

00:35:24,770 --> 00:35:28,250
around the Sun the asteroids and things

00:35:26,330 --> 00:35:31,820
like that and it's not just stars but it

00:35:28,250 --> 00:35:33,980
is mostly stars and we have to you know

00:35:31,820 --> 00:35:35,359
you have to what the idea is that the

00:35:33,980 --> 00:35:37,100
space aging is not just an idea of

00:35:35,359 --> 00:35:38,060
saying here's a star you'd say this star

00:35:37,100 --> 00:35:40,670
is a star

00:35:38,060 --> 00:35:43,100
you know it's a binary stars a pulsar or

00:35:40,670 --> 00:35:45,740
it's these well known one that she

00:35:43,100 --> 00:35:48,290
called Cepheid stars we have to classify

00:35:45,740 --> 00:35:50,540
the same way as you'll classify animals

00:35:48,290 --> 00:35:53,510
you know between reptiles and reptiles

00:35:50,540 --> 00:35:55,520
and mammals and underneath them the

00:35:53,510 --> 00:35:57,109
mammals they have you know like I don't

00:35:55,520 --> 00:35:58,670
know some beneath mammals but you know

00:35:57,109 --> 00:36:01,160
there's man and dog and captain what

00:35:58,670 --> 00:36:05,090
them towers you know the we classify the

00:36:01,160 --> 00:36:06,740
stars in the same sort of hierarchy as

00:36:05,090 --> 00:36:08,090
we've classify animals and plants and

00:36:06,740 --> 00:36:10,940
just about everything else in the world

00:36:08,090 --> 00:36:13,130
yes so our idea here is to build a

00:36:10,940 --> 00:36:17,600
neural network which will take the raw

00:36:13,130 --> 00:36:21,200
data from the satellite and say this

00:36:17,600 --> 00:36:26,770
star is a Cepheid or an RR Lyrae with a

00:36:21,200 --> 00:36:26,770
confidence of 80% or 90% of muster

00:36:30,980 --> 00:36:35,860
[Music]

00:36:33,390 --> 00:36:38,700
okay if there's any questions afterwards

00:36:35,860 --> 00:36:38,700

YouTube URL: https://www.youtube.com/watch?v=4xfnTc5uLUk


