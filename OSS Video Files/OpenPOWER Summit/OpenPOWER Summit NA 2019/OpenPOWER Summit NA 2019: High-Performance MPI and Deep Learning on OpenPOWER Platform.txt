Title: OpenPOWER Summit NA 2019: High-Performance MPI and Deep Learning on OpenPOWER Platform
Publication date: 2019-08-20
Playlist: OpenPOWER Summit NA 2019
Description: 
	Presented by Ching-Hsiang Chu, X-ScaleSolutions

This talk will focus on high-performance and scalable middleware for Message Passing Interface (MPI) and Deep Learning on This talk will focus on high-performance and scalable middleware for Message Passing Interface (MPI) and Deep Learning on OpenPOWER platform with NVIDIA GPGPUs and RDMA-enabled interconnects (InfiniBand and RoCE). The focus will be on two packages with commercial support being available from X-ScaleSolutions. The first package will focus on the OSU MVAPICH2 MPI libraries and their capabilities for high-performance computing with both CPUs (OpenPOWER) and GPUs (NVIDIA). The second package will focus on tight integration between the OSU MVAPICH2-GDR MPI library and the Horovod stack to provide high-performance and scalable Deep Learning with multiple frameworks like TensorFlow and PyTorch. Performance results from the SUMMIT system (#1 supercomputer in the World) with thousands of GPUs and POWER9 CPUs will be presented.
Captions: 
	00:00:00,060 --> 00:00:06,450
hi everyone sure shall we get started

00:00:03,120 --> 00:00:11,340
aye aye I thought it would be a session

00:00:06,450 --> 00:00:14,360
chair or something I don't know we all

00:00:11,340 --> 00:00:17,220
just get started

00:00:14,360 --> 00:00:18,630
good afternoon everyone my name is shin

00:00:17,220 --> 00:00:22,830
Chon true

00:00:18,630 --> 00:00:29,130
I was the software engineer intern at

00:00:22,830 --> 00:00:31,859
scale solutions in this summer right now

00:00:29,130 --> 00:00:34,950
I'm the graduate student a PhD student

00:00:31,859 --> 00:00:37,250
at Ohio State University but today I'm

00:00:34,950 --> 00:00:40,920
going to present this work I have done

00:00:37,250 --> 00:00:43,290
in this summer's at a scale solution is

00:00:40,920 --> 00:00:46,890
about high-performance MPI and deep

00:00:43,290 --> 00:00:49,050
learning open power platforms so a

00:00:46,890 --> 00:00:53,820
little bit information about scale

00:00:49,050 --> 00:00:57,149
solutions the men go or we are doing is

00:00:53,820 --> 00:00:59,550
just trying to provide solutions and

00:00:57,149 --> 00:01:02,820
service support and training to our

00:00:59,550 --> 00:01:05,820
customers for those customers that they

00:01:02,820 --> 00:01:08,010
want to do something on the cluster like

00:01:05,820 --> 00:01:10,409
HPC cluster high performance computing

00:01:08,010 --> 00:01:14,220
clusters strong supercomputer they have

00:01:10,409 --> 00:01:17,729
maybe thousand of knows like CPU or GPU

00:01:14,220 --> 00:01:22,200
and if I don't know how to scale we're

00:01:17,729 --> 00:01:25,710
here to help using some communication

00:01:22,200 --> 00:01:29,520
library there's our expertise using

00:01:25,710 --> 00:01:31,950
sunlight empathy to library or some big

00:01:29,520 --> 00:01:35,430
data library like our DNA base Hadoop

00:01:31,950 --> 00:01:39,090
spark those kind of things today I'm

00:01:35,430 --> 00:01:42,990
going to introduce two product we were

00:01:39,090 --> 00:01:45,869
just going to release today after this

00:01:42,990 --> 00:01:49,140
presentation wines we call it a scale

00:01:45,869 --> 00:01:53,640
HPC is a high-performance HPC solutions

00:01:49,140 --> 00:01:56,399
for like any kind of MP HPC problem an

00:01:53,640 --> 00:01:59,969
open power platform another one is the

00:01:56,399 --> 00:02:03,390
SQL AI is many targeting the AI problem

00:01:59,969 --> 00:02:05,700
I deep learning those cut new very

00:02:03,390 --> 00:02:09,539
important applications nowadays also

00:02:05,700 --> 00:02:11,430
where we are going to support this open

00:02:09,539 --> 00:02:12,910
power platform so if you want to know

00:02:11,430 --> 00:02:15,730
more about this

00:02:12,910 --> 00:02:17,550
you can visit our website or just simply

00:02:15,730 --> 00:02:22,030
send us an email

00:02:17,550 --> 00:02:25,900
so our first briefly introduced some

00:02:22,030 --> 00:02:29,110
trend we observe in the HPC technologies

00:02:25,900 --> 00:02:33,460
then I will talk about our solution

00:02:29,110 --> 00:02:36,880
about first one is a scale HPC and

00:02:33,460 --> 00:02:41,380
second one is how we use all these HPC

00:02:36,880 --> 00:02:44,980
technologies to accelerating your AI

00:02:41,380 --> 00:02:47,650
problem how how we improve your

00:02:44,980 --> 00:02:51,160
performance of the many for deep

00:02:47,650 --> 00:02:55,300
learning training performance open power

00:02:51,160 --> 00:02:58,990
assistance so recently if you are aware

00:02:55,300 --> 00:03:02,710
of the top 500 supercomputers this list

00:02:58,990 --> 00:03:07,270
so you can see right now in recent years

00:03:02,710 --> 00:03:10,960
those computers basically using first of

00:03:07,270 --> 00:03:14,410
of course multi-core many corpsey pyou

00:03:10,960 --> 00:03:16,540
like Intel CPU or open power for nine

00:03:14,410 --> 00:03:19,240
CPUs for example there's a myth number

00:03:16,540 --> 00:03:22,450
one submission system or number two see

00:03:19,240 --> 00:03:25,120
are resistant and only a lot of other

00:03:22,450 --> 00:03:27,400
systems but they are not just using CPUs

00:03:25,120 --> 00:03:29,530
they because you need to connect a lot

00:03:27,400 --> 00:03:31,510
of resource or to get a lot of machines

00:03:29,530 --> 00:03:34,360
all together you need a high performance

00:03:31,510 --> 00:03:36,150
interconnects like InfiniBand menelaus

00:03:34,360 --> 00:03:39,040
InfiniBand provide you very good

00:03:36,150 --> 00:03:41,170
low latency and high bandwidth of course

00:03:39,040 --> 00:03:43,150
there are other interconnects will

00:03:41,170 --> 00:03:46,990
briefly talk about later about like

00:03:43,150 --> 00:03:50,710
homeopath or other interconnects and

00:03:46,990 --> 00:03:54,430
also they have si raters like GPU nvidia

00:03:50,710 --> 00:03:58,150
GPU is kind of the main stream right now

00:03:54,430 --> 00:04:01,170
but there are more like FPGA or GPU from

00:03:58,150 --> 00:04:04,120
other vendors they're coming up as well

00:04:01,170 --> 00:04:06,790
but in this talk I will focus on the

00:04:04,120 --> 00:04:09,100
NVIDIA GPU and also you have some

00:04:06,790 --> 00:04:14,740
storage like SSD or and via these kind

00:04:09,100 --> 00:04:18,390
of things so basically for each specific

00:04:14,740 --> 00:04:21,700
analogies we can briefly like simply

00:04:18,390 --> 00:04:23,680
talk about like first the hardware next

00:04:21,700 --> 00:04:25,990
one is a software so how were sorry

00:04:23,680 --> 00:04:29,860
like the inter interconnects

00:04:25,990 --> 00:04:32,949
in ban rocky on pony path and processes

00:04:29,860 --> 00:04:35,380
you have GPU you have CPU like open

00:04:32,949 --> 00:04:38,620
power CPU or GPU or sorry

00:04:35,380 --> 00:04:41,800
GPU like Google tensor processing unit

00:04:38,620 --> 00:04:46,479
for accelerating specifically for the

00:04:41,800 --> 00:04:49,990
deep learning workload or also the FPGA

00:04:46,479 --> 00:04:51,789
is there as well and you can you not

00:04:49,990 --> 00:04:54,520
only have the hardware you also need a

00:04:51,789 --> 00:04:56,470
very good software to support to scale

00:04:54,520 --> 00:05:01,120
your application if your application is

00:04:56,470 --> 00:05:03,190
going to running on large scale system

00:05:01,120 --> 00:05:06,009
you have thousand of nose you want to

00:05:03,190 --> 00:05:07,990
run on that and still have a good

00:05:06,009 --> 00:05:10,539
performance you need to have a scalable

00:05:07,990 --> 00:05:13,030
communication media where so something

00:05:10,539 --> 00:05:15,009
simple like very classic the message

00:05:13,030 --> 00:05:17,530
passing interface and now because if you

00:05:15,009 --> 00:05:20,259
are using NVIDIA GPU you need a CUDA

00:05:17,530 --> 00:05:23,169
where MPI to have the best possible

00:05:20,259 --> 00:05:26,860
performance also you can also use Nvidia

00:05:23,169 --> 00:05:28,900
Nico library less about for diploid

00:05:26,860 --> 00:05:31,110
specific for deep learning workload I

00:05:28,900 --> 00:05:34,509
also talked about a little bit later so

00:05:31,110 --> 00:05:36,820
first let's look at the hardware in the

00:05:34,509 --> 00:05:40,020
interconnects I you have InfiniBand

00:05:36,820 --> 00:05:43,180
ami PAP the low side very fast

00:05:40,020 --> 00:05:46,259
interconnects also like if something you

00:05:43,180 --> 00:05:49,810
may know coming years there'll be a Cray

00:05:46,259 --> 00:05:52,780
slingshot this also will be coming next

00:05:49,810 --> 00:05:55,030
year I think and they have a very good

00:05:52,780 --> 00:05:58,030
performance low latency very low just

00:05:55,030 --> 00:06:01,120
feel microsecond penny wave is very high

00:05:58,030 --> 00:06:04,509
could be like 200 gigabyte gigabit per

00:06:01,120 --> 00:06:06,310
second and because of the rDNA you can

00:06:04,509 --> 00:06:10,030
bypass the cpu when you're doing a

00:06:06,310 --> 00:06:13,780
communication so it has very low CPU

00:06:10,030 --> 00:06:16,319
overhead and you can see all these kind

00:06:13,780 --> 00:06:19,509
of interconnect in the top 500

00:06:16,319 --> 00:06:22,330
supercomputers Early's it's very common

00:06:19,509 --> 00:06:26,710
there in the CPU side the microprocessor

00:06:22,330 --> 00:06:30,669
we can see over years is keep improving

00:06:26,710 --> 00:06:32,949
like not just the number transistor

00:06:30,669 --> 00:06:35,840
transistors per chip also the power

00:06:32,949 --> 00:06:38,030
consumption is getting lower in lower

00:06:35,840 --> 00:06:40,720
but the single three performance is kind

00:06:38,030 --> 00:06:43,280
of stuck it's still improving but slowly

00:06:40,720 --> 00:06:46,910
that's why people trying to come up

00:06:43,280 --> 00:06:49,580
something new like NVIDIA GPU they they

00:06:46,910 --> 00:06:54,200
give you very good performance like the

00:06:49,580 --> 00:06:56,360
latest Nvidia Volta GPUs gives you very

00:06:54,200 --> 00:06:58,280
good performance especially for the tip

00:06:56,360 --> 00:07:00,860
learning worker like you if you are

00:06:58,280 --> 00:07:04,190
doing some dip the neural network

00:07:00,860 --> 00:07:07,040
trending I resonate 50 or you are doing

00:07:04,190 --> 00:07:08,600
some inference using this GPU gives you

00:07:07,040 --> 00:07:14,510
a very good performance where you could

00:07:08,600 --> 00:07:16,850
speed up so after you have all these

00:07:14,510 --> 00:07:20,479
power in place next one you need the

00:07:16,850 --> 00:07:24,320
software to really utilize these

00:07:20,479 --> 00:07:30,050
hardware efficiently so first one if

00:07:24,320 --> 00:07:32,660
you're going to scale using NPI naively

00:07:30,050 --> 00:07:35,510
you can use MPI like this you because

00:07:32,660 --> 00:07:39,830
the default like most MPI library they

00:07:35,510 --> 00:07:42,770
don't support coda like transparently so

00:07:39,830 --> 00:07:45,200
for most people do will like your

00:07:42,770 --> 00:07:47,600
application will be like this you copy a

00:07:45,200 --> 00:07:50,510
data to a CPU memory or system memory

00:07:47,600 --> 00:07:53,630
then you do it like send receive using

00:07:50,510 --> 00:07:57,560
MPI MPI saying MPI receive data will be

00:07:53,630 --> 00:07:59,720
received on the CPU buffer then you do

00:07:57,560 --> 00:08:04,340
it you need to do another copy to copy a

00:07:59,720 --> 00:08:06,289
data from CPU to GPU is high

00:08:04,340 --> 00:08:09,110
productivity because the code is very

00:08:06,289 --> 00:08:13,250
easy to write like this but the

00:08:09,110 --> 00:08:17,060
performance will not be optimal so like

00:08:13,250 --> 00:08:19,970
maybe top five or eight years ago

00:08:17,060 --> 00:08:22,010
people start working on this cheap you

00:08:19,970 --> 00:08:25,400
aware or CUDA where if we are using

00:08:22,010 --> 00:08:28,100
NVIDIA GPU which means like you you can

00:08:25,400 --> 00:08:31,100
just write a code like this MPI say MPI

00:08:28,100 --> 00:08:33,469
receive then inside am here library they

00:08:31,100 --> 00:08:35,930
can recognize okay this buffer is a GPU

00:08:33,469 --> 00:08:37,640
is from GPU memory so we can have some

00:08:35,930 --> 00:08:40,700
up on my design for example some

00:08:37,640 --> 00:08:43,370
planning to overlap the communication

00:08:40,700 --> 00:08:46,880
because we have our DNA here so you can

00:08:43,370 --> 00:08:49,210
have a not just good productivity the

00:08:46,880 --> 00:08:51,370
code is easy to write and also

00:08:49,210 --> 00:08:53,320
because the this communication media

00:08:51,370 --> 00:08:55,330
will take care of all these kind of

00:08:53,320 --> 00:08:57,450
pipeline in design implementation there

00:08:55,330 --> 00:09:01,899
so the performance will be very good

00:08:57,450 --> 00:09:04,060
this one MPI Ward and recently in recent

00:09:01,899 --> 00:09:08,700
year maybe I guess four years ago

00:09:04,060 --> 00:09:10,899
because all these deep learning become

00:09:08,700 --> 00:09:13,600
more and more popular

00:09:10,899 --> 00:09:15,940
Nvidia introduced their own Nvidia

00:09:13,600 --> 00:09:18,370
collective communication library called

00:09:15,940 --> 00:09:20,520
Nico so basically they are doing they

00:09:18,370 --> 00:09:22,870
are optimized in the GPU communication

00:09:20,520 --> 00:09:25,200
focusing on a collective communication

00:09:22,870 --> 00:09:27,250
because there's the most important

00:09:25,200 --> 00:09:29,709
communication pattern in the deep

00:09:27,250 --> 00:09:31,000
learning workload that like they are

00:09:29,709 --> 00:09:34,120
doing these kind of all reduce

00:09:31,000 --> 00:09:36,540
communication between a lot with GPUs

00:09:34,120 --> 00:09:38,890
well they actually automate this

00:09:36,540 --> 00:09:42,670
communication pattern in a dense

00:09:38,890 --> 00:09:46,120
multi-gpu system like their dgx machine

00:09:42,670 --> 00:09:48,550
which like digit 2 they have 16 voltage

00:09:46,120 --> 00:09:50,920
GPU connect by I'm reading and this

00:09:48,550 --> 00:09:53,500
switch so they automate this health

00:09:50,920 --> 00:09:57,360
system also recently like there recently

00:09:53,500 --> 00:10:00,760
released I think it's Nico 2.4 they also

00:09:57,360 --> 00:10:05,260
give very good performance on the open

00:10:00,760 --> 00:10:06,970
power system like submit so you have

00:10:05,260 --> 00:10:12,150
this communication library this is just

00:10:06,970 --> 00:10:15,610
some background then SQL solution we

00:10:12,150 --> 00:10:19,000
introduced to pact to product here first

00:10:15,610 --> 00:10:23,470
one we call XQ HPC is focusing on the

00:10:19,000 --> 00:10:26,500
HPC application some classic scientific

00:10:23,470 --> 00:10:28,839
application is not using like AI those

00:10:26,500 --> 00:10:30,700
kind of framework so why we need these

00:10:28,839 --> 00:10:33,459
because even though you have a lot of

00:10:30,700 --> 00:10:35,500
good NPR libraries but how you get the

00:10:33,459 --> 00:10:38,260
best possible performance for your

00:10:35,500 --> 00:10:40,750
application is still very hard first you

00:10:38,260 --> 00:10:42,940
need to know what NPR library going to

00:10:40,750 --> 00:10:43,630
use there are a lot of API library

00:10:42,940 --> 00:10:48,130
openmpi

00:10:43,630 --> 00:10:53,500
spectra NPI inhale MPI or inverter to

00:10:48,130 --> 00:10:56,250
Papa you Nene is a lot and even if you

00:10:53,500 --> 00:10:59,650
choose NPR library there are so many

00:10:56,250 --> 00:11:00,870
like parameters may be for each MPI

00:10:59,650 --> 00:11:04,080
library root layer

00:11:00,870 --> 00:11:06,870
could be hunters parameters you can tune

00:11:04,080 --> 00:11:08,850
by together performance on your system

00:11:06,870 --> 00:11:10,890
for different system you need to tune

00:11:08,850 --> 00:11:14,190
these parameter differently so it's a

00:11:10,890 --> 00:11:18,510
very complicated time-consuming most

00:11:14,190 --> 00:11:22,410
important tedious work so that's why we

00:11:18,510 --> 00:11:27,620
are introducing the SQL HPC package

00:11:22,410 --> 00:11:31,710
basically we provide a team fine-tune

00:11:27,620 --> 00:11:34,110
library communication library using MPI

00:11:31,710 --> 00:11:36,779
base then you can give you output box

00:11:34,110 --> 00:11:40,200
the party more performance on open power

00:11:36,779 --> 00:11:44,640
platform so I here and just refer to

00:11:40,200 --> 00:11:47,370
this paper this paper is just presented

00:11:44,640 --> 00:11:50,279
in the international workshop on open

00:11:47,370 --> 00:11:53,460
power HPC like just two months ago

00:11:50,279 --> 00:11:55,650
basically compare OCAD cuda where MPI

00:11:53,460 --> 00:11:58,320
library on the open power system like

00:11:55,650 --> 00:12:00,270
number one so my sister so you can see

00:11:58,320 --> 00:12:02,610
the difference between the different MPA

00:12:00,270 --> 00:12:04,260
libraries you just run in here library

00:12:02,610 --> 00:12:07,140
on the census then you get all kind

00:12:04,260 --> 00:12:09,029
different performance that's that's very

00:12:07,140 --> 00:12:10,920
hard for some people is not very

00:12:09,029 --> 00:12:13,380
familiar with this library so you need

00:12:10,920 --> 00:12:16,470
to spend a lotta to tune it for example

00:12:13,380 --> 00:12:18,990
you can see here for is latency Mr P to

00:12:16,470 --> 00:12:21,510
is Judy are doing very well for small

00:12:18,990 --> 00:12:23,130
message for large message this MPI

00:12:21,510 --> 00:12:27,089
library looks like they are performing

00:12:23,130 --> 00:12:29,490
similar and penguin flies look style

00:12:27,089 --> 00:12:32,040
they are very good like here is the

00:12:29,490 --> 00:12:34,950
internal like to window can receive

00:12:32,040 --> 00:12:36,839
communication between two GPU and she's

00:12:34,950 --> 00:12:39,150
with E&L so it will go through the

00:12:36,839 --> 00:12:41,370
invading so you can see okay all this

00:12:39,150 --> 00:12:44,220
debris looks to be good like pen width

00:12:41,370 --> 00:12:47,279
wise they can saturate and we think and

00:12:44,220 --> 00:12:51,000
for internal communication latency wise

00:12:47,279 --> 00:12:55,230
you see okay I'm happy to Judea and open

00:12:51,000 --> 00:12:57,570
API open MPI plus using UCS provide very

00:12:55,230 --> 00:13:01,020
good performance in baton see in spatial

00:12:57,570 --> 00:13:03,330
NPI a little bit like amplify but I

00:13:01,020 --> 00:13:08,250
think I believe improving the latest

00:13:03,330 --> 00:13:10,530
version and let in penguin wise actually

00:13:08,250 --> 00:13:12,959
you can see special on P is very good in

00:13:10,530 --> 00:13:14,400
Bangui flights but then now open P is

00:13:12,959 --> 00:13:17,250
actually not very good

00:13:14,400 --> 00:13:19,710
been with this is internal like point

00:13:17,250 --> 00:13:23,040
point go through the InfiniBand network

00:13:19,710 --> 00:13:25,440
so there's a trade-off and here we see

00:13:23,040 --> 00:13:28,860
that in happy to provide a reasonable

00:13:25,440 --> 00:13:31,230
stable performance across all different

00:13:28,860 --> 00:13:35,190
like configuration different

00:13:31,230 --> 00:13:37,710
communication pattern so we'll use that

00:13:35,190 --> 00:13:40,760
in our package and next one

00:13:37,710 --> 00:13:44,490
not just the point for you we also

00:13:40,760 --> 00:13:46,650
there's also a work evaluating the

00:13:44,490 --> 00:13:49,200
collective like all reduce or reduce is

00:13:46,650 --> 00:13:52,080
a very important communication pattern

00:13:49,200 --> 00:13:54,720
for deep learning workload so if you

00:13:52,080 --> 00:13:57,200
compare the and we're happy to Judea to

00:13:54,720 --> 00:14:01,320
Nikko you can also see latency wise

00:13:57,200 --> 00:14:04,710
using MPI like using a puppy to actually

00:14:01,320 --> 00:14:07,529
you get better latency and they also

00:14:04,710 --> 00:14:10,950
optimized for will follow penguin as you

00:14:07,529 --> 00:14:14,220
can see here and is scale scale very

00:14:10,950 --> 00:14:19,980
well you can see there on this is ransom

00:14:14,220 --> 00:14:24,330
made system up to like 115 thousand

00:14:19,980 --> 00:14:26,610
sorry 1000 1500 GPUs and get your good

00:14:24,330 --> 00:14:29,190
performance and there's there's some

00:14:26,610 --> 00:14:33,690
issue with speech rampion openmpi at the

00:14:29,190 --> 00:14:39,080
large scale so that's why in this skills

00:14:33,690 --> 00:14:42,900
of HPC package we introduced here we use

00:14:39,080 --> 00:14:45,630
OSU and the p2 library and it's a

00:14:42,900 --> 00:14:48,690
scalable in scalable but not only like

00:14:45,630 --> 00:14:50,610
only used in that we also to MIT we

00:14:48,690 --> 00:14:52,650
spend a lot of time to a lab to neon

00:14:50,610 --> 00:14:55,500
open power system so you like we can

00:14:52,650 --> 00:14:58,650
provide the oval box to find you and

00:14:55,500 --> 00:15:00,570
optimal performance and it open power

00:14:58,650 --> 00:15:04,260
platform we're also working on other

00:15:00,570 --> 00:15:07,200
platform like x86 platform and if you

00:15:04,260 --> 00:15:11,760
are interesting and you can contact us

00:15:07,200 --> 00:15:13,770
we can work out to provide your product

00:15:11,760 --> 00:15:18,600
to test it on your system and we have a

00:15:13,770 --> 00:15:21,450
free trial so just give you a quick look

00:15:18,600 --> 00:15:24,480
how it looks like tag we give a very

00:15:21,450 --> 00:15:27,810
simple installation by just one command

00:15:24,480 --> 00:15:29,670
it looked like this you just run one

00:15:27,810 --> 00:15:34,080
men to install will install MPI library

00:15:29,670 --> 00:15:36,570
also the benchmark there and to run it

00:15:34,080 --> 00:15:40,040
also one command you can run in like if

00:15:36,570 --> 00:15:42,360
you are familiar with like MPI library

00:15:40,040 --> 00:15:44,490
MPI run those kind of things you see

00:15:42,360 --> 00:15:46,680
exactly you're saying come in we use

00:15:44,490 --> 00:15:48,930
here just the the suitable is a little

00:15:46,680 --> 00:15:51,420
bit different but using these to launch

00:15:48,930 --> 00:15:54,570
your job and you can launch your job on

00:15:51,420 --> 00:15:57,390
thousands of GPU there's no problem by

00:15:54,570 --> 00:16:00,390
default we just like run o it use like

00:15:57,390 --> 00:16:02,700
this for some deep learning work but you

00:16:00,390 --> 00:16:06,420
can run any kind of applications here

00:16:02,700 --> 00:16:11,430
just using one simple command and you

00:16:06,420 --> 00:16:14,400
give you already tuned performance and

00:16:11,430 --> 00:16:17,130
the next one is X scale ai is because

00:16:14,400 --> 00:16:19,910
like we all know the I now because of

00:16:17,130 --> 00:16:24,300
deep learning because all these hardware

00:16:19,910 --> 00:16:26,100
is becomes ever popular so we also

00:16:24,300 --> 00:16:29,850
provide a solution based on our

00:16:26,100 --> 00:16:34,560
experiments experience on SQL HPC so we

00:16:29,850 --> 00:16:38,040
use use let to introduce a new product

00:16:34,560 --> 00:16:39,660
of a scale AI so I guess we all know

00:16:38,040 --> 00:16:44,160
these deep learning use cases like

00:16:39,660 --> 00:16:47,490
endless and it is keep according

00:16:44,160 --> 00:16:51,300
grooving every year so you know the

00:16:47,490 --> 00:16:54,750
revenue there is incredible in the deep

00:16:51,300 --> 00:16:57,420
learning they usually use NVIDIA GPU is

00:16:54,750 --> 00:17:00,120
kind of dominating right now but there

00:16:57,420 --> 00:17:02,520
are more and DGPS also coming SPG is

00:17:00,120 --> 00:17:04,020
also coming for tib learning so it could

00:17:02,520 --> 00:17:07,319
be different in future but right now

00:17:04,020 --> 00:17:10,520
still GPU is the most important hardware

00:17:07,319 --> 00:17:13,439
for deep learning guys the workload and

00:17:10,520 --> 00:17:17,880
they are actually using HPC resource

00:17:13,439 --> 00:17:21,420
they are running IPC systems and HP's is

00:17:17,880 --> 00:17:24,510
and there's a lot of HP system actually

00:17:21,420 --> 00:17:26,370
powered by the GPU then they are using

00:17:24,510 --> 00:17:30,179
MPI library like we mentioned those

00:17:26,370 --> 00:17:35,310
special NPI or open up MPI or inverted

00:17:30,179 --> 00:17:37,590
tube the problem is there are a lot of

00:17:35,310 --> 00:17:41,720
deep learning framework google has

00:17:37,590 --> 00:17:41,720
tensorflow facebook at pi torch

00:17:41,960 --> 00:17:47,429
Microsoft has C NT k and u n-- and

00:17:45,179 --> 00:17:49,530
there's a lot different vendors come up

00:17:47,429 --> 00:17:52,770
different deep learning framework for

00:17:49,530 --> 00:17:57,179
different purpose so it's so many

00:17:52,770 --> 00:17:59,100
choices you have and but you know if you

00:17:57,179 --> 00:18:01,650
familiar with deep learning you only

00:17:59,100 --> 00:18:04,740
have two major tasks in all these

00:18:01,650 --> 00:18:07,740
framework first one training you do a

00:18:04,740 --> 00:18:12,090
training of the neural network you

00:18:07,740 --> 00:18:15,590
defined and you do a training you get a

00:18:12,090 --> 00:18:19,980
model then you use that to do inference

00:18:15,590 --> 00:18:21,860
and we're focus here like at the DIA and

00:18:19,980 --> 00:18:25,620
training because this is the most

00:18:21,860 --> 00:18:28,350
compute and communication intensive you

00:18:25,620 --> 00:18:30,120
can take like days or weeks of course so

00:18:28,350 --> 00:18:32,370
that's why we need a faster training a

00:18:30,120 --> 00:18:35,190
lot of work basically from academia or

00:18:32,370 --> 00:18:37,080
industry they they are a lot of work

00:18:35,190 --> 00:18:42,450
like trying to optimize deep learning

00:18:37,080 --> 00:18:45,710
from top to parent and in our in SQL

00:18:42,450 --> 00:18:48,720
solution or as please at the

00:18:45,710 --> 00:18:50,640
communication that's why we focus on the

00:18:48,720 --> 00:18:53,040
parallel and teach beauty training like

00:18:50,640 --> 00:18:58,169
how we can use more GPU and we can skill

00:18:53,040 --> 00:19:00,660
very good for money like 100,000 GPUs so

00:18:58,169 --> 00:19:03,900
it comes back to the same problem we we

00:19:00,660 --> 00:19:05,510
mentioned for the HPG side because you

00:19:03,900 --> 00:19:09,540
need to scale your deep learning

00:19:05,510 --> 00:19:12,750
application to a lot of GPU nose so you

00:19:09,540 --> 00:19:15,210
need a communication then your choice

00:19:12,750 --> 00:19:17,280
you have is again message passing

00:19:15,210 --> 00:19:20,970
interface using MPI or using some

00:19:17,280 --> 00:19:23,160
specific library like nickel or glue or

00:19:20,970 --> 00:19:25,559
pi/2 or deuce or have different

00:19:23,160 --> 00:19:27,960
communication libraries and if you

00:19:25,559 --> 00:19:31,830
prefer you some people also using big

00:19:27,960 --> 00:19:34,080
data wide spot no ha do but it all comes

00:19:31,830 --> 00:19:37,160
to the all come to the same problem like

00:19:34,080 --> 00:19:40,440
how you scale up and how you skill out

00:19:37,160 --> 00:19:42,960
so to give you a brief idea basically

00:19:40,440 --> 00:19:44,370
scale up means how you get a good

00:19:42,960 --> 00:19:46,730
internal performance

00:19:44,370 --> 00:19:51,600
communication performance that you have

00:19:46,730 --> 00:19:54,480
like dozens of GPUs unknown and how you

00:19:51,600 --> 00:19:55,270
can scale up on this one machine yes

00:19:54,480 --> 00:19:58,480
because

00:19:55,270 --> 00:20:00,429
up and we know most of the deep learning

00:19:58,480 --> 00:20:03,820
framework they doing they are doing very

00:20:00,429 --> 00:20:06,070
well at least part the scale up ah but

00:20:03,820 --> 00:20:08,950
to scale out is more important because

00:20:06,070 --> 00:20:11,860
you need to communicate we've all kind

00:20:08,950 --> 00:20:15,070
of a lot of machines there's an internal

00:20:11,860 --> 00:20:19,179
communication so that's what most deep

00:20:15,070 --> 00:20:22,929
deep learning framework do not doing

00:20:19,179 --> 00:20:24,700
very good and that's why we need some

00:20:22,929 --> 00:20:28,510
communication library optimized coming

00:20:24,700 --> 00:20:29,650
Convocation library like NPI or Niko so

00:20:28,510 --> 00:20:32,890
you can see different deep learning

00:20:29,650 --> 00:20:34,900
framework they have different way that

00:20:32,890 --> 00:20:38,650
they choose different direction

00:20:34,900 --> 00:20:41,740
Caffe using MPI or C NT k UC MPI or Nico

00:20:38,650 --> 00:20:44,320
Google tensorflow using maybe gr PC or

00:20:41,740 --> 00:20:46,000
you can also use NPR you can also use

00:20:44,320 --> 00:20:51,880
nickel they keep a lot of different

00:20:46,000 --> 00:20:54,460
options and and know and to to to

00:20:51,880 --> 00:20:57,340
achieve the day up Harrell tip learning

00:20:54,460 --> 00:21:01,600
like or disputed deep learning using MPI

00:20:57,340 --> 00:21:04,030
PC counts to to actually only one

00:21:01,600 --> 00:21:06,490
operation communication right now

00:21:04,030 --> 00:21:08,080
earlier is like broadcast and reduce

00:21:06,490 --> 00:21:11,400
because you have some query and you need

00:21:08,080 --> 00:21:14,500
to exchange those things and now now

00:21:11,400 --> 00:21:17,200
basically everybody agreed like using a

00:21:14,500 --> 00:21:21,660
one or reduce operations Kadhim equal

00:21:17,200 --> 00:21:25,450
equivalent to the procas plus reduce so

00:21:21,660 --> 00:21:29,170
our reduce becomes the most one of most

00:21:25,450 --> 00:21:30,730
important communication operation for

00:21:29,170 --> 00:21:35,890
deep learning I mean for that they have

00:21:30,730 --> 00:21:38,650
hero just for theta pair reason of

00:21:35,890 --> 00:21:41,320
course they are different approach but

00:21:38,650 --> 00:21:45,490
this one's the most popular popular one

00:21:41,320 --> 00:21:49,030
now so less we are focusing here so

00:21:45,490 --> 00:21:51,309
right now we we have the everything

00:21:49,030 --> 00:21:54,970
together we have the hardware you have

00:21:51,309 --> 00:21:57,520
HPC your goal is your to achieve your

00:21:54,970 --> 00:22:01,000
deep learning a high performative deep

00:21:57,520 --> 00:22:04,000
learning training so however you have

00:22:01,000 --> 00:22:06,160
like power 9 system like some it and in

00:22:04,000 --> 00:22:09,370
HPC side the coming communication

00:22:06,160 --> 00:22:12,880
library we choose MPI base

00:22:09,370 --> 00:22:16,690
using a map to like same reason we

00:22:12,880 --> 00:22:19,410
mentioned previously in the SQL HPC then

00:22:16,690 --> 00:22:22,000
we can see very good scalability for

00:22:19,410 --> 00:22:25,270
deep learning workflow so I will show

00:22:22,000 --> 00:22:28,020
some performance number later so first

00:22:25,270 --> 00:22:31,180
like our solution like I mentioned we

00:22:28,020 --> 00:22:33,670
basically our solutions is

00:22:31,180 --> 00:22:36,070
high-performance and scalable because we

00:22:33,670 --> 00:22:39,210
fully you if flow is falling the HPC

00:22:36,070 --> 00:22:42,970
resource base our experience in HPC side

00:22:39,210 --> 00:22:45,760
and we also provide the alto box optimal

00:22:42,970 --> 00:22:48,400
performance and power system power nines

00:22:45,760 --> 00:22:51,280
instance specifically we test it under

00:22:48,400 --> 00:22:54,400
some assistant and we were also working

00:22:51,280 --> 00:22:56,920
on other system it will be coming very

00:22:54,400 --> 00:23:00,790
soon but now right now we focus on power

00:22:56,920 --> 00:23:03,550
9 system and what's inside this skill

00:23:00,790 --> 00:23:05,590
package is that we you have you have

00:23:03,550 --> 00:23:07,480
once you download this package you have

00:23:05,590 --> 00:23:11,650
the fine-toothed cuda where MPI library

00:23:07,480 --> 00:23:14,140
and power system when you we use google

00:23:11,650 --> 00:23:18,160
google tensorflow framework which is

00:23:14,140 --> 00:23:20,230
built with open power system and it's an

00:23:18,160 --> 00:23:22,840
MPI approach and we also use the

00:23:20,230 --> 00:23:27,310
horrible to achieve that which beauty

00:23:22,840 --> 00:23:30,610
deep training using MPI and also we

00:23:27,310 --> 00:23:32,770
provide some easy to use script and run

00:23:30,610 --> 00:23:38,080
time so you can do very simple its

00:23:32,770 --> 00:23:42,210
insulation institution so to install xq

00:23:38,080 --> 00:23:45,820
ai as I mentioned we used in several

00:23:42,210 --> 00:23:48,490
1.12 like if you prefer other versions

00:23:45,820 --> 00:23:50,470
we can work on it as well like because

00:23:48,490 --> 00:23:52,570
that's the version we know if you you

00:23:50,470 --> 00:23:55,060
have ever have experience like building

00:23:52,570 --> 00:23:56,950
tensor flow from source there's a lot of

00:23:55,060 --> 00:23:59,770
problems so we spend a lot of time

00:23:56,950 --> 00:24:01,710
trying to dip our clothes so this is a

00:23:59,770 --> 00:24:04,090
stable version we have one point to help

00:24:01,710 --> 00:24:07,300
that we can work on we are also working

00:24:04,090 --> 00:24:10,060
on the other newer versions then you

00:24:07,300 --> 00:24:13,360
have MPI library with with it and

00:24:10,060 --> 00:24:15,910
horrible and tensorflow benchmark so you

00:24:13,360 --> 00:24:18,430
once you have all these you can quickly

00:24:15,910 --> 00:24:21,460
run some simple benchmark to see where

00:24:18,430 --> 00:24:22,720
you stand and you need to offer this on

00:24:21,460 --> 00:24:25,419
other

00:24:22,720 --> 00:24:28,720
details that you need to make sure you

00:24:25,419 --> 00:24:29,799
have all these like CUDA libraries in

00:24:28,720 --> 00:24:33,159
your environment

00:24:29,799 --> 00:24:35,789
once you have all these you can just

00:24:33,159 --> 00:24:38,320
install in one command like SQL

00:24:35,789 --> 00:24:42,210
installation so you can see here we

00:24:38,320 --> 00:24:45,070
check the licenses then install the

00:24:42,210 --> 00:24:48,640
package you need like a mini Conda

00:24:45,070 --> 00:24:52,360
because the Python dependency and

00:24:48,640 --> 00:24:54,850
tensorflow 1.12 and NPR library which is

00:24:52,360 --> 00:24:58,020
enough you to GTR know right now and

00:24:54,850 --> 00:25:01,210
also the horrible and the benchmark here

00:24:58,020 --> 00:25:03,880
so once you install it you can just

00:25:01,210 --> 00:25:07,120
simply run this coming like this is

00:25:03,880 --> 00:25:08,559
executable after that it could be any of

00:25:07,120 --> 00:25:11,230
could be a benchmark could be your

00:25:08,559 --> 00:25:12,820
application and you just run these by

00:25:11,230 --> 00:25:16,480
default we just run a benchmark like

00:25:12,820 --> 00:25:18,940
real name 15 and do some simple chaining

00:25:16,480 --> 00:25:22,270
to to launch your job on the power

00:25:18,940 --> 00:25:24,760
system like this like you're running and

00:25:22,270 --> 00:25:27,130
by default it runs on using tensorflow

00:25:24,760 --> 00:25:29,260
benchmark to run some experiments so you

00:25:27,130 --> 00:25:34,809
get your performance like image per

00:25:29,260 --> 00:25:38,370
second so it's all come in one package

00:25:34,809 --> 00:25:42,730
it's very easy to use install and use it

00:25:38,370 --> 00:25:44,590
so we have tested first auntie-ji actual

00:25:42,730 --> 00:25:47,650
machine this is not power system but we

00:25:44,590 --> 00:25:51,250
also test it because this one very like

00:25:47,650 --> 00:25:55,409
popular AI system so we also test it and

00:25:51,250 --> 00:25:59,620
we see we have like nine percent higher

00:25:55,409 --> 00:26:01,809
throughput compared to Nikko and we have

00:25:59,620 --> 00:26:04,330
very good scaling efficiency is near

00:26:01,809 --> 00:26:06,840
isn't still not 100% like right like

00:26:04,330 --> 00:26:09,340
ideal place like more than 90 percent

00:26:06,840 --> 00:26:12,250
scaling efficiency and it's much better

00:26:09,340 --> 00:26:15,960
than an eco which is the kind of state

00:26:12,250 --> 00:26:18,970
of the art right now then we also try

00:26:15,960 --> 00:26:23,409
evaluate this the power system and

00:26:18,970 --> 00:26:25,450
submit so we try the resonate 50

00:26:23,409 --> 00:26:29,830
training using a tensorflow benchmark

00:26:25,450 --> 00:26:33,740
and we run like again here like 50

00:26:29,830 --> 00:26:36,220
hunger GPU the system then

00:26:33,740 --> 00:26:40,370
he runs and we get very good performance

00:26:36,220 --> 00:26:43,370
like like the image per second then

00:26:40,370 --> 00:26:45,590
scale very well you'll you can see this

00:26:43,370 --> 00:26:48,070
graph almost double every time you

00:26:45,590 --> 00:26:50,630
double GPU you can't get double the

00:26:48,070 --> 00:26:54,530
performance like image per second so

00:26:50,630 --> 00:26:56,480
roughly like we can you can finish that

00:26:54,530 --> 00:27:01,150
you can do this training in five points

00:26:56,480 --> 00:27:05,420
like five half minutes like using this

00:27:01,150 --> 00:27:09,590
$15 fifteen hundred if you in five and

00:27:05,420 --> 00:27:12,620
half minutes you can change image net 1k

00:27:09,590 --> 00:27:14,990
like 1.2 million images so we have very

00:27:12,620 --> 00:27:17,660
good ability here we can also run like

00:27:14,990 --> 00:27:22,280
we have tried and it works

00:27:17,660 --> 00:27:25,460
Iran's 220 mm GPUs but it's only one

00:27:22,280 --> 00:27:28,100
round so I cannot put those numbers here

00:27:25,460 --> 00:27:30,650
but it runs we just we don't have a

00:27:28,100 --> 00:27:33,470
location to we cannot get lost

00:27:30,650 --> 00:27:35,720
allocation again so I cannot think in

00:27:33,470 --> 00:27:37,670
coolest number here but it runs and it

00:27:35,720 --> 00:27:40,309
skill very well we are sure about it

00:27:37,670 --> 00:27:42,440
then but and if you are interesting we

00:27:40,309 --> 00:27:44,960
can always like talk and work together

00:27:42,440 --> 00:27:47,390
and compared to a nickel we can see

00:27:44,960 --> 00:27:50,720
because we we we have very similar

00:27:47,390 --> 00:27:53,360
performance than Nikko up to 96 GPU but

00:27:50,720 --> 00:27:55,130
after that we are seeing some issues it

00:27:53,360 --> 00:27:57,800
could be issued to be coming from nickel

00:27:55,130 --> 00:28:00,020
or coming from special NPI because we

00:27:57,800 --> 00:28:02,360
use special ampere to launch the job for

00:28:00,020 --> 00:28:04,460
Nico and so we're still trying to figure

00:28:02,360 --> 00:28:07,550
out what's problem what's their problem

00:28:04,460 --> 00:28:10,280
but from here we can see at least from a

00:28:07,550 --> 00:28:14,630
skill ai our product we can see a very

00:28:10,280 --> 00:28:19,040
good scale parity here so to quickly

00:28:14,630 --> 00:28:22,880
conclude is talk like we introduce these

00:28:19,040 --> 00:28:26,600
two products today SQL HPC is optimized

00:28:22,880 --> 00:28:28,250
in P a library and various HP system

00:28:26,600 --> 00:28:31,010
including power system so if you have

00:28:28,250 --> 00:28:34,220
any kind of traditional MPI application

00:28:31,010 --> 00:28:35,510
some scientific operation as long as

00:28:34,220 --> 00:28:37,610
it's using MPI

00:28:35,510 --> 00:28:40,250
you can use this product to get a very

00:28:37,610 --> 00:28:42,890
good performance with scaleable problems

00:28:40,250 --> 00:28:45,220
and a large-scale system and next one is

00:28:42,890 --> 00:28:46,820
the X scale ai similarly based on our

00:28:45,220 --> 00:28:49,130
experience HP

00:28:46,820 --> 00:28:52,130
see we we provide these high-performance

00:28:49,130 --> 00:28:55,100
solution for this beauty training for AI

00:28:52,130 --> 00:28:58,519
problems so you can get auto box the

00:28:55,100 --> 00:29:03,320
team fine-tuned performance and open

00:28:58,519 --> 00:29:05,570
power platforms so little bit so if you

00:29:03,320 --> 00:29:09,470
are interesting feel free to contact us

00:29:05,570 --> 00:29:12,769
or I'm very happy to talk to you offline

00:29:09,470 --> 00:29:15,289
and you can just send me a single email

00:29:12,769 --> 00:29:18,230
and we can you can start using a free

00:29:15,289 --> 00:29:21,080
trial anything so with that I would like

00:29:18,230 --> 00:29:22,639
to thank you like and if you there's any

00:29:21,080 --> 00:29:36,980
coaching I would like to answer it

00:29:22,639 --> 00:29:40,399
Thanks yeah yes oh you mean a coat no no

00:29:36,980 --> 00:29:42,500
no no just just a bill that you have you

00:29:40,399 --> 00:29:45,889
need to have a lot different dependency

00:29:42,500 --> 00:29:47,840
to build it successfully so we just take

00:29:45,889 --> 00:29:49,279
care of those part for you we didn't

00:29:47,840 --> 00:29:51,830
change the vendor for itself

00:29:49,279 --> 00:30:00,620
you still come from the same source core

00:29:51,830 --> 00:30:04,789
you got from Funko girl yeah attention

00:30:00,620 --> 00:30:07,149
for what that opera like yeah you will

00:30:04,789 --> 00:30:10,940
be a RPN then you can just install it

00:30:07,149 --> 00:30:14,029
just up in like a table or a peon you

00:30:10,940 --> 00:30:17,330
can just unzip it like that then install

00:30:14,029 --> 00:30:20,889
run the command is so everything on your

00:30:17,330 --> 00:30:20,889
system yes

00:30:25,250 --> 00:30:30,590
stare and our pop questions

00:30:32,650 --> 00:30:40,589
no I thank you very much

00:30:37,170 --> 00:30:40,589

YouTube URL: https://www.youtube.com/watch?v=WDtnS7GAk-Y


