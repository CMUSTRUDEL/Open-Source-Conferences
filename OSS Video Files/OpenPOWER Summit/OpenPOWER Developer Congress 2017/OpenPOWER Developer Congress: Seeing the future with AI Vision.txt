Title: OpenPOWER Developer Congress: Seeing the future with AI Vision
Publication date: 2017-05-24
Playlist: OpenPOWER Developer Congress 2017
Description: 
	Hear from Mandie Quartly, WW Lead, Machine Learning, OpenPOWER Foundation, Linton Ward, Big Data & Analytics, Power Systems, IBM  and Megan Bell, Delegate, University of Maryland during day three of the OpenPOWER Developer Congress, held May 22-25 in San Francisco, CA.

The OpenPOWER Developer Congress is a hands-on, "Get Your Hands Dirty" event focused on all aspects of full stack solutions - software, tooling, acceleration, hardware and infrastructure.

Please click here for more information: http://bit.ly/2qcMjQ0
Captions: 
	00:00:00,030 --> 00:00:05,180
welcome to the first ever open power

00:00:02,040 --> 00:00:09,889
developers content slipped us in front

00:00:05,180 --> 00:00:12,929
[Music]

00:00:09,889 --> 00:00:14,969
yesterday we have a speaker from our

00:00:12,929 --> 00:00:17,340
China Research Lab from IBM China

00:00:14,969 --> 00:00:19,410
research lab talking about AI vision AI

00:00:17,340 --> 00:00:21,029
vision is is a product that enables

00:00:19,410 --> 00:00:23,160
someone wants to take advantage of

00:00:21,029 --> 00:00:25,560
machine vision but doesn't necessarily

00:00:23,160 --> 00:00:27,570
want to get under the covers of creating

00:00:25,560 --> 00:00:30,060
the model the target or focus of this

00:00:27,570 --> 00:00:32,940
particular set of capabilities is to

00:00:30,060 --> 00:00:35,190
enable image analytics so it could be

00:00:32,940 --> 00:00:37,440
anything from the classical dogs and

00:00:35,190 --> 00:00:40,079
bicycles to people and other other use

00:00:37,440 --> 00:00:41,969
cases so in the second part of the

00:00:40,079 --> 00:00:44,250
session we actually had hands-on where

00:00:41,969 --> 00:00:47,100
we we had accounts from a speaker vessel

00:00:44,250 --> 00:00:49,500
cloud through Siddhanta power servers we

00:00:47,100 --> 00:00:51,180
had a data set of a third that slaves

00:00:49,500 --> 00:00:53,640
different breeds of birds which were

00:00:51,180 --> 00:00:55,500
uploaded we created a model between the

00:00:53,640 --> 00:00:57,719
models just by clicking buttons you know

00:00:55,500 --> 00:01:00,030
no coding required specifically and then

00:00:57,719 --> 00:01:01,859
after that once again you know go go

00:01:00,030 --> 00:01:03,960
onto the internet find an image of a

00:01:01,859 --> 00:01:05,430
bird and then give it to the trained

00:01:03,960 --> 00:01:06,810
model and then the trained model would

00:01:05,430 --> 00:01:08,520
tell you what breed it was and how

00:01:06,810 --> 00:01:11,010
confidence it was that that was the

00:01:08,520 --> 00:01:13,950
right image this particular part of the

00:01:11,010 --> 00:01:16,860
workshop is enabling folks to use cafe

00:01:13,950 --> 00:01:19,530
and other tools underlying a web browser

00:01:16,860 --> 00:01:21,509
interface so they get exposure to the

00:01:19,530 --> 00:01:23,790
methodology and if you will the pipeline

00:01:21,509 --> 00:01:26,580
of data to get to a trained model and

00:01:23,790 --> 00:01:29,250
also some exposure to field programmable

00:01:26,580 --> 00:01:31,409
gate arrays and the capability to model

00:01:29,250 --> 00:01:33,360
them without deep hardware design

00:01:31,409 --> 00:01:36,030
experience so all of that without

00:01:33,360 --> 00:01:37,770
actually having to write any code so

00:01:36,030 --> 00:01:39,360
that's the whole idea of the AI vision

00:01:37,770 --> 00:01:40,560
because machine learning and deep

00:01:39,360 --> 00:01:43,110
learning has some extraordinarily

00:01:40,560 --> 00:01:44,970
technical components but at the same

00:01:43,110 --> 00:01:46,320
time I can divorce the technical

00:01:44,970 --> 00:01:49,110
components and I can also set up

00:01:46,320 --> 00:01:50,790
workflows let's say training I can work

00:01:49,110 --> 00:01:53,310
with an analyst who may have less deep

00:01:50,790 --> 00:01:54,990
learning mathematical knowledge but I

00:01:53,310 --> 00:01:57,240
can use them with training and reporting

00:01:54,990 --> 00:01:58,979
or validation finding somebody with deep

00:01:57,240 --> 00:02:01,460
learning machine multi-link knowledge

00:01:58,979 --> 00:02:04,409
it's expensive but I can leverage that

00:02:01,460 --> 00:02:06,180
upfront good training to find an

00:02:04,409 --> 00:02:08,520
efficient model and then separately roll

00:02:06,180 --> 00:02:10,100
that out in deploy and so AI vision is a

00:02:08,520 --> 00:02:12,690
really good example

00:02:10,100 --> 00:02:15,840
extracting away from the modeling

00:02:12,690 --> 00:02:18,000
underneath and allowing a LNG's and not

00:02:15,840 --> 00:02:21,570
have some really deep expertise but to

00:02:18,000 --> 00:02:23,940
be able to take a set of images train

00:02:21,570 --> 00:02:25,470
using those images and then have a model

00:02:23,940 --> 00:02:27,270
created that they can learn access

00:02:25,470 --> 00:02:28,920
through an API and this particular

00:02:27,270 --> 00:02:30,120
instance through super vessels I think

00:02:28,920 --> 00:02:39,719
it's very personal

00:02:30,120 --> 00:02:39,719

YouTube URL: https://www.youtube.com/watch?v=e0NcW4MORgQ


