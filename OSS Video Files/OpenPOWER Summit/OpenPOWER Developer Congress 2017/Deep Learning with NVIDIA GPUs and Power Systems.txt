Title: Deep Learning with NVIDIA GPUs and Power Systems
Publication date: 2017-08-31
Playlist: OpenPOWER Developer Congress 2017
Description: 
	Bob Crovella, Solutions Architect at NVIDIA, addresses Deep Learning with NVIDIA GPUs and Power Systems.

Questions that Bob address include:
What is deep learning, and why all the excitement?
Why does deep learning need GPU computing?
Why now? Why are GPUs adding so much value?
Why is the combination of GPUs and Power Systems of interest?

For more information, please visit: http://www.openpowerfoundation.org
Captions: 
	00:00:00,520 --> 00:00:09,920
[Music]

00:00:06,610 --> 00:00:11,330
so deep learning is a computer science

00:00:09,920 --> 00:00:13,190
algorithm that's really within the

00:00:11,330 --> 00:00:15,980
category of machine learning algorithms

00:00:13,190 --> 00:00:17,869
and deep learning has some unique

00:00:15,980 --> 00:00:19,550
characteristics to it that differentiate

00:00:17,869 --> 00:00:21,529
it for machine learning for example with

00:00:19,550 --> 00:00:23,029
deep learning we can use all the

00:00:21,529 --> 00:00:24,230
goodness of machine learning and not

00:00:23,029 --> 00:00:26,180
have to worry about something called

00:00:24,230 --> 00:00:28,579
feature extraction or design of features

00:00:26,180 --> 00:00:30,560
this allows us basically to process data

00:00:28,579 --> 00:00:32,630
that we don't fully understand and

00:00:30,560 --> 00:00:33,950
extract useful information from it and

00:00:32,630 --> 00:00:36,739
make decisions based on that information

00:00:33,950 --> 00:00:38,570
and this is very very important for the

00:00:36,739 --> 00:00:40,609
entire AI revolution that's going on

00:00:38,570 --> 00:00:43,129
right now this is a big part of the

00:00:40,609 --> 00:00:44,960
excitement as well because people have

00:00:43,129 --> 00:00:48,140
discovered that with deep learning they

00:00:44,960 --> 00:00:49,969
can very quickly process data a large

00:00:48,140 --> 00:00:51,530
volume of data and derive useful

00:00:49,969 --> 00:00:58,760
information and ask important questions

00:00:51,530 --> 00:01:00,440
and get the answers from that data so

00:00:58,760 --> 00:01:02,269
deep learning and GPU computing have

00:01:00,440 --> 00:01:04,580
almost grew up growing up together over

00:01:02,269 --> 00:01:05,900
the last five or six years and part of

00:01:04,580 --> 00:01:07,490
the reason for that is because deep

00:01:05,900 --> 00:01:10,610
learning is a very data intensive

00:01:07,490 --> 00:01:13,460
process it uses large amounts of data to

00:01:10,610 --> 00:01:14,899
train a model and then use that model to

00:01:13,460 --> 00:01:17,360
answer questions that we would like to

00:01:14,899 --> 00:01:19,790
answer on a wide variety of domains and

00:01:17,360 --> 00:01:21,259
the processing of that data to train the

00:01:19,790 --> 00:01:23,329
model is extremely computationally

00:01:21,259 --> 00:01:25,219
intensive it's probably one of the most

00:01:23,329 --> 00:01:27,619
computationally computationally

00:01:25,219 --> 00:01:29,390
intensive algorithms that we know of in

00:01:27,619 --> 00:01:31,670
computer science if we throw enough data

00:01:29,390 --> 00:01:33,979
at it it's not unlike the processing

00:01:31,670 --> 00:01:35,420
that our brain does when it learns to do

00:01:33,979 --> 00:01:37,100
things over a long period of time

00:01:35,420 --> 00:01:39,829
there's a huge amount of data processing

00:01:37,100 --> 00:01:42,170
that goes on this implies a huge amount

00:01:39,829 --> 00:01:45,469
of computational resources needed and

00:01:42,170 --> 00:01:48,439
GPUs have arisen over the last 10 years

00:01:45,469 --> 00:01:50,119
or so as the premiere device for

00:01:48,439 --> 00:01:56,719
delivering large amounts of

00:01:50,119 --> 00:01:58,039
computational horsepower a number of

00:01:56,719 --> 00:02:00,829
interesting things have happened over

00:01:58,039 --> 00:02:03,229
the last five to seven years one of the

00:02:00,829 --> 00:02:06,079
things is that the whole area of neural

00:02:03,229 --> 00:02:09,200
networks has witnessed a resurgence in

00:02:06,079 --> 00:02:11,300
interest new attention has been paid to

00:02:09,200 --> 00:02:13,730
model development and understanding how

00:02:11,300 --> 00:02:17,270
deep deep learning and neural network

00:02:13,730 --> 00:02:18,830
models can be used secondly GPUs have

00:02:17,270 --> 00:02:19,580
appeared on the scene GPUs have been

00:02:18,830 --> 00:02:22,280
around for 10

00:02:19,580 --> 00:02:23,930
years or so as a computing animal and in

00:02:22,280 --> 00:02:26,240
the last five to seven years they have

00:02:23,930 --> 00:02:28,790
made huge leaps and bounds in terms of

00:02:26,240 --> 00:02:30,410
the delivery of computing horsepower the

00:02:28,790 --> 00:02:32,000
third thing that's happened it's really

00:02:30,410 --> 00:02:33,620
been happening for the past 10 or 20

00:02:32,000 --> 00:02:35,930
years is the whole Big Data revolution

00:02:33,620 --> 00:02:37,550
when we have these piles and piles of

00:02:35,930 --> 00:02:40,130
data that are being generated through

00:02:37,550 --> 00:02:42,890
commerce and the internet and all forms

00:02:40,130 --> 00:02:45,560
of data aggregation we now have a very

00:02:42,890 --> 00:02:48,320
interesting technique that we can use to

00:02:45,560 --> 00:02:50,150
process that data and get answers to

00:02:48,320 --> 00:02:51,800
questions that we would like to ask and

00:02:50,150 --> 00:02:53,960
so the combination of these three

00:02:51,800 --> 00:02:55,730
factors has created what we call the Big

00:02:53,960 --> 00:02:57,500
Bang and machine learning and these

00:02:55,730 --> 00:03:00,080
three factors have really going to

00:02:57,500 --> 00:03:01,490
energize the current wave of artificial

00:03:00,080 --> 00:03:09,130
intelligence work that's going on right

00:03:01,490 --> 00:03:11,450
now so as we've been talking about this

00:03:09,130 --> 00:03:13,220
neural network this deep learning

00:03:11,450 --> 00:03:14,840
challenge that we're facing requires a

00:03:13,220 --> 00:03:17,480
large amount of computational horsepower

00:03:14,840 --> 00:03:18,470
and the GPU is an excellent vehicle to

00:03:17,480 --> 00:03:20,930
get that work done

00:03:18,470 --> 00:03:22,910
however the GPU needs a CPU to get the

00:03:20,930 --> 00:03:25,640
work done as well and the power eight

00:03:22,910 --> 00:03:28,070
and future power architecture CPUs have

00:03:25,640 --> 00:03:30,320
integrated into them a unique connection

00:03:28,070 --> 00:03:33,170
system to the GPU called env link this

00:03:30,320 --> 00:03:36,530
is a high speed data transport bus that

00:03:33,170 --> 00:03:38,570
allows the power8 CPU and the NVIDIA GPU

00:03:36,530 --> 00:03:40,519
to work closely together more closely

00:03:38,570 --> 00:03:43,190
together than any other CPU GPU

00:03:40,519 --> 00:03:45,560
combination and this allows for an even

00:03:43,190 --> 00:03:50,060
higher level of performance furthermore

00:03:45,560 --> 00:03:53,209
the SI 22 LC 4 HPC includes four of our

00:03:50,060 --> 00:03:56,720
most powerful current generation Pascal

00:03:53,209 --> 00:03:58,880
P 100 GPUs so it is directly in line

00:03:56,720 --> 00:04:00,980
with the idea of delivering large

00:03:58,880 --> 00:04:03,459
amounts of computing horsepower in a

00:04:00,980 --> 00:04:03,459
single box

00:04:05,940 --> 00:04:10,930
[Music]

00:04:08,870 --> 00:04:10,930

YouTube URL: https://www.youtube.com/watch?v=75sgRnEJkzw


