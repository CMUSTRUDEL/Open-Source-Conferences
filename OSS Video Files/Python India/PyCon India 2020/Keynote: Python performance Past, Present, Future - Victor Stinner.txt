Title: Keynote: Python performance Past, Present, Future - Victor Stinner
Publication date: 2020-12-03
Playlist: PyCon India 2020
Description: 
	This talk was presented at PyCon India 2020 Online.

PyCon India is the largest gathering of Pythonistas in India for the Python programming language. The 12th edition of PyCon India took place online from 2nd October to 5th October 2020.

Click here to subscribe to the PyCon India channel: https://www.youtube.com/user/inpycon?sub_confirmation=1
Follow PyCon India on Twitter: https://twitter.com/pyconindia
Follow PyCon India on Facebook: https://www.facebook.com/PyConIndia/
Captions: 
	                              so                               welcome back everyone so we have our                               next keynote                               taught by victor stinner so victor is a                               python developer for nine years                               and he is currently working at red hat                               victor has                               has been contributing to open source                               software since                                                       for google open source spear bonus                                one of his greatest projects named                                openstack was nominated for the infinite                                rebate shed in april                                      victor is also the author of piper                                called handler                                and trace mallet modules what do you                                victor                                the stage is yours now                                so hi everybody my name is victor                                stinner                                i'm working for redhats and i'm here to                                talk about                                the python performance the past present                                and                                future and                                i'm going to shut down the video because                                i have some upload issues                                with python performance                                there are multiple implementations of                                the python language                                there is a c python which is the most                                famous                                because it was the first and it is also                                the reference implementation nowadays                                this one was created by guido van rossum                                it was created                                                        one                                jim aguinin created a g python                                later records renamed                                just jaiden this one is written in the                                java language and c python is not                                written in in c                                after this one uh stackless python was                                created by                                christian tismer for a different purpose                                the idea is to run different coatings                                in in not in parallel but to                                it's a concurrent programming it's very                                similar to asynchro but using a                                different implementation                                there is also iron python created also                                by                                engine and jim                                and this one this one is written in                                c-sharp                                using the microsoft clr                                so first target targeting windows                                and more recently there is a new                                implementation called micro python                                by damien george and this one                                is targeting embedded devices with very                                low                                disk space and very low memory                                which means less than one megabyte of                                memory                                this project is very popular and you                                i think it was the same as the one                                running                                in the lighting talk just before                                so there are different uh                                different projects to optimize python                                since                                the creation of a c python the first one                                was created by                                armin rigo and it's called psycho                                the idea was to annotate one specific                                function                                and to to optimize it                                when you execute it multiple times so                                after a specific number of                                execution the the function is optimized                                by a by a jet compiler                                but armin understood that this design is                                not                                is not efficient enough to make a python                                application                                like two times faster so it created a                                new project called                                pi pi and this one is very                                very different because it's a new                                implementation                                there was a project started by two                                employees of                                google called unlondon swallow the goal                                of the project                                was to was to make python application                                running                                for four or five times faster                                and they decided to use llvm                                 another project started by one or two                                 employees of dropbox was called                                 is called by piston                                 and there is also a project started by                                 two employees of a microsoft called                                 pigeon so you can see that there are                                 many                                 there was many attempts to optimize                                 python                                 and i would like to to notice that                                 many of these projects i mean all all of                                 this project                                 except pipeline have an end end date                                 and the end date means that                                 [Music]                                 the project development has stopped for                                 some reason                                 and we will see why they they stopped                                 so there are two two main approaches to                                 optimize python                                 the first one is to take the existing c                                 python                                 for the project and attempt to implement                                 your optimization                                 and the other approach is to write a new                                 implementation from scratch                                 so for the first approach uh for king c                                 biden                                 they are they are an endangered and one                                 and pigeon which took this approach                                 but one of the of the drawback is that                                 performance will be limited by the old c                                 python design                                 and again c python was created                                          ago                                 for example there is a specific memory                                 allocator there is a specific way to                                 store                                 object memory the see structures                                 the there is reference counting                                 to manage object lifetime                                 which means a specific garbage collector                                 a specific way to detect                                 when the objects are should be destroyed                                 and one infamous limitation of c python                                 is also the global interpreter lock                                 known as gill and                                 because of the kill c python cannot run                                 more than one thread                                 and at the same time                                 the second approach to optimize python                                 is to write a new implementation from                                 scratch                                 and for example there are pi pi                                 jaithon on haron python which took in                                 this approach                                 and the great great thing with this                                 approach is that for example jaitun on                                 arrow                                 iron python have no gill at all which                                 means that you can really run                                 many threads in berlin the threads are                                 running on multiple cpus                                 at the same time it's not concurrent                                 programming but really                                 parallelism                                 another nice thing is that you can use a                                 very different                                 way to implement python for example                                 pipeline use                                 an efficient tracing garbage collector                                 he doesn't use reference counting and                                 the nice thing with                                 tracing garbage collector is that the                                 object                                 can move in memory so you can                                 you can shrink the memory to reduce the                                 footprint when you deallocate many                                 objects                                 and also that um reference counting as a                                 as a drawback is that when you run two                                 python                                 threads in parallel or even more                                 the reference content can become a water                                 neck                                 three different threads are accessing                                 the same memory and each time that you                                 have a race                                 race connection you have to protect the                                 memory by                                 atomic operation or locks so the tracing                                 and                                 garbage sorry the reference counting is                                 not efficient                                 for parallelism                                 but this approach as a main drawback is                                 that                                 c python is very famous for for                                 c extensions like a mumbai for example                                 and these implementations written from                                 scratch                                 have no support of the extension on                                 limited support                                 and even if they support a c extension                                 usually it's slower than c python                                 because the c api is not native                                 but they have to emulate the c api                                 for example for the case of pi pi there                                 is a module called the c                                 pi extension which creates a c python                                 pi object on the band and this object                                 has to be synchronized with pi by                                 objects                                 for example imagine that you have a list                                 which is using a very efficient storage                                 for for piper it should it is efficient                                 for the jit compiler it is efficient                                 to reduce the memory footprints but                                 the first time that you use the c api on                                 this list                                 spy pi has to convert the wall list                                 to                                                                  using more memory                                 and they are less efficient for the                                 pipeline jit compiler                                 and the conversion from pipeline                                 to c python layouts requires to                                 to copy memory but also to convert                                 the values so it's not efficient                                 another issue with the other                                 implementation of                                 python is that when you have a different                                 implementation you are                                 in competition with c biden                                 c python project has around                                           core developers                                 to my techniques and a bunch of car                                 developers are                                 even paid to do that which means that                                 they are working for companies who allow                                 them                                 to spend for example one day per week to                                 maintain python                                 in my case i'm paid by reddit                                 to maintain a patent upstream but also                                 downstream                                 on federal on rare                                 and the new features                                 are coming in python python                                      python                                       that you can see in the what's new                                 documents                                 of what's new in python                                               there is a long list of new videos and                                 all of these videos                                 are first implemented in c pylon                                 which means that c which means that pi                                 pi for example has to re-implement all                                 these features                                 but with a smaller team so what you can                                 see is that pie bye                                 is already always a little bit late                                 in terms of pattern version and python                                 features                                 compared to c python i think that today                                 they are supporting python                                                                                                         support in                                 python                                          on the other end the c python is going                                 to be released                                 in version                                                   like next monday i hope                                 so the question is now why would users                                 prefer                                 an outdated and incompatible                                 implementation of python                                 and who is going to sponsor the                                 development of                                 different implementation of python                                 in the case of the en laden swallow                                 projects                                 created nine years ago by google                                 employees                                 the nice thing is that they wrote a                                 report explaining why                                 they stopped the development of the                                 project                                 and i identified three main reasons                                 the first one is that most python code                                 running at                                 google isn't performance critical which                                 means that the performance critical                                 code was written in a different language                                 and python                                 is not really the bottleneck so there is                                 a little benefit so                                 to make pattern really faster                                 the deployments of python and                                 london swallow was too difficult being a                                 replacement was not enough                                 to make python more                                 [Music]                                 and our potential customers eventually                                 found other ways of solving their                                 performance problems                                 and this is something that we see often                                 in biden                                 is that pattern is not the most                                 efficient                                 programming language but there are many                                 ways to                                 work around this limitation so at the                                 end                                 python itself the python interpreter i                                 can see by john                                 is not the the first target when you                                 would like to optimize something                                 in the case of piston as a project                                 created by a dropbox                                 three years ago there was also a report                                 explaining why the development has                                 stopped                                 and i identified two main reasons                                 the first one is that dropbox has                                 increasingly                                 been written writing its performance                                 sensitive code in                                 other languages such as go                                 and the other reason is that                                 we spend much more time than we expected                                 on compatibility                                 this is also an issue that i am seeing                                 often in                                 the other implementation of python                                 is that there are many ideas to optimize                                 python to make it more efficient but                                 when you optimize something                                 it's not uncommon that you change the                                 behavior in a separate way                                 and the issue is that a large project                                 like a django                                 are really based on the assumption that                                 python behaves exactly as as a                                 c python so when you optimize something                                 you have to really                                 behave exactly as a c python and this is                                 something that                                 the pipeline developer has                                 they paid a lot of attention to really                                 mimic                                 the exact behavior of c python and they                                 spend                                 significant time to really be exactly                                 compatible with with c python                                 so the summary of the the past section                                 is that c python remains the reference                                 implementation                                 but it shows its age                                 there are multiple optimization projects                                 but                                 say almost all of them failed                                 and the remaining one piper it's a                                 dropping replacement of c python                                 and it's around four times faster but                                 it's not widely adopted yet so question                                 is why                                 so let's move on to the present section                                 the present of                                 a python performance so to optimize your                                 code                                 the first thing to to consider is that                                 you                                 you have to identify the bottleneck of                                 your application                                 and let's say that you identify your                                 your bottleneck                                 so the question is that how can you                                 optimize this                                 this bottleneck how can you make your                                 code                                 running faster the first thing that you                                 have to                                 have to do is to to just try pipeline                                 because pipeline                                 just works piper is a dropping                                 replacement for c python                                 it's around four times faster than c                                 python in average                                 but i would like to to add that                                 the exact speed up really depends on                                 your your work                                 your workload so it depends on                                 on what which code is running how your                                 code is designed                                 but the great thing is that pipeline is                                 fully compatible with                                 c pylon and what i heard from bye bye is                                 that sometimes                                 there is a small part of your                                 application which is in which is                                 running slower than um than on c python                                 and in this case you can ask wi-fi                                 developers for help                                 and they can explain you how to make                                 your code more efficient on                                 on pipeline                                 but there are some issues with pipeline                                 the                                 the main one is that the support of c                                 extension                                 using the c pi extension model                                 is uh almost as efficient as an c python                                 but sometimes                                 it's slower the great news is that uh                                 two years ago they organized a sprint                                 and they managed to                                 heavily optimize the code                                 but again it's still slower than c                                 python                                 and i would like to to discuss the c api                                 later to explain this issue                                 another issue with piper is that                                 because of the jit compiler the memory                                 footprint of your application                                 is larger when you use pipeline than                                 when you use                                 c pylon it can be an issue if you                                 would like to to spawn many process                                 on your server and your server as a                                 limited amount of memory                                 another last and uh last issue                                 which i would say a minor issue is that                                 when you use pipeline the startup time                                 this                                 is the time just to start your                                 application is usually longer                                 because again the jit compiler                                 if yourself if your application is                                 running for hours                                 or for days you will not notice and it's                                 just fine                                 but there are many usage of python                                 and some people are using python for                                 command line um                                 command line programs which are running                                 for a few seconds                                 and in this case the starting time can                                 be a bottleneck                                 another common way to optimize python is                                 that                                 once you identify your bottom neck of                                 your application                                 [Music]                                 it's common that you you only identify a                                 few                                 files or a few few classes or functions                                 and you can you can start with specific                                 functions and try to                                 revise them in the c language or                                 rest vibrating a c extension                                 or rest extension and by doing that                                 you can you can write way more efficient                                 code                                 because in c you can make more                                 assumption and you can                                 use more efficient way to to implement                                 the same                                 feature and today rest is becoming more                                 and more popular                                 and there are two ways to write rest                                 extension                                 the first one is rest c piden the other                                 one                                 is a pi                                                  this one so i cannot say which one's the                                 best                                 so just try out and make your own                                 opinion on that                                 the nice property of rests is that                                 the the rest compiler provides                                 around warranty that if you write                                 properly your code                                 using the rest memory model the compiler                                 can                                 tell you in advance that you your                                 program will not have memory errors like                                 buffer overflows                                 there are many ways to ensure that there                                 is no race condition                                 and this is really a great great video                                 of the restaurant                                 language but you need                                 responsibility or buy your free to for                                 the glue between                                 pattern and rest for example                                 the mercur projects which is a source                                 tracking                                 ncm source control management similar                                 such as                                 gits but written in python for the                                 mercury project there are some functions                                 which are using heavily the cpu                                 so which are cpu bounds and in that case                                 it's very interesting to rewrite some                                 part of                                 in rests so there is an ongoing effort                                 to rewrite the performance performance                                 bottleneck of                                 mercury in rest and so far the                                 the project is quite successful                                 so let me come back to the infamous                                 global interpreter lock called the gear                                 so in c python there is a lock which                                 prevents you to run                                 many threads in parallel but                                 it depends on your your workload                                 in the case of mathematical functions                                 uh i would say in general a pure python                                 code                                 which is described as a cpu bound                                 cpu bond month means that the                                 performance of your application is not                                 limited by the                                 input on output the i o but it is only                                 limited by the speed of your cpu                                 so for a cpu-bound workload                                 python is not c python is not efficient                                 because you can only run a single thread                                 in at the same time                                 even if your machine has a free cpus and                                 you                                 wrote your code using threads                                 to run them in parallel in practice                                 using c python                                 there is no parallelism there is only                                 concurrency                                 it's one thread after the other so                                 if you imagine a machine with three                                 threads and three cpus                                 here the efficiency is only once one                                 third                                 but the gill is not a bottleneck for                                 any any workloads if your application                                 has a threads                                 and the threads are more higher bounds                                 for example one thread is reading a file                                 another thread                                 is computing the one checksum                                 of the file contents and the third                                 thread is compressing this data using                                 bzip                                  if you have this kind of workload you                                 are not limited by the cpu it's more                                 limited by                                 it's um in this case you can release the                                 gear                                 and if you release again you can really                                 have a parallelism and                                 execute all threads in parallel                                 so if you imagine a machine with three                                 threads and three cpus                                 in that case you have an efficiency of                                                                             if you work load is really a cpu bound                                 there is one simple solution                                 which is called the multi-processing                                 module if you use a multiprocessing                                 module                                 you can have one thread process                                 and run many many process in parallel                                 and thanks to that the operating system                                 is able to                                 execute the process in parallel and                                 really use                                 the power of all use cpus of your                                 machine                                 and the multi-processing module makes                                 make um                                 is making that's easy so you don't have                                 to manage to process                                 yourself the multiprocessing this takes                                 care of                                 spawning the process sending the data                                 retrieving the results and stop the                                 process                                 when the workload is done                                 thanks to the multiprocessing module                                 for cpu-bound workload you can have                                 again                                 an efficiency of                                             and this module is existing is already                                 existing in python for many years                                 so it's a it's a ready                                 ready to use solution                                 so the multi-processing model uh it                                 works around                                 the gill and limitation the global                                 interpreter lock                                 and the great news is that in python                                     release one year and a half release one                                 year ago sorry                                 you can get shared memory and child                                 memory                                 means that you don't have to serialize                                 and copy the memory between the                                 different workers                                 you can just use a chunk of memory which                                 is                                 accessible by all all the workers                                 and thanks to that there is no memory                                 copy anymore                                 so it's way more efficient for very                                 large amount of memory                                 like a very large matrix in                                 numpy and the second great news is that                                 again in python                                                    particle                                 protocol the particle version version                                    again and this new version avoids                                 copying very large objects it's a pep                                                                                   the idea is that you can decide how you                                 send                                 a large amount of uh of memory                                 and for example you can delegate as a                                 serialization using shared memory so you                                 don't have to                                 to copy the memory you can write your                                 own                                 code to decide how to to send                                 the data                                 previously i said that's one way to                                 optimize a function is to                                 rewrite it in the c language the c                                 extension                                 but if you use the c api                                 of c island it can be very boring                                 because you have to manage                                 the memory yourself you have to manage                                 the                                 reference counting you have to manage                                 the exception                                 check for failures of each function                                 and to see language is a less                                 [Music]                                 it takes more time more lines to do the                                 same thing than in biden                                 so the idea of the site and project is                                 that you                                 write code such as python so this is the                                 syntax of your code is very similar to                                 python                                 but you add a few annotation and thanks                                 to this annotation                                 the saturn is able to to produce                                 way more efficient code than you than                                 you                                 would get if you write it in python or                                 in c                                 yourself and                                 the great thing with satan compared to                                 using the ikc api is that if you use                                 cylon you can easily support                                 multiple python versions and also get a                                 better support                                 of pipeline or other python                                 implementation                                 using the same codebase so you don't                                 have                                 to to manage the very subtle differences                                 of the c api between the different                                 python versions                                 and the other nice property of saturn                                 is that you don't have to handle use the                                 reference counting                                 manually saitan is doing that for you                                 and also the the handling the                                 exceptions are all many things which are                                 very boring                                 so you you don't have to write all this                                 bolder                                 plate code                                 and the last great property of                                 satan is that the optimize the saturn                                 optimizer                                 emits efficient code using c python                                 internals for you                                 so you don't know you don't have to know                                 how c python                                 is implemented saturn does that for you                                 and thanks to the knowledge of the c by                                 down internals                                 c python asylum sorry is able to                                 produce way more efficient code than a                                 score that you                                 would write yourself                                 another project to optimize your code is                                 a number                                 number is a different is more                                 specialized to                                 scientific applications                                 for example code using a numpy                                 so number is a jit compiler translating                                 a subset of python                                 and numpy into fast code                                 and there are many many ways to execute                                 the same code faster                                 there are different implementation                                 there is a simplified threading                                 which is a way to run threads but                                 the threads are releasing the game so if                                 you recall                                 what i explained previously what if you                                 release the guild                                 you are able to use all the cpus of your                                 machine                                 and you get parallelism                                 number is also able to emit a single                                 instruction multiple data vectorization                                 which is a way to run the same code way                                 more efficient                                 there are many cpu extensions for that                                 like scc avx ibx                                                                                                  code is the famous gpu acceleration                                 like nvidia cuda but the                                 number also supports imd hoc m                                 which is another way to run your code on                                 the gpu                                 and all the solutions are way more                                 efficient that's                                 the code that you will run usually using                                 a numpy for example                                 and the very nice thing with number is                                 that you don't have to rewrite                                 all your code from scratch what you have                                 to do is just to annotate                                 a few functions using a decorator and                                 that's it                                 to come back to to see python there is a                                 website called                                 speed.python.org which is tracking the                                 performance of python                                 over time the really nice tab                                 is a timeline where you can see the                                 performance over the last                                         here is the example of telco benchmark                                 which is a benchmark on the decimal                                 module                                 it's a benchmark to to compute sums                                 of numbers for                                 [Music]                                 common benchmark called the telco and                                 you here you can see that over five                                 years                                 as a performance of python is becoming                                 uh way better                                 which is a great thing that we don't                                 regress                                 but become more efficient                                 and i spent a lot of time to to make                                 this                                 benchmark more more stable because                                 previously we got                                 a lot of noise in the results a lot of                                 spike faster or slower                                 so what i did is to write a new module                                 called the pi path                                 which is a way to run the benchmark                                 differently to get more stable                                 results and more reducible                                 results and after that i modified the                                 pi and c python benchmark suite called                                 pi performance                                 i modified it to use by buff                                 thanks to that now the results are                                 more stable so it's easier to to use a                                 result to take                                 a decision                                 in summary for the present section                                 pipeline doesn't require any code change                                 so you have to test pipe i on your code                                 and see that it's faster                                 if you identify your the bottleneck or                                 your                                 of your application you can attempt to                                 rewrite                                 a few specific functions as a c                                 or rest x extensions                                 multiprocessing scales it's                                 really enable parallelism on many cpus                                 and it is really easy to use                                 to write the extension use site and                                 please                                 don't use the c api directly                                 and number makes numpy faster                                 okay we saw a lot of existing solutions                                 to make your python code faster and                                 i would like to to                                 to show you a few experimental project                                 to attempt to optimize python even more                                 i spent a lot of time to analyze why                                 c python is slow and why                                 all the optimization project failed in                                 the past                                 and i think that i i'd identify                                 one big reason which is the c api                                 of c biden                                 the c api evolved organically                                 which means that internal functions are                                 exposed                                 by mistake and this c api                                 was first written to be consumed by c                                 python itself                                 there was no overall design                                 so the design is just expose                                 everything and don't think about if it's                                 a good idea or not                                 and                                                             just simple and good enough but                                 what we get today is that the c api                                 expose way too many implementation                                 details                                 and because of that it's way more                                 difficult to optimize by them                                 so in python                                           many changes in on the c api                                 and i'm still working on that in python                                                                     and in the in in the next python                                     and one big change of                                              instead of having a single api at the                                 same place                                 where you get private function internal                                 function                                 and public function as i split                                 this api in three parts so the main                                 include directory                                 is a public stable c api                                 the c python subnear directory is a c                                 api specific to c python                                 and the internal subdirectory is the                                 internal c api                                 so this api is only should only be used                                 by c python itself                                 but i decided to expose it anyway                                 for very specific use case such as a                                 debugger                                 or profiler which has to inspect the                                 the object of python without executing                                 python                                 because that debugger should not modify                                 the state of an                                 application but only inspect what is                                 accurate                                 states and uh                                 many private functions with a prefix on                                 underscore pi                                 and the pi interpreter states structure                                 have been moved to the internal c api                                 in python                                      another interesting thing in python                                 is a stable abi the                                 idea of the stable abi is to support                                 multiple python version                                 for example python                                         and later the next python                                     using a single binary so the idea                                 is that you build your c extension once                                 and you can use the same binary on                                 multiple python version                                 so for example for a linux vendor                                 instead of having                                 one package for python                                                 or python                                     which becomes annoying when you would                                 like to support for example                                    versions of python thanks to the stable                                 api                                 you are able to ship a single package a                                 single binary                                 and support a large number of python                                 version                                 so what change in the python                                     is that the debug build is now abi                                 compatible with the release bullets                                 which is                                 really interesting for debugging purpose                                 because when you                                 get a crash in python it's very likely                                 that the crash doesn't                                 come from python code but more from the                                 c extension                                 and if you use the debug builds you get                                 way more sanity check and running at run                                 time                                 to detect bugs and thanks to this check                                 you it's more likely you get the                                 explanation of or more information about                                 the crash                                 and the thanks to this change you can                                 use your existing                                 c extension in release mode                                 using a python compiling debug build                                 because previously in python                                            to recompile                                 all the extension and that can be very                                 tricky if you get many um                                 many requirements many dependency like                                 header files or compilers                                 or something else just to build your                                 your c extension                                 so to come back to the c api and why                                 it is an issue to expose the                                 implementation detail                                 i would like to take the example of a                                 specialized list                                 in c python a list is basically                                 an array of pointers an array of                                 pointers to pi objects which is a real                                 content                                 of one object but in                                 prepay there is a strategy to get                                 specialized lists                                 so if you consider a list of on                                 which only contains integers pipeline is                                 able to store this list                                 as a as an array of integers                                 so there is no indirection from the list                                 to an object which is stored                                 somewhere else in memory you get                                 directly the value of the item directly                                 in the list and thanks to that you get a                                 lower memory footprint                                 but you also get better performance                                 because there is no indirection                                 so the cpu doesn't have to fetch the                                 the numbers from someone else                                 and the question is now can this                                 optimization be                                 be implemented in c python and can we                                 modify                                 the pi list object structure of c biden                                 and the the answer in short is                                 no we cannot the first problem                                 is that to access an item of the list                                 there is a macro in the c api called                                 s parallelist get item and this macro                                 access directly into the into the list                                 access directly the ob underscore item                                 member of the pi list object                                 and this this item is a pointer to our                                 pi object                                 and the c extensions must not access                                 the pi list object members directly                                 because                                 if you do that you get um                                 you you expose the the                                 direct memory layout and you cannot                                 change the layout                                 because if your machine code is                                 accessing directly the memory to a                                 specific                                 offset if you change the memory layout                                 instead of getting a pi object star                                 you get a number or something else and                                 your problem is going to                                 just crash so the                                 the pile list get item macro could be                                 modified                                 to convert                                 the number of a specialized list to a pi                                 object                                 star but there is a second issue with                                 that                                 and the second issue is called the                                 borrowed references                                 if we come back to the macro pi                                 list get item let's imagine that you                                 implemented the specialized list so in c                                 python a list is made only on numbers                                 and when you access an item of the list                                 the macro magically create an object on                                 demand                                 but if you do that you get a borrowed                                 references                                 which means that the list doesn't know                                 when the caller is still using the                                 object written by the macro                                 or when the object can be destroyed                                 this is a borrower of references and you                                 must not                                 use a pi deck f to decrement                                 the reference counter of the object you                                 must not use it on the object                                 that means that if the macro creates an                                 object                                 the list doesn't know when this                                 temporary object can be described                                  and this is a this is an issue for the                                  for the correctness but also an issue                                  for the performance because you can                                  keep an object alive longer than what it                                  should be                                  we don't know when the object can be                                  destroyed and this is an issue                                  and sadly many c functions of the c                                  api are returning borrowed references                                  so we can we can do a better city api                                  and for that one one thing is                                  to do is to to make the structure a pack                                  which means to to prevent the extension                                  to access the early members of the                                  structure                                  and instead of accessing directly into                                  the structure                                  we we have to use a function call                                  because                                  because the function call is hiding the                                  how the members are stored in memory                                  so for example when you create an object                                  in python                                  using pylon from from long you get an                                  object                                  but currently you can directly access                                  the                                  ob underscore type member of the object                                  to get this type and this is an issue we                                  we should not be able to do that we                                  should                                  hide the way to retrieve the type of an                                  object                                  using for example the pi underscore type                                  macro                                  so second thing to do is to remove                                  functions                                  using borrowed references or functions                                  which still references which is another                                  issue similar to borrowed references                                  and as i said we we have to replace                                  macros                                  with function calls so the                                  when you build your c extension the c                                  extension will                                  only call functions on this function                                  will                                  really do the work for you to hide the                                  information                                  implementation details                                  but by doing that there is one issue                                  that people don't like                                  is a breaking of the backward                                  compatibility                                  because any c api change can break                                  an unknown number of projects                                  and maybe not all c api design issues                                  can be fixed                                  we can try to fix a bunch of c extension                                  uh for example the most popular c                                  extension on pi pi                                  but fixing all six tension on pi                                  pi can take a lot of time or even                                  infinite time so maybe                                  there is another way to to fix this                                  issue                                  there is another project called hpi                                  which is a new c api the idea of                                  hp is to to design the c                                  api correct since day one                                  correct means that you you hide the                                  implementation detail                                  and you design the c api such                                  as it can run and is a mod                                  in the most efficient way on c python                                  but                                  but it's especially in a pipeline                                  because as                                  piper has a different constraint as a                                  different memory layout                                  and h pi um                                  if you use h by is the code is running                                  way more efficient than pipeline                                  the the base design of h bar is that                                  there is no more pi pi object star                                  so there is no more pointer to pi                                  objects                                  is this this api is using pi handle                                  and a handle is a an opaque integer                                  something that you cannot inspect and                                  you can see that                                  such as a unix file descriptor                                  which when you call when you open a file                                  on unix                                  you get a number and you call function                                  you pass this number uh to function and                                  the number is a                                  private reference to an internal object                                  in the                                  lineup scanner for example or you can                                  also see that as a windows handler                                  on windows you can you have also uh                                  when you open a file when you create a                                  unlock when you                                  when you do many operations you get an                                  opaque handle and you only call                                  functions on this on those                                  and the base operation uh open to get a                                  new handle                                  duplicate to to copy and under                                  and close the under                                  and thanks to this new api based on the                                  pi handler                                  you you get better performances as                                  and the nice thing is that you can you                                  don't have to rewrite                                  c c python from scratch you can just                                  take the existing c                                  api and implement h pi on top of                                  the existing c cipr ofc python                                  and this has been done and there is                                  a second implementation of written for                                  especially for pipeline which is take                                  making assumption on pipi internals and                                  thanks to that                                  you can get more efficient code                                  if you write it using edge by resonance                                  and                                  performance that you would get if you                                  use existing c                                  api of c pylon and                                  i think it was last year they managed to                                  rewrite                                  json parser using hpy and the                                  modified jsonparser is running something                                  like four times faster                                  on pipeline than the implementation                                  using the                                  c api the existing c api                                  and four times lesser is really really                                  interesting                                  so it's a it's a proof that the design                                  makes sense and that there is another                                  way to                                  solve this issue                                  and the great thing is that if your c                                  extension                                  is using is written using satan                                  there is a way to not have to modify                                  your code                                  because we can modify satan to generate                                  scores using the                                  the new pioneer for you so using the                                  edge by project so there is an ongoing                                  uh                                  work to modify cyton to generate                                  the i click the h byte code so you don't                                  have to modify your c extension so                                  again please don't write c                                  don't use the c api directly just use                                  cyton                                  okay we'd like to to come back to uh to                                  an issue                                  of c python which is a reference                                  counting                                  the the problem of the reference                                  counting is that it is tied                                  to the to the global interpreter lock                                  and to to to remove the global                                  interpreter lock as a reference counting                                  becomes an issue                                  there is a project called a guilectomy                                  started by                                  larry hastings which is trying to remove                                  the guild from sea biden                                  the idea is to replace a unique guild                                  with one lock per mutable object                                  mutable means that you can modify an                                  object in place                                  like a list dictionary or set objects                                  and for example one way to do that is to                                  implement the reference counting using                                  atomic increments on the command                                  instructions cpu instruction                                  atomic means that if two threads are                                  running                                  the the instruction in parallel                                  the cpu ensure that the operation                                  is there is no race race condition                                  um to to implement the gilectomy the                                  larry tried to to create a log of uh                                  in craft or the craft operation and by                                  doing that                                  it's possible to run the code faster in                                  multiple threads                                  but the implementation is quite                                  complicated and                                  sadly on benchmark this implementation                                  was                                  didn't scale well instead of running                                  code faster than cpiden at the end it                                  was running slower than the pattern                                  which is the opposite of the goal of the                                  overall project                                  so i would say that the world                                  issue with the with the kill comes from                                  the reference counting                                  and the reference counting doesn't scale                                  with the number of threads                                  so what we should try to do in                                            is to replace                                  the reference counting using a tracing                                  garbage collector                                  because many more modern language                                  implementation user tracks in garbage                                  collector                                  like pie pie for example the idea is                                  to be able to move objects in memory                                  and to identify where the object are                                  stored in memory                                  and thanks to that the code is able to                                  run                                  faster in parallel                                  but for for the existing c api if we                                  move to c c python to                                  try single batch collector for the                                  existing c api they will continue to use                                  reference counting for the                                  external codes which will hide                                  how the object are stored in private                                  in the internals internally                                  and to finish there is one cool project                                  called                                  sub interpreters there is a pep                                  written by alex now the pep                                      multiple interpreter in the stone art                                  library                                  the idea is to replace a single global                                  interpreter lock                                  with one lock per interpreter which                                  means                                  that you can run multiple instance of                                  python                                  in parallel and by doing that each                                  interpreter gets the full speed of                                  python                                  so it becomes possible to run                                  to run separated interpreter at a full                                  speed                                  and really scale with the number of cpus                                  there is a work in progress refactoring                                  work                                  of c biden but it's uh                                  it's a long-term project because it                                  takes a lot of time to                                  to modify all the existing code and                                  to uh to not break the backpack                                  compatibility but i think it was in                                  last may i proved that honest on a                                  machine with                                  four physical cpus with four                                  cores it is able to run the code                                  i think up to two times or maybe four                                  times faster                                  using a modified implementation of c                                  python                                  using sub interpreters running in                                  parallel                                  in parallel means one sub interpreter                                  per cpus                                  so i had an an experimental                                  implementation of that                                  and i proved that you get this similar                                  speed or the same speed than                                  multi-processing but inside the same                                  process                                  and the nice thing is that having a                                  single process                                  it's more easy to manage because                                  managing a bunch of process can cause                                  other issues like a                                  larger memory footprint                                  so to to show you to show you that                                  instead of having a single process                                  and multiple threads limited to one cpu                                  per thread                                  so all threads of the same interpreter                                  you have using sub-interpreters                                  the idea is that each interpreter has                                  its own threads                                  and each interpreter is as a dedicated                                  cpus                                  so again you get an efficiency of one                                  hundred percent                                  five minutes left                                  so the expectations of separate                                  interpreters is to                                  get lower memory footprints because we                                  should be able to more                                  to share more memory we should get                                  faster locks because the locks are                                  inside the same process                                  it's not locks between two presses                                  but one of the limitations of                                  sub-interpreters is is that you cannot                                  directly exchange a python object                                  between two interpreters                                  you you you have to design your                                  application directly to not share                                  objects                                  so the summary of the future of the                                  python performance                                  is that the currency api has                                  design issues there is a new pi handle                                  api                                  a new api called called hpy using                                  pyonder which is a work in progress                                  we should try to use a tracing garbage                                  collector for c biden                                  and there is an exciting project which                                  is                                  to run sub-interpreters inside the same                                  process                                  so to conclude there were many previous                                  optimization attempts which failed                                  but saturn multi-processing numpad                                  number but also c and rest extensions                                  are working well to make your code                                  faster                                  and h by tracing gavage collector and                                  seven interpreters are very promising                                  projects                                  to optimize pylon                                  so thank you all i give you links to the                                  different project that i                                  listed like pie pie my notes on                                  how to make c pylon faster to see                                  my notes on the patency api the                                  the speed project where we are tracking                                  the performance of python                                  there is also a mailing list to discuss                                  the                                  optimization project speed at pider.org                                  and the last link is a link to the edge                                  by projects                                  but i will not have enough time for                                  questions                                  so if you you want you can ask me                                  questions on                                  zulip i am victor and                                  thank you
YouTube URL: https://www.youtube.com/watch?v=m6Q-jgoHw6k


