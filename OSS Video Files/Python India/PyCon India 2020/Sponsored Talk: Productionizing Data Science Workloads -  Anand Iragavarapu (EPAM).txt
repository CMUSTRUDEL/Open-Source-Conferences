Title: Sponsored Talk: Productionizing Data Science Workloads -  Anand Iragavarapu (EPAM)
Publication date: 2020-12-03
Playlist: PyCon India 2020
Description: 
	This talk was presented at PyCon India 2020 Online.


PyCon India is the largest gathering of Pythonistas in India for the Python programming language. The 12th edition of PyCon India took place online from 2nd October to 5th October 2020.

Click here to subscribe to the PyCon India channel: https://www.youtube.com/user/inpycon?sub_confirmation=1
Follow PyCon India on Twitter: https://twitter.com/pyconindia
Follow PyCon India on Facebook: https://www.facebook.com/PyConIndia/
Captions: 
	                              very special session                               we have uh the gold sponsors of pycon                               india with this epam                               and we're going to add anand to the                               stream now                               hey anna can you hear me hey hello                               everyone                               nice to meet you so yeah you're very                               fairly audible now let me check the chat                                if everything is fine                                everything is fine now aaron this stage                                is all yours                                okay so uh                                hello everyone uh i'm anand                                i'm a delivery manager at the epam                                systems                                uh i've been in the it industry for more                                than                                             i've been working on                                productionizing data science workloads i                                have been working on data products and                                data platforms for different customers                                trying to kind of get their data sense                                projects from models to                                production so today i wanted to talk                                about                                uh productionizing data science                                workloads and                                and some of the tools which we use for                                productionizing them with an epam of                                course                                so how does a model go from development                                to production                                if you look at a typical ai product life                                cycle right so we have the ideation                                phase where we are defining                                the problem statement the scope of the                                problem we are defining different                                metrics                                uh which you're using to kind of define                                the model or evaluate the model                                you're determining how much of the model                                is actually explainable and whether the                                end users can actually                                understand why the model is behaving                                like that so that they can actually make                                decisions                                once we have kind of defined the problem                                well enough                                we talk about the data preparation part                                where we kind of figure out                                what kind of data sets are available                                are they clean do we need to clean them                                do we need to wrangle them                                do we need to kind of transform them                                into a format                                which is which you can put to the models                                like especially if it is text data we                                need to vectorize them convert them into                                uh into a vector wherein the model can                                kind of                                understand it and go forward and we need                                to do that in a scalable way so we                                we build the etl pipelines we build it                                at data pipelines                                and we call one set of data a data                                product                                so once we have the different data sets                                available                                we kind of do exploratory data analysis                                where we kind of                                do do the training sets visualize the                                data                                try to identify patterns and decide what                                kind of ml algorithms we can run on                                top of this data to achieve our goals                                once we have done all these things we                                kind of                                train test and tune the ml model uh                                once we have achieved satisfactory                                performance of the model                                in the chain and test environment we                                build a binary file                                and in python we can use a we can use a                                pickle file                                and once we kind of build the binary                                file we kind of                                uh deploy the binary file into the                                production system                                now building the binary file would                                typically be a                                pipe should if it is a good engineering                                practice we would automate it with                                continuous integration pipelines                                and help kind of that would kind of                                also automate the training of it so that                                we can repeatedly                                do it faster okay                                [Music]                                 once we have kind of built a model we                                 need to kind of                                 expose it to the end users uh                                 which can be done in multiple ways one                                 of the ways is to kind of expose it as a                                 restful service                                 or a web service some of the other ways                                 is to kind of                                 store the predicted values in a in the                                 database                                 which is further consumed by other                                 applications to                                 do further processing and finally once                                 the deploy development is done we have                                 gone through multiple iterations                                 we go through the production phase where                                 we kind of try to monitor                                 the prediction accuracy                                 and the feedback on the model on a                                 continuous basis                                 we need to kind of continuously monitor                                 the model because the data might change                                 there might be new patterns which might                                 be coming in                                 and we need to be modified monitoring it                                 continuously and                                 once it kind of changes we need to                                 ensure that we go back                                 in cycle go go back using your feedback                                 loop                                 re-prepare it explore it retune it                                 and integrate it with the production                                 system                                 okay so for us to do all these things                                 we need a lot of frameworks and that                                 framework should                                 be able to kind of help us in building a                                 strong                                 injection strategy be able to kind of                                 store the data store the model                                 models provision computation resources                                 for us                                 both for storage as well as for um                                 computation um like if if the ingestion                                 is if you're if you're using big data                                 uh large data we would need to spawn off                                 amr clusters or                                 hadoop clusters or we need to maintain                                 hadoop clusters                                 request for resources build them and                                 kind of pull that through                                 similarly if it is first in case of                                 storage we need to talk                                 we need to kind of store it in hdfs s                                  or something equivalent                                 the framework should also help us in                                 following the engineering                                 best practices so that                                 sorry                                 sorry about that we also need to be                                 following the engineering best practices                                 within                                 um the uh                                 we need we need support in terms of git                                 repository versioning model versioning                                 code versioning in case of                                 data science projects and ml projects                                 just versioning code is not sufficient                                 we also need to version the data or we                                 need to kind of                                 monitor changes to data patterns and                                 that would kind of the framework should                                 help us kind of                                 do that and finally we should have help                                 from the framework in deployment and uh                                 operation operationalization of the                                 model and that is typically done by the                                 cicd pipelines                                 in a software engineering project but in                                 case of a data science project we would                                 need                                 additional capabilities like the typical                                 continuous integration process or the                                 continuous development process                                 of a software engineering project which                                 is not                                 completely applicable for a data science                                 project because we really can't have a                                 lot of automation tests in place                                 the way we kind of uh use the feedback                                 loop is different                                 uh we we can't have a simple uh pass or                                 fail test we would need to kind of look                                 at what are the details of the pass or                                 fail                                 uh fail part of the                                 execution okay so uh i wanted to                                 talk about two open source projects                                 which uh we kind of                                 uh use uh try their opensuse projects as                                 well as frameworks and tools which we                                 kind of use                                 one is d-lab which is essentially the                                 exploratory environment for                                 doing collaborative data science and                                 other is odaho                                 it started off as legion as a project                                 for                                 machine learning ci cd but now it has                                 kind of gone into a universal                                 environment                                 where we kind of have built multiple                                 things                                 around it okay so as part of d lab                                 we generally it's an uh fail-safe                                 self-service exploration environment                                 for collaborative data science work we                                 we use uh self-service web consoles we                                 can use self                                 service web consoles uh we can                                 build layers and like in an enterprise                                 environment                                 uh we want kind of uh specific                                 access to specific environments                                 and a limited access across                                 the across the environments so we can                                 kind of create isolated sandboxes within                                 the environment and that and d-lab                                 actually helps us                                 build isolated environments for playing                                 around                                 and for building uh the exploratory                                 environment                                 for us so we basically do it by enabling                                 the lab to kind of demand compute                                 engines                                 demand notebooks jupiter notebooks is                                 built on top of jupyter notebooks                                 uh how private data storage using                                 uh independent data product data buckets                                 etc                                 so so and we can kind of create                                 different collaboration levels                                 to ensure that the data sources are                                 shared the code repositories are shared                                 uh what level of sharing we can do                                 across the enterprise                                 so odaho is the other tool which we use                                 very frequently which                                 helps us kind of modularize our own our                                 entire machine learning                                 projects and machine learning                                 products okay uh so it helps avoid code                                 rewrite                                 it helps uh what you call community                                 prevents migration and communication                                 issues                                 it helps in kind of                                 scaling the entire machine learning                                 project it helps in kind of provisioning                                 infrastructure                                 we kind of integrate with it has a                                 inbuilt cicd pipeline specifically tuned                                 for machine learning                                 which we can kind of execute on okay                                 so we kind of use it for both on-prem                                 solutions as well as                                 cloud solutions so we kind of can                                 integrate with the                                 spark scaler and tensorflow at the same                                 time we also                                 the the tool also integrates with uh                                 cloud services like aws                                 uh gcp uh we haven't yet connected with                                 azure yet but with                                 aws and dc tv we we do kind of uh                                 connect and uh we integrate with them                                 and                                 use use it for spawning the                                 infrastructure require required for                                 training                                 our models so uh it also uh                                 has different uh layers like you can                                 build on the local machine you can build                                 on the training environment or the                                 training                                 machine and you can have the final                                 execution environment which can kind of                                 uh finally get promoted into a                                 production environment okay                                 so uh if you look at the in continuous                                 integration                                 available in some of these tools that                                 are what we expect from a tool for us to                                 kind of                                 productionize a machine learning model                                 is that                                 we should have exploratory environments                                 which can which we can use for training                                 and testing data sets                                 it should be it should enable us to use                                 ml tool chains                                 which are like chains of                                 tools one after the other you should                                 have data storages                                 ability to storage store different data                                 integration with data lakes                                 wherein we can kind of run data                                 engineering pipelines to                                 take the data and actually push it into                                 a usable format                                 we should have training environments and                                 ability to kind of uh version both                                 models as well as i won't say data data                                 we don't really                                 we can't really version them but we                                 should be able to identify                                 differences in the data shapes which                                 which we kind of get                                 so so we have our we have a docker                                 images                                 register inside this tool                                 which will help us kind of ensure that                                 we push the right images to                                 to the right environments and similarly                                 we can kind of look at the training data                                 sets                                 test test data sets and see how the data                                 set shapes change over a period of time                                 and that would kind of come out as part                                 of our training reports                                 and we can version those training                                 reports and store those training reports                                 etc at the same time we want the compute                                 resources to be spawned up spawn down                                 uh whenever they require so our                                 provisioning scripts so the tools                                 provisioning scripts will help us                                 uh spawn up the more spawn of the                                 computer resources spawn down the                                 compute resources so that we use                                 uh the compute resources which are                                 absolutely required                                 and and helps us build safe costs across                                 the world                                 so and once the training is done we can                                 actually push it                                 to the to the                                 final production where the product is                                 actually                                 executed we can scale up scale down the                                 services uh we need to log it                                 monitor it uh and ensure that we have                                 the right                                 alerts apply a b testing if you want to                                 do                                 do the a b traffic split do a b a b                                 testing                                 if you if you want and kind of uh                                 finally                                 get good reports out of it                                 okay so in conclusion                                 when we kind of for creating and                                 productionizing                                 data science in the real world we                                 require comprehensive and collaborative                                 end-to-end environment that allows                                 different stakeholders different                                 participants to collaborate together                                 work on the same set of data and work                                 closely                                 and be able to kind of incorporate                                 feedback easily and quickly across the                                 entire data science life cycle                                 and this would help us build a reliable                                 and                                 repeatable environment                                 following the good                                 engineering practices                                 thank you for your attention and i'm now                                 open for questions                                 thank you anand let me just quickly go                                 through                                 the chat and uh get some questions out                                 of there                                 okay so okay let me                                 it's a very big question so i'm just                                 gonna break it in chunk and put it on                                 the screen so that everybody can see                                 so the first part i guess goes like this                                 if i miss out something um please wait                                 for the second half                                 so the direction of whether composition                                 is better or better classes plus                                 interfaces are better a good choice for                                 the previous one                                 isn't this regarding the same oh yes sir                                 this looks like yeah yeah yeah so i                                 guess                                 the um attendees are still having                                 conversation with the v                                 from the previous challenge i apologize                                 on that no problem if                                 you have any questions right now feel                                 free to uh                                 share with them share with uh anand on                                 zulip                                 you have the stream named as                                             stage hyphen delhi                                 or if you have a quick question feel                                 free to                                 just post it right here on the hopping                                 chat i'm here to pick it up                                 uh because we'll have to quickly move to                                 the next section and then only had                                    minutes                                 and we have another                                                    uh                                 with stefan so okay no problem no                                 problem                                 uh so let's let's uh invite anand to                                 zulit                                 and feel free to go on solip uh ping                                 anand                                 make sure you put your questions there                                 and ask for any other resource that you                                 need                                 so i will also be available on the epams                                 exceed booth so you can reach me out                                 there as well
YouTube URL: https://www.youtube.com/watch?v=KQmsmBqmjLs


