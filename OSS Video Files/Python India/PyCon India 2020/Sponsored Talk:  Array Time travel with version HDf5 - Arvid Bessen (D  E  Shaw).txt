Title: Sponsored Talk:  Array Time travel with version HDf5 - Arvid Bessen (D  E  Shaw)
Publication date: 2020-12-03
Playlist: PyCon India 2020
Description: 
	This talk was presented at PyCon India 2020 Online.

PyCon India is the largest gathering of Pythonistas in India for the Python programming language. The 12th edition of PyCon India took place online from 2nd October to 5th October 2020.

Click here to subscribe to the PyCon India channel: https://www.youtube.com/user/inpycon?sub_confirmation=1
Follow PyCon India on Twitter: https://twitter.com/pyconindia
Follow PyCon India on Facebook: https://www.facebook.com/PyConIndia/
Captions: 
	                              hello everyone                               i hope you all had a great lunch before                               this                               and uh we just had an amazing lightning                               talk session                               and after that we had victor gave a                               keynote                               um talk and it was so great to know                               about                                the present and future of python and                                where the community is headed                                now after all of this i hope you all are                                relaxed had a good                                break we have a very special dock lined                                up for you                                from our platinum sponsors deisha                                we have arvid with us and i would like                                to add her with                                to the stream right now and we still                                have a couple of minutes to go                                so we'll just yeah hey everybody can you                                hear me now                                i can hear you can you hear me yeah i                                can hear you clearly                                feel free to introduce yourself we still                                have a couple of minutes to start so if                                you want you can                                go on with the introduction and more                                about dja                                okay i actually have a slide for that so                                maybe i'll do that now                                um so i'm arvid bessen i work at the                                deshaw group                                i'm in our new york office we also have                                a big office in                                hyderabad and                                i'll talk about what i'm going to talk                                about in the talk                                but do you sure um yeah                                i have like this this one slide here                                that sums it up                                so we are a global investment and                                technology development firm                                we are known as a pioneer in                                quantitative investing                                so we've been doing that for                                         and what does quantitative investing                                mean while we use a combination of                                quantitative and qualitative tools                                uncover independent                                hard to find sources of return across                                global public and private markets                                okay in case you're wondering what that                                means that means we                                take a lot of data um it's a                                quantitative part and run some                                algorithms on them that tells tell us                                where to invest                                and we do that across yeah                                the world global public and private                                markets so it's                                um public markets in this case uh stock                                markets private markets would be                                private offerings so still i guess                                half a minute left so i'm just gonna                                circulate the news around                                if you're going to start the session on                                that stage so                                if uh if you are on the daily stage                                right now feel free to reach out to your                                friends on zulit                                tweet about it and ask everyone to just                                join daily stage right now we're going                                to start with the session                                and uh it's about time so i'm gonna                                share my screen                                yeah you have a screen right now                                okay let's get started                                all right so this talk is about array                                time travel                                with versioned hdf                                 this is an open source project you can                                find the code on                                github the link is right here and                                it was created by the deshaw group                                which is where i am working on i'm                                working at                                my name is arvid bessen and                                uh yeah let's get started so                                i already showed this slide so the                                deshaw group                                we're a global investment and technology                                firm                                what i'm talking about today version                                hdf                                 was done in conjunction with a firm                                called quansite                                quantsite is a data science and                                analytics consulting firm                                 specializing in open source software                                 they are committed to building the open                                 source data economy                                 by connecting the people and                                 organizations who participate in                                 creating value                                 from data and dee shaw collaborates with                                 quant side on                                 numerous other open source projects not                                 just version htf                                  including for example number task and                                 jupyter                                 and you can google them they're all                                 really cool python projects there might                                 be even a talk about                                 some of those today                                 all right so but what are we going to                                 talk about today                                 um first let's get a handle on the                                 problem that we're trying to                                 tackle when we                                 do computations at the shaw what we do                                 is we have data that comes in                                 and then we have the sort of flow graph                                 of how the data flows through the system                                 we compute something on the data                                 and then eventually you get some result                                 back                                 the problem is that data gets updated                                 every day                                 so let's say a simple example you look                                 at stock prices right so you get                                 new stock prices every day the stock                                 market trades in fact you get stock                                 prices                                 every second or every millisecond how do                                 you                                 handle that data gets updated while                                 you're running computations on them                                 that's the main focus of this talk                                 um the way we are going to tackle that                                 is we're going to                                 first take a step back by looking at                                 hdf                                             file format for numerical data and                                 h                                                                      that                                 that exposes that numerical data as                                 numpy arrays                                 and then we're going to build on top of                                 h                                    and we have this library called version                                 hdf                                  that's in the title of the talk that                                 wraps h                                                               track the evolution of data and                                 finally at the end hopefully version                                 hdf                                                         data flow evolution or updating problem                                 all right so let's look at an example                                 for a data flow graph                                 um i hope you can see my mouse um so                                 um data flow basically means you have                                 some data that you get from somewhere                                 right so you get your stock prices so                                 you get your                                 temperatures from all the various places                                 around the world or you get                                 your latency measurements from                                 all the computers that you own or you                                 get your web page impressions                                 all this data you get from somewhere and                                 then you want to compute something on                                 that                                 right so you combine data one and data                                 two you compute something                                 and probably get some result that that's                                 i call that intermediate result one                                 um that you might want to store on disk                                 as well                                 and then you compute a little bit more                                 on that and eventually you know after                                 many steps after combining many                                 data sets you get your final result                                 i call that res.h                                                     the                                 hdf                                                     you could store it anywhere it could be                                 stored in any file format                                 it could be stored in the database we                                 have roughly the same                                 sort of procedure data comes in you                                 compute intermediate results                                 on the intermediate results you compute                                 more and eventually get some final                                 results                                 now what happens when data gets updated                                 so if you look at your computation like                                 this graph you can see                                 oh if let's say data                                                i only need to rerun computation                                         means i                                 need to rerun computation                                               i need to rerun computation                                              get my new result                                 i don't have to run computation                                       computation                                    that's of the nice part about thinking                                 about your                                 computation as sort of a flow graph                                 where the data flows                                 through your computation                                 but what are the                                 when you get these updates well                                 we have two problems that i want to talk                                 about today                                 the first one is how do we achieve                                 consistent results of the data changes                                 during the computation let's go back to                                 our                                 previous slide so let's say you know                                 data one gets updated right that                                 means we run computation one which means                                 we get the intermediate result one which                                 means we run computation four                                 now computation four reads data                                    what so basically so dependencies are                                                                                                                            while we're running computation                                   somebody                                 updates data                                                        whatever it is                                 new data from your satellites or your                                 your mobile phone network or whatever                                 you're getting                                 um if data                                                that means we'll also need to recompute                                 this path of the graph right so                                 computation three                                 the meter is r                                                        computation five                                 um computation five combines                                 this result with this result but if you                                 look closely you see that data                                         in                                 through two paths                                 and this computation                                                     have an old                                 you could have used an old version of                                 data for while this path                                 could have used a new version of it data                                 four so we will get inconsistent results                                 because data four is not used in                                 uh at the same sort of point in time                                 consistently                                 all right that's problem one if data                                 changes                                 we get inconsistent results the other                                 problem that                                 we're trying to tackle is                                 let's say you know you run your                                 computation and you run it again the                                 next day                                 and then you update it again on the day                                 after that                                 then somebody comes to you and says so                                 the numbers you produce                                 two days ago are wrong how did that                                 happen                                 you know we we                                 lost some money because you suggest the                                 wrong trades in our                                 example that's what we do at d show how                                 do you explain results if the stored                                 data has already changed right it's gone                                 we don't                                 we don't easily reconstruct the data                                 well the solution is well known in                                 computer science                                 literature world it's a temporal                                 database                                 unfortunately that's not something if                                 you go to google and say buy a temporal                                 database                                 unfortunately that's not something you                                 can just do um there are                                 very few commercial vendors that sell                                 you a temporal database                                 it's very much still a research topic um                                 sql server microsoft sql servers are                                 probably the closest                                 but they don't have a true real                                 temporal database either                                 so what does a temporal database do a                                 temporal database                                 records how data changes through time                                 so what's time well there's actually two                                 times                                 and they are normally called valid time                                 and transaction time value time means                                 when a fact became true in the real                                 world                                 think about a time series what if you                                 have your temperature measurements                                 i don't know where you guys are but                                 where i am                                 it's now um                                                              yesterday it was                                                                                                    degrees                                 so you look at this this temperature and                                 you can see how it changes through time                                 that's the                                 time in the real world but then                                 you record those times                                 and their temperatures in the database                                 or in your file or                                 in your store whatever it is                                 um at a certain                                 time as well normally there is a lag                                 right so you have some                                 some data that you measure right now it                                 says uh                                 at this point in time it is this uh                                 degree                                 out there but then it comes into the                                 database                                 a little bit later so let's look at the                                 example right so we say the temperature                                 or                                 whatever it is at                                         is one and it takes a little bit to you                                 know                                 go from the sensor to the internet to                                 our database                                 and at                                            we record in the database yep the                                 temperature was one                                 and then we get another data point right                                 at                                      and                                                                      is                                 now three and then again it takes a                                 little bit                                 to go into the database um                                 so this is the valid time axis once it                                 tells you                                 this is the time in the real world the                                 temperature at                                                 temperature at                                               this is when things landed in the                                 database now if i query                                 my database and say what was the                                 temperature                                 at                                    um over three let's say                                 i'm zero seconds well then                                 uh it would give me this one back but if                                 i ask it                                 um what was the temperature at                                      and                                                                  this three back                                 but uh i can also combine that with the                                 transaction time my query i can say what                                 was the temperature                                 at let's say                                    over three and                                               but did you know about this before                                                                                     and the reason why we do this combined                                 query is                                 because there are situations like this                                 where you know                                 a data com a datum comes in late at                                      and                                            the temperature was two right so now the                                 the value                                 changed in the real world goes from one                                 to two to three                                 but this is only record in the database                                 at a much later time                                 so this kind of database is called a                                 bi-temporal database because it has                                 two time axes it tracks                                 when our data is sort of                                 when a data point is true in the real                                 world and when it enters the database                                 that's kind of what we want we want to                                 record how our data changes and most of                                 our data has the                                 time access it says you know this is the                                 stock price at                                 for this time this is the temperature at                                 this time                                 okay so this is what we want what do we                                 have well what we have is we have numpy                                 arrays we're using python we're using                                 numpy                                 we store those numpy arrays on disk                                 but for now let's just think about why                                 are we using numpy arrays well                                 they're great they're fast as index                                 access                                 our developers love them because they're                                 very very familiar to them                                 and for homogeneous data let's say all                                 integers or floats                                 all dates they're very efficient                                 there's a couple of options to store                                 numpy arrays on disk                                 i listed some here we are using h                                    which is a wrapper around hdf                                          an open source                                 yeah file format um with an associated                                 high performance library that's used in                                 high performance computing and h                                    exposes that                                 high performance file format as a numpy                                 array                                 cool so we have that we have arrays                                 how do we use that well um it's                                 relatively simple you just open a file                                 for writing in this case and then you                                 can restore                                 any numpy array that you want in that                                 file and                                 you can store in fact uh more than one                                 numpy array in the file                                 by giving it a name so i create the                                 so-called dataset foo                                 with this numpy array then i can                                 read that foo data set back out and i                                 can slice it                                 and i can print that numpy array and i                                 can you know                                 write more than one i can also create                                 nested                                 data sets right h                                                  idea of groups which are like folders                                 let's in the bar folder i create two                                 two data sets bass and boo and i can                                 store all my numpy arrays in there                                 cool so we have arrays covered                                 how do we version them well here's the                                 trick                                 so this is what we're going to use the                                 feature of hdf                                                 versioning work                                 hdf                                                 numpy arrays just in a contiguous way                                 like they are stored in memory                                 but numpy is or arrays in general are                                 contiguous blocks of memory so you have                                 you know                                 a million ins and they're all right next                                 to each other in memory                                 what you can do with hdf                                             break this up                                 so if in this case i have a two                                 dimensional array i can break it up into                                 these little chunks here they're all the                                 same size                                 and hdf                                                              individually why is that better well                                 it's better because                                 if i don't want to read the entire array                                 if i just want to                                 access a subset then i just need to look                                 which chunks contain that subset and to                                 only read or load only that                                 into memory only the chunks that i'm                                 actually                                 they actually care about                                 okay chunks and                                 versions how do we combine them well if                                 we want to version an array                                 versioning means we want to record the                                 transaction time                                 we need to be able to handle events                                 inserts updates and deletes so it                                 depends means                                 new data goes to the end                                 inserts mean you insert something in the                                 middle and then                                 everything shifts to the right updates                                 mean                                 you have some data that changes but the                                 array itself                                 stay the same and                                 deletes means you know some elements                                 disappear                                 and then things shift to the left                                 as you probably know from experience                                 with erased                                 just you know in general when you're                                 doing computations with them                                 inserts and deletes are kind of                                 expensive because the old data moves                                 either to the right or the left so you                                 need to                                 update a lot of things um                                 after the point that you modified well                                 pens and updates are kind of                                 cheaper or good because we only touch                                 new data and unfortunately it will turn                                 out for our version hdf                                       that same will be true so let's see how                                 we                                 how we do versioning on hdf                                  well we do use the idea of the chunks                                 that we talked about so                                 version answer hdf                                                that is called a virtual data set that                                 says                                 i can create a data set that doesn't                                 have any data                                 actually stored on disk it just maps                                 its chunks to the chunk of some other                                 data set                                 so let's say we create our version                                       we create some raw data which contains                                 the actual data                                 right so you have chunks in this case                                 from                                                                                                       and we just you know we map chart one to                                 chart one chunk two                                 to chunk two chunk three to chunk three                                 we haven't gained much                                 right we have just a cheap virtual data                                 set but                                 what's the what's the benefit of this                                 well                                 if we write to chunk one and chunk two                                 what we can do is we can actually write                                 copies of chunk one that are modified                                 i call those chunk                                                    also modified i                                 write chunk                                              i can create a new version let's say                                 version                                   where of my virtual data set that points                                 chunk                                   to this chunk                                                                                              while chunk                                                           data                                 so we have two virtual sets v                                         i call them here they both point to this                                 raw data set and they're all                                 accessible all the time because the raw                                 data set                                 is immutable we never change anything                                 cool so summary                                 how do we version how do we do                                 versioning we create a virtual data set                                 for each version and each version is a                                 view on the raw chunks                                 okay so how do we use this                                 well here's a little demo so                                 same syntax as before we open a file                                 with hdf                                               then we wrap this file handle in what we                                 call a version hdf                                       and now we can create version zero and                                 we do that by                                 creating this by having this context                                 here where we say with                                 stage version and we pass in the name i                                 called it v                                                         anything you want and then in this block                                 here                                 you can create your data set                                 and you can assign whatever you want and                                 you can see what i did this assigned the                                 increasing numbers from                                                                                                      next version                                 very similar so open the file                                 wrap the file in a version hdfi file                                 create this context for version v                                  you manipulate your data set                                 in that context all the changes in that                                 or all the                                 changes you you do within that context                                 are accumulated                                 then once you exit this context block                                 once exit this with                                 block it is saved to the file                                 so let's see moment of truth can we get                                 the old data and the new data                                 yes we can so we open the file we've                                 wrapped the file handle and the version                                 hdfi file                                 we can get the current data by saying                                 for                                 version file or version file.current                                 version                                 dataset name just you know slice all of                                 it                                 and indeed this is the current version                                 that we wrote in the previous slide                                 we could also say vf of v                                              also also access things by timestamp                                 that's what i'm doing to get the old                                 versions right so you can get the old                                 version either by name                                 so i said vf of v                                  and that's what is printed here or vf of                                 sometime                                 stamp that's printed here as well                                 so you can get both versions and if you                                 write                                 many more versions you can get all of                                 them                                 as well okay                                 one thing that we need to talk about is                                 how do we reuse chunks if you move                                 things right if you insert or delete                                 things how do we                                 reuse chunks how do we figure out which                                 which                                 virtual data should point to which raw                                 data set                                 the way we do that is by using content                                 hashes                                 by using sha                                                          what git does so in the git                                 version control system you                                 look at each file you hash the contents                                 and see                                 is there already an entry                                 for that hash and if there is then                                 you can you can reuse that you know it                                 didn't change                                 how does it look for us well if we                                 create a data set                                 right we have a virtual data set v                                     my                                 uh so the original version mapping to a                                 raw data set                                 and i picked a chunk size of just three                                 because otherwise it doesn't fit on the                                 slide                                 normally of course you would pick                                 something much bigger right so in this                                 case                                 chunk size is explicitly specified as                                 three                                 so one chunk another chunk                                 this is the raw data set                                 but we also keep a map                                 of content hashes as those are                                                                                  to those slices here                                 and if i then modify my data set let's                                 say i insert                                                                                   so my version one now looks looks like                                 this you can                                 see the arrows right zero one two then                                 the next one goes here                                 seven seven seven followed by three four                                 so the order sort of switched around                                 this hash map allows us to discover that                                 we can                                 reuse data we can reuse the chunk                                 that's kind of a neat trick that we that                                 we exploit                                 um yes there are some collisions but                                 they're extremely unlikely the same way                                 that git                                 has to worry about collisions um and we                                 will implement that                                 in a future version hopefully soon                                 okay what's the performance of this um                                 well remember the slide                                 where i said that inserts and deletes                                 are bad                                 and events and updates are good if you                                 do mostly appends and                                 um mostly updates                                 then the performance penalty over just                                 writing the file                                 straight it's not that big if you do a                                 lot of you know things where                                 data moves around you have a big                                 performance problem                                 well not too big but this is like a                                                                                                            there's a lot more information in this                                 blog post that are linked here                                 at the bottom so                                 almost done we have versioned arrays but                                 what did we want we wanted bi-temporal                                 data we wanted by temple tables in fact                                 well what's a table a table as you                                 probably know from                                 from experience with pandas data frames                                 or                                 similar libraries it's just a collection                                 of columns                                 and the columns are just arrays so                                 let's build our table out of columnar                                 arrays and then when a table                                 changes when we update something                                 we can do that by using transactions                                 right so we                                 modify all the arrays that are in the                                 table in one version                                 and then we can get this kind of table                                 well we update it                                 to version one by updating all three                                 arrays in the table                                 simultaneously as well so they all need                                 to grow                                 six and then we write in your data and                                 then you can get                                 version tables and you can see um in my                                 example i                                 picked something that is actually turned                                 on turns out to be by temple i have a                                 valid time in here                                 this is the stock price for google and                                 this is the stock price for apple                                 and this is how they evolve through time                                 and i can look at version                                   and version                                                       simultaneously                                 all right did we solve our problem i                                 think we did so                                 um we were worried about consistent                                 computation if the data changes while we                                 are computing                                 we can do that by add picking a                                 transaction time let's call it t                                 at the start of the computation then                                 querying all the data                                 with that t                                 can we explain results if stored data                                 has changed                                 yes we can right because all the data                                 and all the old versions are still there                                 we just need to pick the right                                 transaction                                 time and we can query it                                 at the prior transaction time                                 all right and that's it so version hdf                                  can                                 version numpy erase it's a drop in                                 replacement                                 feature complete high performance and                                 it's open source you should try it out                                 and                                 contribute and if you have any questions                                 i'm happy to take them now                                 okay let's see how does this work why                                 would i when i see the questions                                 hey avid yeah that was great                                 so we have questions from the audience                                 number one                                 um interesting like using append only                                 logs                                 basically we are indexing the data sets                                 rather than scanning complete arrays                                 right                                 uh yes so the append only log is a very                                 good comparison right so you                                 you have then sometimes you have a                                 transaction log where you append all the                                 things that you modify it's a little bit                                 different                                 from a transaction log in a database                                 transaction the database will also say                                 what you do let me just store the                                 content                                 and um then yes what we do is we we use                                 this sort of                                 indexing of you know you you have this                                 virtual data set that                                 maps all the indexes of the virtual data                                 set to the                                 raw the log or the raw data                                 that that we saw                                 and because that's you know of one right                                 indexing is this cheap you just                                 uh just need to say okay i want index                                                                         so that maps to index let's say                                          the virtual to the raw data set you can                                 just                                 get that and that's that's cheap and                                 fast                                 okay so we have a second question does                                 using                                 hdf                                                       um i think it does i don't i mean to be                                 honest i've never used it with anything                                 other than                                 than ssds it's                                 the way it works internally and i'm not                                 an expert in hdf                                  so i just sort of i work two abstraction                                 layers                                 higher than that um the way hda                                        internally is it's                                 uses a b                                                         you have all those chunks right and                                 builds a b tree on the chunks so you can                                 insert in the middle                                 and and all that and um                                 so it knows which chunks to load from                                 where                                 and um i think                                 it actually works much better with ssds                                 because with a                                 spinning disc you would need to make                                 sure that                                 you don't seek that's the problem with                                 spinning disks always                                 uh you need to read as much                                 data as you can while making                                 around the around the data around                                 the disk with ssds you can do this seek                                 where you                                 follow the b                                                           the data here's the other                                 data here's a chunk that i want to load                                 got it                                 just one last question what are the                                 these                                 typical chunk sizes typical chunk size                                 is uh normally we try to tune for the                                 operating system buffer size                                 so i think most operating systems have                                 sort of one megabyte or two megabyte                                 buffers so we use it on linux                                 um so you try to sort of                                 choose your chunk size so that depending                                 on the data size but if it's a                                 floating point number that's eight or                                 double that's eight bytes                                 so you would pick you know                                 two to the                                                        that's the chunk size and if it's you                                 know it's a                                                                                                  you know                                                     and that's what you're trying to upgrade                                 so to all the attendees                                 will be present on sulip so feel free                                 to head to delhi stream you'll find                                      hyphen stage hyphen delhi                                 so head there feel free to ask any                                 question you would like to                                 and uh i will be present there if you                                 want some other resources regarding                                 the section that we had feel free to ask                                 okay so thanks a lot urban any                                 closing session oh sorry closing remarks                                 no that that's it so i'm heading over to                                 that uh                                 delhi state right now so ask me any                                 questions thank you                                 you
YouTube URL: https://www.youtube.com/watch?v=ZZ0ku_6VCac


