Title: Talk: Discrete Probability Distributions: Learn & Test with tools from Fourier Analysis - Bala Priya
Publication date: 2020-12-03
Playlist: PyCon India 2020
Description: 
	This talk was presented at PyCon India 2020 Online.

PyCon India is the largest gathering of Pythonistas in India for the Python programming language. The 12th edition of PyCon India took place online from 2nd October to 5th October 2020.

Talk Detail: https://in.pycon.org/cfp/2020/proposals/python-programmable-logic-controllers-a-step-by-step-guide~dwp98/

Click here to subscribe to the PyCon India channel: https://www.youtube.com/user/inpycon?sub_confirmation=1
Follow PyCon India on Twitter: https://twitter.com/pyconindia
Follow PyCon India on Facebook: https://www.facebook.com/PyConIndia/
Captions: 
	                              welcome everyone                               bala priya will be starting a talk now                               hello everyone can you all let me know                               if you can                               see my screen can somebody let me know                               okay then i would start                               so hello everyone i'm here to deliver my                               talk on discrete probability                               distributions                                learning and testing using tools from                                fourier analysis                                so over the next                                                       rough agenda that i hope to cover                                so first will be the motivation for the                                problem followed by                                a little bit of details on what exactly                                is distribution membership testing                                and then i would go over the details of                                what exactly is an nksi                                irv and then on the discrete fourier                                transform                                and its sparsity properties which will                                be the key ideas that we'll be using in                                this problem                                and then we would go over the pseudo                                code of the algorithm with a little bit                                of code snippets                                and then i would leave you with                                references and some open problems                                and a few papers that you can read from                                if you're further                                interested in exploring on this area                                so yeah so why do we even have to talk                                about probability distributions                                so like in today's world we hear about                                data quite a bit so data analytics data                                science                                everything everything involves data here                                and then with the huge amount of data                                how do we analyze it mathematically so                                like whenever we have data there is also                                an underlying probability distribution                                so like the data points should                                essentially come from an underlying                                probability distribution                                and how exactly do we make some                                inferences about                                it this is a very fundamentally relevant                                problem in                                statistical analysis so let us just                                proceed further                                to know more about it okay                                so these are some of the widely studied                                problems                                one is distribution learning where you                                learn                                an approximation to the distribution in                                its entirety                                so ideally you would want the                                distribution to be as                                close as possible to the original                                distribution which means your                                error should be minimized between your                                estimate and the original distribution                                and then there's this well-known problem                                of parameter estimation                                in which you seek to estimate certain                                parameters of the distribution so                                let's just probably consider the hello                                world of                                probability examples tossing of a coin                                so like let us consider a simple                                bernoulli trial                                so as we know a bernoulli random                                variable takes value                                       probability p and takes value                                       probability                                          suppose you are given the problem to                                estimate the probability of success                                t what would you do let us say i give                                you a coin                                and then let success even be the                                probability that you get                                heads and how exactly would you go about                                estimating the probability of success                                which essentially is the parameter                                that the bernoulli random variable takes                                like the bernoulli random variable is                                parameterized by the success probability                                p                                so a very natural thing that you would                                do is                                just take the coin toss it n times                                and then just count the number of times                                you got heads                                add them all up divide them by the total                                number of times you                                toss the coin so this would be an                                estimate to the probability of success                                 it's also the maximum likelihood                                 estimate                                 if you've done a course on estimation                                 theory but then this is a very natural                                 thing to do right                                 so similar to this you might as well                                 estimate the parameters of complex                                 uh distributions so here we will look at                                 the problem of distribution membership                                 testing                                 so let us say there is some family of                                 distributions you can consider it as a                                 very big                                 set and then you have some underlying                                 distribution                                 and some oracle there gives you access                                 to samples from a distribution like you                                 can                                 sample from the distribution collect as                                 many samples as you want                                 do whatever you want with the samples                                 and in the end                                 what is your end goal you should check                                 if the distribution                                 that this data point comes from whether                                 or not it belongs to a particular family                                 or class the class as i said you can                                 just consider it                                 as a very big set you know like lots of                                 probability distributions residing under                                 it                                 you can consider that as class and then                                 you have                                 access to samples basically and then you                                 would like to see if this                                 distribution belongs to that set door so                                 this is called a membership testing                                 problem                                 so like don't worry if things are not                                 very clear at this point we would                                 also try to you know break it up into                                 smaller chunks in the                                 coming slides                                 so as i said earlier in statistical                                 learning this                                 is a fundamental problem and                                 as i said you have to decide membership                                 whether or not it belongs                                 so how many samples do you need in order                                 to decide that                                 do you need very few samples do you need                                 two large samples two large samples                                 would                                 ideally not be sample optimal so the key                                 question that arises is how many samples                                 do you need to see to decide                                 because the distribution could                                 potentially come from                                 like a very large alphabet size meaning                                 it could take a very large                                 set of values in that case how many                                 samples do you need to decide                                 so these are some of the questions that                                 immediately arise you know because                                 every time they are dealing with                                 computational complexity and we                                 wanted to be computationally as                                 efficient as possible                                 okay so the key idea here would be                                 in two steps in step one you would learn                                 the distribution                                 assuming that it belongs to the                                 specified class or family                                 and then in step                                                     your assumption was correct                                 so this is like you are learning                                 first first step is distribution                                 learning which i spoke about in one of                                 the earlier slides                                 you learn assuming that it belongs to                                 the specified class                                 so when you check in step                                              get                                 evidence that your assumption was                                 correct or you run into a contradiction                                 and you decide that your assumption was                                 wrong                                 okay so the class of distributions that                                 would be                                 considered in this particular talk is                                 the n k                                 s i irv so nksi irb                                 is essentially a sum of n independent                                 integer random variables and each of                                 these random variables is supported                                 in                                                        if you can see this figure here                                 okay i hope you can see my pointer as                                 well                                 so this is a discrete uniform                                 distribution probably the simplest of                                 distributions when it comes to discrete                                 distributions                                 which is uniform in the interval a                                 through b                                 and then each of these points have an                                 equal probability of                                 appearing so this if you just switch to                                                                                        by k minus                                                         get a k independent integer random                                 variable                                 and then you add up n such independent                                 copies so that is what it says                                 you have a random variable that takes                                 value                                             minus                                                                  being equally probable and then you add                                 n such independent copies and the                                 resulting distribution is an n k                                 s i ire okay so for simplicity you can                                 even call it in case of                                 if that makes things simpler to                                 pronounce so                                 a toy example like if it is in uniform                                 and                                     through                                                               can                                 see is three here and each of these                                 points                                 zero one and two will have a probability                                 of one by three                                 of okay                                 so this is exactly what i had cited in                                 the previous one                                 each encase if you take has support from                                                                           k minus                                                                because each of the n random variables                                 has support                                                     and ideally you are adding up n such                                 independent                                 copies so even though they are less                                 probable you may still get                                 value k minus                                                            ideally the support will be from                                   through k minus                                               yeah so as i talked about the key                                 concept                                 with effect                                                    and the variance so this is the key                                 concept                                 any nk serve which has sufficiently                                 large variance                                 has small effective fourier support                                 okay                                 on what uh exactly is meant by small                                 effective fourier support we would just                                 do it in a couple of slides from now                                 okay so this is just a quick recap on                                 discrete fourier transform                                 so you essentially have any                                 signal in the time domain which you seek                                 to convert to the frequency domain                                 and you essentially try breaking the                                 signal                                 into frequency components and then you                                 essentially want to analyze how much of                                 each frequency component                                 is there in your time domain signal so                                 this is the gist of the discrete fourier                                 transform                                 don't really worry if the math here is                                 too much to take in because                                 when you code it in python it's just one                                 line of code                                 ignore if you don't like the math                                 details                                 okay so this is also the inverse dft                                 essentially you are mapping back from                                 the frequency domain                                 to the time domain so this again                                 sums up what dft and idft                                 are and now we would go to the algorithm                                 which learns the nksi irb                                 so this input that you have is sample                                 access to a ksi irb                                 let us say i am calling it p and then                                 there is some epsilon which is greater                                 than                                   so let us say you have oracle access to                                 this                                 particular thing sample axis and then                                 in step                                                                  number of samples from the distribution                                 and you estimate the mean and the                                 variance                                 okay so yeah let us just                                 say uh let us go over this code snippet                                 so i have imported the numpy library                                 with its usual alias and yeah                                 if i want initial sample size to be                                     and let us say i                                 fix n to be                                                           this is how i can generate samples                                 so np dot random dot triangle this would                                 essentially give me k and size is equal                                 to n                                 so k in the sense this denotes that the                                 values return could be anywhere between                                                                                                                                            separate probability list so by default                                 all of them will be equally probable and                                 that is the case that we are looking at                                 now                                 okay so size is equal to n so you take n                                 such                                 samples and then add them up that is why                                 the sum here                                 for x in range of initial sample size so                                 you want initial sample size that many                                 samples you want                                 so this is exactly what it does you may                                 even use the scipy stats module because                                 there again you have some                                 random variable functions you can freeze                                 a random variable and then                                 you can call methods on it but then                                 numpy is a little simpler                                 than that so this as you can see                                 is the empirical distribution that is                                 if you can see in the previous slide                                 after                                 estimating the mean and variance this                                 could just be the sample mean and sample                                 variance                                 so you are drawing additional samples in                                 order to find the empirical distribution                                 so this exactly is one such empirical                                 distribution                                 which was obtained for k is equal to                                   which means the support would be                                                                                                                verify this because                                 uh k minus                                                       so here you have                                                     tail values are very less probable                                 and in the empirical distribution these                                 values do not occur very often you can                                 see that there is a nice bell shaped                                 curve you know similar to a gaussian so                                 that is why sums of random variables are                                 of interest because many of the results                                 like                                 law of large numbers central limit                                 theorem all of them lie on                                 sums of independent random variables                                 okay                                 so let us move to the next slide                                 okay now that we have basically covered                                 step one and two                                 this is step three so if you can                                 remember the key                                 idea that i showed you a few slides ago                                 that any random variable which has                                 sufficiently large variance                                 has approximately sparse fourier support                                 so                                 just to check if the variance is large                                 enough you are doing a threshold check                                 if it is not greater than that you just                                 output the empirical distribution and                                 it's done that's it else you go on to                                 compute                                 this m and then find its fourier                                 transform                                 so let us just see so from the estimated                                 variance                                 there is threshold check if yes you have                                 to compute                                 n move to step four else you output q                                 and then stop that's it                                 so let us just see what is so different                                 about dft                                 modulo m so as you can see                                 dft modulo m would require our signal to                                 have                                 m samples like the length of the                                 distribution that we are considering                                 should as well be                                 m so what would essentially happen if                                 you apply directly                                 the fft routine if you directly call the                                 fft routine what would happen is that it                                 could only take                                 the first m points and ignore the rest                                 which means                                 we are not accounting for the whole of                                 the distribution so in order to account                                 for the whole of the distribution                                 so we have we really have to fold the                                 distribution okay                                 so let us say you have point number zero                                 on the distribution                                 and then through m minus one this would                                 occupy m positions and then the mth                                 point you fold back and then add to the                                 zeroth position                                 so this you do until your array entire                                 input array is taken care of okay so the                                 folded empirical pmf                                 will still be of length m okay so this                                 is one such um very simple code snippet                                 that does this                                 so i need only till m                                 and then i have number of force this i                                 can                                 identify from the length of the                                 empirical pmf                                 and then if it exceeds m i just fold                                 back and wrap it around                                 and make it coincide with the                                 pre-existing that is what this                                 code's effect does i hope it's clear                                 thus far                                 so this is the next step computation of                                 effective support so if you can see the                                 effect of support set                                 l this artistic                                 l that is here so it essentially                                 collects all                                 zeta for zeta in                                                     such that this condition holds                                 so let us not worry about this let us go                                 over the code which is a little simple                                 to understand                                 so essentially here you have i and j                                 between                                   and k and then there is delta between                                   and                                         so account for all of that and then you                                 collect                                 all those zeta for zeta in                                             minus                                                                   met absolute of zeta by m minus i by j                                 is ideally less than this particular                                 quantity over here                                 so this you have to extend the support                                 loop                                 over all possible i and j values                                 and then compute all those zeta or                                 rather collect all those zeta for which                                 this condition is true                                 and then effectively you can find the                                 effective support                                 so there may be like disjoint sets but                                 still you can just find the union                                 and collect them all into a single list                                 called effective support okay                                 so now that you've found the effective                                 support there is another                                 thresholding step that essentially                                 checks                                 if the fourier transform at that                                 particular zeta                                 is greater than a specific constant                                 okay so this is just another                                 thresholding check                                 [Music]                                 and then you essentially compute the                                 inverse dft                                 or rather you can even retain the tft                                 or h hat as you can see is no longer a                                 probability distribution you started                                 with the probability distribution                                 and then you applied an fft so now it                                 doesn't have to be a probability                                 distribution per se                                 but it's very close to being                                           what this                                 step                                                              condition is meant                                 if this l                                                        is less than epsilon squared by                                      this is                                 true you accept else reject so this                                 accept essentially consider                                 means that this hypothesis is accepted                                 which means your assumption that this                                 distribution was an encaser was correct                                 and we initially started with an nk                                 server                                 so it is true the assumption is true                                 so this is essentially the last step for                                 every zeta in the support set                                 i am looping through that and then                                 checking if it is                                 less than this epsilon squared by                                   so if you accept it the underlying                                 distribution is an encaser                                 and if you reject it the underlying                                 distribution is not an encaser which                                 means your                                 initial assumption was wrong                                 okay so these are some of the results                                 that i've obtained from simulation                                 i have taken all these k to be                                 prime powers is to be specific powers of                                 two                                 and then i have taken n such samples                                 and then computed this uh                                 all of this and then i've also logged                                 the support set                                 and the actual support for delta equal                                 to                                     so the actual support you can just get                                 by boolean indexing of the array                                 so essentially you would want the                                 fourier mass                                 outside the effective support be                                 arbitrarily small                                 okay so yeah you can find that there are                                 certain cases                                 when the sigma is less than threshold                                 which means                                 these random variables do not have large                                 enough variance                                 so you can just output the empirical                                 distribution and stop you did not                                 go ahead and compute the effective                                 support                                 so if you can see this                                 the effective support here is comparable                                 the support that you                                 obtain is just one larger than the                                 actual support                                 whereas if you consider this case k is                                 equal to                                   the actual support is just of size                                   whereas the support that you get from                                 the algorithm                                 is huge so this is one                                 scope for improvement and it's an open                                 problem which is very evident you know                                 looking at the table you can see                                 but this method of using fourier                                 analysis has been                                 used as a general testing method which                                 means it is not just restricted to the                                 nksi irb that i spoke about now                                 but rather it covers even a                                 broad variety of distributions you know                                 if you can just look it up                                 poisson multinomial distributions                                 poisson binomial distributions all these                                 can be                                 covered under a general testing                                 framework using this fourier analysis                                 but then as i said there is scope to                                 tighten the support                                 so if you tighten the support can you                                 possibly get                                 any improvement in sample complexity                                 that is the question that we seek to                                 address if you go                                 all the way back to the algorithm where                                 it all started                                 so in step you are drawing some                                 number of algorithms so this epsilon is                                 essentially                                 if you can see towards the end it's the                                 distribution and                                 it's like the distance between your                                 distribution                                 and the hypothesis which determines                                 whether or not you                                 accept the hypothesis so if it should be                                 very close                                 to your distribution then this epsilon                                 should be very small                                 but as epsilon becomes small because you                                 have an epsilon square in the                                 denominator                                 this n scales twice as fast as epsilon                                 so if you just pick your epsilon to be                                                                     even for an improvement of                                               essentially need                                                        i hope you get how it scales so if you                                 tighten the support                                 can there be any possible improvement in                                 this m can you draw fewer samples to                                 decide                                 that's exactly the question that we                                 would be seeking and let me go all the                                 way to                                 [Music]                                 this slide that i was in                                 [Music]                                 and then there are there is another                                 question as to this                                 uses the fourier transform so are there                                 other transform techniques that you                                 could as well use                                 and then achieve the same generic                                 testing framework that's another open                                 question to be decided                                 so these are basically the references                                 this first paper has been like a key                                 paper that i started reading about                                 three months back and it's on testing                                 for families of distributions by other                                 fourier transfer                                 so this is the code paper which was                                 presented at new rips                                      and there's also an archive version if                                 you would like a detailed and formal                                 reasoning of proofs                                 called optimal learning via the fourier                                 transform                                 for sums of independent integer random                                 variables                                 and then there's another related paper                                 learning sums so this slide you can find                                 in                                 python's website as well so feel free to                                 look at these references when you're                                 free                                 so yeah thank you                                 thank you for attending my talk                                 so if there are any questions i would                                 take them up                                 at this point                                 thank you priya for an enlightening talk                                 and                                 you know giving such an involved                                 analysis into it                                 uh let me check if there are any                                 questions                                 so the first one we have is can you list                                 some practical                                 example applications where i use this                                 concepts                                 so let us say you have data which is of                                 this form                                 let us just think that there are people                                 in categories and then you have uh                                 you basically assign a number to each of                                 them let us just say                                 uh there are like                                                        index them from                                                          look at related the groups of them                                 and that would essentially you can try                                 to model your data                                 as an in case a irb if it is possible                                 so like wherever you have integer valued                                 data types                                 which i believe is quite common so you                                 can use                                 this to try to formulate it as an nksi                                 irb                                 and then you can apply all these so                                 maybe practical example applications                                 would depend on the dataset that you are                                 considering                                 i would say                                 so we have to look at                                 those points here                                 thank you priya for answering the                                 questions uh                                 in case you have more questions uh priya                                 will be available in the hyderabad stage                                 on zulip chat so you can always post                                 your questions there                                 thank you priya for joining us today                                 yeah sure thank you                                 you
YouTube URL: https://www.youtube.com/watch?v=o8YjPB35GA4


