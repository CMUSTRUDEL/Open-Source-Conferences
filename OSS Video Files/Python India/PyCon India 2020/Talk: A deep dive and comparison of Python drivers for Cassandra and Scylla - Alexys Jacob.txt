Title: Talk: A deep dive and comparison of Python drivers for Cassandra and Scylla - Alexys Jacob
Publication date: 2020-12-03
Playlist: PyCon India 2020
Description: 
	This talk was presented at PyCon India 2020 Online.

PyCon India is the largest gathering of Pythonistas in India for the Python programming language. The 12th edition of PyCon India took place online from 2nd October to 5th October 2020.

Talk Detail: https://in.pycon.org/cfp/2020/proposals/a-deep-dive-and-comparison-of-python-drivers-for-cassandra-and-scylla~azVya/

Click here to subscribe to the PyCon India channel: https://www.youtube.com/user/inpycon?sub_confirmation=1
Follow PyCon India on Twitter: https://twitter.com/pyconindia
Follow PyCon India on Facebook: https://www.facebook.com/PyConIndia/
Captions: 
	                              alexis                               you are on the stage now hello hello                               everyone                               yes can you hear me clearly yeah can you                               hear me as well                               yeah great voice is scared i'm gonna add                               your                               uh presentation as well on the screen                               are you able to see that yeah great                                great                                so stay tuned alexis thank you very much                                well hello everyone i'm really happy to                                be at pycon india                                so in this talk i will detail cassandra                                and                                sila low level architecture and explain                                how it's used by python cassandra driver                                and show you how we extended it to write                                a python driver for sila                                there will be diagrams emojis python                                code and hopefully amazing performance                                graphs                                let's start with a bit about myself as                                you may have guessed from my accent                                i'm from i'm living in paris                                france so that's where i'm broadcasting                                from actually at the moment                                and i'm cto at numberley and i have                                different lives in the open source world                                i'm a gen                                                  where i focus mainly on packaging                                cluster related stuff distributed                                databases such as                                mongodb and obviously sila                                and i'm also part of the containers team                                which means that i spent some time                                making sure that the gen                                                are on the docker hub and then                                on the open source side i'm also a                                contributor of                                i've been contributing to mongodb sila                                obviously as well                                apache airflow and i'm a psf                                contributing member which means that i                                spend a fair amount of my time                                creating or contributing to python                                open source projects                                in this talk i will suppose that you are                                familiar with the basics of                                consistent hashing and cassandra if                                that's not the case or you simply want                                to know more                                or even use it in your python                                applications                                check out the talk that i gave on the                                subject in the previous                                europa item edition but fear not have                                still worked on                                not making this a problem for this                                presentation so everything should be all                                right                                let's get started now and let me                                introduce you to cassandra and silatokin                                ring architectures                                we'll see all they have in common but                                also what sila has done that makes it                                special                                and worth a dedicated python driver                                first of all since i will be using their                                logos in diagrams                                let me introduce them to you the i on                                the upper left                                is the logo of cassandra and the little                                sea monster with                                also only one i is sila                                when you see both of them on the same                                box it means that the presented topic or                                logic applies to both of them                                but when you will see only one of them                                present in a box it means that                                it's specific to either cassandra or                                either sila                                the cassandra or sila cluster is a                                collection of nodes                                or instances visualized as a ring                                all of the nodes should be homogeneous                                using a shared nothing approach                                it means this means that there's nothing                                special                                about a node in this topology a node has                                no special role or anything                                they are all equal on the ring                                this ring is called a token ring in                                which the positions of the nodes on the                                ring                                define token ranges or partitions a                                partition                                is a subset of data that is stored on a                                node                                 this data is usually replicated across                                 nodes                                 thanks to a setting that is called                                 replication factor                                 the replication factor defines how data                                 is replicated on nodes                                 for example a replication factor of                                   means                                 a given token or token range will be                                 stored on two nodes                                 this is how high availability is                                 achieved and how cassandra                                 and sila favor availability and                                 partition tolerance in the caption                                 they are labeled ap this kind of token                                 our                                 ring architecture is sensible to data                                 distribution                                 among the nodes we could get an                                 unbalance of data and query load in the                                 above scenario                                 where we store data in three big                                 partitions                                 and each node holds one range from the                                 previous one                                 and one range from the next one you see                                                                                                                                   this could lead to something that we                                 call hot nodes or hot partitions                                 we need to add more variance in                                 partition to node allocation                                 and this is done by introducing the                                 concept of                                 virtual nodes so instead of placing                                 physical nodes on the ring                                 we replace many virtual instances of                                 them                                 called virtual nodes a virtual node                                 represents                                 a contiguous range of tokens owned by a                                 single node                                 so it's a smaller slice of a partition                                 a physical node may be assigned multiple                                 non-contiguous                                 v nodes this is it for cassandra's data                                 distribution but sila                                 goes one step further on each cellar                                 node                                 tokens of the v-node are further                                 distributed among                                 the cpu cores of the node that                                 are called shards this means that the                                 data stored on sila                                 on the sila cluster is not only bound to                                 a node but can be traced down to one                                 of its cpu cores this is the feature                                 that we will leverage                                 on the python sealer shadower driver                                 later on and we'll get                                 into more details about this but now                                 that we understand how data is stored                                 and distributed                                 on the cluster let's see how it's                                 queried by clients                                 on the physical layer a partition is a                                 unit of data                                 stored on a node and is identified by a                                 partition key                                 a partition key serves to identify the                                 node                                 in the cluster that stores a given                                 partition                                 as well as to distribute data across                                 nodes                                 in a cluster the partitioner or                                 partition hash function                                 using a partition key determines where                                 data is stored on a given node in the                                 cluster                                 it does this by computing a token which                                 is a number                                 for each partition key so if we look at                                 the diagram                                 we have column id which is set up as the                                 partition key                                 we would take the value of it we will                                 apply the partition hash function                                 which by default is murmur hash three                                 and then we will get                                 a token which is a number that we will                                 place on                                 the ring and from this position on the                                 ring we will did                                 we will know which nodes it belongs to                                 so let's recap on cassandra the hash                                 of the partition key gives you a token                                 telling you                                 which node has the data that you are                                 looking for we can call this a sharp                                 pair node architecture on cila the same                                 hash of the partition key                                 gives you a token not only telling you                                 which node has the data                                 but also which cpu score is responsible                                 for handling it                                 so we can call sila architecture a short                                 per core                                 architecture now                                 how does a client driver query a                                 cassandra or sila cluster                                 well since the partitioner hash function                                 is known                                 our client libraries can use it to                                 predict data location                                 on the cluster and optimize their query                                 routing                                 but let's first explain what a naive                                 client would do                                 if not using the partitioner hash                                 function                                 when a client connects to a cassandra                                 cluster it opens a connection to every                                 node of the cluster                                 then when it wants to issue a query                                 knife clients                                 would pick                                 one of its connections randomly and                                 issued a query to the node                                 this node is called the coordinator for                                 the client's query                                 it takes the ephemeral responsibility of                                 routing the query internally to the                                 right replicas                                 gathering their responses and responding                                 to the client                                 but if the coordinator is not a replica                                 for the queried data                                 it has to issue the queries to all                                 replicas itself                                 this is suboptimal as it consumes                                 network and processing power on the                                 coordinate or node                                 for something that the client could have                                 guessed in the first place                                 and this is what we call token aware                                 clients token our clients                                 applies the partitioner logic to select                                 the right connection                                 to the right node and make sure that its                                 coordinator node                                 is also a replica of the queried data                                 as a result we save network hubs and                                 processing                                 allowing for lower cluster internal load                                 and reduce                                 query latency let's see how cassandra                                 driver does it                                 in the python cassandra driver token                                 awareness is achieved thanks to the                                 token aware policy the python cassandra                                 driver defaults to token aware                                 and data center aware round robin load                                 balancing query routing policy it's a                                 bit long to say and it looks complicated                                 and all but it's                                 it's not that much but fear not it works                                 even if your cluster                                 is not spread between multiple data                                 centers it will just                                 pick a default one for you but by doing                                 so                                 query rooting will not only hit the                                 right node holding a copy of the data                                 that you are seeking                                 but load balance queries evenly between                                 all its replicas so if we look at the                                 diagram you see that if we were to issue                                 a query that would hit                                 partition number one that leaves in that                                 x the first one would go                                 maybe to it this one then if you were to                                 reissue the same query again                                 we see we see that partition one is also                                 living on node y so the load balancing                                 strategy would then                                 hit it not y and then not x and then not                                 y etcetera etcetera on the run robin                                 manner                                 this is how load balancing is achieved                                 from the point of view of the python                                 cassandra driver the partition key                                 is seen as a routing key which is used                                 to determine which nodes are replicas                                 for the query                                 to allow our python driver to know about                                 the partition key of a query                                 the query itself must be prepared as a                                 server                                 side statement cassandra's                                 ncls prepared statements can be seen                                 like stored procedure                                 in the sql world it is the recommended                                 and most optimal way to query data                                 because you declare your query once and                                 then only                                 pass a reference to it and the needed                                 parameters                                 it is also the safest as it prevents                                 query injections so please only use                                 prepare statements even more in                                 production                                 so i've put on an example of how it's                                 done                                 you see the session.prepare where we                                 prepare our statement                                 and then we just execute the query with                                 the reference                                 to the statement object and the                                 partition key which will act as a                                 routing key                                 this is awesome and optimal one config                                 that we cannot do better than this                                 that's true on cassandra cluster but not                                 with a sila one why well if you remember                                 because silas shares the data                                 one way further down to a node cpu                                 so that means that we can extend a token                                 aware driver                                 and token aware client to become a shard                                 aware                                 client which is able to route its query                                 not only to nodes                                 but to their right cpu cores                                 and you can note that it will be                                 achieved with the exactly the same code                                 which is pretty cool so you just have to                                 switch from cassandra driver to sila                                 driver and obviously                                 run a sila cluster and then you will                                 have                                 a core cpu core                                 query routing so such driver                                 already existed for us works of data                                 stacks cassandra drivers                                 except for the python one and it made me                                 sad and angry at the time                                 so when i attended silas submit last                                 year in san francisco                                 i did some lobbying and found some                                 friends on the sila dev team                                 and we promised each other to make a                                 python shadow our driver                                 and as a matter of fact we did so                                 what are the expected structural                                 differences between the cassandra driver                                 and it's a sila driver fork                                 the main one as you can see is how we                                 connect                                 to the cluster and to the nodes                                 when in cassandra you have one                                 connection per node                                 because the node is the main and                                 only point that you will be using to                                 root your queries                                 you will need to go further to the core                                 in sila so that means that you will have                                 to open                                 one connection per core per node on the                                 sila                                 shardaware driver then the token                                 calculation                                 that selects the node is the same                                 because this is the same logic                                 but then you go one step further again                                 on sila                                 because you have a shard id calculation                                 that you will be using to select the                                 right connection to the right core                                 based on your query                                 now let's see what needs to be done on                                 the python code side                                 remember that we want to retain                                 compatibility with the cassandra cluster                                 so that even the sealer driver can work                                 and seamlessly with the cassandra                                 cluster                                 and israel faster who is a                                 sila developer from israel i know it's a                                 bit confusing                                 i made the made the first pull request                                 for this python shard awareness                                 all the the words that i highlighted in                                 green                                 represent a class in the python                                 code so all those classes we will have                                 to                                 to check and one of the first thing i                                 contributed myself                                 was aim to solve the connection to every                                 core problem so that's the                                 the second bullet point here when you                                 need to make sure somehow to connect or                                 to get a connection to                                 every core so that any query that is                                 coming in you will have a connection to                                 use                                 and straight to the right core it's                                 easier                                 said than done actually because when i                                 first looked                                 at the initial pr from israel i found                                 out that                                 it was technically not possible for a                                 client to select                                 the shard it wants to connect to on the                                 con at the connection time                                 and after thinking about it it sounds                                 pretty obvious because                                 the sila the cassandra protocol is not                                 uh                                 does not root queries to core so there                                 was no point in the protocol itself and                                 there was no manner in the                                 main protocol itself to do this and                                 since the sila protocol and cassandra                                 protocol                                 are fully compatible sila did not                                 provide this as well                                 which means that all silas hardware                                 drivers are affected by this as well so                                 i wrote                                 an rfc to the cella dev mailing list                                 to find a solution and great news one                                 has been found                                 and is now implemented and it will be                                 part of sila                                     it takes the form of a new shard                                 allocation algorithm                                 that will be made available on a new                                 listening port on the sila server                                 it will on the server side use a modulo                                 of the client source                                 socket port to assign the correct shard                                 to the connection                                 that means that the clients will then                                 have to calculate                                 the right source socket port to get a                                 connection to their desired shard id                                 until then i worked on implementing an                                 asynchronous an optimistic way of                                 dealing with this problem                                 because when you connect you don't know                                 for sure                                 which shard id you will get a connection                                 to so you have to implement some logic                                 to not slow down the connecting and                                 the the popping up of your of your                                 initialization of your of your python                                 driver                                 um but still make out the                                 most of the chance to to get the the                                 right connection to the right core                                 so let's see this this logic that is                                 implemented                                 this is this is where the connection                                 selection happens and where everything                                 is glued together                                 resulting to a shard aware query routing                                 so the first thing that we do                                 is we get a token and we need to                                 calculate the shard id                                 from the query routine key token so we                                 got this                                 and then we will get the shard id so we                                 know which core                                 which is the shard or core id on the                                 node                                 that is responsible for this this slice                                 of data                                 then we will try to find a connection if                                 we have                                 happen to have a connection to the right                                 shard id and core                                 if we do victory we use our direct                                 connection to the right core to root the                                 query                                 this is the ideal scenario if we don't                                 we will just issue asynchronously a new                                 tentative a new connection                                 and maybe we will get lucky and this one                                 will get us                                 connected to this um this shard id or                                 any other missing shard id so this is                                 the current version                                 an optimistic but non-deterministic way                                 the upcoming version when we will be                                 able to use the source port-based                                 algorithm will be deterministic so we                                 will be sure to always have a connection                                 to every shard                                 so if we were not lucky this time we                                 will just pick a random one                                 and just be as good as cassandra driver                                 was                                 one important step also was to implement                                 a way to know if we are connected to a                                 cassandra                                 or sila server detecting shard awareness                                 is done by parsing                                 the message options set back by the                                 server after we connect it                                 in this example you can see that the                                 silashard information tells us which                                 shard id was assigned to the connection                                 by the server                                 and you get also some other interesting                                 things                                 from the sila server side another                                 important implementation                                 was the famous token to shard id                                 calculation                                 so when the cluster class requests a                                 connection from the pool                                 it passes the routing key this routine                                 key is then used                                 by the shard id calculation code to                                 select the connection to the right core                                 but this calculation when it was                                 implemented in pure python was having                                 a severe impact on the driver                                 performance                                 so the first production results were not                                 good at all we were even slower than the                                 cassandra driver so                                 it was a bit disappointing at first so                                 israel                                 moved the shard id computation to saitan                                 cutting down its latency impact by                                 almost seven and this made                                 us faster than the cassandra driver                                 so now that we have seen the                                 implementation details                                 does the sila driver live up to our                                 expectations                                 and what are do the expectations what do                                 we expect when we                                 push this sila driver in production                                 well from the one connection per core                                 per node                                 there is two there are two expectations                                 from there                                 the first one is we expect to see an                                 increase from the point of view of the                                 cluster we expect to see an increase                                 of the number of open connections since                                 instead of having one connection per                                 node we will have one connection per car                                 per node                                 we we expect to see it on the monitoring                                 right                                 and we effectively did as you can see on                                 the first graph                                 the second one is since we have more                                 connections open to handle to keep alive                                 etc we expected also                                 to be more cpu hungry                                 so in our cases we saw that                                 indeed we had to adjust the resources                                 of cpu to avoid cpu saturation on our                                 kubernetes cluster                                 so it's a small increase but it's still                                 an expected increase so                                 this one checked as well but                                 what about the major impact we wanted                                 faster queries                                 lower latencies right because this                                 routing queries to the right core is                                 what we expect we want it to be faster                                 and this is what happened to our                                 production workload queries max latency                                 we got immediately                                                  performance boost we were like                                 oh my god this is amazing and we are                                 super happy                                 please note that the graph that you're                                 seeing is an actual                                 production graph and it's also the max                                 processing                                 time so we like it number early to look                                 at                                 our worst case scenario right so this                                 one is the max                                 it's not the the average or the the                                 minimum it's the                                 the worst of what we what we do on this                                 place on this specific workload                                 so on the worst case we want we win                                    to                                            which means that we are                                                  faster                                 what is interesting to note is that the                                 performance boost                                 is progressive since we can since we                                 connect to shards in the background in                                 an optimistic fashion                                 when you we just roll out the new driver                                 we sure don't have a connection                                 a connection to every core yet we will                                 happen to have it at some point                                 but since the first implementation is                                 still an optimistic and                                 non-deterministic way                                 it means that it will take some time so                                 the longer our application runs the more                                 chance to have a connection to all shard                                 it has                                 the lower the latency gets this is what                                 you can see                                 uh happening as well and we expect                                 this performance boost to be                                 instantaneous when the new connection                                 algorithm                                 will be available from another                                 perspective if we apply                                 a moving median on another deployment we                                 see the major impact of the driver                                 which is pretty as well impressive as                                 you can see                                 and pretty stable as well which has                                 which was a very good                                 a very good um thing to to get                                 we also got an unexpected side effect of                                 reduced latency                                 plus lower cluster food print we could                                 cut by half                                 the pod replica requirements from a                                 higher throughput application                                 so this one we didn't see com we didn't                                 see coming but since                                 our workload was faster and easier                                 on the cluster itself it were it allowed                                 us to                                 actually save a lot of um                                 of of pod replicas on the kubernetes                                 cluster so                                 if you relate this to the small increase                                 of cpu resources that                                 it has been asking for it's clearly a                                 major win                                 we have done some announcements as well                                 we have added some                                 helpers to know uh to let you                                 programmatically know if you are                                 connected or not to a                                 sila cluster we have also added some                                 shadow                                 statistics so that you can know uh if                                 you have                                 and you know fifteen fifty percent only                                 of uh                                 connections to all the nodes and cores                                 uh that                                 are available or not so that's uh can be                                 hand that can be handy                                 to have in your application logs we are                                 also                                 working and we have an open pr forum                                 adding                                 the new support of the new algorithm                                 that will be available in                                                                                                      latest cassandra driver                                 improvements as well so                                 check it out um contributions are very                                 welcome                                 the the driver is obviously available on                                 pipi                                 the documentation is there as well if                                 you see something missing that could be                                 enhanced                                 feel free to to tell us and                                 and thanks for attending and making                                 pycon india a success                                 you can catch me online about everywhere                                 with the handle                                 ultrabug i have listed a quick link of                                 my                                 conference talks if any of them is of                                 interest to you                                 and come have a chat with us on the sila                                 db user slack we have a pythonistas                                 channel where you can hang around and                                 ask questions or give ideas                                 etc thank you very much for attending                                 awesome alex's thanks a lot we have a                                 question from the audience i'm just                                 gonna wait for                                 that to pop up                                 okay so the question is are shards on                                 sila                                 ping to the cpus exactly exactly                                 shards are pin cpus and pin cores of                                 cpus                                 actually yes                                 thanks so alexis will be with us on                                 sulip                                 so head to zulip stream delhi stage                                 okay so our volunteers will share the                                 link with you on the hopping chat as                                 well                                 and thanks a lot lord alexis any closing                                 words                                 thank you very much to you thank you                                 bye bye
YouTube URL: https://www.youtube.com/watch?v=n22tmNWbE8A


