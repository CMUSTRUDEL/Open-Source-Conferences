Title: Predicting Black Swan Events - Michael McKerns
Publication date: 2013-09-16
Playlist: PyCon India 2013
Description: 
	
Captions: 
	00:00:00,170 --> 00:00:08,940
welcome all so we'll be starting with

00:00:03,720 --> 00:00:11,550
our next session and the topic of the

00:00:08,940 --> 00:00:14,969
session is predicting Black Swan event

00:00:11,550 --> 00:00:17,910
and Michael McCants would be presenting

00:00:14,969 --> 00:00:20,520
on on that michael has over 15 years of

00:00:17,910 --> 00:00:24,000
teaching experience in physics applied

00:00:20,520 --> 00:00:27,990
math and computing and has taught 20

00:00:24,000 --> 00:00:30,960
financial science workshops in past

00:00:27,990 --> 00:00:33,750
years alone he has been as research

00:00:30,960 --> 00:00:36,450
scientist at Cal Tech since 2002 where

00:00:33,750 --> 00:00:40,290
he has served as a project manager and

00:00:36,450 --> 00:00:43,350
also a lead developer for 215 million

00:00:40,290 --> 00:00:46,500
software projects on predictive science

00:00:43,350 --> 00:00:57,320
and large-scale computing projects yeah

00:00:46,500 --> 00:01:00,989
welcome thank you thank you I appreciate

00:00:57,320 --> 00:01:05,460
you all having me out here so I came a

00:01:00,989 --> 00:01:09,960
long way and I'm enjoying my con India

00:01:05,460 --> 00:01:13,380
for the first time so today what I

00:01:09,960 --> 00:01:17,880
wanted to talk about was predicting

00:01:13,380 --> 00:01:21,270
Black Swan events so I have the

00:01:17,880 --> 00:01:25,380
developer the main developer on some

00:01:21,270 --> 00:01:29,270
software software called mystic that is

00:01:25,380 --> 00:01:31,549
a framework for highly constrained

00:01:29,270 --> 00:01:35,610
nonlinear non convex optimization

00:01:31,549 --> 00:01:38,070
uncertainty calculations and risk and it

00:01:35,610 --> 00:01:40,700
utilizes some of my other software

00:01:38,070 --> 00:01:42,869
pathos it's a framework for

00:01:40,700 --> 00:01:45,659
heterogeneous parallel computing

00:01:42,869 --> 00:01:50,240
management and graph execution

00:01:45,659 --> 00:01:57,600
I have funding from all sort of agencies

00:01:50,240 --> 00:02:02,369
up here you can see NSF some Department

00:01:57,600 --> 00:02:08,629
of Defense energy etc so this this work

00:02:02,369 --> 00:02:10,800
is in conjunction with these guys

00:02:08,629 --> 00:02:13,830
mathematicians who invented

00:02:10,800 --> 00:02:17,520
hamona wotty who invented the new

00:02:13,830 --> 00:02:21,310
mathematical what uncertainty framework

00:02:17,520 --> 00:02:28,060
these guys were his students and these

00:02:21,310 --> 00:02:34,270
are my developers so what our Black Swan

00:02:28,060 --> 00:02:39,340
events black swans are high-impact rare

00:02:34,270 --> 00:02:40,960
events and black swans strongly impact

00:02:39,340 --> 00:02:47,470
risk uncertainty the probability of

00:02:40,960 --> 00:02:50,950
failure so what they do is they're their

00:02:47,470 --> 00:02:52,420
events that are very hard to supposedly

00:02:50,950 --> 00:02:55,060
very hard to predict are considered

00:02:52,420 --> 00:02:58,480
actually impossible to rigorously

00:02:55,060 --> 00:03:00,520
predict but if you do predict them it

00:02:58,480 --> 00:03:02,230
typically will yield you an enormous

00:03:00,520 --> 00:03:05,970
payoff and it doesn't necessarily have

00:03:02,230 --> 00:03:10,750
to be in finance Black Swan events are

00:03:05,970 --> 00:03:13,720
in really almost everything so black

00:03:10,750 --> 00:03:16,900
swans are again there are game changers

00:03:13,720 --> 00:03:19,720
there when you have when you have data

00:03:16,900 --> 00:03:23,650
and you look at observed data and you

00:03:19,720 --> 00:03:28,269
try to predict the future a Black Swan

00:03:23,650 --> 00:03:33,730
and is an event that changes let's say

00:03:28,269 --> 00:03:38,160
how the stock market goes changes combat

00:03:33,730 --> 00:03:42,519
scenarios or game strategic planning

00:03:38,160 --> 00:03:44,709
black swans can be seen in drilling and

00:03:42,519 --> 00:03:47,769
oil exploration so if you think about it

00:03:44,709 --> 00:03:50,140
what it is is if you were to just drill

00:03:47,769 --> 00:03:53,380
randomly on the planet you would never

00:03:50,140 --> 00:03:56,140
find oil but there's a vein that you

00:03:53,380 --> 00:03:57,820
might find that you can't detect because

00:03:56,140 --> 00:04:00,130
it's deep under the earth or deep under

00:03:57,820 --> 00:04:02,680
the water and you try to predict whether

00:04:00,130 --> 00:04:05,769
that vein is going to be to minimize

00:04:02,680 --> 00:04:10,060
your loss and when you strike it of your

00:04:05,769 --> 00:04:13,209
vein you get oil so that's the same type

00:04:10,060 --> 00:04:15,910
of thing as you see in the stock market

00:04:13,209 --> 00:04:17,410
for hedge and portfolio risk when you're

00:04:15,910 --> 00:04:19,750
gonna have the big payoff when is the

00:04:17,410 --> 00:04:22,180
crash going to happen and actually the

00:04:19,750 --> 00:04:26,730
same underlying mathematics in drug

00:04:22,180 --> 00:04:29,170
design mortality and insurance risk so

00:04:26,730 --> 00:04:32,110
they're everywhere and they're actually

00:04:29,170 --> 00:04:33,820
the place really to look for them is

00:04:32,110 --> 00:04:37,890
wherever corporations spend a lot of

00:04:33,820 --> 00:04:37,890
money trying to predict the future so

00:04:38,550 --> 00:04:45,100
why is a Black Swan so hard to predict

00:04:41,920 --> 00:04:47,010
it's because people hardly anyone

00:04:45,100 --> 00:04:49,960
attempts to solve real-world problems

00:04:47,010 --> 00:04:52,330
real-world problems are high dimensional

00:04:49,960 --> 00:04:55,480
non-linear non convex every new piece of

00:04:52,330 --> 00:04:58,290
information adds a new constraint often

00:04:55,480 --> 00:05:00,790
a nonlinear constraints are a big nasty

00:04:58,290 --> 00:05:03,220
optimization problem every piece of data

00:05:00,790 --> 00:05:05,350
every model every assumption that's

00:05:03,220 --> 00:05:08,230
constraining information so you're

00:05:05,350 --> 00:05:10,450
creating a big nasty nonlinear non

00:05:08,230 --> 00:05:13,060
convex optimization problem so what do

00:05:10,450 --> 00:05:15,060
people do typically what they do is they

00:05:13,060 --> 00:05:18,220
start to reduce the problems by using

00:05:15,060 --> 00:05:20,530
strong approximations or building

00:05:18,220 --> 00:05:23,220
smaller models models that approximate

00:05:20,530 --> 00:05:26,110
the behavior when you start to do that

00:05:23,220 --> 00:05:28,150
what happens the calculations are

00:05:26,110 --> 00:05:30,490
expensive anyway and the mathematics and

00:05:28,150 --> 00:05:32,320
the software to solve it rigorously did

00:05:30,490 --> 00:05:34,750
not exist until just recently

00:05:32,320 --> 00:05:37,510
so this is just what everyone has done

00:05:34,750 --> 00:05:40,900
so look at these the two things most

00:05:37,510 --> 00:05:43,330
common thing people do is you have an

00:05:40,900 --> 00:05:44,770
unknown probability distribution you

00:05:43,330 --> 00:05:49,630
have an unknown distribution you don't

00:05:44,770 --> 00:05:51,790
know what you're sampling out of so what

00:05:49,630 --> 00:05:53,290
people do is they assume what the

00:05:51,790 --> 00:05:55,210
probability distribution looks like

00:05:53,290 --> 00:05:58,900
usually from data if you've done

00:05:55,210 --> 00:06:00,820
Bayesian analysis its priors okay and

00:05:58,900 --> 00:06:02,740
when you assume what the probability

00:06:00,820 --> 00:06:04,840
distribution looks like then you can

00:06:02,740 --> 00:06:08,919
only predict the black swans that you've

00:06:04,840 --> 00:06:10,750
seen already that means you can only see

00:06:08,919 --> 00:06:12,060
what you've observed already as soon as

00:06:10,750 --> 00:06:15,250
you fix a probability distribution

00:06:12,060 --> 00:06:17,110
you'll not see any new ones and the

00:06:15,250 --> 00:06:19,150
second most thing that common thing that

00:06:17,110 --> 00:06:21,729
people do is they assume a normal

00:06:19,150 --> 00:06:23,050
distribution and start to extrapolate so

00:06:21,729 --> 00:06:25,630
when you see something like that when

00:06:23,050 --> 00:06:28,180
you see standard deviations being used

00:06:25,630 --> 00:06:30,520
standard deviations being used away from

00:06:28,180 --> 00:06:32,080
Monte Carlo simulations then what you're

00:06:30,520 --> 00:06:34,479
doing is you're seeing an averaging of

00:06:32,080 --> 00:06:35,979
behavior which completely wipes out the

00:06:34,479 --> 00:06:37,630
ability to predict black swans

00:06:35,979 --> 00:06:40,100
so the two most common things that

00:06:37,630 --> 00:06:42,290
people do are basically get rid of

00:06:40,100 --> 00:06:48,140
ability to break these things in the

00:06:42,290 --> 00:06:51,800
first place so here are some kind of

00:06:48,140 --> 00:06:54,680
simple things that people do you look at

00:06:51,800 --> 00:06:56,930
an optimization for an optimization like

00:06:54,680 --> 00:06:58,670
this with try to get to the bottom of

00:06:56,930 --> 00:07:00,650
the well with no constraints it's

00:06:58,670 --> 00:07:02,660
actually an easy thing to do and as soon

00:07:00,650 --> 00:07:04,220
as you start to add constraints well

00:07:02,660 --> 00:07:05,660
it's a little bit harder thing to do but

00:07:04,220 --> 00:07:07,640
what if you had a surrogate model that

00:07:05,660 --> 00:07:10,310
looked like this he said I only want it

00:07:07,640 --> 00:07:12,170
to be valid within that window that

00:07:10,310 --> 00:07:14,060
starts to be a hard optimization problem

00:07:12,170 --> 00:07:16,220
to do know if you had these data points

00:07:14,060 --> 00:07:19,670
and you said well I want it to be valid

00:07:16,220 --> 00:07:24,530
where my graph has to go through those

00:07:19,670 --> 00:07:26,930
data points then most most optimization

00:07:24,530 --> 00:07:29,030
frameworks can't handle it so we're

00:07:26,930 --> 00:07:31,900
already in something this simple and

00:07:29,030 --> 00:07:34,670
this low dimensional at a place where

00:07:31,900 --> 00:07:36,730
optimization frameworks start to fail

00:07:34,670 --> 00:07:39,050
and people have to use approximations

00:07:36,730 --> 00:07:41,540
these approximations like penalty

00:07:39,050 --> 00:07:44,510
functions they approximate with

00:07:41,540 --> 00:07:47,420
convexity use surrogates and wipe out

00:07:44,510 --> 00:07:50,600
the data throw away the data use this

00:07:47,420 --> 00:07:53,300
best fit surrogate or again worst worst

00:07:50,600 --> 00:07:57,380
case is extract a probability

00:07:53,300 --> 00:07:59,720
distribution and throw away the data

00:07:57,380 --> 00:08:02,510
again so again what if you have these

00:07:59,720 --> 00:08:03,770
black dots here and you say okay I'm

00:08:02,510 --> 00:08:07,790
going to assume a probability

00:08:03,770 --> 00:08:10,190
distribution looks like this and then

00:08:07,790 --> 00:08:12,710
everything else will only be sampled out

00:08:10,190 --> 00:08:14,780
of that probability distribution but

00:08:12,710 --> 00:08:18,200
then your next data point might be up

00:08:14,780 --> 00:08:20,660
here and your probability distribution

00:08:18,200 --> 00:08:23,470
that's best fit again now looks very

00:08:20,660 --> 00:08:28,550
different than the one you just had

00:08:23,470 --> 00:08:31,670
right so that's that would be what a

00:08:28,550 --> 00:08:33,440
Black Swan event would look like totally

00:08:31,670 --> 00:08:38,060
changing your probability distribution

00:08:33,440 --> 00:08:40,560
right there but really what you have to

00:08:38,060 --> 00:08:42,690
do is you have to do this differently

00:08:40,560 --> 00:08:46,160
you you can't make these approximations

00:08:42,690 --> 00:08:49,820
and be able to pick predict rare events

00:08:46,160 --> 00:08:53,370
so the subtle point is that

00:08:49,820 --> 00:08:54,450
approximations were done to make the

00:08:53,370 --> 00:08:56,610
problem solvable

00:08:54,450 --> 00:09:00,180
however they remove your ability to make

00:08:56,610 --> 00:09:03,690
these high-impact rare events

00:09:00,180 --> 00:09:06,210
predictions so again what people are

00:09:03,690 --> 00:09:09,510
doing is they're removing their ability

00:09:06,210 --> 00:09:12,210
to explore unknown probability

00:09:09,510 --> 00:09:15,210
distributions and reducing the problem

00:09:12,210 --> 00:09:18,330
to one of classic probability theory so

00:09:15,210 --> 00:09:23,130
classic probability theory is from 1812

00:09:18,330 --> 00:09:25,620
Laplace updated in 1933 and basically

00:09:23,130 --> 00:09:27,540
the idea is probability distributions

00:09:25,620 --> 00:09:30,150
are approximated as things that are

00:09:27,540 --> 00:09:33,440
known and then standard deviations are

00:09:30,150 --> 00:09:34,740
used to reintroduce the unknown but that

00:09:33,440 --> 00:09:36,779
absolutely

00:09:34,740 --> 00:09:41,490
like I said kills your ability to do and

00:09:36,779 --> 00:09:44,370
predict black swans how does this differ

00:09:41,490 --> 00:09:48,480
from rigorous calculations of risk and

00:09:44,370 --> 00:09:50,940
uncertainty in rigorous calculations of

00:09:48,480 --> 00:09:54,000
risk and uncertainty we say that

00:09:50,940 --> 00:09:56,790
probability distributions are unknown

00:09:54,000 --> 00:09:59,250
and we have to solve for them in the in

00:09:56,790 --> 00:10:04,730
the course of an optimization and the

00:09:59,250 --> 00:10:06,620
first unified uncertainty theory was

00:10:04,730 --> 00:10:09,530
published by awadhi

00:10:06,620 --> 00:10:14,790
myself and a few of the other offers

00:10:09,530 --> 00:10:18,210
earlier this year and it basically gives

00:10:14,790 --> 00:10:20,460
us the ability to address these

00:10:18,210 --> 00:10:22,980
questions directly as large-scale

00:10:20,460 --> 00:10:26,220
optimization problems so what everybody

00:10:22,980 --> 00:10:28,800
does in uncertainty is if they have a

00:10:26,220 --> 00:10:31,230
problem that says I have a bunch of I

00:10:28,800 --> 00:10:33,890
have a hundred colored balls in a in a

00:10:31,230 --> 00:10:36,720
in a bag and I'm picking out red balls

00:10:33,890 --> 00:10:40,260
if you if you actually have data that

00:10:36,720 --> 00:10:41,910
says on average ten balls are red what's

00:10:40,260 --> 00:10:45,440
the likelihood of picking a red ball the

00:10:41,910 --> 00:10:47,459
next time you can address that question

00:10:45,440 --> 00:10:48,959
directly right you don't know how many

00:10:47,459 --> 00:10:51,720
balls are in the bag but you know on

00:10:48,959 --> 00:10:53,610
average they're ten so what people

00:10:51,720 --> 00:10:54,300
usually do what people have done up to

00:10:53,610 --> 00:10:58,770
this point

00:10:54,300 --> 00:11:01,830
is they say okay ten balls are red now I

00:10:58,770 --> 00:11:05,670
can calculate very easily the

00:11:01,830 --> 00:11:08,520
probability but that removes rare events

00:11:05,670 --> 00:11:14,670
right the rare event might be when

00:11:08,520 --> 00:11:17,520
there's no red balls or so so with this

00:11:14,670 --> 00:11:19,590
new type of with this new theory we

00:11:17,520 --> 00:11:22,320
actually can calculate the likelihood of

00:11:19,590 --> 00:11:24,480
picking a red ball explicitly based on

00:11:22,320 --> 00:11:26,310
the information we know what's the best

00:11:24,480 --> 00:11:28,080
case in the worst case and these three

00:11:26,310 --> 00:11:30,420
questions right here will give you the

00:11:28,080 --> 00:11:32,940
hedge that is the definition of what

00:11:30,420 --> 00:11:35,010
people use at the hedge what are the

00:11:32,940 --> 00:11:37,140
bounds what's the average and the

00:11:35,010 --> 00:11:38,640
distance between those are the hedge the

00:11:37,140 --> 00:11:45,380
distance between the bounds or the value

00:11:38,640 --> 00:11:50,880
at risk so this framework can be used on

00:11:45,380 --> 00:11:52,760
things like the stock market okay so I

00:11:50,880 --> 00:11:55,830
won't go through too much of the math

00:11:52,760 --> 00:11:59,900
that's not for this conference right but

00:11:55,830 --> 00:12:03,120
the idea is I just want to show you

00:11:59,900 --> 00:12:05,820
expectation value of some quantity what

00:12:03,120 --> 00:12:09,180
we're actually doing is we're saying the

00:12:05,820 --> 00:12:11,940
expectation value is on average ten

00:12:09,180 --> 00:12:13,620
balls that get picked out of the bag are

00:12:11,940 --> 00:12:16,440
going to be red so we need to write

00:12:13,620 --> 00:12:18,660
things in terms of the expected quantity

00:12:16,440 --> 00:12:22,230
of interest and the thing we're

00:12:18,660 --> 00:12:26,540
optimizing over is the probability of

00:12:22,230 --> 00:12:29,550
the system which we don't know and also

00:12:26,540 --> 00:12:32,690
the governing function of the system

00:12:29,550 --> 00:12:34,110
which we don't know but that's okay

00:12:32,690 --> 00:12:35,730
that's okay

00:12:34,110 --> 00:12:37,560
what we do is we collect all of the

00:12:35,730 --> 00:12:41,100
information about the problem in two

00:12:37,560 --> 00:12:42,600
constraints this a that says anything we

00:12:41,100 --> 00:12:45,330
know about the system we can write it as

00:12:42,600 --> 00:12:47,340
a constraint and by imposing the

00:12:45,330 --> 00:12:49,800
constraints on the problem the right way

00:12:47,340 --> 00:12:51,990
on the optimization problem we encode

00:12:49,800 --> 00:12:55,610
all the information that we know about

00:12:51,990 --> 00:12:59,610
reality without any approximations and

00:12:55,610 --> 00:13:03,290
we can limit we can get the limiting

00:12:59,610 --> 00:13:05,640
behavior maximum minimum and most likely

00:13:03,290 --> 00:13:07,800
for that problem if you know how to

00:13:05,640 --> 00:13:08,880
write the optimization problem

00:13:07,800 --> 00:13:13,260
and you know how to impose the

00:13:08,880 --> 00:13:17,360
constraints without approximation okay

00:13:13,260 --> 00:13:20,940
so that's what that mathematics says and

00:13:17,360 --> 00:13:24,870
here's actually how the here's actually

00:13:20,940 --> 00:13:28,550
how it's written the lowest bound based

00:13:24,870 --> 00:13:34,519
on that information is equal to the

00:13:28,550 --> 00:13:37,800
expectation value over your unknowns and

00:13:34,519 --> 00:13:39,630
it's less than the upper bound so you

00:13:37,800 --> 00:13:44,100
know that the expectation value for your

00:13:39,630 --> 00:13:45,510
system has to be in between your lower

00:13:44,100 --> 00:13:47,490
and your upper bound based on those

00:13:45,510 --> 00:13:49,709
constraints and you actually can write

00:13:47,490 --> 00:13:51,839
the formula for your upper and lower

00:13:49,709 --> 00:13:55,560
bound based on those constraints like

00:13:51,839 --> 00:13:59,550
int which means a minimum or soup which

00:13:55,560 --> 00:14:02,190
means the maximum of this probability

00:13:59,550 --> 00:14:04,410
distribution which means what what are

00:14:02,190 --> 00:14:06,660
we doing we're optimizing over

00:14:04,410 --> 00:14:10,589
probability distributions something

00:14:06,660 --> 00:14:13,200
that's not been done before and again I

00:14:10,589 --> 00:14:16,459
won't hit too much math on here but the

00:14:13,200 --> 00:14:19,649
point is that statistical estimators

00:14:16,459 --> 00:14:20,190
statistical estimators priors optimal

00:14:19,649 --> 00:14:22,980
bounds

00:14:20,190 --> 00:14:26,910
they're just optimization problems and

00:14:22,980 --> 00:14:28,680
they're big nasty ugly mathematical

00:14:26,910 --> 00:14:32,579
optimization problems

00:14:28,680 --> 00:14:35,760
this one says right here the optimal

00:14:32,579 --> 00:14:38,190
prior which is the most likely

00:14:35,760 --> 00:14:42,630
probability distribution that gets you

00:14:38,190 --> 00:14:44,850
the best sampling to collect your Monte

00:14:42,630 --> 00:14:48,930
Carlo samples or whatever out of is the

00:14:44,850 --> 00:14:51,779
the minimum of the maximum of the

00:14:48,930 --> 00:14:55,320
expectation value of the expectation

00:14:51,779 --> 00:14:58,740
value of a function that represents your

00:14:55,320 --> 00:15:03,209
data - your model of your probability

00:14:58,740 --> 00:15:05,100
distribution right that's ugly and also

00:15:03,209 --> 00:15:06,829
you should think it's probably pretty

00:15:05,100 --> 00:15:11,519
expensive too

00:15:06,829 --> 00:15:15,240
and the reason that people don't do this

00:15:11,519 --> 00:15:17,880
is because optimization one the

00:15:15,240 --> 00:15:21,690
mathematics didn't exist for it a few

00:15:17,880 --> 00:15:24,900
years ago and number two is that

00:15:21,690 --> 00:15:27,680
this is multiple nested optimization

00:15:24,900 --> 00:15:30,000
problems over a probability distribution

00:15:27,680 --> 00:15:31,560
where we're definitely talking about non

00:15:30,000 --> 00:15:34,410
linear and non convex and most

00:15:31,560 --> 00:15:38,610
optimizers the best optimizers in the

00:15:34,410 --> 00:15:42,000
world don't go past ten to fifteen

00:15:38,610 --> 00:15:45,840
constraints in the nonlinear sense right

00:15:42,000 --> 00:15:49,530
so it just can't be done well it just

00:15:45,840 --> 00:15:53,190
couldn't be done so one of the things

00:15:49,530 --> 00:15:57,270
we've done is a project with we've

00:15:53,190 --> 00:16:00,600
partnered with like I said the military

00:15:57,270 --> 00:16:10,230
Department of Defense and some hedge

00:16:00,600 --> 00:16:12,260
funds Google IBM is this one is shocked

00:16:10,230 --> 00:16:17,010
in microstructure materials and

00:16:12,260 --> 00:16:20,910
basically for amateur this is nuclear

00:16:17,010 --> 00:16:24,290
weapons testing okay so what happens to

00:16:20,910 --> 00:16:28,860
materials when you have a shockwave and

00:16:24,290 --> 00:16:34,260
how does the has a material react so

00:16:28,860 --> 00:16:37,020
what they do is they have two two

00:16:34,260 --> 00:16:40,290
cylinders smashing into each other where

00:16:37,020 --> 00:16:42,510
there's some microstructured material in

00:16:40,290 --> 00:16:44,250
each of the cylinders and you try to

00:16:42,510 --> 00:16:47,130
predict what happens with the shock

00:16:44,250 --> 00:16:50,100
fronts so the real system is basically a

00:16:47,130 --> 00:16:52,020
description of cylinders the

00:16:50,100 --> 00:16:54,510
microstructure C of X the chemical

00:16:52,020 --> 00:16:58,020
composition and what you're going to

00:16:54,510 --> 00:16:59,280
look at is all of these things and in

00:16:58,020 --> 00:17:02,070
the theoretical system you have the

00:16:59,280 --> 00:17:04,589
equations of state the mesh everything

00:17:02,070 --> 00:17:07,470
that's a physical constant is a lambda a

00:17:04,589 --> 00:17:10,199
lot of parameters to deal with very high

00:17:07,470 --> 00:17:13,319
dimensional problem and again the point

00:17:10,199 --> 00:17:15,240
here is if you have a simulation like

00:17:13,319 --> 00:17:17,250
this it's going to be a big nasty

00:17:15,240 --> 00:17:21,000
optimization problem but we know how to

00:17:17,250 --> 00:17:23,160
write it now all right how do we write

00:17:21,000 --> 00:17:26,370
the model error the model error is the

00:17:23,160 --> 00:17:30,950
actual model error measured minus the

00:17:26,370 --> 00:17:30,950
theoretical model they are with the

00:17:31,670 --> 00:17:36,460
parameters of the cylinders

00:17:34,840 --> 00:17:38,590
and the physical parameters of the

00:17:36,460 --> 00:17:40,720
material I can write the statistical

00:17:38,590 --> 00:17:43,000
error I can write the model uncertainty

00:17:40,720 --> 00:17:47,169
I can write the probability of failure

00:17:43,000 --> 00:17:49,570
this is the probability that the model

00:17:47,169 --> 00:17:52,299
the difference between the model and the

00:17:49,570 --> 00:17:56,320
actual value is less is greater than a

00:17:52,299 --> 00:18:00,400
is less than some epsilon which means is

00:17:56,320 --> 00:18:02,500
it likely or not that I fail so all of

00:18:00,400 --> 00:18:04,809
this stuff here again the optimal bound

00:18:02,500 --> 00:18:07,360
on the statistical estimator it looks

00:18:04,809 --> 00:18:09,850
just like the other optimization problem

00:18:07,360 --> 00:18:13,299
we wrote but the point is we can write

00:18:09,850 --> 00:18:15,850
this right for the first time we can

00:18:13,299 --> 00:18:22,210
write optimization problems that can

00:18:15,850 --> 00:18:25,600
solve these type of problems and what

00:18:22,210 --> 00:18:28,480
that means is rigorous statistics

00:18:25,600 --> 00:18:31,140
statistical estimation becomes an

00:18:28,480 --> 00:18:33,909
extremely high dimensional on convex

00:18:31,140 --> 00:18:36,730
non-linear global optimization problem

00:18:33,909 --> 00:18:39,279
where we have to solve it exactly to

00:18:36,730 --> 00:18:40,870
guarantee our predictions right as soon

00:18:39,279 --> 00:18:42,580
as we start throwing approximations in

00:18:40,870 --> 00:18:45,600
there assume the probability

00:18:42,580 --> 00:18:47,169
distribution we're lost no black swans

00:18:45,600 --> 00:18:51,690
can we do it

00:18:47,169 --> 00:18:55,510
hell yes we can do it so with this new

00:18:51,690 --> 00:18:59,080
mathematical theory and mystic

00:18:55,510 --> 00:19:00,789
optimization package and parallel and

00:18:59,080 --> 00:19:04,510
distributed computing package of of

00:19:00,789 --> 00:19:09,360
pathos we now have the ability to start

00:19:04,510 --> 00:19:11,799
solving these problems so we need

00:19:09,360 --> 00:19:13,750
large-scale parallel and distributed

00:19:11,799 --> 00:19:15,580
computing for a lot of the big problems

00:19:13,750 --> 00:19:18,490
you can run small computing problems on

00:19:15,580 --> 00:19:20,860
your laptop I've run them on that laptop

00:19:18,490 --> 00:19:23,890
right there but as soon as you get into

00:19:20,860 --> 00:19:26,140
real problems like like these cylinder

00:19:23,890 --> 00:19:28,149
smashing into each other then you need

00:19:26,140 --> 00:19:30,220
big large-scale computing you need

00:19:28,149 --> 00:19:33,149
asynchronous computing monitoring

00:19:30,220 --> 00:19:36,159
caching archiving job restarts all the

00:19:33,149 --> 00:19:39,010
large-scale computing stuff batch

00:19:36,159 --> 00:19:40,720
execution of the model but if you look

00:19:39,010 --> 00:19:43,059
at optimizers you look at optimization

00:19:40,720 --> 00:19:45,039
technology what do you see optimizers

00:19:43,059 --> 00:19:48,610
are notoriously serial they're

00:19:45,039 --> 00:19:49,180
asynchronous diagnostic limited fixed

00:19:48,610 --> 00:19:51,880
execution

00:19:49,180 --> 00:19:53,830
strategy they're actually the opposite

00:19:51,880 --> 00:19:56,350
of what's required to do these

00:19:53,830 --> 00:19:59,350
large-scale calculations so what I had

00:19:56,350 --> 00:20:04,000
to do was I had to go in and rewrite the

00:19:59,350 --> 00:20:08,410
optimizers from scratch and customize it

00:20:04,000 --> 00:20:12,090
to run on these large-scale graph

00:20:08,410 --> 00:20:14,620
execution frameworks but the idea of

00:20:12,090 --> 00:20:15,880
abstractions on the programming model so

00:20:14,620 --> 00:20:19,360
if we want to be able to have them run

00:20:15,880 --> 00:20:21,670
on multi processing threading MPI SSH

00:20:19,360 --> 00:20:23,800
GPU or the cloud with a bunch of

00:20:21,670 --> 00:20:25,840
different schedulers what we want to do

00:20:23,800 --> 00:20:27,640
is we want to write things like this if

00:20:25,840 --> 00:20:30,010
anyone's familiar with multi processing

00:20:27,640 --> 00:20:32,680
in Python what you do is you do a

00:20:30,010 --> 00:20:35,410
one-line configuration of a worker pool

00:20:32,680 --> 00:20:39,640
and then you pick the map you want to

00:20:35,410 --> 00:20:42,700
use and it's a MapReduce right so from

00:20:39,640 --> 00:20:49,690
PI you know launchers import slurm pool

00:20:42,700 --> 00:20:52,990
which is a queuing system give me 16

00:20:49,690 --> 00:20:55,420
nodes 8 processors per node now I have

00:20:52,990 --> 00:20:58,120
my worker pool and then I say I want a

00:20:55,420 --> 00:21:00,340
blocking map now I have an MPI based

00:20:58,120 --> 00:21:03,250
blocking map that works on a cluster

00:21:00,340 --> 00:21:07,660
system and then I can evaluate just like

00:21:03,250 --> 00:21:11,380
a Python map my model for the 16 numbers

00:21:07,660 --> 00:21:15,310
in MPI parallel on the machine so that's

00:21:11,380 --> 00:21:17,650
what my problem I pathless package

00:21:15,310 --> 00:21:19,330
basically brings pion is a sub package

00:21:17,650 --> 00:21:22,870
of it and if you want to do the same

00:21:19,330 --> 00:21:25,720
thing with parallel Python distributed

00:21:22,870 --> 00:21:29,020
over a clusters you can do that same

00:21:25,720 --> 00:21:32,890
thing here's IPC pool over some servers

00:21:29,020 --> 00:21:35,200
and get a asynchronous map this time and

00:21:32,890 --> 00:21:36,610
again it's a map function this time as

00:21:35,200 --> 00:21:39,070
you can see it's asynchronous because

00:21:36,610 --> 00:21:43,450
you say get afterwards this basically

00:21:39,070 --> 00:21:45,160
gives you 80 to 90% performance of doing

00:21:43,450 --> 00:21:47,650
the thing by hand but it just took me

00:21:45,160 --> 00:21:51,040
two lines and now i'm launching jobs on

00:21:47,650 --> 00:21:53,020
clusters and so what you get is you get

00:21:51,040 --> 00:21:55,450
the ability to build a a parallel graph

00:21:53,020 --> 00:21:58,210
like this make an extraction on the

00:21:55,450 --> 00:22:00,850
graph and run between a number of

00:21:58,210 --> 00:22:02,050
different resources so that's what we

00:22:00,850 --> 00:22:02,890
want to do we want to run these big

00:22:02,050 --> 00:22:06,580
problems

00:22:02,890 --> 00:22:09,070
in parallel any which way we like fast

00:22:06,580 --> 00:22:10,840
parallel or large-scale parallel we

00:22:09,070 --> 00:22:12,570
wanted to work on a laptop we wanted to

00:22:10,840 --> 00:22:15,940
work on clusters we want to be able to

00:22:12,570 --> 00:22:18,010
sub use sub resources and nest the

00:22:15,940 --> 00:22:20,830
parallel computing and all it is these

00:22:18,010 --> 00:22:23,860
map functions if we use a map function

00:22:20,830 --> 00:22:25,780
and we have the ability to serialize all

00:22:23,860 --> 00:22:30,880
of Python and I can do that with my

00:22:25,780 --> 00:22:33,880
package till then what we can do is nest

00:22:30,880 --> 00:22:35,260
all sort of parallel graphs and build

00:22:33,880 --> 00:22:37,330
the structure we need to do to solve

00:22:35,260 --> 00:22:39,070
these big problems now we need to be

00:22:37,330 --> 00:22:41,950
able to hook that into an optimization

00:22:39,070 --> 00:22:43,840
problem so instead of a one-liner I do

00:22:41,950 --> 00:22:48,179
have these written as one-liners but

00:22:43,840 --> 00:22:51,100
what I can do is I give a very flexible

00:22:48,179 --> 00:22:55,000
highly configurable interface where the

00:22:51,100 --> 00:22:56,860
key thing is my optimizer has state okay

00:22:55,000 --> 00:22:59,620
and I can take my optimizer and I showed

00:22:56,860 --> 00:23:03,820
the one line interface board but this

00:22:59,620 --> 00:23:06,429
also I can say where is this guy

00:23:03,820 --> 00:23:09,100
differential evolution solver and here's

00:23:06,429 --> 00:23:13,030
my solver I can set the save frequency

00:23:09,100 --> 00:23:17,200
so it'll actually pickle and save the

00:23:13,030 --> 00:23:19,570
entire solver every hundred cycles say I

00:23:17,200 --> 00:23:22,179
set my initialization points I set up

00:23:19,570 --> 00:23:24,160
monitors now these monitors they work

00:23:22,179 --> 00:23:26,080
parallel distributed so if you hook up a

00:23:24,160 --> 00:23:30,790
monitor it's setting up the stream and

00:23:26,080 --> 00:23:33,850
it feeds back the the results of the

00:23:30,790 --> 00:23:36,130
best fit for each generation or every

00:23:33,850 --> 00:23:38,260
single evaluation and those guys can be

00:23:36,130 --> 00:23:41,140
configured to I can also configure the

00:23:38,260 --> 00:23:44,530
termination conditions so that's one of

00:23:41,140 --> 00:23:46,990
the one of the best ways to actually

00:23:44,530 --> 00:23:49,510
flexibly solve anything is to set the

00:23:46,990 --> 00:23:51,280
termination conditions it's a it's a

00:23:49,510 --> 00:23:55,320
unique ability in my optimization

00:23:51,280 --> 00:23:58,240
package to be able to do that and then

00:23:55,320 --> 00:24:00,190
we solve we solve for a particular model

00:23:58,240 --> 00:24:03,010
and a particular termination condition

00:24:00,190 --> 00:24:04,780
we get our best solution now what I do

00:24:03,010 --> 00:24:06,820
is I have a bunch of Diagnostics that

00:24:04,780 --> 00:24:09,220
you can go into but then what you can do

00:24:06,820 --> 00:24:11,470
is because it as state write it as state

00:24:09,220 --> 00:24:14,169
like I can solve it here save it here to

00:24:11,470 --> 00:24:16,660
a pickle file it also has state because

00:24:14,169 --> 00:24:19,300
it's a class so my solvers are

00:24:16,660 --> 00:24:21,310
and they have internal state and that

00:24:19,300 --> 00:24:23,950
means I can take one that's already run

00:24:21,310 --> 00:24:25,510
I can change something like the

00:24:23,950 --> 00:24:27,430
termination condition here what I'm

00:24:25,510 --> 00:24:29,110
doing is I'm coupling together two

00:24:27,430 --> 00:24:32,770
termination conditions and changing the

00:24:29,110 --> 00:24:34,810
tolerance I can restart two solver okay

00:24:32,770 --> 00:24:37,200
so I have Diagnostics coming off the

00:24:34,810 --> 00:24:40,090
solver and running parallel distributed

00:24:37,200 --> 00:24:43,090
the Diagnostics can stream out I save

00:24:40,090 --> 00:24:44,740
the state I can restart really very

00:24:43,090 --> 00:24:47,050
flexible even if you're just doing some

00:24:44,740 --> 00:24:50,830
very basic optimization this becomes a

00:24:47,050 --> 00:24:52,990
very powerful tool and to extend the

00:24:50,830 --> 00:24:57,070
optimizers to parallel all I need to do

00:24:52,990 --> 00:25:01,240
is build my map function and then say

00:24:57,070 --> 00:25:04,480
right here set evaluation map right set

00:25:01,240 --> 00:25:06,640
evaluation map with my map function save

00:25:04,480 --> 00:25:09,610
that to the state of the solver and now

00:25:06,640 --> 00:25:13,240
I start solving where the evaluations

00:25:09,610 --> 00:25:16,870
are done in parallel so I can do that

00:25:13,240 --> 00:25:19,480
with a evolutionary algorithm and send

00:25:16,870 --> 00:25:21,670
all the evaluations off in parallel or I

00:25:19,480 --> 00:25:23,410
can do it at a bigger range where I can

00:25:21,670 --> 00:25:24,970
actually run you know if I have a

00:25:23,410 --> 00:25:28,360
cluster of 10,000 nodes

00:25:24,970 --> 00:25:31,000
I can run 10,000 optimizations in

00:25:28,360 --> 00:25:33,820
parallel each running an entire

00:25:31,000 --> 00:25:37,390
optimization problem itself with the

00:25:33,820 --> 00:25:39,430
goal of all trying to find not the best

00:25:37,390 --> 00:25:40,890
solution I can set it so it's the best

00:25:39,430 --> 00:25:43,360
solution because I can terminate the

00:25:40,890 --> 00:25:45,700
because I can customize the termination

00:25:43,360 --> 00:25:47,380
conditions I can pick however I wanted

00:25:45,700 --> 00:25:50,890
to try solve I can actually try to map

00:25:47,380 --> 00:25:52,930
out the entire potential surface with

00:25:50,890 --> 00:25:54,790
10,000 optimizers and have them report

00:25:52,930 --> 00:25:56,740
to me each local minimum that's

00:25:54,790 --> 00:26:00,910
something very important in physical

00:25:56,740 --> 00:26:03,880
chemistry to be able to solve the entire

00:26:00,910 --> 00:26:06,730
potential energy surface so you can do

00:26:03,880 --> 00:26:09,540
that very simply by saying set the

00:26:06,730 --> 00:26:12,310
solver map set the nested solver and

00:26:09,540 --> 00:26:15,040
here we're basically using again

00:26:12,310 --> 00:26:18,610
parallel computing and we're nesting two

00:26:15,040 --> 00:26:21,190
solvers inside of each other right so

00:26:18,610 --> 00:26:23,980
this is this is doing large-scale

00:26:21,190 --> 00:26:24,940
parallel computing on a very like a pile

00:26:23,980 --> 00:26:28,210
directional Falvo

00:26:24,940 --> 00:26:30,990
is a gradient solver so it's fast global

00:26:28,210 --> 00:26:30,990
optimization

00:26:32,139 --> 00:26:40,389
okay so we see problems like this this

00:26:35,739 --> 00:26:44,629
again this cylinder impact with lots of

00:26:40,389 --> 00:26:47,679
lots of physics in there and running

00:26:44,629 --> 00:26:51,139
these things on a large scale cluster

00:26:47,679 --> 00:26:53,509
can take some time they're running in

00:26:51,139 --> 00:26:55,239
parallel on glory which is one of the

00:26:53,509 --> 00:26:57,499
largest computers in the world and it's

00:26:55,239 --> 00:27:00,230
taking a whole bunch of cores to be able

00:26:57,499 --> 00:27:02,899
to calculate it it's already MPI scaled

00:27:00,230 --> 00:27:05,679
and now this is my optimization

00:27:02,899 --> 00:27:09,009
framework my optimization framework

00:27:05,679 --> 00:27:11,840
running it over a period of eight days

00:27:09,009 --> 00:27:15,950
one two three machines three of the

00:27:11,840 --> 00:27:18,710
biggest machines in the world and also

00:27:15,950 --> 00:27:22,369
the one in Cal Tech's basement which is

00:27:18,710 --> 00:27:27,499
smaller so four machines over eight days

00:27:22,369 --> 00:27:31,999
at was it twenty it's actually two one

00:27:27,499 --> 00:27:34,309
one seven petaflop calculation that's an

00:27:31,999 --> 00:27:36,259
optimization problem doing sensitivity

00:27:34,309 --> 00:27:39,200
calculation of what are the most

00:27:36,259 --> 00:27:41,840
important parameters in two cylinders

00:27:39,200 --> 00:27:46,100
smashing into each other and it's all

00:27:41,840 --> 00:27:48,980
run for my laptop so we have the ability

00:27:46,100 --> 00:27:52,879
to do that and actually the great thing

00:27:48,980 --> 00:27:55,399
about the code is I can run this

00:27:52,879 --> 00:27:59,779
petaflop calculation with the same code

00:27:55,399 --> 00:28:02,269
that I can run on my laptop and I do

00:27:59,779 --> 00:28:06,799
absolutely nothing but change the lines

00:28:02,269 --> 00:28:09,259
that say run on the big resources right

00:28:06,799 --> 00:28:10,759
I don't have to rewrite any code I don't

00:28:09,259 --> 00:28:13,190
have to rewrite any I don't have to

00:28:10,759 --> 00:28:15,409
write any MPI code I don't have to write

00:28:13,190 --> 00:28:19,309
any distributed code all I do is make

00:28:15,409 --> 00:28:23,899
the replacement back here of how the

00:28:19,309 --> 00:28:26,450
thing is going to run so hooked up to an

00:28:23,899 --> 00:28:30,639
optimizer gives us this great ability to

00:28:26,450 --> 00:28:30,639
do predictive science at a large scale

00:28:30,730 --> 00:28:38,779
okay so parallelism though is not enough

00:28:34,480 --> 00:28:42,200
like I said most optimizers most

00:28:38,779 --> 00:28:44,870
optimizers cannot handle larger than 10

00:28:42,200 --> 00:28:48,070
non-linear constraints

00:28:44,870 --> 00:28:51,920
the way I write the way I write

00:28:48,070 --> 00:28:54,590
constraints is completely well for

00:28:51,920 --> 00:28:58,940
optimization it's completely new but if

00:28:54,590 --> 00:29:02,990
any of you have done physics quantum

00:28:58,940 --> 00:29:06,230
mechanics set theory what you'll see is

00:29:02,990 --> 00:29:08,750
there's the concept of an operator and

00:29:06,230 --> 00:29:12,200
an operator says if you have a set X and

00:29:08,750 --> 00:29:16,640
you apply an observable to it you now

00:29:12,200 --> 00:29:19,220
get a new set okay and what that does it

00:29:16,640 --> 00:29:22,160
takes information and it constrains a

00:29:19,220 --> 00:29:24,320
set it constrains a set by applying the

00:29:22,160 --> 00:29:26,540
operator and reducing the set if you

00:29:24,320 --> 00:29:28,820
have a set of a hundred things and the

00:29:26,540 --> 00:29:31,430
constraint operator constrains it to be

00:29:28,820 --> 00:29:34,610
only the ten things then what you get is

00:29:31,430 --> 00:29:37,000
the set of ten things at the end all

00:29:34,610 --> 00:29:39,260
right so I use that principle of

00:29:37,000 --> 00:29:42,260
applying constraints applying

00:29:39,260 --> 00:29:44,150
information through operators to reduce

00:29:42,260 --> 00:29:48,200
the set and what that does is that

00:29:44,150 --> 00:29:50,780
allows me to explicitly without

00:29:48,200 --> 00:29:55,070
approximation as long as I can write the

00:29:50,780 --> 00:29:57,080
operator I can apply any information to

00:29:55,070 --> 00:29:59,150
the optimization problem and the

00:29:57,080 --> 00:30:02,320
beautiful thing about it is unlike a

00:29:59,150 --> 00:30:05,360
penalty which is a addition it's a

00:30:02,320 --> 00:30:08,030
multiplication or a filtering so it's

00:30:05,360 --> 00:30:11,000
embarrassingly parallel which means now

00:30:08,030 --> 00:30:13,070
all the information I apply can get farm

00:30:11,000 --> 00:30:15,530
down in parallel and solved

00:30:13,070 --> 00:30:18,080
independently and if it's coupled

00:30:15,530 --> 00:30:20,030
together so what it's actually just a

00:30:18,080 --> 00:30:22,730
big non-linear optimization problem to

00:30:20,030 --> 00:30:25,250
solve a constraint and that just means

00:30:22,730 --> 00:30:27,740
another nesting of a parallel

00:30:25,250 --> 00:30:28,900
optimization problem and so on and so on

00:30:27,740 --> 00:30:32,030
and so on

00:30:28,900 --> 00:30:33,380
it's big and ugly but we already know

00:30:32,030 --> 00:30:35,770
how to do it we have the infrastructure

00:30:33,380 --> 00:30:38,120
to do it we can solve these problems

00:30:35,770 --> 00:30:41,530
explicitly so I give the ability in

00:30:38,120 --> 00:30:45,830
mystic to go to do symbolic and

00:30:41,530 --> 00:30:47,480
statistical constraints to build a exact

00:30:45,830 --> 00:30:49,190
constraints do two data points and

00:30:47,480 --> 00:30:52,940
surrogate models and it scales up

00:30:49,190 --> 00:30:55,160
without bound I have not had I've not

00:30:52,940 --> 00:30:57,640
run into a problem that's too big for it

00:30:55,160 --> 00:31:01,360
to solve and I give all sort of neat

00:30:57,640 --> 00:31:03,670
like memory caching etc for large-scale

00:31:01,360 --> 00:31:07,000
computing that's part of either pathos

00:31:03,670 --> 00:31:10,179
or mystic and they work together so how

00:31:07,000 --> 00:31:12,429
do I write penalty functions penalty

00:31:10,179 --> 00:31:14,410
functions can be applied in general to

00:31:12,429 --> 00:31:17,530
any optimizer and this is the thing that

00:31:14,410 --> 00:31:19,720
I said you typically don't do but you

00:31:17,530 --> 00:31:22,210
can do it if you like with it's a

00:31:19,720 --> 00:31:24,910
decorator with penalty pick the type of

00:31:22,210 --> 00:31:26,170
penalty and then just apply it to a

00:31:24,910 --> 00:31:29,530
penalty function then the penalty

00:31:26,170 --> 00:31:33,040
function that says I'm gonna make sure X

00:31:29,530 --> 00:31:35,410
has a mean of 5 and that's the way that

00:31:33,040 --> 00:31:39,360
people apply penalty functions but I can

00:31:35,410 --> 00:31:42,190
apply it to any of my solvers here is

00:31:39,360 --> 00:31:46,419
doing the same thing applying the mean

00:31:42,190 --> 00:31:48,400
of 5 to a constraint and now this guy

00:31:46,419 --> 00:31:49,840
what he does is instead of applying a

00:31:48,400 --> 00:31:52,120
penalty function and a penalty function

00:31:49,840 --> 00:31:55,150
what a penalty function does is it says

00:31:52,120 --> 00:31:57,970
I apply this penalty to the things that

00:31:55,150 --> 00:32:00,030
don't satisfy the constraint which means

00:31:57,970 --> 00:32:04,030
you're still exploring all of space

00:32:00,030 --> 00:32:06,100
right so if you apply a filter like this

00:32:04,030 --> 00:32:08,590
guy and it says only let the optimizer

00:32:06,100 --> 00:32:12,429
pick places where the mean is equal to 5

00:32:08,590 --> 00:32:15,340
and what that does is it says this space

00:32:12,429 --> 00:32:17,500
here or this space here don't even let

00:32:15,340 --> 00:32:19,210
the optimizer go there and then when you

00:32:17,500 --> 00:32:22,390
have some big nasty problem like that

00:32:19,210 --> 00:32:24,940
and you apply data point and all these

00:32:22,390 --> 00:32:28,290
other constraints so the optimizer will

00:32:24,940 --> 00:32:32,950
only search in the white space which is

00:32:28,290 --> 00:32:34,450
where the real solutions are so that's

00:32:32,950 --> 00:32:36,040
how it looks and you can build

00:32:34,450 --> 00:32:39,760
constraints like that with the variance

00:32:36,040 --> 00:32:41,919
of 10 blah blah blah and say set strict

00:32:39,760 --> 00:32:43,870
ranges set constraints and there's my

00:32:41,919 --> 00:32:47,530
constraints function and you can do this

00:32:43,870 --> 00:32:49,120
with Monte Carlo methods or statistical

00:32:47,530 --> 00:32:50,740
probability functions any of the

00:32:49,120 --> 00:32:56,460
uncertainty quantification stuff we've

00:32:50,740 --> 00:32:59,160
seen uncertainty quantification now what

00:32:56,460 --> 00:33:02,080
what we can do how do we write this on

00:32:59,160 --> 00:33:04,929
probability functions so what we do is

00:33:02,080 --> 00:33:06,850
here we have constraints right here's

00:33:04,929 --> 00:33:09,130
our probability function our wave

00:33:06,850 --> 00:33:11,889
function here's our constraints operator

00:33:09,130 --> 00:33:14,089
and here's the valid

00:33:11,889 --> 00:33:17,209
wavefunction alright so what we're doing

00:33:14,089 --> 00:33:18,709
is constraining with all the information

00:33:17,209 --> 00:33:20,959
we know about the problem the

00:33:18,709 --> 00:33:22,519
probability distribution but what we're

00:33:20,959 --> 00:33:26,029
doing is we're writing the probability

00:33:22,519 --> 00:33:28,399
distribution as a basis set this is done

00:33:26,029 --> 00:33:33,559
a lot in quantum chemistry right that's

00:33:28,399 --> 00:33:34,879
how you solve for atomic atomic problems

00:33:33,559 --> 00:33:36,769
basically but it's not done in

00:33:34,879 --> 00:33:38,859
optimization this says what is the

00:33:36,769 --> 00:33:40,849
weight and what is the position and

00:33:38,859 --> 00:33:42,459
therefore it takes a probability

00:33:40,849 --> 00:33:46,159
distribution that looks like this and

00:33:42,459 --> 00:33:49,579
represents it by physicians and weights

00:33:46,159 --> 00:33:52,429
of these masses and that allows us to

00:33:49,579 --> 00:33:54,559
take these discrete items and have the

00:33:52,429 --> 00:33:56,839
optimizer move them around and get a

00:33:54,559 --> 00:33:58,399
representation exactly of what the

00:33:56,839 --> 00:34:00,619
probability distribution looks like in

00:33:58,399 --> 00:34:03,349
if we have enough of those masses we can

00:34:00,619 --> 00:34:05,649
basically build any probability

00:34:03,349 --> 00:34:09,279
distribution it's just expensive and

00:34:05,649 --> 00:34:13,579
here is a three-dimensional case of

00:34:09,279 --> 00:34:16,009
basically this is a ballistic test again

00:34:13,579 --> 00:34:17,990
military ballistic test and it's a

00:34:16,009 --> 00:34:20,089
probability cloud at the beginning and

00:34:17,990 --> 00:34:22,849
over several optimizations the

00:34:20,089 --> 00:34:26,179
probability kind of shakes out into a

00:34:22,849 --> 00:34:27,980
two point one is failure case and the

00:34:26,179 --> 00:34:31,099
other is a scenario that's non failure

00:34:27,980 --> 00:34:34,970
and what it looks like is boom boom boom

00:34:31,099 --> 00:34:37,279
and over 7,000 iterations we basically

00:34:34,970 --> 00:34:40,549
start to reduce down to a point we

00:34:37,279 --> 00:34:43,639
actually know the critical scenarios for

00:34:40,549 --> 00:34:45,740
success and failure and if I'd drawn the

00:34:43,639 --> 00:34:47,629
picture better you could see the weights

00:34:45,740 --> 00:34:49,730
instead of just having two black dots

00:34:47,629 --> 00:34:53,119
the weights of being with colors are the

00:34:49,730 --> 00:34:55,519
dots okay and how does that actually

00:34:53,119 --> 00:34:57,440
look in the code you have something like

00:34:55,519 --> 00:35:00,049
this constraints I have a product

00:34:57,440 --> 00:35:05,660
measure I'm converting the optimization

00:35:00,049 --> 00:35:07,910
parameters to a product measure if the

00:35:05,660 --> 00:35:10,880
met if the mass of the measure is not

00:35:07,910 --> 00:35:12,950
one then normalized the measure right I

00:35:10,880 --> 00:35:15,680
mean do you have to have the sum of all

00:35:12,950 --> 00:35:17,390
things the sum of all masses equal to

00:35:15,680 --> 00:35:20,930
one the sum of all weights of the mass

00:35:17,390 --> 00:35:23,269
is equal to one and now here since I

00:35:20,930 --> 00:35:25,020
said I want to work on an expectation

00:35:23,269 --> 00:35:29,520
value like on

00:35:25,020 --> 00:35:32,610
average 10 balls are red office times we

00:35:29,520 --> 00:35:35,040
run it right on average I want the

00:35:32,610 --> 00:35:37,470
expectation value and if the expectation

00:35:35,040 --> 00:35:39,720
value is not within my target plus or

00:35:37,470 --> 00:35:42,420
minus some error then set the

00:35:39,720 --> 00:35:45,210
expectation value to be the target and

00:35:42,420 --> 00:35:48,660
that's me applying a constraint operator

00:35:45,210 --> 00:35:50,550
if if it's not what I want then make it

00:35:48,660 --> 00:35:53,760
what I want and that's how my

00:35:50,550 --> 00:35:56,580
constraints work it forces the optimizer

00:35:53,760 --> 00:36:01,620
to only pick valid points and then my

00:35:56,580 --> 00:36:04,380
cost function is I have a product

00:36:01,620 --> 00:36:09,090
measure and I want the probability of

00:36:04,380 --> 00:36:13,860
failure for my model in terms of the

00:36:09,090 --> 00:36:16,320
product measure which means give me give

00:36:13,860 --> 00:36:18,960
me the probability distribution that

00:36:16,320 --> 00:36:20,790
yields the information on the

00:36:18,960 --> 00:36:24,830
probability of failure for my model I'm

00:36:20,790 --> 00:36:26,940
directly asking for what the probability

00:36:24,830 --> 00:36:28,560
distribution looks like and this is the

00:36:26,940 --> 00:36:30,570
maximum and if I didn't have the minus

00:36:28,560 --> 00:36:32,970
sign there it would be the minimum those

00:36:30,570 --> 00:36:35,400
are my bounds that gives me the value at

00:36:32,970 --> 00:36:37,080
risk for the problem so I can write it

00:36:35,400 --> 00:36:40,530
directly in Python it's actually pretty

00:36:37,080 --> 00:36:43,560
simple to see how it works and I've done

00:36:40,530 --> 00:36:49,520
things like large-scale calculations or

00:36:43,560 --> 00:36:52,110
risk synaptic networks semiconductors

00:36:49,520 --> 00:36:56,100
prediction of earthquakes holistic

00:36:52,110 --> 00:36:58,500
impact and just basically this is

00:36:56,100 --> 00:37:01,230
another ballistic impact problem where

00:36:58,500 --> 00:37:03,030
we can actually use this formulation to

00:37:01,230 --> 00:37:06,140
now start testing well what if this were

00:37:03,030 --> 00:37:08,820
true how would it improve the model

00:37:06,140 --> 00:37:12,780
right and when you do that you now have

00:37:08,820 --> 00:37:14,790
a framework for improving not just

00:37:12,780 --> 00:37:16,590
guessing a model and testing it we have

00:37:14,790 --> 00:37:18,770
a framework for testing new models

00:37:16,590 --> 00:37:21,930
testing and improving your understanding

00:37:18,770 --> 00:37:25,230
of these big nasty complicated problems

00:37:21,930 --> 00:37:29,010
this is something that is absolutely

00:37:25,230 --> 00:37:30,510
unique on the planet right so there's no

00:37:29,010 --> 00:37:31,210
other framework I can do something like

00:37:30,510 --> 00:37:37,119
this

00:37:31,210 --> 00:37:39,490
not only in Python but on the planet so

00:37:37,119 --> 00:37:42,520
we have the only software on the planet

00:37:39,490 --> 00:37:44,859
of rigorously solving for uncertainty

00:37:42,520 --> 00:37:48,809
and risk in large-scale realistic

00:37:44,859 --> 00:37:51,670
problems with oh you Q that's Awadh e's

00:37:48,809 --> 00:37:54,369
mathematical framework we pose our

00:37:51,670 --> 00:37:56,200
questions as optimization optimize all

00:37:54,369 --> 00:37:58,599
over the probability distributions all

00:37:56,200 --> 00:38:01,329
possible scenarios and use Mystics

00:37:58,599 --> 00:38:04,990
constraint solvers to reduce the search

00:38:01,329 --> 00:38:06,670
space to the valid problems so we're

00:38:04,990 --> 00:38:08,800
preparing new releases of our software

00:38:06,670 --> 00:38:11,280
to be released later this month you can

00:38:08,800 --> 00:38:14,200
get them here it's all open source I

00:38:11,280 --> 00:38:19,569
don't release models to the stock market

00:38:14,200 --> 00:38:22,300
but I do release framework ok so you can

00:38:19,569 --> 00:38:23,859
write your own and then do calculations

00:38:22,300 --> 00:38:27,160
so on github

00:38:23,859 --> 00:38:30,250
I have track sites and I am always

00:38:27,160 --> 00:38:32,290
looking for contributors collaborators

00:38:30,250 --> 00:38:36,040
business partners I own my own business

00:38:32,290 --> 00:38:39,339
that I partner with companies to do this

00:38:36,040 --> 00:38:48,520
best place to contact me is maternes at

00:38:39,339 --> 00:38:52,109
Cal Tech so thank you very much so we

00:38:48,520 --> 00:38:52,109
have five minutes for questions here

00:38:53,220 --> 00:39:04,059
nice talk basically I eat personally I

00:38:57,130 --> 00:39:05,980
aspire to be something like you but some

00:39:04,059 --> 00:39:09,609
people say the stock market is around a

00:39:05,980 --> 00:39:12,069
random walk others say it's not

00:39:09,609 --> 00:39:15,520
is there any test that can you can apply

00:39:12,069 --> 00:39:22,780
to basically make sure that you are not

00:39:15,520 --> 00:39:24,700
chasing a statistical boost right so it

00:39:22,780 --> 00:39:31,180
depends on the question you ask right

00:39:24,700 --> 00:39:33,700
and what what you'll see is it doesn't

00:39:31,180 --> 00:39:36,549
matter the level of randomness if you

00:39:33,700 --> 00:39:38,799
ask a bad question that can't be framed

00:39:36,549 --> 00:39:42,609
in such a way to get an answer then

00:39:38,799 --> 00:39:44,520
you'll be chasing a goose right but if

00:39:42,609 --> 00:39:46,710
you ask a question that's framed

00:39:44,520 --> 00:39:48,180
such a way that regardless of the

00:39:46,710 --> 00:39:52,530
randomness regardless of the type of

00:39:48,180 --> 00:39:56,280
randomness where you can you can write

00:39:52,530 --> 00:39:59,100
the question like I did in the first few

00:39:56,280 --> 00:40:01,560
slides like it has to be a targeted

00:39:59,100 --> 00:40:03,990
thing what is the likelihood or what is

00:40:01,560 --> 00:40:06,660
the maximum value what's the likelihood

00:40:03,990 --> 00:40:11,340
that this particular portfolio will

00:40:06,660 --> 00:40:13,620
exceed a certain gain will lose a

00:40:11,340 --> 00:40:15,360
certain amount based on market

00:40:13,620 --> 00:40:18,480
conditions right now so if you say

00:40:15,360 --> 00:40:20,070
something like that and you you give as

00:40:18,480 --> 00:40:25,880
much information about the market you

00:40:20,070 --> 00:40:28,640
can you can predict those questions yeah

00:40:25,880 --> 00:40:30,510
so I saw you had a lot of functions for

00:40:28,640 --> 00:40:33,870
predicting events and getting a

00:40:30,510 --> 00:40:36,180
probability distributions and errors now

00:40:33,870 --> 00:40:39,780
the package contains all these functions

00:40:36,180 --> 00:40:43,380
is what I understand like so are they

00:40:39,780 --> 00:40:46,740
specific to predictions in physically

00:40:43,380 --> 00:40:50,130
terms as in collisions or markets how

00:40:46,740 --> 00:40:52,110
could I use it for training a robot

00:40:50,130 --> 00:40:55,110
model or something like that I would

00:40:52,110 --> 00:40:56,910
potentially need things it's it's it's

00:40:55,110 --> 00:41:02,580
generic mathematical risk and

00:40:56,910 --> 00:41:04,200
uncertainty and all of the all the

00:41:02,580 --> 00:41:06,420
things I've used it for I've used it for

00:41:04,200 --> 00:41:07,920
material science I've used it for

00:41:06,420 --> 00:41:10,740
earthquake science

00:41:07,920 --> 00:41:12,870
I've actually working on some partners

00:41:10,740 --> 00:41:18,960
with some partners right now at HP and

00:41:12,870 --> 00:41:21,720
Cray for and Google for predicting at

00:41:18,960 --> 00:41:25,710
the time of a parallel calculation right

00:41:21,720 --> 00:41:28,260
so it's completely abstract there is if

00:41:25,710 --> 00:41:31,610
you wanted to use it for for something

00:41:28,260 --> 00:41:40,260
with robots you can't

00:41:31,610 --> 00:41:43,530
hi ah so uh I don't really have much of

00:41:40,260 --> 00:41:46,800
a clue about optimization I come from

00:41:43,530 --> 00:41:49,790
machine learning and what we have there

00:41:46,800 --> 00:41:53,310
is I mean it looks like you just want to

00:41:49,790 --> 00:41:54,540
fit a highly dimensional model and the

00:41:53,310 --> 00:41:58,410
biggest problem we have in machine

00:41:54,540 --> 00:42:01,380
learning with a huge future spaces

00:41:58,410 --> 00:42:05,870
or feeding and you didn't seem to talk

00:42:01,380 --> 00:42:08,730
about it yeah so for machine learning

00:42:05,870 --> 00:42:10,770
what's done in machine learning and

00:42:08,730 --> 00:42:13,260
typically uses something like Bayesian

00:42:10,770 --> 00:42:15,390
analysis where or an optimization

00:42:13,260 --> 00:42:18,510
upfront right in the optimization up

00:42:15,390 --> 00:42:21,360
front just like it with Bayesian what

00:42:18,510 --> 00:42:23,880
people do is they try to predict what

00:42:21,360 --> 00:42:26,310
the governing behavior is right now and

00:42:23,880 --> 00:42:28,710
they basically you basically build a

00:42:26,310 --> 00:42:30,030
probability distribution and then build

00:42:28,710 --> 00:42:32,010
a model off that probability

00:42:30,030 --> 00:42:34,680
distribution in machine learning you

00:42:32,010 --> 00:42:37,050
often just build the model off of the

00:42:34,680 --> 00:42:40,350
information you have with the beige ins

00:42:37,050 --> 00:42:42,480
what they do is they build a prior that

00:42:40,350 --> 00:42:44,820
and then that's built off of a

00:42:42,480 --> 00:42:47,160
probability distribution that they fit

00:42:44,820 --> 00:42:49,170
to the current data and then you build a

00:42:47,160 --> 00:42:51,210
model off of that all of those things

00:42:49,170 --> 00:42:52,650
what they do that's what I said in the

00:42:51,210 --> 00:42:54,270
very beginning all those things what

00:42:52,650 --> 00:42:57,000
they do is they assume a probability

00:42:54,270 --> 00:42:58,740
distribution and therefore you won't be

00:42:57,000 --> 00:43:00,840
able to predict black swans you won't be

00:42:58,740 --> 00:43:03,630
able to predict these rare events so

00:43:00,840 --> 00:43:05,340
they work in cases where you're

00:43:03,630 --> 00:43:07,500
predicting the behavior of something

00:43:05,340 --> 00:43:09,870
that you've already seen and it doesn't

00:43:07,500 --> 00:43:11,070
deviate too much from that and you know

00:43:09,870 --> 00:43:12,600
in machine learning what starts to

00:43:11,070 --> 00:43:13,050
happen if you get a real a nonlinear

00:43:12,600 --> 00:43:15,180
system

00:43:13,050 --> 00:43:16,200
it applies for a little while and then

00:43:15,180 --> 00:43:19,320
you have to go back and adjust

00:43:16,200 --> 00:43:21,630
parameters and and start again right so

00:43:19,320 --> 00:43:23,640
this is different what what we actually

00:43:21,630 --> 00:43:26,240
do is say I only want to constrain the

00:43:23,640 --> 00:43:29,190
system with the things that I know and

00:43:26,240 --> 00:43:30,840
then let the probability distribution be

00:43:29,190 --> 00:43:32,870
found as part of the optimization

00:43:30,840 --> 00:43:37,020
problem and that actually scales up

00:43:32,870 --> 00:43:40,320
scales up to very very large sizes so

00:43:37,020 --> 00:43:46,980
that was luck on my part for that to

00:43:40,320 --> 00:43:50,130
happen I just have two questions first

00:43:46,980 --> 00:43:51,780
one is uh just a fundamental question I

00:43:50,130 --> 00:43:53,640
know you talk about predicting black

00:43:51,780 --> 00:43:56,430
swans so if you read a Nicholas book on

00:43:53,640 --> 00:43:59,400
the topic what he says is that you know

00:43:56,430 --> 00:44:01,800
black swans are impossibly difficult I

00:43:59,400 --> 00:44:03,720
think you mentioned that so I think my

00:44:01,800 --> 00:44:05,070
question is more like you know the

00:44:03,720 --> 00:44:06,120
fundamental instead of a black swan is

00:44:05,070 --> 00:44:10,380
that there is not enough information

00:44:06,120 --> 00:44:12,150
right now in the past and right now to

00:44:10,380 --> 00:44:12,610
predict projected right so we are

00:44:12,150 --> 00:44:15,460
talking

00:44:12,610 --> 00:44:17,320
information lost there right so if

00:44:15,460 --> 00:44:18,490
you're talking about actually predicting

00:44:17,320 --> 00:44:22,450
them you know what is though

00:44:18,490 --> 00:44:25,090
sure so the the reason the reason that

00:44:22,450 --> 00:44:27,400
people say and in his book he says it's

00:44:25,090 --> 00:44:31,770
impossible to predict is everybody does

00:44:27,400 --> 00:44:34,030
predictions based on the past and

00:44:31,770 --> 00:44:38,620
everybody does everybody knows

00:44:34,030 --> 00:44:41,320
predictions based on the past and so

00:44:38,620 --> 00:44:43,290
what what you get is everybody tries to

00:44:41,320 --> 00:44:47,650
build these probability distributions

00:44:43,290 --> 00:44:50,710
right like this based on data that you

00:44:47,650 --> 00:44:52,750
have already what that does is when you

00:44:50,710 --> 00:44:55,240
start sampling off of these probability

00:44:52,750 --> 00:44:57,250
distributions you're only sampling off

00:44:55,240 --> 00:45:00,190
the events you've seen already so it's

00:44:57,250 --> 00:45:01,720
impossible to ever see an event that

00:45:00,190 --> 00:45:03,490
happens out here and that would be a

00:45:01,720 --> 00:45:06,820
Black Swan that completely changes the

00:45:03,490 --> 00:45:08,110
behavior what I do with constraints what

00:45:06,820 --> 00:45:10,270
I do with an optimization over

00:45:08,110 --> 00:45:12,340
probability distributions is to say

00:45:10,270 --> 00:45:15,370
never select the probability

00:45:12,340 --> 00:45:17,470
distribution let the optimizer find the

00:45:15,370 --> 00:45:20,410
probability distribution as it's going

00:45:17,470 --> 00:45:23,650
and what do I do with information is I

00:45:20,410 --> 00:45:24,460
say I now again don't make any

00:45:23,650 --> 00:45:28,210
assumptions

00:45:24,460 --> 00:45:29,920
I just reduce the search space to the

00:45:28,210 --> 00:45:33,520
places where the constraints are valid

00:45:29,920 --> 00:45:36,520
and what that says is you don't see the

00:45:33,520 --> 00:45:40,180
Black Swan but you leave a place for it

00:45:36,520 --> 00:45:42,340
to happen like this space if this

00:45:40,180 --> 00:45:45,550
satisfied all the constraints I don't

00:45:42,340 --> 00:45:48,190
even need a data point there but because

00:45:45,550 --> 00:45:50,380
all the constraints are valid it's open

00:45:48,190 --> 00:45:53,500
searchable space by the optimizer and

00:45:50,380 --> 00:45:56,080
the probability distribution can find it

00:45:53,500 --> 00:45:58,630
and you can find things there these rare

00:45:56,080 --> 00:46:01,270
events you never see thanks a lot and

00:45:58,630 --> 00:46:03,430
last question do you what features of

00:46:01,270 --> 00:46:05,680
Python you know made you choose the

00:46:03,430 --> 00:46:07,420
language so as to fit this you know

00:46:05,680 --> 00:46:09,400
solving this particular you know problem

00:46:07,420 --> 00:46:12,160
yeah you know I saw a decorator and that

00:46:09,400 --> 00:46:14,710
was amazing to see you know so could you

00:46:12,160 --> 00:46:17,950
just briefly disable sure the the real

00:46:14,710 --> 00:46:21,490
thing is I started writing this ten

00:46:17,950 --> 00:46:24,250
years ago and there wasn't anything that

00:46:21,490 --> 00:46:26,060
could do it and I was running a lot of

00:46:24,250 --> 00:46:27,680
projects at the time and

00:46:26,060 --> 00:46:30,080
Python was the only thing that I could

00:46:27,680 --> 00:46:33,680
do where I could manage a project and go

00:46:30,080 --> 00:46:35,650
back and code at the same time and one

00:46:33,680 --> 00:46:40,490
of the things I wrote back then was this

00:46:35,650 --> 00:46:42,200
serialization piece of code that it's

00:46:40,490 --> 00:46:44,900
called dill and it serializes all of

00:46:42,200 --> 00:46:47,750
Python and when I have that then I can

00:46:44,900 --> 00:46:49,940
ship all of my code out I can ship my

00:46:47,750 --> 00:46:53,480
functions out anywhere I want across

00:46:49,940 --> 00:46:56,120
another process on other machines and it

00:46:53,480 --> 00:46:59,030
basically gives me ability to if I have

00:46:56,120 --> 00:47:01,010
code that runs it then I can run on

00:46:59,030 --> 00:47:02,210
several different machines very

00:47:01,010 --> 00:47:03,860
transparently and as soon as that

00:47:02,210 --> 00:47:05,450
happens I said I want to stay out of sea

00:47:03,860 --> 00:47:07,580
I want to stay out of these higher

00:47:05,450 --> 00:47:10,100
performance languages because with

00:47:07,580 --> 00:47:12,020
Python I can put the code there and run

00:47:10,100 --> 00:47:14,540
it and collect it back again and so I

00:47:12,020 --> 00:47:16,970
started to build everything no see no

00:47:14,540 --> 00:47:19,760
complicated anything no compiling just

00:47:16,970 --> 00:47:21,230
Python because it's fast to go back and

00:47:19,760 --> 00:47:53,120
forth and that's what really did it for

00:47:21,230 --> 00:47:56,330
me yeah sure yeah so that is a great

00:47:53,120 --> 00:47:58,310
observation and actually this slide

00:47:56,330 --> 00:48:01,930
right here at the top I didn't talk

00:47:58,310 --> 00:48:05,180
about this part but here's your prior

00:48:01,930 --> 00:48:07,790
write Bayesian here's your prior those

00:48:05,180 --> 00:48:09,860
how I write my constraints right so I

00:48:07,790 --> 00:48:13,310
actually one of the things I do when I

00:48:09,860 --> 00:48:16,100
go to companies who use Bayesian and

00:48:13,310 --> 00:48:17,840
machine learning as their current method

00:48:16,100 --> 00:48:20,960
and they make approximations and they

00:48:17,840 --> 00:48:22,850
pick a probability distribution I say

00:48:20,960 --> 00:48:25,670
don't pick the probability distribution

00:48:22,850 --> 00:48:28,610
let's do that as a pre factor a pre step

00:48:25,670 --> 00:48:30,650
and get the optimal prior the best

00:48:28,610 --> 00:48:32,300
probability distribution and then you

00:48:30,650 --> 00:48:34,280
can sample off of it and do your machine

00:48:32,300 --> 00:48:36,620
learning and do whatever Bayesian you

00:48:34,280 --> 00:48:39,320
like but you'll be sampling off of the

00:48:36,620 --> 00:48:41,090
best answer instead of one you picked

00:48:39,320 --> 00:48:43,100
and that allows everybody in those

00:48:41,090 --> 00:48:47,090
companies to do stuff at the same speed

00:48:43,100 --> 00:48:50,140
with the same code but get working off

00:48:47,090 --> 00:48:54,110
of a better actually the best solution

00:48:50,140 --> 00:48:56,300
so yes its direct parallel except I

00:48:54,110 --> 00:48:58,760
don't make the critical strong

00:48:56,300 --> 00:49:01,420
approximation that those techniques do

00:48:58,760 --> 00:49:04,190
that allow me to get these black swans

00:49:01,420 --> 00:49:08,090
yeah if you have wrong information wrong

00:49:04,190 --> 00:49:11,620
answers no problem you thanks Mike for

00:49:08,090 --> 00:49:11,620

YouTube URL: https://www.youtube.com/watch?v=Jsv24N-JJbY


