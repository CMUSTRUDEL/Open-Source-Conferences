Title: Liberating tabular data from the clutches of PDFs
Publication date: 2018-01-01
Playlist: PyCon India 2017
Description: 
	
Captions: 
	00:00:00,000 --> 00:00:06,120
our model documents the reason why we

00:00:03,480 --> 00:00:08,630
call the model documents is because they

00:00:06,120 --> 00:00:11,429
let you know what is the intent of

00:00:08,630 --> 00:00:13,320
government in any case any financial

00:00:11,429 --> 00:00:19,380
body because we have different days in

00:00:13,320 --> 00:00:23,580
government from state and municipal

00:00:19,380 --> 00:00:26,939
corporations and the kind of data that

00:00:23,580 --> 00:00:29,699
they publish is always in our future so

00:00:26,939 --> 00:00:32,460
they propose for 2017-18 they will

00:00:29,699 --> 00:00:36,120
propose a budget and that budget is

00:00:32,460 --> 00:00:42,629
basically then released by the center to

00:00:36,120 --> 00:00:45,300
other lower organizations and so the

00:00:42,629 --> 00:00:47,190
kind of data that you get is for this

00:00:45,300 --> 00:00:48,719
year you get a proposed budget and for

00:00:47,190 --> 00:00:52,110
last year's budget you will get a

00:00:48,719 --> 00:00:54,180
revised estimate which is not act your

00:00:52,110 --> 00:00:57,420
actual spend but it's kind of a revised

00:00:54,180 --> 00:00:58,649
of approximate spend and for last two

00:00:57,420 --> 00:01:01,739
last year's you get the actual

00:00:58,649 --> 00:01:04,920
expenditure so you have a two years

00:01:01,739 --> 00:01:07,790
lag and the reason why why that exists

00:01:04,920 --> 00:01:10,350
is this is a audit cycle that happens

00:01:07,790 --> 00:01:14,070
each year that it's supposed to take

00:01:10,350 --> 00:01:17,130
around nine months but it usually ends

00:01:14,070 --> 00:01:20,549
up taking here or year and a half and

00:01:17,130 --> 00:01:23,040
it's suddenly it's all manual so for

00:01:20,549 --> 00:01:28,619
being automated and really helpful but

00:01:23,040 --> 00:01:32,509
you know and so all these documents they

00:01:28,619 --> 00:01:35,250
published by different financial bodies

00:01:32,509 --> 00:01:36,689
separately so the state governments

00:01:35,250 --> 00:01:40,170
publish their data on their own web

00:01:36,689 --> 00:01:42,450
portals and municipal corporations again

00:01:40,170 --> 00:01:44,970
publishes their data on different web

00:01:42,450 --> 00:01:47,939
portals and similarly you have different

00:01:44,970 --> 00:01:52,020
by poodles community trees two districts

00:01:47,939 --> 00:01:55,770
and the reason I mean with that died was

00:01:52,020 --> 00:01:58,829
he kind of get around 150 plus websites

00:01:55,770 --> 00:02:02,399
that website so web portals that kind of

00:01:58,829 --> 00:02:04,740
need to get the data problem and the

00:02:02,399 --> 00:02:09,000
thing is the data that you get again

00:02:04,740 --> 00:02:11,730
Media fall back so the reason why PDS

00:02:09,000 --> 00:02:17,430
don't work is the only way I can use PDF

00:02:11,730 --> 00:02:22,290
data to analyze something is either copy

00:02:17,430 --> 00:02:23,550
pasting from there and that is also if

00:02:22,290 --> 00:02:25,590
you are lucky because if it's a scanned

00:02:23,550 --> 00:02:29,730
image there any kind even do that and

00:02:25,590 --> 00:02:34,500
it's it won't be even searchable so we

00:02:29,730 --> 00:02:38,269
and well with 150 plus different domains

00:02:34,500 --> 00:02:44,130
there's no consistency of formats and

00:02:38,269 --> 00:02:47,730
it's like so you get data from some

00:02:44,130 --> 00:02:50,880
things like this budget I'm not sure if

00:02:47,730 --> 00:02:54,050
it's very visible but yeah this their

00:02:50,880 --> 00:02:58,950
lines here that separate the columns

00:02:54,050 --> 00:03:01,370
which is fairly tabular and then you get

00:02:58,950 --> 00:03:05,670
something like this from West Bengal

00:03:01,370 --> 00:03:08,640
which they don't have any particular

00:03:05,670 --> 00:03:11,579
lines or separating columns or rows they

00:03:08,640 --> 00:03:14,209
do have separators but they indicate

00:03:11,579 --> 00:03:17,090
kind of groups of Spain expenditure and

00:03:14,209 --> 00:03:20,480
then you have God's wrath which is

00:03:17,090 --> 00:03:22,920
multilingual in nature and you

00:03:20,480 --> 00:03:26,760
distinguish Rotti and English at the

00:03:22,920 --> 00:03:29,900
same time and with the resolutions are

00:03:26,760 --> 00:03:32,489
also different across different PDFs and

00:03:29,900 --> 00:03:38,250
so you get a mix of different languages

00:03:32,489 --> 00:03:41,730
and different structures when they

00:03:38,250 --> 00:03:46,620
they're not common across any of them so

00:03:41,730 --> 00:03:50,459
this leads to data that we are getting

00:03:46,620 --> 00:03:54,030
so to describe the algorithms that we've

00:03:50,459 --> 00:03:59,070
built or gone we're gonna divide all the

00:03:54,030 --> 00:04:01,709
PDFs into two categories which are table

00:03:59,070 --> 00:04:06,480
PDS with table boundaries and PDS

00:04:01,709 --> 00:04:11,340
without any table boundaries so the idea

00:04:06,480 --> 00:04:12,080
behind extracting data from a verb ound

00:04:11,340 --> 00:04:15,360
Rees

00:04:12,080 --> 00:04:17,790
but simple that you somehow need to

00:04:15,360 --> 00:04:21,090
detect the lines that separate the

00:04:17,790 --> 00:04:23,580
columns and the rows and once you have

00:04:21,090 --> 00:04:25,620
that then you kind of figure out what

00:04:23,580 --> 00:04:28,350
are the table boundaries and from that

00:04:25,620 --> 00:04:34,340
again you just simply somehow need to

00:04:28,350 --> 00:04:37,230
pass all that information into a CSV now

00:04:34,340 --> 00:04:41,970
the first step to both of the algorithms

00:04:37,230 --> 00:04:45,690
is reading a PDF page into a image so we

00:04:41,970 --> 00:04:47,880
used convert which is from image Matic

00:04:45,690 --> 00:04:50,790
and we passed the density parameter to

00:04:47,880 --> 00:04:53,610
kind of have uniformity across all the

00:04:50,790 --> 00:04:56,760
PDF pages if we were passing and you

00:04:53,610 --> 00:04:59,820
just passed the PDF file path the page

00:04:56,760 --> 00:05:02,940
number and the image file that where you

00:04:59,820 --> 00:05:04,919
want to stop and we use sub-process to

00:05:02,940 --> 00:05:10,290
kind of get the shell output and

00:05:04,919 --> 00:05:12,630
directly read the files so once you read

00:05:10,290 --> 00:05:16,830
it we converted into a grayscale to make

00:05:12,630 --> 00:05:21,710
it easy to process one question is it

00:05:16,830 --> 00:05:21,710
all visible correct or just me

00:05:24,310 --> 00:05:33,490
lately problematic but okay so I'll try

00:05:27,860 --> 00:05:33,490
to explain what it looks like if needed

00:05:34,300 --> 00:05:42,830
so this is Kannadigas budget and very

00:05:38,960 --> 00:05:46,820
well it's separated and the way we draw

00:05:42,830 --> 00:05:51,430
lines is we use an algorithm called half

00:05:46,820 --> 00:05:54,740
lines though so there's a slight

00:05:51,430 --> 00:05:59,870
requirements for transforms or half

00:05:54,740 --> 00:06:01,850
lines is you need edges so from this PDF

00:05:59,870 --> 00:06:05,660
we kind of need to extract edges that

00:06:01,850 --> 00:06:11,030
are possibly lines and the way we draw

00:06:05,660 --> 00:06:14,510
edges is we use Kenny Kenny edge

00:06:11,030 --> 00:06:17,690
detector you can also use adaptive

00:06:14,510 --> 00:06:21,860
thresholding if required but this is

00:06:17,690 --> 00:06:27,830
something what it looks like so all the

00:06:21,860 --> 00:06:31,610
major lines get like you you get the

00:06:27,830 --> 00:06:36,590
text edges plus the lines and from this

00:06:31,610 --> 00:06:41,930
you pass this to Hough transform and the

00:06:36,590 --> 00:06:44,450
way half lines so for each edge so if

00:06:41,930 --> 00:06:46,400
you see this moving graph so all dots

00:06:44,450 --> 00:06:52,220
are edges it's gonna draw a line and

00:06:46,400 --> 00:06:55,580
from each for each line it draws values

00:06:52,220 --> 00:06:58,910
in a very different space so lines what

00:06:55,580 --> 00:07:00,170
we call in an XY plane so what are

00:06:58,910 --> 00:07:03,740
transform does is converted into a

00:07:00,170 --> 00:07:06,680
parametric pain which is distanced by

00:07:03,740 --> 00:07:09,590
distance from the centre and the angle

00:07:06,680 --> 00:07:12,920
at which they are so for each point what

00:07:09,590 --> 00:07:18,800
is representing is how far it is from

00:07:12,920 --> 00:07:21,290
the source and what angle it is so as

00:07:18,800 --> 00:07:24,410
keeps on repeating itself for each edge

00:07:21,290 --> 00:07:29,060
it's gonna draw a line and for that line

00:07:24,410 --> 00:07:32,300
it draws value and it does a voting

00:07:29,060 --> 00:07:33,639
system where it ended highest voted ones

00:07:32,300 --> 00:07:38,229
are the most probable lines

00:07:33,639 --> 00:07:40,270
but half lines per se it gives you a

00:07:38,229 --> 00:07:42,610
combination of these parametric spaces

00:07:40,270 --> 00:07:45,879
so if you see these values this is a

00:07:42,610 --> 00:07:51,819
p-value how far it is and the degree at

00:07:45,879 --> 00:07:54,219
which it is but and so we use something

00:07:51,819 --> 00:07:58,330
called half lines P which directly gives

00:07:54,219 --> 00:08:00,879
you the lines of where it they are

00:07:58,330 --> 00:08:02,349
rather than just the points so figures

00:08:00,879 --> 00:08:07,090
out what are the most probable lines and

00:08:02,349 --> 00:08:10,870
it gives you those XY points well

00:08:07,090 --> 00:08:14,830
actually two points so this is what

00:08:10,870 --> 00:08:16,900
something it starts looking like but as

00:08:14,830 --> 00:08:19,960
you can see these are quite incomplete

00:08:16,900 --> 00:08:22,569
in nature so we do clean up exercise

00:08:19,960 --> 00:08:26,259
where we figure out what the table

00:08:22,569 --> 00:08:29,879
boundaries and what we kind of extend

00:08:26,259 --> 00:08:33,130
these lines and after that you get

00:08:29,879 --> 00:08:38,560
proper structured format and overlay of

00:08:33,130 --> 00:08:40,449
the table so when we have most of the

00:08:38,560 --> 00:08:46,560
things now only thing that we need to

00:08:40,449 --> 00:08:50,680
pass this table of information of tables

00:08:46,560 --> 00:08:53,410
the columns where they are in to CSV so

00:08:50,680 --> 00:08:55,870
we for this particular method something

00:08:53,410 --> 00:08:59,320
called tabular which is an open source

00:08:55,870 --> 00:08:59,949
tool which kind of makes is very easy

00:08:59,320 --> 00:09:03,880
for us

00:08:59,949 --> 00:09:06,459
so you just pass the page page number

00:09:03,880 --> 00:09:09,550
with these pages and then you say the

00:09:06,459 --> 00:09:12,790
area which is the table area and the

00:09:09,550 --> 00:09:15,750
columns so these are geometrical points

00:09:12,790 --> 00:09:19,230
where the columns are and that gives you

00:09:15,750 --> 00:09:21,990
the CSV and the way you read it outputs

00:09:19,230 --> 00:09:24,990
the sees me into the shell again you you

00:09:21,990 --> 00:09:31,920
just feel suffices to get the CSU Union

00:09:24,990 --> 00:09:35,520
right but the problem with this is you

00:09:31,920 --> 00:09:39,270
it doesn't take rows as an input so you

00:09:35,520 --> 00:09:43,640
don't have your toes end up sometimes

00:09:39,270 --> 00:09:47,490
being combined or combined together or

00:09:43,640 --> 00:09:51,660
and your titles in everything so but the

00:09:47,490 --> 00:09:54,090
documents are not very structured so you

00:09:51,660 --> 00:09:55,770
kind of get some orders at times and

00:09:54,090 --> 00:09:58,440
also the limitation is that this method

00:09:55,770 --> 00:10:05,790
only works with where you actually have

00:09:58,440 --> 00:10:10,010
lines so well I'm actually moving quite

00:10:05,790 --> 00:10:10,010
fast so in case someone has a question

00:10:10,130 --> 00:10:18,420
and so extracting table without

00:10:15,720 --> 00:10:20,490
boundaries so I'm gonna do an exercise a

00:10:18,420 --> 00:10:25,110
kind of want anyone to participate in

00:10:20,490 --> 00:10:28,730
this so it just want to point out

00:10:25,110 --> 00:10:43,290
whether this looks like a table or not

00:10:28,730 --> 00:10:46,080
and this one and this one so the reason

00:10:43,290 --> 00:10:48,690
why we can easily see through this is

00:10:46,080 --> 00:10:52,260
because we understand patterns really

00:10:48,690 --> 00:10:55,140
well and as you means we can figure out

00:10:52,260 --> 00:10:56,580
how the relative distances are and what

00:10:55,140 --> 00:10:58,620
kind of structure it's been form and

00:10:56,580 --> 00:11:01,170
forming and we can recognize whether it

00:10:58,620 --> 00:11:03,089
is a table or not and we also recognize

00:11:01,170 --> 00:11:07,350
where the table would break or my

00:11:03,089 --> 00:11:09,690
country given other details so but how

00:11:07,350 --> 00:11:13,640
do we let a machine kind of understand

00:11:09,690 --> 00:11:16,800
these same things the way we do it is

00:11:13,640 --> 00:11:20,160
you need to somehow extract geometrical

00:11:16,800 --> 00:11:22,710
features and extract intimate racal

00:11:20,160 --> 00:11:24,570
features we first convert we need

00:11:22,710 --> 00:11:25,100
something like a block structure where

00:11:24,570 --> 00:11:27,440
we can

00:11:25,100 --> 00:11:31,550
had the machine see it's just not text

00:11:27,440 --> 00:11:35,089
it's either blocks of text and for that

00:11:31,550 --> 00:11:40,459
we used initially at that time we use

00:11:35,089 --> 00:11:43,310
something called smoothing algorithm the

00:11:40,459 --> 00:11:45,560
way it works is for each a promised goes

00:11:43,310 --> 00:11:49,970
from one one pixel to another and it

00:11:45,560 --> 00:11:54,440
takes threshold of values and if it's a

00:11:49,970 --> 00:11:56,930
one it converts that 0 into one and if

00:11:54,440 --> 00:12:01,160
it's not it keeps it as zero so imagine

00:11:56,930 --> 00:12:04,660
being done on a lot of critics and to do

00:12:01,160 --> 00:12:07,639
it in a 2d structure we did we did site

00:12:04,660 --> 00:12:10,730
modification that we take metrics rather

00:12:07,639 --> 00:12:13,480
than just align so and you start getting

00:12:10,730 --> 00:12:19,040
something like block structure like this

00:12:13,480 --> 00:12:27,910
but the problem with this is it took 50

00:12:19,040 --> 00:12:29,779
seconds so an alternative to this is

00:12:27,910 --> 00:12:32,980
morphological operation with an again

00:12:29,779 --> 00:12:35,480
part of open say me and what I can do

00:12:32,980 --> 00:12:37,250
and it actually gives you if we see a

00:12:35,480 --> 00:12:42,920
much more cleaner results it doesn't

00:12:37,250 --> 00:12:45,019
have all the edges the big mountain so

00:12:42,920 --> 00:12:48,800
once you have this you now we need to

00:12:45,019 --> 00:12:51,470
kind of have a metrical separations some

00:12:48,800 --> 00:12:54,380
information on how we can separate them

00:12:51,470 --> 00:12:56,899
or draw patterns and the way we do that

00:12:54,380 --> 00:13:00,920
is something called component with

00:12:56,899 --> 00:13:03,139
starts so comprehend with stats it's got

00:13:00,920 --> 00:13:06,079
a pic for each white block that you see

00:13:03,139 --> 00:13:08,810
it's kind of pick gonna figure out what

00:13:06,079 --> 00:13:11,050
is its left most point what is the top

00:13:08,810 --> 00:13:15,589
post point and the width and the height

00:13:11,050 --> 00:13:17,209
area and centroid so with this

00:13:15,589 --> 00:13:19,160
information you can actually I just like

00:13:17,209 --> 00:13:22,639
more information which is right in

00:13:19,160 --> 00:13:24,949
bottom and you kind of get for each

00:13:22,639 --> 00:13:26,689
block you get all symmetrical

00:13:24,949 --> 00:13:27,259
information with this geometrical

00:13:26,689 --> 00:13:29,689
information

00:13:27,259 --> 00:13:32,329
now you can start seeing patterns by

00:13:29,689 --> 00:13:34,490
just grouping numbers if you group all

00:13:32,329 --> 00:13:37,100
the left side you will be able to see

00:13:34,490 --> 00:13:40,280
all these left aligned parts

00:13:37,100 --> 00:13:41,920
in the same column and with these you

00:13:40,280 --> 00:13:49,190
can estimate where the columns are and

00:13:41,920 --> 00:13:53,180
similarly for those now so but this

00:13:49,190 --> 00:13:56,240
information is not enough to parse the

00:13:53,180 --> 00:13:59,810
PDF so the reason why we want to pass it

00:13:56,240 --> 00:14:04,280
not using tamanna is one and one and

00:13:59,810 --> 00:14:06,800
only it didn't work so tumblr needs

00:14:04,280 --> 00:14:10,280
lines if you don't have lines it's not

00:14:06,800 --> 00:14:12,080
able to pass it correctly and so we need

00:14:10,280 --> 00:14:14,150
to kind of build a custom parser for

00:14:12,080 --> 00:14:17,000
this and to build that we need to do

00:14:14,150 --> 00:14:19,790
something called as layout analysis now

00:14:17,000 --> 00:14:20,930
by layer analysis we mean that we need

00:14:19,790 --> 00:14:25,040
to figure out where the head is

00:14:20,930 --> 00:14:27,910
or where the values are and even the

00:14:25,040 --> 00:14:32,180
empty cells and something called

00:14:27,910 --> 00:14:34,730
groupings by groupings I mean the left

00:14:32,180 --> 00:14:36,920
xed around the values you need to Chi we

00:14:34,730 --> 00:14:38,390
call them groupings because well in

00:14:36,920 --> 00:14:42,470
pandas if you do a grouper you kind of

00:14:38,390 --> 00:14:47,450
get the aggregations it's just going

00:14:42,470 --> 00:14:50,930
down that we use so to convert the text

00:14:47,450 --> 00:14:53,990
from BDF we use something called PDF to

00:14:50,930 --> 00:14:58,250
text which is an Ubuntu you have that

00:14:53,990 --> 00:14:59,780
pool in Linux Ubuntu but we use a very

00:14:58,250 --> 00:15:02,780
specific version which is popular as

00:14:59,780 --> 00:15:05,420
version the reason why I use that as we

00:15:02,780 --> 00:15:08,630
need text from very particular block and

00:15:05,420 --> 00:15:10,310
very particular mitrice and the way we

00:15:08,630 --> 00:15:12,650
kind of do it is because we already have

00:15:10,310 --> 00:15:14,210
the geometrical information of the

00:15:12,650 --> 00:15:16,670
topmost points and the leftmost points

00:15:14,210 --> 00:15:19,040
it's easy to kind of get the text which

00:15:16,670 --> 00:15:19,939
for each block we kind of get something

00:15:19,040 --> 00:15:23,059
like this

00:15:19,939 --> 00:15:25,939
you have the X X length and we start

00:15:23,059 --> 00:15:28,669
adding some more features which is like

00:15:25,939 --> 00:15:34,429
it's a comma-separated or is it text or

00:15:28,669 --> 00:15:35,989
is it number and the text length so I've

00:15:34,429 --> 00:15:38,659
limited these columns so you actually

00:15:35,989 --> 00:15:43,279
have for each row you have a whole set

00:15:38,659 --> 00:15:47,089
of symmetrical features plus it's based

00:15:43,279 --> 00:15:49,909
on these now we start we start marking

00:15:47,089 --> 00:15:52,789
these with numbers header titles

00:15:49,909 --> 00:15:58,699
groupings which was our initial agenda

00:15:52,789 --> 00:16:02,329
to pass these PDFs into CSV and the way

00:15:58,699 --> 00:16:04,429
we do is we have currently built rules

00:16:02,329 --> 00:16:08,720
around this since we don't have much

00:16:04,429 --> 00:16:10,279
data around how we can automate this so

00:16:08,720 --> 00:16:12,049
for number is it's easy to detect

00:16:10,279 --> 00:16:15,109
numbers if it's convertible by

00:16:12,049 --> 00:16:15,649
convertible to a number it's it's way

00:16:15,109 --> 00:16:18,949
simple

00:16:15,649 --> 00:16:21,169
and for headers we kind of make all the

00:16:18,949 --> 00:16:29,989
headers should have at least 70% of the

00:16:21,169 --> 00:16:32,119
values below it as numbers this one so

00:16:29,989 --> 00:16:33,559
all these are headers and most of the

00:16:32,119 --> 00:16:36,829
values that you'll see below this will

00:16:33,559 --> 00:16:41,619
be numbers and the 70% we come up with

00:16:36,829 --> 00:16:44,629
is 70% of the table itself not the page

00:16:41,619 --> 00:16:48,499
the reason we do that is because you

00:16:44,629 --> 00:16:50,629
kind of get a multi tabled page so it

00:16:48,499 --> 00:16:53,779
would have two or three tables at times

00:16:50,629 --> 00:16:56,179
of the page and similarly we kind of

00:16:53,779 --> 00:16:58,069
check titles titles are the you just

00:16:56,179 --> 00:17:01,359
need to detect use a center line and has

00:16:58,069 --> 00:17:04,069
a very huge area compared to others and

00:17:01,359 --> 00:17:07,189
to do relative comparing we kind of the

00:17:04,069 --> 00:17:10,039
scalp you'll see scores which is if you

00:17:07,189 --> 00:17:12,949
don't know it can just simply how far it

00:17:10,039 --> 00:17:14,809
is from the mean it just figures out

00:17:12,949 --> 00:17:16,789
what is the average and how far it is

00:17:14,809 --> 00:17:17,920
and based on those numbers you kind of

00:17:16,789 --> 00:17:21,040
sort out and

00:17:17,920 --> 00:17:24,070
titles and similarly

00:17:21,040 --> 00:17:26,200
gropings is it should have all the

00:17:24,070 --> 00:17:30,370
values on the right of it should be our

00:17:26,200 --> 00:17:32,080
numbers so it does not work completely

00:17:30,370 --> 00:17:34,120
because as you can see you kind of get

00:17:32,080 --> 00:17:37,390
Nan's and everything the reason for that

00:17:34,120 --> 00:17:41,230
is I mean you don't need to make it

00:17:37,390 --> 00:17:42,510
perfect is need to it to run reason you

00:17:41,230 --> 00:17:44,920
get Nance in everything because

00:17:42,510 --> 00:17:47,320
groupings can be multi-level groupings

00:17:44,920 --> 00:17:49,960
so it will detect one and another might

00:17:47,320 --> 00:17:58,290
not be protected so you kind of get a

00:17:49,960 --> 00:18:02,710
mix so beyond this we kind of just do

00:17:58,290 --> 00:18:04,510
passing off go by row by row and we

00:18:02,710 --> 00:18:06,340
generate row indexes column index lists

00:18:04,510 --> 00:18:11,740
and just convert those column and row

00:18:06,340 --> 00:18:14,380
indexes into tables but the thing is do

00:18:11,740 --> 00:18:20,470
not scale and with my past experience

00:18:14,380 --> 00:18:23,710
tried it with NLP tools I know it will

00:18:20,470 --> 00:18:25,930
fail and even though it might work for

00:18:23,710 --> 00:18:28,960
some it's still gonna fail for most of

00:18:25,930 --> 00:18:32,680
them so the point is how do you get

00:18:28,960 --> 00:18:36,190
ready to automate these things or become

00:18:32,680 --> 00:18:41,110
machine learning ready the way we are

00:18:36,190 --> 00:18:43,300
doing we are generating metadata so we

00:18:41,110 --> 00:18:45,190
log on the raw images and all the

00:18:43,300 --> 00:18:46,690
blocked images and then the features

00:18:45,190 --> 00:18:48,040
that we immunity which look start

00:18:46,690 --> 00:18:50,350
talking something like this you have the

00:18:48,040 --> 00:18:56,040
area of bottom centroids and this is for

00:18:50,350 --> 00:18:59,110
each block and then you kind of have

00:18:56,040 --> 00:19:02,110
this the outputs office with the scenes

00:18:59,110 --> 00:19:04,230
piece and using these these gives you

00:19:02,110 --> 00:19:08,520
opportunities to

00:19:04,230 --> 00:19:11,070
do some crazy stuff like now given the

00:19:08,520 --> 00:19:12,990
text and the information that I have for

00:19:11,070 --> 00:19:15,240
each block so I have the text edit hands

00:19:12,990 --> 00:19:16,140
and I have the intimate Racal

00:19:15,240 --> 00:19:18,990
positioning of it

00:19:16,140 --> 00:19:20,809
I can build a small Beijing model just

00:19:18,990 --> 00:19:26,520
because just with the small data set

00:19:20,809 --> 00:19:30,000
which would have trials of we'll be able

00:19:26,520 --> 00:19:33,780
to estimate if some table lies at the

00:19:30,000 --> 00:19:35,970
center and its values are numbered then

00:19:33,780 --> 00:19:39,840
it's most probably a number value rather

00:19:35,970 --> 00:19:43,080
than being a header or grouping and

00:19:39,840 --> 00:19:47,790
similarly the left align would be the

00:19:43,080 --> 00:19:49,290
groupings and so these are like trying

00:19:47,790 --> 00:19:54,240
trying to build intelligences to a

00:19:49,290 --> 00:19:57,929
system and another thing that we plan to

00:19:54,240 --> 00:19:59,850
do is we can we convert directly from

00:19:57,929 --> 00:20:03,510
image can reach generate the markings

00:19:59,850 --> 00:20:07,380
which is a very good use case for our an

00:20:03,510 --> 00:20:09,960
enzyme cnn's and but we're still

00:20:07,380 --> 00:20:12,320
generating the data and as most of us

00:20:09,960 --> 00:20:15,320
would know that takes a lot of data to

00:20:12,320 --> 00:20:15,320
train

00:20:19,290 --> 00:20:25,140
so I mean before I move forward like I

00:20:22,549 --> 00:20:30,330
kind of just try to emphasize of why

00:20:25,140 --> 00:20:36,350
we're doing this so recently there was a

00:20:30,330 --> 00:20:42,660
study by NDTV so good Roth has been

00:20:36,350 --> 00:20:44,220
reducing its spend or buy from in the

00:20:42,660 --> 00:20:46,760
last three years it is reduced from five

00:20:44,220 --> 00:20:53,700
point six percent to 5 percent only and

00:20:46,760 --> 00:20:58,830
most of it is focused on rural only in

00:20:53,700 --> 00:21:02,669
the rural areas urban areas which is a

00:20:58,830 --> 00:21:05,490
problem and if we can figure out and

00:21:02,669 --> 00:21:10,860
analyze these kind of problems at early

00:21:05,490 --> 00:21:13,950
it's easier to kind of ask you no matter

00:21:10,860 --> 00:21:16,860
what you're doing it's not questioning

00:21:13,950 --> 00:21:23,940
and start kind of help bringing a change

00:21:16,860 --> 00:21:25,830
in the system so this is how we adopt a

00:21:23,940 --> 00:21:27,030
token budgets India which is the

00:21:25,830 --> 00:21:30,270
audition that I'm working with we

00:21:27,030 --> 00:21:33,299
believe to build an open environment so

00:21:30,270 --> 00:21:41,059
all our research to all our code is open

00:21:33,299 --> 00:21:43,830
source and we believe in open by default

00:21:41,059 --> 00:21:45,990
ideology so we've been trying to make

00:21:43,830 --> 00:21:48,090
all this research kind of open and the

00:21:45,990 --> 00:21:51,850
things that we found out that we are

00:21:48,090 --> 00:21:54,280
lacking is more like

00:21:51,850 --> 00:21:57,480
from colleges or spoons make

00:21:54,280 --> 00:21:59,680
reproducibility is one that and

00:21:57,480 --> 00:22:03,010
understandability and collaborate with

00:21:59,680 --> 00:22:04,600
this so - for reproducibility what we're

00:22:03,010 --> 00:22:07,090
trying to do is have all the data

00:22:04,600 --> 00:22:10,990
sources at least on the Putin which is

00:22:07,090 --> 00:22:13,800
very common SEC in the tech area but not

00:22:10,990 --> 00:22:16,750
so common in the research area and

00:22:13,800 --> 00:22:22,090
understandability is documentation so

00:22:16,750 --> 00:22:26,380
ipython notebooks and for collaborative

00:22:22,090 --> 00:22:28,810
nests is we use github so issues

00:22:26,380 --> 00:22:31,330
discussions everything goes together

00:22:28,810 --> 00:22:34,600
this is how we're trying to build a

00:22:31,330 --> 00:22:36,400
culture for open research and the plus

00:22:34,600 --> 00:22:37,990
pointer that says the research need not

00:22:36,400 --> 00:22:48,040
be complete to be published

00:22:37,990 --> 00:22:51,820
it's monic ideas need to be shared the

00:22:48,040 --> 00:22:54,460
contributions from most people present

00:22:51,820 --> 00:22:57,070
so you can kind of help us generate more

00:22:54,460 --> 00:22:58,840
open data if you pick up the algorithms

00:22:57,070 --> 00:23:01,060
that we have or the Riperton notebooks

00:22:58,840 --> 00:23:04,030
that we have and you can try it out with

00:23:01,060 --> 00:23:05,800
your own data and in case you kind of

00:23:04,030 --> 00:23:12,170
have problems doing that you can always

00:23:05,800 --> 00:23:15,200
reach out us on it have genders of male

00:23:12,170 --> 00:23:18,620
not writing and help us improve our

00:23:15,200 --> 00:23:22,850
algorithms and code base we need a lot

00:23:18,620 --> 00:23:24,470
of help on that so it's mainly because

00:23:22,850 --> 00:23:27,440
we there's a lot of work to be done

00:23:24,470 --> 00:23:32,750
because even with we have one of the

00:23:27,440 --> 00:23:37,220
best sources of PDFs we only have let's

00:23:32,750 --> 00:23:38,780
say five states and a lot of hereand I

00:23:37,220 --> 00:23:43,309
agree I mean most of the union

00:23:38,780 --> 00:23:47,559
territories and 50 or so but even with

00:23:43,309 --> 00:23:47,559
that we have around 7000 to 8000 series

00:23:47,650 --> 00:23:55,400
and there's a lot more to be passed and

00:23:52,840 --> 00:23:58,280
so even if you are interested you can do

00:23:55,400 --> 00:24:02,419
studies like let's say how senses and

00:23:58,280 --> 00:24:04,549
but it is comparing maybe chai mark I

00:24:02,419 --> 00:24:07,309
will be and what investments are the

00:24:04,549 --> 00:24:09,169
government is proposing during this year

00:24:07,309 --> 00:24:10,790
and how it has been proposed in the last

00:24:09,169 --> 00:24:13,940
few years so you can do all those

00:24:10,790 --> 00:24:18,890
analysis also and we can help you with

00:24:13,940 --> 00:24:20,870
that or you could just simply help us by

00:24:18,890 --> 00:24:23,840
giving pointers or big bags or new ideas

00:24:20,870 --> 00:24:26,380
what our designs methodology is or the

00:24:23,840 --> 00:24:26,380
code itself

00:24:29,559 --> 00:24:33,340
yeah open for questions

00:24:39,350 --> 00:24:43,010
so how do you measure the accuracy like

00:24:41,840 --> 00:24:45,590
now because these are different

00:24:43,010 --> 00:24:47,090
documents it is not that you have so

00:24:45,590 --> 00:24:48,590
many of the same format that you can

00:24:47,090 --> 00:24:50,450
really take into it but it is going to

00:24:48,590 --> 00:24:52,610
be very difficult to my job rather than

00:24:50,450 --> 00:24:54,290
my the whole process working when I am

00:24:52,610 --> 00:24:57,260
exerting the right word is right yeah

00:24:54,290 --> 00:24:58,490
how do you approach so currently we are

00:24:57,260 --> 00:25:01,550
doing it finally we have a validation

00:24:58,490 --> 00:25:04,760
team that sits and does it but ideally

00:25:01,550 --> 00:25:07,850
there is a paper on how to do validation

00:25:04,760 --> 00:25:11,020
on layout analysis and what kind of

00:25:07,850 --> 00:25:11,020
patterns they should because I drink

00:25:19,280 --> 00:25:24,830
hi my name is Jorge I will I'm part of a

00:25:23,360 --> 00:25:26,480
company that builds automation for

00:25:24,830 --> 00:25:28,550
enterprise companies we're called

00:25:26,480 --> 00:25:30,530
Sirocco so we actually deal with this

00:25:28,550 --> 00:25:33,050
problem a lot so first off I want to

00:25:30,530 --> 00:25:33,980
thank you for presenting on it I think

00:25:33,050 --> 00:25:36,290
what a lot of people probably don't

00:25:33,980 --> 00:25:38,360
appreciate here without having worked on

00:25:36,290 --> 00:25:39,950
this is what comes from the previous

00:25:38,360 --> 00:25:42,350
question as well which is how important

00:25:39,950 --> 00:25:44,720
being correct is so if you're handling

00:25:42,350 --> 00:25:46,370
an insurance document and you get one

00:25:44,720 --> 00:25:50,270
number wrong I mean you're really hosed

00:25:46,370 --> 00:25:52,130
right yeah and so so it's actually very

00:25:50,270 --> 00:25:53,810
important and I just want to you know

00:25:52,130 --> 00:25:55,010
bring up oh one other thing and then ask

00:25:53,810 --> 00:25:56,630
you a question so some of the other

00:25:55,010 --> 00:25:58,100
things that we run into a lot when

00:25:56,630 --> 00:26:01,580
trying to deal with documents like this

00:25:58,100 --> 00:26:03,800
our tables inside of tables right and

00:26:01,580 --> 00:26:06,160
that constantly throws us off and

00:26:03,800 --> 00:26:09,260
finally if you have any thoughts on

00:26:06,160 --> 00:26:11,000
sometimes things are laid out as if it's

00:26:09,260 --> 00:26:12,860
a table like at the top of a document

00:26:11,000 --> 00:26:14,720
you might have an address in your name

00:26:12,860 --> 00:26:16,520
and then a phone number it looks like a

00:26:14,720 --> 00:26:19,040
table it's kind of implicitly a table

00:26:16,520 --> 00:26:22,040
but it's not in the sense that it's not

00:26:19,040 --> 00:26:23,510
labeled like there's an e next to it so

00:26:22,040 --> 00:26:24,890
just your thoughts on that as well

00:26:23,510 --> 00:26:28,880
Thanks yeah

00:26:24,890 --> 00:26:30,800
so that's the first part of it well

00:26:28,880 --> 00:26:34,520
table inside tables the only way I can

00:26:30,800 --> 00:26:37,660
think of as initially building rules to

00:26:34,520 --> 00:26:38,900
detect Authority kind of create a

00:26:37,660 --> 00:26:41,600
raising

00:26:38,900 --> 00:26:43,010
table number or a table index that's how

00:26:41,600 --> 00:26:45,050
we have been taught to doing it so the

00:26:43,010 --> 00:26:46,910
way we have it is in one page we kind of

00:26:45,050 --> 00:26:51,500
have sometimes two or three tables and

00:26:46,910 --> 00:26:53,630
the way we handled is just half table

00:26:51,500 --> 00:26:55,970
indexes and there would be cases where

00:26:53,630 --> 00:26:58,250
the outer table is supposed to be a

00:26:55,970 --> 00:27:02,180
higher level dropping also which is also

00:26:58,250 --> 00:27:05,360
another case that we handle is we take

00:27:02,180 --> 00:27:07,400
the margins of how far it is and we kind

00:27:05,360 --> 00:27:09,380
of once we know that it is more was

00:27:07,400 --> 00:27:11,450
probably dropping

00:27:09,380 --> 00:27:13,279
once you know that it's easy to kind of

00:27:11,450 --> 00:27:22,429
figure out these Donna what is this is

00:27:13,279 --> 00:27:23,179
from the left side I hope that tells ya

00:27:22,429 --> 00:27:25,669
hi

00:27:23,179 --> 00:27:28,279
first of all great talk I mean partly

00:27:25,669 --> 00:27:29,860
because tables contain huge amounts of

00:27:28,279 --> 00:27:32,419
information that we need to extract and

00:27:29,860 --> 00:27:34,549
the thing is the PDFs are necessarily

00:27:32,419 --> 00:27:37,100
evil I mean difficult to extract

00:27:34,549 --> 00:27:38,149
information from them my question is how

00:27:37,100 --> 00:27:40,580
are you

00:27:38,149 --> 00:27:44,929
exactly Tec ting the borders for the

00:27:40,580 --> 00:27:47,679
unbothered table as in you don't so ok

00:27:44,929 --> 00:27:50,000
the way we do it is for our case is

00:27:47,679 --> 00:27:52,370
tables are generally separated by titles

00:27:50,000 --> 00:27:54,139
that's a pattern that we would see ok

00:27:52,370 --> 00:27:57,649
case in budget documents they need to

00:27:54,139 --> 00:28:00,769
specify what budget code which is under

00:27:57,649 --> 00:28:03,470
which category that below information

00:28:00,769 --> 00:28:07,129
lies correct so we detect those titles

00:28:03,470 --> 00:28:09,200
as table endings and stockings so if it

00:28:07,129 --> 00:28:11,240
weren't for budgets then if it was some

00:28:09,200 --> 00:28:13,129
other kind of table then it might fail

00:28:11,240 --> 00:28:15,620
in that yes so these rules are very

00:28:13,129 --> 00:28:18,679
specific again by the channel that's why

00:28:15,620 --> 00:28:21,649
let's say encoding text might help to

00:28:18,679 --> 00:28:23,659
use it in other formats but I mean we

00:28:21,649 --> 00:28:26,259
need to delete the data to kind of build

00:28:23,659 --> 00:28:26,259
models on that

00:28:33,890 --> 00:28:41,059
hi so you mentioned and maybe you

00:28:38,360 --> 00:28:42,830
already saw this but you mentioned with

00:28:41,059 --> 00:28:44,390
respect to Gujarat's data I think it's

00:28:42,830 --> 00:28:47,720
Mitel menu and stuff

00:28:44,390 --> 00:28:50,840
it's are you concerned about the actual

00:28:47,720 --> 00:28:53,950
text itself or do you don't do not care

00:28:50,840 --> 00:28:56,750
about the country just go for the layout

00:28:53,950 --> 00:29:00,140
the content is so how to transmit rate

00:28:56,750 --> 00:29:04,399
and stuff so being in technology the

00:29:00,140 --> 00:29:07,220
outcomes not more important to me to us

00:29:04,399 --> 00:29:09,260
it is getting it connect and is very

00:29:07,220 --> 00:29:15,380
very important because you might just if

00:29:09,260 --> 00:29:17,899
you kind of miss a row and it can mess

00:29:15,380 --> 00:29:20,470
up an analysis because that's what we're

00:29:17,899 --> 00:29:20,470
aiming for

00:29:20,559 --> 00:29:26,929
it's that's what we have a validation

00:29:22,850 --> 00:29:30,019
team that does that for us and as far as

00:29:26,929 --> 00:29:32,059
multilingual tea is concerned that is an

00:29:30,019 --> 00:29:35,299
experiment that I have to start so

00:29:32,059 --> 00:29:41,480
anyone is kind of welcome to join us and

00:29:35,299 --> 00:29:43,220
that will push everything so the way we

00:29:41,480 --> 00:29:46,610
are planning to handle with multilingual

00:29:43,220 --> 00:29:51,649
system is most of the documents the

00:29:46,610 --> 00:29:53,360
other language is mostly Hindi and maybe

00:29:51,649 --> 00:29:55,100
we can build a corpus around because

00:29:53,360 --> 00:29:57,799
there are documents that has English and

00:29:55,100 --> 00:29:59,960
Hindi both and the documents that have

00:29:57,799 --> 00:30:02,000
sustained the alone so if we can build a

00:29:59,960 --> 00:30:04,850
corpus around English to maybe push the

00:30:02,000 --> 00:30:10,090
translation it's very far fetched ago I

00:30:04,850 --> 00:30:10,090
would say but yeah coming years

00:30:12,409 --> 00:30:18,559
who's the end consumer and how do they

00:30:15,019 --> 00:30:19,399
use the output of your that's a very

00:30:18,559 --> 00:30:22,960
good question

00:30:19,399 --> 00:30:29,210
so the end users for this particular

00:30:22,960 --> 00:30:33,350
tool is technologists who want kind of

00:30:29,210 --> 00:30:35,480
export budget but the users of the CSV

00:30:33,350 --> 00:30:38,440
is our budget researchers who

00:30:35,480 --> 00:30:42,100
specifically work on analyzing budget

00:30:38,440 --> 00:30:42,100
it's part of the jobs

00:30:42,109 --> 00:30:52,340
other than we are building tools where

00:30:47,149 --> 00:30:55,039
the people normal citizen can also kind

00:30:52,340 --> 00:30:56,450
of get there and search for let's say

00:30:55,039 --> 00:31:00,080
hospitals and they'll get information

00:30:56,450 --> 00:31:03,830
for hospitals across States so those

00:31:00,080 --> 00:31:06,379
tools are kind of next year's next

00:31:03,830 --> 00:31:13,369
year's plan so right now we have

00:31:06,379 --> 00:31:15,980
something called but we have something

00:31:13,369 --> 00:31:17,929
called you can go to open budgets India

00:31:15,980 --> 00:31:23,210
dot all and you have a stories and later

00:31:17,929 --> 00:31:26,119
at the top corner they does the

00:31:23,210 --> 00:31:28,219
information that we have been sensitized

00:31:26,119 --> 00:31:33,319
easy to consume so you can check that

00:31:28,219 --> 00:31:36,109
out okay hello

00:31:33,319 --> 00:31:38,329
it's thanks for the dog so now there

00:31:36,109 --> 00:31:41,359
were few questions specifically a few

00:31:38,329 --> 00:31:44,119
cases where we need to extract data from

00:31:41,359 --> 00:31:47,119
tables the first one that many times

00:31:44,119 --> 00:31:50,299
your row span multiple rows and your

00:31:47,119 --> 00:31:52,639
tables fan multiple pages and another

00:31:50,299 --> 00:31:56,019
question is most of the time

00:31:52,639 --> 00:31:56,019

YouTube URL: https://www.youtube.com/watch?v=nVrNt97Bv9Y


