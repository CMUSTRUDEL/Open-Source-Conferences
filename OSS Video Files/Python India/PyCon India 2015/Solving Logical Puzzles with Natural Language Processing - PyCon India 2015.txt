Title: Solving Logical Puzzles with Natural Language Processing - PyCon India 2015
Publication date: 2015-10-11
Playlist: PyCon India 2015
Description: 
	Speaker: Ashutosh Trivedi

Ever Imagined if your algorithm is able to solve the classic Odd Man Out problems for you, or solving Analogy problems? Well, thats not impossible provided you have large data. Consider how our brain solve these problems- If we have not seen the words asked in the puzzle we can not solve such problems. We need contextual information about the words. Also, if you notice carefully both the problems are somewhat similar and only require your memory or precisely how you store contextual information.

Details: https://in.pycon.org/cfp/pycon-india-2015/proposals/a-deep-dive-into-natural-language-processing-and-web-information-retrieval/
Captions: 
	00:00:00,000 --> 00:00:05,819
good morning everyone so am I audible at

00:00:03,480 --> 00:00:09,990
the back and not too loud not too low

00:00:05,819 --> 00:00:13,410
it's cool okay so my name is a sheet or

00:00:09,990 --> 00:00:15,360
Zebedee and today's talk is solving

00:00:13,410 --> 00:00:17,100
logical results with natural language

00:00:15,360 --> 00:00:21,210
processing but before that I want to

00:00:17,100 --> 00:00:23,640
share story back in 2013 when I was a

00:00:21,210 --> 00:00:26,699
grad student at Triple A D bangalore I

00:00:23,640 --> 00:00:29,130
was I didn't know about pycon at all and

00:00:26,699 --> 00:00:32,369
I actually didn't know about open source

00:00:29,130 --> 00:00:34,260
at all so by coming here there were few

00:00:32,369 --> 00:00:36,840
of my friends said there is a conference

00:00:34,260 --> 00:00:39,120
called PyCon and I said why there is a

00:00:36,840 --> 00:00:41,910
conference for a language is it really

00:00:39,120 --> 00:00:44,700
required so when I came here and I saw

00:00:41,910 --> 00:00:48,870
that how a community drives the language

00:00:44,700 --> 00:00:52,410
and for betterment of all the language

00:00:48,870 --> 00:00:55,230
then I got to know that yeah this is

00:00:52,410 --> 00:00:56,640
something even I wanted to go and I got

00:00:55,230 --> 00:00:58,500
involved with few of open source

00:00:56,640 --> 00:01:01,230
projects and it was a great learning

00:00:58,500 --> 00:01:04,019
experience so I just want to give a

00:01:01,230 --> 00:01:06,960
round of applause to all those all those

00:01:04,019 --> 00:01:12,409
developers out there and and awesome

00:01:06,960 --> 00:01:15,930
conference like this so yeah thank you

00:01:12,409 --> 00:01:18,689
so before I start the talk I want to

00:01:15,930 --> 00:01:21,630
just beef give a brief introduction

00:01:18,689 --> 00:01:24,240
about what this talk is about and what

00:01:21,630 --> 00:01:26,340
this talk is not about because in

00:01:24,240 --> 00:01:30,299
natural language processing it's

00:01:26,340 --> 00:01:32,100
actually a very heavy research topic and

00:01:30,299 --> 00:01:34,979
there is lots of new research coming

00:01:32,100 --> 00:01:37,500
along every day so all the things which

00:01:34,979 --> 00:01:40,530
I will be talking about natural language

00:01:37,500 --> 00:01:43,950
processing are the things which were in

00:01:40,530 --> 00:01:47,009
research papers just either one year

00:01:43,950 --> 00:01:49,049
back or this year so things are pretty

00:01:47,009 --> 00:01:51,420
fresh and pretty new and people are

00:01:49,049 --> 00:01:53,759
still experimenting with it nothing has

00:01:51,420 --> 00:01:59,430
been went into production till now so

00:01:53,759 --> 00:02:00,780
and since it involves so there is a club

00:01:59,430 --> 00:02:02,909
between machine learning deep learning

00:02:00,780 --> 00:02:06,090
and natural language processing which is

00:02:02,909 --> 00:02:08,640
the current research status so i won't

00:02:06,090 --> 00:02:10,459
go into mathematical details the talk

00:02:08,640 --> 00:02:12,810
will actually motivate you to start

00:02:10,459 --> 00:02:13,710
working on it start exploring these

00:02:12,810 --> 00:02:16,760
things

00:02:13,710 --> 00:02:20,550
and do some cool stuff around it so I

00:02:16,760 --> 00:02:21,840
start let me just have an idea of the

00:02:20,550 --> 00:02:24,740
crowd how many people are actually

00:02:21,840 --> 00:02:30,060
working with natural language processing

00:02:24,740 --> 00:02:33,390
machine learning deep learning and if

00:02:30,060 --> 00:02:37,080
you okay good so I'll be going through

00:02:33,390 --> 00:02:39,120
basics and if you have any problem will

00:02:37,080 --> 00:02:40,650
be taking questions at the end and if

00:02:39,120 --> 00:02:51,990
you want to do math I'll be outside

00:02:40,650 --> 00:02:53,910
we'll sit and do math okay so so I start

00:02:51,990 --> 00:02:55,920
with what is actually natural language

00:02:53,910 --> 00:02:59,790
processing and how does it start it so

00:02:55,920 --> 00:03:04,080
when when we so language was always

00:02:59,790 --> 00:03:05,820
there with us why is not okay so

00:03:04,080 --> 00:03:07,560
language was always there with us as

00:03:05,820 --> 00:03:10,050
soon as we got computers we wanted to

00:03:07,560 --> 00:03:11,700
understand language programatically why

00:03:10,050 --> 00:03:15,180
it's a very obvious answer because

00:03:11,700 --> 00:03:17,370
computer needs an interface and what is

00:03:15,180 --> 00:03:21,210
the best interface rather than our own

00:03:17,370 --> 00:03:25,410
language right so moving forward we

00:03:21,210 --> 00:03:26,370
built something which was there that how

00:03:25,410 --> 00:03:28,170
can we understand language

00:03:26,370 --> 00:03:30,000
programmatically so that we can do

00:03:28,170 --> 00:03:33,570
automation and we can have computers do

00:03:30,000 --> 00:03:35,550
stuff around natural language so these

00:03:33,570 --> 00:03:38,910
were the things which people who are

00:03:35,550 --> 00:03:41,430
doing and which is there right now so

00:03:38,910 --> 00:03:44,700
what we can do with natural language

00:03:41,430 --> 00:03:47,850
since our grammar has a rule so we can

00:03:44,700 --> 00:03:50,580
actually follow those rules and say we

00:03:47,850 --> 00:03:54,210
can Cal we can we can compute part of

00:03:50,580 --> 00:03:57,060
speech tags for each word if so I am

00:03:54,210 --> 00:03:59,160
taking English language as the basic and

00:03:57,060 --> 00:04:00,540
all these talk will be focused on that

00:03:59,160 --> 00:04:02,070
and there will be different rules and

00:04:00,540 --> 00:04:03,660
different set of things around different

00:04:02,070 --> 00:04:07,890
language say German and/or Sanskrit

00:04:03,660 --> 00:04:09,720
which are heavily grammar oriented so so

00:04:07,890 --> 00:04:12,180
we know our part of speech we also know

00:04:09,720 --> 00:04:15,570
our vocabulary our vocabulary is growing

00:04:12,180 --> 00:04:18,770
every day but it's still we can store

00:04:15,570 --> 00:04:21,180
all of that and Engram is a thing which

00:04:18,770 --> 00:04:25,530
which we can apply on any language

00:04:21,180 --> 00:04:27,270
Engram can be a diagram diagram or more

00:04:25,530 --> 00:04:29,550
than that so it won't

00:04:27,270 --> 00:04:31,379
consider a word as a word every word

00:04:29,550 --> 00:04:35,250
will be broken into if it's a bigram

00:04:31,379 --> 00:04:38,250
then to word to character words will be

00:04:35,250 --> 00:04:42,479
by grams across the sentences and words

00:04:38,250 --> 00:04:44,639
style is capitalization or non caps all

00:04:42,479 --> 00:04:47,129
gaps these kind of things and now

00:04:44,639 --> 00:04:49,710
detection and we this is a complicated

00:04:47,129 --> 00:04:51,870
problem but still we can detect nouns so

00:04:49,710 --> 00:04:55,169
these are the things you are doing to

00:04:51,870 --> 00:04:58,800
identify that how we can do some thing

00:04:55,169 --> 00:05:01,500
around natural language so given a

00:04:58,800 --> 00:05:03,720
sentence can you process that sentence

00:05:01,500 --> 00:05:05,370
and how can you process that sentence so

00:05:03,720 --> 00:05:07,590
that you can get some information out of

00:05:05,370 --> 00:05:09,449
it and if I am talking about information

00:05:07,590 --> 00:05:11,849
what computers can do they can change

00:05:09,449 --> 00:05:13,169
numbers so every information which is

00:05:11,849 --> 00:05:15,810
which is there which is called

00:05:13,169 --> 00:05:19,050
unstructured data which is text image

00:05:15,810 --> 00:05:21,569
video so as this talk is focus on text

00:05:19,050 --> 00:05:23,130
what you can do with text and how can

00:05:21,569 --> 00:05:27,419
you convert it to numbers so that you

00:05:23,130 --> 00:05:30,419
can process it right so let's let's

00:05:27,419 --> 00:05:33,300
let's go sentence if there is a sentence

00:05:30,419 --> 00:05:36,810
let's go word by word so currently what

00:05:33,300 --> 00:05:39,509
is state-of-the-art two years ago was

00:05:36,810 --> 00:05:42,150
researchers was actually doing this they

00:05:39,509 --> 00:05:43,800
were calculated so if you go word by

00:05:42,150 --> 00:05:46,440
word you have current word you have

00:05:43,800 --> 00:05:48,590
previous or next word correct word and

00:05:46,440 --> 00:05:52,229
ground previous word and Gramps and

00:05:48,590 --> 00:05:55,259
pasta X surrounding bostic's word shapes

00:05:52,229 --> 00:05:57,539
surrounding word shape and anything

00:05:55,259 --> 00:06:00,449
around it so there are lots of things

00:05:57,539 --> 00:06:02,819
you can do with it and you can actually

00:06:00,449 --> 00:06:06,330
convert everything into numbers so by

00:06:02,819 --> 00:06:08,550
numbers I mean indexes so a current word

00:06:06,330 --> 00:06:10,529
is an index in your vocabulary similarly

00:06:08,550 --> 00:06:12,930
your next word even a post AG you can

00:06:10,529 --> 00:06:16,110
you can create an index so it's it's all

00:06:12,930 --> 00:06:18,719
getting into that and this all will

00:06:16,110 --> 00:06:20,969
generate some information and that

00:06:18,719 --> 00:06:23,250
information is used to understand that

00:06:20,969 --> 00:06:24,870
sentence so if a sentence has ten words

00:06:23,250 --> 00:06:26,639
for each word will calculate this much

00:06:24,870 --> 00:06:30,539
information create a feature out of it

00:06:26,639 --> 00:06:33,440
and do something so if i say feature

00:06:30,539 --> 00:06:36,210
let's let's say there is a sentence and

00:06:33,440 --> 00:06:37,860
it has all these features so we

00:06:36,210 --> 00:06:40,320
calculated all these features and they

00:06:37,860 --> 00:06:40,900
are in numbers so what we can do with

00:06:40,320 --> 00:06:43,990
that

00:06:40,900 --> 00:06:46,389
if I if I go forward with machine

00:06:43,990 --> 00:06:47,979
learning model so let's say I just want

00:06:46,389 --> 00:06:51,669
to know what is the sentiment of that

00:06:47,979 --> 00:06:53,410
sentence so for that I need to tag few

00:06:51,669 --> 00:06:57,460
sentences before so that we can learn

00:06:53,410 --> 00:07:01,780
right so suppose sentence 1 will

00:06:57,460 --> 00:07:05,320
calculate that this is the vector if I

00:07:01,780 --> 00:07:07,479
if I have n features so it will be a

00:07:05,320 --> 00:07:11,259
dimensional vector and that n

00:07:07,479 --> 00:07:13,120
dimensional vector has a tab ok and we

00:07:11,259 --> 00:07:15,760
have some weights associated with each

00:07:13,120 --> 00:07:18,460
feature so every feature has some

00:07:15,760 --> 00:07:21,550
importance to identify the tag so as

00:07:18,460 --> 00:07:24,880
soon as we we got exposed to more and

00:07:21,550 --> 00:07:26,650
more training data these weights will be

00:07:24,880 --> 00:07:28,830
adjusted and this way it will be

00:07:26,650 --> 00:07:31,690
actually the weight of hyperplane

00:07:28,830 --> 00:07:35,070
separating all those separating these

00:07:31,690 --> 00:07:37,539
two classes so with each with each

00:07:35,070 --> 00:07:39,930
training set that hyper plane will

00:07:37,539 --> 00:07:43,690
adjust and try to fit the model so that

00:07:39,930 --> 00:07:45,460
the negative ones will be at one at one

00:07:43,690 --> 00:07:47,229
side of the hyperplane and positive ones

00:07:45,460 --> 00:07:49,180
will be at the ones other side of hyper

00:07:47,229 --> 00:07:53,130
plane as soon as you finish training

00:07:49,180 --> 00:07:55,419
your model once you get the sentence a

00:07:53,130 --> 00:07:57,389
new sentence you again calculate the

00:07:55,419 --> 00:08:00,520
features just plug in those weights and

00:07:57,389 --> 00:08:02,289
your hyper plane will tell you that it

00:08:00,520 --> 00:08:03,789
falls into positive category or or a

00:08:02,289 --> 00:08:06,729
negative category so it's a very

00:08:03,789 --> 00:08:08,229
simplistic way of explaining a machine

00:08:06,729 --> 00:08:10,150
learning algorithm how it works and how

00:08:08,229 --> 00:08:14,830
it will it will work on as to

00:08:10,150 --> 00:08:17,110
unstructured data so what are your

00:08:14,830 --> 00:08:19,300
thoughts on that what we are actually

00:08:17,110 --> 00:08:21,430
doing is through natural language

00:08:19,300 --> 00:08:24,400
processing and our set of rule with lots

00:08:21,430 --> 00:08:27,909
of processing we calculate data features

00:08:24,400 --> 00:08:29,530
and through machine learning we just can

00:08:27,909 --> 00:08:32,680
we just optimize that weight we just

00:08:29,530 --> 00:08:36,070
learn that hyper plane right so these

00:08:32,680 --> 00:08:39,400
two fields if they combine you can

00:08:36,070 --> 00:08:42,370
understand your data better but there is

00:08:39,400 --> 00:08:44,320
a problem is it scalable it's not at all

00:08:42,370 --> 00:08:47,020
scalable there will be new languages and

00:08:44,320 --> 00:08:50,079
our language vocabulary will increase

00:08:47,020 --> 00:08:51,959
and we are talking about AI here machine

00:08:50,079 --> 00:08:54,220
learning is is one of the field which is

00:08:51,959 --> 00:08:59,170
right now leading in AI

00:08:54,220 --> 00:09:02,259
it's one of 11 school of thought so what

00:08:59,170 --> 00:09:04,389
is wrong with that the wrong thing is we

00:09:02,259 --> 00:09:06,850
are doing lots of processing and that

00:09:04,389 --> 00:09:09,490
processes are actually dumb processes so

00:09:06,850 --> 00:09:11,680
if you are calculating part of speech

00:09:09,490 --> 00:09:14,829
tag for a sentence it's a process it's a

00:09:11,680 --> 00:09:16,509
standard program which run and which

00:09:14,829 --> 00:09:19,480
will fail if you have any non standard

00:09:16,509 --> 00:09:22,360
thing similarly other features suppose I

00:09:19,480 --> 00:09:24,129
am calculating 50 features and and if

00:09:22,360 --> 00:09:26,490
you if you assume every feature is a

00:09:24,129 --> 00:09:29,529
process we have 50 processes running

00:09:26,490 --> 00:09:31,480
generating some data feeding it to an

00:09:29,529 --> 00:09:35,079
optimization algorithm which will learn

00:09:31,480 --> 00:09:38,319
the your hyper plane so it's it's not at

00:09:35,079 --> 00:09:39,939
all scalable plus it's not iterative as

00:09:38,319 --> 00:09:43,689
soon as you have new data you have to

00:09:39,939 --> 00:09:44,949
again do the same thing so ah so

00:09:43,689 --> 00:09:46,779
something is wrong with that and

00:09:44,949 --> 00:09:48,610
researchers also find out that yeah

00:09:46,779 --> 00:09:51,100
something it's going wrong and we are

00:09:48,610 --> 00:09:53,800
not scaling up in NLP so whatever the

00:09:51,100 --> 00:09:55,990
research was there for last 10 years was

00:09:53,800 --> 00:09:58,809
actually superseded by last two years

00:09:55,990 --> 00:10:00,490
research which was introduction of deep

00:09:58,809 --> 00:10:04,420
learning into natural language

00:10:00,490 --> 00:10:06,309
processing so we hit the basic first how

00:10:04,420 --> 00:10:10,420
do we represent a word so when I said

00:10:06,309 --> 00:10:12,220
we'll process a word and if you process

00:10:10,420 --> 00:10:14,620
if you process a sentence and I say we

00:10:12,220 --> 00:10:17,980
will process it word by word so there is

00:10:14,620 --> 00:10:19,750
an eye in line problem with that our

00:10:17,980 --> 00:10:22,089
word representation are actually very

00:10:19,750 --> 00:10:24,939
weak so how do we represent a word

00:10:22,089 --> 00:10:28,149
suppose i say i will build indexes so if

00:10:24,939 --> 00:10:30,370
we have a vocabulary and if there is a

00:10:28,149 --> 00:10:33,790
word which is called nice and it it

00:10:30,370 --> 00:10:36,189
comes as a 30th index so we will create

00:10:33,790 --> 00:10:39,279
a one hot vector so this vector is

00:10:36,189 --> 00:10:41,800
called one hot it's it's zero zero zero

00:10:39,279 --> 00:10:44,410
and one at the place where where its

00:10:41,800 --> 00:10:46,720
index is so 38 index is one west are all

00:10:44,410 --> 00:10:49,720
as you so dimension of the vector is

00:10:46,720 --> 00:10:52,000
your size of the vocabulary so this is

00:10:49,720 --> 00:10:55,930
how you represent suppose word is nice

00:10:52,000 --> 00:10:59,170
which is at 30th place and word is good

00:10:55,930 --> 00:11:00,850
which is as 45th place so with this

00:10:59,170 --> 00:11:04,389
representation can we say these two are

00:11:00,850 --> 00:11:06,069
actually same we can now so the main

00:11:04,389 --> 00:11:07,990
problem is how do we represent the word

00:11:06,069 --> 00:11:10,660
or word representation does not

00:11:07,990 --> 00:11:13,149
any information about that word we

00:11:10,660 --> 00:11:15,430
calculated we calculate information

00:11:13,149 --> 00:11:19,290
about the world by the surrounding

00:11:15,430 --> 00:11:23,529
surrounding neighborhood words so if we

00:11:19,290 --> 00:11:25,570
if we improve our representation of word

00:11:23,529 --> 00:11:28,149
we'll be saving a lot of computation and

00:11:25,570 --> 00:11:31,450
we'll be doing some more analysis on top

00:11:28,149 --> 00:11:33,820
of that so there was an answer for that

00:11:31,450 --> 00:11:35,589
earlier and people people actually

00:11:33,820 --> 00:11:39,399
thought about it and they created

00:11:35,589 --> 00:11:41,770
Burnitz so what Nate's are a tree or a

00:11:39,399 --> 00:11:43,240
graph you can say and they can they

00:11:41,770 --> 00:11:46,240
actually capture the hierarchical

00:11:43,240 --> 00:11:49,540
information about all these all these

00:11:46,240 --> 00:11:52,120
words so so if you have awesome good

00:11:49,540 --> 00:11:54,220
nice okay it's a hierarchy and they are

00:11:52,120 --> 00:11:55,899
connected and their distance tells you

00:11:54,220 --> 00:11:59,230
that how good they are connected and

00:11:55,899 --> 00:12:01,420
they're announced also but there is a

00:11:59,230 --> 00:12:04,750
problem with one Nets tube it's it's a

00:12:01,420 --> 00:12:06,580
lot of manual labor by linguists so they

00:12:04,750 --> 00:12:08,350
created over the period of time and

00:12:06,580 --> 00:12:10,420
Princeton has highest wordnet right now

00:12:08,350 --> 00:12:13,690
you can access it and i will be showing

00:12:10,420 --> 00:12:16,510
python how we can do it with python so

00:12:13,690 --> 00:12:18,820
what nets has problem for it's actually

00:12:16,510 --> 00:12:21,820
a lot of manual labor scalability issues

00:12:18,820 --> 00:12:23,740
will be again there and as soon as you

00:12:21,820 --> 00:12:25,420
have new words how do we associate those

00:12:23,740 --> 00:12:26,829
new words to previous words again you

00:12:25,420 --> 00:12:28,240
have to do a lot of computation and

00:12:26,829 --> 00:12:31,180
there will be a lot of validation from

00:12:28,240 --> 00:12:35,110
linguists and they may not agree so if

00:12:31,180 --> 00:12:37,510
LOL law has to come into wordnet so how

00:12:35,110 --> 00:12:38,980
do you connect it and few people will

00:12:37,510 --> 00:12:41,920
not even agree that it's not even a word

00:12:38,980 --> 00:12:45,160
so the lots of things has to go into

00:12:41,920 --> 00:12:48,040
that which is language specific and the

00:12:45,160 --> 00:12:51,370
biggest problem is now we have we are

00:12:48,040 --> 00:12:53,800
generating nounce every day so how do

00:12:51,370 --> 00:12:56,050
you how do you incorporate all the nouns

00:12:53,800 --> 00:12:59,350
in that and how do you say that this

00:12:56,050 --> 00:13:02,890
noun is actually similar to that now so

00:12:59,350 --> 00:13:06,750
all these problems are there I wish I

00:13:02,890 --> 00:13:10,690
just give you a demo of burnetts through

00:13:06,750 --> 00:13:13,420
white n NT k is natural language

00:13:10,690 --> 00:13:16,329
processing library in Python through NLT

00:13:13,420 --> 00:13:18,730
k you can see that they have this

00:13:16,329 --> 00:13:21,070
wordnet saved into that in corpus and

00:13:18,730 --> 00:13:27,050
you can access those word nets

00:13:21,070 --> 00:13:30,260
okay so so so this is how we can we can

00:13:27,050 --> 00:13:32,120
we can access word nets and what Nets

00:13:30,260 --> 00:13:35,269
has lots of information and lots of

00:13:32,120 --> 00:13:36,980
rules in that so you can define is a

00:13:35,269 --> 00:13:39,680
relationship which is called hyper names

00:13:36,980 --> 00:13:47,779
so if i say so this demo shows that

00:13:39,680 --> 00:13:50,600
panda and up and okay so let's say so

00:13:47,779 --> 00:13:52,399
that that is not Rendon so let's say we

00:13:50,600 --> 00:13:54,470
want to say that how many synonyms are

00:13:52,399 --> 00:13:56,350
therefore nice and it will show you the

00:13:54,470 --> 00:13:59,870
hierarchy of all those synonyms

00:13:56,350 --> 00:14:02,630
similarly for these nouns also and it

00:13:59,870 --> 00:14:03,800
also tells you that what is the region

00:14:02,630 --> 00:14:05,540
and domain so there are lots of

00:14:03,800 --> 00:14:07,820
information in word nets you can get out

00:14:05,540 --> 00:14:10,970
of it so what is the a terminology of of

00:14:07,820 --> 00:14:12,620
a word and how it is connected to which

00:14:10,970 --> 00:14:16,550
region it is connected to so buckeyes

00:14:12,620 --> 00:14:18,649
and Hindi word so its region is India so

00:14:16,550 --> 00:14:24,699
these kinds of information you can take

00:14:18,649 --> 00:14:28,490
from ordinates what i can do is I'll

00:14:24,699 --> 00:14:33,310
switch to these demos at the end so that

00:14:28,490 --> 00:14:36,230
it will be straightforward so again

00:14:33,310 --> 00:14:38,240
let's let's get back to where we were so

00:14:36,230 --> 00:14:40,070
what Nate's we say that we have some

00:14:38,240 --> 00:14:41,720
problems with what net so let's let's

00:14:40,070 --> 00:14:45,079
think about how do we associate with the

00:14:41,720 --> 00:14:46,490
word right if I if I say a word there

00:14:45,079 --> 00:14:49,790
are lots of things in our human mind

00:14:46,490 --> 00:14:51,620
which goes inside it so we associate it

00:14:49,790 --> 00:14:54,110
if it's a now and we associate it with a

00:14:51,620 --> 00:14:57,290
person and we we create a context around

00:14:54,110 --> 00:14:59,750
it right and that context could be taste

00:14:57,290 --> 00:15:01,880
smell time visual or feelings so there

00:14:59,750 --> 00:15:03,800
are lots of hooks with a word which

00:15:01,880 --> 00:15:08,300
which gives us that ok this word

00:15:03,800 --> 00:15:10,519
represent this so what we can do

00:15:08,300 --> 00:15:12,980
programmatically so there is a code

00:15:10,519 --> 00:15:15,769
which says that you are an average human

00:15:12,980 --> 00:15:17,569
of the five humans around you and there

00:15:15,769 --> 00:15:19,250
are similar code in natural language

00:15:17,569 --> 00:15:21,610
processing which says that you shall

00:15:19,250 --> 00:15:26,240
know a word by the company it keeps so

00:15:21,610 --> 00:15:29,120
that that is for there to create context

00:15:26,240 --> 00:15:31,180
so how good how easy it is to create

00:15:29,120 --> 00:15:33,730
context we can actually create

00:15:31,180 --> 00:15:37,620
co-occurrences matrices

00:15:33,730 --> 00:15:42,430
and it's one of the most successful

00:15:37,620 --> 00:15:45,040
ideas of NLP so how do we calculate

00:15:42,430 --> 00:15:47,470
co-occurrence matrices is is this way so

00:15:45,040 --> 00:15:49,600
suppose we have a corpus and that corpus

00:15:47,470 --> 00:15:51,670
represent these three sentences which is

00:15:49,600 --> 00:15:56,740
I like deep learning I like an MP and I

00:15:51,670 --> 00:15:59,949
enjoy flying so in that if we create a

00:15:56,740 --> 00:16:02,800
matrix which has every single word from

00:15:59,949 --> 00:16:04,839
that so the dimension of the matrices

00:16:02,800 --> 00:16:07,209
again n cross and if you have n words

00:16:04,839 --> 00:16:09,220
and we are creating this co-occurrence

00:16:07,209 --> 00:16:11,980
matrix as one neighbor co-occurrence

00:16:09,220 --> 00:16:14,649
matrix that means every word which is

00:16:11,980 --> 00:16:17,290
just adjacent to another word will give

00:16:14,649 --> 00:16:21,029
a count one so and increase the count

00:16:17,290 --> 00:16:24,760
one so you can do that for higher

00:16:21,029 --> 00:16:29,800
context also so if that matrix is there

00:16:24,760 --> 00:16:34,029
and so I represent our complete corpus

00:16:29,800 --> 00:16:38,079
in that matrix again there is a problem

00:16:34,029 --> 00:16:39,970
with that yes we can do some analysis on

00:16:38,079 --> 00:16:44,410
that matrix again we have converted our

00:16:39,970 --> 00:16:47,769
corpus into a matrix a numbers but and

00:16:44,410 --> 00:16:49,600
we have represented all the all our text

00:16:47,769 --> 00:16:52,209
data into numbers which actually capture

00:16:49,600 --> 00:16:54,100
the context of the world again it is

00:16:52,209 --> 00:16:56,350
very high-dimensional if you have

00:16:54,100 --> 00:16:58,029
millions of if you have millions of

00:16:56,350 --> 00:17:00,370
words again it will be a million cross

00:16:58,029 --> 00:17:04,689
million matrix and you won't be able to

00:17:00,370 --> 00:17:07,179
do much processing on that and if we run

00:17:04,689 --> 00:17:09,040
and this matrix will be actually sparse

00:17:07,179 --> 00:17:10,900
so by sparse I mean there will be lots

00:17:09,040 --> 00:17:12,760
of zeros because not everybody is

00:17:10,900 --> 00:17:14,740
associated with every word and they

00:17:12,760 --> 00:17:16,689
don't come together so with lots of

00:17:14,740 --> 00:17:18,669
zeros if we run a classification

00:17:16,689 --> 00:17:21,010
algorithm there will be a problem I'll

00:17:18,669 --> 00:17:22,689
show a demo of co-occurrence matrix but

00:17:21,010 --> 00:17:27,040
at the last because I have to against

00:17:22,689 --> 00:17:29,740
which such back these things so what we

00:17:27,040 --> 00:17:31,390
can do with co-occurrence matrices is we

00:17:29,740 --> 00:17:34,210
can reduce the dimension of a matrix

00:17:31,390 --> 00:17:35,950
always right so if we reduce the

00:17:34,210 --> 00:17:40,419
dimension the matrix will be smaller and

00:17:35,950 --> 00:17:42,250
and it's always if you have a high

00:17:40,419 --> 00:17:44,290
dimensional data which which shows the

00:17:42,250 --> 00:17:45,610
dimension which we understand and if you

00:17:44,290 --> 00:17:46,630
reduce the dimension there will be some

00:17:45,610 --> 00:17:49,740
latent in

00:17:46,630 --> 00:17:52,180
mission there so a dimension can be a

00:17:49,740 --> 00:17:54,340
dimension of similarity so it's it's

00:17:52,180 --> 00:17:56,770
quite abstract and we don't we cannot

00:17:54,340 --> 00:17:59,320
understand which dimension it is but we

00:17:56,770 --> 00:18:01,420
can reduce the dimension using SVD so

00:17:59,320 --> 00:18:04,480
SVD singular value decomposition how

00:18:01,420 --> 00:18:06,940
many people know here SVD okay great I

00:18:04,480 --> 00:18:08,830
won't go into math of that so SVD is a

00:18:06,940 --> 00:18:12,160
method by which you can actually reduce

00:18:08,830 --> 00:18:13,900
your matrix into a smaller matrix and if

00:18:12,160 --> 00:18:16,150
we'll be having low dimension and those

00:18:13,900 --> 00:18:20,260
dimensions will be representing the most

00:18:16,150 --> 00:18:23,020
important features of of that matrix so

00:18:20,260 --> 00:18:26,920
any matrix you can actually represent in

00:18:23,020 --> 00:18:28,570
it into these three matrices and you can

00:18:26,920 --> 00:18:30,340
reduce the dimension I won't go into

00:18:28,570 --> 00:18:33,430
math and this mo I will show you later

00:18:30,340 --> 00:18:35,170
how we can calculate SVD through Python

00:18:33,430 --> 00:18:40,090
and how we can represent the word in low

00:18:35,170 --> 00:18:42,670
dimensional space so again we have

00:18:40,090 --> 00:18:45,040
another problem with this video so the

00:18:42,670 --> 00:18:46,870
problem with SVD is calculating the SVD

00:18:45,040 --> 00:18:50,920
on that high dimensional vector high

00:18:46,870 --> 00:18:53,950
dimensional matrix and if you have again

00:18:50,920 --> 00:18:55,840
billion words converting into million

00:18:53,950 --> 00:18:57,970
words will again not solve problems

00:18:55,840 --> 00:18:59,410
right so billion by billion matrix if

00:18:57,970 --> 00:19:00,940
you convert it to million by million

00:18:59,410 --> 00:19:04,060
matrix it's still million by million

00:19:00,940 --> 00:19:06,850
matrix and you cannot do away with that

00:19:04,060 --> 00:19:09,550
so again inherent problem with SPD's

00:19:06,850 --> 00:19:12,040
calculations and iterative and s again

00:19:09,550 --> 00:19:14,410
if i have a new sentence in my corpus i

00:19:12,040 --> 00:19:19,300
have to calculate the SVD of the whole

00:19:14,410 --> 00:19:21,640
corpus again and then do this stuff so

00:19:19,300 --> 00:19:24,460
this is where last two year research has

00:19:21,640 --> 00:19:27,460
contributed this is the algorithm called

00:19:24,460 --> 00:19:29,470
word to whack it's by Google and by

00:19:27,460 --> 00:19:31,950
Thomas michelob and Jeff Pennington and

00:19:29,470 --> 00:19:35,530
there is another research happened in

00:19:31,950 --> 00:19:38,230
Stanford by Richard Scioscia and yo

00:19:35,530 --> 00:19:40,270
shaban tube which is the which are very

00:19:38,230 --> 00:19:42,670
famous guys in NLP so what they have

00:19:40,270 --> 00:19:45,010
done is instead of V calculating the

00:19:42,670 --> 00:19:47,680
lower dimension matrix why don't we

00:19:45,010 --> 00:19:49,270
learn the lower dimension matrix as it

00:19:47,680 --> 00:19:51,610
is we do not go into high dimensional

00:19:49,270 --> 00:19:54,670
matrix at all and we learn the low

00:19:51,610 --> 00:19:56,770
dimension matrices directly so they

00:19:54,670 --> 00:19:59,120
calculated they they devised this

00:19:56,770 --> 00:20:02,270
algorithm called word to work or glove

00:19:59,120 --> 00:20:04,400
global vector of firts it's actually

00:20:02,270 --> 00:20:07,250
very fast and you can do a lot of things

00:20:04,400 --> 00:20:09,530
with that so what do we do in word

00:20:07,250 --> 00:20:11,600
vector says we set right we can

00:20:09,530 --> 00:20:16,460
understand a word by the company keeps

00:20:11,600 --> 00:20:18,380
so instead of going to the singular

00:20:16,460 --> 00:20:20,380
value decomposition and calculating the

00:20:18,380 --> 00:20:24,320
co-occurrence matrix why don't we just

00:20:20,380 --> 00:20:26,090
predict the context so there are two

00:20:24,320 --> 00:20:28,700
school of thoughts inward to wake again

00:20:26,090 --> 00:20:31,280
say two algorithms which is one is

00:20:28,700 --> 00:20:33,320
called skip Graham model so what this

00:20:31,280 --> 00:20:36,080
Kip Graham model does is suppose you

00:20:33,320 --> 00:20:38,000
have you have you take a sentence and

00:20:36,080 --> 00:20:40,640
you just say that I look at this

00:20:38,000 --> 00:20:42,710
sentence with the context of five so if

00:20:40,640 --> 00:20:44,630
you if I have a center word I look into

00:20:42,710 --> 00:20:47,809
three words on the left and three words

00:20:44,630 --> 00:20:50,030
on the right and if I have a word and if

00:20:47,809 --> 00:20:54,170
I start predicting the words in my

00:20:50,030 --> 00:20:55,850
context so that as much more I'll see

00:20:54,170 --> 00:20:58,250
those words together with that sentence

00:20:55,850 --> 00:20:59,870
together with that Center word will be

00:20:58,250 --> 00:21:02,720
will be having its probability

00:20:59,870 --> 00:21:04,460
increasing so we what we do is we

00:21:02,720 --> 00:21:06,770
maximize the log probability of the

00:21:04,460 --> 00:21:09,559
context of the word so I will represent

00:21:06,770 --> 00:21:12,410
it in pretty simple manner suppose this

00:21:09,559 --> 00:21:15,230
is the sentence and so sentence is very

00:21:12,410 --> 00:21:17,720
long say it has lots of words w 12 w

00:21:15,230 --> 00:21:21,020
1200w hundred but I am only interested

00:21:17,720 --> 00:21:24,679
in a context window of three so I'll

00:21:21,020 --> 00:21:28,460
take this word WC and what I do is I

00:21:24,679 --> 00:21:29,900
will create I'll run a log I learn

00:21:28,460 --> 00:21:32,230
optimization algorithm which will

00:21:29,900 --> 00:21:34,280
increase the probability of this and

00:21:32,230 --> 00:21:37,910
sorry which will increase the

00:21:34,280 --> 00:21:40,370
probability of these context words it's

00:21:37,910 --> 00:21:42,380
all going a highly mathematical and it's

00:21:40,370 --> 00:21:45,380
actually very difficult to give a demo

00:21:42,380 --> 00:21:46,940
on how will be calculating it but i will

00:21:45,380 --> 00:21:48,470
be uploading all the ipython notebooks

00:21:46,940 --> 00:21:51,140
which which can actually do all this

00:21:48,470 --> 00:21:54,140
stuff and i'll show you a demo also but

00:21:51,140 --> 00:21:57,050
that will be that will not be too much

00:21:54,140 --> 00:21:59,059
information in this talk but i will be

00:21:57,050 --> 00:22:01,370
outside and i can i can go through all

00:21:59,059 --> 00:22:04,730
those things so what's good and were to

00:22:01,370 --> 00:22:07,010
wake is first of all it is unsupervised

00:22:04,730 --> 00:22:09,770
we don't have to tag our data is just

00:22:07,010 --> 00:22:11,990
dumb text you just give it to that it

00:22:09,770 --> 00:22:12,830
will go word by word and predict the

00:22:11,990 --> 00:22:14,029
probabilities of

00:22:12,830 --> 00:22:17,330
each other words which come with that

00:22:14,029 --> 00:22:20,059
and and what happens with word to work

00:22:17,330 --> 00:22:22,190
is they actually create a low

00:22:20,059 --> 00:22:24,470
dimensional space so that lower

00:22:22,190 --> 00:22:26,510
dimensional space is it depends on you

00:22:24,470 --> 00:22:28,820
which how much dimension you want you

00:22:26,510 --> 00:22:31,220
1000 dimension or 300 dimension so that

00:22:28,820 --> 00:22:33,799
will be will be our vector instead of

00:22:31,220 --> 00:22:36,380
complete one heart vector which is the

00:22:33,799 --> 00:22:40,669
size of the vocabulary so these lower

00:22:36,380 --> 00:22:43,640
dimensions space are can be anything it

00:22:40,669 --> 00:22:46,250
can be a space of say a dimension of

00:22:43,640 --> 00:22:48,740
similarity this dimension only shows the

00:22:46,250 --> 00:22:50,899
similarity between words or it can be a

00:22:48,740 --> 00:22:53,809
dimension of sentiment this dimension

00:22:50,899 --> 00:22:55,580
shows that these words are actually

00:22:53,809 --> 00:22:57,740
aligned with their sentiments or they

00:22:55,580 --> 00:22:59,389
can be all caps dimension so in that

00:22:57,740 --> 00:23:03,559
dimension every single word will be

00:22:59,389 --> 00:23:05,659
capital or or it can be anything so it's

00:23:03,559 --> 00:23:08,149
up to you to figure out an explorer but

00:23:05,659 --> 00:23:09,590
what other lower dimension space are

00:23:08,149 --> 00:23:13,039
actually talking about and they are

00:23:09,590 --> 00:23:16,220
abstract and very hard to find out so it

00:23:13,039 --> 00:23:17,990
was there that word to work actually

00:23:16,220 --> 00:23:22,309
captures the dimension of similarity

00:23:17,990 --> 00:23:25,130
right so dimension of similarity is

00:23:22,309 --> 00:23:28,220
pretty broad so it can be similar in

00:23:25,130 --> 00:23:30,139
anything and all these algorithms they

00:23:28,220 --> 00:23:32,720
capture some tactical information as

00:23:30,139 --> 00:23:34,970
well as semantical information so if i

00:23:32,720 --> 00:23:38,720
say syntactical information singular or

00:23:34,970 --> 00:23:41,840
plural so after that after all these

00:23:38,720 --> 00:23:43,580
algorithms we we we calculate a vector

00:23:41,840 --> 00:23:45,649
right for every word we have a vector

00:23:43,580 --> 00:23:46,730
which is lower dimensional vector say

00:23:45,649 --> 00:23:50,779
three hundred or thousand dimensional

00:23:46,730 --> 00:23:53,690
vector so inward to work you can you can

00:23:50,779 --> 00:23:55,460
get a vector out of every word so we

00:23:53,690 --> 00:23:57,409
said that our representation was wrong

00:23:55,460 --> 00:23:58,970
and we were not our vector

00:23:57,409 --> 00:24:01,340
representation of every word was not

00:23:58,970 --> 00:24:03,590
carrying any information but after that

00:24:01,340 --> 00:24:07,389
our vector representation of every word

00:24:03,590 --> 00:24:10,399
has a lot of information so if i

00:24:07,389 --> 00:24:13,340
subtract that vector of apple minus

00:24:10,399 --> 00:24:16,909
vector Apple's it is exactly similar to

00:24:13,340 --> 00:24:18,380
vector of car- vector of cars and it's

00:24:16,909 --> 00:24:21,409
actually similar to family minus

00:24:18,380 --> 00:24:23,960
families so that means it said so if i

00:24:21,409 --> 00:24:25,700
if i subtract one vector from another

00:24:23,960 --> 00:24:26,750
vector it will be a dimension again it

00:24:25,700 --> 00:24:29,450
will be a vector right

00:24:26,750 --> 00:24:31,250
and if we go into same dimension that

00:24:29,450 --> 00:24:34,930
dimension is actually capturing a

00:24:31,250 --> 00:24:37,280
singular plural relationship so and

00:24:34,930 --> 00:24:40,400
there are lots of interesting things you

00:24:37,280 --> 00:24:42,290
can do with that vector of shared minus

00:24:40,400 --> 00:24:45,020
vector of clothing is equals to vector

00:24:42,290 --> 00:24:46,850
chair- furniture so shirt is a type of

00:24:45,020 --> 00:24:49,490
clothing and chair is a type of

00:24:46,850 --> 00:24:52,580
furniture similarly this is very famous

00:24:49,490 --> 00:24:55,280
example sorry there is a typo so vector

00:24:52,580 --> 00:24:57,170
of king- vector of men is actually equal

00:24:55,280 --> 00:25:00,140
to vector vector of Queen minus vector

00:24:57,170 --> 00:25:03,470
of women so it is actually calculating

00:25:00,140 --> 00:25:05,870
here that all these semantical

00:25:03,470 --> 00:25:07,490
information which is not anyway in sync

00:25:05,870 --> 00:25:09,430
tactical information so how do we

00:25:07,490 --> 00:25:14,750
calculate this dimension of similarity

00:25:09,430 --> 00:25:17,480
so here the the the topic of the of this

00:25:14,750 --> 00:25:20,240
talk is solving logical presence through

00:25:17,480 --> 00:25:22,490
natural language processing right so the

00:25:20,240 --> 00:25:24,770
logical puzzles which which you can

00:25:22,490 --> 00:25:26,750
solve our analogies and odd man out

00:25:24,770 --> 00:25:30,980
through that so how do we solve

00:25:26,750 --> 00:25:34,400
analogies so suppose and how do we solve

00:25:30,980 --> 00:25:36,590
analogies through our brain if we have

00:25:34,400 --> 00:25:38,600
not seen that word we cannot solve it's

00:25:36,590 --> 00:25:41,240
an allergy problem and if we have not

00:25:38,600 --> 00:25:44,050
seen that word we cannot solve it's odd

00:25:41,240 --> 00:25:46,640
man out problem so sim so it's actually

00:25:44,050 --> 00:25:50,680
the this limitation applies to that

00:25:46,640 --> 00:25:55,250
algorithm as well which we human as and

00:25:50,680 --> 00:25:58,220
so so if man is women and what is an

00:25:55,250 --> 00:25:59,900
illogical to if man and women and then

00:25:58,220 --> 00:26:02,720
what is an illogical to King and what

00:25:59,900 --> 00:26:05,810
right so that will so answer is queen so

00:26:02,720 --> 00:26:10,220
in were to wake you can just simply say

00:26:05,810 --> 00:26:12,200
add king- men plus women minus so that

00:26:10,220 --> 00:26:14,030
will be queen so if you can see that men

00:26:12,200 --> 00:26:16,580
and women direction is exactly similar

00:26:14,030 --> 00:26:18,170
to king and queen direction so if we

00:26:16,580 --> 00:26:20,840
have these three information we can

00:26:18,170 --> 00:26:23,000
directly jump into queen so in this way

00:26:20,840 --> 00:26:26,930
you can solve your analogy problems and

00:26:23,000 --> 00:26:28,730
in this way you can also solve your odd

00:26:26,930 --> 00:26:33,710
man out problem so how do I solve odd

00:26:28,730 --> 00:26:35,900
man out problem is suppose when we were

00:26:33,710 --> 00:26:37,250
training our word to work model when we

00:26:35,900 --> 00:26:40,040
are calculating all these probabilities

00:26:37,250 --> 00:26:40,640
that which word comes together what we

00:26:40,040 --> 00:26:43,130
can do is

00:26:40,640 --> 00:26:45,620
can just infuse some bad data random

00:26:43,130 --> 00:26:48,080
text not even random which is wrong in

00:26:45,620 --> 00:26:50,750
grammar right so that is called negative

00:26:48,080 --> 00:26:52,760
sampling so we are giving wrong data to

00:26:50,750 --> 00:26:55,310
the algorithm and we will say that this

00:26:52,760 --> 00:26:57,650
is wrong so we just tag that this is say

00:26:55,310 --> 00:26:59,990
all the nice words are one which has

00:26:57,650 --> 00:27:02,450
grammatically correct all the wrong

00:26:59,990 --> 00:27:04,760
words are 0 tap 20 which are

00:27:02,450 --> 00:27:06,830
grammatically not correct so algorithm

00:27:04,760 --> 00:27:09,830
will learn that these words should not

00:27:06,830 --> 00:27:11,930
come together which are which are not

00:27:09,830 --> 00:27:14,450
the part of the grammatical structure so

00:27:11,930 --> 00:27:15,980
if we if we go into odd man out it will

00:27:14,450 --> 00:27:18,650
say that these words have not come

00:27:15,980 --> 00:27:21,920
together in any dimension so this word

00:27:18,650 --> 00:27:25,190
is actually out of that so I will so i

00:27:21,920 --> 00:27:30,650
will go through all these demos now so

00:27:25,190 --> 00:27:35,380
that we can see that in action so I was

00:27:30,650 --> 00:27:37,820
talking about SVD is a font clear okay

00:27:35,380 --> 00:27:39,650
so these are three words I like deep

00:27:37,820 --> 00:27:44,570
learning I like an LPN I enjoy flying

00:27:39,650 --> 00:27:46,520
right so using numpy linear algebra

00:27:44,570 --> 00:27:49,610
library we can directly calculate SVD

00:27:46,520 --> 00:27:51,110
and these are our words right I like NLP

00:27:49,610 --> 00:27:54,710
deep learning and all these words are

00:27:51,110 --> 00:27:58,790
there in this in this thing and if I

00:27:54,710 --> 00:28:00,160
create a desk context matrix manually

00:27:58,790 --> 00:28:02,510
you can always create it

00:28:00,160 --> 00:28:06,680
programmatically and if you calculate

00:28:02,510 --> 00:28:09,410
SVD of it so this hue this this matrix

00:28:06,680 --> 00:28:13,100
which is a context matrix X is actually

00:28:09,410 --> 00:28:18,350
is now break it into three matrix us NV

00:28:13,100 --> 00:28:22,100
v dash and so Hugh has all these eigen

00:28:18,350 --> 00:28:24,860
vectors so if we if we show these

00:28:22,100 --> 00:28:28,760
eigenvectors on this two-dimensional

00:28:24,860 --> 00:28:33,050
plane we can see that these are now

00:28:28,760 --> 00:28:35,450
coming in some patterns so this is very

00:28:33,050 --> 00:28:37,130
small and it will actually not capture

00:28:35,450 --> 00:28:39,260
any pattern if you do this with large

00:28:37,130 --> 00:28:41,720
data it will capture a lot of patterns

00:28:39,260 --> 00:28:47,230
so those kinds of patterns you can see

00:28:41,720 --> 00:28:49,450
here so so these are TS any

00:28:47,230 --> 00:28:52,940
visualization which is used for

00:28:49,450 --> 00:28:54,620
visualizing a high-dimensional data so

00:28:52,940 --> 00:28:55,970
you say you can see that pal

00:28:54,620 --> 00:29:00,740
maintain elections are coming together

00:28:55,970 --> 00:29:05,080
and banks and reporters actually

00:29:00,740 --> 00:29:07,840
ministry and union so all these things

00:29:05,080 --> 00:29:13,430
so this is how we will calculate SVD and

00:29:07,840 --> 00:29:16,340
move forward but so I will go into demo

00:29:13,430 --> 00:29:18,680
forward to it so python has an awesome

00:29:16,340 --> 00:29:21,260
library called jensen and you guys can

00:29:18,680 --> 00:29:25,550
go through it's very simple and radam

00:29:21,260 --> 00:29:27,770
rick is the author of that so in word to

00:29:25,550 --> 00:29:29,360
wake is actually written in c so he has

00:29:27,770 --> 00:29:31,610
converted into siphon and it's actually

00:29:29,360 --> 00:29:33,830
very fast and you can do distributed

00:29:31,610 --> 00:29:36,920
programming also because word to wake is

00:29:33,830 --> 00:29:40,040
limited to high volume data so Google

00:29:36,920 --> 00:29:41,870
has google has given a train model of

00:29:40,040 --> 00:29:44,809
word to work which is 100 billion word

00:29:41,870 --> 00:29:47,690
model it has 300 million unique words

00:29:44,809 --> 00:29:50,600
and it contains almost everything we

00:29:47,690 --> 00:29:52,730
speak today even Indian words so you can

00:29:50,600 --> 00:29:57,260
check your name there whose name you are

00:29:52,730 --> 00:29:59,630
similar to so so this is how so you just

00:29:57,260 --> 00:30:01,790
load that model and that model is there

00:29:59,630 --> 00:30:04,460
in binary form and if you load that

00:30:01,790 --> 00:30:07,730
model and you say that you give positive

00:30:04,460 --> 00:30:10,790
as women and king- as man so you'll get

00:30:07,730 --> 00:30:12,350
that Queen as the highest number so this

00:30:10,790 --> 00:30:16,220
is how you will solve your analogies

00:30:12,350 --> 00:30:17,480
problems and so this is your so it has

00:30:16,220 --> 00:30:19,910
another method called doesn't match

00:30:17,480 --> 00:30:24,170
which is odd man out so if I say

00:30:19,910 --> 00:30:26,840
breakfast cereal dinner a lunch so which

00:30:24,170 --> 00:30:30,320
of these does not fall into same

00:30:26,840 --> 00:30:34,700
category is cereal and you have

00:30:30,320 --> 00:30:36,020
similarity is there and it's a

00:30:34,700 --> 00:30:39,470
dictionary so if you want to see that

00:30:36,020 --> 00:30:41,450
what is the raw vector of this computer

00:30:39,470 --> 00:30:44,030
so this is the vector this is 300

00:30:41,450 --> 00:30:46,070
dimension vector so once you have vector

00:30:44,030 --> 00:30:47,450
you can do all this stuff you you want

00:30:46,070 --> 00:30:50,380
to do with your machine learning

00:30:47,450 --> 00:30:53,929
libraries so you can learn you can learn

00:30:50,380 --> 00:30:58,160
sentiments you can do named entity

00:30:53,929 --> 00:31:00,950
recognition and all and it's a and it is

00:30:58,160 --> 00:31:03,290
completely semantical so sushi and shop

00:31:00,950 --> 00:31:06,650
and japanese and restaurant are actually

00:31:03,290 --> 00:31:07,770
similar and yeah this is this i just

00:31:06,650 --> 00:31:08,940
cooked up so

00:31:07,770 --> 00:31:11,850
because we were talking about moving

00:31:08,940 --> 00:31:13,800
engage revile a lot so if I say which is

00:31:11,850 --> 00:31:16,890
most similar to Modi so these are the

00:31:13,800 --> 00:31:19,410
results were too i give so it depends on

00:31:16,890 --> 00:31:22,290
your data so this data is actually last

00:31:19,410 --> 00:31:23,910
ten year google news data set and they

00:31:22,290 --> 00:31:26,520
trained upon it and it's a very huge

00:31:23,910 --> 00:31:36,300
model it takes around four or five gb of

00:31:26,520 --> 00:31:39,330
ram in your computer okay I'll quickly

00:31:36,300 --> 00:31:45,810
go through one of the things which is

00:31:39,330 --> 00:31:52,230
about where to work is or maybe I should

00:31:45,810 --> 00:31:55,050
just okay so if you want to train your

00:31:52,230 --> 00:32:00,630
model so you have you have to create

00:31:55,050 --> 00:32:02,730
your you have to create your text two to

00:32:00,630 --> 00:32:04,800
two sentences and then you can train

00:32:02,730 --> 00:32:07,860
your own model and then do do queries on

00:32:04,800 --> 00:32:09,540
this so I have other things which I can

00:32:07,860 --> 00:32:13,620
talk offline and I'm open for questions

00:32:09,540 --> 00:32:16,280
right now if you have I this is more of

00:32:13,620 --> 00:32:19,170
an announcement rather than a question

00:32:16,280 --> 00:32:21,690
my name is Amit Rao and i am a freelance

00:32:19,170 --> 00:32:24,570
technology consultant with a background

00:32:21,690 --> 00:32:27,120
in research development a natural

00:32:24,570 --> 00:32:30,780
language processing so i have created an

00:32:27,120 --> 00:32:32,730
open space slot at twelve thirty four

00:32:30,780 --> 00:32:35,220
people are interested in the natural

00:32:32,730 --> 00:32:38,100
language procedure and python so as i

00:32:35,220 --> 00:32:40,650
said there is a connecticut toolkit and

00:32:38,100 --> 00:32:42,570
i think there's opportunity to create a

00:32:40,650 --> 00:32:46,410
community of people are interested in

00:32:42,570 --> 00:32:48,990
NLT NL first thing so those of you

00:32:46,410 --> 00:32:51,810
intersect please come to the first floor

00:32:48,990 --> 00:32:54,060
at wealth it and we will brainstorm on

00:32:51,810 --> 00:32:58,080
how we can contribute to this nature

00:32:54,060 --> 00:33:00,540
sure thang hello yeah thank you for the

00:32:58,080 --> 00:33:01,620
great talk I have two questions ok the

00:33:00,540 --> 00:33:03,840
first question is when we are talking

00:33:01,620 --> 00:33:06,780
about word to wake you said it is based

00:33:03,840 --> 00:33:08,400
on context aware predictions which is

00:33:06,780 --> 00:33:12,570
maximizing the likelihood of avoid

00:33:08,400 --> 00:33:14,850
nearby right so the idea that was

00:33:12,570 --> 00:33:16,650
responsible for the word way were to

00:33:14,850 --> 00:33:18,290
wake was that the original

00:33:16,650 --> 00:33:19,910
co-occurrences model was very you

00:33:18,290 --> 00:33:22,790
chose to write that it was n square

00:33:19,910 --> 00:33:25,640
implementation so can you tell me how

00:33:22,790 --> 00:33:29,270
exactly they are storing the context out

00:33:25,640 --> 00:33:31,280
of a word in word to ache so what do

00:33:29,270 --> 00:33:33,560
work is is a neural network so the

00:33:31,280 --> 00:33:36,200
algorithm is neural network so initially

00:33:33,560 --> 00:33:38,180
you give one hot encoding as your first

00:33:36,200 --> 00:33:39,950
layer second layer is your dimension

00:33:38,180 --> 00:33:42,620
suppose you want 300 dimension vector

00:33:39,950 --> 00:33:44,840
you train it to just 300 dimensions and

00:33:42,620 --> 00:33:47,030
after that you just output that word

00:33:44,840 --> 00:33:48,980
again so once you are calculating

00:33:47,030 --> 00:33:50,720
gradient descent and that time we will

00:33:48,980 --> 00:33:52,520
calculate gradient descent or any

00:33:50,720 --> 00:33:56,060
optimization on the weights of maximum

00:33:52,520 --> 00:33:58,610
likelihood of your context words so so

00:33:56,060 --> 00:34:00,500
so you'll save that as numpy Ernest ok

00:33:58,610 --> 00:34:04,880
so aren't we actually constraining the

00:34:00,500 --> 00:34:07,820
model understand the limit of context

00:34:04,880 --> 00:34:09,470
out of work so there is a possibility of

00:34:07,820 --> 00:34:11,360
what is associated with another word

00:34:09,470 --> 00:34:13,100
which we are not allowing it to train on

00:34:11,360 --> 00:34:16,190
because we are putting a constraint of

00:34:13,100 --> 00:34:19,669
so mention so that depends on your

00:34:16,190 --> 00:34:22,730
computational power ok so your context

00:34:19,669 --> 00:34:24,919
is free you can take a hundred 100 left

00:34:22,730 --> 00:34:26,900
and hundred right word context and you

00:34:24,919 --> 00:34:30,740
can increase your dimension to say 2000

00:34:26,900 --> 00:34:32,360
or 5000 so that is that is to cable that

00:34:30,740 --> 00:34:35,090
that is a parameter which is in there in

00:34:32,360 --> 00:34:37,210
else and one last thing when you are

00:34:35,090 --> 00:34:40,790
talking about the vectors in n square

00:34:37,210 --> 00:34:42,679
co-occurrences so you give an example of

00:34:40,790 --> 00:34:45,169
say the word is nice and it is at the

00:34:42,679 --> 00:34:46,880
30th index so there will be a 1 on that

00:34:45,169 --> 00:34:49,940
point so that will generate n square

00:34:46,880 --> 00:34:52,820
matrix right right so right now I was

00:34:49,940 --> 00:34:55,850
thinking if we take a straight of single

00:34:52,820 --> 00:34:58,220
vector where words are indexed with

00:34:55,850 --> 00:35:00,200
their index number only so nice is a

00:34:58,220 --> 00:35:02,900
vector that is connected directly to a

00:35:00,200 --> 00:35:06,740
big position 30 turner linear index

00:35:02,900 --> 00:35:09,140
right and to associate a word we can add

00:35:06,740 --> 00:35:11,840
it index we can add the index to it

00:35:09,140 --> 00:35:14,180
suppose there are 100 words and this

00:35:11,840 --> 00:35:16,340
thing is at 30 so its index 28 is point

00:35:14,180 --> 00:35:18,950
Lee and we want to associate it like

00:35:16,340 --> 00:35:22,490
nice weather weather is at 70th index so

00:35:18,950 --> 00:35:26,870
it is point seven so we can act 39.7 so

00:35:22,490 --> 00:35:27,780
30.7 the number itself is someway

00:35:26,870 --> 00:35:30,480
signifying the

00:35:27,780 --> 00:35:32,880
nice where there is a context correct so

00:35:30,480 --> 00:35:35,010
I mean I don't know I just came up with

00:35:32,880 --> 00:35:37,410
it but that is a very nice view of

00:35:35,010 --> 00:35:39,360
linearizing n square order problem

00:35:37,410 --> 00:35:40,980
correct that is that is that is the way

00:35:39,360 --> 00:35:43,650
and people are actually doing it but

00:35:40,980 --> 00:35:45,510
it's very limited to your context if you

00:35:43,650 --> 00:35:47,580
train a model that is only limited to

00:35:45,510 --> 00:35:50,100
your own context it's not a generalized

00:35:47,580 --> 00:35:52,440
like where to work that you can do a lot

00:35:50,100 --> 00:35:53,600
of things so in your view nice and

00:35:52,440 --> 00:35:55,500
whether you are giving the weights

00:35:53,600 --> 00:35:58,680
you're not learning the way it's

00:35:55,500 --> 00:36:02,700
automatically out of the the corpora

00:35:58,680 --> 00:36:04,290
right it's your view so i can always

00:36:02,700 --> 00:36:06,180
disagree with your view that don't

00:36:04,290 --> 00:36:08,460
associate nice with whether i associated

00:36:06,180 --> 00:36:10,620
awesome with the well and I give point 7

00:36:08,460 --> 00:36:14,820
so there are these kinds of problems are

00:36:10,620 --> 00:36:17,070
there okay one more question out here up

00:36:14,820 --> 00:36:20,520
here up your knee all right okay hello

00:36:17,070 --> 00:36:22,590
yeah so one of the things you talked

00:36:20,520 --> 00:36:25,380
about right now using these vectors

00:36:22,590 --> 00:36:27,240
right so you're associating some words

00:36:25,380 --> 00:36:28,740
in this neighborhood and you're

00:36:27,240 --> 00:36:30,540
predicting a likelihood that the similar

00:36:28,740 --> 00:36:32,540
words will appear next to it correct

00:36:30,540 --> 00:36:36,870
right so how do you handle multiple

00:36:32,540 --> 00:36:38,160
meanings right like nice right nice

00:36:36,870 --> 00:36:40,110
could have three or four meanings it

00:36:38,160 --> 00:36:41,790
could be nice as in nice it could be

00:36:40,110 --> 00:36:43,440
something talking about a nice level

00:36:41,790 --> 00:36:44,820
where it does not feel good like the CPU

00:36:43,440 --> 00:36:47,160
a nice level or you could be talking

00:36:44,820 --> 00:36:48,900
about the nice biscuit correct right so

00:36:47,160 --> 00:36:50,280
when you create these vectors how do you

00:36:48,900 --> 00:36:51,930
account for multiple meanings of words

00:36:50,280 --> 00:36:53,880
which is fairly common apples apples

00:36:51,930 --> 00:36:57,540
also a very common correct example yeah

00:36:53,880 --> 00:37:02,100
a that and be computational performance

00:36:57,540 --> 00:37:05,190
versus it's like accuracy right once I

00:37:02,100 --> 00:37:07,710
say a posting list and the word net with

00:37:05,190 --> 00:37:10,620
links on the graph well what would it

00:37:07,710 --> 00:37:13,830
straight off me so these problems are

00:37:10,620 --> 00:37:15,840
there so a disambiguation of the word so

00:37:13,830 --> 00:37:17,760
this problem is there but what you can

00:37:15,840 --> 00:37:19,500
do is once you have these word vectors

00:37:17,760 --> 00:37:21,960
which actually have some information

00:37:19,500 --> 00:37:24,390
about that word you can again train them

00:37:21,960 --> 00:37:26,670
so what vector is not a train model it's

00:37:24,390 --> 00:37:28,290
actually words so whatever the words you

00:37:26,670 --> 00:37:30,720
are saving in your vocabulary as were

00:37:28,290 --> 00:37:32,460
not encoding our inward next you can

00:37:30,720 --> 00:37:35,960
have work to work and then again train

00:37:32,460 --> 00:37:35,960
your disambiguation algorithms

00:37:37,550 --> 00:37:43,020
just too sorry to interrupt actually

00:37:40,710 --> 00:37:45,480
we're like really short in time you can

00:37:43,020 --> 00:37:47,940
take these questions yes afterwards okay

00:37:45,480 --> 00:37:52,619
so just don't be mad at me because we're

00:37:47,940 --> 00:37:54,420
really short in time okay so thank you

00:37:52,619 --> 00:37:57,530
for such a nice talk can we have a round

00:37:54,420 --> 00:37:57,530

YouTube URL: https://www.youtube.com/watch?v=LSQpWZlKAaA


