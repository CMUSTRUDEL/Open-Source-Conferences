Title: Explore Big Data using Simple Python Code and Cloud Environment - PyCon India 2015
Publication date: 2015-10-12
Playlist: PyCon India 2015
Description: 
	Speaker: Harikrishna

Details: https://in.pycon.org/cfp/pycon-india-2015/proposals/explore-big-data-using-simple-python-code-and-cloud-environment/
Captions: 
	00:00:00,000 --> 00:00:05,460
our next talk we have a topic explore

00:00:03,300 --> 00:00:08,400
big data using simple Python code and

00:00:05,460 --> 00:00:10,650
cloud involvement by Hari Krishna raava

00:00:08,400 --> 00:00:16,410
is working as a performance engineer

00:00:10,650 --> 00:00:18,570
lead in accenture it is good evening my

00:00:16,410 --> 00:00:20,910
name is Hari Krishna i am from accenture

00:00:18,570 --> 00:00:24,119
so working is a performance engineering

00:00:20,910 --> 00:00:26,490
late that is my day job so I should but

00:00:24,119 --> 00:00:29,460
I love I create a lot of Python programs

00:00:26,490 --> 00:00:33,329
on my own to make improvements and

00:00:29,460 --> 00:00:36,570
product out in my project so as part of

00:00:33,329 --> 00:00:39,480
that like even I explored a big data and

00:00:36,570 --> 00:00:42,090
Python and clothed like I can using

00:00:39,480 --> 00:00:43,500
million Python alright so just want to

00:00:42,090 --> 00:00:46,559
share my knowledge on like you know

00:00:43,500 --> 00:00:49,590
what's a exploring a big big data using

00:00:46,559 --> 00:00:52,800
a very simple Python code it need not be

00:00:49,590 --> 00:00:59,190
like a your expert in that and also make

00:00:52,800 --> 00:01:00,780
use of the cloud all right so it is a

00:00:59,190 --> 00:01:02,550
simple eyes and I am just going to

00:01:00,780 --> 00:01:04,409
follow in here so we will mainly discuss

00:01:02,550 --> 00:01:09,439
about like we will take an example of a

00:01:04,409 --> 00:01:13,200
big data set and try to explore the data

00:01:09,439 --> 00:01:16,470
hmm and create a some problem statement

00:01:13,200 --> 00:01:19,380
on that and also how we cannot do it in

00:01:16,470 --> 00:01:21,270
a traditional process and instead of

00:01:19,380 --> 00:01:26,189
that how we can use a heart of MapReduce

00:01:21,270 --> 00:01:28,229
and how we meant also like using Python

00:01:26,189 --> 00:01:31,619
code how you can write program to

00:01:28,229 --> 00:01:34,049
process the data and a later we will

00:01:31,619 --> 00:01:38,340
talk about Amazonia more that is the

00:01:34,049 --> 00:01:40,320
further abstraction like you can oh you

00:01:38,340 --> 00:01:42,960
can create enteric software as a service

00:01:40,320 --> 00:01:44,159
and just you need to write your program

00:01:42,960 --> 00:01:47,430
and submit that's it you need not worry

00:01:44,159 --> 00:01:49,170
about enter infrastructure so and then

00:01:47,430 --> 00:01:52,229
we will talk I will quickly go through

00:01:49,170 --> 00:01:59,280
the demo and some interesting stuff

00:01:52,229 --> 00:02:01,590
later all right so before that like this

00:01:59,280 --> 00:02:03,060
is the solution so instead of talking

00:02:01,590 --> 00:02:05,880
about problem like I have the solution

00:02:03,060 --> 00:02:09,239
proof initially like so this is the top

00:02:05,880 --> 00:02:09,840
10 web pages viewed in Wikipedia English

00:02:09,239 --> 00:02:14,519
during

00:02:09,840 --> 00:02:16,860
may month all right so you can see

00:02:14,519 --> 00:02:23,670
something like that can you relate that

00:02:16,860 --> 00:02:26,400
to the month of me alright so maybe

00:02:23,670 --> 00:02:28,560
there's a spaceflight happened in 2015

00:02:26,400 --> 00:02:33,450
something that may be the reason like a

00:02:28,560 --> 00:02:37,019
lot of places on that and the Falcon 9 I

00:02:33,450 --> 00:02:40,519
think will be 1.1 the project got kick

00:02:37,019 --> 00:02:46,920
it off or something like that yep and

00:02:40,519 --> 00:02:50,340
second it's June in June month also you

00:02:46,920 --> 00:02:53,519
can just try to relate maybe there is a

00:02:50,340 --> 00:03:00,840
number nine game of thrones maybe that

00:02:53,519 --> 00:03:04,349
is the season 5 started right it's a

00:03:00,840 --> 00:03:07,200
number three so that in during August

00:03:04,349 --> 00:03:09,629
month these are the top ten pages viewed

00:03:07,200 --> 00:03:12,090
in Wikipedia the first one is the top

00:03:09,629 --> 00:03:15,120
one is almost like seven million hits on

00:03:12,090 --> 00:03:18,319
that nuclear magnetic resource resonance

00:03:15,120 --> 00:03:21,510
and also you can just relate to like

00:03:18,319 --> 00:03:22,859
there is a presidential campaign is

00:03:21,510 --> 00:03:29,340
happening that may be the reason like a

00:03:22,859 --> 00:03:33,359
number nine Donald Trump right and this

00:03:29,340 --> 00:03:35,359
is the sep tember so maybe you have a

00:03:33,359 --> 00:03:43,139
playstation 3 something happen in

00:03:35,359 --> 00:03:45,989
September and you have maybe Google

00:03:43,139 --> 00:03:48,239
maybe they spin up to alphabet that may

00:03:45,989 --> 00:03:52,079
be the reason like you got it with

00:03:48,239 --> 00:03:54,180
google all right and one thing you can

00:03:52,079 --> 00:03:57,060
just try to correlate is that I don't

00:03:54,180 --> 00:04:01,380
know why June you have number five

00:03:57,060 --> 00:04:03,780
deaths in 2015 and August you have

00:04:01,380 --> 00:04:05,669
deaths in 2015 I don't know what is the

00:04:03,780 --> 00:04:08,790
reason like so many people are hitting

00:04:05,669 --> 00:04:12,150
on white that sin 2015 yeah all right

00:04:08,790 --> 00:04:14,190
and in September again you can see that

00:04:12,150 --> 00:04:15,840
Hillary Clinton so you can just try to

00:04:14,190 --> 00:04:18,090
relate to what's happening in the world

00:04:15,840 --> 00:04:22,680
versus what are the pages viewed in

00:04:18,090 --> 00:04:23,550
Wikipedia alright so how I got all this

00:04:22,680 --> 00:04:27,750
data

00:04:23,550 --> 00:04:31,710
alright so everyone knows Wikipedia

00:04:27,750 --> 00:04:33,870
there is no introduction to that but

00:04:31,710 --> 00:04:38,759
think of its Wikipedia is like a

00:04:33,870 --> 00:04:40,979
voluntarily created data by across the

00:04:38,759 --> 00:04:43,979
world you can see that like almost like

00:04:40,979 --> 00:04:45,840
49 million articles created in English

00:04:43,979 --> 00:04:49,229
and all these things all right so

00:04:45,840 --> 00:04:51,300
amazing we thought we keep period

00:04:49,229 --> 00:04:54,690
without a stack workflow or all these

00:04:51,300 --> 00:04:56,909
things like how search engine should be

00:04:54,690 --> 00:04:59,849
we'll be right so chances are they're

00:04:56,909 --> 00:05:01,650
that in your top ten pages searched in a

00:04:59,849 --> 00:05:03,449
Google or any other research engine

00:05:01,650 --> 00:05:09,960
chances are that you will have a

00:05:03,449 --> 00:05:13,650
Wikipedia on the top right now so how we

00:05:09,960 --> 00:05:16,710
got that so Wikipedia is like it's like

00:05:13,650 --> 00:05:20,490
a non-government non-commercial and so

00:05:16,710 --> 00:05:23,460
non profitable organization created by

00:05:20,490 --> 00:05:26,759
volunteers with funding from across the

00:05:23,460 --> 00:05:29,669
public right and so they have a they

00:05:26,759 --> 00:05:31,620
have a freedom to provide everything

00:05:29,669 --> 00:05:33,870
open data to everyone unlike a

00:05:31,620 --> 00:05:37,259
commercial they can't disclose how many

00:05:33,870 --> 00:05:39,779
customers they have or how many users

00:05:37,259 --> 00:05:41,099
they have or how many hits per second

00:05:39,779 --> 00:05:44,099
they're getting all these things all

00:05:41,099 --> 00:05:47,490
right so what they do is that they have

00:05:44,099 --> 00:05:49,949
a website in wikipedia.org so we're on

00:05:47,490 --> 00:05:54,740
they have they will create hourly log

00:05:49,949 --> 00:05:58,050
files about what are the pages viewed

00:05:54,740 --> 00:06:01,469
the page name and the number of requests

00:05:58,050 --> 00:06:03,719
in that one hour and how much size of

00:06:01,469 --> 00:06:08,129
the data downloaded as part of that with

00:06:03,719 --> 00:06:10,139
one and if you see here the first one so

00:06:08,129 --> 00:06:14,159
the first one is the project that is

00:06:10,139 --> 00:06:17,419
different languages like de dirt and en

00:06:14,159 --> 00:06:23,460
english es means espaÃ±ol / ever france

00:06:17,419 --> 00:06:27,330
right so you have hourly log files so

00:06:23,460 --> 00:06:29,729
now de maison like i have overly log

00:06:27,330 --> 00:06:31,319
file size of 100 mb in compressed format

00:06:29,729 --> 00:06:35,490
and if you uncompress it it's almost

00:06:31,319 --> 00:06:36,840
like a 400 mb and in a month if letters

00:06:35,490 --> 00:06:39,360
i want to process the data

00:06:36,840 --> 00:06:42,600
extract the data from one month it will

00:06:39,360 --> 00:06:45,930
be approximately like 24 hours 31 into

00:06:42,600 --> 00:06:48,330
400 that is a 300 gig of the data and

00:06:45,930 --> 00:06:52,440
approximately five million of Records

00:06:48,330 --> 00:06:54,479
right so you can do process in a normal

00:06:52,440 --> 00:06:56,699
your computer or server or something

00:06:54,479 --> 00:06:59,940
like that so that requires a different

00:06:56,699 --> 00:07:02,850
ballgame so how traditional process you

00:06:59,940 --> 00:07:05,940
can't put it in a large and high-speed

00:07:02,850 --> 00:07:08,040
data base it is expensive and also here

00:07:05,940 --> 00:07:09,990
we are just talking about 300g we think

00:07:08,040 --> 00:07:13,169
about like a teradata petabytes or

00:07:09,990 --> 00:07:15,950
something like that so it's expensive

00:07:13,169 --> 00:07:18,600
and it can't scale as data increases

00:07:15,950 --> 00:07:22,040
similarly like file processing on a high

00:07:18,600 --> 00:07:24,690
CPU like Enterprise world-class servers

00:07:22,040 --> 00:07:26,790
like a super computers and something

00:07:24,690 --> 00:07:28,889
like that okay so they're also like very

00:07:26,790 --> 00:07:32,490
expensive and not possible for any

00:07:28,889 --> 00:07:35,669
normal people and also if something

00:07:32,490 --> 00:07:37,020
fails he in the 99 percent completed and

00:07:35,669 --> 00:07:40,800
one person feel you need to read on the

00:07:37,020 --> 00:07:42,960
job again so in both the cases it is its

00:07:40,800 --> 00:07:47,430
infrastructure is expensive and takes

00:07:42,960 --> 00:07:49,650
more time to provide the solution so so

00:07:47,430 --> 00:07:51,270
the solution we have is like I think

00:07:49,650 --> 00:07:53,460
everyone knows about of MapReduce but I

00:07:51,270 --> 00:07:58,560
just give a quick overview of what is

00:07:53,460 --> 00:08:00,780
that so you have so what we do is that

00:07:58,560 --> 00:08:04,340
here like you spilt the input file into

00:08:00,780 --> 00:08:07,770
blocks of 64 mb or higher and distribute

00:08:04,340 --> 00:08:09,660
to the multiple commodity computers when

00:08:07,770 --> 00:08:12,060
it's a community computers it's like a

00:08:09,660 --> 00:08:14,370
normal and it's not a normal enterprise

00:08:12,060 --> 00:08:17,310
servers it may be any normal computers

00:08:14,370 --> 00:08:21,060
we have like a fuzzy brown for get down

00:08:17,310 --> 00:08:23,190
and normal intel pentium shipped some

00:08:21,060 --> 00:08:25,139
single air okay and in the same network

00:08:23,190 --> 00:08:26,580
like you distribute the you split the

00:08:25,139 --> 00:08:28,110
file into the blocks and distribute

00:08:26,580 --> 00:08:31,410
across the multiple community computers

00:08:28,110 --> 00:08:33,599
in the same network and process the

00:08:31,410 --> 00:08:35,870
blocks so you have the blocks the data

00:08:33,599 --> 00:08:38,490
in the DOTA you call it a data nodes and

00:08:35,870 --> 00:08:41,039
you will push the tasks you will send

00:08:38,490 --> 00:08:43,080
the tasks processes to execute on the

00:08:41,039 --> 00:08:45,060
data and the locally instead of like

00:08:43,080 --> 00:08:48,300
normal in a database means that you will

00:08:45,060 --> 00:08:50,880
face the data and put it in you process

00:08:48,300 --> 00:08:53,000
it in your app server but here

00:08:50,880 --> 00:08:57,990
it's processing will happen locally

00:08:53,000 --> 00:09:01,530
right and also your you are distributing

00:08:57,990 --> 00:09:03,630
the data across the multiple nodes so

00:09:01,530 --> 00:09:06,270
even one when we are doing a parallel

00:09:03,630 --> 00:09:09,030
processing across the nodes even if one

00:09:06,270 --> 00:09:11,490
node fails you may need to rerun the job

00:09:09,030 --> 00:09:14,610
again right so you will not have an

00:09:11,490 --> 00:09:16,020
integrity of the data so the one more

00:09:14,610 --> 00:09:17,910
solution for this is that you you

00:09:16,020 --> 00:09:20,790
replicate the blocks like instead of one

00:09:17,910 --> 00:09:23,370
copy of the block you will distribute

00:09:20,790 --> 00:09:25,890
across you copy make it two or three

00:09:23,370 --> 00:09:27,660
copies of that one copy in one node and

00:09:25,890 --> 00:09:30,090
another copy in another node so if one

00:09:27,660 --> 00:09:33,060
node fails and the another node is there

00:09:30,090 --> 00:09:35,960
which will process the data alright so

00:09:33,060 --> 00:09:40,440
that is how hard of MapReduce works

00:09:35,960 --> 00:09:42,210
called heard of MapReduce so the first

00:09:40,440 --> 00:09:44,880
one we call as a distribution of the

00:09:42,210 --> 00:09:47,850
files is called HDFS or duper

00:09:44,880 --> 00:09:51,600
distributed file systems and the next

00:09:47,850 --> 00:09:53,760
one is that MapReduce that is the logic

00:09:51,600 --> 00:09:57,450
you write to process the data on the

00:09:53,760 --> 00:10:00,600
blocks and get the output so this is how

00:09:57,450 --> 00:10:03,330
it works so you have different splits

00:10:00,600 --> 00:10:06,510
blocks and on each block here the mapper

00:10:03,330 --> 00:10:11,700
will run and you will get the output so

00:10:06,510 --> 00:10:14,730
let us say you have 64 gb or 64 GB means

00:10:11,700 --> 00:10:17,160
you have almost like a thousand tasks or

00:10:14,730 --> 00:10:19,350
thousand splits and on each split two

00:10:17,160 --> 00:10:22,110
you will have a map executed and you

00:10:19,350 --> 00:10:24,810
will get in a output and once you got

00:10:22,110 --> 00:10:27,720
output from all the thousand mappers you

00:10:24,810 --> 00:10:30,780
will merge them and sort it by a key and

00:10:27,720 --> 00:10:33,840
give it to the reducer and reducer will

00:10:30,780 --> 00:10:36,000
aggregate the data and push into the

00:10:33,840 --> 00:10:37,500
output so that is how the parallel

00:10:36,000 --> 00:10:42,630
processing happens in the heart of

00:10:37,500 --> 00:10:46,530
MapReduce so so you need to write a

00:10:42,630 --> 00:10:49,650
mapper and reducer we call s so the road

00:10:46,530 --> 00:10:51,690
of MapReduce normally mostly written in

00:10:49,650 --> 00:10:54,300
Java and most of the progress will write

00:10:51,690 --> 00:10:56,190
will be written in Java but due to the

00:10:54,300 --> 00:11:01,350
popularity or something else like they

00:10:56,190 --> 00:11:02,920
hello pi Python and Ruby own you can

00:11:01,350 --> 00:11:06,250
write the programs and

00:11:02,920 --> 00:11:09,930
there is a new product streaming so that

00:11:06,250 --> 00:11:13,029
is a API for non Java programs so that

00:11:09,930 --> 00:11:15,100
is exposed and you can just write the

00:11:13,029 --> 00:11:17,800
program and you make use of the API so

00:11:15,100 --> 00:11:20,649
how the API works is that so you will

00:11:17,800 --> 00:11:24,690
have it they'll expose of blocks as a

00:11:20,649 --> 00:11:24,690
standard input to the your mapper and

00:11:24,810 --> 00:11:29,589
and then once you get the output the

00:11:28,180 --> 00:11:32,050
output is written into the standard

00:11:29,589 --> 00:11:38,019
output of the UNIX and that's how it

00:11:32,050 --> 00:11:41,589
happens right so now we will just look

00:11:38,019 --> 00:11:43,570
into the how we written the input so

00:11:41,589 --> 00:11:47,709
this problem statement like how you

00:11:43,570 --> 00:11:50,889
solve it so this is the format of the

00:11:47,709 --> 00:11:57,250
user input file so which is a space

00:11:50,889 --> 00:11:59,709
delimited with four columns with one so

00:11:57,250 --> 00:12:03,310
now I need to extract the data in

00:11:59,709 --> 00:12:07,300
english and i just need only the page

00:12:03,310 --> 00:12:10,240
name and the number of requests so

00:12:07,300 --> 00:12:13,899
column number two and three i do not

00:12:10,240 --> 00:12:15,459
require one and four so that is how the

00:12:13,899 --> 00:12:19,240
mappa does like we just filter the data

00:12:15,459 --> 00:12:22,120
with English and it's a very simple one

00:12:19,240 --> 00:12:23,860
okay so it is only like six lines of the

00:12:22,120 --> 00:12:26,350
seven lies of the program you are

00:12:23,860 --> 00:12:29,079
written and you just filtrate thought

00:12:26,350 --> 00:12:31,360
that ok whether it is a frequency is

00:12:29,079 --> 00:12:34,649
greater than 100 so why a chosen

00:12:31,360 --> 00:12:37,060
frequency is greater than 100 is that

00:12:34,649 --> 00:12:39,459
normally new top 10 you will not have a

00:12:37,060 --> 00:12:41,790
hand it definitely more than thousands

00:12:39,459 --> 00:12:44,050
so I just filtered more than 100 and

00:12:41,790 --> 00:12:45,850
project is English that is what you

00:12:44,050 --> 00:12:50,079
extract the data and there is a mapper

00:12:45,850 --> 00:12:51,550
output then you have a shuffle shuffle

00:12:50,079 --> 00:12:52,750
will be taken care by the Hadoop

00:12:51,550 --> 00:12:56,800
MapReduce you need not write anything

00:12:52,750 --> 00:12:58,510
for that so if you see here like you

00:12:56,800 --> 00:13:01,600
need to give the mapper output in a tab

00:12:58,510 --> 00:13:04,149
tab delimited tab delimited key and

00:13:01,600 --> 00:13:07,959
value pair so what shuffle does is that

00:13:04,149 --> 00:13:11,140
it will match all the output and sort it

00:13:07,959 --> 00:13:15,519
by the key here the key is the first

00:13:11,140 --> 00:13:16,329
column that is the page name and you

00:13:15,519 --> 00:13:19,269
have

00:13:16,329 --> 00:13:22,299
it will just sort it out by that so that

00:13:19,269 --> 00:13:25,269
is out reducer input so you can see here

00:13:22,299 --> 00:13:27,449
like so the same for example you are

00:13:25,269 --> 00:13:30,189
processing the data from the multiple

00:13:27,449 --> 00:13:32,139
hourly log files so each log file

00:13:30,189 --> 00:13:34,420
multiple log files can have the same

00:13:32,139 --> 00:13:36,790
keyword so that is where you see that in

00:13:34,420 --> 00:13:39,489
the output you have the two instances of

00:13:36,790 --> 00:13:42,179
a keyword and the two inches of a

00:13:39,489 --> 00:13:47,410
keyword zip code so this is how the

00:13:42,179 --> 00:13:49,509
input to the reducer then you have the

00:13:47,410 --> 00:13:51,639
then what you need to do is that we just

00:13:49,509 --> 00:13:55,089
need to sum up like you just read the

00:13:51,639 --> 00:13:57,699
line the line by line and the first till

00:13:55,089 --> 00:13:59,850
you I and counter the next keyword you

00:13:57,699 --> 00:14:02,290
just read it out and you sum up all the

00:13:59,850 --> 00:14:05,920
values and you just give it that is a

00:14:02,290 --> 00:14:10,629
reducer written it is a bit complicated

00:14:05,920 --> 00:14:14,860
than the mapper but it's easy one so you

00:14:10,629 --> 00:14:16,779
just try get the output that is a just

00:14:14,860 --> 00:14:19,299
to aggregate by each keyword like what

00:14:16,779 --> 00:14:20,889
is the total sum that is the output so

00:14:19,299 --> 00:14:23,949
from there you can pick up the data and

00:14:20,889 --> 00:14:26,559
you can just sort it by your values and

00:14:23,949 --> 00:14:29,669
you get at the output so that is how

00:14:26,559 --> 00:14:31,989
your mapper and reducer is written right

00:14:29,669 --> 00:14:33,279
so the only thing you need to do is it

00:14:31,989 --> 00:14:36,279
in addition you to write your mapper and

00:14:33,279 --> 00:14:39,519
reducer out of my producers will take

00:14:36,279 --> 00:14:41,379
care of the hard work entered logic of

00:14:39,519 --> 00:14:43,959
like splitting the files put

00:14:41,379 --> 00:14:47,319
distributing them and taking care of the

00:14:43,959 --> 00:14:49,540
tasks execution and ensure that hundred

00:14:47,319 --> 00:14:51,369
percent completion of the data in job

00:14:49,540 --> 00:14:56,049
and you get the output you need not

00:14:51,369 --> 00:14:59,470
worry about anything else right so but

00:14:56,049 --> 00:15:02,169
but still you need to write you need to

00:14:59,470 --> 00:15:04,149
create infrastructure like you have

00:15:02,169 --> 00:15:06,429
something called master node and corn or

00:15:04,149 --> 00:15:08,199
data no data nodes are the ones you put

00:15:06,429 --> 00:15:10,480
the data and you process the data and

00:15:08,199 --> 00:15:13,660
you have the master node which will keep

00:15:10,480 --> 00:15:17,709
track of where my files are distributed

00:15:13,660 --> 00:15:20,470
and also where how I am assigning the

00:15:17,709 --> 00:15:23,619
task to each of that each of the data

00:15:20,470 --> 00:15:27,369
node and keep track of like all the

00:15:23,619 --> 00:15:29,319
tasks are completed or not so still you

00:15:27,369 --> 00:15:30,220
need to install the software you need to

00:15:29,319 --> 00:15:33,550
configure your

00:15:30,220 --> 00:15:37,600
master node and corn oats we call as

00:15:33,550 --> 00:15:40,360
data nodes and that is a very plenty of

00:15:37,600 --> 00:15:41,800
effort requires even though out of my

00:15:40,360 --> 00:15:44,830
abilities provides a lot of conversation

00:15:41,800 --> 00:15:47,680
you know in the bolts are not still you

00:15:44,830 --> 00:15:50,980
need to configure it so for a beginner

00:15:47,680 --> 00:15:53,830
like me if I want to explain aim is it

00:15:50,980 --> 00:15:56,650
to explore the data rather than do not

00:15:53,830 --> 00:15:58,360
worry about what inside like how much

00:15:56,650 --> 00:16:02,530
effort required to create the

00:15:58,360 --> 00:16:04,390
infrastructure for me so for that so you

00:16:02,530 --> 00:16:06,520
have an Amazon Cloud I think everyone is

00:16:04,390 --> 00:16:09,430
aware of them is in cloud so they have

00:16:06,520 --> 00:16:12,820
Amazon EMR elastic MapReduce ER so which

00:16:09,430 --> 00:16:16,750
is the service provided by the Amazon

00:16:12,820 --> 00:16:18,610
where you just need to define how many

00:16:16,750 --> 00:16:21,220
master know how many code notes you

00:16:18,610 --> 00:16:24,670
require and what type of the instance we

00:16:21,220 --> 00:16:28,030
call is in ec2 is a virtual server they

00:16:24,670 --> 00:16:29,830
call is ec2 so ec2 instance type so you

00:16:28,030 --> 00:16:36,180
have a different types of the instances

00:16:29,830 --> 00:16:36,180
in amazon so i will just show you

00:16:41,160 --> 00:16:48,089
so this is the different types of the

00:16:43,470 --> 00:16:50,940
instances they have so pi by each one is

00:16:48,089 --> 00:16:54,240
having a different CP number of CPUs ram

00:16:50,940 --> 00:16:57,029
and hard disk based on that they will

00:16:54,240 --> 00:16:59,879
charge per hour basis like this so if I

00:16:57,029 --> 00:17:05,419
am just see for example C 3 2 X logic

00:16:59,879 --> 00:17:11,850
which is approximately around 28 CPU and

00:17:05,419 --> 00:17:15,169
around 800 gb hard disk SSD and almost

00:17:11,850 --> 00:17:19,140
like 50 or something like the gb of ram

00:17:15,169 --> 00:17:25,699
so that cause almost like a point for

00:17:19,140 --> 00:17:28,650
dollars per hour all right so sorry okay

00:17:25,699 --> 00:17:31,620
so you just need to give them and it on

00:17:28,650 --> 00:17:35,190
the amazon will take care of configuring

00:17:31,620 --> 00:17:37,830
the cluster for me then next what i need

00:17:35,190 --> 00:17:41,309
to do is that I just need to provide

00:17:37,830 --> 00:17:43,559
where is my mapper program where is my

00:17:41,309 --> 00:17:46,890
reducer program and where is my input

00:17:43,559 --> 00:17:48,299
file location folder location and where

00:17:46,890 --> 00:17:51,270
I want the output location that is it

00:17:48,299 --> 00:17:54,419
even an inert log into that cluster and

00:17:51,270 --> 00:17:56,880
do anything on that all right so it is

00:17:54,419 --> 00:17:59,820
as as simple as like you have a GUI you

00:17:56,880 --> 00:18:01,650
just need to click and self-explanatory

00:17:59,820 --> 00:18:05,250
you just use my upper reducer all these

00:18:01,650 --> 00:18:06,840
things that's it and that's it okay so

00:18:05,250 --> 00:18:11,690
this is the sort of like a theory part

00:18:06,840 --> 00:18:11,690
now let's go and lose quick demo on that

00:18:18,640 --> 00:18:23,830
so you before going into that in Amazon

00:18:22,030 --> 00:18:27,070
you have something called s3 that is the

00:18:23,830 --> 00:18:31,240
place you store your data it is like a

00:18:27,070 --> 00:18:33,010
external extension to your hard disk it

00:18:31,240 --> 00:18:36,040
can act as a like hard disk to your

00:18:33,010 --> 00:18:39,070
virtual server created in your Amazon so

00:18:36,040 --> 00:18:41,620
there they call it like a buckets okay

00:18:39,070 --> 00:18:45,340
so you create your own bucket it is a

00:18:41,620 --> 00:18:47,950
unique across the world of the users so

00:18:45,340 --> 00:18:53,100
I created a packet called Baptist map

00:18:47,950 --> 00:18:57,160
and I created an input file input folder

00:18:53,100 --> 00:19:01,470
and i just downloaded all the data from

00:18:57,160 --> 00:19:05,590
the wikipedia so all the files like I

00:19:01,470 --> 00:19:09,880
created a one instance I created one

00:19:05,590 --> 00:19:12,070
virtual server like in ec2 that is like

00:19:09,880 --> 00:19:15,430
virtual server and just downloaded the

00:19:12,070 --> 00:19:17,080
files to the ec2 it is a free of cost

00:19:15,430 --> 00:19:19,660
like I think of like you download the

00:19:17,080 --> 00:19:21,790
300 gb of data into your local mission

00:19:19,660 --> 00:19:23,890
so instead of think of like how much

00:19:21,790 --> 00:19:25,480
bandwidth it will cost for you so but

00:19:23,890 --> 00:19:27,760
here like you can just download free of

00:19:25,480 --> 00:19:29,020
cost the bandwidth is completely free

00:19:27,760 --> 00:19:32,890
downloading anything from the

00:19:29,020 --> 00:19:34,770
internetter to the your amazon so I was

00:19:32,890 --> 00:19:36,790
not I haven't paid even a single rupee

00:19:34,770 --> 00:19:39,070
for downloading the data from the

00:19:36,790 --> 00:19:42,310
wikipedia so I downloaded all the files

00:19:39,070 --> 00:19:48,760
and uncompressed them and push into the

00:19:42,310 --> 00:19:52,440
s3 so this is my input folder and I

00:19:48,760 --> 00:19:55,620
created another folder called output and

00:19:52,440 --> 00:19:55,620
all right

00:19:57,580 --> 00:20:05,200
this is fine thank you all right so i

00:20:02,230 --> 00:20:07,780
created an input folder of output folder

00:20:05,200 --> 00:20:10,330
where I want the output and where I have

00:20:07,780 --> 00:20:13,240
the scripts so whatever I showed you the

00:20:10,330 --> 00:20:17,380
script I just put that in the mapper and

00:20:13,240 --> 00:20:22,330
reducer that's it so now I just go to

00:20:17,380 --> 00:20:26,290
the elastic MapReduce ER so I'll just to

00:20:22,330 --> 00:20:28,620
do the cluster create cluster and I will

00:20:26,290 --> 00:20:31,620
just give the names all these things and

00:20:28,620 --> 00:20:31,620
then

00:20:39,240 --> 00:20:44,550
so I just need to give what is my

00:20:42,570 --> 00:20:46,910
configuration so if you can see here

00:20:44,550 --> 00:20:49,710
like you have a different types of the

00:20:46,910 --> 00:20:52,650
instance types so they have a compute

00:20:49,710 --> 00:20:54,780
optimized memory optimized and store is

00:20:52,650 --> 00:20:57,800
optimized if you see the different

00:20:54,780 --> 00:20:57,800
configuration of that

00:21:20,559 --> 00:21:31,460
okay it will takes few seconds to load

00:21:24,380 --> 00:21:34,640
the pricing data yeah so you can see

00:21:31,460 --> 00:21:37,010
here like T to micro is a very smallest

00:21:34,640 --> 00:21:41,480
one where you have the memory of one gig

00:21:37,010 --> 00:21:48,970
and it costs approximately one rupee per

00:21:41,480 --> 00:21:53,740
hour vs. the top post is almost like

00:21:48,970 --> 00:21:58,159
c-38 x-large which is 32 cpu 60 gb and

00:21:53,740 --> 00:22:01,720
620 is gb SSD which caused almost like

00:21:58,159 --> 00:22:01,720
this much okay all right

00:22:07,430 --> 00:22:15,380
so I just need to provide the how much

00:22:11,050 --> 00:22:18,590
sigh how many number of instances I need

00:22:15,380 --> 00:22:21,050
for core and then I just need to as I

00:22:18,590 --> 00:22:24,730
mentioned previously like I just need to

00:22:21,050 --> 00:22:27,860
select my step as a streaming program

00:22:24,730 --> 00:22:33,530
configure and add and I just need to

00:22:27,860 --> 00:22:43,910
give my location where my map / script

00:22:33,530 --> 00:22:46,180
is there and where my reducer script is

00:22:43,910 --> 00:22:46,180
there

00:22:48,870 --> 00:22:59,250
and where is my input file so let us I

00:22:53,520 --> 00:23:02,820
want input file and September and I just

00:22:59,250 --> 00:23:08,850
select it and I just need to view the

00:23:02,820 --> 00:23:10,920
output location so this s3 will be there

00:23:08,850 --> 00:23:13,020
even though your server is discount

00:23:10,920 --> 00:23:16,380
become you are terminate or something

00:23:13,020 --> 00:23:18,230
which is available for you so it is as

00:23:16,380 --> 00:23:21,630
if like your google drive or something

00:23:18,230 --> 00:23:25,460
so I just need to create a and their

00:23:21,630 --> 00:23:28,140
output September five September output

00:23:25,460 --> 00:23:32,820
that's it and I just need to click on

00:23:28,140 --> 00:23:37,910
add right so once I do the ad then what

00:23:32,820 --> 00:23:44,880
happens is because it takes some time

00:23:37,910 --> 00:23:47,070
almost like I haven't okay around the

00:23:44,880 --> 00:23:49,500
net so it takes almost like eight

00:23:47,070 --> 00:23:52,770
minutes to provision that hardware for

00:23:49,500 --> 00:23:54,420
you and configure it right which I

00:23:52,770 --> 00:23:56,750
already created the cluster for this

00:23:54,420 --> 00:23:56,750
demo

00:24:01,960 --> 00:24:07,590
so I already created a cluster

00:24:12,350 --> 00:24:19,910
with the c32 x-large of six nodes each

00:24:16,210 --> 00:24:23,360
there are six code roads and one for

00:24:19,910 --> 00:24:27,950
master node and I just need to add the

00:24:23,360 --> 00:24:31,010
steps here so since morning I am just

00:24:27,950 --> 00:24:35,270
running a different jobs 1 is all the

00:24:31,010 --> 00:24:37,880
sep tember files top 10 French pages top

00:24:35,270 --> 00:24:42,799
10 Dutch pages all these things so

00:24:37,880 --> 00:24:52,390
currently all Joel a Dutch job is

00:24:42,799 --> 00:24:52,390
running so I can just show the jobs

00:24:53,850 --> 00:25:00,240
and use a tasks so which is bit

00:24:58,590 --> 00:25:03,299
misleading but what you can say that you

00:25:00,240 --> 00:25:06,720
have a almost like 11 said that 1173

00:25:03,299 --> 00:25:11,190
pending tasks so it will show all the

00:25:06,720 --> 00:25:14,490
mapper and runner my apparent reducer

00:25:11,190 --> 00:25:17,309
ease were executing here so it will show

00:25:14,490 --> 00:25:22,010
most of the data for you so even if you

00:25:17,309 --> 00:25:22,010
see here jobs

00:25:27,280 --> 00:25:31,420
so I can this is the one previously come

00:25:29,620 --> 00:25:36,250
okay this is the one previously

00:25:31,420 --> 00:25:39,640
completed for all July fringe so I can

00:25:36,250 --> 00:25:44,200
see the output of that so if you see is

00:25:39,640 --> 00:25:46,990
log which will tell which will show you

00:25:44,200 --> 00:25:49,210
like a quick log of like or not it is

00:25:46,990 --> 00:25:51,550
just executing 78% seventy-nine percent

00:25:49,210 --> 00:25:55,590
all these things and it will give you

00:25:51,550 --> 00:25:55,590
the quick matrix of like how many

00:26:01,490 --> 00:26:09,440
how many bytes read how many bytes

00:26:04,940 --> 00:26:12,380
written and you can see here like this

00:26:09,440 --> 00:26:13,960
many input records are processed first

00:26:12,380 --> 00:26:16,460
part of that almost like a five billion

00:26:13,960 --> 00:26:18,830
records processed and these are the

00:26:16,460 --> 00:26:21,260
output records we got it so all the

00:26:18,830 --> 00:26:25,040
matrix it will be provided so and also

00:26:21,260 --> 00:26:27,679
like Hadoop MapReduce provides a web

00:26:25,040 --> 00:26:29,929
interface so what what you just need to

00:26:27,679 --> 00:26:33,650
do is that you do you need to do the ssh

00:26:29,929 --> 00:26:36,590
tunneling to the the master node which I

00:26:33,650 --> 00:26:39,920
already did and this is the Hadoop

00:26:36,590 --> 00:26:42,050
interface or web connector application

00:26:39,920 --> 00:26:44,480
so there you can see what is the current

00:26:42,050 --> 00:26:49,640
job and what is the current status so if

00:26:44,480 --> 00:26:55,850
I see here so this is the job currently

00:26:49,640 --> 00:27:04,429
we are executing so so you can see that

00:26:55,850 --> 00:27:06,320
money of total and and how many are

00:27:04,429 --> 00:27:10,340
pending how many are running you can

00:27:06,320 --> 00:27:12,800
just see the completed ones and it takes

00:27:10,340 --> 00:27:14,630
some time to download so you can see

00:27:12,800 --> 00:27:16,940
that what exactly it is doing and how

00:27:14,630 --> 00:27:19,700
much time it took you can see that each

00:27:16,940 --> 00:27:24,080
map a task is executed for 10 seconds

00:27:19,700 --> 00:27:27,170
approximately and you can see some

00:27:24,080 --> 00:27:29,530
counters like when you define a map

00:27:27,170 --> 00:27:33,200
there is a by default there is a mappers

00:27:29,530 --> 00:27:34,640
mapper definition like this much of the

00:27:33,200 --> 00:27:36,950
memory I need to be allocated to each

00:27:34,640 --> 00:27:39,230
map a task or these are the by default

00:27:36,950 --> 00:27:41,780
values you can also change that values

00:27:39,230 --> 00:27:44,450
and uni not for that Union at login to

00:27:41,780 --> 00:27:46,280
the cluster and configure it you just

00:27:44,450 --> 00:27:50,770
need to change it from there you I only

00:27:46,280 --> 00:27:53,480
so these are the different counters and

00:27:50,770 --> 00:27:55,520
these are this whichever whatever I

00:27:53,480 --> 00:27:56,720
showed you in the syslog and also these

00:27:55,520 --> 00:27:59,059
are the different configuration

00:27:56,720 --> 00:28:03,230
parameters you can see that almost like

00:27:59,059 --> 00:28:05,450
there are 7 16 parameters in the Hadoop

00:28:03,230 --> 00:28:08,150
which you can configure like what is a

00:28:05,450 --> 00:28:10,910
block size so one more important thing

00:28:08,150 --> 00:28:13,429
is that we normally like the block size

00:28:10,910 --> 00:28:15,530
in how to place more than 64 mb the

00:28:13,429 --> 00:28:19,070
reason is that if you

00:28:15,530 --> 00:28:22,220
the time to seek from the hard disk

00:28:19,070 --> 00:28:24,860
should be less than the time you took to

00:28:22,220 --> 00:28:27,230
read the file entire file you should not

00:28:24,860 --> 00:28:29,900
have more of our head on your seek time

00:28:27,230 --> 00:28:32,270
so that is the reason you we give

00:28:29,900 --> 00:28:35,090
minimum 64mb and the block says can

00:28:32,270 --> 00:28:37,940
increase further but you can configure

00:28:35,090 --> 00:28:39,530
like so initially I did a trial and

00:28:37,940 --> 00:28:42,470
error like I gave a different types of

00:28:39,530 --> 00:28:44,030
the nodes different types and how I will

00:28:42,470 --> 00:28:46,730
see that okay is it like a memory

00:28:44,030 --> 00:28:48,290
intensive or it is like a CPU intensive

00:28:46,730 --> 00:28:50,750
all these things you just need to

00:28:48,290 --> 00:28:54,320
monitor trial and error and come up with

00:28:50,750 --> 00:28:57,440
a optimized number of nodes and what

00:28:54,320 --> 00:28:59,480
type of the instance type I need to have

00:28:57,440 --> 00:29:01,970
is it like how much memory I need to

00:28:59,480 --> 00:29:04,760
have and what is what should be my

00:29:01,970 --> 00:29:08,180
memory per each map at ask all these

00:29:04,760 --> 00:29:14,720
things we need to configure it all right

00:29:08,180 --> 00:29:17,780
so so that's about the demo so that is

00:29:14,720 --> 00:29:22,400
how i got the output so here you can see

00:29:17,780 --> 00:29:23,780
that i haven't done any configuration or

00:29:22,400 --> 00:29:26,210
anything i just need to write a mapper

00:29:23,780 --> 00:29:31,870
and reducer and i use in my local pi

00:29:26,210 --> 00:29:34,730
ipython to test it again is that so i

00:29:31,870 --> 00:29:37,460
tested again is to the local ipython and

00:29:34,730 --> 00:29:40,190
the mapper and reducer and put it there

00:29:37,460 --> 00:29:42,590
that's it and i need not log into any

00:29:40,190 --> 00:29:45,830
mission and do the configuration of

00:29:42,590 --> 00:29:56,540
anything else ok so that is a beauty of

00:29:45,830 --> 00:30:01,130
this so so thats it on the demo now this

00:29:56,540 --> 00:30:03,050
is the one i learnt in one of the online

00:30:01,130 --> 00:30:06,410
course there is something called

00:30:03,050 --> 00:30:08,930
Simpsons paradox so what exactly it

00:30:06,410 --> 00:30:10,910
tells is that like you know so depends

00:30:08,930 --> 00:30:14,000
upon how deep you are looking into the

00:30:10,910 --> 00:30:18,050
data the insights of the data will

00:30:14,000 --> 00:30:20,960
change alright so because now going

00:30:18,050 --> 00:30:24,110
forward we will be we will be exploring

00:30:20,960 --> 00:30:26,210
lee in your day job day in day out job

00:30:24,110 --> 00:30:28,650
you will be dealing with most of the

00:30:26,210 --> 00:30:30,750
data like a huge data

00:30:28,650 --> 00:30:33,630
and how to interpret the data is very

00:30:30,750 --> 00:30:35,850
important so if this is called Simpson

00:30:33,630 --> 00:30:38,250
paradox if you see the example here I

00:30:35,850 --> 00:30:41,700
think everyone can see here like there

00:30:38,250 --> 00:30:43,920
is an a University where you have a two

00:30:41,700 --> 00:30:47,310
major courses major here and measured be

00:30:43,920 --> 00:30:50,310
and you have people applied for from

00:30:47,310 --> 00:30:53,220
different gender and if you see major a

00:30:50,310 --> 00:30:59,430
and major be you can see that acceptance

00:30:53,220 --> 00:31:02,370
rate is higher for one gender versus and

00:30:59,430 --> 00:31:04,920
the gender but if you combine both of

00:31:02,370 --> 00:31:06,990
them right if you just look into like

00:31:04,920 --> 00:31:11,100
combine both the Missouri and major B

00:31:06,990 --> 00:31:15,120
and project your acceptance rate my

00:31:11,100 --> 00:31:16,620
acceptance rate will reverse it right so

00:31:15,120 --> 00:31:19,500
now you can see that the different

00:31:16,620 --> 00:31:22,730
gender have the higher acceptance rate

00:31:19,500 --> 00:31:26,550
than the other agenda so this is called

00:31:22,730 --> 00:31:29,190
Simpsons paradox so that should be very

00:31:26,550 --> 00:31:31,760
important if you know looking into the

00:31:29,190 --> 00:31:35,760
data in your day-to-day job right so

00:31:31,760 --> 00:31:37,890
using this is this is one of the

00:31:35,760 --> 00:31:40,340
information i extracted from the

00:31:37,890 --> 00:31:42,780
wikipedia but there are n number of

00:31:40,340 --> 00:31:44,940
insights you can get it from wikipedia

00:31:42,780 --> 00:31:47,310
because it is a more of accurate

00:31:44,940 --> 00:31:51,180
information representing what is the

00:31:47,310 --> 00:31:53,730
users currently trying to do it in the

00:31:51,180 --> 00:31:55,470
internet and using that you can make use

00:31:53,730 --> 00:31:59,670
of a lot of applications on top of that

00:31:55,470 --> 00:32:02,010
and make use of that as the inputs other

00:31:59,670 --> 00:32:04,110
than that there is one more community

00:32:02,010 --> 00:32:06,720
driven so the wikipedia is also like a

00:32:04,110 --> 00:32:08,310
community-driven website similarly like

00:32:06,720 --> 00:32:11,130
there is a another community driven big

00:32:08,310 --> 00:32:12,390
data set we have I don't know whether

00:32:11,130 --> 00:32:14,900
everyone knows about this there is

00:32:12,390 --> 00:32:19,290
something called India or I'll info.com

00:32:14,900 --> 00:32:23,640
so it is also like a a community driven

00:32:19,290 --> 00:32:25,890
by data set so okay I think you can scan

00:32:23,640 --> 00:32:28,020
see that so I am trying to search it

00:32:25,890 --> 00:32:29,880
this is a community-driven even we don't

00:32:28,020 --> 00:32:32,190
know like who is the founder for that

00:32:29,880 --> 00:32:34,320
website so it gives like you just to

00:32:32,190 --> 00:32:36,180
give the source and dish nation like I

00:32:34,320 --> 00:32:41,880
want to go from Bangalore Chennai it

00:32:36,180 --> 00:32:42,600
will give you the different trains with

00:32:41,880 --> 00:32:45,539
star time

00:32:42,600 --> 00:32:49,590
anti arrival and departure time with the

00:32:45,539 --> 00:32:52,200
hour to the average delay of arrival and

00:32:49,590 --> 00:32:54,570
departure and also you have the

00:32:52,200 --> 00:32:57,120
community community users like

00:32:54,570 --> 00:33:00,840
volunteers they will create other paths

00:32:57,120 --> 00:33:02,610
for example like you can there is a time

00:33:00,840 --> 00:33:04,530
which goes directly from Bangalore

00:33:02,610 --> 00:33:06,960
Chennai that is what you will see in the

00:33:04,530 --> 00:33:09,120
top two records it's actually the

00:33:06,960 --> 00:33:11,789
Bangalore to pondicherry but the other

00:33:09,120 --> 00:33:14,190
ones are created by the community say

00:33:11,789 --> 00:33:16,350
that ok you can go from Bangalore to

00:33:14,190 --> 00:33:18,570
some madre and from Madurai to another

00:33:16,350 --> 00:33:20,340
one with a delay of this much all these

00:33:18,570 --> 00:33:23,340
things are created by the community ok

00:33:20,340 --> 00:33:27,090
it is a very big used data set if people

00:33:23,340 --> 00:33:29,820
want to explore it it's very very

00:33:27,090 --> 00:33:32,400
accurate information and you can get lot

00:33:29,820 --> 00:33:37,230
of insights on that ok so i will just

00:33:32,400 --> 00:33:39,480
give you one example here so i'll just

00:33:37,230 --> 00:33:42,179
try to give the difference between that

00:33:39,480 --> 00:33:44,970
community driven website versus a

00:33:42,179 --> 00:33:46,679
commercial travel aggregator so think of

00:33:44,970 --> 00:33:49,679
like i am going to from generate a

00:33:46,679 --> 00:33:52,860
bangalore so is it make sense to go

00:33:49,679 --> 00:33:56,700
through by a Mumbai with a very time of

00:33:52,860 --> 00:33:58,620
18 hours so that is what your commercial

00:33:56,700 --> 00:34:04,230
traveller greh Gator will provide which

00:33:58,620 --> 00:34:05,909
is no making no sense right so when you

00:34:04,230 --> 00:34:09,330
try to search for Bangalore Chennai

00:34:05,909 --> 00:34:11,010
Bangalore it does not make sense sir

00:34:09,330 --> 00:34:13,740
Bangalore Chennai it does not make sense

00:34:11,010 --> 00:34:16,889
to good travel tamoom Mumbai and from

00:34:13,740 --> 00:34:20,520
there wait overnight and come back in

00:34:16,889 --> 00:34:22,710
and the flight but where I see it is

00:34:20,520 --> 00:34:24,210
almost like 18 hours plus almost like

00:34:22,710 --> 00:34:25,800
three hours so you are taking 20 hours

00:34:24,210 --> 00:34:27,659
to travel from one place to another

00:34:25,800 --> 00:34:30,690
place which is hardly takes six hours

00:34:27,659 --> 00:34:32,129
even better on right that that is a

00:34:30,690 --> 00:34:33,840
normal commercial traveller greh Gator

00:34:32,129 --> 00:34:34,950
which is mainly depends upon like you do

00:34:33,840 --> 00:34:38,780
the data mining and provide the

00:34:34,950 --> 00:34:42,300
information whereas a community website

00:34:38,780 --> 00:34:44,700
like India irrelevant for calm you can

00:34:42,300 --> 00:34:47,280
see that the maximal wait time is almost

00:34:44,700 --> 00:34:51,419
like less than four of us when i'm

00:34:47,280 --> 00:34:55,200
trying to get from bangalore to circa

00:34:51,419 --> 00:34:55,740
which one pondicherry right so if you

00:34:55,200 --> 00:34:58,320
can see

00:34:55,740 --> 00:35:00,300
last but column that is the users with

00:34:58,320 --> 00:35:07,190
the community who created the user

00:35:00,300 --> 00:35:07,190
created that row of the data right yep

00:35:09,410 --> 00:35:20,360
okay sure sure yep so oh that's it ok

00:35:17,280 --> 00:35:23,730
I'm done sorry yeah so I am done with my

00:35:20,360 --> 00:35:26,670
representation so these are the tools

00:35:23,730 --> 00:35:28,619
are used it and you can get a there is a

00:35:26,670 --> 00:35:30,860
github link which this link is already

00:35:28,619 --> 00:35:34,440
available in your Python soap icon

00:35:30,860 --> 00:35:37,290
proposal you can get it and you can get

00:35:34,440 --> 00:35:39,990
all the step by steps right from install

00:35:37,290 --> 00:35:43,440
creating the accountant amazon AWS and

00:35:39,990 --> 00:35:45,840
right from installing the python ipython

00:35:43,440 --> 00:35:49,010
all the tools on all the steps are

00:35:45,840 --> 00:35:54,480
available there you can also explore a

00:35:49,010 --> 00:35:58,400
big data using that so that's it from my

00:35:54,480 --> 00:36:03,030
side so if anyone have any questions

00:35:58,400 --> 00:36:05,670
please let me know any questions yeah so

00:36:03,030 --> 00:36:08,760
you said for the English data there is a

00:36:05,670 --> 00:36:11,609
gb of data right sorry for english

00:36:08,760 --> 00:36:14,960
english data from realizing how long did

00:36:11,609 --> 00:36:18,750
it take to process the entire 300 gb Oh

00:36:14,960 --> 00:36:20,760
even though it is running so okay it

00:36:18,750 --> 00:36:23,670
took almost like it depends upon how

00:36:20,760 --> 00:36:25,080
many notes you give so due to some

00:36:23,670 --> 00:36:27,390
mother budget constraints and other

00:36:25,080 --> 00:36:30,150
things I just give six nodes because it

00:36:27,390 --> 00:36:33,710
is scalable if I give so if I gave only

00:36:30,150 --> 00:36:36,390
six nodes of this type so it took hardly

00:36:33,710 --> 00:36:38,490
40 minutes 46 minutes something with it

00:36:36,390 --> 00:36:42,300
and out of MapReduce is linearly

00:36:38,490 --> 00:36:45,420
scalable if I give almost like a instead

00:36:42,300 --> 00:36:48,300
of six nodes you can give 12 loads it

00:36:45,420 --> 00:36:52,440
can process in 20 minutes at the same

00:36:48,300 --> 00:36:56,070
costly no now I think no machines lesser

00:36:52,440 --> 00:36:58,560
time yeah actually I'm SNP asked you to

00:36:56,070 --> 00:37:01,440
pay per hour whereas maybe I hit the

00:36:58,560 --> 00:37:06,300
other cloud guys are giving per second

00:37:01,440 --> 00:37:14,230
also in exertion and

00:37:06,300 --> 00:37:17,770
questions hi third row here yeah my

00:37:14,230 --> 00:37:20,260
question is how well does Python work

00:37:17,770 --> 00:37:22,240
when using Hadoop when compared to java

00:37:20,260 --> 00:37:24,250
yeah actually when I did some research

00:37:22,240 --> 00:37:26,950
on this big I know I don't know the Java

00:37:24,250 --> 00:37:32,370
and all but for a normal text processing

00:37:26,950 --> 00:37:36,160
and all it is comparatively okay but its

00:37:32,370 --> 00:37:39,610
performance wise it is you know it is on

00:37:36,160 --> 00:37:42,400
the downside but for me at the car

00:37:39,610 --> 00:37:43,990
doesn't matter for me as I like but

00:37:42,400 --> 00:37:46,240
still you can do is a lot of tuning on

00:37:43,990 --> 00:37:49,360
this part yeah I need to still work on

00:37:46,240 --> 00:37:52,630
that thank you photo session and I have

00:37:49,360 --> 00:37:55,210
one question here yeah somewhere in my

00:37:52,630 --> 00:37:58,540
organization I have seen people using

00:37:55,210 --> 00:38:01,840
the light Twitter feeds and created

00:37:58,540 --> 00:38:05,260
beautiful websites or apps using the

00:38:01,840 --> 00:38:09,460
angularjs or something so I ease it also

00:38:05,260 --> 00:38:12,630
does it uses the MapReduce to process

00:38:09,460 --> 00:38:15,670
the traitor live feeds or how is it I

00:38:12,630 --> 00:38:18,280
don't know much about that but I what I

00:38:15,670 --> 00:38:21,210
feel is it and they call it like a spark

00:38:18,280 --> 00:38:23,620
storm and all these things okay the main

00:38:21,210 --> 00:38:26,200
fundamental concepts is still you have a

00:38:23,620 --> 00:38:28,090
like Hadoop MapReduce is what I feel

00:38:26,200 --> 00:38:30,520
okay okay but I'm not the expert to

00:38:28,090 --> 00:38:36,040
comment on that car thank you and still

00:38:30,520 --> 00:38:40,040
I am learner ok any questions questions

00:38:36,040 --> 00:38:46,070
guys ok

00:38:40,040 --> 00:38:49,010
that's it alright thanks very much and

00:38:46,070 --> 00:38:51,410
you have good asset u.com like most of

00:38:49,010 --> 00:38:54,110
your data science courses and all the

00:38:51,410 --> 00:38:57,380
things you can learn in audacity com

00:38:54,110 --> 00:39:02,030
most of that website tutorials are based

00:38:57,380 --> 00:39:03,590
on Python whether it is a analytics

00:39:02,030 --> 00:39:06,560
whether it is a like artificial

00:39:03,590 --> 00:39:09,350
intelligence or robotics whatever it is

00:39:06,560 --> 00:39:13,220
mostly it is they will give a demo in

00:39:09,350 --> 00:39:15,740
Python okay small announcement at 4 45

00:39:13,220 --> 00:39:18,290
we are having a feedback session guys

00:39:15,740 --> 00:39:21,650
you can stay and give all this your

00:39:18,290 --> 00:39:24,370
feedback how was it an order until that

00:39:21,650 --> 00:39:27,050
time we are having a lightning talk

00:39:24,370 --> 00:39:31,390
please come ahead if anyone wants to

00:39:27,050 --> 00:39:34,370
give please come ahead come on guys or

00:39:31,390 --> 00:39:37,940
for this writing talk we won't be having

00:39:34,370 --> 00:39:40,010
the projector setup you just need to

00:39:37,940 --> 00:39:50,360
come here and talk that's it for three

00:39:40,010 --> 00:39:54,710
minutes I'm here to good evening good

00:39:50,360 --> 00:39:59,270
evening so exhausted energy guys energy

00:39:54,710 --> 00:40:06,440
good evening china p will you DJ Jamie

00:39:59,270 --> 00:40:10,760
Lee okay so I'm here to just share an

00:40:06,440 --> 00:40:12,670
experience and the I've got the points

00:40:10,760 --> 00:40:17,720
of here and you don't give me slides

00:40:12,670 --> 00:40:19,520
okay so why did you guys the talk the

00:40:17,720 --> 00:40:21,170
talk is about how to how to get the most

00:40:19,520 --> 00:40:23,660
out of pike on and how to have a good

00:40:21,170 --> 00:40:26,000
time over here so most of you came here

00:40:23,660 --> 00:40:28,510
with an expectation of having of getting

00:40:26,000 --> 00:40:31,340
to attend very great talks and having

00:40:28,510 --> 00:40:33,200
awesome like you know getting to learn I

00:40:31,340 --> 00:40:34,970
don't know everything that is there to

00:40:33,200 --> 00:40:36,950
learn in just three or four just just

00:40:34,970 --> 00:40:38,900
just two days with two are within two

00:40:36,950 --> 00:40:41,000
hour sessions probably get our expertise

00:40:38,900 --> 00:40:42,020
in Hadoop and maybe learn everything

00:40:41,000 --> 00:40:45,830
that is there to know about meta

00:40:42,020 --> 00:40:48,980
programming to be very fair that's quite

00:40:45,830 --> 00:40:51,920
impossible you will get a basic idea but

00:40:48,980 --> 00:40:53,150
not the whole deal that requires a lot

00:40:51,920 --> 00:40:56,299
of effort so

00:40:53,150 --> 00:40:59,089
my strategy and attending pycon and you

00:40:56,299 --> 00:41:01,099
know having getting to know and getting

00:40:59,089 --> 00:41:03,829
to her you know having a good time over

00:41:01,099 --> 00:41:06,380
here is is to follow a couple of you

00:41:03,829 --> 00:41:08,720
know mantras what I try to do is

00:41:06,380 --> 00:41:11,119
interact with people as much as i can

00:41:08,720 --> 00:41:12,559
make friends just bump into random

00:41:11,119 --> 00:41:14,990
strangers and say hello hey who are you

00:41:12,559 --> 00:41:16,490
what do you do I mean just you might

00:41:14,990 --> 00:41:18,289
just make very good friends you might

00:41:16,490 --> 00:41:21,170
get to know about people who are doing

00:41:18,289 --> 00:41:23,440
great work in their own offices and are

00:41:21,170 --> 00:41:25,010
changing the world probably and you know

00:41:23,440 --> 00:41:28,010
that could be a very good experience

00:41:25,010 --> 00:41:30,020
other than that knowing people and

00:41:28,010 --> 00:41:31,849
getting known yourselves can help you

00:41:30,020 --> 00:41:33,349
not only solve your problems in personal

00:41:31,849 --> 00:41:34,819
life as in the professional technical

00:41:33,349 --> 00:41:37,849
problems that you face at your job but

00:41:34,819 --> 00:41:40,190
also in you know getting job

00:41:37,849 --> 00:41:42,200
opportunities may people are looking for

00:41:40,190 --> 00:41:44,299
good talent every time not just the

00:41:42,200 --> 00:41:47,000
sponsors but almost everyone who is

00:41:44,299 --> 00:41:48,859
working in any job is looking for good

00:41:47,000 --> 00:41:51,170
talent to get into their team I know I

00:41:48,859 --> 00:41:54,349
am and I mean that that's a universal

00:41:51,170 --> 00:41:56,809
problem get good talent so if people

00:41:54,349 --> 00:41:58,549
know you that you're great you'll land

00:41:56,809 --> 00:42:00,319
in multiple job offers you'll be

00:41:58,549 --> 00:42:06,980
drowning and job offers if you're well

00:42:00,319 --> 00:42:10,220
known so clean and South karam and apart

00:42:06,980 --> 00:42:12,670
from the firing thing having a good time

00:42:10,220 --> 00:42:17,240
and being a part of the initiative is a

00:42:12,670 --> 00:42:18,799
is a is a golden opportunity you get out

00:42:17,240 --> 00:42:22,849
of Python because it's a voluntary or

00:42:18,799 --> 00:42:24,079
voluntary initiator so you have an

00:42:22,849 --> 00:42:25,789
opportunity to be a part of the

00:42:24,079 --> 00:42:27,740
organizing team you can help out in any

00:42:25,789 --> 00:42:29,420
which way which you can like you think

00:42:27,740 --> 00:42:31,430
you can help out with the network before

00:42:29,420 --> 00:42:33,410
cursing out loud salon at each other bae

00:42:31,430 --> 00:42:35,599
carlo a cuchini got a net an etiologic

00:42:33,410 --> 00:42:37,430
there yeah a confidence kz conference

00:42:35,599 --> 00:42:39,349
Karev hi a job come along we will

00:42:37,430 --> 00:42:40,880
welcome you if you can help us out your

00:42:39,349 --> 00:42:42,740
SS admin you know how to set up router

00:42:40,880 --> 00:42:44,119
us you know how to lay out cables come

00:42:42,740 --> 00:42:46,549
along help us you think the talk

00:42:44,119 --> 00:42:48,440
selection was bad pathetic boring talks

00:42:46,549 --> 00:42:49,970
we were selected dis time be a part of

00:42:48,440 --> 00:42:51,950
the talk selection committee next time

00:42:49,970 --> 00:42:54,980
it's people like you who are doing it so

00:42:51,950 --> 00:42:58,700
well why not you just be a part of it

00:42:54,980 --> 00:43:00,529
and well in any which way so well

00:42:58,700 --> 00:43:03,680
hopefully you had a good time this this

00:43:00,529 --> 00:43:04,760
year and you'll be a better I'm you will

00:43:03,680 --> 00:43:06,260
you will involve more cells you arm

00:43:04,760 --> 00:43:08,270
yourself more in the next year and

00:43:06,260 --> 00:43:09,650
we'll make this is it was good i'm

00:43:08,270 --> 00:43:16,190
pretty sure we'll have a better pike on

00:43:09,650 --> 00:43:20,930
next year as well thank you ok the next

00:43:16,190 --> 00:43:28,100
talk quick raymond is talk please hello

00:43:20,930 --> 00:43:29,930
my name is Vamsi yes my name is Vamsi

00:43:28,100 --> 00:43:32,090
there are many talks on Lancer language

00:43:29,930 --> 00:43:33,860
processing but I wanted to keep it more

00:43:32,090 --> 00:43:36,260
simple we are working on all basic stuff

00:43:33,860 --> 00:43:38,110
so I would like to share my experience

00:43:36,260 --> 00:43:40,250
with about natural language processing a

00:43:38,110 --> 00:43:43,430
few days back we have a working on a

00:43:40,250 --> 00:43:47,450
project called news aggregator the thing

00:43:43,430 --> 00:43:49,550
is that you're taking XML feeds from

00:43:47,450 --> 00:43:53,710
various sites like hindu Deccan

00:43:49,550 --> 00:43:57,950
Chronicle times of india various things

00:43:53,710 --> 00:43:59,840
you keep on so then after taking the xml

00:43:57,950 --> 00:44:02,360
feeds we are just filtering out of all

00:43:59,840 --> 00:44:05,390
the titles out of the xml fields using

00:44:02,360 --> 00:44:07,700
beautiful soup for and the beautiful

00:44:05,390 --> 00:44:09,980
soup is extraordinary library for

00:44:07,700 --> 00:44:12,770
filtering out all those things and after

00:44:09,980 --> 00:44:14,330
that we were in a state of filtering all

00:44:12,770 --> 00:44:17,330
those things like we want to stack

00:44:14,330 --> 00:44:18,560
similar items at one spot so father for

00:44:17,330 --> 00:44:21,080
that we have been thinking about about

00:44:18,560 --> 00:44:26,080
various libraries which are available at

00:44:21,080 --> 00:44:28,370
that stage I went along with an l TK so

00:44:26,080 --> 00:44:32,360
ok there is a problem with the projector

00:44:28,370 --> 00:44:35,480
so I took the NLT k viewer on n line PK

00:44:32,360 --> 00:44:39,380
and we got segmentation tokenizing and

00:44:35,480 --> 00:44:41,090
POS tagging of the title only title as

00:44:39,380 --> 00:44:46,130
the projector is not working I can't

00:44:41,090 --> 00:44:48,980
show you that and the second stage as we

00:44:46,130 --> 00:44:52,310
use an a base classifier for stacking up

00:44:48,980 --> 00:44:54,770
items and we have couple of even other

00:44:52,310 --> 00:44:59,120
libraries called text blob even in

00:44:54,770 --> 00:45:01,370
inside our engine but the only thing I

00:44:59,120 --> 00:45:03,260
thought of showing you that the only

00:45:01,370 --> 00:45:05,990
thing in the production when we want to

00:45:03,260 --> 00:45:08,480
release that is nit case too slow it's

00:45:05,990 --> 00:45:11,090
almost taking 12 seconds of time to just

00:45:08,480 --> 00:45:12,980
import that library so for that reason I

00:45:11,090 --> 00:45:15,890
would also recommend libraries like

00:45:12,980 --> 00:45:18,530
pattern text blob and jainism even

00:45:15,890 --> 00:45:19,910
jainism is used for similar sentence

00:45:18,530 --> 00:45:23,990
recognization

00:45:19,910 --> 00:45:25,819
so that is for it so who r end with an l

00:45:23,990 --> 00:45:28,160
TK they can also use other libraries

00:45:25,819 --> 00:45:30,829
call this text block and the text block

00:45:28,160 --> 00:45:33,980
is on the shoulders of both NLT can

00:45:30,829 --> 00:45:36,549
pattern go also try text block that

00:45:33,980 --> 00:45:36,549

YouTube URL: https://www.youtube.com/watch?v=ix0eBavAnbE


