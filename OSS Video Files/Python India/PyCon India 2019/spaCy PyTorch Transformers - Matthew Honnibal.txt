Title: spaCy PyTorch Transformers - Matthew Honnibal
Publication date: 2019-11-16
Playlist: PyCon India 2019
Description: 
	This talk was presented at PyCon India 2019, on Oct 12th - 13th, at the Chennai Trade Centre.
Website: https://in.pycon.org/2019
Captions: 
	00:00:06,200 --> 00:00:11,450
my talk today is about a pretty new

00:00:08,540 --> 00:00:13,570
piece of work that I've been doing with

00:00:11,450 --> 00:00:18,460
innocent the other member

00:00:13,570 --> 00:00:21,099
I on connecting spacy to the latest NLP

00:00:18,460 --> 00:00:22,779
models which DS transformer models so

00:00:21,099 --> 00:00:24,669
this is the first time I've given this

00:00:22,779 --> 00:00:26,410
talk so it there may be bits where it's

00:00:24,669 --> 00:00:28,450
a little bit rough then as I said there

00:00:26,410 --> 00:00:29,830
works very much in progress so some of

00:00:28,450 --> 00:00:31,840
the things which I'll be describing a

00:00:29,830 --> 00:00:33,340
basically the intended functionality or

00:00:31,840 --> 00:00:36,040
how things would be will be working in

00:00:33,340 --> 00:00:39,100
the future rather than what exactly can

00:00:36,040 --> 00:00:42,010
be used if you install the the package

00:00:39,100 --> 00:00:45,160
right away so just as a bit of

00:00:42,010 --> 00:00:47,920
background so I'm the developer of this

00:00:45,160 --> 00:00:48,969
open source library space II I've been

00:00:47,920 --> 00:00:52,360
working on natural language processing

00:00:48,969 --> 00:00:53,860
for pretty much my whole career I was

00:00:52,360 --> 00:00:56,469
lucky enough to get into the field when

00:00:53,860 --> 00:01:00,760
it was still quite small I finished my

00:00:56,469 --> 00:01:03,309
PhD in 2009 and then around 2014 I

00:01:00,760 --> 00:01:04,360
decided to leave academia basically at

00:01:03,309 --> 00:01:06,420
the point where I would have had to

00:01:04,360 --> 00:01:09,039
start writing grant proposals and

00:01:06,420 --> 00:01:12,330
decided to start working on this open

00:01:09,039 --> 00:01:14,679
source fibers spacing and my main

00:01:12,330 --> 00:01:16,210
colleague on this is my co-founder Innes

00:01:14,679 --> 00:01:18,520
he'll be talking tomorrow in the keynote

00:01:16,210 --> 00:01:21,720
session who's also been working on space

00:01:18,520 --> 00:01:25,330
he pretty much since its first release

00:01:21,720 --> 00:01:28,090
so this is you know you can find Spacey

00:01:25,330 --> 00:01:29,740
at there's Spacey i/o it has a quite

00:01:28,090 --> 00:01:31,780
good documentation and as I said it's

00:01:29,740 --> 00:01:33,100
quite popular these days so with it's

00:01:31,780 --> 00:01:34,810
always hard to estimate usage for an

00:01:33,100 --> 00:01:36,340
open source library but we figure we've

00:01:34,810 --> 00:01:38,890
probably got at least a hundred thousand

00:01:36,340 --> 00:01:41,619
users who are I you know basically using

00:01:38,890 --> 00:01:43,869
the library quite actively and we've got

00:01:41,619 --> 00:01:47,380
you know many stars and github like

00:01:43,869 --> 00:01:50,619
around 15,000 so the other main project

00:01:47,380 --> 00:01:52,060
that we do at explosion I am the company

00:01:50,619 --> 00:01:55,630
that innocent I co-founded is this

00:01:52,060 --> 00:01:57,070
annotation tool prodigy so this is

00:01:55,630 --> 00:01:59,350
especially useful in conjunction with

00:01:57,070 --> 00:02:01,299
Spacey because it lets you basically

00:01:59,350 --> 00:02:02,560
train your own models for Spacey so if

00:02:01,299 --> 00:02:03,939
you want to train your own named entity

00:02:02,560 --> 00:02:06,340
recognition models or train your own

00:02:03,939 --> 00:02:07,780
text classification models prodigy is a

00:02:06,340 --> 00:02:09,489
very easy way to do that and it's also

00:02:07,780 --> 00:02:11,200
an easy way to basically have a little

00:02:09,489 --> 00:02:13,180
bit of interactivity in your local data

00:02:11,200 --> 00:02:15,100
science workflow so that you can do sort

00:02:13,180 --> 00:02:17,230
of error analysis and basically have a

00:02:15,100 --> 00:02:18,610
deeper connection to the data because

00:02:17,230 --> 00:02:21,579
often when you're doing things like

00:02:18,610 --> 00:02:23,079
natural language processing the being

00:02:21,579 --> 00:02:24,609
able to look at the data and basically

00:02:23,079 --> 00:02:26,350
think about what to do next is much more

00:02:24,609 --> 00:02:28,150
important than deciding like you know

00:02:26,350 --> 00:02:31,480
what model architecture

00:02:28,150 --> 00:02:33,130
these sorts of things and it's actually

00:02:31,480 --> 00:02:34,690
prodigy that was a special motivation

00:02:33,130 --> 00:02:37,420
for the Transformers work that I'll be

00:02:34,690 --> 00:02:38,980
talking about because the these

00:02:37,420 --> 00:02:41,349
developments in transfer learning that

00:02:38,980 --> 00:02:43,239
you know the spatial Transformers

00:02:41,349 --> 00:02:45,459
package that will help you use are

00:02:43,239 --> 00:02:47,860
especially useful in conjunction with an

00:02:45,459 --> 00:02:49,510
annotation tool to basically allow you

00:02:47,860 --> 00:02:52,660
to take make use of the fact that you

00:02:49,510 --> 00:02:57,660
now need so much less training data so

00:02:52,660 --> 00:03:00,000
as with Spacey The Prodigy tool is quite

00:02:57,660 --> 00:03:01,959
really quite popular we've got a lot of

00:03:00,000 --> 00:03:03,640
users using it including a lot of

00:03:01,959 --> 00:03:05,170
companies and this is actually how we

00:03:03,640 --> 00:03:07,630
support the development of Spacey its

00:03:05,170 --> 00:03:13,030
sales of probably data funding the

00:03:07,630 --> 00:03:13,989
explosion AI company okay so I'm - you

00:03:13,030 --> 00:03:15,819
know basically what I'll be talking

00:03:13,989 --> 00:03:17,590
about here so I don't know how many of

00:03:15,819 --> 00:03:19,720
you have heard of transformer models um

00:03:17,590 --> 00:03:21,580
I wanted a you know asking people to

00:03:19,720 --> 00:03:26,709
raise their hands because um you don't

00:03:21,580 --> 00:03:28,000
generally nobody does but there's been a

00:03:26,709 --> 00:03:32,260
lot of headlines around this and a lot

00:03:28,000 --> 00:03:34,600
of excitement and so the idea here is

00:03:32,260 --> 00:03:36,340
that it's you know basically been a goal

00:03:34,600 --> 00:03:37,450
of natural language processing to move

00:03:36,340 --> 00:03:40,090
through what's called this knowledge

00:03:37,450 --> 00:03:42,489
acquisition bottleneck and this is

00:03:40,090 --> 00:03:44,200
really the problem that in any language

00:03:42,489 --> 00:03:46,329
processing application there's a lot of

00:03:44,200 --> 00:03:48,519
knowledge about word usage and knowledge

00:03:46,329 --> 00:03:51,340
about the world which generalizes across

00:03:48,519 --> 00:03:53,140
tasks and is really hard to include for

00:03:51,340 --> 00:03:54,310
a specific task that you want to do so

00:03:53,140 --> 00:03:58,209
you want to solve a particular problem

00:03:54,310 --> 00:04:01,060
like sorting your support tickets into n

00:03:58,209 --> 00:04:02,590
different categories and if you have to

00:04:01,060 --> 00:04:05,680
learn all of the information from that

00:04:02,590 --> 00:04:08,980
from tickets that 'verily classified the

00:04:05,680 --> 00:04:10,329
problem is really hard and the obvious

00:04:08,980 --> 00:04:11,890
observation is that most of the

00:04:10,329 --> 00:04:13,930
information that's in those tickets if

00:04:11,890 --> 00:04:16,030
things that anybody knows from a whole

00:04:13,930 --> 00:04:17,560
host of other background tasks it's non

00:04:16,030 --> 00:04:19,419
information which is specific to the

00:04:17,560 --> 00:04:22,030
problem that you're trying to solve so

00:04:19,419 --> 00:04:23,650
what we want to be able to do is gain

00:04:22,030 --> 00:04:25,389
that knowledge from somewhere else some

00:04:23,650 --> 00:04:28,360
general sort of knowledge of the

00:04:25,389 --> 00:04:29,410
language and then be able to reuse that

00:04:28,360 --> 00:04:31,539
knowledge across different applications

00:04:29,410 --> 00:04:33,340
in just the same way as if you were

00:04:31,539 --> 00:04:35,470
teaching somebody to do this task you

00:04:33,340 --> 00:04:36,909
would expect them to know the language

00:04:35,470 --> 00:04:38,500
and be able to read the tickets and then

00:04:36,909 --> 00:04:40,180
build on top of that the specific

00:04:38,500 --> 00:04:40,930
knowledge about what your classification

00:04:40,180 --> 00:04:44,020
scheme

00:04:40,930 --> 00:04:46,360
so although this had been a goal of

00:04:44,020 --> 00:04:47,770
natural language processing for you know

00:04:46,360 --> 00:04:52,360
basically since the field had begun

00:04:47,770 --> 00:04:54,370
efforts to use raw text in this way were

00:04:52,360 --> 00:04:55,990
not reworking that well before neural

00:04:54,370 --> 00:04:58,870
networks and even after neural networks

00:04:55,990 --> 00:05:01,270
came out the initial ways that we could

00:04:58,870 --> 00:05:03,400
reuse raw text resources and gain

00:05:01,270 --> 00:05:05,289
knowledge from broad text was really

00:05:03,400 --> 00:05:06,970
limited to basically the dictionary

00:05:05,289 --> 00:05:09,490
level or the meaning of individual words

00:05:06,970 --> 00:05:11,110
and so what's happened over the last

00:05:09,490 --> 00:05:13,660
couple of years we've really just gotten

00:05:11,110 --> 00:05:15,880
much better at a importing knowledge

00:05:13,660 --> 00:05:17,560
from raw text into our applications

00:05:15,880 --> 00:05:18,849
including knowledge from real text about

00:05:17,560 --> 00:05:21,190
the context of words and these

00:05:18,849 --> 00:05:22,300
contextual representations so that's

00:05:21,190 --> 00:05:25,509
really what these transformers my

00:05:22,300 --> 00:05:26,889
transform levels do and that's what the

00:05:25,509 --> 00:05:30,160
sort of knowledge that we want to

00:05:26,889 --> 00:05:32,169
connect here so this was all nicely

00:05:30,160 --> 00:05:34,750
summarized in a blog post by Sebastian

00:05:32,169 --> 00:05:36,580
Ritter in the if you want to look it up

00:05:34,750 --> 00:05:38,770
the blog post is called enemy's image

00:05:36,580 --> 00:05:40,300
net moment has finally arrived and this

00:05:38,770 --> 00:05:42,490
is really an analogy with computer

00:05:40,300 --> 00:05:46,120
vision where people import knowledge

00:05:42,490 --> 00:05:50,940
from computer vision tasks and and

00:05:46,120 --> 00:05:54,479
basically reuse that into you know other

00:05:50,940 --> 00:05:58,870
small specific computer vision models

00:05:54,479 --> 00:06:01,180
okay so and then of course this was a

00:05:58,870 --> 00:06:03,070
you know culminated in an article about

00:06:01,180 --> 00:06:04,449
the New York Times and it did sort of

00:06:03,070 --> 00:06:06,610
blow my mind a little bit that this

00:06:04,449 --> 00:06:09,010
field that I had started off in in 2009

00:06:06,610 --> 00:06:10,810
that you know what was actually kind of

00:06:09,010 --> 00:06:13,270
an incremental update in this ends up

00:06:10,810 --> 00:06:14,889
being like for news in like a major

00:06:13,270 --> 00:06:16,659
publication so you can kind of see how

00:06:14,889 --> 00:06:19,930
the field has evolved in how this has

00:06:16,659 --> 00:06:22,000
kind of made a mainstream splash so um

00:06:19,930 --> 00:06:23,470
in practice the way that a lot of people

00:06:22,000 --> 00:06:26,500
are using these transformal models in

00:06:23,470 --> 00:06:28,449
their applications is via a package

00:06:26,500 --> 00:06:30,699
developed by the our friends at hugging

00:06:28,449 --> 00:06:34,750
face and this is um started off being

00:06:30,699 --> 00:06:36,940
called um I think it was like Burt

00:06:34,750 --> 00:06:39,099
whitewater transformers or pied water

00:06:36,940 --> 00:06:40,630
bird then it became part of transformers

00:06:39,099 --> 00:06:42,759
and now it's just transformers because

00:06:40,630 --> 00:06:44,949
it supports tensorflow as well so this

00:06:42,759 --> 00:06:46,449
is a really quite a popular library and

00:06:44,949 --> 00:06:48,550
it gives you a pretty easy way to use to

00:06:46,449 --> 00:06:50,560
pre train weights so the models take a

00:06:48,550 --> 00:06:52,599
long time to kind of calculate on the

00:06:50,560 --> 00:06:54,249
raw text and cost quite a lot of money

00:06:52,599 --> 00:06:56,889
um at several thousand

00:06:54,249 --> 00:06:59,019
in many cases but this gives you an easy

00:06:56,889 --> 00:07:02,169
way to use those model artifacts that

00:06:59,019 --> 00:07:03,519
people have developed and even if the

00:07:02,169 --> 00:07:05,169
models have been trained with and tends

00:07:03,519 --> 00:07:07,299
to flow they kind of translate them into

00:07:05,169 --> 00:07:11,199
PI drop so that you can use them in PI

00:07:07,299 --> 00:07:12,819
torch so this is like a quick view of

00:07:11,199 --> 00:07:15,249
what the API looks like but basically

00:07:12,819 --> 00:07:16,869
you get an easy way to load the month so

00:07:15,249 --> 00:07:18,909
once this was developed we said all

00:07:16,869 --> 00:07:20,319
right well now that we've got this you

00:07:18,909 --> 00:07:22,539
know nice way to use the models in pi

00:07:20,319 --> 00:07:25,899
torch we want to basically have a

00:07:22,539 --> 00:07:27,789
connection for this for spacing so this

00:07:25,899 --> 00:07:29,799
is the Spacey transformers library and

00:07:27,789 --> 00:07:32,169
this is kind of what the usage of it

00:07:29,799 --> 00:07:34,119
looks like and and also what the

00:07:32,169 --> 00:07:37,389
pipeline here looks like so as with

00:07:34,119 --> 00:07:39,159
other Spacey models you'll call a space

00:07:37,389 --> 00:07:41,079
you've got load to load off the

00:07:39,159 --> 00:07:43,479
pre-trained model the models are

00:07:41,079 --> 00:07:46,419
distributed kind of packaged as pip

00:07:43,479 --> 00:07:48,099
packages that an individual package can

00:07:46,419 --> 00:07:49,479
have dependencies and basically declare

00:07:48,099 --> 00:07:51,369
itself that way and you can serve it out

00:07:49,479 --> 00:07:53,679
to your applications in exactly the same

00:07:51,369 --> 00:07:56,439
way as you're serving you know all of

00:07:53,679 --> 00:07:57,909
the other Python dependencies so it's

00:07:56,439 --> 00:08:00,129
kind of uses standard tools in this kind

00:07:57,909 --> 00:08:02,679
of easy to work with that way and then

00:08:00,129 --> 00:08:04,059
this NOP object can be just used as a

00:08:02,679 --> 00:08:05,829
function so you can call it with a

00:08:04,059 --> 00:08:08,139
single text you can also call it with

00:08:05,829 --> 00:08:10,989
batches of text with NOP dot pipes and

00:08:08,139 --> 00:08:14,949
it will do the mini matching internally

00:08:10,989 --> 00:08:16,779
for efficient processing so it's alright

00:08:14,949 --> 00:08:21,069
what can we do with this well out of the

00:08:16,779 --> 00:08:22,659
box and it before we connect on a a

00:08:21,069 --> 00:08:24,339
model which is trained for your specific

00:08:22,659 --> 00:08:25,839
task there's actually not that much that

00:08:24,339 --> 00:08:27,909
we can do but we can already say all

00:08:25,839 --> 00:08:30,909
right we can kind of get a similarity

00:08:27,909 --> 00:08:31,689
judgment it may not match the similarity

00:08:30,909 --> 00:08:33,339
that you want for a particular

00:08:31,689 --> 00:08:35,709
application but you can at least look at

00:08:33,339 --> 00:08:39,339
how similar different vectors are and

00:08:35,709 --> 00:08:40,659
then you can also access the the

00:08:39,339 --> 00:08:44,920
representations that have been assigned

00:08:40,659 --> 00:08:48,370
to the 2d tokens by the transformer

00:08:44,920 --> 00:08:50,769
model and then importantly one of the

00:08:48,370 --> 00:08:52,959
quite neat usage things that we will and

00:08:50,769 --> 00:08:54,759
a place to develop is that the

00:08:52,959 --> 00:08:57,009
transformer models tend to use a non

00:08:54,759 --> 00:08:58,930
linguistic tokenization scheme in order

00:08:57,009 --> 00:09:00,759
to limit the number of different

00:08:58,930 --> 00:09:03,129
vocabulary words so they tend to

00:09:00,759 --> 00:09:05,470
basically divide up a single word that's

00:09:03,129 --> 00:09:06,110
as long as two words rare so something

00:09:05,470 --> 00:09:09,200
like Chennai

00:09:06,110 --> 00:09:10,910
might be in two tokens Chen and nie and

00:09:09,200 --> 00:09:13,790
it does this so that the model doesn't

00:09:10,910 --> 00:09:15,410
have an unbounded Li large vocabulary of

00:09:13,790 --> 00:09:17,089
words to deal with now of course when

00:09:15,410 --> 00:09:18,589
you actually go to use the models the

00:09:17,089 --> 00:09:20,990
fact that you don't have a vector that

00:09:18,589 --> 00:09:23,959
represents an actual word for your

00:09:20,990 --> 00:09:27,320
application is quite a pain so what we

00:09:23,959 --> 00:09:29,089
do in the space we Patrick in this

00:09:27,320 --> 00:09:31,160
pastie transformers library is basically

00:09:29,089 --> 00:09:33,260
just do this alignment process so that

00:09:31,160 --> 00:09:35,089
even if you want to work with the word

00:09:33,260 --> 00:09:37,130
level later on um you're able to get a

00:09:35,089 --> 00:09:42,709
word representation out of these bird

00:09:37,130 --> 00:09:46,339
models or a telnet models and use that

00:09:42,709 --> 00:09:48,290
to use the alignment to basically give

00:09:46,339 --> 00:09:51,800
you a representation that for the words

00:09:48,290 --> 00:09:53,630
that you actually want to work with so

00:09:51,800 --> 00:09:57,560
here you can see but here you can see

00:09:53,630 --> 00:09:59,450
this about the alignment we basically

00:09:57,560 --> 00:10:01,160
asked whether we've got something like

00:09:59,450 --> 00:10:04,640
placed which has been split up into two

00:10:01,160 --> 00:10:08,149
tokens by the word paste tokenization

00:10:04,640 --> 00:10:10,279
and we align those back up to LAist here

00:10:08,149 --> 00:10:13,010
so that when we calculating the vectors

00:10:10,279 --> 00:10:14,570
we can able to say ask what's the vector

00:10:13,010 --> 00:10:16,490
representation for laced rather than

00:10:14,570 --> 00:10:21,649
just what's the representation for L and

00:10:16,490 --> 00:10:24,140
paste so um after you add something like

00:10:21,649 --> 00:10:27,890
a text classifier model onto the end we

00:10:24,140 --> 00:10:30,199
can say all right we'll use spaces API

00:10:27,890 --> 00:10:33,140
for text classification so you can use

00:10:30,199 --> 00:10:37,670
it within your own or P pipeline and you

00:10:33,140 --> 00:10:39,829
can train this text classifier and and

00:10:37,670 --> 00:10:42,980
have it back propagate into the Bert

00:10:39,829 --> 00:10:45,620
model or other transformer model so that

00:10:42,980 --> 00:10:47,470
you're learning a very accurate text

00:10:45,620 --> 00:10:49,699
classification model using these

00:10:47,470 --> 00:10:51,290
transformer models and then soon we'll

00:10:49,699 --> 00:10:54,620
have other pipeline components that work

00:10:51,290 --> 00:10:56,240
in the same way now as soon as you have

00:10:54,620 --> 00:10:58,610
multiple pipeline components here though

00:10:56,240 --> 00:11:00,440
there's a bit of a challenge for the

00:10:58,610 --> 00:11:01,490
design of these things so that's what

00:11:00,440 --> 00:11:02,390
I'm going to talk about in this next

00:11:01,490 --> 00:11:04,010
section and talk about different

00:11:02,390 --> 00:11:07,820
trade-offs here and different ways that

00:11:04,010 --> 00:11:09,079
we want this to work so first here's a

00:11:07,820 --> 00:11:11,420
little bit of background about the way

00:11:09,079 --> 00:11:12,800
that space he does components and the

00:11:11,420 --> 00:11:16,640
kind of component system that we have

00:11:12,800 --> 00:11:18,170
here so what you're able to do is at

00:11:16,640 --> 00:11:18,800
least in space you transformers and this

00:11:18,170 --> 00:11:20,209
is coming soon

00:11:18,800 --> 00:11:22,190
into the space of library as well you

00:11:20,209 --> 00:11:24,490
can decorate a function and describe it

00:11:22,190 --> 00:11:27,320
as a basically registered as a way to

00:11:24,490 --> 00:11:30,200
describe a model architecture and so

00:11:27,320 --> 00:11:32,450
that then in your components you can

00:11:30,200 --> 00:11:34,940
swap out this model architectures in the

00:11:32,450 --> 00:11:36,920
config files and you can you know

00:11:34,940 --> 00:11:38,540
basically bring your own architecture or

00:11:36,920 --> 00:11:41,420
define your own architectures for these

00:11:38,540 --> 00:11:43,700
things so here with another nice detail

00:11:41,420 --> 00:11:45,589
here is that Spacey has basically this

00:11:43,700 --> 00:11:48,380
interface library and think which does

00:11:45,589 --> 00:11:51,079
its machine learning and we have a

00:11:48,380 --> 00:11:52,610
wrapper for PI torch that you can put

00:11:51,079 --> 00:11:55,370
around the pad watch models to use them

00:11:52,610 --> 00:11:58,399
in Spacey with no copies involved so you

00:11:55,370 --> 00:12:01,010
can use a custom Python player and use

00:11:58,399 --> 00:12:02,899
it to power a Spacely component even if

00:12:01,010 --> 00:12:04,190
it's not a transform model you'll be

00:12:02,899 --> 00:12:05,779
able to do the same thing even if you

00:12:04,190 --> 00:12:08,260
just want to say you as a buyer SDM

00:12:05,779 --> 00:12:11,120
tagger or some other custom PI torch

00:12:08,260 --> 00:12:13,339
model and as soon as tensorflow to

00:12:11,120 --> 00:12:15,950
supports this DL pack format you'll be

00:12:13,339 --> 00:12:17,269
able to do the same with tensorflow so

00:12:15,950 --> 00:12:19,510
what you would do if you wanted to

00:12:17,269 --> 00:12:21,860
define a new component for say and you

00:12:19,510 --> 00:12:24,680
NLP tasks but you want them to solve and

00:12:21,860 --> 00:12:26,510
you would subclass this NLP pipe and

00:12:24,680 --> 00:12:29,930
this basically pipeline the pipe

00:12:26,510 --> 00:12:31,910
component and you would define your dis

00:12:29,930 --> 00:12:33,200
function that returns a model and you

00:12:31,910 --> 00:12:35,360
don't really have to do much here you

00:12:33,200 --> 00:12:38,420
basically can just use the registry

00:12:35,360 --> 00:12:40,579
system here and just pass it it's config

00:12:38,420 --> 00:12:42,920
through and that will you know basically

00:12:40,579 --> 00:12:44,380
do enough to instantiate the model

00:12:42,920 --> 00:12:46,490
architecture that you've defined up here

00:12:44,380 --> 00:12:48,050
and then the other parts of the

00:12:46,490 --> 00:12:51,709
lifecycle that you'll want to define is

00:12:48,050 --> 00:12:53,720
this a predict method where you can say

00:12:51,709 --> 00:12:56,870
okay extract some features from the docs

00:12:53,720 --> 00:12:58,070
like you know their word IDs or any

00:12:56,870 --> 00:12:59,959
other types of features that you want

00:12:58,070 --> 00:13:01,790
your model to have access to and then

00:12:59,959 --> 00:13:03,920
you would pass that representation

00:13:01,790 --> 00:13:06,980
forward in your model and then return

00:13:03,920 --> 00:13:08,899
the scores from it then you have the

00:13:06,980 --> 00:13:11,750
opportunity to set any annotations based

00:13:08,899 --> 00:13:15,519
on those scores so let's say you wanted

00:13:11,750 --> 00:13:17,510
to have custom features or custom

00:13:15,519 --> 00:13:19,220
attributes that you wanted to define for

00:13:17,510 --> 00:13:20,720
some tasks that's basic as a support you

00:13:19,220 --> 00:13:22,250
can have extension attributes and

00:13:20,720 --> 00:13:23,930
basically calculate all of the things to

00:13:22,250 --> 00:13:26,329
update the doc objects that you want to

00:13:23,930 --> 00:13:30,040
work with and then finally you can have

00:13:26,329 --> 00:13:30,040
an update function which will

00:13:30,400 --> 00:13:34,990
you know basically allow you to

00:13:32,670 --> 00:13:36,280
calculate a weight update for your model

00:13:34,990 --> 00:13:38,680
based on some gold standard information

00:13:36,280 --> 00:13:40,090
and based on The Bachelor documents so

00:13:38,680 --> 00:13:41,650
this is all very nice this is you know

00:13:40,090 --> 00:13:43,300
basically how a pipeline component works

00:13:41,650 --> 00:13:44,590
since basically and once you've defined

00:13:43,300 --> 00:13:47,380
it you can add it to your Spacely

00:13:44,590 --> 00:13:48,610
pipeline you can register it as an entry

00:13:47,380 --> 00:13:50,350
point so that you can have a package

00:13:48,610 --> 00:13:51,880
that basically people can just pip

00:13:50,350 --> 00:13:53,200
install and your component will be all

00:13:51,880 --> 00:13:54,970
available and ready to use

00:13:53,200 --> 00:13:59,230
so that you can extend spacy in these

00:13:54,970 --> 00:14:01,210
ways so if we imagine this kind of like

00:13:59,230 --> 00:14:03,280
you know working through with multiple

00:14:01,210 --> 00:14:05,860
pipeline components let's say we've got

00:14:03,280 --> 00:14:08,350
some text space we'll tokenize that into

00:14:05,860 --> 00:14:10,510
a dock object and then inside the NLP

00:14:08,350 --> 00:14:12,310
call or an LP pipe method you'll call

00:14:10,510 --> 00:14:13,720
the something like the named entity

00:14:12,310 --> 00:14:15,430
recognizer and then call the text

00:14:13,720 --> 00:14:17,140
classifier and as you're going through

00:14:15,430 --> 00:14:20,380
your update annotations in the dock and

00:14:17,140 --> 00:14:23,080
then get out a dock object cool okay so

00:14:20,380 --> 00:14:25,240
now one question here is how does this

00:14:23,080 --> 00:14:28,080
all work when we're also updating the

00:14:25,240 --> 00:14:30,640
the models so if we let's imagine that

00:14:28,080 --> 00:14:32,560
we have our transformer models here and

00:14:30,640 --> 00:14:35,110
we've got say a named entity recognition

00:14:32,560 --> 00:14:36,910
head on top of a transformer and then

00:14:35,110 --> 00:14:39,010
also a text classification head that's

00:14:36,910 --> 00:14:40,690
on top of the transformer so how should

00:14:39,010 --> 00:14:42,640
this work should we have two copies of

00:14:40,690 --> 00:14:45,339
the transformer models or should we have

00:14:42,640 --> 00:14:46,839
only have one do we want to so the

00:14:45,339 --> 00:14:49,360
transformer models are quite slow to run

00:14:46,839 --> 00:14:51,460
they and they require pretty expensive

00:14:49,360 --> 00:14:53,920
hardware so do we really want to be

00:14:51,460 --> 00:14:56,440
running the model twice especially if

00:14:53,920 --> 00:14:59,470
it's quite similar or do we want to run

00:14:56,440 --> 00:15:02,230
it once and sort of reuse the that

00:14:59,470 --> 00:15:03,610
result through it so here's kind of like

00:15:02,230 --> 00:15:05,470
the sketch of the architecture of what

00:15:03,610 --> 00:15:08,860
it would look like if we want to run it

00:15:05,470 --> 00:15:11,440
once an update so here we would say ok

00:15:08,860 --> 00:15:13,600
have this component token vector encoder

00:15:11,440 --> 00:15:17,080
which will run the transform model and

00:15:13,600 --> 00:15:18,550
put the set all of the vectors up on the

00:15:17,080 --> 00:15:20,170
dock object and set all of those

00:15:18,550 --> 00:15:24,760
extension attributes that we had and

00:15:20,170 --> 00:15:26,800
then we can pass that talk forward the

00:15:24,760 --> 00:15:28,780
named entity recognizer will make use of

00:15:26,800 --> 00:15:31,810
those features that the token vector

00:15:28,780 --> 00:15:33,910
encoder extracted and then it's able to

00:15:31,810 --> 00:15:36,040
pass gradients back into that model so

00:15:33,910 --> 00:15:37,420
that we can update it and then similarly

00:15:36,040 --> 00:15:39,220
we can move forward into the text

00:15:37,420 --> 00:15:40,959
classifier and the text classifier can

00:15:39,220 --> 00:15:43,030
also make use of the information that

00:15:40,959 --> 00:15:44,830
was calculated back there

00:15:43,030 --> 00:15:48,070
and also update that single shared

00:15:44,830 --> 00:15:49,960
representation so on some way it sort of

00:15:48,070 --> 00:15:53,050
um in other words this can be described

00:15:49,960 --> 00:15:54,460
as multitask learning but it's really

00:15:53,050 --> 00:15:56,650
just a question of whether we want to

00:15:54,460 --> 00:15:57,970
run early in set state or whether we

00:15:56,650 --> 00:16:00,400
want to have all of these things be

00:15:57,970 --> 00:16:02,380
independent so there's actually you know

00:16:00,400 --> 00:16:04,510
quite an old general trade-off that we

00:16:02,380 --> 00:16:07,200
have in code around here so the question

00:16:04,510 --> 00:16:09,130
is whether we want to have more modular

00:16:07,200 --> 00:16:11,290
components that are independent and

00:16:09,130 --> 00:16:14,410
don't depend on it much state externally

00:16:11,290 --> 00:16:16,090
or do we want to sort of have a more

00:16:14,410 --> 00:16:18,070
complicated set up with more assumptions

00:16:16,090 --> 00:16:22,060
and therefore gain better efficiencies

00:16:18,070 --> 00:16:24,190
and that's definitely not a you know a

00:16:22,060 --> 00:16:26,110
question that is only going to be come

00:16:24,190 --> 00:16:28,600
up in machine learning this is something

00:16:26,110 --> 00:16:30,370
that we you know have to figure out with

00:16:28,600 --> 00:16:32,560
code all the time and these trade-offs

00:16:30,370 --> 00:16:35,380
around making something more modular and

00:16:32,560 --> 00:16:37,210
making it run more efficiently or you

00:16:35,380 --> 00:16:39,910
know in some cases actually perform

00:16:37,210 --> 00:16:41,260
better in terms of accuracy so here's

00:16:39,910 --> 00:16:43,090
what it would look like in this sort of

00:16:41,260 --> 00:16:45,760
alternate architecture here we would

00:16:43,090 --> 00:16:47,500
have an individual named entity

00:16:45,760 --> 00:16:51,130
recognize a component and it will kind

00:16:47,500 --> 00:16:52,840
of own its own transformer weights so we

00:16:51,130 --> 00:16:54,580
don't have any multitask learning here

00:16:52,840 --> 00:16:56,860
and we have kind of like you know some

00:16:54,580 --> 00:16:59,460
nice architectural simplicity where we

00:16:56,860 --> 00:17:02,020
don't have to depend as much on previous

00:16:59,460 --> 00:17:03,760
you know computations and similarly

00:17:02,020 --> 00:17:05,230
we'll passport a nanotech classifier and

00:17:03,760 --> 00:17:06,760
then we'll run the transformer again

00:17:05,230 --> 00:17:08,860
we'll run a completely separate set of

00:17:06,760 --> 00:17:11,080
transformer weights so again this has

00:17:08,860 --> 00:17:12,790
like you know some problems um

00:17:11,080 --> 00:17:14,620
not least of which if you're running

00:17:12,790 --> 00:17:16,450
this on a machine with only one GPU

00:17:14,620 --> 00:17:17,680
you're actually gonna really struggle

00:17:16,450 --> 00:17:20,680
with memory here because you're probably

00:17:17,680 --> 00:17:23,880
going to have all of this stuff set up

00:17:20,680 --> 00:17:26,380
in memory and then a whole separate

00:17:23,880 --> 00:17:28,570
model loaded in memory and you may run

00:17:26,380 --> 00:17:30,850
out of memory if you know because even

00:17:28,570 --> 00:17:32,590
if your card has say 12 gigabytes of GPU

00:17:30,850 --> 00:17:35,410
memory the transformer models are so

00:17:32,590 --> 00:17:36,820
large that actually you you may still

00:17:35,410 --> 00:17:38,770
struggle here so you end up having to

00:17:36,820 --> 00:17:41,230
provision machines with multiple GPUs

00:17:38,770 --> 00:17:43,720
and as your pipeline grows and you say

00:17:41,230 --> 00:17:45,460
have you no more of these components you

00:17:43,720 --> 00:17:47,410
have to ask questions about how many

00:17:45,460 --> 00:17:49,180
could what's the nature of the software

00:17:47,410 --> 00:17:51,310
that I'm running and what machines do I

00:17:49,180 --> 00:17:52,900
have to provision it sodium so the

00:17:51,310 --> 00:17:54,940
deployment and you know infrastructure

00:17:52,900 --> 00:17:57,220
questions start to get quite tricky and

00:17:54,940 --> 00:17:58,960
where you know even ones little config

00:17:57,220 --> 00:18:00,550
change in your fight in your conflict

00:17:58,960 --> 00:18:02,410
file to say load a different component

00:18:00,550 --> 00:18:04,240
means that the whole different hardware

00:18:02,410 --> 00:18:05,710
has to be provisioned and if you over

00:18:04,240 --> 00:18:07,420
provision the hardware you'll find that

00:18:05,710 --> 00:18:09,100
it's like costs thousands of extra

00:18:07,420 --> 00:18:10,900
dollars a month because the GPUs

00:18:09,100 --> 00:18:13,180
especially on cloud I'm quite expensive

00:18:10,900 --> 00:18:13,750
so we have like you know basically this

00:18:13,180 --> 00:18:17,230
dilemma

00:18:13,750 --> 00:18:19,860
so to summarize the dilemma here and you

00:18:17,230 --> 00:18:21,760
know we there's really strong code

00:18:19,860 --> 00:18:23,440
motivations for wanting a modular

00:18:21,760 --> 00:18:27,400
architecture we want the functions to be

00:18:23,440 --> 00:18:29,770
small and self-contained and our systems

00:18:27,400 --> 00:18:31,560
are much more much easy to reason about

00:18:29,770 --> 00:18:34,210
if we avoid state and side effects and

00:18:31,560 --> 00:18:36,070
of course we can compose lots of Syst

00:18:34,210 --> 00:18:39,010
lots of different systems from a smaller

00:18:36,070 --> 00:18:41,860
number of coded parts so we get lots of

00:18:39,010 --> 00:18:43,540
code reuse and lots of combinations and

00:18:41,860 --> 00:18:45,280
we can get lots of complexity resulting

00:18:43,540 --> 00:18:48,030
from a small amount of code which is

00:18:45,280 --> 00:18:52,330
great but on the other hand well

00:18:48,030 --> 00:18:54,010
performance so if we divide our part

00:18:52,330 --> 00:18:55,510
work into lots of small functions we

00:18:54,010 --> 00:18:56,890
have to repeat lots of work a lot at a

00:18:55,510 --> 00:18:59,020
time we can't make as many assumptions

00:18:56,890 --> 00:19:00,850
about the total computation that's

00:18:59,020 --> 00:19:03,070
running and so we can't optimize as

00:19:00,850 --> 00:19:05,020
efficiently another thing is that

00:19:03,070 --> 00:19:07,870
without that state models can lose

00:19:05,020 --> 00:19:09,760
information and that can actually limit

00:19:07,870 --> 00:19:11,440
the number of features or what how we

00:19:09,760 --> 00:19:12,700
can actually define our models and that

00:19:11,440 --> 00:19:15,700
can actually also result in less

00:19:12,700 --> 00:19:17,230
accuracy finally I think that you know

00:19:15,700 --> 00:19:18,370
something that is unique to machine

00:19:17,230 --> 00:19:20,890
learning when we're making this sort of

00:19:18,370 --> 00:19:24,100
trade-off is that models are often not

00:19:20,890 --> 00:19:25,690
that interchangeable so the behavior of

00:19:24,100 --> 00:19:27,250
one model even if it has the same sort

00:19:25,690 --> 00:19:28,450
of signature as long as it's trained

00:19:27,250 --> 00:19:30,430
differently it's going to have different

00:19:28,450 --> 00:19:32,500
behaviors and that means that you really

00:19:30,430 --> 00:19:35,050
it's actually becomes quite difficult to

00:19:32,500 --> 00:19:36,670
compose the pipelines together and to

00:19:35,050 --> 00:19:39,820
treat these different building blocks as

00:19:36,670 --> 00:19:40,900
actually interchangeable units and I

00:19:39,820 --> 00:19:42,550
think that that's actually a good

00:19:40,900 --> 00:19:45,280
motivation to take a different approach

00:19:42,550 --> 00:19:47,380
here and instead of making things as

00:19:45,280 --> 00:19:50,920
modular as possible in some set in some

00:19:47,380 --> 00:19:53,590
ways we may want to change this so in

00:19:50,920 --> 00:19:55,330
spacy actually initially we had a very

00:19:53,590 --> 00:19:57,700
non modular architecture because the

00:19:55,330 --> 00:20:00,070
tagger was used as a teacher in the

00:19:57,700 --> 00:20:02,080
plaza and then the parsers features were

00:20:00,070 --> 00:20:04,090
used in the named entity recognition so

00:20:02,080 --> 00:20:06,550
and this actually defined the way that

00:20:04,090 --> 00:20:08,830
the spacy api was worked initially

00:20:06,550 --> 00:20:10,900
and it's sort of an informed way that

00:20:08,830 --> 00:20:12,640
spaces API has worked since so that's

00:20:10,900 --> 00:20:14,980
why you load up this one model and it

00:20:12,640 --> 00:20:16,750
gives you a whole set of components that

00:20:14,980 --> 00:20:18,870
are defined together like you load up a

00:20:16,750 --> 00:20:21,940
whole configured pipeline rather than a

00:20:18,870 --> 00:20:23,920
an API which you know where you might

00:20:21,940 --> 00:20:25,690
say okay I'll load up like I'll

00:20:23,920 --> 00:20:27,190
initialize a pipeline and then I'll add

00:20:25,690 --> 00:20:29,260
the components that I want to it and

00:20:27,190 --> 00:20:31,060
then I'll run which in some ways would

00:20:29,260 --> 00:20:33,580
seem like a you know more modular design

00:20:31,060 --> 00:20:36,460
and the decision was made to give it

00:20:33,580 --> 00:20:38,440
this sort of style because if there's

00:20:36,460 --> 00:20:40,510
only one valid combination of the

00:20:38,440 --> 00:20:41,740
components or only one valid ordering it

00:20:40,510 --> 00:20:43,690
doesn't actually make sense to make

00:20:41,740 --> 00:20:45,010
people you know chant that incantation

00:20:43,690 --> 00:20:46,810
every time if there's only one answer

00:20:45,010 --> 00:20:49,720
you don't have to make people code it up

00:20:46,810 --> 00:20:51,910
you should just like do it so that's why

00:20:49,720 --> 00:20:53,440
it's sort of work display and you know

00:20:51,910 --> 00:20:55,870
the same trade-off is now being seen in

00:20:53,440 --> 00:20:58,390
the transformer models so for v2 at

00:20:55,870 --> 00:21:00,210
least and we managed to break these

00:20:58,390 --> 00:21:02,470
dependencies and make things a bit more

00:21:00,210 --> 00:21:05,950
modular so the tag it doesn't actually

00:21:02,470 --> 00:21:07,750
depend on the previous state in the v2

00:21:05,950 --> 00:21:09,730
v2 point to point 1 and V 2 point 2

00:21:07,750 --> 00:21:13,510
models because you know as I said it's

00:21:09,730 --> 00:21:15,040
nice to have things modular but now that

00:21:13,510 --> 00:21:16,240
we're moving to transformers we actually

00:21:15,040 --> 00:21:19,780
want to have a slightly different

00:21:16,240 --> 00:21:22,900
approach so moving forward to kind of

00:21:19,780 --> 00:21:24,490
the conclusion thing of the talk the

00:21:22,900 --> 00:21:26,890
pros and cons that are transformers at

00:21:24,490 --> 00:21:28,480
it and the transformer architecture

00:21:26,890 --> 00:21:29,950
really makes it easy to define networks

00:21:28,480 --> 00:21:31,600
for new tasks there's kind of a clear

00:21:29,950 --> 00:21:33,940
right answer a clear template to follow

00:21:31,600 --> 00:21:34,330
for how to add new heads and how to do

00:21:33,940 --> 00:21:36,610
things

00:21:34,330 --> 00:21:38,170
you get great accuracy the accuracy

00:21:36,610 --> 00:21:39,490
improvements have been really you know

00:21:38,170 --> 00:21:41,560
quite remarkable and they keep coming

00:21:39,490 --> 00:21:43,120
out so we're you know really moving up

00:21:41,560 --> 00:21:45,520
in accuracy quite quickly which makes

00:21:43,120 --> 00:21:46,930
the models very much easier to use in

00:21:45,520 --> 00:21:48,790
applications because you can reason

00:21:46,930 --> 00:21:50,590
about the you can kind of expect these

00:21:48,790 --> 00:21:53,320
in performance which makes to really

00:21:50,590 --> 00:21:55,180
takes a lot of the guesswork out and a

00:21:53,320 --> 00:21:57,310
big improvement as well as that you need

00:21:55,180 --> 00:21:58,600
fewer annotator II and ahead of examples

00:21:57,310 --> 00:22:01,660
and that really helps you move quickly

00:21:58,600 --> 00:22:03,910
and you can iterate much quicker on the

00:22:01,660 --> 00:22:06,880
tasks and you know basically reason

00:22:03,910 --> 00:22:08,350
about things much more easily but you

00:22:06,880 --> 00:22:10,360
know there's sort of significant

00:22:08,350 --> 00:22:12,220
downside to that and the models are slow

00:22:10,360 --> 00:22:15,250
and expensive if they take expensive

00:22:12,220 --> 00:22:16,810
GPUs to run and they particularly take

00:22:15,250 --> 00:22:18,520
large batches of text to operate

00:22:16,810 --> 00:22:19,420
efficiently and that means that if

00:22:18,520 --> 00:22:20,120
you're using them in a streaming

00:22:19,420 --> 00:22:21,770
environment

00:22:20,120 --> 00:22:24,980
or something then it's they're very hard

00:22:21,770 --> 00:22:26,540
to deploy and also the fact that the

00:22:24,980 --> 00:22:27,950
models of quite bleeding-edge and things

00:22:26,540 --> 00:22:30,050
are changing quickly around this space

00:22:27,950 --> 00:22:32,840
is also a significant downside for using

00:22:30,050 --> 00:22:34,160
them in your applications so if it's

00:22:32,840 --> 00:22:37,280
basically transformers this is kind of

00:22:34,160 --> 00:22:39,170
what it looks like for the code example

00:22:37,280 --> 00:22:43,160
you can add different architectures on

00:22:39,170 --> 00:22:46,040
this and what you'll be able to do is

00:22:43,160 --> 00:22:47,960
add on a text classifier and and entity

00:22:46,040 --> 00:22:49,490
recognizer and have those use the same

00:22:47,960 --> 00:22:53,870
shared States that you can only

00:22:49,490 --> 00:22:56,090
calculate the the tensest once and then

00:22:53,870 --> 00:22:59,420
I reuse those that's a you know key way

00:22:56,090 --> 00:23:01,610
that we want to make this work so at the

00:22:59,420 --> 00:23:04,340
moment and you can already do people

00:23:01,610 --> 00:23:07,400
install space your transformers and we

00:23:04,340 --> 00:23:10,220
support text cat so text classification

00:23:07,400 --> 00:23:12,050
models we have the aligned tokenization

00:23:10,220 --> 00:23:13,730
working and there's a really a pretty

00:23:12,050 --> 00:23:16,190
nice system for defining custom models

00:23:13,730 --> 00:23:19,360
or custom you know architectures and

00:23:16,190 --> 00:23:21,140
things we're mostly way through

00:23:19,360 --> 00:23:23,750
designing the named entity recognition

00:23:21,140 --> 00:23:25,220
component and a tagging component will

00:23:23,750 --> 00:23:26,570
be pretty easy to do as well and I think

00:23:25,220 --> 00:23:30,530
the dependency parsing model should

00:23:26,570 --> 00:23:31,850
follow afterwards then another thing

00:23:30,530 --> 00:23:34,430
that we really want to get working is

00:23:31,850 --> 00:23:36,320
having a remote procedure calls for the

00:23:34,430 --> 00:23:38,630
transformer components so that you can

00:23:36,320 --> 00:23:40,820
host the just a transformer on a GPU

00:23:38,630 --> 00:23:43,130
server and have it do the batching for

00:23:40,820 --> 00:23:45,500
you and you know have the rest of your

00:23:43,130 --> 00:23:47,360
pipelines on CPU and calling into it I

00:23:45,500 --> 00:23:49,340
think that that'll be a really effective

00:23:47,360 --> 00:23:51,080
way to ease some of these efficiency

00:23:49,340 --> 00:23:54,170
requirements and somebody's deployment

00:23:51,080 --> 00:23:56,000
requirements as well and then finally

00:23:54,170 --> 00:23:57,470
we're very excited about having to

00:23:56,000 --> 00:24:01,130
support for the transformer models in

00:23:57,470 --> 00:24:02,780
prodigy our annotation tool because in

00:24:01,130 --> 00:24:04,820
many cases you only need a few hundred

00:24:02,780 --> 00:24:06,290
examples to get decent performance with

00:24:04,820 --> 00:24:08,330
their transformer models because of the

00:24:06,290 --> 00:24:10,160
power of the transfer learning and this

00:24:08,330 --> 00:24:11,330
means that having a annotation tool

00:24:10,160 --> 00:24:12,950
that's scriptable that you can run

00:24:11,330 --> 00:24:15,530
locally and sort of very quickly click

00:24:12,950 --> 00:24:17,450
through and save the results out into a

00:24:15,530 --> 00:24:20,540
database that's just running on your

00:24:17,450 --> 00:24:22,610
local machine will be very useful and so

00:24:20,540 --> 00:24:27,500
we're very excited to have that working

00:24:22,610 --> 00:24:30,520
and so yeah thanks and you can follow me

00:24:27,500 --> 00:24:37,430
on twitter at one a ball and then

00:24:30,520 --> 00:24:40,460
explosion and that explosion so since

00:24:37,430 --> 00:24:44,540
thanks Matthew for the dog and we have

00:24:40,460 --> 00:24:46,610
enough time for Q&A so anyone have

00:24:44,540 --> 00:24:49,360
questions yeah for here

00:24:46,610 --> 00:24:49,360
first row

00:24:58,830 --> 00:25:09,940
hello I'm already with hello yeah yeah

00:25:06,880 --> 00:25:13,149
yeah okay so my question is already

00:25:09,940 --> 00:25:15,700
Spacey had any other algorithms like

00:25:13,149 --> 00:25:19,299
trigram CNN best algorithms and things

00:25:15,700 --> 00:25:23,889
like that so for the small data set like

00:25:19,299 --> 00:25:25,299
say 500 600 K and then records is it

00:25:23,889 --> 00:25:26,169
worth taking all the pain of

00:25:25,299 --> 00:25:28,539
transformers

00:25:26,169 --> 00:25:32,559
when already the things are pretty much

00:25:28,539 --> 00:25:36,970
work so uh um okay so I'm not entirely

00:25:32,559 --> 00:25:39,700
sure I got the question is it that if

00:25:36,970 --> 00:25:42,580
you have you know in a small moderately

00:25:39,700 --> 00:25:44,950
sized data set and you're getting decent

00:25:42,580 --> 00:25:46,629
performance with spicy yeah is it likely

00:25:44,950 --> 00:25:50,139
to be worth the trouble of switching

00:25:46,629 --> 00:25:52,960
over to the transformer models yeah I

00:25:50,139 --> 00:25:54,460
would say probably not especially since

00:25:52,960 --> 00:25:57,309
the fact that you have to run them on

00:25:54,460 --> 00:26:00,309
GPU really makes life difficult for many

00:25:57,309 --> 00:26:02,200
applications on the other hand there are

00:26:00,309 --> 00:26:04,960
sis there are lots of situations where

00:26:02,200 --> 00:26:05,950
you basically load up your data setting

00:26:04,960 --> 00:26:07,480
you don't immediately get good

00:26:05,950 --> 00:26:10,539
performance and this happens in machine

00:26:07,480 --> 00:26:12,309
learning all the time and this can

00:26:10,539 --> 00:26:13,480
actually be quite tricky to solve

00:26:12,309 --> 00:26:15,279
because you don't know what to try next

00:26:13,480 --> 00:26:18,309
and you actually don't even know whether

00:26:15,279 --> 00:26:19,509
the task can be solved so um I would say

00:26:18,309 --> 00:26:21,370
that one of the great things about

00:26:19,509 --> 00:26:22,929
having trick the transform models is

00:26:21,370 --> 00:26:25,000
sort of like getting a peek at the

00:26:22,929 --> 00:26:27,370
answer where you can say alright is

00:26:25,000 --> 00:26:29,620
there an existence proof for what sort

00:26:27,370 --> 00:26:31,299
of accuracy could be achieved and let's

00:26:29,620 --> 00:26:33,419
say you try out a simpler model like a

00:26:31,299 --> 00:26:35,799
bag of words model leaven nor with

00:26:33,419 --> 00:26:38,110
scikit-learn or you try out spacing

00:26:35,799 --> 00:26:40,450
model or some other thing and you get

00:26:38,110 --> 00:26:42,340
say 70 percent accuracy and then you

00:26:40,450 --> 00:26:45,279
load up on the transformer model and you

00:26:42,340 --> 00:26:47,710
get like 92 percent accuracy it makes

00:26:45,279 --> 00:26:49,179
you think well okay if I'm only getting

00:26:47,710 --> 00:26:50,710
like a little bit better than a bag of

00:26:49,179 --> 00:26:53,379
words or you know my native words model

00:26:50,710 --> 00:26:54,820
is getting this what can I actually do

00:26:53,379 --> 00:26:56,350
and it makes you think well maybe I

00:26:54,820 --> 00:26:58,440
should change my features a little bit

00:26:56,350 --> 00:27:00,639
or even just trial a bit harder to and

00:26:58,440 --> 00:27:02,110
train the other the simpler model

00:27:00,639 --> 00:27:04,059
another thing you can do is use the

00:27:02,110 --> 00:27:05,559
transform model to label text for the

00:27:04,059 --> 00:27:07,419
simpler model even if you don't want to

00:27:05,559 --> 00:27:08,110
deploy a transformer model you might be

00:27:07,419 --> 00:27:10,809
able to say well

00:27:08,110 --> 00:27:12,399
I'll use it in all train from the 92%

00:27:10,809 --> 00:27:13,809
accurate data and then maybe I'll

00:27:12,399 --> 00:27:16,299
recover some of the accuracy and get up

00:27:13,809 --> 00:27:17,620
to say 84% but if the models are any

00:27:16,299 --> 00:27:19,360
getting decent performance there's

00:27:17,620 --> 00:27:22,690
always no end of other things you can do

00:27:19,360 --> 00:27:24,490
so um you know typically there's a lot

00:27:22,690 --> 00:27:26,980
of other work in getting the application

00:27:24,490 --> 00:27:29,080
working correctly and you know trying

00:27:26,980 --> 00:27:30,850
out different ideas so if you once you

00:27:29,080 --> 00:27:33,159
really hit acceptable accuracy and

00:27:30,850 --> 00:27:35,049
you're kind of at the like you know the

00:27:33,159 --> 00:27:37,450
top end of the curve where you know

00:27:35,049 --> 00:27:38,919
always performance has success shape if

00:27:37,450 --> 00:27:41,470
you think well I'm not getting that much

00:27:38,919 --> 00:27:42,940
utility from extra accuracy there's

00:27:41,470 --> 00:27:44,200
definitely no point in moving on with

00:27:42,940 --> 00:27:47,889
that and you probably want to prioritize

00:27:44,200 --> 00:27:50,559
different work hello

00:27:47,889 --> 00:27:53,200
so how do you pay the recently released

00:27:50,559 --> 00:27:55,809
their digital bird model hello Hey yeah

00:27:53,200 --> 00:27:58,779
I mean could you repeat that slightly

00:27:55,809 --> 00:28:00,820
yeah so hugging paper recently their

00:27:58,779 --> 00:28:02,919
disregard model and the this will be due

00:28:00,820 --> 00:28:04,960
to model which is my performance is much

00:28:02,919 --> 00:28:06,309
faster than the original bird so it's

00:28:04,960 --> 00:28:09,190
basically already have support for that

00:28:06,309 --> 00:28:12,130
or do we have to port it to Spacey so

00:28:09,190 --> 00:28:14,230
okay so um it's the question whether we

00:28:12,130 --> 00:28:17,019
which models we support or like whether

00:28:14,230 --> 00:28:19,539
we support disturber typically yeah okay

00:28:17,019 --> 00:28:22,960
and yes we have a package for disturber

00:28:19,539 --> 00:28:25,360
and in fact the process for creating the

00:28:22,960 --> 00:28:26,830
special packages is very quick so

00:28:25,360 --> 00:28:28,059
there's a single script that runs that

00:28:26,830 --> 00:28:30,870
basically downloads the printer and

00:28:28,059 --> 00:28:34,059
model and packages it up to Spacey and

00:28:30,870 --> 00:28:37,870
so you know we've got this added to our

00:28:34,059 --> 00:28:39,490
model automation now so um we added

00:28:37,870 --> 00:28:41,260
support for the straw burped within like

00:28:39,490 --> 00:28:42,970
you know a day of it being announced and

00:28:41,260 --> 00:28:45,250
actually most of the wait was waiting

00:28:42,970 --> 00:28:46,990
for the hydro transformers library to

00:28:45,250 --> 00:28:48,580
release into your version that included

00:28:46,990 --> 00:28:50,139
this as fought for it so I think you'll

00:28:48,580 --> 00:28:52,090
be able to expect pretty quick support

00:28:50,139 --> 00:28:53,889
for new architectures as they come out

00:28:52,090 --> 00:28:55,860
and we already do support this Dilbert

00:28:53,889 --> 00:29:01,470
which I'm excited to experiment with

00:28:55,860 --> 00:29:06,630
thank you hey hey this thing

00:29:01,470 --> 00:29:08,970
I hear clearly so this is any question

00:29:06,630 --> 00:29:11,850
about the packaging of Spacey treatin

00:29:08,970 --> 00:29:14,580
any and models more often when we try to

00:29:11,850 --> 00:29:18,210
use Spacey inside EDA breaks or any

00:29:14,580 --> 00:29:21,360
proprietary they say PC which is from

00:29:18,210 --> 00:29:23,340
any corporate since thus Spacey in ear

00:29:21,360 --> 00:29:25,290
molds are not coming up with a package

00:29:23,340 --> 00:29:27,750
of Spacely yourself we are facing

00:29:25,290 --> 00:29:30,390
problem on installing them differently

00:29:27,750 --> 00:29:32,930
so if their plan as in probably to a

00:29:30,390 --> 00:29:35,910
tittle of that I'm sorry

00:29:32,930 --> 00:29:37,950
the spec is a little bit distorted and I

00:29:35,910 --> 00:29:39,300
but I really didn't catch that and I

00:29:37,950 --> 00:29:42,750
think we're actually at the end of the

00:29:39,300 --> 00:29:45,750
time so I should have when we use a

00:29:42,750 --> 00:29:48,360
speci in ER models the space if retain

00:29:45,750 --> 00:29:51,270
in ear models we have to install its

00:29:48,360 --> 00:29:52,860
language model separately after we

00:29:51,270 --> 00:29:54,780
install species yes

00:29:52,860 --> 00:29:56,790
so that is creating a lot of problem

00:29:54,780 --> 00:30:00,330
when we try to use that in dynamics or

00:29:56,790 --> 00:30:03,360
probably in any proprietary system okay

00:30:00,330 --> 00:30:07,530
well um you can always just the models

00:30:03,360 --> 00:30:10,020
are just these like tar files to the

00:30:07,530 --> 00:30:12,500
served out of github and you can point

00:30:10,020 --> 00:30:15,720
spicy to a directory instead of a

00:30:12,500 --> 00:30:17,850
package so if you if your deployment

00:30:15,720 --> 00:30:22,650
needs to refer to the most directories

00:30:17,850 --> 00:30:25,260
instead of as you know basically pip

00:30:22,650 --> 00:30:27,990
packages you can do that but you can

00:30:25,260 --> 00:30:29,310
also just download the the archives and

00:30:27,990 --> 00:30:30,810
however you're installing space and you

00:30:29,310 --> 00:30:33,060
should be also be able to install the

00:30:30,810 --> 00:30:34,590
models so there should be a range of

00:30:33,060 --> 00:30:39,210
ways that you can solve this sort of

00:30:34,590 --> 00:30:41,430
problem the challenge if you have seen

00:30:39,210 --> 00:30:43,860
in derivatives is the moment we do that

00:30:41,430 --> 00:30:45,920
every time you install it every time we

00:30:43,860 --> 00:30:48,630
reshot the cluster we have been so again

00:30:45,920 --> 00:30:50,910
I'm sorry but I think we're out of time

00:30:48,630 --> 00:30:51,450
and we can talk about this offline I'm

00:30:50,910 --> 00:30:53,750
sorry

00:30:51,450 --> 00:30:53,750

YouTube URL: https://www.youtube.com/watch?v=NxqY0eVRgp0


