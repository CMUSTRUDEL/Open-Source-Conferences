Title: Performance Optimization with Python Elasticsearch - Anisha Swain & Manaswini Das
Publication date: 2019-11-16
Playlist: PyCon India 2019
Description: 
	This talk was presented at PyCon India 2019, on Oct 12th - 13th, at the Chennai Trade Centre.
Website: https://in.pycon.org/2019
Captions: 
	00:00:04,690 --> 00:00:10,900
hello Pythian estas hello Chennai and

00:00:07,240 --> 00:00:12,430
hello world we are here to present on

00:00:10,900 --> 00:00:14,860
performance optimization with

00:00:12,430 --> 00:00:17,619
elasticsearch I'm initially doesn't

00:00:14,860 --> 00:00:19,150
she's anisha swine and before we go

00:00:17,619 --> 00:00:21,550
forward with the talk we would like to

00:00:19,150 --> 00:00:21,939
introduce ourselves we both are Red

00:00:21,550 --> 00:00:23,590
Hatters

00:00:21,939 --> 00:00:25,779
and we are active open-source

00:00:23,590 --> 00:00:28,239
contributors and speaking about any

00:00:25,779 --> 00:00:30,520
shows she is research fellow with Indian

00:00:28,239 --> 00:00:32,980
Academy of Sciences and she is also not

00:00:30,520 --> 00:00:36,700
this I will hand over the control to any

00:00:32,980 --> 00:00:38,740
show right now sorry for that so first

00:00:36,700 --> 00:00:41,770
we will talk about basic elasticsearch

00:00:38,740 --> 00:00:43,480
and its architecture and then Monash uni

00:00:41,770 --> 00:00:45,820
will show you some of the demos how

00:00:43,480 --> 00:00:48,730
exactly we index data in elasticsearch

00:00:45,820 --> 00:00:50,890
and so on and at the end I will tell

00:00:48,730 --> 00:00:53,110
about how the techniques that we can use

00:00:50,890 --> 00:00:55,960
to increase the performance in when you

00:00:53,110 --> 00:00:57,880
are using elastic search so what is

00:00:55,960 --> 00:01:00,370
elastic search elastic search is a

00:00:57,880 --> 00:01:02,950
distributed search and analytics engine

00:01:00,370 --> 00:01:05,590
and please mark the word powerful it's a

00:01:02,950 --> 00:01:08,500
very powerful system elastic search is

00:01:05,590 --> 00:01:10,749
the heart of elastic sack in elastics

00:01:08,500 --> 00:01:12,999
that we use log stash and beads to

00:01:10,749 --> 00:01:15,700
collect and aggregate data by aggregate

00:01:12,999 --> 00:01:18,460
I mean I mean do more analysis on the

00:01:15,700 --> 00:01:21,429
data and we use visual cabana for

00:01:18,460 --> 00:01:23,679
visualization in elastic search we can

00:01:21,429 --> 00:01:27,490
store both structured and unstructured

00:01:23,679 --> 00:01:30,009
data and just not take status or we can

00:01:27,490 --> 00:01:33,340
store numerical and geospatial data as

00:01:30,009 --> 00:01:37,799
well and yeah it's comparatively easier

00:01:33,340 --> 00:01:42,130
than other analytics and search engines

00:01:37,799 --> 00:01:45,399
so this is the whole structure of

00:01:42,130 --> 00:01:46,990
elastic stack we use beats to collect

00:01:45,399 --> 00:01:49,179
the data and then block stirrers

00:01:46,990 --> 00:01:51,880
aggregate and analyze on the collected

00:01:49,179 --> 00:01:54,579
data elastic elastic search your index

00:01:51,880 --> 00:01:58,779
it and store it and after that we

00:01:54,579 --> 00:02:00,639
visualize the data in Cabana elastic

00:01:58,779 --> 00:02:04,090
search are also provides a premiere

00:02:00,639 --> 00:02:06,779
version of of the software which is

00:02:04,090 --> 00:02:09,280
called X pack which also gives security

00:02:06,779 --> 00:02:11,890
alerting when there is a problem in the

00:02:09,280 --> 00:02:17,110
system monitoring and also some machine

00:02:11,890 --> 00:02:19,210
learning analysis so a why lastik search

00:02:17,110 --> 00:02:23,320
elastic searches are persistent data

00:02:19,210 --> 00:02:26,650
storage which is a distributed document

00:02:23,320 --> 00:02:29,140
store it so it stores data now serialize

00:02:26,650 --> 00:02:32,590
JSON format which makes it makes it very

00:02:29,140 --> 00:02:37,600
reliable and very fast the such time is

00:02:32,590 --> 00:02:39,400
or approximately one second so yeah it

00:02:37,600 --> 00:02:41,230
is very high indexing rate which is

00:02:39,400 --> 00:02:45,340
approximately two to three times of

00:02:41,230 --> 00:02:47,800
normal indexing it provides sequential

00:02:45,340 --> 00:02:51,100
input-output pattern and yeah

00:02:47,800 --> 00:02:53,260
it also indexes key malays data with the

00:02:51,100 --> 00:02:55,330
dynamic mapping it can automatically

00:02:53,260 --> 00:02:58,240
detect and add new fields in the date

00:02:55,330 --> 00:03:02,620
new fields and data types when you we

00:02:58,240 --> 00:03:04,750
are storing the data it's very easy to

00:03:02,620 --> 00:03:07,900
maintain data in elastic search because

00:03:04,750 --> 00:03:10,300
of its scalability elastic source

00:03:07,900 --> 00:03:12,160
elastic searches are a physical

00:03:10,300 --> 00:03:14,980
collection of documents we can

00:03:12,160 --> 00:03:16,750
distribute the documents over various

00:03:14,980 --> 00:03:20,320
nodes and the nodes can again be

00:03:16,750 --> 00:03:23,200
distributed in our cluster so if we need

00:03:20,320 --> 00:03:25,620
our data is increasing then we can

00:03:23,200 --> 00:03:29,200
simply add more nodes to the cluster and

00:03:25,620 --> 00:03:31,690
scale it for real faster search elastic

00:03:29,200 --> 00:03:35,280
search uses data structures like

00:03:31,690 --> 00:03:37,450
inverted index kdb which gives

00:03:35,280 --> 00:03:41,470
approximately real-time full-text

00:03:37,450 --> 00:03:43,500
queries and as I said ability of

00:03:41,470 --> 00:03:47,019
clustering and ease of data maintenance

00:03:43,500 --> 00:03:49,269
we can also write three types of queries

00:03:47,019 --> 00:03:52,150
- for elastic search structured queries

00:03:49,269 --> 00:03:54,790
like we write in SQL or full text

00:03:52,150 --> 00:03:57,780
queries like query DSL and we can also

00:03:54,790 --> 00:04:00,489
combine both to make complex queries now

00:03:57,780 --> 00:04:03,250
I will hand over to Monash uni to talk

00:04:00,489 --> 00:04:06,010
about the basic architecture Thank You

00:04:03,250 --> 00:04:09,190
Anisha yeah as you can see right here

00:04:06,010 --> 00:04:11,560
there are in a node there are two charts

00:04:09,190 --> 00:04:13,180
one is you know you know a dark blue

00:04:11,560 --> 00:04:16,269
color and then it's a light blue color

00:04:13,180 --> 00:04:18,299
both of them are shards so each shard

00:04:16,269 --> 00:04:20,290
contains a collection of documents and

00:04:18,299 --> 00:04:22,419
these documents are distributed over

00:04:20,290 --> 00:04:23,830
shots and there are a collection of

00:04:22,419 --> 00:04:26,320
shards which are distributed over nodes

00:04:23,830 --> 00:04:28,270
and all these nodes are distributed over

00:04:26,320 --> 00:04:30,639
clusters as is already mentioned by

00:04:28,270 --> 00:04:32,520
Anisha elasticsearch is

00:04:30,639 --> 00:04:35,409
logical grouping of physical shunts and

00:04:32,520 --> 00:04:37,990
each document is a part of a primary

00:04:35,409 --> 00:04:43,180
shot and obviously we can have a replica

00:04:37,990 --> 00:04:45,370
of a primary shot so what are primary

00:04:43,180 --> 00:04:46,960
and replica shots here we can see there

00:04:45,370 --> 00:04:49,569
are three notes but we will concentrate

00:04:46,960 --> 00:04:52,599
on the first two notes there is one

00:04:49,569 --> 00:04:54,580
primary sharp one replica shot but there

00:04:52,599 --> 00:04:57,550
you can note this that the replica shard

00:04:54,580 --> 00:05:00,009
of the first node is there in the second

00:04:57,550 --> 00:05:02,080
node that is in case if the second you

00:05:00,009 --> 00:05:04,210
know the first load node clashes then

00:05:02,080 --> 00:05:07,740
there is second node which can you know

00:05:04,210 --> 00:05:07,740
give a hot reload of the data

00:05:07,949 --> 00:05:12,729
moving on to performance consideration

00:05:10,419 --> 00:05:14,889
about shard sizes as discussed before if

00:05:12,729 --> 00:05:16,870
the shard the number of shots

00:05:14,889 --> 00:05:18,550
increases the cluster will grow and so

00:05:16,870 --> 00:05:20,740
there is always a maintenance problem

00:05:18,550 --> 00:05:22,330
the larger the shards it is difficult to

00:05:20,740 --> 00:05:25,360
move it within the elastic search so

00:05:22,330 --> 00:05:28,529
obviously it depends average card size

00:05:25,360 --> 00:05:30,610
is from few GB to 10 a few tens of GB

00:05:28,529 --> 00:05:33,599
nodes need to be in the same network

00:05:30,610 --> 00:05:37,210
which search engine inside a cluster

00:05:33,599 --> 00:05:39,099
plus plus the application Raskin cluster

00:05:37,210 --> 00:05:42,219
replication is a type of replication in

00:05:39,099 --> 00:05:44,050
which a copy of the remote cluster is

00:05:42,219 --> 00:05:49,930
stored in a local cluster so this is

00:05:44,050 --> 00:05:52,539
used to provide backup for anyone who

00:05:49,930 --> 00:05:55,149
doesn't know here elasticsearch also has

00:05:52,539 --> 00:05:57,250
a Python client by the name of elastic

00:05:55,149 --> 00:06:02,500
search by if you want to know more you

00:05:57,250 --> 00:06:05,680
can just go to this link it's demo time

00:06:02,500 --> 00:06:10,259
so I already have kibana analyze so such

00:06:05,680 --> 00:06:10,259
setup in my device I just check

00:06:17,110 --> 00:06:23,020
so yeah looking to the first query I am

00:06:20,560 --> 00:06:25,629
trying to index some documents so this

00:06:23,020 --> 00:06:27,280
contains a put request we can also have

00:06:25,629 --> 00:06:29,110
a post request but the difference is

00:06:27,280 --> 00:06:31,750
what request can have an ID that we have

00:06:29,110 --> 00:06:33,879
right here post request doesn't need to

00:06:31,750 --> 00:06:37,629
have an ID there can be a hash value

00:06:33,879 --> 00:06:45,729
that is automatically given to it if you

00:06:37,629 --> 00:06:48,550
run this we get this answer so yeah here

00:06:45,729 --> 00:06:53,830
we have a total of two shots one

00:06:48,550 --> 00:06:56,740
successful shard and since this is

00:06:53,830 --> 00:07:00,300
already indexed we can just run a get

00:06:56,740 --> 00:07:03,009
command so yeah you have it right here

00:07:00,300 --> 00:07:08,289
we can as we have already indexed it we

00:07:03,009 --> 00:07:12,819
can also try to delete it yeah so here

00:07:08,289 --> 00:07:17,349
we can see the result is deleted so I

00:07:12,819 --> 00:07:20,469
have tried to you know do this you know

00:07:17,349 --> 00:07:22,449
indexing of bulk of documents we already

00:07:20,469 --> 00:07:24,580
have accounts adjacent that is already

00:07:22,449 --> 00:07:26,110
there in the website of elasticsearch so

00:07:24,580 --> 00:07:29,580
if you wanna you know look at the

00:07:26,110 --> 00:07:33,279
database you can go there and look so

00:07:29,580 --> 00:07:36,370
yeah that is come just have a look at

00:07:33,279 --> 00:07:38,259
the response that we have received the

00:07:36,370 --> 00:07:40,000
two key value shows the amount of

00:07:38,259 --> 00:07:43,449
milliseconds that elasticsearch is taken

00:07:40,000 --> 00:07:46,029
to produce the response timed out is

00:07:43,449 --> 00:07:49,479
just to check whether the search has

00:07:46,029 --> 00:07:52,539
timed out or not shards means then a

00:07:49,479 --> 00:07:55,419
number of shards that it is looking for

00:07:52,539 --> 00:07:58,690
looking into it is the total of one shot

00:07:55,419 --> 00:08:00,520
and we have one successful chart hits

00:07:58,690 --> 00:08:03,819
means the number of result that is

00:08:00,520 --> 00:08:08,259
received and here we can see thousand

00:08:03,819 --> 00:08:10,930
and relation is equal and moving on to

00:08:08,259 --> 00:08:13,719
score score is like how will the datum

00:08:10,930 --> 00:08:17,169
HS matches with the search query so here

00:08:13,719 --> 00:08:19,650
we have a maximum score of 1.0 so these

00:08:17,169 --> 00:08:23,949
are the hits elasticsearch provides a

00:08:19,650 --> 00:08:26,669
default of 10 documents so we can see 10

00:08:23,949 --> 00:08:26,669
documents right here

00:08:27,940 --> 00:08:32,870
we can also have a match query match

00:08:30,980 --> 00:08:36,320
query in which suppose we want to match

00:08:32,870 --> 00:08:38,810
a particular you know here right here we

00:08:36,320 --> 00:08:41,360
have a example that is we have we want

00:08:38,810 --> 00:08:43,640
to match whether like the count number

00:08:41,360 --> 00:08:48,680
should be twenty so here we write a

00:08:43,640 --> 00:08:52,030
match query and if we run it we have one

00:08:48,680 --> 00:08:55,970
successful shard and we have one hit

00:08:52,030 --> 00:08:59,570
okay moving on we also have a bull query

00:08:55,970 --> 00:09:03,470
which keeps a response with when only

00:08:59,570 --> 00:09:06,380
the output is true so here we have a

00:09:03,470 --> 00:09:11,390
match query again where the address

00:09:06,380 --> 00:09:15,110
should be either mil or plain if we just

00:09:11,390 --> 00:09:20,450
run it we get this a value of nineteen

00:09:15,110 --> 00:09:23,390
heads then we move on to executing

00:09:20,450 --> 00:09:26,540
filters so elasticsearch has two types

00:09:23,390 --> 00:09:29,180
of contexts like two types of query da

00:09:26,540 --> 00:09:30,530
cells that would say query context and

00:09:29,180 --> 00:09:33,950
filter context so this is the filter

00:09:30,530 --> 00:09:35,870
context filter context as you might be

00:09:33,950 --> 00:09:38,000
knowing if you guys are already using

00:09:35,870 --> 00:09:39,500
Flipkart that everyone uses like wilcott

00:09:38,000 --> 00:09:41,960
amazon all of that so you have a filter

00:09:39,500 --> 00:09:45,470
of each other we can you know have our

00:09:41,960 --> 00:09:49,030
t-shirts with full sleeves or not full

00:09:45,470 --> 00:09:49,030
sleeve so it just should match

00:09:51,860 --> 00:09:58,440
yeah summarizing we index some documents

00:09:55,440 --> 00:10:01,620
we did some put and post requests we

00:09:58,440 --> 00:10:04,740
deleted some like deleted some documents

00:10:01,620 --> 00:10:09,060
we conduct like ran a match query bool

00:10:04,740 --> 00:10:10,950
query executed filters when you're

00:10:09,060 --> 00:10:12,660
coming across elasticsearch we always

00:10:10,950 --> 00:10:14,640
get to cross this world compress this

00:10:12,660 --> 00:10:16,170
word called aggregations which just

00:10:14,640 --> 00:10:18,810
provides an ability to extract

00:10:16,170 --> 00:10:21,360
statistics from the data that is we can

00:10:18,810 --> 00:10:23,700
conduct a lot of SQL Roubaix queries or

00:10:21,360 --> 00:10:25,050
query similar to that this is very

00:10:23,700 --> 00:10:27,240
powerful and efficient that is we can

00:10:25,050 --> 00:10:28,710
run our query within our query and we

00:10:27,240 --> 00:10:30,030
can run queries and multiple

00:10:28,710 --> 00:10:31,590
aggregations and multiple of these

00:10:30,030 --> 00:10:33,780
queries all together like in a nested

00:10:31,590 --> 00:10:37,470
manner is well get the results back or

00:10:33,780 --> 00:10:39,870
both either in one shot avoiding network

00:10:37,470 --> 00:10:42,840
round trips these are the types of

00:10:39,870 --> 00:10:44,700
aggregation since there is a lot of time

00:10:42,840 --> 00:10:47,010
constraint so I'm just you know skimming

00:10:44,700 --> 00:10:49,350
across this matrix aggregation pipeline

00:10:47,010 --> 00:10:50,400
matrix and bucket so matrix is like just

00:10:49,350 --> 00:10:51,600
providing the statistics of the

00:10:50,400 --> 00:10:53,700
aggregation pipeline is like the

00:10:51,600 --> 00:10:55,620
aggregation of aggregations matrix is

00:10:53,700 --> 00:10:58,260
like providing a matrix out of the

00:10:55,620 --> 00:11:00,180
number of documents that is doubtful

00:10:58,260 --> 00:11:01,650
whether it will be there in the next

00:11:00,180 --> 00:11:02,880
versions or not and we have bucket

00:11:01,650 --> 00:11:05,340
aggregation bucket is a set of documents

00:11:02,880 --> 00:11:07,530
and it will just run an aggregation on a

00:11:05,340 --> 00:11:11,880
bucket orders on a particular set of

00:11:07,530 --> 00:11:15,120
documents so we have analysis which is

00:11:11,880 --> 00:11:18,380
like converting a text to terms or

00:11:15,120 --> 00:11:22,320
tokens according to a given filter so

00:11:18,380 --> 00:11:24,840
these are there are analyzers normalize

00:11:22,320 --> 00:11:26,790
earths tokenizer token filters and

00:11:24,840 --> 00:11:30,480
character filters moving on to what they

00:11:26,790 --> 00:11:32,520
are what these are analyzer weather can

00:11:30,480 --> 00:11:34,890
be built in or custom it's just a packet

00:11:32,520 --> 00:11:36,870
that is like three which has three

00:11:34,890 --> 00:11:38,730
building blocks that is character

00:11:36,870 --> 00:11:43,230
filters organizers and token filters

00:11:38,730 --> 00:11:46,170
will slowly come across this anomaly is

00:11:43,230 --> 00:11:48,930
like it only emits a single token and it

00:11:46,170 --> 00:11:51,480
requires the use of token filters token

00:11:48,930 --> 00:11:55,170
is like just taking a text and you know

00:11:51,480 --> 00:11:56,190
dividing it into unique tokens there are

00:11:55,170 --> 00:11:58,290
many types of tokens that we come

00:11:56,190 --> 00:12:03,210
because this is just a very small subset

00:11:58,290 --> 00:12:05,050
of the token Isis that we have there are

00:12:03,210 --> 00:12:07,840
token filters like

00:12:05,050 --> 00:12:10,030
this accepts a set of tokens from the

00:12:07,840 --> 00:12:12,670
tokenizer and can modify them you know

00:12:10,030 --> 00:12:15,490
just do it lowercase or just you know

00:12:12,670 --> 00:12:17,740
remove stop words remove some words that

00:12:15,490 --> 00:12:21,100
are being repeated and we have character

00:12:17,740 --> 00:12:22,240
filter which have which are used to

00:12:21,100 --> 00:12:24,340
pre-process the stream of characters

00:12:22,240 --> 00:12:26,380
before it is passed to the tokenizer we

00:12:24,340 --> 00:12:28,300
have read stream achieve a strip card

00:12:26,380 --> 00:12:31,150
character which is an example which is

00:12:28,300 --> 00:12:35,080
just you to know remove the opening and

00:12:31,150 --> 00:12:38,470
closing tags in HTML and also we have

00:12:35,080 --> 00:12:42,070
suppose we have you know patterns such

00:12:38,470 --> 00:12:47,170
as ampersand a pause it just converts

00:12:42,070 --> 00:12:50,770
its to apostrophe moving on to scanners

00:12:47,170 --> 00:12:54,220
to LAPI so this can search type and this

00:12:50,770 --> 00:12:55,390
role epi I used to you know retrieve

00:12:54,220 --> 00:12:57,010
large number of documents from

00:12:55,390 --> 00:12:59,290
elasticsearch efficiently without paying

00:12:57,010 --> 00:13:01,960
the price of big pagination stole is

00:12:59,290 --> 00:13:04,630
like you know sending multiple requests

00:13:01,960 --> 00:13:06,400
and just giving us the request in you

00:13:04,630 --> 00:13:09,940
know very then we reach the end of the

00:13:06,400 --> 00:13:11,440
page page then you have scan this just

00:13:09,940 --> 00:13:15,760
removes the costly part of deep

00:13:11,440 --> 00:13:18,610
pagination so this just makes the tasks

00:13:15,760 --> 00:13:21,520
really easier so I hope this was not too

00:13:18,610 --> 00:13:23,590
fast we used to still we have till

00:13:21,520 --> 00:13:29,830
minutes left I'm handing over the

00:13:23,590 --> 00:13:32,020
control panel shop yeah so we will

00:13:29,830 --> 00:13:33,880
discuss about some of the performance

00:13:32,020 --> 00:13:36,370
enhancement techniques that we can use

00:13:33,880 --> 00:13:40,060
to enhance the performance of indexing

00:13:36,370 --> 00:13:44,140
searching and make it less expensive for

00:13:40,060 --> 00:13:46,780
us so generally these are the doors that

00:13:44,140 --> 00:13:49,930
we shouldn't do while using

00:13:46,780 --> 00:13:52,240
elasticsearch we shouldn't return large

00:13:49,930 --> 00:13:54,280
number of data set at a time instead of

00:13:52,240 --> 00:13:57,610
that we should use scan and scroll API

00:13:54,280 --> 00:13:59,920
so that we can get some data and then

00:13:57,610 --> 00:14:02,290
again using a particular ID we can fetch

00:13:59,920 --> 00:14:05,620
another set of data Oh

00:14:02,290 --> 00:14:07,840
second is indexing large documents so

00:14:05,620 --> 00:14:11,560
indexing allow very large amount of

00:14:07,840 --> 00:14:14,410
document can now effectively sir affect

00:14:11,560 --> 00:14:16,320
the performance in a high amount so

00:14:14,410 --> 00:14:18,029
instead of that water

00:14:16,320 --> 00:14:21,209
the max Continental length offer

00:14:18,029 --> 00:14:24,149
document is added by default hundred MB

00:14:21,209 --> 00:14:26,610
but we can scale it up to 2 GB by some

00:14:24,149 --> 00:14:29,009
setting but it is expensive right like

00:14:26,610 --> 00:14:32,550
we can have memory you more memory usage

00:14:29,009 --> 00:14:35,100
and more stress on Network so we should

00:14:32,550 --> 00:14:37,829
know what exactly to index and we should

00:14:35,100 --> 00:14:43,259
index those the whole dock or the whole

00:14:37,829 --> 00:14:45,779
data in like small small parts so as

00:14:43,259 --> 00:14:48,480
monotony has already showed you the the

00:14:45,779 --> 00:14:52,199
scoring so scoring as the score here

00:14:48,480 --> 00:14:54,000
shows how efficiently we have searched

00:14:52,199 --> 00:14:56,279
the whole documents like what's the

00:14:54,000 --> 00:14:59,100
score of the document that we searched

00:14:56,279 --> 00:15:00,480
or filtered so scoring might not be

00:14:59,100 --> 00:15:04,500
consistent because when you search

00:15:00,480 --> 00:15:07,500
further or data it might go to replica

00:15:04,500 --> 00:15:10,440
shots or a primary shot depending

00:15:07,500 --> 00:15:13,259
depending on your search so elastic

00:15:10,440 --> 00:15:16,170
search has a setting of brief reference

00:15:13,259 --> 00:15:19,230
so that if a particular user is logging

00:15:16,170 --> 00:15:21,930
in with its ID the search will go to

00:15:19,230 --> 00:15:25,350
that particular shot so that it can get

00:15:21,930 --> 00:15:28,500
a consistent scoring and Ishod is

00:15:25,350 --> 00:15:31,250
responsible for its own scoring so the

00:15:28,500 --> 00:15:34,769
scoring of a document in a shot is

00:15:31,250 --> 00:15:39,120
depends on the index statistics so

00:15:34,769 --> 00:15:42,480
either we have to index all those shots

00:15:39,120 --> 00:15:45,149
with the same index tactics or we have

00:15:42,480 --> 00:15:48,750
to go through those all the shots and

00:15:45,149 --> 00:15:54,510
get a relative index statistics with DFS

00:15:48,750 --> 00:15:56,670
query than fetch so indexing of indexing

00:15:54,510 --> 00:15:59,100
performance enhancement to index the

00:15:56,670 --> 00:16:01,199
performance of indexing we shouldn't try

00:15:59,100 --> 00:16:04,889
it we should try to put bulk requests

00:16:01,199 --> 00:16:06,930
but again we have to figure out the

00:16:04,889 --> 00:16:09,029
optimal amount of bulk liquids that we

00:16:06,930 --> 00:16:10,829
can get so it's a trial and run process

00:16:09,029 --> 00:16:13,829
we have to run a benchmark and get our

00:16:10,829 --> 00:16:15,660
optimal size we should use multi thread

00:16:13,829 --> 00:16:18,120
six-term but again keep a watch on too

00:16:15,660 --> 00:16:20,519
many requests because it can throw an

00:16:18,120 --> 00:16:22,550
exception so if it is throwing an

00:16:20,519 --> 00:16:25,290
exception we can pause and again resume

00:16:22,550 --> 00:16:26,850
should increase refresh intervals or

00:16:25,290 --> 00:16:29,009
refresh intervals is

00:16:26,850 --> 00:16:31,769
the elasticsearch periodically refresh

00:16:29,009 --> 00:16:34,799
the indices which receives more or not

00:16:31,769 --> 00:16:37,019
more than searches in previous 32nd so

00:16:34,799 --> 00:16:39,449
either it's a good way to do it when

00:16:37,019 --> 00:16:42,779
your site has less traffic or increase

00:16:39,449 --> 00:16:44,879
the refresh interval and yeah we

00:16:42,779 --> 00:16:47,129
shouldn't create replica shots at the

00:16:44,879 --> 00:16:49,350
first we should disable the replica for

00:16:47,129 --> 00:16:53,100
initial load I know it's a little risky

00:16:49,350 --> 00:16:56,039
but we it's so very good for enhancing

00:16:53,100 --> 00:16:57,629
the performance of indexing and we can

00:16:56,039 --> 00:17:01,529
make use a technique called index

00:16:57,629 --> 00:17:04,110
generation to like lower the risk and

00:17:01,529 --> 00:17:07,980
make a replica shot according to the

00:17:04,110 --> 00:17:11,189
updated document so there are some more

00:17:07,980 --> 00:17:13,799
points like she don't know swapping

00:17:11,189 --> 00:17:16,439
should allocate more memory to the file

00:17:13,799 --> 00:17:19,230
system cache for buffering input/output

00:17:16,439 --> 00:17:21,750
operations and it's a good practice to

00:17:19,230 --> 00:17:23,699
use auto-generated IDs by elasticsearch

00:17:21,750 --> 00:17:26,730
so that it doesn't have to go and check

00:17:23,699 --> 00:17:29,190
for duplicates and yeah of course faster

00:17:26,730 --> 00:17:31,769
hardware and virtualized storage works

00:17:29,190 --> 00:17:34,850
with better input/output per second

00:17:31,769 --> 00:17:39,149
works pretty better with elasticsearch

00:17:34,850 --> 00:17:43,440
and we should also keep check on cluster

00:17:39,149 --> 00:17:46,440
health of elasticsearch because if a

00:17:43,440 --> 00:17:50,639
node goes down then it takes some some

00:17:46,440 --> 00:17:53,549
time to initialize the initialize it

00:17:50,639 --> 00:17:55,500
again so at that time it would be a low

00:17:53,549 --> 00:17:58,129
but it should be green when it's having

00:17:55,500 --> 00:18:01,289
primary and replication cryptic assad's

00:17:58,129 --> 00:18:04,110
visible merge throttling so if we're

00:18:01,289 --> 00:18:06,179
having problem in merging then we should

00:18:04,110 --> 00:18:08,639
disable much throttling for some time

00:18:06,179 --> 00:18:10,519
but again it's not read off because it

00:18:08,639 --> 00:18:13,259
might degrade the search performance

00:18:10,519 --> 00:18:15,269
third pulling rejection error comes when

00:18:13,259 --> 00:18:20,070
we send too many requests to your nodes

00:18:15,269 --> 00:18:22,320
it at a faster rate so he does we can

00:18:20,070 --> 00:18:29,600
scale the nodes or slow down our process

00:18:22,320 --> 00:18:32,490
of sending the requests disk usage so

00:18:29,600 --> 00:18:34,259
should disable to preserve disk usage

00:18:32,490 --> 00:18:36,659
sure we should disable unnecessary

00:18:34,259 --> 00:18:38,490
feature like we if we want to create a

00:18:36,659 --> 00:18:38,680
histogram out of numeric value then we

00:18:38,490 --> 00:18:40,540
do

00:18:38,680 --> 00:18:43,240
made filtering so you should disable

00:18:40,540 --> 00:18:45,670
indexing in string by default they have

00:18:43,240 --> 00:18:48,330
normalizes but if we don't one

00:18:45,670 --> 00:18:51,310
normalization then we can disable norms

00:18:48,330 --> 00:18:53,770
so you should also have a dynamic string

00:18:51,310 --> 00:18:56,920
mapping because it maps every string as

00:18:53,770 --> 00:19:00,000
the text and the keyword so we can

00:18:56,920 --> 00:19:03,130
specifically give our template for

00:19:00,000 --> 00:19:06,940
string mappings we should disable source

00:19:03,130 --> 00:19:09,820
if we don't need original JSON and or we

00:19:06,940 --> 00:19:14,170
can use best compression for taking

00:19:09,820 --> 00:19:16,510
negligible spaces force much API reduce

00:19:14,170 --> 00:19:19,920
the number of segments per shot which is

00:19:16,510 --> 00:19:23,380
physical memory which I shall have

00:19:19,920 --> 00:19:28,240
shrink API is used to reduce charts per

00:19:23,380 --> 00:19:29,910
index we should know use to decrease the

00:19:28,240 --> 00:19:37,630
disk usage you should use smallest

00:19:29,910 --> 00:19:40,420
numeric type and excuse me so if the

00:19:37,630 --> 00:19:43,570
Eternals are running low on desk then we

00:19:40,420 --> 00:19:47,890
have to add more data nodes to our

00:19:43,570 --> 00:19:51,970
cluster and scale it horizontally so or

00:19:47,890 --> 00:19:54,370
if we have only certain nodes running

00:19:51,970 --> 00:19:56,530
out of disk then it so you usually it's

00:19:54,370 --> 00:19:58,870
the same that we have initialized the

00:19:56,530 --> 00:20:05,530
index with very less amount of shots and

00:19:58,870 --> 00:20:09,430
we need to add on more shots so yeah we

00:20:05,530 --> 00:20:13,390
can scale the ELA we if were there are

00:20:09,430 --> 00:20:15,370
less there disk usage then we sorry a

00:20:13,390 --> 00:20:18,070
more disk usage then we can now scale

00:20:15,370 --> 00:20:22,660
the day scale the note horizontally and

00:20:18,070 --> 00:20:24,700
we can use rollover an alias for that so

00:20:22,660 --> 00:20:29,480
I think that's it for the performance

00:20:24,700 --> 00:20:32,529
constant thank you

00:20:29,480 --> 00:20:36,070
[Applause]

00:20:32,529 --> 00:20:36,070
any questions

00:20:45,500 --> 00:20:50,960
inaudible so how are the performance

00:20:49,230 --> 00:20:53,159
metrics for traditional relational data

00:20:50,960 --> 00:20:54,870
public will it work well for relational

00:20:53,159 --> 00:21:00,690
data also or should we prefer an SQL

00:20:54,870 --> 00:21:02,370
over it but the difference is it will be

00:21:00,690 --> 00:21:05,220
you know JSON format does that answer

00:21:02,370 --> 00:21:07,169
your question yeah I mean what are the

00:21:05,220 --> 00:21:11,010
performance metrics compared to

00:21:07,169 --> 00:21:15,090
traditional SQL databases have you used

00:21:11,010 --> 00:21:18,960
a few will ever know so a graph you'll

00:21:15,090 --> 00:21:20,909
in a particular way there are certain

00:21:18,960 --> 00:21:23,370
constraints that you have in relational

00:21:20,909 --> 00:21:27,630
databases that is let me just take this

00:21:23,370 --> 00:21:30,900
yeah that is sometimes we have to you

00:21:27,630 --> 00:21:33,000
know fill in a null data and that is it

00:21:30,900 --> 00:21:35,100
is something that you have to give a

00:21:33,000 --> 00:21:36,960
particular theater to a particular field

00:21:35,100 --> 00:21:39,510
like it is mandatory but it is not

00:21:36,960 --> 00:21:42,059
mandatory when it comes to elasticsearch

00:21:39,510 --> 00:21:43,770
that is a particular document can have a

00:21:42,059 --> 00:21:46,490
particular field or cannot have a

00:21:43,770 --> 00:21:46,490
particular field so

00:21:52,100 --> 00:21:59,570
okay yeah hi

00:21:57,140 --> 00:22:02,240
the question is very relevant to elastic

00:21:59,570 --> 00:22:07,220
seven products so we are actually using

00:22:02,240 --> 00:22:09,950
the necessary indexing with elastic so I

00:22:07,220 --> 00:22:13,040
it would be nice if you can just give

00:22:09,950 --> 00:22:15,620
give us some best practice to search or

00:22:13,040 --> 00:22:17,120
do filter queries in nested things so

00:22:15,620 --> 00:22:19,280
what we are trying to achieve is

00:22:17,120 --> 00:22:21,620
actually to want to get the component in

00:22:19,280 --> 00:22:25,070
the nested not as if not as a parent in

00:22:21,620 --> 00:22:27,920
the result while searching we have a

00:22:25,070 --> 00:22:30,560
similar kind of use case for our we had

00:22:27,920 --> 00:22:32,450
all we had to index some directory

00:22:30,560 --> 00:22:34,610
structure so it was pretty nested as

00:22:32,450 --> 00:22:36,590
well so what we are doing is as I

00:22:34,610 --> 00:22:39,680
mentioned earlier if there is a range

00:22:36,590 --> 00:22:42,170
query I can just move that to a keyword

00:22:39,680 --> 00:22:45,950
or a term and then we I can search that

00:22:42,170 --> 00:22:47,900
term instead of nesting I can like

00:22:45,950 --> 00:22:49,850
elaborate you the whole structure that

00:22:47,900 --> 00:22:52,730
we are using after the talk because it

00:22:49,850 --> 00:22:59,170
might need pen and paper okay I will

00:22:52,730 --> 00:23:00,670
continue this way

00:22:59,170 --> 00:23:03,520
[Music]

00:23:00,670 --> 00:23:06,910
hey we are using elasticsales you know

00:23:03,520 --> 00:23:10,330
and I getting the same performance issue

00:23:06,910 --> 00:23:12,970
what you will mention and even I tried

00:23:10,330 --> 00:23:15,220
most of the things I have order lots of

00:23:12,970 --> 00:23:26,350
fields I have removed lots of things

00:23:15,220 --> 00:23:28,960
then still a meeting the issue so after

00:23:26,350 --> 00:23:31,540
30 days and getting the issue so I need

00:23:28,960 --> 00:23:35,890
to delete the data again I need to do a

00:23:31,540 --> 00:23:37,930
clean new sitter then the problem is we

00:23:35,890 --> 00:23:40,210
usually everything is the same order

00:23:37,930 --> 00:23:43,390
everything is the same only the data is

00:23:40,210 --> 00:23:45,940
mattress so whether we need to delete

00:23:43,390 --> 00:23:48,420
the data every but close days or

00:23:45,940 --> 00:23:48,420
something like that

00:23:57,870 --> 00:24:06,610
and I'm not pretty much sure how exactly

00:24:03,820 --> 00:24:11,350
but it stores it somewhere like you know

00:24:06,610 --> 00:24:14,530
compressed whale and it uses it later or

00:24:11,350 --> 00:24:17,250
it's a good way to delete the data if

00:24:14,530 --> 00:24:17,250
you don't use it

00:24:21,299 --> 00:24:30,940
around xing concept is that is what you

00:24:25,120 --> 00:24:34,590
are talking about reindex snapshot

00:24:30,940 --> 00:24:34,590

YouTube URL: https://www.youtube.com/watch?v=z8TC7U0QEAM


