Title: IndicNLP - Adam Shamsudeen, Kamal Raj, Selva Kumar
Publication date: 2019-11-16
Playlist: PyCon India 2019
Description: 
	This talk was presented at PyCon India 2019, on Oct 12th - 13th, at the Chennai Trade Centre.
Website: https://in.pycon.org/2019
Captions: 
	00:00:04,819 --> 00:00:12,049
yeah so guys it's afternoon right

00:00:09,049 --> 00:00:15,080
it's the last day seriously I was very

00:00:12,049 --> 00:00:18,769
excited to be here you know back in

00:00:15,080 --> 00:00:20,660
PyCon everyone so much people right

00:00:18,769 --> 00:00:23,840
everyone is working on pi python yeah

00:00:20,660 --> 00:00:27,110
I'm working on Python so learn something

00:00:23,840 --> 00:00:29,029
new every time when I come to qui-gon we

00:00:27,110 --> 00:00:30,289
can go back and experiment come back

00:00:29,029 --> 00:00:34,149
next year meet new people that's

00:00:30,289 --> 00:00:36,920
happening for some time yeah yeah and so

00:00:34,149 --> 00:00:40,519
today we are here to present in Deakin

00:00:36,920 --> 00:00:43,370
LP so we have been working on this for

00:00:40,519 --> 00:00:45,170
and year now I will start with the story

00:00:43,370 --> 00:00:47,690
everyone has been asking me how we

00:00:45,170 --> 00:00:51,429
started this why did you go with

00:00:47,690 --> 00:00:53,239
indicate LP what was the motivation and

00:00:51,429 --> 00:00:54,399
everything but it's a funny story

00:00:53,239 --> 00:00:56,929
actually

00:00:54,399 --> 00:01:04,879
so first I'll introduce my teammates I'm

00:00:56,929 --> 00:01:07,659
Adam shim Sabine this is selvakumar it's

00:01:04,879 --> 00:01:10,040
caramel Raj and we also have so home

00:01:07,659 --> 00:01:13,070
currently is not with us because he's in

00:01:10,040 --> 00:01:15,950
Singapore doing his master's so so

00:01:13,070 --> 00:01:18,730
homework done Bangla and I income and

00:01:15,950 --> 00:01:27,230
worked in Malayalam and Telugu and Tamil

00:01:18,730 --> 00:01:30,590
so yeah yeah everyone has this dilemma

00:01:27,230 --> 00:01:33,470
right should we do it or not right

00:01:30,590 --> 00:01:36,170
that's the whole how this began

00:01:33,470 --> 00:01:38,660
I work in summer technologies all all of

00:01:36,170 --> 00:01:41,240
us actually when we started we work I

00:01:38,660 --> 00:01:45,020
was working on a project called Dalia

00:01:41,240 --> 00:01:48,260
it's a chat board platform so we were

00:01:45,020 --> 00:01:50,860
using all the you know latest NLP

00:01:48,260 --> 00:01:53,480
techniques you know transfer learning

00:01:50,860 --> 00:01:55,850
you LM Fiat DP CNN different

00:01:53,480 --> 00:01:58,340
architectures right so I was thinking

00:01:55,850 --> 00:01:59,930
why why why can't we do the same thing

00:01:58,340 --> 00:02:01,729
in our own languages because there are a

00:01:59,930 --> 00:02:04,670
lot of things out there a lot of

00:02:01,729 --> 00:02:07,490
research papers out of datasets but why

00:02:04,670 --> 00:02:10,580
isn't anything in our mother tanks maybe

00:02:07,490 --> 00:02:13,549
Tamil or Malayalam or Hindi there isn't

00:02:10,580 --> 00:02:15,530
much right so what I did is I went

00:02:13,549 --> 00:02:17,780
through a couple of the papers like we

00:02:15,530 --> 00:02:22,040
can we get a data set that's when I

00:02:17,780 --> 00:02:25,880
so paper from Salesforce it's from Steve

00:02:22,040 --> 00:02:28,580
and melody and I don't know the coder so

00:02:25,880 --> 00:02:30,470
I thought maybe I'll tweeted him and how

00:02:28,580 --> 00:02:33,320
he got that I said how can I do the same

00:02:30,470 --> 00:02:34,910
thing so back in August last year I was

00:02:33,320 --> 00:02:37,400
just thinking I'll just tweet at him

00:02:34,910 --> 00:02:38,959
then I thought should I do this when

00:02:37,400 --> 00:02:40,730
everyone thinks that way right should I

00:02:38,959 --> 00:02:43,310
send them and he's like a big guy

00:02:40,730 --> 00:02:44,990
should I send him on to it really

00:02:43,310 --> 00:02:46,940
respond what will I everyone think I

00:02:44,990 --> 00:02:49,700
thought okay let's do this

00:02:46,940 --> 00:02:51,800
so I send him a tweet and he responds

00:02:49,700 --> 00:02:54,320
back he said yeah we have script for

00:02:51,800 --> 00:02:57,770
that write it just send me an email or a

00:02:54,320 --> 00:03:00,980
message I send you this thing so yeah

00:02:57,770 --> 00:03:02,840
and so we wanted to do ul of it so

00:03:00,980 --> 00:03:06,860
that's when you elf it came out so in

00:03:02,840 --> 00:03:08,390
your lymph it L what we were excited

00:03:06,860 --> 00:03:10,190
because it was transfer learning there

00:03:08,390 --> 00:03:11,959
was never transfer learning in text so

00:03:10,190 --> 00:03:14,510
we wanted to do it for Malayalam because

00:03:11,959 --> 00:03:17,540
we don't have enough data right

00:03:14,510 --> 00:03:19,489
so we also I also tweeted at Sebastian

00:03:17,540 --> 00:03:22,209
ruder and Jeremy holes so Sebastian

00:03:19,489 --> 00:03:26,299
Rhoda if you're into NLP like he's like

00:03:22,209 --> 00:03:28,160
really really good in NLP so I tweeted

00:03:26,299 --> 00:03:29,989
at him and he also responded so that's

00:03:28,160 --> 00:03:32,360
like a boost these guys are really big

00:03:29,989 --> 00:03:34,370
into an LP and they're ready to help

00:03:32,360 --> 00:03:35,900
anywhere anyone anywhere in the world if

00:03:34,370 --> 00:03:38,989
you have just a Twitter right just weed

00:03:35,900 --> 00:03:44,959
at them they'll be happy to respond so

00:03:38,989 --> 00:03:49,160
what we did is we got the data because

00:03:44,959 --> 00:03:51,920
there was some helper these things so we

00:03:49,160 --> 00:03:55,910
went on we downloaded it we we got the

00:03:51,920 --> 00:03:58,670
data right so what next

00:03:55,910 --> 00:04:01,340
that's what the story of a banana cake

00:03:58,670 --> 00:04:05,329
comes in okay so it's a funny story

00:04:01,340 --> 00:04:08,480
again I got the data set I don't know

00:04:05,329 --> 00:04:11,329
what to do because I know NLP but our

00:04:08,480 --> 00:04:14,660
team has a very good NLP people like

00:04:11,329 --> 00:04:19,400
Silva and kummel so I asked common man

00:04:14,660 --> 00:04:21,229
can we do Ulf it on this you know our

00:04:19,400 --> 00:04:25,190
interest is like yeah we'll do next week

00:04:21,229 --> 00:04:26,630
man we'll do it next week maybe if you

00:04:25,190 --> 00:04:29,990
buy me a banana click I'll do it today

00:04:26,630 --> 00:04:31,759
right it was like okay I'll do it today

00:04:29,990 --> 00:04:35,029
if you buy me a banana case I got

00:04:31,759 --> 00:04:37,490
okay then I go I went out I just bought

00:04:35,029 --> 00:04:40,219
a banana cake I he had like two slices

00:04:37,490 --> 00:04:42,860
he started coding and I told him like if

00:04:40,219 --> 00:04:46,430
you are done with when you finish it you

00:04:42,860 --> 00:04:48,680
can have the whole cake right so he got

00:04:46,430 --> 00:04:52,669
excited and got excited because of the

00:04:48,680 --> 00:04:55,330
cake and he started doing it and we were

00:04:52,669 --> 00:04:57,770
really excited that we got a very good

00:04:55,330 --> 00:05:01,129
perplexity on the language model that we

00:04:57,770 --> 00:05:02,270
trained and we were like thinking what

00:05:01,129 --> 00:05:04,550
should we do next we have a language

00:05:02,270 --> 00:05:06,800
model that language model predicts the

00:05:04,550 --> 00:05:08,870
next word what should be Unix so that

00:05:06,800 --> 00:05:13,099
when he that's when we realised there is

00:05:08,870 --> 00:05:16,009
in any datasets for any task in

00:05:13,099 --> 00:05:17,779
Malayalam so we went on to a website a

00:05:16,009 --> 00:05:20,689
popular Murray our website and we

00:05:17,779 --> 00:05:23,599
downloaded some newspaper articles say

00:05:20,689 --> 00:05:26,029
and we categorized in them to five like

00:05:23,599 --> 00:05:29,120
sports business entertainment and we did

00:05:26,029 --> 00:05:31,279
a text classification on top of that and

00:05:29,120 --> 00:05:32,749
we got a very good accuracy of 92% and

00:05:31,279 --> 00:05:35,349
that's when we realized we can actually

00:05:32,749 --> 00:05:39,229
apply a lot of deep learning techniques

00:05:35,349 --> 00:05:39,680
in our languages so when we were done

00:05:39,229 --> 00:05:42,080
with that

00:05:39,680 --> 00:05:44,779
I told Salwa that we got a good thing so

00:05:42,080 --> 00:05:47,810
he's like oh you guys did it I also

00:05:44,779 --> 00:05:49,729
wanted to do it for thummell so he got

00:05:47,810 --> 00:05:52,189
excited and he did something for

00:05:49,729 --> 00:05:54,139
thummell and then soham joined he did

00:05:52,189 --> 00:05:56,749
something for bangla so that's how it

00:05:54,139 --> 00:05:59,990
all started right so when we went in

00:05:56,749 --> 00:06:03,649
deep we realized that there isn't much

00:05:59,990 --> 00:06:05,330
right no data sets in Malayalam Tamil or

00:06:03,649 --> 00:06:10,219
English that is publicly available and

00:06:05,330 --> 00:06:14,180
even if there is these are with academia

00:06:10,219 --> 00:06:15,979
like universities in India and they are

00:06:14,180 --> 00:06:17,059
not willing to give any data sets to

00:06:15,979 --> 00:06:19,430
outsiders

00:06:17,059 --> 00:06:21,589
you might be seeing a lot of research

00:06:19,430 --> 00:06:24,649
publications in I tip Li saying that

00:06:21,589 --> 00:06:28,580
they go to good accuracy on this data

00:06:24,649 --> 00:06:32,449
set using this model and they have a

00:06:28,580 --> 00:06:34,580
good classifier right but none of these

00:06:32,449 --> 00:06:36,979
things are publicly available so people

00:06:34,580 --> 00:06:41,509
like us can actually work on it improve

00:06:36,979 --> 00:06:43,860
it improve it and bring maybe a new

00:06:41,509 --> 00:06:47,310
models right new models

00:06:43,860 --> 00:06:49,380
so even though one day we found a in

00:06:47,310 --> 00:06:51,210
your data set publicly available and we

00:06:49,380 --> 00:06:53,040
were so excited right I don't know the

00:06:51,210 --> 00:06:55,200
name is first fire I guess have you

00:06:53,040 --> 00:06:57,290
downloaded and we realized that it's a

00:06:55,200 --> 00:06:59,430
zip file but it is password protected

00:06:57,290 --> 00:07:02,880
and they're not willing to share the

00:06:59,430 --> 00:07:04,920
password with us so these are the issues

00:07:02,880 --> 00:07:08,490
that we went through next I will call

00:07:04,920 --> 00:07:10,890
Salva and he'll explain how and how why

00:07:08,490 --> 00:07:23,070
not language is hard and why we need an

00:07:10,890 --> 00:07:24,570
open data platform I have a 14 year old

00:07:23,070 --> 00:07:27,000
friend who develops a hundred

00:07:24,570 --> 00:07:30,600
applications and I have a 40 year old

00:07:27,000 --> 00:07:33,450
friend who does data science I use the

00:07:30,600 --> 00:07:35,040
Arduino to create robots and people use

00:07:33,450 --> 00:07:38,460
Raspberry Pi Beagle bone to create

00:07:35,040 --> 00:07:40,470
drones in there is entire Hardware

00:07:38,460 --> 00:07:45,540
movements to create minute work like an

00:07:40,470 --> 00:07:48,510
alternative internet so these are all

00:07:45,540 --> 00:07:49,950
possible due to many factors but one of

00:07:48,510 --> 00:07:54,840
the primary thing is free software

00:07:49,950 --> 00:07:57,350
movement having access to code and how

00:07:54,840 --> 00:07:57,350
the software works

00:08:00,770 --> 00:08:09,319
so this office we use Lim Emacs UNIX

00:08:04,759 --> 00:08:12,440
Firefox and of course Python it's they

00:08:09,319 --> 00:08:16,009
are very famous and stood the test of

00:08:12,440 --> 00:08:18,919
time and they all have something in

00:08:16,009 --> 00:08:22,580
common with the bycicle they are easier

00:08:18,919 --> 00:08:24,349
to modify you can disassemble I mean you

00:08:22,580 --> 00:08:26,860
can disassemble and reassemble by cycle

00:08:24,349 --> 00:08:34,820
you know matter of a four-day holiday

00:08:26,860 --> 00:08:36,620
and why does it matter so one of the

00:08:34,820 --> 00:08:39,560
things with free software CC it's easily

00:08:36,620 --> 00:08:43,849
customized it can be tailored to run

00:08:39,560 --> 00:08:47,540
from smartwatches to supercomputers and

00:08:43,849 --> 00:08:49,630
how did this happen where there are

00:08:47,540 --> 00:08:53,240
tech-savvy people like Linus ovals and

00:08:49,630 --> 00:08:56,839
Stallman who have worked on harder code

00:08:53,240 --> 00:09:00,050
bases but also community effort from

00:08:56,839 --> 00:09:03,110
come population so if communities have

00:09:00,050 --> 00:09:06,529
access to the inner workings other

00:09:03,110 --> 00:09:10,760
things they use civilizations move

00:09:06,529 --> 00:09:15,170
forward and but that is a problem coming

00:09:10,760 --> 00:09:18,110
up because software's are changing in a

00:09:15,170 --> 00:09:20,209
sense that the inner workings of the

00:09:18,110 --> 00:09:22,279
software is not just code anymore the

00:09:20,209 --> 00:09:25,070
code doesn't give you a full picture of

00:09:22,279 --> 00:09:29,209
what the software does because the ml

00:09:25,070 --> 00:09:30,920
and a a train has they are learning they

00:09:29,209 --> 00:09:33,560
just hand coded they are learning from

00:09:30,920 --> 00:09:35,990
data and even if you have access to the

00:09:33,560 --> 00:09:38,750
entire code base without access to the

00:09:35,990 --> 00:09:44,649
date data that they learn from it's very

00:09:38,750 --> 00:09:44,649
hard to modify to suit our needs

00:09:44,740 --> 00:09:52,420
that's why open data is very essential

00:09:49,970 --> 00:09:52,420
for

00:09:52,440 --> 00:09:57,130
another revolution that happened in free

00:09:55,120 --> 00:10:02,700
software to happen in machine learning

00:09:57,130 --> 00:10:02,700
or ei also another thing my language

00:10:09,080 --> 00:10:12,280
I'll come to the later

00:10:14,240 --> 00:10:24,470
so people thought solving chess checkers

00:10:20,360 --> 00:10:25,910
go will lead to a because intelligence

00:10:24,470 --> 00:10:29,119
was thought to be a rational and

00:10:25,910 --> 00:10:32,569
calculating but we have solved with just

00:10:29,119 --> 00:10:37,550
go and shake us almost completely but we

00:10:32,569 --> 00:10:39,379
still don't what we meant by AI so the

00:10:37,550 --> 00:10:42,230
definition of intelligence is looking

00:10:39,379 --> 00:10:44,929
different and it's evolving one of their

00:10:42,230 --> 00:10:47,329
definitions that as a guy who works in

00:10:44,929 --> 00:10:49,939
NLP I have is if a system understands

00:10:47,329 --> 00:10:52,309
language I can safely say that it is

00:10:49,939 --> 00:10:57,259
pretty smart at least smarter than a

00:10:52,309 --> 00:11:01,879
monkey so while language everybody

00:10:57,259 --> 00:11:10,459
speaks language how hard could it be

00:11:01,879 --> 00:11:13,639
let's see in French medieval times they

00:11:10,459 --> 00:11:16,490
had a game there is a part you pull your

00:11:13,639 --> 00:11:18,679
money in and if you hit a chicken with a

00:11:16,490 --> 00:11:21,670
stone you get all the money this is

00:11:18,679 --> 00:11:26,379
where the term pool comes from because

00:11:21,670 --> 00:11:28,639
chicken in French is poultry whole a

00:11:26,379 --> 00:11:34,970
that's where the memory pool thread pool

00:11:28,639 --> 00:11:38,869
we all use right so another thing why

00:11:34,970 --> 00:11:40,670
cat c80 refers to an animal and why does

00:11:38,869 --> 00:11:44,540
it refers to an animal that meows and

00:11:40,670 --> 00:11:50,869
not the dog that barks what is in that C

00:11:44,540 --> 00:11:53,509
a and T in that harder makes up a cat or

00:11:50,869 --> 00:11:56,480
the meaning of words entirely composed

00:11:53,509 --> 00:12:01,850
from individual letters or the

00:11:56,480 --> 00:12:06,369
relationship is arbitrary this is at

00:12:01,850 --> 00:12:06,369
word level right when you go to sentence

00:12:07,220 --> 00:12:14,509
so what do you think this is let's talk

00:12:10,940 --> 00:12:17,600
about rights and lifts you're right so I

00:12:14,509 --> 00:12:20,870
left but if you read it there are too

00:12:17,600 --> 00:12:27,139
many words to stock outright sand our

00:12:20,870 --> 00:12:29,709
right and soil chunking sentences into

00:12:27,139 --> 00:12:33,680
word pieces is a very hard thing to do

00:12:29,709 --> 00:12:35,689
for us it's easier but then you speed

00:12:33,680 --> 00:12:39,310
him to a tokenizer

00:12:35,689 --> 00:12:44,269
just goes crazy another thing is how

00:12:39,310 --> 00:12:47,689
words follow order and create meaning

00:12:44,269 --> 00:12:50,959
out of individual words how are you is a

00:12:47,689 --> 00:12:56,750
meaningful sentence but our view is not

00:12:50,959 --> 00:12:59,839
in English and that is the order of

00:12:56,750 --> 00:13:02,420
words the syntax this thing the green

00:12:59,839 --> 00:13:05,180
marble went to sleep furiously last year

00:13:02,420 --> 00:13:08,420
it's a valid English sentence in terms

00:13:05,180 --> 00:13:10,339
of noun and verb order but it doesn't

00:13:08,420 --> 00:13:12,410
make any sense right now this is a

00:13:10,339 --> 00:13:16,779
semantic problem because we need

00:13:12,410 --> 00:13:18,800
knowledge of the world we live in to

00:13:16,779 --> 00:13:21,139
understand the meaning of sentence and

00:13:18,800 --> 00:13:23,569
the sentence to be valid it has to

00:13:21,139 --> 00:13:28,130
follow some worldly loss like physical

00:13:23,569 --> 00:13:31,189
loss another thing so when your wife

00:13:28,130 --> 00:13:33,459
yells look at the must because if she

00:13:31,189 --> 00:13:36,649
does not admire the mosquitoes she's

00:13:33,459 --> 00:13:39,160
telling the windows are open and asking

00:13:36,649 --> 00:13:39,160
you to shut it

00:13:39,730 --> 00:13:44,720
so that's why language is a very hard

00:13:42,230 --> 00:13:45,290
problem and if a system understand

00:13:44,720 --> 00:13:48,140
language

00:13:45,290 --> 00:13:53,450
we can't impart knowledge onto it just

00:13:48,140 --> 00:13:57,050
by talking and we I hope to do we all

00:13:53,450 --> 00:14:01,580
hope to do so in at least next 50 years

00:13:57,050 --> 00:14:06,560
so I hand out the camel who will talk

00:14:01,580 --> 00:14:08,480
about the technical aspects of as an

00:14:06,560 --> 00:14:11,450
indica Nellie platform I will explain

00:14:08,480 --> 00:14:13,070
what I feel done so far so we collected

00:14:11,450 --> 00:14:15,170
our data from the initial data collected

00:14:13,070 --> 00:14:17,060
from now Wikipedia them unstructured

00:14:15,170 --> 00:14:19,250
text that's where we created our

00:14:17,060 --> 00:14:22,700
language model with around perplexity of

00:14:19,250 --> 00:14:24,950
36 so then again like after developing a

00:14:22,700 --> 00:14:26,510
language model we didn't know how to

00:14:24,950 --> 00:14:28,130
well at the language model we need

00:14:26,510 --> 00:14:30,050
further downstream tasks to develop that

00:14:28,130 --> 00:14:33,320
calm performance of the language model

00:14:30,050 --> 00:14:35,120
so we collected data from news articles

00:14:33,320 --> 00:14:37,700
on that like different category and the

00:14:35,120 --> 00:14:39,230
time and sports business etc so that's

00:14:37,700 --> 00:14:41,000
where we collect data for our supervised

00:14:39,230 --> 00:14:44,570
classification model in Tamil and

00:14:41,000 --> 00:14:46,190
English Malayalam so as a first our deep

00:14:44,570 --> 00:14:49,339
learning approach we created a word to

00:14:46,190 --> 00:14:53,300
work motor now it's we actually use the

00:14:49,339 --> 00:14:56,030
open source cool tool called gentian so

00:14:53,300 --> 00:14:59,380
we have created word to work model of

00:14:56,030 --> 00:15:01,790
Malayalam Bank line thummell see we all

00:14:59,380 --> 00:15:07,670
already opens of this course and model

00:15:01,790 --> 00:15:10,070
and we also have a UI on top of it we

00:15:07,670 --> 00:15:12,320
also try transfer learning in NLP so we

00:15:10,070 --> 00:15:15,890
use Anna you LM feed so currently we are

00:15:12,320 --> 00:15:17,570
working on bird so first off most NLP

00:15:15,890 --> 00:15:19,970
problem is how to represent text in a

00:15:17,570 --> 00:15:21,740
machine understand so initially there

00:15:19,970 --> 00:15:24,080
were like statistical models like bag of

00:15:21,740 --> 00:15:25,640
words where we just represents a count

00:15:24,080 --> 00:15:28,910
of word we just take the count of words

00:15:25,640 --> 00:15:31,070
so the words doesn't have any meaning so

00:15:28,910 --> 00:15:32,930
like we here we are representing the

00:15:31,070 --> 00:15:36,080
words using that continuous vector space

00:15:32,930 --> 00:15:38,480
where semantically similar words are

00:15:36,080 --> 00:15:40,670
mapper to approximate the points in this

00:15:38,480 --> 00:15:43,070
geometrical space so what we can do is

00:15:40,670 --> 00:15:44,630
we can click on a word then we can get

00:15:43,070 --> 00:15:46,520
all the similar words associated to it

00:15:44,630 --> 00:15:48,500
so that word level meaning is very

00:15:46,520 --> 00:15:50,950
important when we are coming to the NLP

00:15:48,500 --> 00:15:52,570
problems and also when we

00:15:50,950 --> 00:15:54,640
so we have two models and it's

00:15:52,570 --> 00:15:57,580
continuous web of model and I skip

00:15:54,640 --> 00:15:59,950
Parramatta so what can a nice bag of

00:15:57,580 --> 00:16:01,960
model does it's so it takes an input as

00:15:59,950 --> 00:16:04,570
a one current word so it tries to

00:16:01,960 --> 00:16:06,820
predicts the context of its words that's

00:16:04,570 --> 00:16:08,260
the first second mortar so the skip

00:16:06,820 --> 00:16:10,840
grammarly is exactly opposite to the

00:16:08,260 --> 00:16:13,870
conveners bag of words model so it's

00:16:10,840 --> 00:16:15,720
takes the parent word and like its

00:16:13,870 --> 00:16:19,360
output the all other word context words

00:16:15,720 --> 00:16:21,670
so we trained skip gram model for the

00:16:19,360 --> 00:16:24,070
Malayalam and Tamil language as we don't

00:16:21,670 --> 00:16:27,910
have a lot of data Scoob kupdegra model

00:16:24,070 --> 00:16:30,610
works both with illustrator so and like

00:16:27,910 --> 00:16:32,500
for evaluating the unsupervised model

00:16:30,610 --> 00:16:34,540
for evaluating also we don't have any

00:16:32,500 --> 00:16:36,700
data set so we have what we actually did

00:16:34,540 --> 00:16:39,070
is we have a tensorflow embedding

00:16:36,700 --> 00:16:40,930
projector so we projected our word

00:16:39,070 --> 00:16:43,540
representation using that then we manly

00:16:40,930 --> 00:16:45,250
analyze the word clusters so so where we

00:16:43,540 --> 00:16:48,220
found like how we tune the

00:16:45,250 --> 00:16:50,290
hyperparameters and like and like we

00:16:48,220 --> 00:16:52,510
manually created our the size of our

00:16:50,290 --> 00:16:56,740
capillary so most of our like word

00:16:52,510 --> 00:16:58,960
vocabulary is around 10000 so then we

00:16:56,740 --> 00:17:02,260
after creating the word to work model we

00:16:58,960 --> 00:17:06,420
try the normal lsdm classifiers okay I

00:17:02,260 --> 00:17:06,420
will show a demo of how word web works

00:17:09,940 --> 00:17:19,900
so this is for Bangla so we are picking

00:17:14,740 --> 00:17:22,380
on the word called okay thumri there is

00:17:19,900 --> 00:17:25,839
a word quadrat so when we click on the

00:17:22,380 --> 00:17:28,690
rabbit we get all the animals like rat

00:17:25,839 --> 00:17:30,040
and I would cheat and everything in

00:17:28,690 --> 00:17:31,630
Malayalam we are clicking on the word

00:17:30,040 --> 00:17:32,740
court January when we are clicking on

00:17:31,630 --> 00:17:36,610
the word code general to be or getting

00:17:32,740 --> 00:17:37,930
all the months related to the January

00:17:36,610 --> 00:17:42,370
right is numbered is somewhere and

00:17:37,930 --> 00:17:44,110
everything okay so so after creating the

00:17:42,370 --> 00:17:47,260
word to work model what we did is we

00:17:44,110 --> 00:17:49,110
create a normal lsdm based classifier so

00:17:47,260 --> 00:17:51,250
like for as a first layer we use the

00:17:49,110 --> 00:17:53,620
pretending that we already created using

00:17:51,250 --> 00:17:55,810
the jinsub model on the second layer we

00:17:53,620 --> 00:17:58,240
have created the LST and bidirectional

00:17:55,810 --> 00:17:59,620
STM so bi-directional STM is used to

00:17:58,240 --> 00:18:01,720
capture the word meanings from the both

00:17:59,620 --> 00:18:03,460
direction has like in its in a sentence

00:18:01,720 --> 00:18:07,150
the word can have dependencies in both

00:18:03,460 --> 00:18:09,070
direction then the final output from the

00:18:07,150 --> 00:18:11,290
lsdm layer is taken into a fully

00:18:09,070 --> 00:18:13,780
connected dense layer followed by a soft

00:18:11,290 --> 00:18:16,180
max actuation so in this configuration

00:18:13,780 --> 00:18:18,280
we got around 89% accuracy on our nose

00:18:16,180 --> 00:18:20,260
classification data set without any like

00:18:18,280 --> 00:18:22,270
fine-tuning like for the high poverty

00:18:20,260 --> 00:18:24,730
meters or anything it took around one

00:18:22,270 --> 00:18:26,440
hour for training so after doing that we

00:18:24,730 --> 00:18:28,900
are excited to do transfer learning in

00:18:26,440 --> 00:18:32,260
NLP so transfer learning is an approach

00:18:28,900 --> 00:18:36,040
where like you play we train in large

00:18:32,260 --> 00:18:37,870
model on a huge amount of data then we

00:18:36,040 --> 00:18:39,850
use that domain knowledge to transfer

00:18:37,870 --> 00:18:41,470
that knowledge to a similar simple task

00:18:39,850 --> 00:18:44,560
where we don't need a huge amount of

00:18:41,470 --> 00:18:46,840
data so the transfer learning is very

00:18:44,560 --> 00:18:47,410
common in computer vision so this is the

00:18:46,840 --> 00:18:50,500
first

00:18:47,410 --> 00:18:52,210
papering transfer learning on in texas

00:18:50,500 --> 00:18:55,990
this is model is called Ulf it this is

00:18:52,210 --> 00:18:56,850
the implementation of fast j so what are

00:18:55,990 --> 00:18:59,200
we doing in like

00:18:56,850 --> 00:19:01,360
lmpd's firstly you train a language

00:18:59,200 --> 00:19:03,040
model as the first figure so what

00:19:01,360 --> 00:19:05,710
happens in language model training is we

00:19:03,040 --> 00:19:06,880
input a two so two or three words then

00:19:05,710 --> 00:19:09,040
we will try to predict the next word

00:19:06,880 --> 00:19:11,260
that's how the language model it turn to

00:19:09,040 --> 00:19:12,700
learnt so it actually learns the

00:19:11,260 --> 00:19:15,070
semantics and like relationship building

00:19:12,700 --> 00:19:16,950
birds and all other things so what did

00:19:15,070 --> 00:19:19,480
we do in the seven step second step is

00:19:16,950 --> 00:19:21,380
we will fine-tune the language model on

00:19:19,480 --> 00:19:23,179
the our preferred data set here

00:19:21,380 --> 00:19:24,470
the land first language morally trained

00:19:23,179 --> 00:19:26,480
on the Wikipedia and second language

00:19:24,470 --> 00:19:28,580
model is fine-tuned on the hour news

00:19:26,480 --> 00:19:29,990
classification that has it finally on

00:19:28,580 --> 00:19:32,750
the third class fare fine-tuning

00:19:29,990 --> 00:19:35,240
we remove the final language model layer

00:19:32,750 --> 00:19:38,480
and and add an external X softmax layer

00:19:35,240 --> 00:19:41,179
as in the lsdm class fare to Class VI on

00:19:38,480 --> 00:19:43,429
the gnosis here we actually own with

00:19:41,179 --> 00:19:49,070
around 10k samples we got around 95%

00:19:43,429 --> 00:19:52,400
accuracy then currently we are working

00:19:49,070 --> 00:19:53,780
on like bird for Indic languages so

00:19:52,400 --> 00:19:55,190
birdies are like little different

00:19:53,780 --> 00:19:57,049
architecture compared to all the

00:19:55,190 --> 00:19:59,210
transform all the other transfer

00:19:57,049 --> 00:20:02,090
learning models bird is on like works

00:19:59,210 --> 00:20:03,679
not a transformer model this transfer

00:20:02,090 --> 00:20:06,650
model is from attention is all unit

00:20:03,679 --> 00:20:08,870
paper for google so there's a little

00:20:06,650 --> 00:20:10,340
different from all other language model

00:20:08,870 --> 00:20:12,679
this is a math language model where we

00:20:10,340 --> 00:20:15,080
predict instead of the next word we will

00:20:12,679 --> 00:20:17,419
predict the mass and there's one more

00:20:15,080 --> 00:20:19,909
task for annex and inspiration so we

00:20:17,419 --> 00:20:22,970
train with jointly with two sentences

00:20:19,909 --> 00:20:25,130
and like bird has embroidery like in

00:20:22,970 --> 00:20:27,440
english itself it's similar like around

00:20:25,130 --> 00:20:29,330
5% days in bro vendor all the NLP tasks

00:20:27,440 --> 00:20:31,539
so currently we are working on it even

00:20:29,330 --> 00:20:34,250
though the code is open source by Google

00:20:31,539 --> 00:20:35,659
the tokenizer is not compatible with our

00:20:34,250 --> 00:20:37,880
Indic languages currently we are working

00:20:35,659 --> 00:20:39,950
on like in the talk nicer part as far as

00:20:37,880 --> 00:20:41,809
we finish we will also open so this so

00:20:39,950 --> 00:20:43,309
all the models and codes are open so

00:20:41,809 --> 00:20:44,510
straight so you can check out our

00:20:43,309 --> 00:20:47,270
indicator P dot o-r-g

00:20:44,510 --> 00:20:49,480
so other we'll talk about more about our

00:20:47,270 --> 00:20:49,480
platform

00:20:51,900 --> 00:20:57,400
so whatever we have done we put it on

00:20:55,390 --> 00:21:01,540
our github and we found a lot of people

00:20:57,400 --> 00:21:02,830
interested in the project and we wanted

00:21:01,540 --> 00:21:04,480
to bring everyone together that's when

00:21:02,830 --> 00:21:06,640
we launched the website called in the

00:21:04,480 --> 00:21:09,250
canal P dot or Z so we put all the

00:21:06,640 --> 00:21:11,620
information that about the project we

00:21:09,250 --> 00:21:13,030
have what we have done so far whatever

00:21:11,620 --> 00:21:16,990
tools that we have all the Gator

00:21:13,030 --> 00:21:18,970
projects in this website so we there is

00:21:16,990 --> 00:21:19,600
no data tagging platform for Indian

00:21:18,970 --> 00:21:22,200
languages

00:21:19,600 --> 00:21:24,970
maybe not for any language that is

00:21:22,200 --> 00:21:27,250
publicly available so we wanted to do a

00:21:24,970 --> 00:21:32,550
translation task that's when we I build

00:21:27,250 --> 00:21:32,550
a small Django app and for translation

00:21:32,790 --> 00:21:37,960
but I know it didn't work that well but

00:21:35,530 --> 00:21:41,620
that's when we found a project called

00:21:37,960 --> 00:21:43,600
Doki a know that is recently released in

00:21:41,620 --> 00:21:46,720
that project right now we have added

00:21:43,600 --> 00:21:49,390
data set for named entity recognition so

00:21:46,720 --> 00:21:51,970
we can select so what I am selecting is

00:21:49,390 --> 00:21:55,480
Bangladesh it's a location within

00:21:51,970 --> 00:21:57,340
Muhammad it's the name of the person so

00:21:55,480 --> 00:22:01,559
this is an old article and we are

00:21:57,340 --> 00:22:01,559
tagging it for named entity recognition

00:22:02,850 --> 00:22:08,530
similarly in Tamil we have the data set

00:22:06,820 --> 00:22:13,030
uploaded and we have just started

00:22:08,530 --> 00:22:15,400
tagging so you're coming to the big

00:22:13,030 --> 00:22:18,940
question how can you guys help us I mean

00:22:15,400 --> 00:22:22,030
how can we do this as a you know as a

00:22:18,940 --> 00:22:24,550
project together right yeah so if you

00:22:22,030 --> 00:22:27,220
know how to tag that you can just come

00:22:24,550 --> 00:22:30,190
to our platform and help tag that us so

00:22:27,220 --> 00:22:31,650
if you know Tamil or Malayalam or any

00:22:30,190 --> 00:22:34,990
other language in the language you can

00:22:31,650 --> 00:22:36,970
we can help you I'd you know work with

00:22:34,990 --> 00:22:40,240
us and improve this in the can LP in to

00:22:36,970 --> 00:22:42,760
all Indian languages so and if you know

00:22:40,240 --> 00:22:44,170
English and Indian languages we can use

00:22:42,760 --> 00:22:46,360
it for translation we don't have any

00:22:44,170 --> 00:22:48,520
translation that'sit for Indian

00:22:46,360 --> 00:22:50,320
languages publicly available family if

00:22:48,520 --> 00:22:55,780
you are linguist you can help us you

00:22:50,320 --> 00:22:57,190
know frame good tasks which is different

00:22:55,780 --> 00:22:59,260
from English there will be different

00:22:57,190 --> 00:23:01,900
tasks in Malayalam or Tamil which we

00:22:59,260 --> 00:23:05,020
don't know yet you know how the language

00:23:01,900 --> 00:23:07,660
NLP models improves is that we actually

00:23:05,020 --> 00:23:10,300
created an asset and people try to

00:23:07,660 --> 00:23:12,730
create the state-of-art models on top of

00:23:10,300 --> 00:23:15,460
that so if we can actually create a good

00:23:12,730 --> 00:23:18,040
data set in Indian languages then all of

00:23:15,460 --> 00:23:20,020
different people will be competing to

00:23:18,040 --> 00:23:23,910
create the best state-of-the-art models

00:23:20,020 --> 00:23:26,350
so we will actually have a good code and

00:23:23,910 --> 00:23:29,670
you know data sets publicly available

00:23:26,350 --> 00:23:31,480
that's what we are aiming for right now

00:23:29,670 --> 00:23:33,280
so these are some of the other

00:23:31,480 --> 00:23:38,140
contributors that has helped us in

00:23:33,280 --> 00:23:42,430
building the UI in our building the you

00:23:38,140 --> 00:23:44,290
know the fights and everything and even

00:23:42,430 --> 00:23:46,810
the crawlers so we have written some of

00:23:44,290 --> 00:23:49,810
the crawlers that goes to Indian website

00:23:46,810 --> 00:23:50,950
us and crawl and get us new status and

00:23:49,810 --> 00:23:54,850
everything you can also contribute to

00:23:50,950 --> 00:23:58,420
that meet again so can you talk about

00:23:54,850 --> 00:24:03,280
your volunteer community especially the

00:23:58,420 --> 00:24:04,870
non-technical people so currently we so

00:24:03,280 --> 00:24:06,550
after coming here I spoke to a lot of

00:24:04,870 --> 00:24:09,130
people and we have started a telegram

00:24:06,550 --> 00:24:11,350
group yeah so you can join the telegram

00:24:09,130 --> 00:24:13,540
group I'll be providing to the link to

00:24:11,350 --> 00:24:15,670
the K telegram group in the website can

00:24:13,540 --> 00:24:18,280
come there and there is a Google

00:24:15,670 --> 00:24:20,470
developers group you can sign up in that

00:24:18,280 --> 00:24:21,910
so you will be getting all the emails

00:24:20,470 --> 00:24:23,830
about all the different languages

00:24:21,910 --> 00:24:26,140
there's also a Facebook group can be a

00:24:23,830 --> 00:24:27,880
part of that so yeah I would suggest

00:24:26,140 --> 00:24:29,440
join the Google group because you will

00:24:27,880 --> 00:24:30,790
be sending you'll be getting emails and

00:24:29,440 --> 00:24:32,890
you again if you're working on anything

00:24:30,790 --> 00:24:38,940
you can also share it with everyone

00:24:32,890 --> 00:24:38,940
yeah again we'll we'll also join sure

00:24:46,630 --> 00:24:50,210
hello

00:24:48,050 --> 00:24:54,260
so actually I'm asking what kind of

00:24:50,210 --> 00:24:57,050
tokenization method did you use for

00:24:54,260 --> 00:24:59,090
Malayalam bank Bangla yeah so currently

00:24:57,050 --> 00:25:01,730
we are talkin izing only using the space

00:24:59,090 --> 00:25:03,800
in punctuation so that's actually not

00:25:01,730 --> 00:25:06,260
correct so but the bond model used to

00:25:03,800 --> 00:25:08,510
learn it anyway so currently we are only

00:25:06,260 --> 00:25:09,950
or also working on like backbiter based

00:25:08,510 --> 00:25:12,440
and coding that's a word piece for

00:25:09,950 --> 00:25:14,720
capillary based organization so actually

00:25:12,440 --> 00:25:16,850
you can use byte pair with English but I

00:25:14,720 --> 00:25:19,880
was like I am also working on same thing

00:25:16,850 --> 00:25:22,550
so how can we use byte pair for Hindi or

00:25:19,880 --> 00:25:24,380
any other Indian languages if you can

00:25:22,550 --> 00:25:26,480
share something yeah there's already a

00:25:24,380 --> 00:25:28,400
library by googles and then species you

00:25:26,480 --> 00:25:30,440
know I don't know I know so you could

00:25:28,400 --> 00:25:32,750
trainer like the large corpus we already

00:25:30,440 --> 00:25:35,230
created this and word piece vocabulary

00:25:32,750 --> 00:25:37,820
for Malayalam for like bird training

00:25:35,230 --> 00:25:44,780
okay so I'll share the boards with you

00:25:37,820 --> 00:25:46,340
okay okay thank you so so most of the

00:25:44,780 --> 00:25:48,800
approaches that you mentioned the

00:25:46,340 --> 00:25:50,780
processes come when it comes to crawling

00:25:48,800 --> 00:25:53,090
all training a word - Eggman which is

00:25:50,780 --> 00:25:56,660
unsupervised all of this can be applied

00:25:53,090 --> 00:25:58,250
to any language so what is what so what

00:25:56,660 --> 00:26:01,100
did you have to do specific to Indian

00:25:58,250 --> 00:26:02,720
languages or how is training more NLP

00:26:01,100 --> 00:26:05,000
models for Indian language is different

00:26:02,720 --> 00:26:07,310
from training it from any other language

00:26:05,000 --> 00:26:11,060
or say English because most of the stuff

00:26:07,310 --> 00:26:13,670
there if I found in your taco is genetic

00:26:11,060 --> 00:26:16,100
the technically we didn't do anything

00:26:13,670 --> 00:26:17,240
different one thing we are trying is to

00:26:16,100 --> 00:26:19,880
create a tokenizer

00:26:17,240 --> 00:26:23,690
that's the one problem we face right now

00:26:19,880 --> 00:26:25,520
but this is like the very few of the

00:26:23,690 --> 00:26:28,450
items to create language modeling and

00:26:25,520 --> 00:26:30,980
release the model data also in public

00:26:28,450 --> 00:26:34,040
are there any challenges that you faced

00:26:30,980 --> 00:26:36,620
with Indian languages that weren't with

00:26:34,040 --> 00:26:38,720
English one primary thing is

00:26:36,620 --> 00:26:41,690
tokenization because even with the bytes

00:26:38,720 --> 00:26:44,510
are encoding the Unicode representation

00:26:41,690 --> 00:26:47,870
of our Indic languages is a little

00:26:44,510 --> 00:26:50,650
different or single if I'm not sure

00:26:47,870 --> 00:26:54,530
whether you if you're from tama

00:26:50,650 --> 00:26:58,010
means car plus there is a dot in the top

00:26:54,530 --> 00:26:59,030
those are different unicode points these

00:26:58,010 --> 00:27:01,190
crates of drawing you

00:26:59,030 --> 00:27:04,880
we abide by encoding we are working on

00:27:01,190 --> 00:27:08,420
solving that we are seeking help from

00:27:04,880 --> 00:27:11,990
people who have knowledge in linguistics

00:27:08,420 --> 00:27:13,940
who know understands language and the

00:27:11,990 --> 00:27:16,840
grammar grammatical structure because we

00:27:13,940 --> 00:27:20,600
are not linguistic savvy people so

00:27:16,840 --> 00:27:24,410
that's one challenge we face another

00:27:20,600 --> 00:27:31,480
thing is lack of datasets because we

00:27:24,410 --> 00:28:04,660
just crawl the newspapers for example

00:27:31,480 --> 00:28:04,660
the data comes to speech in English

00:28:09,000 --> 00:28:12,049
[Music]

00:28:20,530 --> 00:28:26,120
actually thinking about using like

00:28:24,050 --> 00:28:28,310
applying deep learning on top of

00:28:26,120 --> 00:28:30,500
transliterated text because applying

00:28:28,310 --> 00:28:32,810
Bert and all these libraries on top of

00:28:30,500 --> 00:28:34,370
transliterated tests will be easier so

00:28:32,810 --> 00:28:36,010
we haven't tried that approach will be

00:28:34,370 --> 00:28:38,330
we are actually looking forward to that

00:28:36,010 --> 00:28:40,370
read a statistic today available like if

00:28:38,330 --> 00:28:41,870
you do any name editing activation a lot

00:28:40,370 --> 00:28:43,220
of data is available in English if you

00:28:41,870 --> 00:28:46,040
can translate rate in a language it

00:28:43,220 --> 00:28:47,990
becomes easier some way okay another but

00:28:46,040 --> 00:28:49,750
a lot of early country even in

00:28:47,990 --> 00:28:52,670
government they don't use Unicode much

00:28:49,750 --> 00:28:55,010
it's in like in Tamil it's Langella or

00:28:52,670 --> 00:28:57,110
if use railways they are not is in

00:28:55,010 --> 00:29:00,080
Unicode for now that's what I understand

00:28:57,110 --> 00:29:02,060
from so older content if you can have a

00:29:00,080 --> 00:29:03,230
more Assange a lot of other format tamil

00:29:02,060 --> 00:29:05,570
90 and there we had a lot of Senate's

00:29:03,230 --> 00:29:08,000
earlier because I did early about 20

00:29:05,570 --> 00:29:09,900
years back so I'm looking at what you

00:29:08,000 --> 00:29:11,610
can come with that part

00:29:09,900 --> 00:29:13,650
one thing I want to mention is you

00:29:11,610 --> 00:29:16,560
mentioned about translating data sets

00:29:13,650 --> 00:29:17,520
right that is Squa did I said we said

00:29:16,560 --> 00:29:19,620
question-answering it is that you are

00:29:17,520 --> 00:29:20,820
given a passage and some questions are

00:29:19,620 --> 00:29:23,280
supposed to answer those questions

00:29:20,820 --> 00:29:25,290
within this context of the package we

00:29:23,280 --> 00:29:28,770
try translating most of the sentences

00:29:25,290 --> 00:29:30,000
were gibberish for example to give an

00:29:28,770 --> 00:29:32,310
example

00:29:30,000 --> 00:29:34,800
neelaveni what is null

00:29:32,310 --> 00:29:36,890
this is a Tamil phrase which means there

00:29:34,800 --> 00:29:40,670
is a lady in the moon who is cooking up

00:29:36,890 --> 00:29:43,760
what I but the translation we got was

00:29:40,670 --> 00:29:49,200
there is a old lady burning in the moon

00:29:43,760 --> 00:29:52,110
so it is a little hard to just trust the

00:29:49,200 --> 00:29:54,660
translation maybe with even that was

00:29:52,110 --> 00:29:56,910
from Google translation I think

00:29:54,660 --> 00:30:03,990
cockroach Malayalam was a little

00:29:56,910 --> 00:30:04,530
different I don't think we can say hello

00:30:03,990 --> 00:30:08,400
yeah

00:30:04,530 --> 00:30:10,890
so I decided different question Oh like

00:30:08,400 --> 00:30:12,480
you said about tokenizer and oil can we

00:30:10,890 --> 00:30:14,870
apply the techniques that you are using

00:30:12,480 --> 00:30:16,920
first things like applying CTC loss on

00:30:14,870 --> 00:30:18,150
text that you're trying to do for

00:30:16,920 --> 00:30:19,680
example if I'm trying to handwriting

00:30:18,150 --> 00:30:23,100
recognition but I want to run an NLP

00:30:19,680 --> 00:30:25,860
task and what I get from it like a CDC

00:30:23,100 --> 00:30:28,440
loss to fix it or word grams to try and

00:30:25,860 --> 00:30:29,730
predict missed or sentences can you talk

00:30:28,440 --> 00:30:32,490
a little bit about those kind of

00:30:29,730 --> 00:30:36,810
applications in Indic languages I found

00:30:32,490 --> 00:30:38,700
correctly applying OCR right Yara

00:30:36,810 --> 00:30:41,190
Monsieur I do not have personal

00:30:38,700 --> 00:30:44,910
experience with the OCR but we trained

00:30:41,190 --> 00:30:46,890
speech to text model that also uses CDC

00:30:44,910 --> 00:30:50,840
but I still do not understand completely

00:30:46,890 --> 00:30:50,840
to answer a question how CTC works

00:30:50,970 --> 00:30:55,940
what about skip grams or engrams

00:30:59,610 --> 00:31:04,600
so if I if I'm trying to classify if I'm

00:31:02,650 --> 00:31:06,250
missing a word in the middle trying to

00:31:04,600 --> 00:31:08,559
predict that word based on the previous

00:31:06,250 --> 00:31:11,110
words and the words after that yeah you

00:31:08,559 --> 00:31:13,029
could actually use any language model so

00:31:11,110 --> 00:31:14,740
have they worked properly with any with

00:31:13,029 --> 00:31:16,120
Indic languages in your experience or

00:31:14,740 --> 00:31:18,490
have you faced any problems that's my

00:31:16,120 --> 00:31:20,799
question so you could actually try our

00:31:18,490 --> 00:31:25,270
own fit model which does a state of art

00:31:20,799 --> 00:31:27,250
now this is around 36% perplexity we had

00:31:25,270 --> 00:31:28,390
actually we try to generate samples

00:31:27,250 --> 00:31:29,710
using that language model it was

00:31:28,390 --> 00:31:33,070
actually pretty good grammatically it's

00:31:29,710 --> 00:31:34,990
correct so it was like we generated from

00:31:33,070 --> 00:31:36,669
with just a scratch so we didn't

00:31:34,990 --> 00:31:38,679
actually try just just filling out some

00:31:36,669 --> 00:31:41,010
words so actually I can assure okay

00:31:38,679 --> 00:31:41,010

YouTube URL: https://www.youtube.com/watch?v=rgCXWaKzMKU


