Title: Lightning Talk - Praveen J - AWS
Publication date: 2019-11-16
Playlist: PyCon India 2019
Description: 
	This talk was presented at PyCon India 2019, on Oct 12th - 13th, at the Chennai Trade Centre.
Website: https://in.pycon.org/2019
Captions: 
	00:00:04,630 --> 00:00:09,280
gruffly on everybody my name is praveen

00:00:07,690 --> 00:00:12,160
I'm a senior Solutions Architect with

00:00:09,280 --> 00:00:14,530
Amazon Web Services so therefore the

00:00:12,160 --> 00:00:17,200
next five minutes we will talk about

00:00:14,530 --> 00:00:19,270
what are all the I mean different

00:00:17,200 --> 00:00:21,130
workloads you could run in AWS the

00:00:19,270 --> 00:00:23,439
Python workloads you could run in AWS

00:00:21,130 --> 00:00:26,679
and how AWS could help you with that

00:00:23,439 --> 00:00:30,669
right so before even going there we

00:00:26,679 --> 00:00:32,500
provide you an SDK called moto3 SDK so

00:00:30,669 --> 00:00:34,840
that's the SDK through which you would

00:00:32,500 --> 00:00:37,870
be interacting with any aw service if

00:00:34,840 --> 00:00:40,210
you want to use be it our ec2 instance a

00:00:37,870 --> 00:00:42,309
virtual machine or our storage you would

00:00:40,210 --> 00:00:44,649
be using that right so the Moto 3 is the

00:00:42,309 --> 00:00:46,870
SDK if you are working on Python with

00:00:44,649 --> 00:00:49,120
AWS you should have you should have it

00:00:46,870 --> 00:00:51,940
so let's talk about some of the

00:00:49,120 --> 00:00:54,039
workloads which you could run with a

00:00:51,940 --> 00:00:57,670
Python workloads you could run with AWS

00:00:54,039 --> 00:00:59,170
let's take the web workload so if you

00:00:57,670 --> 00:01:01,269
want to run a website or a micro

00:00:59,170 --> 00:01:03,579
services architecture API based anything

00:01:01,269 --> 00:01:05,650
you have something called up elastic

00:01:03,579 --> 00:01:08,680
Beanstalk right so which is our path

00:01:05,650 --> 00:01:09,490
service and what it provides we natively

00:01:08,680 --> 00:01:12,250
support

00:01:09,490 --> 00:01:14,799
Django flask application you could

00:01:12,250 --> 00:01:17,740
directly directly run on it typically

00:01:14,799 --> 00:01:19,450
how we run run your web application is

00:01:17,740 --> 00:01:22,329
that you probably you will get a virtual

00:01:19,450 --> 00:01:24,369
machine install install Python on it

00:01:22,329 --> 00:01:26,380
install your application on it you are

00:01:24,369 --> 00:01:28,060
responsible for auto scaling that

00:01:26,380 --> 00:01:30,189
particular application you are

00:01:28,060 --> 00:01:33,040
responsible if the load increases you

00:01:30,189 --> 00:01:35,290
need to you need to make sure that your

00:01:33,040 --> 00:01:38,310
capacity is over there right so that is

00:01:35,290 --> 00:01:41,110
what kind of avoid you you could use

00:01:38,310 --> 00:01:43,479
elastic beanstalk which could help you

00:01:41,110 --> 00:01:45,340
in scaling your application we take care

00:01:43,479 --> 00:01:47,110
of automatically horizontally scaling

00:01:45,340 --> 00:01:49,210
your application and if you want to

00:01:47,110 --> 00:01:51,040
connect to any other data store like a

00:01:49,210 --> 00:01:53,409
dynamo DB or something you could you

00:01:51,040 --> 00:01:55,210
could use elastic beanstalk so that's

00:01:53,409 --> 00:01:56,890
the first computer option you have the

00:01:55,210 --> 00:02:00,130
second option which I want to talk about

00:01:56,890 --> 00:02:03,070
is the earth on lambda lambda is our

00:02:00,130 --> 00:02:04,659
service service based architecture

00:02:03,070 --> 00:02:08,320
function as a service if you want to

00:02:04,659 --> 00:02:10,539
call you could have your API call the

00:02:08,320 --> 00:02:13,240
call lambda function or there are about

00:02:10,539 --> 00:02:15,190
60 plus different events within AWS

00:02:13,240 --> 00:02:16,830
which could invoke your lambda function

00:02:15,190 --> 00:02:19,920
right so lambda

00:02:16,830 --> 00:02:21,810
so as a function as a service you upload

00:02:19,920 --> 00:02:24,510
the function you don't really need to

00:02:21,810 --> 00:02:27,000
worry about how to scale it patch it and

00:02:24,510 --> 00:02:29,160
if the load suddenly increases we will

00:02:27,000 --> 00:02:31,200
make sure that we will bring up the

00:02:29,160 --> 00:02:32,940
number of instances for you right so

00:02:31,200 --> 00:02:35,730
that is what the second option to run

00:02:32,940 --> 00:02:37,290
your compute is lambda on the third

00:02:35,730 --> 00:02:39,870
let's say if you have something like a

00:02:37,290 --> 00:02:42,000
salary which you wanted to run a kind of

00:02:39,870 --> 00:02:43,890
a task cube or distributed to ask you

00:02:42,000 --> 00:02:45,870
then again we provided you a couple of

00:02:43,890 --> 00:02:47,790
options one you could always bring up

00:02:45,870 --> 00:02:50,640
your virtual machine the ec2 virtual

00:02:47,790 --> 00:02:52,560
machine install your salary distributed

00:02:50,640 --> 00:02:54,840
application over there and run it the

00:02:52,560 --> 00:02:57,060
second option is that we support docker

00:02:54,840 --> 00:02:58,980
containers right so we we provide

00:02:57,060 --> 00:03:00,810
something called ECS which is a

00:02:58,980 --> 00:03:02,850
container service you could use our

00:03:00,810 --> 00:03:04,170
container orchestration service you

00:03:02,850 --> 00:03:05,640
could use that but if you are

00:03:04,170 --> 00:03:08,070
comfortable using something like a

00:03:05,640 --> 00:03:10,470
kubernetes we also provide you what we

00:03:08,070 --> 00:03:12,390
call it as a eks is elastic kubernetes

00:03:10,470 --> 00:03:14,730
service which you put which you could

00:03:12,390 --> 00:03:16,440
use it right so you could use your

00:03:14,730 --> 00:03:18,180
virtual machine you could use the docker

00:03:16,440 --> 00:03:20,250
containers or you could run it in the

00:03:18,180 --> 00:03:23,610
serverless environment this is for a web

00:03:20,250 --> 00:03:26,130
kind of application and let's say if you

00:03:23,610 --> 00:03:27,690
want to run a typical machine learning

00:03:26,130 --> 00:03:30,150
or artificial intelligence a deep

00:03:27,690 --> 00:03:32,700
learning kind of application then we do

00:03:30,150 --> 00:03:35,130
provide various services for that as

00:03:32,700 --> 00:03:37,260
well one one service which I want to

00:03:35,130 --> 00:03:39,780
talk about a service called sage maker

00:03:37,260 --> 00:03:41,820
so what sage maker does is that it kind

00:03:39,780 --> 00:03:44,459
of gives you about 17 different most

00:03:41,820 --> 00:03:46,680
popular algorithms and you could start

00:03:44,459 --> 00:03:48,690
using it right away and the next thing

00:03:46,680 --> 00:03:50,550
it provides you for you is that make

00:03:48,690 --> 00:03:53,100
your life easier when you want to do a

00:03:50,550 --> 00:03:54,780
distributed training it it provides you

00:03:53,100 --> 00:03:56,910
a wave what we call it as a one-click

00:03:54,780 --> 00:03:58,560
training you just need to tell us what

00:03:56,910 --> 00:04:00,810
is the algorithm you want to use where

00:03:58,560 --> 00:04:02,790
the data is there and how many nodes you

00:04:00,810 --> 00:04:05,130
want to do the training we will actually

00:04:02,790 --> 00:04:07,140
provision those many machines run the

00:04:05,130 --> 00:04:09,270
training for you once the training is

00:04:07,140 --> 00:04:12,060
complete we actually bring down those

00:04:09,270 --> 00:04:13,830
those machines and then we actually

00:04:12,060 --> 00:04:15,930
throw away the machine copy the model

00:04:13,830 --> 00:04:18,060
into a secure location which you could

00:04:15,930 --> 00:04:20,040
start using it right so again that's the

00:04:18,060 --> 00:04:21,900
second feature which sage maker provides

00:04:20,040 --> 00:04:23,700
and the third thing it provides is it

00:04:21,900 --> 00:04:26,700
helps you with Elsie with the inference

00:04:23,700 --> 00:04:28,680
so if you want to deploy your model you

00:04:26,700 --> 00:04:29,870
just say one-click deployment you just

00:04:28,680 --> 00:04:31,610
deploy your model over there

00:04:29,870 --> 00:04:33,440
and then we provide you a restful

00:04:31,610 --> 00:04:36,110
endpoint which your application can

00:04:33,440 --> 00:04:38,330
start using it so this this inference

00:04:36,110 --> 00:04:40,310
layer can do auto scaling when your load

00:04:38,330 --> 00:04:42,020
increases we can automatically scale it

00:04:40,310 --> 00:04:44,450
for you right so these are some of the

00:04:42,020 --> 00:04:46,400
benefits of using sage maker apart from

00:04:44,450 --> 00:04:48,980
that we also provide you a layer where

00:04:46,400 --> 00:04:51,890
we probably do the GPU capabilities at

00:04:48,980 --> 00:04:52,700
the Tesla NVIDIA GPU if you want if you

00:04:51,890 --> 00:04:54,920
are doing some deep learning

00:04:52,700 --> 00:04:56,870
applications we do provide you if you

00:04:54,920 --> 00:04:59,240
have any questions on what I spoke now

00:04:56,870 --> 00:05:01,130
we are both over there the AWS booth I

00:04:59,240 --> 00:05:04,060
can we can meet you over there and

00:05:01,130 --> 00:05:04,060

YouTube URL: https://www.youtube.com/watch?v=KDCZB4Uck18


