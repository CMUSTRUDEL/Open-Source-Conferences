Title: Coly Li - Linux I O latency measurement in large scale cloud service - openSUSE Conference 2013
Publication date: 2013-07-22
Playlist: openSUSE Conference 2013
Description: 
	Observation, analysis and practice solution

In large scale could service, I/O latency is an important factor of overall could service performance. In this talk, an experience from one of the largest could service provider in China introduces how hard disk I/O latency is measured and identified in cloud storage infrastructure, and the solution or workaround to solve/handle headachy I/O latency issues.
Captions: 
	00:00:00,350 --> 00:00:06,390
getting this is Colleen he will give us

00:00:03,929 --> 00:00:16,379
a talk about linux I of measurement in

00:00:06,390 --> 00:00:20,100
legend glow tape yeah so hello everyone

00:00:16,379 --> 00:00:22,800
coming here is a correct choice so my

00:00:20,100 --> 00:00:26,570
talk the topic is linux Iola teens in

00:00:22,800 --> 00:00:30,269
merriment in large-scale cloud service

00:00:26,570 --> 00:00:35,340
most of the content here is observation

00:00:30,269 --> 00:00:37,380
and analysis and the challenge I will

00:00:35,340 --> 00:00:40,410
spend the more than half time to

00:00:37,380 --> 00:00:44,129
introduce the background and for the

00:00:40,410 --> 00:00:47,460
less than half time to share the data we

00:00:44,129 --> 00:00:51,329
observed and a little little bit

00:00:47,460 --> 00:00:57,140
analysis and some challenge we have to

00:00:51,329 --> 00:01:00,079
handle later so my name is khalili i

00:00:57,140 --> 00:01:07,260
worked for Souza for almost five years

00:01:00,079 --> 00:01:09,420
in Susa labs and the after i left celebs

00:01:07,260 --> 00:01:14,299
i draw in the alibaba group to help them

00:01:09,420 --> 00:01:16,439
to build the the colonel team in china

00:01:14,299 --> 00:01:18,600
currently I'm the software engineer

00:01:16,439 --> 00:01:24,810
insider reliable engineering team of

00:01:18,600 --> 00:01:28,439
Alibaba Group the responsibility of this

00:01:24,810 --> 00:01:34,850
team just maintain a lot of servers a

00:01:28,439 --> 00:01:37,229
lot of means hundreds thousand right now

00:01:34,850 --> 00:01:44,490
also I'm the OpenSocial user and

00:01:37,229 --> 00:01:47,420
developer so first i will introduce the

00:01:44,490 --> 00:01:51,119
the hard disk Ophelia in general just

00:01:47,420 --> 00:01:55,590
this kind of conditions that how the

00:01:51,119 --> 00:02:00,049
disk is broken but most of the videos

00:01:55,590 --> 00:02:03,630
there this header crash Best Actor

00:02:00,049 --> 00:02:07,920
connection filia or motor filia or some

00:02:03,630 --> 00:02:11,930
other reasons that most of us most of

00:02:07,920 --> 00:02:14,660
the reasons I don't know either for some

00:02:11,930 --> 00:02:18,810
fault phenomena

00:02:14,660 --> 00:02:22,489
we can observe the i/o error complaint

00:02:18,810 --> 00:02:25,739
from Colonel just from the D message and

00:02:22,489 --> 00:02:29,520
we can find the hard disk just it's

00:02:25,739 --> 00:02:31,730
perry from the system in the maybe

00:02:29,520 --> 00:02:36,870
sometimes the whole system of frozen and

00:02:31,730 --> 00:02:40,860
the kernel panic or either corrupted but

00:02:36,870 --> 00:02:46,709
for cloud service that might not be the

00:02:40,860 --> 00:02:48,330
major problem for personal user if the

00:02:46,709 --> 00:02:53,340
hard disk there is a problem for the

00:02:48,330 --> 00:02:55,709
hard disk a maybe we just reboot the

00:02:53,340 --> 00:02:58,290
machine if the machine works we just

00:02:55,709 --> 00:03:00,390
backup the data if the machine to not

00:02:58,290 --> 00:03:03,480
work we just replace the hard disk and

00:03:00,390 --> 00:03:06,830
the backup they restore the data form

00:03:03,480 --> 00:03:11,910
back up and just a waste the time and

00:03:06,830 --> 00:03:17,970
money for me that there's nothing very

00:03:11,910 --> 00:03:20,640
important but for cloud service if the

00:03:17,970 --> 00:03:23,489
cloud instruct infrastructure serving

00:03:20,640 --> 00:03:27,269
and millions lueders the aftermath of

00:03:23,489 --> 00:03:31,500
hard disk filia is magnified to be much

00:03:27,269 --> 00:03:35,269
more complicated in the next page I will

00:03:31,500 --> 00:03:42,060
introduce of what is the disco filia is

00:03:35,269 --> 00:03:44,579
magnified a single disk driver filia

00:03:42,060 --> 00:03:47,790
maybe not magnify the in cloud

00:03:44,579 --> 00:03:50,160
infrastructure because on a single note

00:03:47,790 --> 00:03:53,160
there might be a molecule virtual

00:03:50,160 --> 00:03:57,630
machine or a container instance running

00:03:53,160 --> 00:04:01,590
on a single note and the service request

00:03:57,630 --> 00:04:04,069
is not served only one note the request

00:04:01,590 --> 00:04:06,420
will be distributed on a cluster and the

00:04:04,069 --> 00:04:09,359
result will be gathered and then

00:04:06,420 --> 00:04:11,760
returned to the user at different a

00:04:09,359 --> 00:04:13,470
request on a single note maybe many

00:04:11,760 --> 00:04:18,500
different the requests are served on a

00:04:13,470 --> 00:04:18,500
single note in the and

00:04:19,850 --> 00:04:26,090
time to time maybe a very frequently or

00:04:22,340 --> 00:04:28,850
emergent there is a requirement that the

00:04:26,090 --> 00:04:32,090
program binary should be upgrade maybe

00:04:28,850 --> 00:04:37,790
four bucks maybe for a version update

00:04:32,090 --> 00:04:41,450
and a reason and there and even we find

00:04:37,790 --> 00:04:44,960
the hardware is a it's filia we do not

00:04:41,450 --> 00:04:48,530
change them I was just a schedule it to

00:04:44,960 --> 00:04:52,220
the decided time maybe delay two weeks

00:04:48,530 --> 00:04:55,150
or one or two months because some some

00:04:52,220 --> 00:04:58,010
data center is far away from the company

00:04:55,150 --> 00:05:01,660
maybe every one or two months we just

00:04:58,010 --> 00:05:09,080
traveled there to replace the hardware

00:05:01,660 --> 00:05:13,610
so if a a single hard disk a field maybe

00:05:09,080 --> 00:05:19,400
we will find a positive influence on

00:05:13,610 --> 00:05:22,940
many virtual machine many requests even

00:05:19,400 --> 00:05:26,720
we can other fix the hardware filia very

00:05:22,940 --> 00:05:33,260
fast so here is some data we observe a

00:05:26,720 --> 00:05:35,600
line for the hard disk video we can see

00:05:33,260 --> 00:05:38,900
that for hard disk of running online for

00:05:35,600 --> 00:05:40,550
more than three years the for the

00:05:38,900 --> 00:05:45,950
percentage is quite quite high it's

00:05:40,550 --> 00:05:48,740
around there it's around 0.5 present the

00:05:45,950 --> 00:05:52,270
numbers seems very small to personal

00:05:48,740 --> 00:05:57,380
usage but for a cloud infrastructure

00:05:52,270 --> 00:05:59,920
every server have 12 hot disk and the

00:05:57,380 --> 00:06:06,530
right now we have maybe hundred thousand

00:05:59,920 --> 00:06:11,210
servers that's a big number the here is

00:06:06,530 --> 00:06:14,510
the photo rate among different hottest

00:06:11,210 --> 00:06:17,570
vendor I'm not able I'm not authorized

00:06:14,510 --> 00:06:23,090
to to show the name of the vendors just

00:06:17,570 --> 00:06:25,420
there yeah name them ABC here is the

00:06:23,090 --> 00:06:30,350
photo rate among all vendors I mean that

00:06:25,420 --> 00:06:33,980
all the failed disk from the whole IDC

00:06:30,350 --> 00:06:38,870
and we we just divide them

00:06:33,980 --> 00:06:44,240
against the vendors brand yeah and the

00:06:38,870 --> 00:06:47,930
that picture is the forger reach just

00:06:44,240 --> 00:06:50,840
weaving each vendor that how many hard

00:06:47,930 --> 00:06:54,950
disks we we bought and how many hard

00:06:50,840 --> 00:07:01,040
disk among the this brand just that we

00:06:54,950 --> 00:07:04,550
found a fear food from the data we can

00:07:01,040 --> 00:07:11,420
find that hard disk is a quite easy to

00:07:04,550 --> 00:07:14,540
be broken even much easier than the

00:07:11,420 --> 00:07:19,910
usage in personal use users because in

00:07:14,540 --> 00:07:26,900
the IDC there is a shake the heart noise

00:07:19,910 --> 00:07:28,850
lot of negative factors so if a disco

00:07:26,900 --> 00:07:33,830
filia results a kernel panic we are

00:07:28,850 --> 00:07:36,740
lucky we are lucky because because there

00:07:33,830 --> 00:07:40,520
are a lot of copy or duplication among

00:07:36,740 --> 00:07:43,700
the the infrastructure panic doesn't

00:07:40,520 --> 00:07:46,550
matter too much all virtual machines are

00:07:43,700 --> 00:07:50,360
instance have to be migrated or restart

00:07:46,550 --> 00:07:53,270
and Tina has has to be balanced or

00:07:50,360 --> 00:07:55,730
rebalance to other nodes in a wait for

00:07:53,270 --> 00:08:01,540
arranged time to repair or replace the

00:07:55,730 --> 00:08:04,940
hardware that's ok that's ok because

00:08:01,540 --> 00:08:08,990
there's no very very tight relationship

00:08:04,940 --> 00:08:10,750
between each node in the cluster if one

00:08:08,990 --> 00:08:14,020
know the died that's ok we just the

00:08:10,750 --> 00:08:17,390
transfer this service to other nodes

00:08:14,020 --> 00:08:21,230
because we we are not able to always fix

00:08:17,390 --> 00:08:25,160
the hardware at once if the server downs

00:08:21,230 --> 00:08:28,000
we we always arrange a decided time may

00:08:25,160 --> 00:08:33,470
be delays one or two week I decide to

00:08:28,000 --> 00:08:36,590
replace them but but if these can filia

00:08:33,470 --> 00:08:41,900
results a long latency I oh that's

00:08:36,590 --> 00:08:45,950
Heydrich Tessa if service request might

00:08:41,900 --> 00:08:47,500
be distributed mahendra servers if one

00:08:45,950 --> 00:08:52,640
or two servers

00:08:47,500 --> 00:08:56,209
take too much time to return the result

00:08:52,640 --> 00:09:00,589
the whole service request will be made

00:08:56,209 --> 00:09:03,310
the very slow and the program battery

00:09:00,589 --> 00:09:06,290
updates are blocked that's because

00:09:03,310 --> 00:09:10,250
sometimes the hard disk the hard disk

00:09:06,290 --> 00:09:13,640
there we can see that from the system

00:09:10,250 --> 00:09:18,019
but when we try to access I owe on a

00:09:13,640 --> 00:09:24,950
diet device the operation is blocked for

00:09:18,019 --> 00:09:28,040
some demon program if they happens they

00:09:24,950 --> 00:09:31,160
happen to access the IO the the disk

00:09:28,040 --> 00:09:33,079
into the IO and it blocked and the

00:09:31,160 --> 00:09:35,630
operation people want to update the

00:09:33,079 --> 00:09:38,480
program the updated programmer will

00:09:35,630 --> 00:09:41,240
return ebz or something else that we

00:09:38,480 --> 00:09:49,070
cannot update the battery just because

00:09:41,240 --> 00:09:52,459
the inode is busy because we have a lot

00:09:49,070 --> 00:09:56,570
of service lot of virtual machine or

00:09:52,459 --> 00:10:00,920
instance running on a single note we

00:09:56,570 --> 00:10:04,910
have a around 10 or 12 hard disk only

00:10:00,920 --> 00:10:09,110
one hard disk does not work the rest can

00:10:04,910 --> 00:10:11,630
still provide the services but if the

00:10:09,110 --> 00:10:14,120
system demon is a block and then we are

00:10:11,630 --> 00:10:16,820
not able to update them that's haddock

00:10:14,120 --> 00:10:18,769
that means the whole system all the

00:10:16,820 --> 00:10:25,250
services are running running on this

00:10:18,769 --> 00:10:28,399
server is the functional that's why I

00:10:25,250 --> 00:10:30,339
say that the negative influence of hard

00:10:28,399 --> 00:10:35,260
disk Ophelia is very easy to be

00:10:30,339 --> 00:10:35,260
magnified in large-scale cloud service

00:10:39,139 --> 00:10:46,920
but how much how much I owe latency

00:10:44,190 --> 00:10:50,009
means the hard disk is almost a broken

00:10:46,920 --> 00:10:52,470
it's not easy to see because fast and

00:10:50,009 --> 00:10:56,100
the slow is a relative to everyone

00:10:52,470 --> 00:10:59,730
different the person but we need to know

00:10:56,100 --> 00:11:03,029
that if the ladies of the IO on a disk

00:10:59,730 --> 00:11:06,420
is too slow we should not use this disc

00:11:03,029 --> 00:11:09,240
anymore because the negative influence

00:11:06,420 --> 00:11:13,110
will be magnified to the whole cluster

00:11:09,240 --> 00:11:17,449
we need to disable or isolate the hard

00:11:13,110 --> 00:11:25,589
disk at once but how much latency yes

00:11:17,449 --> 00:11:29,100
it's not a simple story yeah so let's

00:11:25,589 --> 00:11:33,209
see that where the arrow latency comes

00:11:29,100 --> 00:11:37,889
from the major I oh Linda see observed

00:11:33,209 --> 00:11:41,120
from application comes from that the

00:11:37,889 --> 00:11:45,480
application being primped by the

00:11:41,120 --> 00:11:47,970
processor scheduler or the waiting for

00:11:45,480 --> 00:11:50,519
some available clearing pages that means

00:11:47,970 --> 00:11:53,490
that maybe they did before I oh and the

00:11:50,519 --> 00:11:56,160
dirty too much pages there is no claim

00:11:53,490 --> 00:11:58,529
page so waiting for the right back to to

00:11:56,160 --> 00:12:03,180
reclaim more claim pages to continue the

00:11:58,529 --> 00:12:06,389
i/o or some locking box but that's

00:12:03,180 --> 00:12:10,889
that's easy to to encounter if we have a

00:12:06,389 --> 00:12:14,190
lot of machine and the the end of the

00:12:10,889 --> 00:12:17,370
i/o request just insert into the io

00:12:14,190 --> 00:12:23,160
scheduler and need to stay for some time

00:12:17,370 --> 00:12:25,620
to reorder there is some delay and the

00:12:23,160 --> 00:12:27,779
IO issued to the hard disk just waiting

00:12:25,620 --> 00:12:31,019
for the complete that's kind of delay

00:12:27,779 --> 00:12:35,310
the most of the i/o delay I all agency

00:12:31,019 --> 00:12:39,649
costs up by almost broken Hadees comes

00:12:35,310 --> 00:12:39,649
from the last two reasons

00:12:40,259 --> 00:12:45,779
that's why that we only mirror the air

00:12:43,559 --> 00:12:49,709
latency from the application is not

00:12:45,779 --> 00:12:53,959
enough because if we marry our latest

00:12:49,709 --> 00:12:58,889
from application only maybe we will

00:12:53,959 --> 00:13:02,699
counter this enter this into the latency

00:12:58,889 --> 00:13:05,479
but that's incorrect because the first

00:13:02,699 --> 00:13:14,239
two reasons for the latency is not

00:13:05,479 --> 00:13:18,139
related directly to hard disk yeah so

00:13:14,239 --> 00:13:22,999
the large the previous pages just

00:13:18,139 --> 00:13:28,109
introduce the background that why we

00:13:22,999 --> 00:13:31,439
marry the arrow lenta see because very

00:13:28,109 --> 00:13:35,489
long latency introduced negative

00:13:31,439 --> 00:13:38,879
influence to our service and as the the

00:13:35,489 --> 00:13:42,299
negative influence is a magnified to the

00:13:38,879 --> 00:13:46,739
whole cluster maybe so we we just want

00:13:42,299 --> 00:13:54,199
to mirror the i/o latency and then learn

00:13:46,739 --> 00:13:57,809
learn at what threshold we could use to

00:13:54,199 --> 00:14:01,619
to make the judgment that the hard disk

00:13:57,809 --> 00:14:04,410
is almost a broken and we should isolate

00:14:01,619 --> 00:14:09,899
the hard disk and their replace them in

00:14:04,410 --> 00:14:12,649
a arrange the time so right now let's

00:14:09,899 --> 00:14:12,649
get to the mid part

00:14:14,509 --> 00:14:21,060
so we we do not only ameri the latency

00:14:18,839 --> 00:14:23,899
from application level also we need to

00:14:21,060 --> 00:14:28,170
marry that leading from a colonel and

00:14:23,899 --> 00:14:33,410
the there are some motivation from this

00:14:28,170 --> 00:14:37,110
this work first we need to identify the

00:14:33,410 --> 00:14:41,850
slow hard disk slow means slow means

00:14:37,110 --> 00:14:46,970
they're hard disk is almost broken so

00:14:41,850 --> 00:14:46,970
the i/o is very slow I don't mean that

00:14:47,360 --> 00:14:57,810
some healthy disk it's just a very slow

00:14:51,180 --> 00:15:01,410
I don't mean this in the ignore all the

00:14:57,810 --> 00:15:05,699
latency that no relationship to the hard

00:15:01,410 --> 00:15:11,519
disk for example the locking or the rep

00:15:05,699 --> 00:15:17,910
back post the process or primped ignore

00:15:11,519 --> 00:15:21,209
them so we can learn a latency threshold

00:15:17,910 --> 00:15:27,209
from almost a broken disk we learn the

00:15:21,209 --> 00:15:31,649
number and use this number back to the

00:15:27,209 --> 00:15:35,939
online system to check it first if the

00:15:31,649 --> 00:15:39,829
number is correct second we should

00:15:35,939 --> 00:15:46,069
detected almost a failure disk before it

00:15:39,829 --> 00:15:51,060
really very slow to make the service

00:15:46,069 --> 00:15:55,589
very slow and after we find and identify

00:15:51,060 --> 00:15:59,339
the the slow hard disk that's easy we

00:15:55,589 --> 00:16:03,540
just mask the hard disk and from

00:15:59,339 --> 00:16:07,230
application may be from a name node or

00:16:03,540 --> 00:16:10,529
maybe from some application just on each

00:16:07,230 --> 00:16:16,819
server just a mark do not access this

00:16:10,529 --> 00:16:23,010
hard disk anymore that's all and we will

00:16:16,819 --> 00:16:25,170
maybe maybe sometimes there are three or

00:16:23,010 --> 00:16:26,860
four hard disk on a single note all

00:16:25,170 --> 00:16:29,170
filled

00:16:26,860 --> 00:16:35,490
we just decided to shut down the machine

00:16:29,170 --> 00:16:39,130
and I replace hard disk but there are

00:16:35,490 --> 00:16:42,780
some restriction because the whole

00:16:39,130 --> 00:16:47,530
systems are lying they really make money

00:16:42,780 --> 00:16:52,680
by server other people's just like a

00:16:47,530 --> 00:16:57,400
change something on a running car so

00:16:52,680 --> 00:17:01,660
from the operation team they require

00:16:57,400 --> 00:17:04,990
that no colonel reboot if there is

00:17:01,660 --> 00:17:09,970
Colonel reboot that means that the data

00:17:04,990 --> 00:17:14,580
has to be rebalanced and the virtual

00:17:09,970 --> 00:17:20,860
machine instant have to be a macro edit

00:17:14,580 --> 00:17:25,600
that's a penalty on time on traffic so

00:17:20,860 --> 00:17:30,120
no Colonel reboot and no volume a reboot

00:17:25,600 --> 00:17:36,490
or fail sesame reformat that means that

00:17:30,120 --> 00:17:39,780
it's an acceptable if we just changed

00:17:36,490 --> 00:17:43,960
some code in a colonel or in block layer

00:17:39,780 --> 00:17:47,980
we have to ask the operation team to

00:17:43,960 --> 00:17:50,500
reboot the the logical volume or

00:17:47,980 --> 00:17:55,140
reformat the whole file system that

00:17:50,500 --> 00:18:01,270
ottomans that they have to maybe

00:17:55,140 --> 00:18:05,920
rebalance the data reboot the system for

00:18:01,270 --> 00:18:10,720
the rebalance that data for a server

00:18:05,920 --> 00:18:13,540
with around 12 hard disks and each hard

00:18:10,720 --> 00:18:17,830
disk is a two terabyte that is spending

00:18:13,540 --> 00:18:19,780
really a very long time even among the

00:18:17,830 --> 00:18:25,810
high speed network that's been a very

00:18:19,780 --> 00:18:31,330
long time for the cluster with maybe

00:18:25,810 --> 00:18:34,300
10,000 servers that's midnight so let's

00:18:31,330 --> 00:18:39,720
Numair so the operation team will not

00:18:34,300 --> 00:18:39,720
accept to reformat or reboot the volume

00:18:39,990 --> 00:18:48,370
and when we marry the air latency in a

00:18:45,160 --> 00:18:54,000
cluster we should make sure that there

00:18:48,370 --> 00:18:57,880
is no too much oh there it has to be

00:18:54,000 --> 00:19:02,530
little performance penalty when we

00:18:57,880 --> 00:19:04,210
mirror the the latency and them that

00:19:02,530 --> 00:19:11,860
when we mirror the latency the matter

00:19:04,210 --> 00:19:14,740
should be tunable and the flexible so we

00:19:11,860 --> 00:19:20,610
we just implement a method we caught a

00:19:14,740 --> 00:19:23,710
vm latency this method just a patch

00:19:20,610 --> 00:19:28,270
Colonel model we just passed it a device

00:19:23,710 --> 00:19:31,560
memory code so we we avoid the globe

00:19:28,270 --> 00:19:38,050
change global change in block layer in

00:19:31,560 --> 00:19:41,170
the because we we because we need to

00:19:38,050 --> 00:19:43,750
merit the latency so we needed some

00:19:41,170 --> 00:19:50,110
resource for the time step we do this

00:19:43,750 --> 00:19:55,090
one ketan guide kitten kite user global

00:19:50,110 --> 00:19:59,230
timer resource which needs a spin lock

00:19:55,090 --> 00:20:02,410
when accessing the the tamari sauce so

00:19:59,230 --> 00:20:07,750
the spin lock is a global serial code

00:20:02,410 --> 00:20:11,350
path that is not acceptable for

00:20:07,750 --> 00:20:15,130
high-speed storage media for demo PCIe

00:20:11,350 --> 00:20:18,580
SSD but we test it for hard disk that's

00:20:15,130 --> 00:20:21,880
okay for her disc most of the heartaches

00:20:18,580 --> 00:20:29,170
right now the i/o tips is around 31

00:20:21,880 --> 00:20:35,830
maybe 442 of hard disk yeah just the 300

00:20:29,170 --> 00:20:37,440
no more than 400 I'll issue to the oil

00:20:35,830 --> 00:20:41,650
block at same time so

00:20:37,440 --> 00:20:44,400
for every second no more than 400 arrow

00:20:41,650 --> 00:20:52,630
conflicts with the spin lock that's okay

00:20:44,400 --> 00:20:58,870
it's not too much in it we just change

00:20:52,630 --> 00:21:01,300
the vest member and in in this method

00:20:58,870 --> 00:21:03,910
that means that we don't need to reboot

00:21:01,300 --> 00:21:07,660
the colonel we just insert the current

00:21:03,910 --> 00:21:10,660
model in the use it if we do note you we

00:21:07,660 --> 00:21:15,790
do not use it we just remove the current

00:21:10,660 --> 00:21:18,520
model that's okay and the there are a

00:21:15,790 --> 00:21:22,530
lot of target device mode in device

00:21:18,520 --> 00:21:28,720
member one of the simplest is Melania

00:21:22,530 --> 00:21:33,130
linear map we just to learning a pipe on

00:21:28,720 --> 00:21:35,790
the existing file system everything's

00:21:33,130 --> 00:21:40,050
done we don't need to we don't need to

00:21:35,790 --> 00:21:43,420
reformat a file system we just mount the

00:21:40,050 --> 00:21:46,540
device paper target in the reader did

00:21:43,420 --> 00:21:49,360
all the metadata audit just existing on

00:21:46,540 --> 00:21:52,390
the hard disk Kristen mapping table is

00:21:49,360 --> 00:21:57,450
just the memory we don't need to change

00:21:52,390 --> 00:22:00,430
the any data on a disk so we avoid to

00:21:57,450 --> 00:22:04,120
review the volume or reformatted file

00:22:00,430 --> 00:22:09,700
system and that this method is very

00:22:04,120 --> 00:22:15,760
simple the patch just their change three

00:22:09,700 --> 00:22:18,760
files just to hundreds no other for

00:22:15,760 --> 00:22:25,960
hundreds changed in the patches very

00:22:18,760 --> 00:22:31,390
simple just the wii we are not able to

00:22:25,960 --> 00:22:34,570
observe any extra latency on this cal in

00:22:31,390 --> 00:22:38,920
this method so I see that a little extra

00:22:34,570 --> 00:22:45,490
delay for hard disk that's okay but this

00:22:38,920 --> 00:22:49,570
method is introduced observe the latency

00:22:45,490 --> 00:22:53,070
for high-speed a PCIe SSD yeah so we

00:22:49,570 --> 00:22:53,070
just use it for hard disk

00:22:55,200 --> 00:23:01,020
so this is the basic idea of DM latency

00:23:02,460 --> 00:23:09,940
here is the map device we use the term

00:23:06,640 --> 00:23:12,280
onto the file system and here is the

00:23:09,940 --> 00:23:14,980
device labor logic inside the logic

00:23:12,280 --> 00:23:18,250
there is the map people the map people

00:23:14,980 --> 00:23:21,990
just decide which I owe should be issued

00:23:18,250 --> 00:23:28,270
two to reach and the line target device

00:23:21,990 --> 00:23:32,800
for for for us in this case we just set

00:23:28,270 --> 00:23:36,490
up a line at target device for each disk

00:23:32,800 --> 00:23:39,400
so here only one lie near target device

00:23:36,490 --> 00:23:41,770
that's very simple so we don't need to

00:23:39,400 --> 00:23:45,040
do something like a mirror or read no

00:23:41,770 --> 00:23:48,550
just the linear device lania target

00:23:45,040 --> 00:23:54,850
device and it divides in my part logic

00:23:48,550 --> 00:23:58,270
here we change the code to places one is

00:23:54,850 --> 00:24:01,960
when I issue which in the code start io

00:23:58,270 --> 00:24:06,040
account to write a time step in the

00:24:01,960 --> 00:24:08,950
structure DMI oh and the win the IO

00:24:06,040 --> 00:24:11,380
complete it in a wheel change will be

00:24:08,950 --> 00:24:14,590
patched in the Iowa count just calculate

00:24:11,380 --> 00:24:17,800
the agency and the record it to map the

00:24:14,590 --> 00:24:21,790
device the memory device is a per-device

00:24:17,800 --> 00:24:25,240
data structure so we can just the record

00:24:21,790 --> 00:24:30,790
or different Layton sees that is take

00:24:25,240 --> 00:24:37,830
information to each to each map the

00:24:30,790 --> 00:24:41,140
device in the we just add a little

00:24:37,830 --> 00:24:47,800
change to the core data structure for

00:24:41,140 --> 00:24:52,300
DML at a design the long variable to

00:24:47,800 --> 00:24:55,990
record the time when the i/o is easy to

00:24:52,300 --> 00:24:59,020
enter line a devices and therefore this

00:24:55,990 --> 00:25:02,730
map devices we add a three data

00:24:59,020 --> 00:25:06,360
structure to record the latency

00:25:02,730 --> 00:25:09,280
distribution information

00:25:06,360 --> 00:25:15,690
that's all so that's why the patch the

00:25:09,280 --> 00:25:15,690
patch is a very simple for the interface

00:25:16,620 --> 00:25:27,150
when all the latest information record

00:25:21,370 --> 00:25:27,150
here the colonel provide an interface to

00:25:28,530 --> 00:25:36,100
make the user space to access the

00:25:31,300 --> 00:25:39,310
latency data that's the latency data in

00:25:36,100 --> 00:25:43,930
second level millisecond or micro second

00:25:39,310 --> 00:25:49,750
and also there is a file to recite all

00:25:43,930 --> 00:25:53,620
the counters that's very easy all the

00:25:49,750 --> 00:25:56,620
information are tax based so a bash even

00:25:53,620 --> 00:26:01,470
the bash script can handle Odin or the

00:25:56,620 --> 00:26:08,320
data this is the format of the data

00:26:01,470 --> 00:26:13,030
export it easy just the range of the

00:26:08,320 --> 00:26:15,330
latency and the L members just hit the

00:26:13,030 --> 00:26:15,330
range

00:26:31,389 --> 00:26:37,539
the bar is ready to take your orders

00:26:33,989 --> 00:26:41,829
there's a one who talks finish please go

00:26:37,539 --> 00:26:44,190
down get some orders later on hopefully

00:26:41,829 --> 00:26:44,190
get

00:26:49,059 --> 00:26:55,789
they could also be down there to the

00:26:51,649 --> 00:27:06,049
heart of your questions thank you very

00:26:55,789 --> 00:27:11,960
much thank you very much okay okay let's

00:27:06,049 --> 00:27:15,710
back to the topic so this is all the

00:27:11,960 --> 00:27:19,759
information and the code running in

00:27:15,710 --> 00:27:22,159
chrono site it's very simple and all the

00:27:19,759 --> 00:27:25,840
events they're all complicated the logic

00:27:22,159 --> 00:27:25,840
wrong in user space

00:27:32,920 --> 00:27:43,550
here is some real example for the data

00:27:37,430 --> 00:27:45,890
how the data is used in one of the align

00:27:43,550 --> 00:27:48,170
cloud system we use a gun Liam

00:27:45,890 --> 00:27:51,080
monitoring system to monitor the

00:27:48,170 --> 00:27:53,540
resource or the performance number my

00:27:51,080 --> 00:27:55,880
colleagues helped me to integrate the

00:27:53,540 --> 00:28:00,500
the latency number into the courier

00:27:55,880 --> 00:28:02,540
system the the different color the line

00:28:00,500 --> 00:28:06,590
with different color means that

00:28:02,540 --> 00:28:09,800
different range of the latency and the

00:28:06,590 --> 00:28:18,320
value means how many aisles just hit

00:28:09,800 --> 00:28:21,260
into the little syringe did this isn't

00:28:18,320 --> 00:28:24,290
just the into credited maybe within

00:28:21,260 --> 00:28:26,740
within two weeks we still learn how to

00:28:24,290 --> 00:28:26,740
use it

00:28:31,290 --> 00:28:41,360
yeah and another Vanessa I want to

00:28:38,010 --> 00:28:44,850
mention it sir this method this method

00:28:41,360 --> 00:28:50,220
there is no any resource allocation or

00:28:44,850 --> 00:28:54,480
reclaim no we just calculate the number

00:28:50,220 --> 00:28:55,770
in the check the index and the right

00:28:54,480 --> 00:28:59,850
call that delayed into the data

00:28:55,770 --> 00:29:04,680
structure there is no tree walk on the

00:28:59,850 --> 00:29:06,780
tree or some memory allocated no sew the

00:29:04,680 --> 00:29:10,410
patch is a very simple we don't need to

00:29:06,780 --> 00:29:21,840
worry about memory leaks or some very

00:29:10,410 --> 00:29:24,660
hot code path no so just I try to to to

00:29:21,840 --> 00:29:28,740
find the three different typical cloud

00:29:24,660 --> 00:29:33,150
services to collect the data a one is

00:29:28,740 --> 00:29:36,690
the MapReduce service one is virtual

00:29:33,150 --> 00:29:38,940
machine cluster no one is the elastic

00:29:36,690 --> 00:29:45,510
block service at block storage service

00:29:38,940 --> 00:29:51,510
like cluster now all the services with a

00:29:45,510 --> 00:29:54,900
very high workload so for some

00:29:51,510 --> 00:29:57,090
interesting phenomena observed from the

00:29:54,900 --> 00:30:00,450
data we collected from the different

00:29:57,090 --> 00:30:04,920
services when ladies a distribution it

00:30:00,450 --> 00:30:08,340
is displayed it in the former paragraph

00:30:04,920 --> 00:30:12,210
put the data just like theater in number

00:30:08,340 --> 00:30:16,520
it I don't have too much sense but when

00:30:12,210 --> 00:30:20,610
we translate in the diagram interesting

00:30:16,520 --> 00:30:27,630
sorry it's not very clear I will try to

00:30:20,610 --> 00:30:32,900
explain there are some observation from

00:30:27,630 --> 00:30:37,250
the data first Lisa I only didn't see in

00:30:32,900 --> 00:30:44,690
microseconds level I don't find any

00:30:37,250 --> 00:30:44,690
regulation from the latency I mean

00:30:45,140 --> 00:30:53,220
from the data I cannot recognize is it

00:30:49,230 --> 00:30:59,190
some kind of service or is the hard disk

00:30:53,220 --> 00:31:03,030
broken no no regulation I guess maybe

00:30:59,190 --> 00:31:09,000
maybe the reason is that normally hard

00:31:03,030 --> 00:31:13,940
disk um balita io around 10 micro 10

00:31:09,000 --> 00:31:18,180
millisecond for the latency within

00:31:13,940 --> 00:31:21,090
microseconds I mean less than 1 million

00:31:18,180 --> 00:31:25,800
seconds that's just a catch to catch

00:31:21,090 --> 00:31:30,440
copy because the the model is not able

00:31:25,800 --> 00:31:33,440
to complete the IO waiting so small time

00:31:30,440 --> 00:31:37,500
so the just a cache to cache memory

00:31:33,440 --> 00:31:46,230
cache to catch a memory copy so there is

00:31:37,500 --> 00:31:51,840
no real disk platter access observation

00:31:46,230 --> 00:31:56,390
to slow this slow io discs are easy to

00:31:51,840 --> 00:32:02,660
be identified from other discus in the

00:31:56,390 --> 00:32:06,350
millisecond I all agency here is the

00:32:02,660 --> 00:32:11,190
verified healthy discus verify the means

00:32:06,350 --> 00:32:15,000
we check that the disc whether it's a

00:32:11,190 --> 00:32:18,860
really work correctly or healthy for

00:32:15,000 --> 00:32:26,070
this disk latency distribution like this

00:32:18,860 --> 00:32:32,090
all the IO latency hits here it's just

00:32:26,070 --> 00:32:32,090
within 10 milliseconds

00:32:32,720 --> 00:32:43,480
yeah this in the photo verify the slow

00:32:38,600 --> 00:32:43,480
disk the latency distribution like this

00:32:46,090 --> 00:32:58,880
here is around the 500 millisecond they

00:32:54,230 --> 00:33:02,090
are very different and we also observe

00:32:58,880 --> 00:33:06,500
some distribution that we call that too

00:33:02,090 --> 00:33:11,510
suspicious like this there is a long

00:33:06,500 --> 00:33:14,289
tale of the latency long tail but for

00:33:11,510 --> 00:33:18,110
the online monitoring system there is no

00:33:14,289 --> 00:33:25,370
complain that this guy always very slow

00:33:18,110 --> 00:33:27,260
I need to check because I feel maybe

00:33:25,370 --> 00:33:34,909
there are some back for the online

00:33:27,260 --> 00:33:39,010
monitoring system or maybe we may be

00:33:34,909 --> 00:33:41,840
that we need to conclude some rules that

00:33:39,010 --> 00:33:45,309
we need to pay attention on this kind of

00:33:41,840 --> 00:33:45,309
latency distribution

00:33:50,980 --> 00:33:57,220
so we can see that from the graphics

00:33:53,970 --> 00:34:00,669
most of the aisle agency are really slow

00:33:57,220 --> 00:34:03,179
hard disk are much more beyond 10

00:34:00,669 --> 00:34:09,369
milliseconds even more than 300

00:34:03,179 --> 00:34:16,500
millisecond average member is around 500

00:34:09,369 --> 00:34:21,429
millisecond above the upper division 3

00:34:16,500 --> 00:34:24,600
the i/o latency unhealthy disk the

00:34:21,429 --> 00:34:27,429
distribution very similar to a

00:34:24,600 --> 00:34:31,690
chi-squared distribution doesn't matter

00:34:27,429 --> 00:34:36,550
i I just find that the shape the very

00:34:31,690 --> 00:34:40,330
similar no reason i jested synod it's

00:34:36,550 --> 00:34:46,510
like a chi-squared distribution I don't

00:34:40,330 --> 00:34:48,940
know why it looks like this but it's a

00:34:46,510 --> 00:34:51,250
reasonable because for a healthy disc

00:34:48,940 --> 00:34:53,520
most of the Iola dance they should be

00:34:51,250 --> 00:34:53,520
here

00:34:59,590 --> 00:35:05,570
the observation forest a latency

00:35:02,660 --> 00:35:09,340
distribution on slow disc looks

00:35:05,570 --> 00:35:16,870
following a likely normal distribution

00:35:09,340 --> 00:35:21,250
yeah like this there is a peak in the

00:35:16,870 --> 00:35:21,250
decrease on both side

00:35:25,260 --> 00:35:33,130
I don't know why I don't know why

00:35:28,590 --> 00:35:36,520
because right now way from the data I

00:35:33,130 --> 00:35:40,150
only find their three very slow and

00:35:36,520 --> 00:35:42,610
verify the very slow disk so I need more

00:35:40,150 --> 00:35:48,580
data to to to make the more accurate

00:35:42,610 --> 00:35:51,700
conclusion but at least we know that we

00:35:48,580 --> 00:35:55,120
can learn some one data from the graphic

00:35:51,700 --> 00:35:58,000
maybe we can set some trash out to check

00:35:55,120 --> 00:36:06,100
whether the disk is very slow come on

00:35:58,000 --> 00:36:10,270
this data so after we learn from the

00:36:06,100 --> 00:36:13,450
real data we have some conclusion 1s I

00:36:10,270 --> 00:36:17,940
old ladies a distribution is independent

00:36:13,450 --> 00:36:22,150
from el patents in railroad casa

00:36:17,940 --> 00:36:27,580
inferior continuous i request should be

00:36:22,150 --> 00:36:30,100
much faster than random i/o request but

00:36:27,580 --> 00:36:32,950
from the above data which is collected

00:36:30,100 --> 00:36:36,730
from different tapco cloud services with

00:36:32,950 --> 00:36:38,820
different patterns there is no distinct

00:36:36,730 --> 00:36:41,980
difference on our little situation

00:36:38,820 --> 00:36:47,320
distribution pattern I mean in a long

00:36:41,980 --> 00:36:51,220
term there is no different in other

00:36:47,320 --> 00:36:55,420
words no matter continuous random or

00:36:51,220 --> 00:36:59,260
mixed io patents with a long time a real

00:36:55,420 --> 00:37:02,170
workload I mean real workload I already

00:36:59,260 --> 00:37:05,680
say distribution is a similar I'll

00:37:02,170 --> 00:37:08,370
health hard disk I mean the the

00:37:05,680 --> 00:37:12,190
distribution are all healthy hard disk

00:37:08,370 --> 00:37:16,360
the looks very similar no matter which

00:37:12,190 --> 00:37:20,250
can of our patterns the same rule works

00:37:16,360 --> 00:37:20,250
on a slow hard disk

00:37:21,260 --> 00:37:28,950
congruent to disa it seems that 500 and

00:37:26,700 --> 00:37:32,340
million seconds and the eighty-five

00:37:28,950 --> 00:37:34,740
percent might be a fracture to value to

00:37:32,340 --> 00:37:39,830
decide whether it is slow hard dicks is

00:37:34,740 --> 00:37:45,540
about to fail there there is no any

00:37:39,830 --> 00:37:48,330
scientific reason we just we just gather

00:37:45,540 --> 00:37:52,650
data from all the data we observed it

00:37:48,330 --> 00:37:57,810
from online systems because we are not

00:37:52,650 --> 00:38:00,600
only to to use the this single member to

00:37:57,810 --> 00:38:07,890
determine whether the hard disk is about

00:38:00,600 --> 00:38:13,320
to fail we also need to to know how many

00:38:07,890 --> 00:38:18,180
iOS are slow io among all the i/o

00:38:13,320 --> 00:38:20,870
request and we observe that all real

00:38:18,180 --> 00:38:20,870
slow disk

00:38:25,730 --> 00:38:36,440
on a real slow disk the i/o request

00:38:30,410 --> 00:38:42,920
number just after here it's around there

00:38:36,440 --> 00:38:50,350
I mean if here is the 500 milliseconds

00:38:42,920 --> 00:38:53,590
all the value after the point this up to

00:38:50,350 --> 00:38:53,590
eighty-five percent

00:39:02,620 --> 00:39:10,970
and that there is a question there is a

00:39:05,630 --> 00:39:14,060
question that the latency mirrored by DM

00:39:10,970 --> 00:39:19,220
latency is not exactly hard disk and

00:39:14,060 --> 00:39:22,850
servant delay why because when the

00:39:19,220 --> 00:39:26,240
device mapper issued the i/o request to

00:39:22,850 --> 00:39:31,220
the underlying layer we record the time

00:39:26,240 --> 00:39:34,430
step and we calculate the latency when

00:39:31,220 --> 00:39:38,140
the arrow completed back to the device

00:39:34,430 --> 00:39:41,900
metal layer for the underlying layer

00:39:38,140 --> 00:39:46,820
there is a pool cues for each and the

00:39:41,900 --> 00:39:49,940
line devices there is IO elevator for

00:39:46,820 --> 00:39:54,620
each underlying devices the requester

00:39:49,940 --> 00:39:57,260
has to be stay in the i/o elevator to be

00:39:54,620 --> 00:40:00,890
reordered with a different a low

00:39:57,260 --> 00:40:07,760
priority by the arrow scheduler that

00:40:00,890 --> 00:40:13,300
takes time we just right now the latency

00:40:07,760 --> 00:40:16,400
number returned by TM latency is the sum

00:40:13,300 --> 00:40:21,310
by two parts one is the latency in

00:40:16,400 --> 00:40:26,180
device ioq one is a latency in hard disk

00:40:21,310 --> 00:40:30,110
so in current observation latency in

00:40:26,180 --> 00:40:34,850
device ioq influenza little very little

00:40:30,110 --> 00:40:38,960
on the low latency distribution but it's

00:40:34,850 --> 00:40:41,810
only current resolution or limited hard

00:40:38,960 --> 00:40:45,110
disk from limited service we need more

00:40:41,810 --> 00:40:49,910
time more data to make decision whether

00:40:45,110 --> 00:40:53,080
it's real that the delay in the io q has

00:40:49,910 --> 00:40:53,080
a little influence

00:40:55,660 --> 00:41:05,660
last one the challenge this is the

00:41:01,880 --> 00:41:09,470
challenge and then the next work I'm

00:41:05,660 --> 00:41:12,400
about to do one is when we need more

00:41:09,470 --> 00:41:14,960
accurate I all readings in merriment

00:41:12,400 --> 00:41:17,749
divided the current and latency studies

00:41:14,960 --> 00:41:21,529
the info into two parts it's better to

00:41:17,749 --> 00:41:24,109
know how much time spent on the arrow

00:41:21,529 --> 00:41:28,880
scheduler and the how much time I really

00:41:24,109 --> 00:41:33,130
spent on hard disk currently funded this

00:41:28,880 --> 00:41:37,249
information should be very helpful and

00:41:33,130 --> 00:41:42,829
if we want to mirror how much time

00:41:37,249 --> 00:41:46,519
spending in our schedule or in hard disk

00:41:42,829 --> 00:41:51,470
we need more data structure for ten step

00:41:46,519 --> 00:41:55,210
and when we record the temp step I have

00:41:51,470 --> 00:42:00,140
to try my best to avoid the global lock

00:41:55,210 --> 00:42:03,049
global lock means that the parallel I'll

00:42:00,140 --> 00:42:06,369
request have to be serialized that's the

00:42:03,049 --> 00:42:12,469
performance penalty performance cooler

00:42:06,369 --> 00:42:17,960
so if i use the time step counter the

00:42:12,469 --> 00:42:22,719
counter is a per cpu so in case that the

00:42:17,960 --> 00:42:29,269
cpu which the i/o completed back is

00:42:22,719 --> 00:42:34,880
different from the cpu which issued so

00:42:29,269 --> 00:42:39,920
the latency calculated by different tsc

00:42:34,880 --> 00:42:45,380
number should be very in a curated if i

00:42:39,920 --> 00:42:49,009
use a global tamara resource like h pad

00:42:45,380 --> 00:42:53,359
or something like that use a key type k

00:42:49,009 --> 00:42:58,009
tomcat there is a spin lock inside which

00:42:53,359 --> 00:43:02,749
means on a high-speed IO devices for

00:42:58,009 --> 00:43:06,990
example PCIe SSD hand resort thousands l

00:43:02,749 --> 00:43:11,820
request conflict honestly lock

00:43:06,990 --> 00:43:15,210
that's a big performance penalty so how

00:43:11,820 --> 00:43:19,020
to do how to do this problem that's a

00:43:15,210 --> 00:43:24,960
challenge another is a colonel ABI

00:43:19,020 --> 00:43:28,050
compatibility if the modification in a

00:43:24,960 --> 00:43:31,040
colonel not the colonel model sometimes

00:43:28,050 --> 00:43:35,310
the third part or the commercial driver

00:43:31,040 --> 00:43:39,840
reference this kind of internal routines

00:43:35,310 --> 00:43:42,750
all data structures if we purchase these

00:43:39,840 --> 00:43:45,240
routines and data structures this kind

00:43:42,750 --> 00:43:48,690
of error should not work correctly maybe

00:43:45,240 --> 00:43:53,910
the system will crash so that's a

00:43:48,690 --> 00:43:57,869
challenge how to how to get the correct

00:43:53,910 --> 00:44:01,460
and latency with the compatibility of

00:43:57,869 --> 00:44:07,280
the colonel ABA that's hard very hard

00:44:01,460 --> 00:44:10,800
and we need more testing um more server

00:44:07,280 --> 00:44:15,600
more service and more workload that's

00:44:10,800 --> 00:44:22,170
possible because there are a lot of

00:44:15,600 --> 00:44:25,619
machines in the in in the next several

00:44:22,170 --> 00:44:27,890
months I will continue to work on this

00:44:25,619 --> 00:44:27,890
topic

00:44:31,320 --> 00:44:34,280
look at credit

00:44:37,509 --> 00:44:42,500
suggestion

00:44:39,710 --> 00:44:46,450
I don't think you need to measure all my

00:44:42,500 --> 00:44:49,609
requests preservative sample should do

00:44:46,450 --> 00:44:54,190
so he could write a personal Kate I'm

00:44:49,609 --> 00:44:54,190
get this tribal cafe

00:44:59,010 --> 00:45:03,640
good

00:45:01,119 --> 00:45:09,000
ok

00:45:03,640 --> 00:45:09,000
let me think good suggestion

00:45:12,220 --> 00:45:15,869
so any question

00:45:40,020 --> 00:45:45,220
good I would like to ask what is the

00:45:43,300 --> 00:45:48,850
data structure we hold all the

00:45:45,220 --> 00:45:51,400
information you collect is it you said

00:45:48,850 --> 00:45:53,580
it is simple but is it a file is it that

00:45:51,400 --> 00:45:55,380
you collected in the database afterwards

00:45:53,580 --> 00:46:03,220
because that could be time consuming

00:45:55,380 --> 00:46:06,580
yeah let me calculate let me explain so

00:46:03,220 --> 00:46:09,190
for every io inside the device mapper

00:46:06,580 --> 00:46:14,920
layer there at least data structure TMI

00:46:09,190 --> 00:46:19,420
oh I added time step just here so when

00:46:14,920 --> 00:46:24,730
the i/o complic completed back to the

00:46:19,420 --> 00:46:27,580
device metal layer I just call I just

00:46:24,730 --> 00:46:32,680
call Kate and guide the function to get

00:46:27,580 --> 00:46:39,040
another tix no no no it's a micro second

00:46:32,680 --> 00:46:42,310
it's a microsecond ND i just manners the

00:46:39,040 --> 00:46:48,120
previous value by the current value so

00:46:42,310 --> 00:46:53,310
there is a latency in microseconds

00:46:48,120 --> 00:47:01,050
there's the judgment for the range if

00:46:53,310 --> 00:47:06,820
the latency within 1000 lineno is a 999

00:47:01,050 --> 00:47:11,710
microseconds just the record here for

00:47:06,820 --> 00:47:17,050
here I just do the moat calculation the

00:47:11,710 --> 00:47:22,420
curstyn the every item in aerie it's a

00:47:17,050 --> 00:47:25,210
it's a 10 micro second so I just mowed

00:47:22,420 --> 00:47:29,790
the value by turn so I can get the index

00:47:25,210 --> 00:47:32,170
number just the increase one that's all

00:47:29,790 --> 00:47:37,089
so that's a very fast

00:47:32,170 --> 00:47:40,030
there is this all calculation even there

00:47:37,089 --> 00:47:44,380
is a divided calculation but all the

00:47:40,030 --> 00:47:47,890
calculations are integer based and I

00:47:44,380 --> 00:47:51,549
just do a mode that's fast I mean

00:47:47,890 --> 00:48:00,940
comparing to the hard disk i/o that's

00:47:51,549 --> 00:48:06,069
very fast and when when we display the

00:48:00,940 --> 00:48:08,770
data only when we read the data from the

00:48:06,069 --> 00:48:12,220
fire we just display all the information

00:48:08,770 --> 00:48:16,690
from the data structure so most of time

00:48:12,220 --> 00:48:20,280
I mean maybe every five seconds or every

00:48:16,690 --> 00:48:26,619
five minutes this fire is a red ones so

00:48:20,280 --> 00:48:31,540
within within the interval just to

00:48:26,619 --> 00:48:37,230
increase each item by one that's all the

00:48:31,540 --> 00:48:37,230
operation so that's very fast

00:48:44,120 --> 00:48:46,780
please

00:48:50,400 --> 00:48:58,420
ok so you attack only hard disk or if

00:48:54,730 --> 00:49:00,820
you take if you have let's say SSD in

00:48:58,420 --> 00:49:03,880
your system you take if yours is the

00:49:00,820 --> 00:49:09,570
jepson problem this problem is happy

00:49:03,880 --> 00:49:12,970
natural in the SS this for SSD

00:49:09,570 --> 00:49:15,990
especially the high-speed SSD that's a

00:49:12,970 --> 00:49:15,990
different story

00:49:22,740 --> 00:49:24,800

YouTube URL: https://www.youtube.com/watch?v=fuhfKpjSOrA


