Title: Johannes Thumshin: Introduction to the Linux Block I O Layer
Publication date: 2016-06-26
Playlist: openSUSE Conference 2016
Description: 
	https://media.ccc.de/v/784-introduction-to-the-linux-block-i-o-layer

In the last years the kernel's block I/O layer has been subject to quite some substantial changes. Ever increasing IOPS rates delivered by Flash based storage and high end SAN infrastructures demanded a refactoring of the I/O layer towards a lockless, multicore utilizing design.

This talk will give a short introduction to both, the classic block layer and the so called multi queue block layer.

Johannes Thumshin
Captions: 
	00:00:15,780 --> 00:00:21,570
a talk my name is ernest wilson and i'm

00:00:19,320 --> 00:00:24,720
working with the mezuzah labs as a

00:00:21,570 --> 00:00:30,840
current developer for storage today's

00:00:24,720 --> 00:00:36,480
talk is some 5,000 feet overview of the

00:00:30,840 --> 00:00:39,300
Linux block IO layer small agenda we'll

00:00:36,480 --> 00:00:42,020
start bit with the Iowa architecture the

00:00:39,300 --> 00:00:45,300
architecture of the the whole IO stack

00:00:42,020 --> 00:00:49,710
then the scalability issues that came

00:00:45,300 --> 00:00:52,199
from this architecture I explain the

00:00:49,710 --> 00:00:54,960
principle between the old single cue

00:00:52,199 --> 00:00:59,629
block layer and the new fancy multi cue

00:00:54,960 --> 00:01:02,640
block layer and one common misconception

00:00:59,629 --> 00:01:05,370
between the difference between multiple

00:01:02,640 --> 00:01:08,520
cues and what's actually the cute the

00:01:05,370 --> 00:01:16,440
depth of a cue with Scotty T secure or

00:01:08,520 --> 00:01:20,479
serial ata and secure o first very very

00:01:16,440 --> 00:01:23,939
simplified diagram of the aisle stack

00:01:20,479 --> 00:01:26,790
your application does a write into the

00:01:23,939 --> 00:01:28,920
virtual file system or maybe just an

00:01:26,790 --> 00:01:32,509
nmap and write directly into the buffer

00:01:28,920 --> 00:01:36,600
cache this then goes to the filesystem

00:01:32,509 --> 00:01:41,820
to the block layer and either directly

00:01:36,600 --> 00:01:44,579
to a device driver or to the scuzzy mid

00:01:41,820 --> 00:01:47,219
layer from the sky C mid layer than to

00:01:44,579 --> 00:01:50,840
the sky C driver or to your serial ata

00:01:47,219 --> 00:01:55,320
layer and from this year ata layer to

00:01:50,840 --> 00:02:02,540
the driver and then eventually it's some

00:01:55,320 --> 00:02:07,250
when hit a disk so

00:02:02,540 --> 00:02:09,830
every requests will get into a per

00:02:07,250 --> 00:02:13,450
process blacklist that has an energy

00:02:09,830 --> 00:02:17,330
analogy of a kitchen sink or a bathtub

00:02:13,450 --> 00:02:21,290
we're all requests are all laters filled

00:02:17,330 --> 00:02:24,080
in and to a specific level and then the

00:02:21,290 --> 00:02:29,090
plaque is pulled and the request is

00:02:24,080 --> 00:02:32,870
drained it goes to an i/o scheduler and

00:02:29,090 --> 00:02:35,840
currently we have three of them first

00:02:32,870 --> 00:02:38,630
very basic i/o schedulers and no op it's

00:02:35,840 --> 00:02:44,390
basically just a five for just nothing

00:02:38,630 --> 00:02:46,790
much just click out the requests the

00:02:44,390 --> 00:02:51,500
next possible i/o scheduler would be a

00:02:46,790 --> 00:02:54,830
deadline scheduler that prefers reads

00:02:51,500 --> 00:02:57,980
over writes because usually read as a

00:02:54,830 --> 00:03:02,540
synchronous action whereas the right as

00:02:57,980 --> 00:03:05,570
an asynchronous action and when a

00:03:02,540 --> 00:03:08,420
specific deadline is reached or there

00:03:05,570 --> 00:03:11,600
are too many requests cute it sends it

00:03:08,420 --> 00:03:14,780
out or then there's this highly

00:03:11,600 --> 00:03:18,590
sophisticated completely fair queuing I

00:03:14,780 --> 00:03:28,580
all scheduled err that does Iowa

00:03:18,590 --> 00:03:31,820
counting a lot of merging etc and then

00:03:28,580 --> 00:03:40,370
goes all to dispatch queue and then to

00:03:31,820 --> 00:03:44,230
be processed by the lower levels yep

00:03:40,370 --> 00:03:47,530
there some scalability issues arose

00:03:44,230 --> 00:03:51,650
because nowadays you don't have one

00:03:47,530 --> 00:03:56,680
extremely fast CPU anymore but a lot of

00:03:51,650 --> 00:04:00,170
CPUs that are yeah somewhat fast and

00:03:56,680 --> 00:04:03,290
that they all can run parallel

00:04:00,170 --> 00:04:06,200
applications in parallel that hit in

00:04:03,290 --> 00:04:10,299
parallel the file system go into the

00:04:06,200 --> 00:04:14,989
block layer and then have to be cute and

00:04:10,299 --> 00:04:16,489
to enqueue it you have to take a look so

00:04:14,989 --> 00:04:19,009
in this lock content

00:04:16,489 --> 00:04:22,160
a lot and then there's one single cue

00:04:19,009 --> 00:04:24,849
down to the drivers and you had a lot of

00:04:22,160 --> 00:04:27,800
luck contention and lost a lot of time

00:04:24,849 --> 00:04:30,830
this was okay and the times with

00:04:27,800 --> 00:04:33,919
rotating discs where a seat costed some

00:04:30,830 --> 00:04:37,340
milliseconds this cage burned some

00:04:33,919 --> 00:04:42,229
thousand CPU cycles because hardware is

00:04:37,340 --> 00:04:45,580
slow but nowadays with SSD is especially

00:04:42,229 --> 00:04:49,729
nvme discs this isn't the case anymore

00:04:45,580 --> 00:04:55,069
so there had to be done something to get

00:04:49,729 --> 00:04:59,919
a way of this lock contention this is

00:04:55,069 --> 00:05:04,970
when the multi cue block layer came up

00:04:59,919 --> 00:05:09,560
it has not one per system cue anymore

00:05:04,970 --> 00:05:13,220
but one per CPU could queue so you can

00:05:09,560 --> 00:05:15,860
submit all your i/o and parallel have

00:05:13,220 --> 00:05:17,960
nearly no global state anymore over the

00:05:15,860 --> 00:05:21,580
i/o there's no more lock contention

00:05:17,960 --> 00:05:24,259
all's fine and dandy

00:05:21,580 --> 00:05:26,990
these software queues then get mapped to

00:05:24,259 --> 00:05:30,050
Hardware queues because some new

00:05:26,990 --> 00:05:33,229
hardware like nvme some fiber channel

00:05:30,050 --> 00:05:37,789
adapters actually expose multiple queues

00:05:33,229 --> 00:05:40,099
to the host OS so you can queue into the

00:05:37,789 --> 00:05:42,469
hardware in parallel like some network

00:05:40,099 --> 00:05:48,969
drivers some network devices to that as

00:05:42,469 --> 00:05:52,009
well and every IO that is dispatched

00:05:48,969 --> 00:05:56,860
triggers an interrupt someone when the

00:05:52,009 --> 00:06:00,620
i/o is done and these aisles get handled

00:05:56,860 --> 00:06:02,750
hopefully on the CPUs that did the

00:06:00,620 --> 00:06:07,969
submission so everything is still cache

00:06:02,750 --> 00:06:11,180
hot and there's a lot of reaction

00:06:07,969 --> 00:06:15,259
between inter-process our interrupts so

00:06:11,180 --> 00:06:21,469
the memory is it's still on the node but

00:06:15,259 --> 00:06:23,830
the memory of this request was quite a

00:06:21,469 --> 00:06:23,830
first

00:06:25,090 --> 00:06:29,950
but there's a drawback

00:06:26,620 --> 00:06:34,840
we have no Iowa scheduler currently in

00:06:29,950 --> 00:06:39,480
this model so with slow disks like real

00:06:34,840 --> 00:06:43,810
rotating rust there is no scheduler

00:06:39,480 --> 00:06:51,010
there's nothing that stops the i/o from

00:06:43,810 --> 00:06:54,370
flooding out to to disk 6 etc so it

00:06:51,010 --> 00:06:56,740
might not be the best way for every kind

00:06:54,370 --> 00:06:59,139
of i/o for every system that's the

00:06:56,740 --> 00:07:02,440
reason why you can turn it on or off

00:06:59,139 --> 00:07:07,680
we're the kernel command line and scre

00:07:02,440 --> 00:07:09,820
see we have this gassy mq use block mq

00:07:07,680 --> 00:07:14,370
kernel Kamat line parameter you can

00:07:09,820 --> 00:07:22,600
switch to yes or no depending on what

00:07:14,370 --> 00:07:25,450
hospice adaptors you use and for common

00:07:22,600 --> 00:07:29,220
misconception between what's actually

00:07:25,450 --> 00:07:34,210
multi cue what single queue what's this

00:07:29,220 --> 00:07:38,650
ncq that the serial ata in HCI disks F

00:07:34,210 --> 00:07:42,880
and the classical single queue model you

00:07:38,650 --> 00:07:45,490
had one request pickup dispatched to the

00:07:42,880 --> 00:07:48,789
drive with the drive is done writing

00:07:45,490 --> 00:07:51,370
this i/o triggers an interrupt it's

00:07:48,789 --> 00:07:57,220
acknowledged the next IO can be

00:07:51,370 --> 00:08:00,520
dispatched in the tcq ncq model you can

00:07:57,220 --> 00:08:05,889
just fetch multiple requests to the

00:08:00,520 --> 00:08:10,240
drive serial ata and secure has 31

00:08:05,889 --> 00:08:14,080
requests you can cue every request gets

00:08:10,240 --> 00:08:18,990
attack like a unique cookie to identify

00:08:14,080 --> 00:08:24,789
the tag the drives generate an interrupt

00:08:18,990 --> 00:08:27,250
when the i/o is done and then the the

00:08:24,789 --> 00:08:30,220
block layer can look up the tag which

00:08:27,250 --> 00:08:36,760
tags completed the drive can reorder all

00:08:30,220 --> 00:08:40,389
the requests so to its own convenience

00:08:36,760 --> 00:08:42,760
so there's no guarantee that if you ride

00:08:40,389 --> 00:08:44,529
one two three four that the interrupts

00:08:42,760 --> 00:08:49,019
come in the sequence of one two three

00:08:44,529 --> 00:08:53,190
four five but maybe four to three one

00:08:49,019 --> 00:09:04,019
depending on the seek of the drives etc

00:08:53,190 --> 00:09:08,680
and then in the multi cue setup you have

00:09:04,019 --> 00:09:13,720
yeah multiple cues that have a specific

00:09:08,680 --> 00:09:16,410
depth like the enemy drives specify the

00:09:13,720 --> 00:09:21,579
the enemy's back says there can be up to

00:09:16,410 --> 00:09:27,130
64 K cues in parallel and each cue can

00:09:21,579 --> 00:09:30,850
be up to 64 can have up to 64 K slots so

00:09:27,130 --> 00:09:35,800
you have 64 K times 64 K possible

00:09:30,850 --> 00:09:41,019
requests pending and each of these cues

00:09:35,800 --> 00:09:44,230
can be assigned to a CPU or an

00:09:41,019 --> 00:09:47,680
application or whatever the

00:09:44,230 --> 00:09:53,529
administrator likes usually it's um it's

00:09:47,680 --> 00:09:58,500
assigned to a CPU so if you have IO from

00:09:53,529 --> 00:10:04,810
72 CPUs on a server running down on your

00:09:58,500 --> 00:10:08,829
nvme drive then they should all fly in

00:10:04,810 --> 00:10:11,170
parallel until they content on your PCI

00:10:08,829 --> 00:10:16,449
Express bus because you don't have 72

00:10:11,170 --> 00:10:20,230
lanes there for it tagging

00:10:16,449 --> 00:10:24,459
I know how many people if you know the

00:10:20,230 --> 00:10:28,420
concept of tag DHCP on some like kiosk

00:10:24,459 --> 00:10:30,550
in future camp and the old days the HTTP

00:10:28,420 --> 00:10:32,920
was very unreliable and they invented

00:10:30,550 --> 00:10:37,480
something called pack th severe well

00:10:32,920 --> 00:10:39,279
they use tax and wrote your host part of

00:10:37,480 --> 00:10:43,360
your IP address on it so you can stick

00:10:39,279 --> 00:10:45,029
it on your cables to know which IP

00:10:43,360 --> 00:10:49,630
address you had on the apeman's new

00:10:45,029 --> 00:10:53,470
which the HTTP lease was out

00:10:49,630 --> 00:10:56,200
this is the that's the same concept and

00:10:53,470 --> 00:11:00,460
with the request tagging we just have an

00:10:56,200 --> 00:11:01,600
address for each request we stick a tag

00:11:00,460 --> 00:11:05,790
do it to it

00:11:01,600 --> 00:11:10,360
the drive completes tells us yeah

00:11:05,790 --> 00:11:12,190
request number five got completed so the

00:11:10,360 --> 00:11:14,920
block layer and the drivers know okay

00:11:12,190 --> 00:11:18,010
this request got completed don't have to

00:11:14,920 --> 00:11:20,860
retry it we can tell the filesystem okay

00:11:18,010 --> 00:11:29,410
it's written down it's written out the

00:11:20,860 --> 00:11:32,730
data should be safe now

00:11:29,410 --> 00:11:35,980
not all hardware has multiple cues or

00:11:32,730 --> 00:11:41,140
not all Hardware has as much cues as

00:11:35,980 --> 00:11:47,080
your you have CPUs like HCI has one cue

00:11:41,140 --> 00:11:51,580
but it is 31 slot Steve and your common

00:11:47,080 --> 00:11:54,520
notebook has like four or eight CPUs so

00:11:51,580 --> 00:11:58,750
you still can use your multi cue block

00:11:54,520 --> 00:12:02,680
layer goes down and to the low-level

00:11:58,750 --> 00:12:06,580
device driver and parallel then gets

00:12:02,680 --> 00:12:10,810
mapped to the hardware cues it's not

00:12:06,580 --> 00:12:13,050
that interesting with yeah HCI with only

00:12:10,810 --> 00:12:18,160
one cue but it's kinda interesting with

00:12:13,050 --> 00:12:22,390
low-end nvme drives that maybe a four or

00:12:18,160 --> 00:12:25,780
eight Hardware cues but you on the test

00:12:22,390 --> 00:12:31,000
of system have maybe sixteen CPUs eight

00:12:25,780 --> 00:12:35,890
Hardware cues on your drive so there's

00:12:31,000 --> 00:12:38,170
an easier way of mapping your requests

00:12:35,890 --> 00:12:47,790
down from the software cues to the

00:12:38,170 --> 00:12:47,790
artwork use yeah and I was way too fast

00:12:48,450 --> 00:12:55,470
so this more time for questions any

00:12:52,990 --> 00:12:55,470
questions

00:12:59,200 --> 00:13:15,760
no there's been an ongoing debate in the

00:13:12,730 --> 00:13:19,300
internet about the comparison of the

00:13:15,760 --> 00:13:21,610
Windows NT kernel io until Linux kernel

00:13:19,300 --> 00:13:24,000
i/o and many argued that the

00:13:21,610 --> 00:13:28,030
asynchronous i/o implementation of the

00:13:24,000 --> 00:13:30,580
NT kernel is much better especially the

00:13:28,030 --> 00:13:33,040
joke loud overlapping i/o they can use

00:13:30,580 --> 00:13:38,380
their what's your point of view on that

00:13:33,040 --> 00:13:42,820
I unfortunately have no idea what's over

00:13:38,380 --> 00:13:45,310
over the Windows NT kernel sorry not my

00:13:42,820 --> 00:13:55,810
business closed source OS can't look

00:13:45,310 --> 00:13:58,090
into it but only maybe knows one

00:13:55,810 --> 00:14:00,970
question if you have a software device

00:13:58,090 --> 00:14:03,340
like a rate array or caching solution

00:14:00,970 --> 00:14:05,350
where is the queue for the hardware

00:14:03,340 --> 00:14:09,130
device or is there one for the software

00:14:05,350 --> 00:14:13,150
device your rate arrays are above the

00:14:09,130 --> 00:14:16,000
block layer there biobased so they're

00:14:13,150 --> 00:14:22,600
not request you should so their request

00:14:16,000 --> 00:14:26,320
paste no its software solution not the

00:14:22,600 --> 00:14:29,500
hardware raid arrays MD rate or diem

00:14:26,320 --> 00:14:32,830
rate they're bio-based that's above the

00:14:29,500 --> 00:14:36,190
block layer that's directly these are

00:14:32,830 --> 00:14:38,400
block devices but they get the piles and

00:14:36,190 --> 00:14:38,400
then

00:14:48,160 --> 00:14:53,860
this how is the IO queueing a few a

00:14:51,910 --> 00:14:56,230
sample different hardware devices

00:14:53,860 --> 00:14:58,779
fulfill new software 8 for example local

00:14:56,230 --> 00:15:03,459
disk and the I scuzzy disk how is the

00:14:58,779 --> 00:15:06,910
skewing done then yeah on the low levels

00:15:03,459 --> 00:15:11,019
these are for the lower level drivers

00:15:06,910 --> 00:15:12,879
these are just the this low level

00:15:11,019 --> 00:15:19,779
drivers don't know if they are in the

00:15:12,879 --> 00:15:23,110
rate array on your cereal is ta I scuzzy

00:15:19,779 --> 00:15:26,589
whatever scuzzy driver you don't know

00:15:23,110 --> 00:15:29,470
what the upper layers did so if you have

00:15:26,589 --> 00:15:31,870
a rate there are multiple requests

00:15:29,470 --> 00:15:36,550
dispatched every request is dispatched

00:15:31,870 --> 00:15:41,470
in parallel to the lower level drivers

00:15:36,550 --> 00:15:44,620
and these then dispatch it so the they

00:15:41,470 --> 00:15:46,420
have no idea what's over what's going on

00:15:44,620 --> 00:15:49,959
in the upper layers of the stack and

00:15:46,420 --> 00:15:54,459
they don't need to because the upper

00:15:49,959 --> 00:15:59,620
layers of the stack have been completely

00:15:54,459 --> 00:16:02,079
parallel since the dawn of time or since

00:15:59,620 --> 00:16:35,020
the abandonment of the the kernel lock

00:16:02,079 --> 00:16:39,160
at least on my quest

00:16:35,020 --> 00:16:39,160
that think we are done

00:16:40,190 --> 00:16:42,250
you

00:16:52,579 --> 00:16:54,639

YouTube URL: https://www.youtube.com/watch?v=ebHINEF6PDk


