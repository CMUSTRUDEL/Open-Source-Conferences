Title: openSUSE Conference 2017 The importance of performance testing in the NFV world
Publication date: 2017-05-28
Playlist: openSUSE Conference 2017
Description: 
	https://media.ccc.de/v/1436-the-importance-of-performance-testing-in-the-nfv-world

Challenges and state-of-the-art in the OPNFV community

Performance testing is very important in various different deployments and scenarios however in the NFV (Network Function Virtualization) it is even more meaningful. With a growing umbrella of projects and hence infrastructure NFV and SDN represent a very complex environment to test and, as a result, to give the correct interpretation to figures gathered through performance testing. Is my result good or bad? And how much better could it be and why? These are just few of the questions which people involved in performance testing ask themselves regularly. The OPNFV community is focused on creating a framework (made of multiple projects) which can help with performance gathering but not yet with analytics. 
This talk will provide an insight on performance testing for NFV, the state-of-the-art in the OPNFV community and a short demo running on an openSUSE distribution.



Marco Varlese
Captions: 
	00:00:07,760 --> 00:00:17,810
hi everyone it's me again this talk is

00:00:13,230 --> 00:00:24,140
about the importance of performance

00:00:17,810 --> 00:00:27,029
testing in a devii the challenges and

00:00:24,140 --> 00:00:30,840
where the open source communities ads

00:00:27,029 --> 00:00:32,669
these days so we're going through a

00:00:30,840 --> 00:00:36,170
little bit of the basics of performance

00:00:32,669 --> 00:00:40,219
testing really briefly projects in the

00:00:36,170 --> 00:00:44,129
open IV area around performance testing

00:00:40,219 --> 00:00:46,980
I will talk a little bit more into

00:00:44,129 --> 00:00:49,800
details of the vs perf architecture

00:00:46,980 --> 00:00:52,230
which is also the tool I'm going to show

00:00:49,800 --> 00:00:56,100
you with the demo

00:00:52,230 --> 00:01:03,229
some results that are we gathered and

00:00:56,100 --> 00:01:06,409
the demo at the end so in n MV

00:01:03,229 --> 00:01:11,729
performance are a crucial requirement

00:01:06,409 --> 00:01:15,479
for the nav offering the fact that

00:01:11,729 --> 00:01:21,500
something works is no longer the problem

00:01:15,479 --> 00:01:26,100
or the only problem and the the thing is

00:01:21,500 --> 00:01:28,740
performance are really those things that

00:01:26,100 --> 00:01:31,530
requirement that is also driving really

00:01:28,740 --> 00:01:35,490
really hard the choice of of both

00:01:31,530 --> 00:01:39,479
software and hardware it's such an

00:01:35,490 --> 00:01:45,729
important aspect of nav that

00:01:39,479 --> 00:01:48,670
entities like Etsy and the ID ietf have

00:01:45,729 --> 00:01:51,549
dedicated quite a broad range of

00:01:48,670 --> 00:01:56,439
documents and dropped around this topic

00:01:51,549 --> 00:01:58,560
so you may appreciate how important that

00:01:56,439 --> 00:01:58,560
is

00:01:58,860 --> 00:02:06,579
the so I set up to benchmark heavy

00:02:03,460 --> 00:02:09,429
workloads helps to tune hardware and

00:02:06,579 --> 00:02:11,290
software parameters to increase

00:02:09,429 --> 00:02:15,880
throughput latency scalability

00:02:11,290 --> 00:02:21,880
scalability and obviously yes to reduce

00:02:15,880 --> 00:02:24,190
the latency not increase it obviously an

00:02:21,880 --> 00:02:28,319
automated the infrastructure to run

00:02:24,190 --> 00:02:29,769
performance benchmarking on na VI is

00:02:28,319 --> 00:02:34,269
fundamentally is of fundamental

00:02:29,769 --> 00:02:37,120
importance to support for example

00:02:34,269 --> 00:02:43,810
regression in any of the sub components

00:02:37,120 --> 00:02:46,900
used so what is the challenges that we

00:02:43,810 --> 00:02:50,019
see in the nav with regards to

00:02:46,900 --> 00:02:52,780
performance well they're both Hardware

00:02:50,019 --> 00:02:57,250
related and software related so from

00:02:52,780 --> 00:03:00,790
hardware setup perspective you will face

00:02:57,250 --> 00:03:05,109
the choice of for example do I enable

00:03:00,790 --> 00:03:07,590
hyper threading or not what am I going

00:03:05,109 --> 00:03:10,480
to do with power management

00:03:07,590 --> 00:03:14,139
what about huge pages

00:03:10,480 --> 00:03:16,840
what about Silv what about iommu what

00:03:14,139 --> 00:03:19,989
about all the NIC offloads that these

00:03:16,840 --> 00:03:22,750
days I have on very expensive NICs that

00:03:19,989 --> 00:03:26,470
very likely are part of the machine I

00:03:22,750 --> 00:03:29,620
have in my data center and from a

00:03:26,470 --> 00:03:31,840
software design perspective there are

00:03:29,620 --> 00:03:36,250
obviously different choices that we can

00:03:31,840 --> 00:03:38,100
make are we going to Paul packets or are

00:03:36,250 --> 00:03:40,030
we going to use an interrupt driven

00:03:38,100 --> 00:03:42,780
approach

00:03:40,030 --> 00:03:46,540
what about threads am I going to

00:03:42,780 --> 00:03:48,430
affinity thread specific course or leave

00:03:46,540 --> 00:03:50,290
it up to the us to share your threads

00:03:48,430 --> 00:03:55,689
wherever it's

00:03:50,290 --> 00:03:58,239
best for the US am I going to use memory

00:03:55,689 --> 00:04:00,840
location techniques like memory pools

00:03:58,239 --> 00:04:04,360
memory buffers memory management

00:04:00,840 --> 00:04:06,700
infrastructure layers to manage my

00:04:04,360 --> 00:04:12,370
memory or I am going to stick with the

00:04:06,700 --> 00:04:15,760
usual malloc what am I gonna do and what

00:04:12,370 --> 00:04:19,269
about Hardware specific architecture

00:04:15,760 --> 00:04:21,430
specific intrinsics for example am I

00:04:19,269 --> 00:04:25,630
going to take advantage of the greatest

00:04:21,430 --> 00:04:30,699
and latest SSC for instructions or will

00:04:25,630 --> 00:04:33,580
I give it up so they're just few of the

00:04:30,699 --> 00:04:35,760
questions that you left to task yourself

00:04:33,580 --> 00:04:40,720
when you search you know designing

00:04:35,760 --> 00:04:43,120
software component in this nav world to

00:04:40,720 --> 00:04:45,580
embrace the performance requirements it

00:04:43,120 --> 00:04:50,020
also not to restrict yourself to just a

00:04:45,580 --> 00:04:53,620
specific niche so what are we testing

00:04:50,020 --> 00:04:56,260
well the the what we are testing is

00:04:53,620 --> 00:05:00,010
called sut which stands for system under

00:04:56,260 --> 00:05:02,740
test and there are different things that

00:05:00,010 --> 00:05:04,990
we can test and that we want to test so

00:05:02,740 --> 00:05:07,599
the the first of all the first thing is

00:05:04,990 --> 00:05:12,550
we need to distinguish between what the

00:05:07,599 --> 00:05:15,550
app vnf is versus what the NA VI is so

00:05:12,550 --> 00:05:20,590
the vnf is the virtual network function

00:05:15,550 --> 00:05:22,930
which is something that is a string the

00:05:20,590 --> 00:05:26,229
work is is basically handling a specific

00:05:22,930 --> 00:05:29,080
type of traffic so you can think of an

00:05:26,229 --> 00:05:33,729
application that is consuming traffic X

00:05:29,080 --> 00:05:35,979
or producing traffic X and the NFV is

00:05:33,729 --> 00:05:38,500
that is what I spoke just earlier about

00:05:35,979 --> 00:05:40,210
which is all those software components

00:05:38,500 --> 00:05:46,539
that are part of the stack the piece

00:05:40,210 --> 00:05:49,990
which the TPK dee-dee-dee-dee controller

00:05:46,539 --> 00:05:53,080
if you wish which part of that overall

00:05:49,990 --> 00:05:55,040
stock you want to test from a

00:05:53,080 --> 00:05:59,000
performance perspective

00:05:55,040 --> 00:06:01,190
some of them are really on the on the

00:05:59,000 --> 00:06:03,140
critical path of execution when when

00:06:01,190 --> 00:06:08,950
you're talking about networking others

00:06:03,140 --> 00:06:13,430
are not impacted so much then it's about

00:06:08,950 --> 00:06:14,990
how you test it so which traffic

00:06:13,430 --> 00:06:17,140
profiles are we going to choose as I

00:06:14,990 --> 00:06:19,850
said earlier there are different type of

00:06:17,140 --> 00:06:22,880
requirements depending on the industry

00:06:19,850 --> 00:06:26,090
that you're in so am I going to use

00:06:22,880 --> 00:06:28,430
small packets mid-range packets

00:06:26,090 --> 00:06:31,970
I mix type of products am I going to use

00:06:28,430 --> 00:06:35,840
jumbo frames at all on the network is it

00:06:31,970 --> 00:06:40,960
worth it is not worth it and then for

00:06:35,840 --> 00:06:44,840
how long my test scenarios will run on

00:06:40,960 --> 00:06:47,030
in my lab is it just like one shot for a

00:06:44,840 --> 00:06:50,960
few minutes and and that's it or am I

00:06:47,030 --> 00:06:54,430
going to run it for hours days when I'm

00:06:50,960 --> 00:06:59,120
going to say that it's good enough and

00:06:54,430 --> 00:07:01,610
is a burst of traffic going to impact my

00:06:59,120 --> 00:07:04,340
study versus a continuous type of

00:07:01,610 --> 00:07:06,110
traffic well depending on which choice

00:07:04,340 --> 00:07:08,510
you've made about your hardware and your

00:07:06,110 --> 00:07:13,730
software yes it will so you will have to

00:07:08,510 --> 00:07:16,610
test both to see where you are one thing

00:07:13,730 --> 00:07:24,920
is for sure we we have test objectives

00:07:16,610 --> 00:07:27,920
and it's a it's a almost needless to say

00:07:24,920 --> 00:07:31,640
but we want very high throughput from

00:07:27,920 --> 00:07:34,250
our system we want low latency we want a

00:07:31,640 --> 00:07:38,540
very high number of flows to be handled

00:07:34,250 --> 00:07:40,730
by our our platform the more flows we

00:07:38,540 --> 00:07:43,820
can handle the more things we can park

00:07:40,730 --> 00:07:46,640
on one single machine we want it to be

00:07:43,820 --> 00:07:49,550
with a high capacity which again goes

00:07:46,640 --> 00:07:52,700
very likely hand by hand with the high

00:07:49,550 --> 00:07:56,080
number of flows so we want to make the

00:07:52,700 --> 00:07:58,820
perfect use of our CPU of our memory

00:07:56,080 --> 00:08:00,360
make the most basically out of what we

00:07:58,820 --> 00:08:04,990
have

00:08:00,360 --> 00:08:08,379
and the last one is just like a wish to

00:08:04,990 --> 00:08:12,849
gain to get the so famous linear

00:08:08,379 --> 00:08:19,919
scalability which is just a wish and a

00:08:12,849 --> 00:08:23,680
dream so what about the test profiles

00:08:19,919 --> 00:08:26,139
well there are in the based on what I

00:08:23,680 --> 00:08:27,849
said the earlier talks we we are facing

00:08:26,139 --> 00:08:31,000
two different type of scenarios because

00:08:27,849 --> 00:08:34,750
of the V switch now we can have intra BM

00:08:31,000 --> 00:08:36,669
traffic and we can have inter machine

00:08:34,750 --> 00:08:39,760
traffic so we want to verify both we

00:08:36,669 --> 00:08:41,950
want to see how our machine behaves when

00:08:39,760 --> 00:08:43,659
we have east-west type of traffic and

00:08:41,950 --> 00:08:49,720
how it behaves when we have north-south

00:08:43,659 --> 00:08:52,829
traffic and these scenarios are the ones

00:08:49,720 --> 00:08:52,829
that we are actually going to verify

00:08:52,860 --> 00:09:00,250
last but not least we also have the

00:08:55,959 --> 00:09:04,149
different type of layers in the protocol

00:09:00,250 --> 00:09:06,820
stacks so the V switch as we said is

00:09:04,149 --> 00:09:09,910
usually doing an l2 type of work but

00:09:06,820 --> 00:09:11,920
what if I'm doing B X lon and what I'm

00:09:09,910 --> 00:09:14,170
about if I'm using overlays and

00:09:11,920 --> 00:09:17,769
underlays networks am I going to do L

00:09:14,170 --> 00:09:19,510
trees am I going to do L force and is it

00:09:17,769 --> 00:09:23,440
worth it is it not working well it

00:09:19,510 --> 00:09:25,930
depends really on what your use case is

00:09:23,440 --> 00:09:27,130
and what your what you're going to

00:09:25,930 --> 00:09:29,620
deploy in your production environment

00:09:27,130 --> 00:09:34,269
and that's that's what's driving it's

00:09:29,620 --> 00:09:36,760
driving your final test case and then

00:09:34,269 --> 00:09:38,860
obviously the number of flows which is

00:09:36,760 --> 00:09:41,620
also very important to to verify and

00:09:38,860 --> 00:09:44,470
test the scalability of the system is it

00:09:41,620 --> 00:09:46,680
just like one single flow is it few

00:09:44,470 --> 00:09:52,300
hundreds thousands millions of flows

00:09:46,680 --> 00:09:58,779
hitting the server or your switch base

00:09:52,300 --> 00:10:00,550
software in this case then we have to

00:09:58,779 --> 00:10:02,980
talk about the matrix that we want to

00:10:00,550 --> 00:10:06,550
collect in order to to come up with our

00:10:02,980 --> 00:10:08,529
report obviously we want to collect the

00:10:06,550 --> 00:10:10,840
throughput which is usually measured in

00:10:08,529 --> 00:10:13,540
million packets per second

00:10:10,840 --> 00:10:16,450
the latency these days it's a very

00:10:13,540 --> 00:10:18,940
common to get it in nanoseconds the

00:10:16,450 --> 00:10:21,730
flows as I said in number the SIP

00:10:18,940 --> 00:10:25,330
utilization which is taken as a percent

00:10:21,730 --> 00:10:28,230
of the usage or free depending on what's

00:10:25,330 --> 00:10:32,080
being chosen and the memory again as a

00:10:28,230 --> 00:10:39,730
percent of the total system memory on

00:10:32,080 --> 00:10:41,250
the system so because we we want to make

00:10:39,730 --> 00:10:43,750
sure that we have a reproducible

00:10:41,250 --> 00:10:48,250
environment there are few choices that

00:10:43,750 --> 00:10:51,670
we have to make even if they might mean

00:10:48,250 --> 00:10:52,680
less performance for our system the

00:10:51,670 --> 00:10:54,850
reason why I'm saying that is because

00:10:52,680 --> 00:10:56,700
there are few things that can boost your

00:10:54,850 --> 00:11:00,280
system to a much higher degree of

00:10:56,700 --> 00:11:03,580
performance but when you go into the

00:11:00,280 --> 00:11:04,690
study of performance you need to make

00:11:03,580 --> 00:11:07,570
sure that your environment is

00:11:04,690 --> 00:11:10,540
reproducible otherwise you will never

00:11:07,570 --> 00:11:12,520
know what you measured ones versus

00:11:10,540 --> 00:11:15,430
another time and it becomes very very

00:11:12,520 --> 00:11:17,500
difficult then come up with a rationale

00:11:15,430 --> 00:11:20,950
behind the tuning or the changing of a

00:11:17,500 --> 00:11:24,540
specific setting so because of that and

00:11:20,950 --> 00:11:29,230
because of the higher degree of

00:11:24,540 --> 00:11:31,510
uncertainty of the system the

00:11:29,230 --> 00:11:34,870
hyper-threading is usually disabled the

00:11:31,510 --> 00:11:37,030
power management gets disabled things

00:11:34,870 --> 00:11:42,220
like see states and P States they can be

00:11:37,030 --> 00:11:45,910
turned off what you usually do is to to

00:11:42,220 --> 00:11:47,800
tune the system by the user space

00:11:45,910 --> 00:11:49,390
governor on Linux so you set it to use a

00:11:47,800 --> 00:11:51,370
space governor you can change through

00:11:49,390 --> 00:11:53,770
the CPU frequency tool difference that

00:11:51,370 --> 00:11:55,420
you want to set it to you can set it to

00:11:53,770 --> 00:11:57,660
the maximum and just run it at that

00:11:55,420 --> 00:11:57,660
speed

00:11:58,250 --> 00:12:03,140
something has just because of very

00:12:00,890 --> 00:12:05,420
likely the tools that then you're going

00:12:03,140 --> 00:12:07,040
to use and the software components that

00:12:05,420 --> 00:12:09,440
are going to be deployed on the machine

00:12:07,040 --> 00:12:12,530
it's a it's very likely the huge pages

00:12:09,440 --> 00:12:13,850
to be enabled the iommu enabled in

00:12:12,530 --> 00:12:19,160
pass-through mode

00:12:13,850 --> 00:12:21,860
sro be enabled and wherever possible the

00:12:19,160 --> 00:12:23,570
NIC offloads so trying to get advantage

00:12:21,860 --> 00:12:30,170
of the arguer that we have as much as

00:12:23,570 --> 00:12:32,570
possible i think it's clear but if you

00:12:30,170 --> 00:12:36,710
think about all the stuff that i said so

00:12:32,570 --> 00:12:38,840
far what we want to test how to test it

00:12:36,710 --> 00:12:42,260
and what to get out of it

00:12:38,840 --> 00:12:44,750
you're talking about a matrix that is no

00:12:42,260 --> 00:12:46,940
longer a two by two or a four by four

00:12:44,750 --> 00:12:51,980
not even an eight by eight is a huge

00:12:46,940 --> 00:12:56,960
matrix and the way I like to think about

00:12:51,980 --> 00:13:01,430
it this is of at Apple made of which

00:12:56,960 --> 00:13:05,110
kernel am I using it's important in some

00:13:01,430 --> 00:13:10,220
cases to to identify and verify the

00:13:05,110 --> 00:13:11,930
goodness of real-time behaviors for for

00:13:10,220 --> 00:13:15,910
things like jitter and latency for

00:13:11,930 --> 00:13:18,560
specific workloads so I'm going to use a

00:13:15,910 --> 00:13:23,420
out-of-the-box kernel vanilla kernel and

00:13:18,560 --> 00:13:29,800
an RT version of it which sends for real

00:13:23,420 --> 00:13:34,130
time the packet sizes going from the

00:13:29,800 --> 00:13:37,220
smallest 64 bytes packet all above the

00:13:34,130 --> 00:13:41,410
jumbo frames and mixed in between with I

00:13:37,220 --> 00:13:45,040
mix traffic to verify this so you shall

00:13:41,410 --> 00:13:48,800
enterprise data center type of traffic

00:13:45,040 --> 00:13:51,980
multiple streams going from one to a

00:13:48,800 --> 00:13:56,780
thousand to a million to verify how my

00:13:51,980 --> 00:13:59,980
system scales the traffic types at 2 vs

00:13:56,780 --> 00:13:59,980
l 3 vs l 4

00:14:00,160 --> 00:14:05,139
and the scenarios as I said we have

00:14:03,220 --> 00:14:08,139
physical to physical machine we have

00:14:05,139 --> 00:14:11,680
physical to VM which then goes back to

00:14:08,139 --> 00:14:17,050
to physical and then we have in travail

00:14:11,680 --> 00:14:18,370
M which is the pppp scenario and for all

00:14:17,050 --> 00:14:20,410
of this we want to capture the

00:14:18,370 --> 00:14:28,779
throughput latency the CPU utilization

00:14:20,410 --> 00:14:31,930
memory utilization so why why is it so

00:14:28,779 --> 00:14:37,029
important to run the performance testing

00:14:31,930 --> 00:14:40,480
in the Navy because the architecture

00:14:37,029 --> 00:14:42,269
itself is made of many many different

00:14:40,480 --> 00:14:46,269
parts

00:14:42,269 --> 00:14:49,899
most of them interact with each other

00:14:46,269 --> 00:14:52,000
and they're all developed and maintained

00:14:49,899 --> 00:14:55,509
by different communities which means

00:14:52,000 --> 00:14:57,339
that each community may have specific

00:14:55,509 --> 00:14:59,649
functional testing and performance

00:14:57,339 --> 00:15:04,209
testing based on input that they might

00:14:59,649 --> 00:15:07,089
get from other stakeholders and that's

00:15:04,209 --> 00:15:09,639
also reason where that's that's where

00:15:07,089 --> 00:15:12,750
Opie nav as an open source project comes

00:15:09,639 --> 00:15:16,089
in trying to basically consolidate and

00:15:12,750 --> 00:15:18,910
harmonize the different components and

00:15:16,089 --> 00:15:23,199
making sure that use cases and test

00:15:18,910 --> 00:15:28,269
cases can be defined and verified as a

00:15:23,199 --> 00:15:30,639
whole and as I say that's well the de

00:15:28,269 --> 00:15:34,509
suite the superset of the test cases is

00:15:30,639 --> 00:15:37,569
huge and in this case because of all the

00:15:34,509 --> 00:15:41,649
data that is being collected automation

00:15:37,569 --> 00:15:43,720
helps really a lot because it takes a

00:15:41,649 --> 00:15:46,269
lot of time just to run the test and

00:15:43,720 --> 00:15:50,709
then you left to process the results and

00:15:46,269 --> 00:15:54,009
make something out of it so in in this

00:15:50,709 --> 00:15:58,540
regards as I said Opie nav which is the

00:15:54,009 --> 00:16:02,139
open source project for nfe has two

00:15:58,540 --> 00:16:04,899
different projects one is called vs

00:16:02,139 --> 00:16:06,300
pervs vs perf and the other one is

00:16:04,899 --> 00:16:10,230
called the are sick

00:16:06,300 --> 00:16:13,500
so vs perv provides an automated test

00:16:10,230 --> 00:16:16,260
framework and also comprehensive test

00:16:13,500 --> 00:16:20,130
suite which is based on industry

00:16:16,260 --> 00:16:23,700
standards and any zip and is basically

00:16:20,130 --> 00:16:31,140
used to measure data plane performance

00:16:23,700 --> 00:16:34,140
of telco and a/v switching we in the lab

00:16:31,140 --> 00:16:35,700
we have a we have set up using vs perf

00:16:34,140 --> 00:16:40,460
and actually I'm going to show you just

00:16:35,700 --> 00:16:40,460
later how it looks like and what it does

00:16:40,860 --> 00:16:47,280
the nice thing of vs purpose that is

00:16:43,470 --> 00:16:49,760
traffic generator agnostic in fact it

00:16:47,280 --> 00:16:52,650
it supports four or five different

00:16:49,760 --> 00:16:53,660
traffic generators talking about Axius

00:16:52,650 --> 00:16:57,540
Pyland

00:16:53,660 --> 00:17:00,630
and few others and also has support for

00:16:57,540 --> 00:17:02,880
munjin which is a lower base traffic

00:17:00,630 --> 00:17:08,240
generator which is the one that I'm

00:17:02,880 --> 00:17:11,459
actually using currently it's a port

00:17:08,240 --> 00:17:14,850
open piece which it supports open V

00:17:11,459 --> 00:17:19,860
switch DP D K and B P P as V switches to

00:17:14,850 --> 00:17:22,560
be tested and it has all the nice things

00:17:19,860 --> 00:17:24,450
that I already mentioned before so it

00:17:22,560 --> 00:17:27,510
allows me to change packet sizes number

00:17:24,450 --> 00:17:34,560
of flows which type of traffic I want to

00:17:27,510 --> 00:17:36,660
to play with and what we did as part of

00:17:34,560 --> 00:17:41,090
the community with Sousa we've basically

00:17:36,660 --> 00:17:46,230
took this project we had support for

00:17:41,090 --> 00:17:49,380
openSUSE and we start contributing to

00:17:46,230 --> 00:17:52,950
features like multiple flows and latency

00:17:49,380 --> 00:17:55,920
to be collected via the use case using

00:17:52,950 --> 00:17:57,210
moon gem this is all available up

00:17:55,920 --> 00:18:00,330
streams all this work has been done

00:17:57,210 --> 00:18:06,549
upstream and can be found on the PS

00:18:00,330 --> 00:18:09,419
perky tub of Opie nav the other

00:18:06,549 --> 00:18:14,980
the other project called the arsenic

00:18:09,419 --> 00:18:18,220
instead is is aimed to to to test the

00:18:14,980 --> 00:18:22,270
infrastructure compliance of DNF

00:18:18,220 --> 00:18:25,200
applications so while these BS per

00:18:22,270 --> 00:18:28,679
focuses exactly on the B's which part

00:18:25,200 --> 00:18:33,610
yaar sig tries to identify performance

00:18:28,679 --> 00:18:37,929
issues or gaps from a vnf perspective in

00:18:33,610 --> 00:18:39,910
fact if you if you if you look for it on

00:18:37,929 --> 00:18:44,679
the internet you'll see that a lot of

00:18:39,910 --> 00:18:46,510
the tests run by yardstick they use

00:18:44,679 --> 00:18:51,549
things like the ping

00:18:46,510 --> 00:18:54,150
net perf they use top command to get

00:18:51,549 --> 00:18:58,470
specific type of things on different VMs

00:18:54,150 --> 00:19:02,740
but it's all targeted to the vnf word

00:18:58,470 --> 00:19:05,950
from that perspective we are not so much

00:19:02,740 --> 00:19:08,650
interested in focusing on this specific

00:19:05,950 --> 00:19:13,510
type of project because it's more for

00:19:08,650 --> 00:19:16,150
BNF vendors that might use this as a

00:19:13,510 --> 00:19:23,080
sort of performance and functional

00:19:16,150 --> 00:19:26,950
testing still what yardstick does is to

00:19:23,080 --> 00:19:28,900
use OpenStack to basically deploy the

00:19:26,950 --> 00:19:31,570
VMS and deploy all the environment

00:19:28,900 --> 00:19:35,500
that's being used for running a specific

00:19:31,570 --> 00:19:38,740
test case and what it does very nicely

00:19:35,500 --> 00:19:41,890
that allows you to have different type

00:19:38,740 --> 00:19:44,620
of backends to collect your data one of

00:19:41,890 --> 00:19:48,070
the backend to be used that can be used

00:19:44,620 --> 00:19:50,340
is in flux DB which obviously can also

00:19:48,070 --> 00:19:52,470
be linked to graph on a-- and

00:19:50,340 --> 00:19:56,700
automatically you get your plotted

00:19:52,470 --> 00:19:59,440
charts of a specific workloads that

00:19:56,700 --> 00:20:02,140
works pretty nicely

00:19:59,440 --> 00:20:05,560
when I approach this project there was

00:20:02,140 --> 00:20:07,810
no support for for Suzi distribution so

00:20:05,560 --> 00:20:09,760
it was quite a bit of work to be done

00:20:07,810 --> 00:20:12,880
there in terms of hiding the support for

00:20:09,760 --> 00:20:15,070
it and fixing all the documentation

00:20:12,880 --> 00:20:17,740
piece and bits and pieces and again

00:20:15,070 --> 00:20:22,660
that's being working upstream so it's

00:20:17,740 --> 00:20:27,250
available upstream to be used on honest

00:20:22,660 --> 00:20:30,520
with the distribution so as I said I'm

00:20:27,250 --> 00:20:34,000
going to focus on vs perf mainly for

00:20:30,520 --> 00:20:37,030
this how does it look like the system

00:20:34,000 --> 00:20:38,830
architecture well first of all the the

00:20:37,030 --> 00:20:42,340
traffic generator that you see on the

00:20:38,830 --> 00:20:48,160
left hand side in my case that's a

00:20:42,340 --> 00:20:50,440
server machine for whoever has a very

00:20:48,160 --> 00:20:53,470
expensive traffic generator in the lab

00:20:50,440 --> 00:20:58,210
like XE or Spirent they could be one of

00:20:53,470 --> 00:21:00,910
those machines the concept anyway is the

00:20:58,210 --> 00:21:03,040
same whether you using a proprietary

00:21:00,910 --> 00:21:04,780
traffic generator or an open-source

00:21:03,040 --> 00:21:07,510
traffic generator with my case was

00:21:04,780 --> 00:21:11,410
munjin you'll have to connect

00:21:07,510 --> 00:21:13,600
back-to-back two machines so you have

00:21:11,410 --> 00:21:15,150
two ports on one machine two ports on

00:21:13,600 --> 00:21:18,190
the other machine and you basically

00:21:15,150 --> 00:21:22,120
interconnect the servers using this four

00:21:18,190 --> 00:21:24,670
different parts one port would send the

00:21:22,120 --> 00:21:28,000
traffic to the other machine the machine

00:21:24,670 --> 00:21:30,940
would route the frame on the port of the

00:21:28,000 --> 00:21:36,850
same server and just send it out to the

00:21:30,940 --> 00:21:39,400
other port of the other machine from a

00:21:36,850 --> 00:21:45,520
node P nav infrastructure perspective

00:21:39,400 --> 00:21:47,590
set specifically to vs perf we have the

00:21:45,520 --> 00:21:52,180
traffic generator which is moongeun as I

00:21:47,590 --> 00:21:53,560
said and the traffic generator can speak

00:21:52,180 --> 00:21:56,080
to the vs perf

00:21:53,560 --> 00:21:56,970
running on the sut which is the system

00:21:56,080 --> 00:22:01,060
under test

00:21:56,970 --> 00:22:04,450
so is able to basically send back the

00:22:01,060 --> 00:22:07,780
data collected information collected and

00:22:04,450 --> 00:22:09,990
can also be configured by vs perf to run

00:22:07,780 --> 00:22:13,120
a specific type of that

00:22:09,990 --> 00:22:14,110
and I'll show you tin in practice what

00:22:13,120 --> 00:22:17,200
what's happening

00:22:14,110 --> 00:22:22,000
but basically vs perf uses his own

00:22:17,200 --> 00:22:25,570
configuration files for traffic for

00:22:22,000 --> 00:22:28,360
which piece which to be used if you're

00:22:25,570 --> 00:22:29,799
using a specific vnf or not all this

00:22:28,360 --> 00:22:32,860
config all this configuration is

00:22:29,799 --> 00:22:35,890
collected is packed in a config file

00:22:32,860 --> 00:22:41,650
which is shipped to the traffic

00:22:35,890 --> 00:22:42,970
generator and is under the form of a lua

00:22:41,650 --> 00:22:45,030
configuration file

00:22:42,970 --> 00:22:48,370
that's because moonshine is based on Lua

00:22:45,030 --> 00:22:51,280
copied in a specific folder where the

00:22:48,370 --> 00:22:56,470
traffic generator picks it up and starts

00:22:51,280 --> 00:22:59,679
running the flows so just briefly

00:22:56,470 --> 00:23:01,150
because I said which scenarios are

00:22:59,679 --> 00:23:03,760
important to us well the first one is

00:23:01,150 --> 00:23:07,870
the physical to physical in order to

00:23:03,760 --> 00:23:10,750
know what is the capability of your

00:23:07,870 --> 00:23:12,610
system the first thing to do is to not

00:23:10,750 --> 00:23:15,400
think about all the virtualization and

00:23:12,610 --> 00:23:17,080
VMs I can run on your machine all you

00:23:15,400 --> 00:23:20,260
want to do is to really have a shortcut

00:23:17,080 --> 00:23:22,030
between the port where the frames are

00:23:20,260 --> 00:23:26,080
received to the port where the frames

00:23:22,030 --> 00:23:30,010
are sent this is the shortest path for a

00:23:26,080 --> 00:23:35,590
frame to be going from ports ports

00:23:30,010 --> 00:23:40,150
egress to the port ingress then you have

00:23:35,590 --> 00:23:43,570
the physical to VM to physical so what

00:23:40,150 --> 00:23:45,730
happens when my frame is sitting the V

00:23:43,570 --> 00:23:48,400
switch and the V switch takes the

00:23:45,730 --> 00:23:51,610
decision to send the frame to a specific

00:23:48,400 --> 00:23:54,520
VM and when this VM then routes the

00:23:51,610 --> 00:23:58,679
packet out to the V switch which then

00:23:54,520 --> 00:24:03,700
sends it to the to the network finally

00:23:58,679 --> 00:24:06,820
there is the more complex scenario where

00:24:03,700 --> 00:24:09,970
you have the frame hitting your vis

00:24:06,820 --> 00:24:12,220
which then going up to a VM and then

00:24:09,970 --> 00:24:16,660
that VM decides to send that frame to

00:24:12,220 --> 00:24:18,610
another PM this is the interim flow so

00:24:16,660 --> 00:24:20,740
the V switch text is frame again figures

00:24:18,610 --> 00:24:22,000
that out sends it to another VM and then

00:24:20,740 --> 00:24:24,190
the second VM is the

00:24:22,000 --> 00:24:25,930
decides to send the packet out so the

00:24:24,190 --> 00:24:31,630
piece which will figure this out and

00:24:25,930 --> 00:24:33,760
send it to the to the network card so

00:24:31,630 --> 00:24:40,510
I'll show you I'll show you some results

00:24:33,760 --> 00:24:42,610
that I collected do not focus too much

00:24:40,510 --> 00:24:47,020
about the platform that I'm using is a

00:24:42,610 --> 00:24:49,300
quite a big powerful platform but it

00:24:47,020 --> 00:24:54,750
doesn't have to be like this so in my

00:24:49,300 --> 00:24:54,750
case it's a four blades with 96 course

00:24:54,930 --> 00:25:02,440
256 gigabyte of RAM it's just huge

00:24:57,520 --> 00:25:05,560
machine it's not even needed and as I

00:25:02,440 --> 00:25:07,780
said I'm running both the vanilla kernel

00:25:05,560 --> 00:25:11,010
and an RT version of the kernel in

00:25:07,780 --> 00:25:14,800
different cases for for latency purposes

00:25:11,010 --> 00:25:20,110
the V switches are both with DP TK and

00:25:14,800 --> 00:25:22,570
non DP decay and the vnf that basically

00:25:20,110 --> 00:25:26,590
takes care of the packet forwarding in

00:25:22,570 --> 00:25:29,110
the VM is the so-called test PMD that

00:25:26,590 --> 00:25:30,970
comes with the with the PDK and it's

00:25:29,110 --> 00:25:33,370
basically capable of taking the packet

00:25:30,970 --> 00:25:38,020
and making a forward of the packet just

00:25:33,370 --> 00:25:42,340
changing the MAC address of it so

00:25:38,020 --> 00:25:44,680
hopefully you can see this the three

00:25:42,340 --> 00:25:46,330
different to treat the three different

00:25:44,680 --> 00:25:48,970
lines are the three different use cases

00:25:46,330 --> 00:25:52,420
right is the physical two physical the

00:25:48,970 --> 00:25:56,830
physical to VM to physical and then the

00:25:52,420 --> 00:26:06,790
physical vm vm physical in a short is

00:25:56,830 --> 00:26:10,150
p2p pvp PBB P what is really nice to see

00:26:06,790 --> 00:26:14,920
and is not new to the to the networking

00:26:10,150 --> 00:26:17,500
words and the DP DK word is the big gap

00:26:14,920 --> 00:26:20,380
and the big gain that you actually get

00:26:17,500 --> 00:26:24,950
by using DP D K versus unknown DP decay

00:26:20,380 --> 00:26:30,140
mode for a four packet processing it's a

00:26:24,950 --> 00:26:35,250
it goes the p2p scenario goes from a shy

00:26:30,140 --> 00:26:39,240
20% to what 70 percent of throughput for

00:26:35,250 --> 00:26:43,620
a 10 gigabit network card and here we're

00:26:39,240 --> 00:26:50,310
actually using the V switch so it's not

00:26:43,620 --> 00:26:54,900
just going in going out and in a similar

00:26:50,310 --> 00:26:58,110
way the good things done by the OBS the

00:26:54,900 --> 00:27:01,440
PDK with regards to Indra VM traffic

00:26:58,110 --> 00:27:06,120
where for very small packet sizes which

00:27:01,440 --> 00:27:13,680
are the 64 bytes packet I think the it

00:27:06,120 --> 00:27:16,890
was 0.7% that's already gone up to 210

00:27:13,680 --> 00:27:22,260
and above so you're talking about 10x

00:27:16,890 --> 00:27:27,360
magnitude another another experiment

00:27:22,260 --> 00:27:30,410
which was nice was the decompression of

00:27:27,360 --> 00:27:34,080
the vanilla kernel versus the RT kernel

00:27:30,410 --> 00:27:36,570
with an eye on the latency aspects of

00:27:34,080 --> 00:27:40,170
the traffic so if you have telco

00:27:36,570 --> 00:27:44,760
workloads and you are very strict

00:27:40,170 --> 00:27:49,950
Cheeta requirements again in this case

00:27:44,760 --> 00:27:53,070
what's nice to observe is the better job

00:27:49,950 --> 00:27:55,080
or the different job done by the RT

00:27:53,070 --> 00:27:57,990
Cardinal versus the vanilla kernel with

00:27:55,080 --> 00:28:00,630
regards to the latency so we go from all

00:27:57,990 --> 00:28:04,160
the way up to the 60,000 nanoseconds

00:28:00,630 --> 00:28:14,320
that's gone down to the maximum of

00:28:04,160 --> 00:28:16,929
25,000 a

00:28:14,320 --> 00:28:19,779
one thing though this this scenario was

00:28:16,929 --> 00:28:22,360
with the standard obvious vanilla

00:28:19,779 --> 00:28:26,100
standard obvious that comes out of the

00:28:22,360 --> 00:28:30,909
box not the DPD commode so this guy is

00:28:26,100 --> 00:28:34,120
affected by the interrupt management by

00:28:30,909 --> 00:28:36,940
context switching happening on the my on

00:28:34,120 --> 00:28:40,480
the box and that's where the RT patch is

00:28:36,940 --> 00:28:43,659
really doing a great job in fact if we

00:28:40,480 --> 00:28:46,210
take the OBS DP DK instead and we we do

00:28:43,659 --> 00:28:49,570
the same comparison we take the vanilla

00:28:46,210 --> 00:28:53,080
kernel and the article then all the

00:28:49,570 --> 00:28:55,809
benefits of it they disappear and it

00:28:53,080 --> 00:28:57,610
kind of makes sense because tippity case

00:28:55,809 --> 00:29:00,159
the polling continuously from the

00:28:57,610 --> 00:29:02,590
network card it's pinned to a specific

00:29:00,159 --> 00:29:04,570
thread on your on your on your core that

00:29:02,590 --> 00:29:08,080
core is never interrupted it just keeps

00:29:04,570 --> 00:29:10,120
polling polling polling polling so the

00:29:08,080 --> 00:29:13,570
beneficial effects that the RT patches

00:29:10,120 --> 00:29:17,320
on the interrupts are not really seen in

00:29:13,570 --> 00:29:25,570
a user space poor main polling mode from

00:29:17,320 --> 00:29:27,879
indy PDK again this chart puts just

00:29:25,570 --> 00:29:31,230
side-by-side the different results

00:29:27,879 --> 00:29:34,929
gained for both OBS and obviously PDK

00:29:31,230 --> 00:29:39,580
and if it wasn't clear before it kind of

00:29:34,929 --> 00:29:41,500
shows the 3x boost of the obvious the

00:29:39,580 --> 00:29:47,379
PDK versus the obvious vanilla running

00:29:41,500 --> 00:29:51,970
for 64 bytes pocket and the same thing

00:29:47,379 --> 00:29:54,519
can be can be seen about the vanilla

00:29:51,970 --> 00:29:59,529
kernel versus the RT kernel for obvious

00:29:54,519 --> 00:30:03,159
DP decay where the only real benefit can

00:29:59,529 --> 00:30:07,350
be seen at 512 bytes pocket where the

00:30:03,159 --> 00:30:10,720
most complex scenario DPP pvp scenario

00:30:07,350 --> 00:30:13,740
can reach line rate much much sooner

00:30:10,720 --> 00:30:13,740
than than before

00:30:14,399 --> 00:30:21,419
finally just the latency results put

00:30:18,840 --> 00:30:23,399
side by side I would skip the the mean

00:30:21,419 --> 00:30:26,309
and the max because they're usually

00:30:23,399 --> 00:30:29,039
either really really bad because of

00:30:26,309 --> 00:30:35,220
anything or too good so it's just

00:30:29,039 --> 00:30:39,559
represent the noise but the average is

00:30:35,220 --> 00:30:44,429
actually showing how the RT kernel

00:30:39,559 --> 00:30:46,859
benefit the the obvious vanilla for

00:30:44,429 --> 00:30:50,909
packet processing in a low lower latency

00:30:46,859 --> 00:30:53,759
environment this is something that I was

00:30:50,909 --> 00:30:56,909
already showed in the other charts but

00:30:53,759 --> 00:30:59,940
this is just a summary of the mean the

00:30:56,909 --> 00:31:03,029
marks in the average latency so now if

00:30:59,940 --> 00:31:14,779
you bear with me I will show you what's

00:31:03,029 --> 00:31:14,779
happening from a demo perspective okay

00:31:16,460 --> 00:31:22,349
so I have this two machine one it's

00:31:19,590 --> 00:31:27,149
called from fun enough it's actually

00:31:22,349 --> 00:31:31,080
machine sending the traffic and another

00:31:27,149 --> 00:31:35,039
machine that's called low which is

00:31:31,080 --> 00:31:39,059
basically represent the traffic

00:31:35,039 --> 00:31:42,299
generator so from is my sut the system

00:31:39,059 --> 00:31:47,759
under test and low is where the traffic

00:31:42,299 --> 00:31:50,879
generator will eventually run now vs

00:31:47,759 --> 00:31:53,749
perf as I said allows you to run a huge

00:31:50,879 --> 00:31:53,749
variety of use cases

00:33:40,680 --> 00:33:50,200
okay so um I was yes so from is the sut

00:33:47,140 --> 00:33:52,930
law is the traffic generator and they

00:33:50,200 --> 00:33:57,300
are connected back to back through two

00:33:52,930 --> 00:34:02,680
different ports port 0 / 0 port 1 port 1

00:33:57,300 --> 00:34:05,560
so I going to run vs perf

00:34:02,680 --> 00:34:08,679
telling vs perf to use the moon gen

00:34:05,560 --> 00:34:13,929
traffic generator or configuration and

00:34:08,679 --> 00:34:16,650
to test the V switch or VSD PDK I could

00:34:13,929 --> 00:34:20,140
say obvious vanilla or I could say v PP

00:34:16,650 --> 00:34:22,360
to test the other piece witches what

00:34:20,140 --> 00:34:26,700
this does it goes through my

00:34:22,360 --> 00:34:29,890
configuration figures out where on the

00:34:26,700 --> 00:34:31,940
traffic generator machine to put the

00:34:29,890 --> 00:34:35,359
configuration file

00:34:31,940 --> 00:34:37,190
and instructs traffic generator to send

00:34:35,359 --> 00:34:41,000
a specific type of traffic based on my

00:34:37,190 --> 00:34:45,109
configuration all the configuration can

00:34:41,000 --> 00:34:59,240
be found in the conf directory under ear

00:34:45,109 --> 00:35:01,460
which does you have all this set of

00:34:59,240 --> 00:35:05,470
files and they are used to configure

00:35:01,460 --> 00:35:09,589
different aspects of the of the tests

00:35:05,470 --> 00:35:13,099
the main ones are really the o2 piece

00:35:09,589 --> 00:35:16,910
which all three traffic in case you're

00:35:13,099 --> 00:35:21,460
doing the virtual machine type of test

00:35:16,910 --> 00:35:30,549
you care about the vnf the 0.4 BNF comp

00:35:21,460 --> 00:35:35,180
and that's it so what's happening it Ron

00:35:30,549 --> 00:35:37,609
so vs perf now is configuring obvious DP

00:35:35,180 --> 00:35:40,609
DK with the rules and the flows that

00:35:37,609 --> 00:35:42,950
that are needed to basically run all the

00:35:40,609 --> 00:35:45,680
use cases that it has configured in this

00:35:42,950 --> 00:35:48,200
case is going to run both physical to

00:35:45,680 --> 00:35:51,109
physical the physical to VM to physical

00:35:48,200 --> 00:35:54,230
and the physical to VM to VM to physical

00:35:51,109 --> 00:35:59,480
so it's quite a lot going on what you

00:35:54,230 --> 00:36:03,309
see now here in in yellow and brighter

00:35:59,480 --> 00:36:06,140
white these are actually the messages

00:36:03,309 --> 00:36:08,960
coming from the traffic generator that's

00:36:06,140 --> 00:36:11,180
running on the other side that's running

00:36:08,960 --> 00:36:17,119
on the other on the other machine in

00:36:11,180 --> 00:36:19,579
fact if I do top here I would see that

00:36:17,119 --> 00:36:21,799
the first process that's running there

00:36:19,579 --> 00:36:25,279
is a munjin which is my traffic

00:36:21,799 --> 00:36:28,640
generator who's using actually DP DK to

00:36:25,279 --> 00:36:35,299
burst packets to send packets to the

00:36:28,640 --> 00:36:38,089
network and this tests will keep run the

00:36:35,299 --> 00:36:42,279
way that vs perf works is it starts from

00:36:38,089 --> 00:36:45,280
the highest of highest possible

00:36:42,279 --> 00:36:48,280
throughput so in case of

00:36:45,280 --> 00:36:52,540
a 10 gigabit network card is 14 point 4

00:36:48,280 --> 00:36:55,950
million packets per second and it uses a

00:36:52,540 --> 00:36:59,140
binary search to find what is the

00:36:55,950 --> 00:37:01,900
throughput that can happen with zero

00:36:59,140 --> 00:37:03,700
percent packet loss actually the packet

00:37:01,900 --> 00:37:04,300
loss is configurable you can tell

00:37:03,700 --> 00:37:07,570
moongeun

00:37:04,300 --> 00:37:10,030
you can tell vs perf to to be happy with

00:37:07,570 --> 00:37:12,970
five percent packet loss but if you want

00:37:10,030 --> 00:37:14,740
to stick to the RFC s for performance

00:37:12,970 --> 00:37:19,840
testing then should be at zero point

00:37:14,740 --> 00:37:24,730
zero zero two percent to make it real so

00:37:19,840 --> 00:37:29,290
it will keep sending traffic and as you

00:37:24,730 --> 00:37:31,060
say as you see say the differs the first

00:37:29,290 --> 00:37:34,570
slot is finished and you found out that

00:37:31,060 --> 00:37:39,190
it's a 55 percent packet loss with a 14

00:37:34,570 --> 00:37:41,680
point four traffic so it will keep going

00:37:39,190 --> 00:37:46,870
and now it basically goes off of it

00:37:41,680 --> 00:37:49,360
sending 7.55 until it finds out which is

00:37:46,870 --> 00:37:51,910
the which is the throughput of the

00:37:49,360 --> 00:37:55,480
system at the same time it collects also

00:37:51,910 --> 00:37:58,750
the latency and latency stored on the

00:37:55,480 --> 00:38:01,210
traffic generator machine and can be

00:37:58,750 --> 00:38:03,600
correlated with specific ideas to which

00:38:01,210 --> 00:38:13,110
traffic type you send which test case

00:38:03,600 --> 00:38:19,770
you're running this is pretty much

00:38:13,110 --> 00:38:21,600
it and this is BS perv - run

00:38:19,770 --> 00:38:24,540
it runs for a while if you run all the

00:38:21,600 --> 00:38:27,090
tests it takes like eight nine hours -

00:38:24,540 --> 00:38:28,470
to finish it up so just running in the

00:38:27,090 --> 00:38:33,570
morning and you find the results in the

00:38:28,470 --> 00:38:42,340
evening thank you if you have any

00:38:33,570 --> 00:38:47,399
questions okay thanks

00:38:42,340 --> 00:38:47,399

YouTube URL: https://www.youtube.com/watch?v=8tCswVhaPtk


