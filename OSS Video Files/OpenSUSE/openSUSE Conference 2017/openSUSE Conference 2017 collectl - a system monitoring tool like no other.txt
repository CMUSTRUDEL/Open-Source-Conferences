Title: openSUSE Conference 2017 collectl - a system monitoring tool like no other
Publication date: 2017-05-28
Playlist: openSUSE Conference 2017
Description: 
	https://media.ccc.de/v/1312-collectl-a-system-monitoring-tool-like-no-other

Collectl is a comprehensive, fine-grained monitoring tool that collects a vast quantity of system metrics

Collectl was developed over a dozen years ago as a very lightweight yet highly detailed system monitoring tool, capable of collecting hundreds system performance metrics as frequently as every second. Its companion tool colplot, provides an easy to use web-based plotting package capable of displaying detailed statistics for multiple systems at the same time. Think of colmux, a third tool, as top-anything across a number of machines. It is capable of displaying anything collectl can collect in top-format, sorted by any column of your choice. For example, say you have a 100 node cluster, with colmux you can look at the disk wait times across thousands of disks sorted sorted from slowest to fasted, allowing you to easily identify bad or hot drives, OR look at memory consumption for leaks. How about a bad NIC consuming a CPU by interrupts? Or how about which process is doing the more disk reads, or write, or page faults? And remember, this is across a cluster.

The focus of collectl has always been highly efficient metrics collection and display via a CLI, no pretty pictures and no databases to slow it down. However what it does have is an API to allow it to pass those metrics onto whatever high level tools one may wish to communicate with. For example it has native support for ganglia or graphite over a socket. If you have some other favorite tool it can usually be adapted to communicate with it as well. Unfortunately most centralized tools are easily overwhelmed with fine-grained metrics and can only deal with them at granularities in the 1-min range. Not to worry, collectl has the ability to record and save metrics to local disk at one rate and send simultaneously send them to a central tool at a different rate, making it possible to get a coarser-grained centralized view and if there is a problem, still have access to finer-grained data.

Collectl has been used for monitoring some of the largest computing clusters in the world and in the last several years has been enhanced for monitoring Open Stack Clouds. It is currently packaged as part of OpenSUSE.



markseger
Captions: 
	00:00:07,370 --> 00:00:12,150
my name is Marc Seager I'm I'm pretty

00:00:10,349 --> 00:00:13,920
new to Sue's I just joined a couple of

00:00:12,150 --> 00:00:17,630
months a couple of months ago I came

00:00:13,920 --> 00:00:21,539
over with the with the HP e Cloud

00:00:17,630 --> 00:00:24,660
acquisition um prior to prior to my

00:00:21,539 --> 00:00:25,920
cloud work I'd spent about a dozen years

00:00:24,660 --> 00:00:29,010
working in high-performance computing

00:00:25,920 --> 00:00:29,760
and in the world of high-performance

00:00:29,010 --> 00:00:32,759
computing

00:00:29,760 --> 00:00:35,880
back then we were still Digital

00:00:32,759 --> 00:00:38,219
Equipment Corporation and we built this

00:00:35,880 --> 00:00:40,440
thing called the alpha processor and we

00:00:38,219 --> 00:00:44,010
had built some very very large compute

00:00:40,440 --> 00:00:47,899
clusters which were typically in several

00:00:44,010 --> 00:00:50,309
in the top ten in the in the hype in the

00:00:47,899 --> 00:00:52,230
supercomputing 500 list if anybody's

00:00:50,309 --> 00:00:53,579
familiar with it so we built a lot of

00:00:52,230 --> 00:00:57,360
big systems and we had this data

00:00:53,579 --> 00:00:59,969
collection tool on on UNIX called

00:00:57,360 --> 00:01:02,250
collect and writer write around within

00:00:59,969 --> 00:01:04,589
the next year or two so it became clear

00:01:02,250 --> 00:01:05,610
everybody was moving the Linux and we

00:01:04,589 --> 00:01:07,320
were trying to figure out all kind of

00:01:05,610 --> 00:01:09,150
data collection tools we had on Linux

00:01:07,320 --> 00:01:11,820
and it was like geez abhi nice if we had

00:01:09,150 --> 00:01:13,710
collect for Linux and if you kind of say

00:01:11,820 --> 00:01:16,200
collect for Linux real quick you wind up

00:01:13,710 --> 00:01:17,820
with collect all so that's where this

00:01:16,200 --> 00:01:21,600
all came from it originated from the

00:01:17,820 --> 00:01:23,520
high performance computing arena so what

00:01:21,600 --> 00:01:24,960
I basically wanted to talk about was

00:01:23,520 --> 00:01:26,640
first of all do we really need another

00:01:24,960 --> 00:01:28,470
monitoring till we got a zillion

00:01:26,640 --> 00:01:31,200
monitoring tools out there already why

00:01:28,470 --> 00:01:32,610
do we need one more so I want to talk a

00:01:31,200 --> 00:01:33,990
little about that I like to talk a

00:01:32,610 --> 00:01:36,210
little bit all some of the basics about

00:01:33,990 --> 00:01:37,950
how collect the works and then there's a

00:01:36,210 --> 00:01:40,320
couple utilities that I wrote that kind

00:01:37,950 --> 00:01:42,180
of height that kind of enhance the data

00:01:40,320 --> 00:01:45,240
the collector collects called call plot

00:01:42,180 --> 00:01:47,100
call mocks and then afterwards something

00:01:45,240 --> 00:01:48,540
that I find real useful is hey this is

00:01:47,100 --> 00:01:50,040
all interesting but what the hell do I

00:01:48,540 --> 00:01:51,990
do with it you know how is it going to

00:01:50,040 --> 00:01:55,170
how am I going to apply this to my daily

00:01:51,990 --> 00:02:00,439
job and hopefully I can give you a few

00:01:55,170 --> 00:02:03,400
examples that might get you there hmm

00:02:00,439 --> 00:02:05,500
what happened

00:02:03,400 --> 00:02:07,480
there we go so one of the first

00:02:05,500 --> 00:02:09,580
questions a lot of people ask is you

00:02:07,480 --> 00:02:11,470
know well why do I need system

00:02:09,580 --> 00:02:13,090
monitoring data and I believe there's a

00:02:11,470 --> 00:02:15,550
lot of different reasons and different

00:02:13,090 --> 00:02:17,050
people might have different needs some

00:02:15,550 --> 00:02:19,480
people want to know what is my system

00:02:17,050 --> 00:02:21,520
doing right now and when that happens on

00:02:19,480 --> 00:02:25,870
a Linux system you usually run something

00:02:21,520 --> 00:02:28,960
like top or you know in peace that or

00:02:25,870 --> 00:02:29,800
vmstat or one of these things sometimes

00:02:28,960 --> 00:02:31,600
you want to know what happened on my

00:02:29,800 --> 00:02:33,250
system yesterday and now you can't run

00:02:31,600 --> 00:02:34,750
these because they're not logging data

00:02:33,250 --> 00:02:37,930
to a fund they're not logging data

00:02:34,750 --> 00:02:40,330
anywhere so if you happen to remember to

00:02:37,930 --> 00:02:42,580
set up something like Tsar as an example

00:02:40,330 --> 00:02:44,490
it'll collect the data for you and then

00:02:42,580 --> 00:02:46,510
you can go back and play it back um

00:02:44,490 --> 00:02:48,280
sometimes you want to know have you know

00:02:46,510 --> 00:02:50,740
my system crashed why did it crash and

00:02:48,280 --> 00:02:54,100
you may need different kinds of data for

00:02:50,740 --> 00:02:55,510
that there's also times where you want

00:02:54,100 --> 00:02:57,670
to use some of the data for system

00:02:55,510 --> 00:03:00,220
tuning sometimes you want to do

00:02:57,670 --> 00:03:02,760
benchmarking and sometimes you actually

00:03:00,220 --> 00:03:05,950
want to troubleshoot an application and

00:03:02,760 --> 00:03:08,410
the point is there's a lot of different

00:03:05,950 --> 00:03:09,970
tools people use for different reasons

00:03:08,410 --> 00:03:17,290
depending on what problem is they're

00:03:09,970 --> 00:03:18,880
trying to solve and again I ask the

00:03:17,290 --> 00:03:21,940
question do we really need another tool

00:03:18,880 --> 00:03:24,070
and there's the problem is there's a lot

00:03:21,940 --> 00:03:27,880
of tools to choose from you know I

00:03:24,070 --> 00:03:30,130
already mentioned things like like top

00:03:27,880 --> 00:03:34,150
and stat you know some of the stat tools

00:03:30,130 --> 00:03:36,790
etc and a lot of the tools focus on

00:03:34,150 --> 00:03:38,769
different things some of them will show

00:03:36,790 --> 00:03:40,900
summary data of what your system is

00:03:38,769 --> 00:03:42,489
doing some will show detailed data of

00:03:40,900 --> 00:03:44,620
what your system is doing and it's it's

00:03:42,489 --> 00:03:45,640
not necessarily that easy I think to map

00:03:44,620 --> 00:03:48,610
from one to the other

00:03:45,640 --> 00:03:50,290
some tools will print timestamps when

00:03:48,610 --> 00:03:53,190
they report the data some tools won't

00:03:50,290 --> 00:03:56,560
support timestamps and then there's also

00:03:53,190 --> 00:04:01,690
there's also technology specific tools

00:03:56,560 --> 00:04:03,540
in the case of moving to Linux and

00:04:01,690 --> 00:04:07,000
working in high-performance computing

00:04:03,540 --> 00:04:08,530
there were some very technology specific

00:04:07,000 --> 00:04:10,989
tools back then I don't know if anybody

00:04:08,530 --> 00:04:12,760
ever heard of quadric Center Kinect but

00:04:10,989 --> 00:04:14,490
this is the this is the big hunk and

00:04:12,760 --> 00:04:17,130
interconnect with like

00:04:14,490 --> 00:04:19,770
just a couple of microseconds latency

00:04:17,130 --> 00:04:21,600
and things and it was very expensive and

00:04:19,770 --> 00:04:23,790
that was kind of what Bor that was kind

00:04:21,600 --> 00:04:26,880
of what bred InfiniBand which is now

00:04:23,790 --> 00:04:28,590
that the interconnect technology of

00:04:26,880 --> 00:04:30,480
choice for high performance computing

00:04:28,590 --> 00:04:33,000
well there were certain tools that let

00:04:30,480 --> 00:04:34,550
you monitored on Infini bian there were

00:04:33,000 --> 00:04:37,440
certain tools that let you monitor

00:04:34,550 --> 00:04:39,210
quadric s-- and if you wanted to collect

00:04:37,440 --> 00:04:41,370
that data you needed yet another tool

00:04:39,210 --> 00:04:43,380
and then of course in high performance

00:04:41,370 --> 00:04:46,080
computing a lot of people were using a

00:04:43,380 --> 00:04:48,630
lustre is it is a parallel file system

00:04:46,080 --> 00:04:50,730
and it too had tools to monitor it so

00:04:48,630 --> 00:04:52,350
the question then becomes if I'm trying

00:04:50,730 --> 00:04:55,050
to monitor my cluster how many windows

00:04:52,350 --> 00:04:57,960
do I need am i running top in one window

00:04:55,050 --> 00:04:59,700
you know IO stat in another window you

00:04:57,960 --> 00:05:01,410
know infinity end monitoring in another

00:04:59,700 --> 00:05:04,710
window and and then you have to be able

00:05:01,410 --> 00:05:05,790
to avoid getting eyeball whiplash trying

00:05:04,710 --> 00:05:10,110
to see what's going on on all the

00:05:05,790 --> 00:05:11,850
windows in coordinating now I'm not sure

00:05:10,110 --> 00:05:15,330
if this is going to show up or not

00:05:11,850 --> 00:05:16,800
umm I noticed as some of the earlier

00:05:15,330 --> 00:05:18,390
presentations people will put up

00:05:16,800 --> 00:05:19,620
screenshots and it'd be kind of hard to

00:05:18,390 --> 00:05:21,570
see if you're in the back of the room so

00:05:19,620 --> 00:05:24,360
I'll be I'll be real quick on this but

00:05:21,570 --> 00:05:27,420
the point is if you look at SAR this

00:05:24,360 --> 00:05:29,150
this is the exist with SAR when you want

00:05:27,420 --> 00:05:31,980
to look at what your CPU is doing and

00:05:29,150 --> 00:05:34,770
that takes up almost the entire screen

00:05:31,980 --> 00:05:36,600
and I try to do the same thing with with

00:05:34,770 --> 00:05:39,140
this collective tool down below with

00:05:36,600 --> 00:05:41,370
like maybe a single line of output and

00:05:39,140 --> 00:05:44,010
and that's kind of the the whole idea

00:05:41,370 --> 00:05:46,320
with this is you want to be able to

00:05:44,010 --> 00:05:47,910
integrate your your you want to be able

00:05:46,320 --> 00:05:49,110
to integrate the data from all the

00:05:47,910 --> 00:05:52,140
different things that your system is

00:05:49,110 --> 00:05:55,770
doing in such a way that makes it fairly

00:05:52,140 --> 00:05:57,690
simple to spot change because when

00:05:55,770 --> 00:05:59,340
you're benchmarking or like if you're

00:05:57,690 --> 00:06:02,700
running even top then you're looking at

00:05:59,340 --> 00:06:07,320
the CPU column you want to be able to

00:06:02,700 --> 00:06:09,630
see when that CPU column changes so

00:06:07,320 --> 00:06:11,820
another another difficulty here is that

00:06:09,630 --> 00:06:14,700
not I mentioned already not all tools

00:06:11,820 --> 00:06:15,990
log to a log file which means if you're

00:06:14,700 --> 00:06:18,360
not watching the data in real time

00:06:15,990 --> 00:06:20,910
you're going to lose it and that's a

00:06:18,360 --> 00:06:23,729
problem again some tools don't time

00:06:20,910 --> 00:06:24,960
don't timestamp their data different

00:06:23,729 --> 00:06:27,480
tools may have different levels of

00:06:24,960 --> 00:06:30,060
granularity one tool may show you

00:06:27,480 --> 00:06:31,020
what your overall cpu performance is but

00:06:30,060 --> 00:06:35,610
you're not going to be able to see what

00:06:31,020 --> 00:06:38,100
the individual CPUs are doing um what

00:06:35,610 --> 00:06:40,350
else do I want to say oh yeah of course

00:06:38,100 --> 00:06:42,600
if you if you do collect the data with a

00:06:40,350 --> 00:06:44,880
particular tool is there an easy way to

00:06:42,600 --> 00:06:46,650
pass that data from one tool to another

00:06:44,880 --> 00:06:48,050
tool for analysis later and that's not

00:06:46,650 --> 00:06:50,490
always the case

00:06:48,050 --> 00:06:52,350
centralized tools is an interesting

00:06:50,490 --> 00:06:54,600
topic that I was just starting to have

00:06:52,350 --> 00:06:57,920
before I went on and that and that has

00:06:54,600 --> 00:07:00,450
to do with this notion of again more in

00:06:57,920 --> 00:07:02,160
supercomputing if you but it applies

00:07:00,450 --> 00:07:04,260
other places as well if you're

00:07:02,160 --> 00:07:06,120
collecting a high frequency of data and

00:07:04,260 --> 00:07:08,550
you're sending it to a centralized

00:07:06,120 --> 00:07:10,470
monitoring tool can that centralized

00:07:08,550 --> 00:07:13,230
monitoring tool stick in a database in

00:07:10,470 --> 00:07:16,260
real time so that you can look at it and

00:07:13,230 --> 00:07:18,330
then perform some analysis but there's

00:07:16,260 --> 00:07:19,710
another really important issue here as

00:07:18,330 --> 00:07:22,410
well that we didn't we didn't get a

00:07:19,710 --> 00:07:24,210
chance to get into and that is if you're

00:07:22,410 --> 00:07:27,060
sending the data over the network and

00:07:24,210 --> 00:07:28,020
you're having a network issue the data

00:07:27,060 --> 00:07:29,760
is never going to get to where it's

00:07:28,020 --> 00:07:32,190
going so how are you going to diagnose

00:07:29,760 --> 00:07:33,900
the network issue which again I see some

00:07:32,190 --> 00:07:36,810
nods in the back which is another good

00:07:33,900 --> 00:07:39,330
reason why I'm a major fan of logging

00:07:36,810 --> 00:07:41,280
the data locally you'll you'll hear a

00:07:39,330 --> 00:07:43,140
little bit later that collect I'll

00:07:41,280 --> 00:07:45,840
couldn't live the data locally and send

00:07:43,140 --> 00:07:47,850
it over the wire so you can get both so

00:07:45,840 --> 00:07:51,210
if the network were to die you would

00:07:47,850 --> 00:07:52,830
still have your local copy so some of

00:07:51,210 --> 00:07:54,210
the features some of the features in

00:07:52,830 --> 00:07:56,070
collect on I'll try to get through this

00:07:54,210 --> 00:07:58,590
fairly quickly because I think sings and

00:07:56,070 --> 00:08:00,330
examples will help a lot more it's got

00:07:58,590 --> 00:08:02,340
this notion of multiple ways of

00:08:00,330 --> 00:08:04,800
displaying the data so you can display

00:08:02,340 --> 00:08:06,900
the data once you know let's say you're

00:08:04,800 --> 00:08:08,850
playing it back and you want to look at

00:08:06,900 --> 00:08:10,710
it a different way with different green

00:08:08,850 --> 00:08:13,560
you know you can look at it in multiple

00:08:10,710 --> 00:08:15,360
formats so one format might give you a

00:08:13,560 --> 00:08:16,560
high-level view of what's going on and

00:08:15,360 --> 00:08:17,940
then you might want to be able to drill

00:08:16,560 --> 00:08:21,000
down deeper and look at it differently

00:08:17,940 --> 00:08:23,100
it also has the notion of fractional and

00:08:21,000 --> 00:08:25,500
sub second monitoring intervals and it's

00:08:23,100 --> 00:08:27,150
like why the hell would you ever want to

00:08:25,500 --> 00:08:29,730
monitor your data more frequently than

00:08:27,150 --> 00:08:31,380
once a second and the answer is well if

00:08:29,730 --> 00:08:33,000
you're saying something that if you're

00:08:31,380 --> 00:08:34,860
seeing a spike that's only there for a

00:08:33,000 --> 00:08:36,630
tenth of a second well maybe you do want

00:08:34,860 --> 00:08:38,360
to see tenth of a second monitoring or

00:08:36,630 --> 00:08:41,990
half second monitoring or

00:08:38,360 --> 00:08:44,420
ever khalaqtul can run as a service just

00:08:41,990 --> 00:08:46,160
likes are you started up when the system

00:08:44,420 --> 00:08:49,820
boots and it just keeps running forever

00:08:46,160 --> 00:08:52,760
it manages its log files etc it's very

00:08:49,820 --> 00:08:54,950
lightweight one of the big issues again

00:08:52,760 --> 00:08:56,660
this has its roots and high performance

00:08:54,950 --> 00:08:58,220
computing and I might not have been as

00:08:56,660 --> 00:09:00,500
careful if I wasn't doing high

00:08:58,220 --> 00:09:02,930
performance computing collector uses

00:09:00,500 --> 00:09:06,589
about a tenth of a percent of a single

00:09:02,930 --> 00:09:08,510
CPU to collect data and that's if you're

00:09:06,589 --> 00:09:11,019
doing it at a 10 second monitoring

00:09:08,510 --> 00:09:14,029
interval which is the default but of

00:09:11,019 --> 00:09:15,769
late not being in a high performance

00:09:14,029 --> 00:09:17,480
computing environment where we have more

00:09:15,769 --> 00:09:18,980
seat where we have more cycles to burn

00:09:17,480 --> 00:09:20,540
I've been running it in one second

00:09:18,980 --> 00:09:23,060
monitoring interval you get held a lot

00:09:20,540 --> 00:09:24,769
more data and it's costing you 1% of a

00:09:23,060 --> 00:09:29,510
CPU which is still which is still not

00:09:24,769 --> 00:09:31,430
bad and it tries really hard to optimize

00:09:29,510 --> 00:09:33,709
screen real estate and what I mean by

00:09:31,430 --> 00:09:35,680
that if you had seen what I was trying

00:09:33,709 --> 00:09:39,680
to get across in that earlier SAR slide

00:09:35,680 --> 00:09:41,870
the more data I can fit on a single line

00:09:39,680 --> 00:09:44,959
and that's the important key a single

00:09:41,870 --> 00:09:49,579
line if I'm looking at CPU Network disk

00:09:44,959 --> 00:09:53,600
InfiniBand you know whatever traffic all

00:09:49,579 --> 00:09:55,820
on one row and then all the sudden one

00:09:53,600 --> 00:09:58,640
column changes from two digits to six

00:09:55,820 --> 00:10:00,440
digits I immediately see that whereas if

00:09:58,640 --> 00:10:02,870
I'm not writing it all on one row I'm

00:10:00,440 --> 00:10:06,140
not going to see that so it tries real

00:10:02,870 --> 00:10:09,410
hard to be as efficient as possible one

00:10:06,140 --> 00:10:11,870
one tool it's a great tool ice I stole

00:10:09,410 --> 00:10:14,959
their their the data they display is i/o

00:10:11,870 --> 00:10:15,519
stat but iOS that does stuff like show

00:10:14,959 --> 00:10:19,220
you

00:10:15,519 --> 00:10:21,290
iOS per second to the hundredth why are

00:10:19,220 --> 00:10:24,070
you showing me iOS to the hundredth

00:10:21,290 --> 00:10:26,390
you're wasting three characters and

00:10:24,070 --> 00:10:28,100
again three characters doesn't sound

00:10:26,390 --> 00:10:29,899
like much but if every single column in

00:10:28,100 --> 00:10:31,279
your thing is to three decimal two

00:10:29,899 --> 00:10:33,260
decimal digits you're wasting three

00:10:31,279 --> 00:10:35,690
three characters towards each column and

00:10:33,260 --> 00:10:38,089
again as you'll see in a little bit by

00:10:35,690 --> 00:10:41,170
not including that you have that much

00:10:38,089 --> 00:10:44,630
more room to show much more information

00:10:41,170 --> 00:10:46,430
so and again the last bullet we don't

00:10:44,630 --> 00:10:48,589
need to get through but it

00:10:46,430 --> 00:10:50,300
it it has the typical kinds of stuff you

00:10:48,589 --> 00:10:53,750
might expect to see in a monitoring tool

00:10:50,300 --> 00:10:58,010
from disks to CPUs to networks etc etc

00:10:53,750 --> 00:10:59,899
so H I already mentioned a couple words

00:10:58,010 --> 00:11:01,970
about fractional intervals but one quick

00:10:59,899 --> 00:11:03,770
comment and this was interesting because

00:11:01,970 --> 00:11:07,160
when I first built it I didn't I wasn't

00:11:03,770 --> 00:11:09,380
thinking about really crazy intervals it

00:11:07,160 --> 00:11:11,870
turned out probably about 10 years ago

00:11:09,380 --> 00:11:15,800
there was a default setting in one

00:11:11,870 --> 00:11:17,920
vendors a network driver that was

00:11:15,800 --> 00:11:20,270
reporting statistics every second

00:11:17,920 --> 00:11:21,950
currently I think network drivers makes

00:11:20,270 --> 00:11:23,660
the test that's available at a much

00:11:21,950 --> 00:11:26,029
higher frequency but in the day it was

00:11:23,660 --> 00:11:27,500
only once a second well it turns out if

00:11:26,029 --> 00:11:30,170
you're monitoring your if you're looking

00:11:27,500 --> 00:11:34,459
at the counters once a second and the

00:11:30,170 --> 00:11:37,040
fact that arm the clocks in Linux don't

00:11:34,459 --> 00:11:39,020
go by exactly a second because it's

00:11:37,040 --> 00:11:40,399
really a binary number of jiffy's and

00:11:39,020 --> 00:11:42,529
things and they just don't line up to a

00:11:40,399 --> 00:11:46,130
second if you are looking at network

00:11:42,529 --> 00:11:48,260
statistics what you would see is you

00:11:46,130 --> 00:11:50,360
might see and let's just say you were

00:11:48,260 --> 00:11:52,130
looking at bandwidth of a hundred just

00:11:50,360 --> 00:11:58,270
for the sake of a number you would see a

00:11:52,130 --> 00:12:00,140
hundred 100 100 100 0 200 100 100 100 0

00:11:58,270 --> 00:12:01,970
200 and it was like what the hell's

00:12:00,140 --> 00:12:04,370
going on and it had and it all had to do

00:12:01,970 --> 00:12:07,550
with this granularity well it turns out

00:12:04,370 --> 00:12:10,160
by changing the monitoring interval 2.97

00:12:07,550 --> 00:12:11,450
five seconds and collect all all the

00:12:10,160 --> 00:12:13,220
columns showed up with right numbers

00:12:11,450 --> 00:12:15,290
because that was the frequency that the

00:12:13,220 --> 00:12:17,540
driver was actually updating the

00:12:15,290 --> 00:12:19,250
statistics again kind of a silly trivial

00:12:17,540 --> 00:12:21,620
thing but it it makes a difference if

00:12:19,250 --> 00:12:22,940
you're looking at disk drives and you're

00:12:21,620 --> 00:12:26,089
trying to see what's going on with your

00:12:22,940 --> 00:12:28,190
cache in your controller a lot of times

00:12:26,089 --> 00:12:30,650
the cache in the controller you might do

00:12:28,190 --> 00:12:32,839
a write and the first write goes to the

00:12:30,650 --> 00:12:34,670
cache and then the additional the

00:12:32,839 --> 00:12:36,680
additional writes start going to the

00:12:34,670 --> 00:12:38,420
physical hard drive well by monitoring

00:12:36,680 --> 00:12:39,860
your data at a tenth of a second you can

00:12:38,420 --> 00:12:42,470
actually watch the controller cache

00:12:39,860 --> 00:12:45,230
filling so you'll see a very very high

00:12:42,470 --> 00:12:46,730
i/o rate and then after the first second

00:12:45,230 --> 00:12:50,660
then it will drop down to something more

00:12:46,730 --> 00:12:52,579
reasonable not worth getting into a lot

00:12:50,660 --> 00:12:55,670
this is just kind of showing that the

00:12:52,579 --> 00:12:58,340
way collect all is written it arm

00:12:55,670 --> 00:13:01,010
it doesn't care whether the data is

00:12:58,340 --> 00:13:02,810
coming from /proc or from it or from a

00:13:01,010 --> 00:13:04,580
disk file that you've logged it to it

00:13:02,810 --> 00:13:06,320
all it all gives you the same data which

00:13:04,580 --> 00:13:08,000
is really handy so you can look at in

00:13:06,320 --> 00:13:11,300
real time you can look at after the fact

00:13:08,000 --> 00:13:14,360
a couple of basic switches in collector

00:13:11,300 --> 00:13:15,740
because it is a command-line utility you

00:13:14,360 --> 00:13:17,450
can give it an you can tell it what

00:13:15,740 --> 00:13:18,920
subsystems you want to look at you can

00:13:17,450 --> 00:13:20,540
tell it the frequency you want to look

00:13:18,920 --> 00:13:22,370
at it you can tell what file you wants

00:13:20,540 --> 00:13:24,260
to write it to if you want to play it

00:13:22,370 --> 00:13:25,610
back from a file you tell if I want to

00:13:24,260 --> 00:13:27,770
play it back from and then this switch

00:13:25,610 --> 00:13:30,980
is really kind of nifty is the uppercase

00:13:27,770 --> 00:13:33,560
P which is I want to generate plot file

00:13:30,980 --> 00:13:35,870
format and what it does is it generates

00:13:33,560 --> 00:13:38,840
some really ugly looking output that's

00:13:35,870 --> 00:13:41,240
basically date time variable space

00:13:38,840 --> 00:13:43,460
variable space variable space which is

00:13:41,240 --> 00:13:46,040
exactly what tools like a new plot or

00:13:43,460 --> 00:13:47,870
other kinds of other tools want to see

00:13:46,040 --> 00:13:51,590
and it makes it extremely easy to plot

00:13:47,870 --> 00:13:54,010
the data so if I jump into this and can

00:13:51,590 --> 00:13:59,000
people see that or should i zoom in

00:13:54,010 --> 00:14:01,250
because it's kind of tough and I I don't

00:13:59,000 --> 00:14:03,290
want people to lose out on this if or

00:14:01,250 --> 00:14:06,830
maybe shape recognition is sufficient

00:14:03,290 --> 00:14:09,500
but what you're looking at here is this

00:14:06,830 --> 00:14:11,600
is something called brief format and in

00:14:09,500 --> 00:14:14,000
brief format we're looking at the CPUs

00:14:11,600 --> 00:14:16,640
of the disks and the network and you'll

00:14:14,000 --> 00:14:19,580
notice all I'm showing all I'm showing

00:14:16,640 --> 00:14:21,890
for CPUs is the system in the user load

00:14:19,580 --> 00:14:23,870
and I'm showing the context which is in

00:14:21,890 --> 00:14:26,210
the interrupt that's it for CPUs for

00:14:23,870 --> 00:14:31,370
disks I'm showing reads and write reads

00:14:26,210 --> 00:14:35,030
and writes and kilobytes and similar for

00:14:31,370 --> 00:14:37,820
network the point is most of the time I

00:14:35,030 --> 00:14:41,570
have found when you're looking at system

00:14:37,820 --> 00:14:43,970
behavior you don't care about individual

00:14:41,570 --> 00:14:47,390
disks and you don't care about

00:14:43,970 --> 00:14:50,480
individual CPUs you're looking at if

00:14:47,390 --> 00:14:52,790
there's a spike in my disk usage whether

00:14:50,480 --> 00:14:54,950
you're looking at individual disks or

00:14:52,790 --> 00:14:57,950
all the disks as a single number you're

00:14:54,950 --> 00:15:00,680
still going to see the spike so the

00:14:57,950 --> 00:15:02,600
point of summary data is I want a

00:15:00,680 --> 00:15:04,880
high-level overview of what my system is

00:15:02,600 --> 00:15:08,420
going if I think my system is lightly

00:15:04,880 --> 00:15:09,670
loaded and I see my CPU load at 50 G's

00:15:08,420 --> 00:15:12,730
I'm probably pretty busy

00:15:09,670 --> 00:15:16,240
similar things for discs networks etc on

00:15:12,730 --> 00:15:18,850
the other hand sometimes I want more

00:15:16,240 --> 00:15:21,700
information so there's this other thing

00:15:18,850 --> 00:15:24,760
called verbose mode and now it breaks

00:15:21,700 --> 00:15:27,370
the CPU the disk in the network in two

00:15:24,760 --> 00:15:29,290
separate lines and by breaking into

00:15:27,370 --> 00:15:32,050
separate lines you can now start seeing

00:15:29,290 --> 00:15:34,510
more information instead of instead of

00:15:32,050 --> 00:15:37,090
just seeing the CPU system and user load

00:15:34,510 --> 00:15:40,360
now you get to see the system load the

00:15:37,090 --> 00:15:43,780
user load the nice load the load of

00:15:40,360 --> 00:15:45,870
processing interrupts etc etc etc so

00:15:43,780 --> 00:15:48,400
that gives you a little bit more detail

00:15:45,870 --> 00:15:50,830
finally the third format is what I call

00:15:48,400 --> 00:15:54,010
detail format and when you're looking at

00:15:50,830 --> 00:15:58,000
detail format it's literally dividing

00:15:54,010 --> 00:16:00,250
things out individually so now again I'm

00:15:58,000 --> 00:16:02,260
I apologize if people can't quite see

00:16:00,250 --> 00:16:05,640
this but now when I'm looking at CPU

00:16:02,260 --> 00:16:08,350
load I'm looking at individual CPUs or

00:16:05,640 --> 00:16:11,680
individual discs or individual networks

00:16:08,350 --> 00:16:15,580
so the whole idea is at least the way I

00:16:11,680 --> 00:16:17,680
use collect all if I see there's if I

00:16:15,580 --> 00:16:22,240
see there's some unusual system behavior

00:16:17,680 --> 00:16:25,930
I can then rerun it and look at the

00:16:22,240 --> 00:16:29,410
individual CPUs if I'm looking at

00:16:25,930 --> 00:16:34,420
recorded data I literally can play back

00:16:29,410 --> 00:16:37,150
the exact time slice and see both the

00:16:34,420 --> 00:16:39,520
CPU summary and then the individual

00:16:37,150 --> 00:16:43,960
loads on the individual CPUs in where

00:16:39,520 --> 00:16:45,880
this gets really important is nowadays

00:16:43,960 --> 00:16:47,740
see because collector was written if

00:16:45,880 --> 00:16:49,420
somebody had a to course system that was

00:16:47,740 --> 00:16:51,760
they were they were Fat City you know

00:16:49,420 --> 00:16:54,370
they had two cores now it's not unusual

00:16:51,760 --> 00:16:57,190
to have systems with 48 cores or more

00:16:54,370 --> 00:17:02,410
and when you have a 48 core system if

00:16:57,190 --> 00:17:04,839
your CPU load is 3% does that mean that

00:17:02,410 --> 00:17:07,600
your average CPU is running at 3 per son

00:17:04,839 --> 00:17:09,760
he'll know I've seen systems at 3

00:17:07,600 --> 00:17:12,670
percent load where one CPU was at a

00:17:09,760 --> 00:17:14,650
hundred percent so it now begins to get

00:17:12,670 --> 00:17:18,490
more important to be able to drill into

00:17:14,650 --> 00:17:20,070
the individual CPUs so that that's

00:17:18,490 --> 00:17:22,620
really something that you

00:17:20,070 --> 00:17:24,720
keep in mind now there's also some

00:17:22,620 --> 00:17:28,170
additional formatting options and the

00:17:24,720 --> 00:17:30,510
top chunk is just showing you can tell

00:17:28,170 --> 00:17:33,030
it to include timestamps and you can

00:17:30,510 --> 00:17:35,370
tell it to include millisecond time

00:17:33,030 --> 00:17:38,340
stamps I virtually never look at

00:17:35,370 --> 00:17:41,600
millisecond time stamps on unless I

00:17:38,340 --> 00:17:45,660
happen to be running at a very fine

00:17:41,600 --> 00:17:48,900
monitoring interval and again in almost

00:17:45,660 --> 00:17:50,790
all cases I find that a second a

00:17:48,900 --> 00:17:53,670
one-second monitoring interval is more

00:17:50,790 --> 00:17:56,430
than enough I can't tell if he's in the

00:17:53,670 --> 00:17:58,050
audience or not maybe not Santiago has

00:17:56,430 --> 00:18:00,360
been working on that he's recently

00:17:58,050 --> 00:18:07,110
started looking at this problem where I

00:18:00,360 --> 00:18:09,390
guess some versions of I don't know

00:18:07,110 --> 00:18:11,250
whether it was leaper tumbleweed or

00:18:09,390 --> 00:18:13,530
whatever but some versions they were

00:18:11,250 --> 00:18:15,090
running on an ARM processor and the

00:18:13,530 --> 00:18:16,680
system would boot and like within a

00:18:15,090 --> 00:18:18,570
couple of seconds it would crash and

00:18:16,680 --> 00:18:20,490
we've been talking about using collect

00:18:18,570 --> 00:18:22,230
all and enabling it to monitor like a

00:18:20,490 --> 00:18:23,310
tenth of a second and try to figure out

00:18:22,230 --> 00:18:24,090
what the hell's going on before the

00:18:23,310 --> 00:18:27,090
system crashed

00:18:24,090 --> 00:18:29,550
and we're both kind of enthusiastic

00:18:27,090 --> 00:18:32,070
about trying that out the bottom the

00:18:29,550 --> 00:18:34,020
bottom mess is just simply platformer

00:18:32,070 --> 00:18:36,030
and it's just this squish together thing

00:18:34,020 --> 00:18:39,000
that really isn't intended for humans to

00:18:36,030 --> 00:18:40,410
read there's a couple unique things that

00:18:39,000 --> 00:18:43,770
collect will does that's kind of cool I

00:18:40,410 --> 00:18:45,870
think if if you look at this this is

00:18:43,770 --> 00:18:49,250
what you get when you look at processes

00:18:45,870 --> 00:18:51,330
and it looks very similar to PS or top

00:18:49,250 --> 00:18:53,070
because there's not a whole lot to say

00:18:51,330 --> 00:18:54,900
about processes everybody says the same

00:18:53,070 --> 00:18:56,360
thing about processes but the thing

00:18:54,900 --> 00:19:00,650
that's kind of cool over here is

00:18:56,360 --> 00:19:03,600
collector will show you the the disk IO

00:19:00,650 --> 00:19:05,040
associate with each process sometimes

00:19:03,600 --> 00:19:06,480
you care sometimes you don't but the

00:19:05,040 --> 00:19:08,100
columns are there in case you ever want

00:19:06,480 --> 00:19:09,540
to look at them the other thing

00:19:08,100 --> 00:19:11,910
collector will let you do is look at

00:19:09,540 --> 00:19:13,530
slab memory I don't know how many people

00:19:11,910 --> 00:19:15,900
ever find the need to look at slab

00:19:13,530 --> 00:19:17,880
memory but there have been cases where

00:19:15,900 --> 00:19:20,880
you're looking at the overall system

00:19:17,880 --> 00:19:24,210
memory and generally just as a ballpark

00:19:20,880 --> 00:19:27,120
but generally you you you might have you

00:19:24,210 --> 00:19:29,820
know a couple dozen you know megabytes

00:19:27,120 --> 00:19:31,500
tied up in slab sometimes you look at

00:19:29,820 --> 00:19:33,350
your slab memory you see you've got five

00:19:31,500 --> 00:19:35,600
gigabytes of slab memories

00:19:33,350 --> 00:19:37,970
holy crap who's using up all my slab

00:19:35,600 --> 00:19:40,820
memory well collect I can tell you in a

00:19:37,970 --> 00:19:43,340
nice formatted way what which which

00:19:40,820 --> 00:19:45,169
types of which types of data structures

00:19:43,340 --> 00:19:47,830
are tying up your slab and it can make

00:19:45,169 --> 00:19:50,690
it real quick to find the answer and

00:19:47,830 --> 00:19:54,620
finally the bottom chunk and this is

00:19:50,690 --> 00:19:58,730
more for the four KVM heads collect all

00:19:54,620 --> 00:20:01,460
knows about KVM and you can tell it to

00:19:58,730 --> 00:20:04,730
show me all my virtual machines and by

00:20:01,460 --> 00:20:11,110
virtual machine it will show you the cpu

00:20:04,730 --> 00:20:12,890
load the network load etc excuse me

00:20:11,110 --> 00:20:14,270
something else is kind of cool and

00:20:12,890 --> 00:20:17,299
collect all is it you can look at

00:20:14,270 --> 00:20:22,490
interrupts at the CPU level so on the

00:20:17,299 --> 00:20:26,090
top what we're actually seeing is for

00:20:22,490 --> 00:20:29,799
each separate CPU we're seeing how many

00:20:26,090 --> 00:20:31,400
interrupts per second that CPU is doing

00:20:29,799 --> 00:20:33,679
does it matter

00:20:31,400 --> 00:20:35,090
most of the time it doesn't but

00:20:33,679 --> 00:20:36,950
depending on you know what your

00:20:35,090 --> 00:20:38,780
particular kind of problem is it may

00:20:36,950 --> 00:20:42,650
again the thing to keep in mind would

00:20:38,780 --> 00:20:45,440
collect I'll collect the depending on

00:20:42,650 --> 00:20:47,990
how you're using it oftentimes you just

00:20:45,440 --> 00:20:51,440
turn it on and ignore it and it happily

00:20:47,990 --> 00:20:53,840
collects data for a week and then just

00:20:51,440 --> 00:20:56,900
keeps throwing off the last the last

00:20:53,840 --> 00:20:59,000
day's worth of data and sometimes

00:20:56,900 --> 00:21:00,350
sometimes my machine will run for six

00:20:59,000 --> 00:21:02,539
months and I'll never look at any of the

00:21:00,350 --> 00:21:04,580
collected data but then if there happens

00:21:02,539 --> 00:21:06,650
to be a particularly odd thing that

00:21:04,580 --> 00:21:08,270
occurred you can go back to the collect

00:21:06,650 --> 00:21:10,250
all day and say geez what's my system

00:21:08,270 --> 00:21:13,850
doing then and that and that can be very

00:21:10,250 --> 00:21:16,940
handy and the other thing that you can

00:21:13,850 --> 00:21:19,429
do with um with interrupts data is you

00:21:16,940 --> 00:21:21,679
can actually see the individual

00:21:19,429 --> 00:21:24,020
interrupts by CPU so that's what this

00:21:21,679 --> 00:21:25,669
bottom that's what this bottom display

00:21:24,020 --> 00:21:28,340
is showing you might have 50 different

00:21:25,669 --> 00:21:30,289
interrupt lines each interrupting at a

00:21:28,340 --> 00:21:33,320
different frequency each on a different

00:21:30,289 --> 00:21:36,830
CPU and you can glance at this matrix

00:21:33,320 --> 00:21:40,720
and if there's any spikes or whatever in

00:21:36,830 --> 00:21:40,720
your different CPUs it'll tell you that

00:21:41,640 --> 00:21:47,350
processes and slabs there's a lot of

00:21:44,410 --> 00:21:49,570
data I mean if you could picture if you

00:21:47,350 --> 00:21:52,030
could picture running a PS command every

00:21:49,570 --> 00:21:54,670
5 or 10 or 20 seconds and writing it to

00:21:52,030 --> 00:21:57,280
a log file and then you let it run for a

00:21:54,670 --> 00:22:01,300
day and then at the end of the day it's

00:21:57,280 --> 00:22:03,820
kind of like hmm I wonder which process

00:22:01,300 --> 00:22:06,100
was using the most CPU or I wonder which

00:22:03,820 --> 00:22:07,930
process had the most page faults or I

00:22:06,100 --> 00:22:10,660
wonder which process was you know blah

00:22:07,930 --> 00:22:11,380
blah blah or I had a I had a system

00:22:10,660 --> 00:22:14,040
crash

00:22:11,380 --> 00:22:16,420
at 2:30 in the afternoon yesterday I

00:22:14,040 --> 00:22:18,310
wonder which processes were running at

00:22:16,420 --> 00:22:21,010
2:30 in the afternoon and was any one of

00:22:18,310 --> 00:22:23,110
them thrashing my CPU well what proc

00:22:21,010 --> 00:22:24,820
analyzes it's a switch that you can use

00:22:23,110 --> 00:22:28,060
with collect I'll and it'll come back

00:22:24,820 --> 00:22:30,940
and it'll generate the equivalent of of

00:22:28,060 --> 00:22:33,460
the equivalent of a spreadsheet where

00:22:30,940 --> 00:22:35,530
each row is a process and for each

00:22:33,460 --> 00:22:37,120
process it'll tell you the time it

00:22:35,530 --> 00:22:40,360
started the time it stopped the

00:22:37,120 --> 00:22:43,420
aggregate CPU time the minimum the

00:22:40,360 --> 00:22:45,580
maximum amount of memory it used etc etc

00:22:43,420 --> 00:22:47,170
etcetera the kinds of stuff that you see

00:22:45,580 --> 00:22:49,540
when you normally run a PS command

00:22:47,170 --> 00:22:51,580
so again and I keep saying this I

00:22:49,540 --> 00:22:53,920
apologize for sounding like a broken

00:22:51,580 --> 00:22:56,170
record but it's true it's um

00:22:53,920 --> 00:22:58,030
you don't know what you want until you

00:22:56,170 --> 00:23:00,340
want it and then you're still not sure

00:22:58,030 --> 00:23:03,300
so this just collects everything and

00:23:00,340 --> 00:23:05,590
then later on depending on how your

00:23:03,300 --> 00:23:08,380
diagnostic journey takes you you may

00:23:05,590 --> 00:23:10,000
decide you're glad that it collected a

00:23:08,380 --> 00:23:11,950
certain thing so that's that's kind of

00:23:10,000 --> 00:23:14,080
like a real high level of collect I'll

00:23:11,950 --> 00:23:16,990
collect I'll does a whole lot more than

00:23:14,080 --> 00:23:18,370
I just talked about but clearly that

00:23:16,990 --> 00:23:19,110
hopefully gave you a little bit of

00:23:18,370 --> 00:23:21,670
flavor

00:23:19,110 --> 00:23:25,270
so the question is if collect I'll only

00:23:21,670 --> 00:23:27,190
runs on one machine I'm sorry if collect

00:23:25,270 --> 00:23:30,160
I'll only writes to a local log file

00:23:27,190 --> 00:23:32,550
what happens if I want to do some

00:23:30,160 --> 00:23:35,110
cluster monitoring because Clutton

00:23:32,550 --> 00:23:39,040
collectible doesn't really know about

00:23:35,110 --> 00:23:41,230
clusters well it kind of sort it does

00:23:39,040 --> 00:23:44,500
because I said earlier I used to run

00:23:41,230 --> 00:23:47,830
this on two thousand node clusters one

00:23:44,500 --> 00:23:51,310
of the things it does is every log file

00:23:47,830 --> 00:23:53,200
that it writes contains the host name so

00:23:51,310 --> 00:23:53,419
if you want to take all the logs and put

00:23:53,200 --> 00:23:55,159
them

00:23:53,419 --> 00:23:56,959
one directory somewhere for who knows

00:23:55,159 --> 00:23:59,719
what you at least know where the hell

00:23:56,959 --> 00:24:01,909
they came from is it I don't know if

00:23:59,719 --> 00:24:06,079
it's sour or not but I think it might be

00:24:01,909 --> 00:24:07,820
sour it would write files like 10 11 12

00:24:06,079 --> 00:24:09,769
and it would just simply be the day at a

00:24:07,820 --> 00:24:13,969
week so if you try to put it on another

00:24:09,769 --> 00:24:17,209
machine you'd have a mess this next

00:24:13,969 --> 00:24:20,570
comment is really subtle but it's kind

00:24:17,209 --> 00:24:22,879
of important in an HPC world it actually

00:24:20,570 --> 00:24:24,589
it works in other worlds as well let's

00:24:22,879 --> 00:24:29,589
say you had something go wrong on your

00:24:24,589 --> 00:24:32,359
cluster at three seconds after midnight

00:24:29,589 --> 00:24:34,789
well and let's say you were collecting

00:24:32,359 --> 00:24:37,099
data every 10 seconds you might go to

00:24:34,789 --> 00:24:39,499
one machine and something happened and

00:24:37,099 --> 00:24:42,559
it might have written it's log files at

00:24:39,499 --> 00:24:45,259
two seconds and 12 seconds and 22

00:24:42,559 --> 00:24:47,149
seconds another one might assuming 10

00:24:45,259 --> 00:24:49,999
second intervals another one might have

00:24:47,149 --> 00:24:53,359
written it 5 10 and 15 another one might

00:24:49,999 --> 00:24:56,629
have written it at 7 17 and 27 collect

00:24:53,359 --> 00:25:01,339
Oh collects its data at the exact same

00:24:56,629 --> 00:25:02,690
interval at the every system and I say

00:25:01,339 --> 00:25:07,179
that with a little bit of a smile

00:25:02,690 --> 00:25:12,259
because really it tries to do it within

00:25:07,179 --> 00:25:15,320
within a microsecond based on how good

00:25:12,259 --> 00:25:17,809
your clocks are but the reality is the

00:25:15,320 --> 00:25:21,440
data is pretty damn close so when

00:25:17,809 --> 00:25:23,239
there's a cluster event at least you can

00:25:21,440 --> 00:25:24,829
see what all the different machines in

00:25:23,239 --> 00:25:27,679
the cluster we're doing at approximately

00:25:24,829 --> 00:25:31,659
the same time doesn't necessarily make

00:25:27,679 --> 00:25:31,659
it that important but it's really handy

00:25:31,690 --> 00:25:36,469
some things that I've seen people do a

00:25:33,979 --> 00:25:38,809
collective is instead of writing the

00:25:36,469 --> 00:25:42,079
data to a local directory write it to an

00:25:38,809 --> 00:25:43,879
NFS server and then everybody can go to

00:25:42,079 --> 00:25:47,529
the NFS server and look at the data in

00:25:43,879 --> 00:25:51,459
real time it can also export data to

00:25:47,529 --> 00:25:54,950
ganglia in graphite and you can let them

00:25:51,459 --> 00:25:56,659
do stuff with that data the only problem

00:25:54,950 --> 00:25:59,329
is I was saying to someone earlier if

00:25:56,659 --> 00:26:00,889
you use if you use ganglia and I don't

00:25:59,329 --> 00:26:02,509
know if it's true a graphite or not but

00:26:00,889 --> 00:26:05,839
with ganglia

00:26:02,509 --> 00:26:08,119
it uses this thing called rrdtool for

00:26:05,839 --> 00:26:09,559
for plotting the data and if you look at

00:26:08,119 --> 00:26:14,599
the data it plots you're going to find

00:26:09,559 --> 00:26:16,339
out that it lies and by lying I mean

00:26:14,599 --> 00:26:20,779
what it will do is it will take your

00:26:16,339 --> 00:26:23,929
data and quote normalize and munge it so

00:26:20,779 --> 00:26:25,999
if it's putting up depending on how

00:26:23,929 --> 00:26:28,070
sumed in or zoomed out you are of the

00:26:25,999 --> 00:26:31,219
data you might have one data point

00:26:28,070 --> 00:26:32,299
representing an hour so the data point

00:26:31,219 --> 00:26:33,709
that it's going to put up that

00:26:32,299 --> 00:26:36,079
represents that hour is going to be the

00:26:33,709 --> 00:26:38,269
average over an hour so you're looking

00:26:36,079 --> 00:26:40,579
at this plot you see how my network is

00:26:38,269 --> 00:26:43,699
over here no big deal but if you were to

00:26:40,579 --> 00:26:45,259
zoom in to ten minutes you're all of a

00:26:43,699 --> 00:26:48,469
sudden now your network load is up to

00:26:45,259 --> 00:26:50,149
50% and if you zoomed in to one minute

00:26:48,469 --> 00:26:53,059
you might see your network load was

00:26:50,149 --> 00:26:54,409
ninety percent so as soon as I found

00:26:53,059 --> 00:26:56,719
that out I said I'm not going to use it

00:26:54,409 --> 00:27:00,409
for doing any my any of my graphing

00:26:56,719 --> 00:27:01,789
because I demand accuracy and I didn't

00:27:00,409 --> 00:27:04,209
want any I didn't want to look at any

00:27:01,789 --> 00:27:06,949
misleading data

00:27:04,209 --> 00:27:09,169
the one thing that collect will does do

00:27:06,949 --> 00:27:11,719
that's kind of cool that I haven't seen

00:27:09,169 --> 00:27:14,989
anybody really take advantage of you can

00:27:11,719 --> 00:27:19,039
configure it in such a way that it will

00:27:14,989 --> 00:27:21,649
collect data both locally and send it

00:27:19,039 --> 00:27:24,799
over a network furthermore you can

00:27:21,649 --> 00:27:26,179
collect data at one frequency and send

00:27:24,799 --> 00:27:29,539
it over the network at a second

00:27:26,179 --> 00:27:32,269
frequency and finally you can collect a

00:27:29,539 --> 00:27:35,299
subset you can collect data locally and

00:27:32,269 --> 00:27:38,419
send a subset over the network at a

00:27:35,299 --> 00:27:40,099
different frequency so again let's say

00:27:38,419 --> 00:27:42,109
you had a thousand node cluster you

00:27:40,099 --> 00:27:44,690
wanted a money your data every second

00:27:42,109 --> 00:27:46,699
and you wanted to send some of it to

00:27:44,690 --> 00:27:50,269
ganglia if you if that's your thing

00:27:46,699 --> 00:27:52,039
every 30 seconds or every minute and you

00:27:50,269 --> 00:27:53,779
don't want to overwhelm your database

00:27:52,039 --> 00:27:56,440
server that's logging all the data that

00:27:53,779 --> 00:27:58,579
will let you do it you could then use a

00:27:56,440 --> 00:28:00,859
centralized tool to get a high-level

00:27:58,579 --> 00:28:01,969
view of what your system is doing and if

00:28:00,859 --> 00:28:04,759
you think there's something really

00:28:01,969 --> 00:28:06,529
suspicious going on then you can log

00:28:04,759 --> 00:28:09,250
into the machine directly and look at

00:28:06,529 --> 00:28:11,650
the data more carefully

00:28:09,250 --> 00:28:14,000
so if we change gears for a minute

00:28:11,650 --> 00:28:16,520
there's this tool I wrote called call

00:28:14,000 --> 00:28:19,370
plot and the thing that inspired call

00:28:16,520 --> 00:28:23,900
plot was as I said before you can have

00:28:19,370 --> 00:28:25,460
collected data in platform app and what

00:28:23,900 --> 00:28:27,950
one of my colleagues and I used to do

00:28:25,460 --> 00:28:30,590
all the time we would take this data in

00:28:27,950 --> 00:28:33,350
platform at and then write these little

00:28:30,590 --> 00:28:36,710
scripts that would build a config file

00:28:33,350 --> 00:28:40,010
and then you would run the new plot and

00:28:36,710 --> 00:28:42,500
point to that config file and upward

00:28:40,010 --> 00:28:45,260
plot up would pop a plot and you gave it

00:28:42,500 --> 00:28:46,970
a file name and a column number and you

00:28:45,260 --> 00:28:49,480
know what kind of lines you want what

00:28:46,970 --> 00:28:52,010
colors you want and it was pretty handy

00:28:49,480 --> 00:28:55,400
but every time you ran a new set of

00:28:52,010 --> 00:28:59,440
graphs you had to edit or hack up the

00:28:55,400 --> 00:29:02,080
the script and obviously that's dumb

00:28:59,440 --> 00:29:04,460
but it worked so we did it and

00:29:02,080 --> 00:29:08,380
ultimately I wound up writing a little

00:29:04,460 --> 00:29:11,299
web interface that would basically put

00:29:08,380 --> 00:29:13,910
up a put up a silly-looking window you

00:29:11,299 --> 00:29:15,919
would click on a few checkboxes and then

00:29:13,910 --> 00:29:18,559
you'd hit the Go button and what it

00:29:15,919 --> 00:29:21,440
would do is it would build that command

00:29:18,559 --> 00:29:24,799
file for you run new plot for you and

00:29:21,440 --> 00:29:26,600
then display the plot and it works great

00:29:24,799 --> 00:29:28,970
it works great and I've been doing it

00:29:26,600 --> 00:29:31,460
for a long time the thing that's also

00:29:28,970 --> 00:29:34,160
kind of interesting is if you do display

00:29:31,460 --> 00:29:36,580
the plots it kind of displays them in

00:29:34,160 --> 00:29:38,630
what I think of this spreadsheet format

00:29:36,580 --> 00:29:40,790
so let's say you want to look at your

00:29:38,630 --> 00:29:46,059
disk your network and your CPU load on

00:29:40,790 --> 00:29:49,610
20 systems you'll actually wind up with

00:29:46,059 --> 00:29:52,460
660 plots that's 6 0 and the way it will

00:29:49,610 --> 00:29:55,309
display them is for system 1 it'll show

00:29:52,460 --> 00:29:57,290
CPU Network and disk for system 2 it'll

00:29:55,309 --> 00:29:59,690
show CPU Network and disk and then you

00:29:57,290 --> 00:30:02,000
can scroll right and left and you can

00:29:59,690 --> 00:30:04,730
look at all 60 plots on one window in

00:30:02,000 --> 00:30:08,540
your web browser it also has the ability

00:30:04,730 --> 00:30:09,169
to rearrange them and say I want 3

00:30:08,540 --> 00:30:12,559
columns

00:30:09,169 --> 00:30:14,720
CPU disk and network and stack all the

00:30:12,559 --> 00:30:16,460
CPU one stack all the network ones and

00:30:14,720 --> 00:30:18,640
stack all the disk ones so depending on

00:30:16,460 --> 00:30:20,680
what kind of data you're looking at and

00:30:18,640 --> 00:30:23,560
how you want to if you want to correlate

00:30:20,680 --> 00:30:25,720
the network traffic across all 50 nodes

00:30:23,560 --> 00:30:30,160
or how many of our I said you can do

00:30:25,720 --> 00:30:31,870
that quite simply um yeah so when I

00:30:30,160 --> 00:30:33,460
wrote Kol plot I also kind of built my

00:30:31,870 --> 00:30:35,770
own little plot definition language

00:30:33,460 --> 00:30:38,590
because I knew that I knew that I was

00:30:35,770 --> 00:30:40,270
going to want to do a lot of plots and

00:30:38,590 --> 00:30:43,180
there's something like 50 plots defined

00:30:40,270 --> 00:30:44,890
and it's fairly simple if you have other

00:30:43,180 --> 00:30:50,680
plots you can define them yourself

00:30:44,890 --> 00:30:52,390
fairly simple so again not important if

00:30:50,680 --> 00:30:55,320
you can read the whole thing but what it

00:30:52,390 --> 00:30:58,030
is this is the call plot main window and

00:30:55,320 --> 00:31:00,040
the really the real trick here is

00:30:58,030 --> 00:31:01,570
there's a bunch of check boxes and if

00:31:00,040 --> 00:31:03,370
you want to look at a CP if you want to

00:31:01,570 --> 00:31:04,900
look at CPU you click the check box that

00:31:03,370 --> 00:31:06,370
says CPU and if you want to look at

00:31:04,900 --> 00:31:09,070
Network you click the check box that

00:31:06,370 --> 00:31:10,540
says network there's also there's also a

00:31:09,070 --> 00:31:12,940
check box it says I want to see

00:31:10,540 --> 00:31:14,770
everything and that's usually the first

00:31:12,940 --> 00:31:17,440
thing I do when I'm trying to diagnose a

00:31:14,770 --> 00:31:20,350
problem is all on the very near the top

00:31:17,440 --> 00:31:23,920
you can put in the date and time from

00:31:20,350 --> 00:31:26,470
and through and I'll often just look at

00:31:23,920 --> 00:31:29,350
24 hours worth of data for a particular

00:31:26,470 --> 00:31:31,030
date and generate all the plots and see

00:31:29,350 --> 00:31:34,120
if there's any spikes so what are the

00:31:31,030 --> 00:31:36,310
plots look like here's an example and I

00:31:34,120 --> 00:31:38,530
again can people can people see the

00:31:36,310 --> 00:31:39,700
stuff back there I'm hoping it's big

00:31:38,530 --> 00:31:42,130
enough that through shape recognition

00:31:39,700 --> 00:31:45,670
you can see what's happening so this is

00:31:42,130 --> 00:31:48,880
an example of some benchmarking that I

00:31:45,670 --> 00:31:51,370
did and it's kind of amusing because

00:31:48,880 --> 00:31:52,750
when I look at the file name the file

00:31:51,370 --> 00:31:55,360
name goes right up here when I look at

00:31:52,750 --> 00:31:57,670
the file name is from 2006 I guess I

00:31:55,360 --> 00:31:59,650
haven't updated the slide in a while but

00:31:57,670 --> 00:32:01,360
I was doing some bench working I was

00:31:59,650 --> 00:32:04,990
doing some benchmarking on a network

00:32:01,360 --> 00:32:07,450
card and this was one of the new this

00:32:04,990 --> 00:32:09,940
was one of the new ten gig network cards

00:32:07,450 --> 00:32:11,860
so I guess that kind of dates when 10

00:32:09,940 --> 00:32:16,180
gig networking first started coming out

00:32:11,860 --> 00:32:20,620
and I was what I was trying to do was

00:32:16,180 --> 00:32:22,540
run run different file sizes over a 10

00:32:20,620 --> 00:32:25,450
gig network to see how the 10 gig

00:32:22,540 --> 00:32:28,330
network behaved and what you can action

00:32:25,450 --> 00:32:29,920
and the other thing that's kind of an

00:32:28,330 --> 00:32:31,399
interesting thing to do when you're

00:32:29,920 --> 00:32:34,969
doing these kinds of things

00:32:31,399 --> 00:32:37,909
is you you stall a little bit between

00:32:34,969 --> 00:32:40,129
tests and the reason you stall between

00:32:37,909 --> 00:32:41,779
tests on the one hand you want to kind

00:32:40,129 --> 00:32:44,599
of let the system settle down between

00:32:41,779 --> 00:32:47,799
tests but on the other hand if you stall

00:32:44,599 --> 00:32:49,940
it puts it puts gaps between the graphs

00:32:47,799 --> 00:32:51,710
between the plots it makes them a lot

00:32:49,940 --> 00:32:55,070
easier to look at so what we're looking

00:32:51,710 --> 00:32:57,710
at over here and I don't remember what

00:32:55,070 --> 00:32:59,509
the I don't remember what the with the

00:32:57,710 --> 00:33:01,249
values of any of this stuff is this is

00:32:59,509 --> 00:33:03,169
obviously this must be megabytes per

00:33:01,249 --> 00:33:05,779
second or something no I'm sorry this is

00:33:03,169 --> 00:33:09,950
CPU load this is this is this is

00:33:05,779 --> 00:33:13,070
megabytes per second no this is

00:33:09,950 --> 00:33:15,679
interrupts I apologize anyhow what you

00:33:13,070 --> 00:33:19,249
can see is between each test you can see

00:33:15,679 --> 00:33:22,099
the CPU load starting to increase but it

00:33:19,249 --> 00:33:23,809
kind of stopped right here it never you

00:33:22,099 --> 00:33:26,210
know you increase the load and it never

00:33:23,809 --> 00:33:29,179
went up and if we looked at the

00:33:26,210 --> 00:33:31,429
interrupts load the interrupts load kind

00:33:29,179 --> 00:33:33,889
of increased and peaked and it was kind

00:33:31,429 --> 00:33:37,009
of telling us that we were kind of

00:33:33,889 --> 00:33:40,249
limited at this point to how fast these

00:33:37,009 --> 00:33:42,769
these NICs could go and it was only

00:33:40,249 --> 00:33:45,589
later it would have been nice if I had

00:33:42,769 --> 00:33:48,320
it was only later that I actually added

00:33:45,589 --> 00:33:53,359
this Network I'm sorry this this

00:33:48,320 --> 00:33:55,460
interrupt processing to coaxial and if

00:33:53,359 --> 00:33:57,320
you looked at the interrupts you could

00:33:55,460 --> 00:33:59,539
actually see the reason this was

00:33:57,320 --> 00:34:02,719
happening was all the interrupts were

00:33:59,539 --> 00:34:05,679
going to a single CPU and we changed

00:34:02,719 --> 00:34:07,909
vendors for the for the NIC that

00:34:05,679 --> 00:34:10,579
distributed the interrupts across all

00:34:07,909 --> 00:34:12,980
the CPUs and things things went much

00:34:10,579 --> 00:34:16,929
better so that was what inspired putting

00:34:12,980 --> 00:34:21,169
the network interface processing in so

00:34:16,929 --> 00:34:24,109
here's oh this is just simply showing

00:34:21,169 --> 00:34:27,470
you an example of looking at both the

00:34:24,109 --> 00:34:29,389
client and the server that's right I was

00:34:27,470 --> 00:34:31,789
sending in I was sending traffic over

00:34:29,389 --> 00:34:34,460
the network and the thing it's

00:34:31,789 --> 00:34:37,099
interesting is here's here's the client

00:34:34,460 --> 00:34:40,700
and here's the server and you can see

00:34:37,099 --> 00:34:42,910
that the client is using a lot more CPU

00:34:40,700 --> 00:34:46,030
than the server

00:34:42,910 --> 00:34:47,890
maybe depending on what you're looking

00:34:46,030 --> 00:34:50,260
at that may or may not be interesting

00:34:47,890 --> 00:34:52,810
but again this is the advantage of being

00:34:50,260 --> 00:34:57,700
able to look at multiple servers at the

00:34:52,810 --> 00:35:00,220
same time shift gears again there's this

00:34:57,700 --> 00:35:02,500
other tool that I wrote that works with

00:35:00,220 --> 00:35:03,940
all this stuff called call MUX because

00:35:02,500 --> 00:35:06,940
and this stands for collector

00:35:03,940 --> 00:35:10,180
multiplexer and basically what it

00:35:06,940 --> 00:35:11,620
amounted to is yeah all this is good but

00:35:10,180 --> 00:35:12,850
I don't want to log in each machine to

00:35:11,620 --> 00:35:13,960
see what's going on that's a real pain

00:35:12,850 --> 00:35:16,540
in the ass can't you do anything

00:35:13,960 --> 00:35:18,760
centrally and the answer is well maybe

00:35:16,540 --> 00:35:22,690
sort of kind of and what happens with

00:35:18,760 --> 00:35:25,120
call MUX is you point it to a bunch of

00:35:22,690 --> 00:35:28,090
servers and you give it a collectible

00:35:25,120 --> 00:35:30,520
command to run and it runs that collect

00:35:28,090 --> 00:35:33,070
I'll command on each server sends all

00:35:30,520 --> 00:35:36,670
the data back to the central point over

00:35:33,070 --> 00:35:39,880
a networking over over a socket and then

00:35:36,670 --> 00:35:41,950
collect all sorts it and displays it

00:35:39,880 --> 00:35:44,680
along with the name of the machine that

00:35:41,950 --> 00:35:47,650
it came from and you can sort it by

00:35:44,680 --> 00:35:49,690
different columns so that's kind of a

00:35:47,650 --> 00:35:53,320
long-winded way of saying think of top

00:35:49,690 --> 00:35:55,810
think of the top command and and instead

00:35:53,320 --> 00:35:58,900
of it running on your one machine you're

00:35:55,810 --> 00:36:01,870
now running top on ten machines and on

00:35:58,900 --> 00:36:04,000
the far left-hand column in addition to

00:36:01,870 --> 00:36:06,130
showing you the process and the CPU load

00:36:04,000 --> 00:36:09,190
and all the other good stuff it also

00:36:06,130 --> 00:36:12,310
shows you the host name so now you can

00:36:09,190 --> 00:36:15,700
actually see the top processes across

00:36:12,310 --> 00:36:19,090
your entire cluster the next thing to

00:36:15,700 --> 00:36:22,690
imagine is well what if I don't want to

00:36:19,090 --> 00:36:25,480
look at processes sorted by CPU I want

00:36:22,690 --> 00:36:28,000
to look at processes sorted by memory

00:36:25,480 --> 00:36:29,590
size well not a problem you just held

00:36:28,000 --> 00:36:33,900
co-op you just tell call MUX sort by

00:36:29,590 --> 00:36:36,940
this column instead so it's a it's a

00:36:33,900 --> 00:36:40,210
cluster I've collected from that from

00:36:36,940 --> 00:36:41,380
that perspective and I'm going to just

00:36:40,210 --> 00:36:43,030
kind of skip through some of this stuff

00:36:41,380 --> 00:36:46,030
because it's probably more detail than

00:36:43,030 --> 00:36:47,590
you guys need to get into right away and

00:36:46,030 --> 00:36:49,050
I don't want anybody to fall asleep and

00:36:47,590 --> 00:36:51,550
it's also getting late in the day and

00:36:49,050 --> 00:36:53,650
it'll give me a little more time for Q&A

00:36:51,550 --> 00:36:55,420
and also to go into some of my scenario

00:36:53,650 --> 00:36:58,000
stuff so

00:36:55,420 --> 00:37:00,490
so basically the way you kind of sort of

00:36:58,000 --> 00:37:02,920
get started with call marks is you you

00:37:00,490 --> 00:37:06,309
actually have to give it a collective

00:37:02,920 --> 00:37:09,160
command and you and you literally need

00:37:06,309 --> 00:37:11,200
to tell it what columns of data you're

00:37:09,160 --> 00:37:15,010
interested in this is this is actually

00:37:11,200 --> 00:37:16,059
the real-time by cop by actually maybe I

00:37:15,010 --> 00:37:18,940
shouldn't have skipped the previous

00:37:16,059 --> 00:37:21,549
slide the previous slide discusses this

00:37:18,940 --> 00:37:23,470
mode where you can actually tell it I

00:37:21,549 --> 00:37:25,960
mean I'm only interested in one or two

00:37:23,470 --> 00:37:28,750
pieces of data from each machine so this

00:37:25,960 --> 00:37:31,059
so display it in one long line because

00:37:28,750 --> 00:37:33,309
again that ID here is looking for

00:37:31,059 --> 00:37:37,150
exceptions not necessarily looking for

00:37:33,309 --> 00:37:40,420
arm it depends what you're trying to do

00:37:37,150 --> 00:37:43,539
with commas so what what winds up

00:37:40,420 --> 00:37:47,369
happening is in in what I call single

00:37:43,539 --> 00:37:49,769
line mode what we're looking at here

00:37:47,369 --> 00:37:53,200
what we're looking at here is actually

00:37:49,769 --> 00:37:54,849
four different clusters and for the sake

00:37:53,200 --> 00:37:57,400
of argument they're called test one

00:37:54,849 --> 00:38:01,500
through test for and what we're actually

00:37:57,400 --> 00:38:04,829
looking at is um what are we looking at

00:38:01,500 --> 00:38:08,950
we're looking at network traffic and

00:38:04,829 --> 00:38:12,609
these four are showing us the network

00:38:08,950 --> 00:38:16,299
traffic in and these four are showing

00:38:12,609 --> 00:38:18,970
the network traffic out and then finally

00:38:16,299 --> 00:38:22,180
these two columns are showing us the

00:38:18,970 --> 00:38:25,569
total network in and network out across

00:38:22,180 --> 00:38:30,069
all four machines the only thing that's

00:38:25,569 --> 00:38:32,289
of real interest here really is when you

00:38:30,069 --> 00:38:34,900
start it up get a bunch of minus ones

00:38:32,289 --> 00:38:37,059
and the reason you get the minus ones is

00:38:34,900 --> 00:38:40,119
it says I haven't heard back from this

00:38:37,059 --> 00:38:42,460
machine and it usually takes a second or

00:38:40,119 --> 00:38:45,130
two for collector to start and processes

00:38:42,460 --> 00:38:48,309
are sending data back and then what

00:38:45,130 --> 00:38:52,089
you're seeing over here is very very low

00:38:48,309 --> 00:38:53,710
network traffic in which is typical of

00:38:52,089 --> 00:38:55,720
an idle machine you get you know a

00:38:53,710 --> 00:38:58,509
little bit of network data going on and

00:38:55,720 --> 00:39:01,390
then if you look on the outbound side

00:38:58,509 --> 00:39:03,640
there's almost nothing but more

00:39:01,390 --> 00:39:06,490
importantly there's this one blip right

00:39:03,640 --> 00:39:08,730
down here for a burst of traffic and the

00:39:06,490 --> 00:39:10,890
only point of this format

00:39:08,730 --> 00:39:14,370
is it makes it real easy to see a big

00:39:10,890 --> 00:39:16,320
blast or changes in behavior and I have

00:39:14,370 --> 00:39:18,000
some examples I don't know if it's the

00:39:16,320 --> 00:39:22,640
next slot yeah here's the next slide so

00:39:18,000 --> 00:39:26,220
here's an example of doing some luster

00:39:22,640 --> 00:39:27,750
benchmarking over infinity in and it

00:39:26,220 --> 00:39:29,880
really doesn't matter if you know what

00:39:27,750 --> 00:39:32,910
luster is or not but another way to

00:39:29,880 --> 00:39:36,540
think of it is just let's let's just say

00:39:32,910 --> 00:39:38,640
a cluster eyes file server like NFS or

00:39:36,540 --> 00:39:40,890
staff or anybody else so we're going to

00:39:38,640 --> 00:39:44,400
run a benchmark and what we want to do

00:39:40,890 --> 00:39:49,380
is we want to monitor we want to monitor

00:39:44,400 --> 00:39:51,870
the the InfiniBand traffic between

00:39:49,380 --> 00:39:54,510
machines so we want to look at

00:39:51,870 --> 00:39:56,600
InfiniBand in and infinite infinite

00:39:54,510 --> 00:39:59,130
being out and by the way this is not

00:39:56,600 --> 00:40:00,830
this is not intended for you to be able

00:39:59,130 --> 00:40:04,380
to read the details

00:40:00,830 --> 00:40:10,080
however from from purely a pattern

00:40:04,380 --> 00:40:14,910
recognition perspective if if these are

00:40:10,080 --> 00:40:16,860
my servers and I'm assuming you can see

00:40:14,910 --> 00:40:19,100
there's a lot of stuff going on here and

00:40:16,860 --> 00:40:22,590
there's not too much stuff going on here

00:40:19,100 --> 00:40:25,260
can I at least get a yeah I'm getting

00:40:22,590 --> 00:40:28,440
some nods so good so basically what is

00:40:25,260 --> 00:40:31,380
saying what this is saying is the

00:40:28,440 --> 00:40:36,180
clients the clients are writing some

00:40:31,380 --> 00:40:40,920
data to the clients are writing some

00:40:36,180 --> 00:40:44,070
data over InfiniBand so the servers are

00:40:40,920 --> 00:40:45,840
seeing a whole lot of kb in and the

00:40:44,070 --> 00:40:47,390
clients aren't seeing any kayvyun

00:40:45,840 --> 00:40:51,690
because all they're doing is writing

00:40:47,390 --> 00:40:54,750
meanwhile on this side the server's

00:40:51,690 --> 00:40:58,140
aren't seeing any kb out because they're

00:40:54,750 --> 00:41:01,500
reading however the network for the

00:40:58,140 --> 00:41:03,420
clients is doing kb out so it gives you

00:41:01,500 --> 00:41:05,160
a chant and this and then the opposite

00:41:03,420 --> 00:41:08,940
is true when we get when we get into

00:41:05,160 --> 00:41:11,850
doing the reads but the point is you can

00:41:08,940 --> 00:41:15,390
very easily visually see the difference

00:41:11,850 --> 00:41:20,100
across your twenty different machines

00:41:15,390 --> 00:41:22,660
all on a single line and personally i

00:41:20,100 --> 00:41:26,990
think it's pretty cool

00:41:22,660 --> 00:41:31,309
here's the exact same thing where things

00:41:26,990 --> 00:41:34,210
aren't so cool and again it's not a quiz

00:41:31,309 --> 00:41:39,619
or an eye test but if you come back here

00:41:34,210 --> 00:41:43,760
I'm hoping that you can see some of the

00:41:39,619 --> 00:41:45,740
columns are wider than others it's kind

00:41:43,760 --> 00:41:48,170
of erratic some of the columns are

00:41:45,740 --> 00:41:50,359
registering on the order of eleven

00:41:48,170 --> 00:41:52,880
twelve thirteen hundred some of the

00:41:50,359 --> 00:41:55,369
columns are down in the five six seven

00:41:52,880 --> 00:41:57,470
which is not good you know from a

00:41:55,369 --> 00:42:02,869
benchmarking perspective you want to see

00:41:57,470 --> 00:42:06,380
very uniform numbers if you look it here

00:42:02,869 --> 00:42:09,500
these are my 16 clients one column is

00:42:06,380 --> 00:42:11,380
zero which immediately tells me this cot

00:42:09,500 --> 00:42:14,569
this client isn't doing anything and

00:42:11,380 --> 00:42:16,880
then there's some other things where you

00:42:14,569 --> 00:42:20,029
typically want to see the benchmark

00:42:16,880 --> 00:42:22,190
conclude and all the nodes kind of

00:42:20,029 --> 00:42:25,220
finished together or close to together

00:42:22,190 --> 00:42:27,740
and here you can very easily see that

00:42:25,220 --> 00:42:30,170
some machines are finishing before other

00:42:27,740 --> 00:42:33,500
machines so you know what's what's going

00:42:30,170 --> 00:42:35,270
on here and oftentimes it means okay I

00:42:33,500 --> 00:42:38,180
got to roll up my sleeves and dig into

00:42:35,270 --> 00:42:39,980
the code and things are not right it

00:42:38,180 --> 00:42:42,049
could be a network problem it could be

00:42:39,980 --> 00:42:43,520
it could be an infinite being hardware

00:42:42,049 --> 00:42:46,069
problem it could be a lot of problems

00:42:43,520 --> 00:42:47,690
and this is not going to tell you what

00:42:46,069 --> 00:42:50,960
the answer is but it's going to at least

00:42:47,690 --> 00:42:54,400
let you look inside the behavior and get

00:42:50,960 --> 00:42:57,740
a feel for it this this other slide

00:42:54,400 --> 00:42:59,510
again this is not a big deal anymore so

00:42:57,740 --> 00:43:01,640
I guess I'm showing a little the history

00:42:59,510 --> 00:43:03,470
behind all this stuff I was at a

00:43:01,640 --> 00:43:05,510
customer site they were running this

00:43:03,470 --> 00:43:10,250
machine with a couple thousand computers

00:43:05,510 --> 00:43:12,829
on it and they had this display of three

00:43:10,250 --> 00:43:14,779
five foot wide monitors and it was like

00:43:12,829 --> 00:43:19,069
whoa that's really cool it was a it was

00:43:14,779 --> 00:43:20,660
a excuse me and I'm there wow I bet I

00:43:19,069 --> 00:43:24,559
could fit a lot of I bet I could fit a

00:43:20,660 --> 00:43:27,140
lot of comics data on it so what we did

00:43:24,559 --> 00:43:28,609
we set call mocks up and in this case I

00:43:27,140 --> 00:43:31,190
think it was only looking at a couple of

00:43:28,609 --> 00:43:34,339
hundred machines but we're looking at

00:43:31,190 --> 00:43:35,140
the CPU load on two hundred Mississippi

00:43:34,339 --> 00:43:38,650
on two hundred

00:43:35,140 --> 00:43:42,070
machines once a second in parallel in

00:43:38,650 --> 00:43:46,360
real time and again like on the earlier

00:43:42,070 --> 00:43:48,970
slide I showed it's relatively easy even

00:43:46,360 --> 00:43:50,920
if you can't read the slide to see when

00:43:48,970 --> 00:43:52,990
machines were you know see when the CPUs

00:43:50,920 --> 00:43:55,210
were busy when the CPUs weren't busy and

00:43:52,990 --> 00:43:56,980
again this is if you could look at the

00:43:55,210 --> 00:43:59,050
data you would see that when things are

00:43:56,980 --> 00:44:01,360
busy it's a hundred percent you know

00:43:59,050 --> 00:44:04,510
that that's the way HPC machines run all

00:44:01,360 --> 00:44:12,400
the CPUs run it you know real close to

00:44:04,510 --> 00:44:14,320
100 percent um yeah this next thing

00:44:12,400 --> 00:44:17,680
talks about this thing I call multi line

00:44:14,320 --> 00:44:20,050
node which is basically it's basically

00:44:17,680 --> 00:44:23,050
like I said before when you do taht when

00:44:20,050 --> 00:44:25,720
you do like a top command so what you

00:44:23,050 --> 00:44:27,850
can see if I can remember how to back

00:44:25,720 --> 00:44:29,470
this up so what you actually so what you

00:44:27,850 --> 00:44:31,090
can see in multi line node let me jump

00:44:29,470 --> 00:44:34,240
back here real quick so what we're

00:44:31,090 --> 00:44:35,800
looking at multiline node here's those

00:44:34,240 --> 00:44:38,230
same four hosts that I showed you before

00:44:35,800 --> 00:44:41,110
and now we're looking at memory

00:44:38,230 --> 00:44:42,730
utilization so you can look at you know

00:44:41,110 --> 00:44:44,410
the amount of free memory the amount of

00:44:42,730 --> 00:44:48,250
buffers how much is tied up with cash

00:44:44,410 --> 00:44:52,420
and shortly after I joined the cloud

00:44:48,250 --> 00:44:57,100
group at HP we had this public cloud

00:44:52,420 --> 00:44:58,390
with like two thousand machines on it or

00:44:57,100 --> 00:45:00,220
something like that and people were

00:44:58,390 --> 00:45:02,320
wondering about what they were doing and

00:45:00,220 --> 00:45:04,090
I said let me take a look at it with

00:45:02,320 --> 00:45:06,130
call MUX and it turns out call MUX

00:45:04,090 --> 00:45:07,660
doesn't do that well with a couple

00:45:06,130 --> 00:45:09,220
thousand machines but it can do pretty

00:45:07,660 --> 00:45:11,200
well with a couple hundred machines and

00:45:09,220 --> 00:45:13,450
I was looking at the memory load on a

00:45:11,200 --> 00:45:16,470
couple hundred machines and with call

00:45:13,450 --> 00:45:19,360
MUX you can actually use the arrow keys

00:45:16,470 --> 00:45:21,850
right and left to change the column that

00:45:19,360 --> 00:45:24,070
you're sorting on in real time which is

00:45:21,850 --> 00:45:25,780
kind of a nifty thing to do so I start

00:45:24,070 --> 00:45:28,030
just zipping around looking at and all

00:45:25,780 --> 00:45:32,230
of a sudden I notice that the system

00:45:28,030 --> 00:45:34,450
buffers were like 40 gigabytes and I'm

00:45:32,230 --> 00:45:36,190
there 40 gigabytes a system buffer

00:45:34,450 --> 00:45:38,410
clearly is not something you know and

00:45:36,190 --> 00:45:40,540
these were like 256 gig machines so they

00:45:38,410 --> 00:45:42,730
certainly had the memory but I mentioned

00:45:40,540 --> 00:45:45,400
it to somebody and these are oh you know

00:45:42,730 --> 00:45:47,740
there was this there was this there was

00:45:45,400 --> 00:45:48,430
this bug in the BIOS on some of these

00:45:47,740 --> 00:45:50,710
control

00:45:48,430 --> 00:45:53,020
that we've been updating and when we

00:45:50,710 --> 00:45:55,840
didn't update them it was chewing up a

00:45:53,020 --> 00:45:58,780
lot of system buffers so literally in

00:45:55,840 --> 00:46:02,020
like 15 seconds we found three or four

00:45:58,780 --> 00:46:04,390
servers they had bad BIOS in in one of

00:46:02,020 --> 00:46:08,980
their controllers so really really handy

00:46:04,390 --> 00:46:10,930
so what I want to do now like for the

00:46:08,980 --> 00:46:11,950
next few minutes and I'm probably not I

00:46:10,930 --> 00:46:14,380
don't know if I'll be able to get

00:46:11,950 --> 00:46:16,360
through all of these or not but maybe if

00:46:14,380 --> 00:46:17,980
I talk fast I want to I want to talk

00:46:16,360 --> 00:46:20,050
about a few examples of real-world

00:46:17,980 --> 00:46:21,190
problems that I've used collect Allah

00:46:20,050 --> 00:46:23,260
and I've actually figured out what's

00:46:21,190 --> 00:46:25,000
going on and I think I said that's

00:46:23,260 --> 00:46:27,190
probably six times already there's

00:46:25,000 --> 00:46:29,140
really no recipe for how you do this a

00:46:27,190 --> 00:46:30,490
lot of people say okay tell me what I

00:46:29,140 --> 00:46:32,770
need to do a collector to solve my

00:46:30,490 --> 00:46:34,420
problem and the answer is collect a lot

00:46:32,770 --> 00:46:36,250
of day to study the hell out of it and

00:46:34,420 --> 00:46:39,100
you know keep looking and drilling down

00:46:36,250 --> 00:46:41,440
until you figure out the problem this

00:46:39,100 --> 00:46:43,240
stuff is hard I mean sometimes you get

00:46:41,440 --> 00:46:45,070
real locking in the first ten minutes oh

00:46:43,240 --> 00:46:46,900
look at this there's the problem but

00:46:45,070 --> 00:46:49,930
sometimes it you know can take a really

00:46:46,900 --> 00:46:52,090
long time and sometimes you need

00:46:49,930 --> 00:46:54,250
sometimes collectives not the answer

00:46:52,090 --> 00:46:56,920
I've seen situations where we collect

00:46:54,250 --> 00:46:59,320
all I can get to the point where yeah

00:46:56,920 --> 00:47:01,030
there's some unusual CPU load going on

00:46:59,320 --> 00:47:03,430
here I really don't know what it is it's

00:47:01,030 --> 00:47:06,280
time to get out it's time to get out a

00:47:03,430 --> 00:47:08,290
system profiling tool if everybody if

00:47:06,280 --> 00:47:10,510
anybody's ever used like profile and you

00:47:08,290 --> 00:47:13,300
can literally see you know which

00:47:10,510 --> 00:47:15,340
functions in the kernel are executing

00:47:13,300 --> 00:47:19,780
and and sometimes you just need to do

00:47:15,340 --> 00:47:22,930
that and and again depending on the

00:47:19,780 --> 00:47:24,670
problem in hand we always like to talk

00:47:22,930 --> 00:47:26,500
about bringing in the subject matter

00:47:24,670 --> 00:47:29,650
expert if you have a particular

00:47:26,500 --> 00:47:31,960
application that's misbehaving the only

00:47:29,650 --> 00:47:33,970
the best way to get to the bottom of is

00:47:31,960 --> 00:47:36,970
have the person sitting next to you

00:47:33,970 --> 00:47:39,910
looking at the data together and use

00:47:36,970 --> 00:47:42,910
their look what the system is doing and

00:47:39,910 --> 00:47:45,820
it's like whoa and and and sometimes the

00:47:42,910 --> 00:47:47,770
answers come a lot quicker and the only

00:47:45,820 --> 00:47:49,390
heart you know the only frustrating part

00:47:47,770 --> 00:47:52,330
is sometimes you go away for a week or

00:47:49,390 --> 00:47:54,130
two trying to diagnose a problem and you

00:47:52,330 --> 00:47:56,050
tell management all we had to do is

00:47:54,130 --> 00:47:57,790
change this config setting and set one

00:47:56,050 --> 00:47:59,530
it took you two weeks and it's like

00:47:57,790 --> 00:48:00,800
sometimes it's really hard to find these

00:47:59,530 --> 00:48:02,660
problems

00:48:00,800 --> 00:48:06,170
so this was kind of an interesting

00:48:02,660 --> 00:48:09,680
situation we had this um we had this

00:48:06,170 --> 00:48:14,840
Cisco switch and it was a 10 gig Cisco

00:48:09,680 --> 00:48:18,020
Network and it was supposed to have a 40

00:48:14,840 --> 00:48:20,750
gigabyte per second backplane and what I

00:48:18,020 --> 00:48:22,610
did was I ran some network tests between

00:48:20,750 --> 00:48:24,830
you know I had like eight machines

00:48:22,610 --> 00:48:26,810
connected to it and what I did was I

00:48:24,830 --> 00:48:28,610
would run collect 'el on all the

00:48:26,810 --> 00:48:30,440
machines and I would have like one

00:48:28,610 --> 00:48:32,840
machine pinging another machine and not

00:48:30,440 --> 00:48:35,300
paying run run a high bandwidth network

00:48:32,840 --> 00:48:38,570
test and it might come back and get you

00:48:35,300 --> 00:48:42,230
like you know like 50 or 60 megabytes

00:48:38,570 --> 00:48:45,680
per second um 10 gig I'm sorry

00:48:42,230 --> 00:48:47,030
500 megabyte milometer now a couple of

00:48:45,680 --> 00:48:50,120
hundred megabytes per second or whatever

00:48:47,030 --> 00:48:52,370
was because 10 gig is going to limit you

00:48:50,120 --> 00:48:55,580
10 gig is going to limit you to about a

00:48:52,370 --> 00:48:57,830
thousand megabytes per second on a

00:48:55,580 --> 00:48:59,600
particular machine and now we were

00:48:57,830 --> 00:49:01,700
getting about we're getting about 800

00:48:59,600 --> 00:49:06,350
megabytes per second I would start up a

00:49:01,700 --> 00:49:08,840
second pair of of tests and instead of

00:49:06,350 --> 00:49:13,280
going up to 1600 like I would have

00:49:08,840 --> 00:49:15,440
expected the one that was running at 800

00:49:13,280 --> 00:49:17,540
dropped down to about 400 and the second

00:49:15,440 --> 00:49:19,370
one started up at about 400 so I was

00:49:17,540 --> 00:49:22,070
well I was still only getting about 800

00:49:19,370 --> 00:49:23,540
then if I fired up a third one then the

00:49:22,070 --> 00:49:25,310
other two would drop down and my

00:49:23,540 --> 00:49:29,890
aggregate was never getting more than

00:49:25,310 --> 00:49:32,510
like 800 850 something like that so I

00:49:29,890 --> 00:49:33,800
talked to the local people about is

00:49:32,510 --> 00:49:35,450
there something wrong with the switch

00:49:33,800 --> 00:49:37,580
and they said well no it should be

00:49:35,450 --> 00:49:39,710
working and I showed him the data so we

00:49:37,580 --> 00:49:41,240
escalated a call to Cisco I called

00:49:39,710 --> 00:49:43,130
somebody I told them what was going on

00:49:41,240 --> 00:49:44,150
and they didn't believe me so I said

00:49:43,130 --> 00:49:46,790
could I speak to your manager

00:49:44,150 --> 00:49:48,980
so they escalated me again I spoke still

00:49:46,790 --> 00:49:50,660
wouldn't believe me I escalated up a few

00:49:48,980 --> 00:49:53,540
more levels because this is a big Cisco

00:49:50,660 --> 00:49:55,760
company they finally got a senior person

00:49:53,540 --> 00:49:57,590
to come on and look around and it turned

00:49:55,760 --> 00:50:00,710
out that something was misconfigured on

00:49:57,590 --> 00:50:02,750
the switch that was sending a mirror of

00:50:00,710 --> 00:50:04,190
all the traffic out one of the links and

00:50:02,750 --> 00:50:05,930
the link that it was sending out was a

00:50:04,190 --> 00:50:08,360
10 gig link so it was limiting the

00:50:05,930 --> 00:50:10,070
entire switch to 10 gigs so problem

00:50:08,360 --> 00:50:13,010
solved

00:50:10,070 --> 00:50:15,720
this was kind of an interesting scenario

00:50:13,010 --> 00:50:17,070
I've mentioned luster a few times and

00:50:15,720 --> 00:50:19,590
that's kind of because a lot of the

00:50:17,070 --> 00:50:22,020
earlier collect the work was done with

00:50:19,590 --> 00:50:23,580
high performance computing but what

00:50:22,020 --> 00:50:26,250
we're looking at here is a couple of

00:50:23,580 --> 00:50:29,850
graphs and these graphs were generated

00:50:26,250 --> 00:50:33,270
by call plot and what we're doing and I

00:50:29,850 --> 00:50:37,770
did a couple of of i/o tests doing reads

00:50:33,270 --> 00:50:40,890
and writes and what happens is the the

00:50:37,770 --> 00:50:45,390
top graph is actually showing the data

00:50:40,890 --> 00:50:48,750
that luster sees and the bottom graph is

00:50:45,390 --> 00:50:50,910
the data the network sees I mean that

00:50:48,750 --> 00:50:53,640
the performance and what you would

00:50:50,910 --> 00:50:58,800
expect when you're doing a bunch of

00:50:53,640 --> 00:51:01,950
writes is the network and the actual

00:50:58,800 --> 00:51:05,250
data on the other end are the same or

00:51:01,950 --> 00:51:07,650
very close and it worked great for

00:51:05,250 --> 00:51:14,460
sequential operations but we did random

00:51:07,650 --> 00:51:17,880
operations what we found was the arm the

00:51:14,460 --> 00:51:20,520
network load was far higher than the

00:51:17,880 --> 00:51:22,860
client that then the than the data load

00:51:20,520 --> 00:51:24,540
and we didn't and we're steering in and

00:51:22,860 --> 00:51:26,910
staring at it and then it occurred to us

00:51:24,540 --> 00:51:30,480
that what was happening we were doing

00:51:26,910 --> 00:51:33,390
some reads and luster has this property

00:51:30,480 --> 00:51:35,550
called read ahead so when you wanted to

00:51:33,390 --> 00:51:38,760
read a megabyte worth of data it would

00:51:35,550 --> 00:51:42,150
read 40 megabytes worth of data so as a

00:51:38,760 --> 00:51:45,360
result we were generating 40 times the

00:51:42,150 --> 00:51:47,390
network traffic over the data traffic

00:51:45,360 --> 00:51:49,590
and that's why things ran so slow and

00:51:47,390 --> 00:51:53,880
collect they'll made it real easy to

00:51:49,590 --> 00:51:56,750
visualize that this was another one of

00:51:53,880 --> 00:52:01,620
these weird these weird situations where

00:51:56,750 --> 00:52:05,340
we had this customer who who had eight

00:52:01,620 --> 00:52:08,130
cores running trying to talk to lustre

00:52:05,340 --> 00:52:11,070
and each application was reading a

00:52:08,130 --> 00:52:13,050
twenty gig file which is not necessarily

00:52:11,070 --> 00:52:15,210
that big a deal but it was killing it

00:52:13,050 --> 00:52:16,920
was not only was it killing the clot not

00:52:15,210 --> 00:52:18,450
only was it killing the performance of

00:52:16,920 --> 00:52:21,750
the local machine it was killing the

00:52:18,450 --> 00:52:23,490
entire cluster and after a little bit of

00:52:21,750 --> 00:52:25,500
digging we found out that the entire

00:52:23,490 --> 00:52:28,470
bandwidth going out to the luster

00:52:25,500 --> 00:52:30,360
servers was being consumed we also find

00:52:28,470 --> 00:52:34,590
out I also found out by talking to the

00:52:30,360 --> 00:52:36,390
guy who wrote the application that even

00:52:34,590 --> 00:52:40,500
though he was only reading 20 gigabytes

00:52:36,390 --> 00:52:42,480
he was actually doing it eight times he

00:52:40,500 --> 00:52:48,000
was he was a lousy coder he was allowed

00:52:42,480 --> 00:52:51,060
a lousy coder and so as a result these

00:52:48,000 --> 00:52:54,470
eight cores running on this machine each

00:52:51,060 --> 00:52:56,970
core was reading 160 gigabytes of data

00:52:54,470 --> 00:52:59,160
so I mean it was reading a huge amount

00:52:56,970 --> 00:53:01,440
of data and now again lustres pretty

00:52:59,160 --> 00:53:03,930
fast so what was going on the order you

00:53:01,440 --> 00:53:06,119
know of um you know seven eight nine

00:53:03,930 --> 00:53:07,830
hundred gigabytes a second but that but

00:53:06,119 --> 00:53:10,140
it was consuming everything just trying

00:53:07,830 --> 00:53:12,450
to read the data and thinking about it

00:53:10,140 --> 00:53:15,990
for a while I happen to know that luster

00:53:12,450 --> 00:53:20,580
luster does do caching and I knew that

00:53:15,990 --> 00:53:22,950
these machines had arm they might have

00:53:20,580 --> 00:53:25,110
been 64 gigs something like that and I

00:53:22,950 --> 00:53:28,290
said you know you're absolutely killing

00:53:25,110 --> 00:53:31,770
the system if if instead of running

00:53:28,290 --> 00:53:35,040
eight copies of your application on the

00:53:31,770 --> 00:53:38,970
eight cores if you run one copy on the

00:53:35,040 --> 00:53:42,540
eight cores at the very least the first

00:53:38,970 --> 00:53:44,280
20 gigs should get cached so when you

00:53:42,540 --> 00:53:46,920
read it seven more times it should come

00:53:44,280 --> 00:53:49,530
out of cache which is what he did in the

00:53:46,920 --> 00:53:53,040
problem went away so here's a little

00:53:49,530 --> 00:53:55,950
plot showing you no cache consumption

00:53:53,040 --> 00:53:57,869
and the challenge is can you spot the

00:53:55,950 --> 00:54:00,380
guy who it turns out there was also a

00:53:57,869 --> 00:54:04,260
bug there was also a bug that we found

00:54:00,380 --> 00:54:07,200
that looking at the slab data which I

00:54:04,260 --> 00:54:09,060
talked about ages ago we found when we

00:54:07,200 --> 00:54:11,910
read because we saw the slab memory

00:54:09,060 --> 00:54:14,520
usage was really high and going in to

00:54:11,910 --> 00:54:17,850
collect all I found out the name of the

00:54:14,520 --> 00:54:19,619
slab that was really high and it was ll

00:54:17,850 --> 00:54:23,760
underscore or something or another

00:54:19,619 --> 00:54:26,460
well ll stands for luster light and if

00:54:23,760 --> 00:54:29,910
you googled the name of that slab the

00:54:26,460 --> 00:54:32,070
very first hit you got was memory

00:54:29,910 --> 00:54:34,020
leakage caused by luster light and

00:54:32,070 --> 00:54:37,029
here's the name of a slammer and the way

00:54:34,020 --> 00:54:39,009
you fix it is you unmount the data and

00:54:37,029 --> 00:54:41,109
mount the date and that frees up your

00:54:39,009 --> 00:54:43,089
memory and everybody's happy again but

00:54:41,109 --> 00:54:47,769
again it's an example of using a

00:54:43,089 --> 00:54:51,219
combination of collect alone Google this

00:54:47,769 --> 00:54:53,529
was this was probably one of one of my

00:54:51,219 --> 00:54:55,150
finest hours and maybe it actually this

00:54:53,529 --> 00:54:58,630
may be mice this may be my last slide

00:54:55,150 --> 00:55:02,079
I'm not sure we had this we had this HP

00:54:58,630 --> 00:55:05,739
customer and they had a 200 node high

00:55:02,079 --> 00:55:07,959
performance system and they they were

00:55:05,739 --> 00:55:10,659
upgrading their hardware so they bought

00:55:07,959 --> 00:55:13,209
you know 200 shiny new machines from us

00:55:10,659 --> 00:55:15,909
and then they plugged it in and they

00:55:13,209 --> 00:55:18,909
said we got a big problem every 5 or 10

00:55:15,909 --> 00:55:21,909
seconds we're seeing a drop in our

00:55:18,909 --> 00:55:24,699
Infinity in traffic now the thing you

00:55:21,909 --> 00:55:27,390
have to remember when you look at a high

00:55:24,699 --> 00:55:29,979
performance cluster which is something

00:55:27,390 --> 00:55:31,150
it kind of boggles the mind

00:55:29,979 --> 00:55:34,569
because when you look at a high

00:55:31,150 --> 00:55:37,959
performance cluster Network system every

00:55:34,569 --> 00:55:39,789
CPU is running it at or very close to a

00:55:37,959 --> 00:55:41,589
hundred percent it kind of it kind of

00:55:39,789 --> 00:55:43,380
like violates every law you've ever

00:55:41,589 --> 00:55:45,759
heard about you know system

00:55:43,380 --> 00:55:47,739
administration you know behavior

00:55:45,759 --> 00:55:51,189
whatever every machine is running at

00:55:47,739 --> 00:55:53,380
100% CPU every every InfiniBand network

00:55:51,189 --> 00:55:55,239
is pinned I mean these machines are

00:55:53,380 --> 00:55:57,159
working their butts off and that's the

00:55:55,239 --> 00:56:00,999
whole point of HPC you want to utilize

00:55:57,159 --> 00:56:02,799
all the resources so a five-year ten

00:56:00,999 --> 00:56:06,579
second drop-in finem and traffic is

00:56:02,799 --> 00:56:11,589
horrible and when you take a look at all

00:56:06,579 --> 00:56:14,799
the machines in parallel when when this

00:56:11,589 --> 00:56:18,069
drop occurred it occurred on all hundred

00:56:14,799 --> 00:56:19,569
machines at the same time and like your

00:56:18,069 --> 00:56:20,679
immediate thought is aren't it's a

00:56:19,569 --> 00:56:23,469
hardware problem there's something wrong

00:56:20,679 --> 00:56:25,719
with UNIF in demand network but if you

00:56:23,469 --> 00:56:28,839
think about it this is one of these

00:56:25,719 --> 00:56:31,449
deals where you need a subject matter

00:56:28,839 --> 00:56:33,880
expert or at least you need to know

00:56:31,449 --> 00:56:36,579
enough about the environment and it

00:56:33,880 --> 00:56:39,009
turns out in high performance computing

00:56:36,579 --> 00:56:45,309
if folks aren't familiar there's this

00:56:39,009 --> 00:56:47,049
thing called MPI which is the it's some

00:56:45,309 --> 00:56:49,150
kind of pair I forgot the M stands a

00:56:47,049 --> 00:56:50,080
multi processing interface or something

00:56:49,150 --> 00:56:52,950
like that

00:56:50,080 --> 00:56:55,270
and what what happens with MPI is

00:56:52,950 --> 00:56:57,520
picture-picture even a couple of

00:56:55,270 --> 00:56:59,950
machines and one of them says I got a

00:56:57,520 --> 00:57:01,810
problem to solve I'm going to hand it

00:56:59,950 --> 00:57:04,120
out so I'm going to hand it out to like

00:57:01,810 --> 00:57:05,980
you know all you guys solve the problem

00:57:04,120 --> 00:57:06,700
and then when you solve it give me the

00:57:05,980 --> 00:57:08,680
answer

00:57:06,700 --> 00:57:09,880
and I'll put your results together and

00:57:08,680 --> 00:57:11,440
then I'll give you some more work to do

00:57:09,880 --> 00:57:15,640
and then come back and give me some more

00:57:11,440 --> 00:57:17,950
work to do etc now an MPI this typically

00:57:15,640 --> 00:57:22,870
happens on the order of thousands of

00:57:17,950 --> 00:57:26,470
times a second sometimes even more well

00:57:22,870 --> 00:57:30,520
what that means is if one machine is a

00:57:26,470 --> 00:57:32,470
little slower than the others everybody

00:57:30,520 --> 00:57:34,660
is going to wait because every time I

00:57:32,470 --> 00:57:36,720
give you the problem I gotta wait a

00:57:34,660 --> 00:57:39,520
little longer for you to finish

00:57:36,720 --> 00:57:42,400
furthermore if there's an intermittent

00:57:39,520 --> 00:57:44,650
problem and one time this machine is

00:57:42,400 --> 00:57:47,020
slow and one time that machine is slow

00:57:44,650 --> 00:57:52,660
and one time that everybody's got to

00:57:47,020 --> 00:57:55,150
wait so I was asking myself is is this

00:57:52,660 --> 00:57:57,940
behavior cause because there's something

00:57:55,150 --> 00:57:59,440
wrong with the infiniium or is there

00:57:57,940 --> 00:58:02,620
some bizarre problem in the system

00:57:59,440 --> 00:58:04,690
somewhere and that's causing every

00:58:02,620 --> 00:58:06,700
button that's causing different machines

00:58:04,690 --> 00:58:08,680
to slow down and the fact that those

00:58:06,700 --> 00:58:11,620
machines are slowing down is what's

00:58:08,680 --> 00:58:13,360
causing the InfiniBand a slowdown so I'm

00:58:11,620 --> 00:58:17,410
sitting there saying oh boy I got myself

00:58:13,360 --> 00:58:19,000
a tough one and this was like I thought

00:58:17,410 --> 00:58:21,580
I was already into like four or five

00:58:19,000 --> 00:58:23,950
hours looking at data and and what you

00:58:21,580 --> 00:58:26,500
do the only thing you can really do is

00:58:23,950 --> 00:58:30,820
just start looking at stuff and seeing

00:58:26,500 --> 00:58:32,920
if there's any anomalies so you know I I

00:58:30,820 --> 00:58:36,790
did my standard call plot thing and I

00:58:32,920 --> 00:58:39,370
said plot everything and I noticed

00:58:36,790 --> 00:58:42,790
something really bizarre and it wasn't

00:58:39,370 --> 00:58:49,680
completely obvious immediately but if I

00:58:42,790 --> 00:58:52,150
happen to look at page faults per system

00:58:49,680 --> 00:58:54,640
every once in a while there was a little

00:58:52,150 --> 00:58:56,830
spike in page faults not much but there

00:58:54,640 --> 00:58:59,800
was a little page fault spike and it

00:58:56,830 --> 00:59:01,690
kind of sort of looked like when that

00:58:59,800 --> 00:59:05,500
page fault spiked

00:59:01,690 --> 00:59:08,349
the InfiniBand dropped and no matter

00:59:05,500 --> 00:59:10,660
where does page fault was all the

00:59:08,349 --> 00:59:11,500
infinity hands dropped and it's like oh

00:59:10,660 --> 00:59:14,259
man

00:59:11,500 --> 00:59:17,380
in it is this a coincidence or is this

00:59:14,259 --> 00:59:20,009
real so I'm there I wonder if I can tell

00:59:17,380 --> 00:59:23,769
which process is causing the page fault

00:59:20,009 --> 00:59:27,579
well if you may remember I said before

00:59:23,769 --> 00:59:30,099
that when you run when you run collect

00:59:27,579 --> 00:59:33,430
Oh looking at process data you can

00:59:30,099 --> 00:59:36,670
actually run it sorted by the column of

00:59:33,430 --> 00:59:38,259
your choice and actually know I was

00:59:36,670 --> 00:59:39,819
talking about call MUX doing that but

00:59:38,259 --> 00:59:41,980
you can also do that with collect I'll

00:59:39,819 --> 00:59:44,950
running it there's a there's a top

00:59:41,980 --> 00:59:46,930
switch there's a lot of switches but the

00:59:44,950 --> 00:59:49,829
bottom line is I was running collect all

00:59:46,930 --> 00:59:52,809
I was sorting it by page faults and

00:59:49,829 --> 00:59:55,890
every so often this one process showed

00:59:52,809 --> 00:59:58,599
up at the top of the screen

00:59:55,890 --> 01:00:00,339
corresponding exactly to when that

00:59:58,599 --> 01:00:04,680
InfiniBand thing occurred and it turned

01:00:00,339 --> 01:00:07,420
out it turned out it was a puppet client

01:00:04,680 --> 01:00:10,299
you know I don't know what the hell's

01:00:07,420 --> 01:00:13,089
puppet doing with InfiniBand and again

01:00:10,299 --> 01:00:16,420
our subject matter expert the customer

01:00:13,089 --> 01:00:16,839
he's there oh I said what and he's

01:00:16,420 --> 01:00:19,660
there

01:00:16,839 --> 01:00:23,589
I thought our machine was identical but

01:00:19,660 --> 01:00:27,759
I forgot when we set up our puppet

01:00:23,589 --> 01:00:31,119
client we were pointing him to a

01:00:27,759 --> 01:00:32,980
different network and he was going over

01:00:31,119 --> 01:00:37,240
all these switches and there's probably

01:00:32,980 --> 01:00:39,759
a delay that's causing puppet to fault

01:00:37,240 --> 01:00:42,220
which in turn is slowing down that

01:00:39,759 --> 01:00:44,789
machine so he's there let me try

01:00:42,220 --> 01:00:47,920
disabling the puppet client so he goes

01:00:44,789 --> 01:00:51,250
disables the puppet client no InfiniBand

01:00:47,920 --> 01:00:54,940
drop so I thought that was pretty cool I

01:00:51,250 --> 01:00:56,619
thought that was pretty cool so that was

01:00:54,940 --> 01:00:58,930
that I guess that was my last slide so

01:00:56,619 --> 01:01:00,400
in any event if anybody has any

01:00:58,930 --> 01:01:04,059
questions I'd certainly be happy to

01:01:00,400 --> 01:01:06,880
entertain them and if you guys know

01:01:04,059 --> 01:01:09,809
Santiago who again I don't see here but

01:01:06,880 --> 01:01:12,180
that's okay um he

01:01:09,809 --> 01:01:14,670
he and I were talking about collect

01:01:12,180 --> 01:01:16,319
along I may have made a convert and he's

01:01:14,670 --> 01:01:17,969
already got it running and he's going to

01:01:16,319 --> 01:01:20,189
be starting to use it so if anybody

01:01:17,969 --> 01:01:22,380
needs any local collect I'll consulting

01:01:20,189 --> 01:01:26,430
then maybe he can help provide it so

01:01:22,380 --> 01:01:30,029
I'll be quiet I'm just thinking how to

01:01:26,430 --> 01:01:32,819
formulate the question I'm very

01:01:30,029 --> 01:01:35,910
interested in in monitoring so my

01:01:32,819 --> 01:01:39,719
question would be can you define some

01:01:35,910 --> 01:01:45,089
levels for specific CPUs or i/o

01:01:39,719 --> 01:01:47,130
bandwidth issues and then have some kind

01:01:45,089 --> 01:01:49,229
of event that can be triggered if those

01:01:47,130 --> 01:01:51,180
levels are reached that's a that's a

01:01:49,229 --> 01:01:54,660
great queen there that's a great

01:01:51,180 --> 01:01:57,689
question in the simple answer is now the

01:01:54,660 --> 01:02:00,119
reason I say this the reason I say this

01:01:57,689 --> 01:02:03,209
and this is purely this is purely a

01:02:00,119 --> 01:02:06,539
philosophical question from a silk from

01:02:03,209 --> 01:02:10,890
a philosophical perspective I'm a big

01:02:06,539 --> 01:02:14,099
fan of doing one or two things and doing

01:02:10,890 --> 01:02:16,769
them really well and the more the more

01:02:14,099 --> 01:02:18,989
stuff that you stick on if you start

01:02:16,769 --> 01:02:21,269
sticking on gooeys if you start logging

01:02:18,989 --> 01:02:23,339
to databases if you start all this other

01:02:21,269 --> 01:02:25,619
stuff there's that much more to support

01:02:23,339 --> 01:02:27,869
and the more stuff that you have to

01:02:25,619 --> 01:02:29,910
support the less attention you might be

01:02:27,869 --> 01:02:32,729
paying to other things so the short

01:02:29,910 --> 01:02:37,380
answer is no it doesn't do it however

01:02:32,729 --> 01:02:39,209
however I tried to to put a lot of

01:02:37,380 --> 01:02:43,289
capabilities in collect all that would

01:02:39,209 --> 01:02:45,420
allow other tools to use it so for

01:02:43,289 --> 01:02:48,059
example one thing that you can do with

01:02:45,420 --> 01:02:50,429
collect I'll is you can tell it send

01:02:48,059 --> 01:02:51,390
this output over a socket and if

01:02:50,429 --> 01:02:53,699
somebody's on the other end of the

01:02:51,390 --> 01:02:56,279
socket listening then they could do it

01:02:53,699 --> 01:02:57,779
however for some people that's a pain in

01:02:56,279 --> 01:02:59,429
the butt I don't want to figure out how

01:02:57,779 --> 01:03:01,380
to do socket Pro I just want to write a

01:02:59,429 --> 01:03:01,799
bash grip you know forget all the socket

01:03:01,380 --> 01:03:05,069
stuff

01:03:01,799 --> 01:03:09,630
Coleco has another option where you can

01:03:05,069 --> 01:03:11,339
tell it when when the time comes - and

01:03:09,630 --> 01:03:13,229
you know you could also say well I could

01:03:11,339 --> 01:03:14,459
always tail to collect the log but not

01:03:13,229 --> 01:03:17,569
axe a pain in the butt

01:03:14,459 --> 01:03:20,640
- well there's another option that says

01:03:17,569 --> 01:03:23,789
whenever you take a sample

01:03:20,640 --> 01:03:27,240
write it to a file with this name in

01:03:23,789 --> 01:03:30,480
this directory and the next time you

01:03:27,240 --> 01:03:33,150
take a sample overwrite the file with

01:03:30,480 --> 01:03:35,190
the same with the same name in the same

01:03:33,150 --> 01:03:38,609
directory with the new information and

01:03:35,190 --> 01:03:42,180
now if you do that you can write a bash

01:03:38,609 --> 01:03:44,369
script and every 5 seconds 10 seconds 2

01:03:42,180 --> 01:03:45,690
hours whatever you want you can read the

01:03:44,369 --> 01:03:48,480
file and you can see what collect we'll

01:03:45,690 --> 01:03:50,460
just did so that's kind of a backhanded

01:03:48,480 --> 01:03:51,839
way to say well if you want to figure

01:03:50,460 --> 01:03:53,730
out how to do the event thing you can

01:03:51,839 --> 01:03:57,480
but Caleta can get the data for you and

01:03:53,730 --> 01:03:59,099
put it where you want it and and again

01:03:57,480 --> 01:04:00,059
if you if you actually want to follow up

01:03:59,099 --> 01:04:03,210
on that you know you could certainly

01:04:00,059 --> 01:04:03,750
shoot me an email or something so

01:04:03,210 --> 01:04:06,329
anything else

01:04:03,750 --> 01:04:10,109
I know it's kind of tough being the last

01:04:06,329 --> 01:04:11,519
act of the show I was wondering what the

01:04:10,109 --> 01:04:14,730
packaging state is something is it in

01:04:11,519 --> 01:04:17,010
tumbleweed and is it in leap already and

01:04:14,730 --> 01:04:19,319
what's the situation with that

01:04:17,010 --> 01:04:20,910
yeah I'm still trying to get my brain

01:04:19,319 --> 01:04:23,549
straight around halt how all this

01:04:20,910 --> 01:04:25,349
release stuff works however and I yes I

01:04:23,549 --> 01:04:28,200
see someone coming here to help me out

01:04:25,349 --> 01:04:35,220
but I know that I know that collect all

01:04:28,200 --> 01:04:38,279
is correct humble weed and as yeah it's

01:04:35,220 --> 01:04:41,490
it's everywhere in openSUSE as far as I

01:04:38,279 --> 01:04:44,789
know it's also in leap it's definitely

01:04:41,490 --> 01:04:46,769
in factory in tumbleweed and I've just

01:04:44,789 --> 01:05:03,569
opened a feature request to also get it

01:04:46,769 --> 01:05:05,940
in slits the one other thing it was kind

01:05:03,569 --> 01:05:08,460
of an accident but I got lucky when I

01:05:05,940 --> 01:05:11,099
when I when I picked the name collective

01:05:08,460 --> 01:05:12,869
as far as I know I don't think there's I

01:05:11,099 --> 01:05:15,150
don't think that word exists in any

01:05:12,869 --> 01:05:18,000
language because if you google it you

01:05:15,150 --> 01:05:19,799
immediately find it whereas some of

01:05:18,000 --> 01:05:22,200
these other things you google them and

01:05:19,799 --> 01:05:24,060
you find out things you don't want to

01:05:22,200 --> 01:05:28,350
know

01:05:24,060 --> 01:05:32,830
so if that's it thanks for coming and

01:05:28,350 --> 01:05:34,290
have a couple beers for me and I guess

01:05:32,830 --> 01:05:39,710
that's it

01:05:34,290 --> 01:05:41,770
[Applause]

01:05:39,710 --> 01:05:41,770

YouTube URL: https://www.youtube.com/watch?v=rZwlHnrlOJ8


