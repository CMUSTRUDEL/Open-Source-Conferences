Title: openSUSE Conference 2017 Ceph Explained - With Raspberry Pis
Publication date: 2017-05-27
Playlist: openSUSE Conference 2017
Description: 
	https://media.ccc.de/v/1428-ceph-explained-with-raspberry-pis

Demonstration of Ceph on a Raspberry Pi cluster

This talk provides an introduction into Ceph, a software defined storage with openSUSE. The talk is going to explain and demonstrate how Ceph distributes data over hosts, racks and other failure domains and scales with the number of available nodes. The handling of failed hardware is explained and demonstrated. The talk concentrates on the theoretical concepts and a live demonstration.



Sven Seeberg
Captions: 
	00:00:07,849 --> 00:00:14,190
hello and welcome to the storks f

00:00:11,219 --> 00:00:16,680
explained with raspberry PI's and my

00:00:14,190 --> 00:00:19,580
name is Sven seebik I am working for

00:00:16,680 --> 00:00:23,730
Susanna documentation team and today

00:00:19,580 --> 00:00:28,500
yeah I want to use raspberry PI's to

00:00:23,730 --> 00:00:30,630
explain Saif Ali at least a little the

00:00:28,500 --> 00:00:33,270
problem initially I thought we are well

00:00:30,630 --> 00:00:36,059
raspberry PI's could be used they do not

00:00:33,270 --> 00:00:38,640
fit the system requirements but they do

00:00:36,059 --> 00:00:41,430
not miss that far but okay I thought I

00:00:38,640 --> 00:00:44,010
could use them and then I started

00:00:41,430 --> 00:00:45,239
building the slides and I wanted to put

00:00:44,010 --> 00:00:47,940
a recommendation there that they are

00:00:45,239 --> 00:00:50,250
they should not be used for productions

00:00:47,940 --> 00:00:52,260
of purposes and now my conclusion after

00:00:50,250 --> 00:00:54,719
running the first test is you shouldn't

00:00:52,260 --> 00:00:59,789
use them for any purposes except running

00:00:54,719 --> 00:01:02,879
LEDs on them okay and I want to

00:00:59,789 --> 00:01:05,610
emphasize that I'm not a chef expert or

00:01:02,879 --> 00:01:08,040
anything like that I am just having fun

00:01:05,610 --> 00:01:12,299
with raspberry PI's and deploying a safe

00:01:08,040 --> 00:01:14,790
cluster but I will try to explain the

00:01:12,299 --> 00:01:18,979
core concepts of safe with the help of

00:01:14,790 --> 00:01:22,189
the LEDs attached to the recipe pies

00:01:18,979 --> 00:01:24,270
okay and as this is a live demo um

00:01:22,189 --> 00:01:26,159
here's the warning

00:01:24,270 --> 00:01:30,119
it is a live demo don't expect it to

00:01:26,159 --> 00:01:32,009
work at least all the time and I know

00:01:30,119 --> 00:01:34,860
that some of the things I did here with

00:01:32,009 --> 00:01:37,110
LEDs could also be done with several

00:01:34,860 --> 00:01:41,430
monitoring tools for example open attic

00:01:37,110 --> 00:01:44,460
but for the heck of it I used LEDs okay

00:01:41,430 --> 00:01:46,140
and if they are question questions to Ed

00:01:44,460 --> 00:01:48,540
during the talk please feel free to ask

00:01:46,140 --> 00:01:55,740
any time and remind me to repeat it for

00:01:48,540 --> 00:01:58,439
a microphone I do not want to talk about

00:01:55,740 --> 00:01:59,969
how you set up your safe cluster at home

00:01:58,439 --> 00:02:03,719
I don't want to explain how to

00:01:59,969 --> 00:02:05,610
administrate it and I am NOT going into

00:02:03,719 --> 00:02:08,940
detailed informations about the features

00:02:05,610 --> 00:02:11,610
I'm just going to explain the general

00:02:08,940 --> 00:02:13,650
ideas behind safe so that's what I'm

00:02:11,610 --> 00:02:16,260
going to talk about today they

00:02:13,650 --> 00:02:18,780
are some other other talks and on Sunday

00:02:16,260 --> 00:02:21,599
for example Joshua has a talk at 11 a.m.

00:02:18,780 --> 00:02:24,840
about how to deploy a safe cluster with

00:02:21,599 --> 00:02:27,769
salt I recommend going there if you want

00:02:24,840 --> 00:02:30,480
to set up your own safe cluster Tom and

00:02:27,769 --> 00:02:33,500
Abbey and then it's r tire talking about

00:02:30,480 --> 00:02:40,200
how to monitor yourself cluster with

00:02:33,500 --> 00:02:44,040
elasticsearch okay now what is F well

00:02:40,200 --> 00:02:46,530
thus these pilot points out basically on

00:02:44,040 --> 00:02:49,139
every slide that is introducing safe and

00:02:46,530 --> 00:02:51,389
it's a pre-designed define storage it's

00:02:49,139 --> 00:02:53,250
open source highly scalable the

00:02:51,389 --> 00:02:56,310
developers claim it's scaling into the

00:02:53,250 --> 00:02:59,340
petabytes and it actually does and you

00:02:56,310 --> 00:03:01,409
can run hundreds of gigabyte terabyte or

00:02:59,340 --> 00:03:06,450
even hundreds of petabytes of storage

00:03:01,409 --> 00:03:09,329
with this F the interesting part is that

00:03:06,450 --> 00:03:11,970
you can use let's say commodity hardware

00:03:09,329 --> 00:03:18,000
but not as commodity as raspberry PI's

00:03:11,970 --> 00:03:20,250
and yeah the whole thing is self-healing

00:03:18,000 --> 00:03:20,790
managing what that means I will explain

00:03:20,250 --> 00:03:24,930
later

00:03:20,790 --> 00:03:28,410
and the other interesting part and that

00:03:24,930 --> 00:03:29,459
already saved my ass a few times it's it

00:03:28,410 --> 00:03:32,099
doesn't have a single point of failure

00:03:29,459 --> 00:03:35,280
so some of the pies already crashed and

00:03:32,099 --> 00:03:39,419
I could manage to write it continue to

00:03:35,280 --> 00:03:42,680
run okay and CF is finally something

00:03:39,419 --> 00:03:42,680
that barely runs in raspberry PI's

00:03:42,709 --> 00:03:47,190
another word about safe in general it

00:03:45,209 --> 00:03:49,680
does not magically solve all storage

00:03:47,190 --> 00:03:53,840
problems you really need to know what

00:03:49,680 --> 00:03:56,609
your requirements are and you need to

00:03:53,840 --> 00:04:00,840
define your cluster requirements

00:03:56,609 --> 00:04:03,120
accordingly now before I start talking

00:04:00,840 --> 00:04:06,449
about the concepts and demonstrating a

00:04:03,120 --> 00:04:11,310
few things I want to m.s the microphone

00:04:06,449 --> 00:04:12,659
volume okay yeah okay I first want to

00:04:11,310 --> 00:04:14,669
give us a shortened to track

00:04:12,659 --> 00:04:17,820
introduction into the different nodes

00:04:14,669 --> 00:04:21,779
that are used in the rest in a safe

00:04:17,820 --> 00:04:24,120
cluster and especially the two I'm using

00:04:21,779 --> 00:04:27,150
right now that are the OSD and monitor

00:04:24,120 --> 00:04:29,370
nodes and the other ones that are you

00:04:27,150 --> 00:04:33,300
usually associated with our self

00:04:29,370 --> 00:04:34,949
clusters and are in theory available in

00:04:33,300 --> 00:04:37,729
here but I did not have the time to set

00:04:34,949 --> 00:04:40,530
them up so we are going not to use them

00:04:37,729 --> 00:04:44,490
first of all they already always these

00:04:40,530 --> 00:04:47,669
the object storage devices the CFO SD

00:04:44,490 --> 00:04:51,120
OSD is short for object storage device

00:04:47,669 --> 00:04:56,910
and the OSD daemon is the daemon that is

00:04:51,120 --> 00:04:59,550
handling the raw disk so to speak they

00:04:56,910 --> 00:05:02,039
actually started data and a cluster

00:04:59,550 --> 00:05:05,400
they always community communicate with

00:05:02,039 --> 00:05:07,620
each other and they take care that data

00:05:05,400 --> 00:05:10,440
is replicated replicated within the

00:05:07,620 --> 00:05:12,060
cluster so I can say I want 3

00:05:10,440 --> 00:05:14,970
replications of each object in the

00:05:12,060 --> 00:05:17,820
cluster and table automatically do that

00:05:14,970 --> 00:05:19,530
and if one node fails they will take

00:05:17,820 --> 00:05:21,990
another node and duplicate the data

00:05:19,530 --> 00:05:25,410
there so I always have the three

00:05:21,990 --> 00:05:27,509
replications available and they also

00:05:25,410 --> 00:05:31,919
yeah I already mentioned it basically

00:05:27,509 --> 00:05:35,669
they see failures of other OS DS and

00:05:31,919 --> 00:05:41,039
begin replicating data again and the OSD

00:05:35,669 --> 00:05:45,599
is the daemon that is managing the data

00:05:41,039 --> 00:05:48,570
on a disk or it could potentially be 2

00:05:45,599 --> 00:05:50,630
disks one is used for the data the other

00:05:48,570 --> 00:05:53,159
one is used for the journal of the data

00:05:50,630 --> 00:05:56,639
for the object start and data disc and

00:05:53,159 --> 00:05:59,760
you can have both partitions on one hard

00:05:56,639 --> 00:06:03,680
drive it is usually recommended to put

00:05:59,760 --> 00:06:06,659
the journal on an SSD and the data on a

00:06:03,680 --> 00:06:09,090
magnetic Drive that is much slower and

00:06:06,659 --> 00:06:13,740
in this case I'm using very slow USB

00:06:09,090 --> 00:06:19,949
drives and performance is accordingly ok

00:06:13,740 --> 00:06:22,919
and the monitor notes are used to track

00:06:19,949 --> 00:06:24,990
the status of the cluster so they always

00:06:22,919 --> 00:06:30,810
know which notes are available in the

00:06:24,990 --> 00:06:33,030
cluster and exchange this map called a

00:06:30,810 --> 00:06:36,320
crash map but I will talk about this

00:06:33,030 --> 00:06:39,029
later between each other so they always

00:06:36,320 --> 00:06:40,650
communicate with each other about the

00:06:39,029 --> 00:06:43,949
current status of the

00:06:40,650 --> 00:06:48,419
and they usually decide or not usually

00:06:43,949 --> 00:06:50,130
they decide by majority which a map or

00:06:48,419 --> 00:06:54,240
which version of the map is the current

00:06:50,130 --> 00:06:57,780
map and right version and that's that

00:06:54,240 --> 00:06:59,639
why I have three large racks

00:06:57,780 --> 00:07:04,080
I call them racks of raspberry pies in

00:06:59,639 --> 00:07:09,389
here yeah about that in a few minutes

00:07:04,080 --> 00:07:12,750
okay and then the other ones for example

00:07:09,389 --> 00:07:15,120
the metadata server is used for if you

00:07:12,750 --> 00:07:17,760
want to export a file system that is

00:07:15,120 --> 00:07:20,520
POSIX compatible you could use the

00:07:17,760 --> 00:07:23,190
metadata server or Redis Gateway

00:07:20,520 --> 00:07:27,199
basically provides a REST API to the

00:07:23,190 --> 00:07:30,030
object sort in the cluster and now to

00:07:27,199 --> 00:07:33,210
give you an insight into the setup of

00:07:30,030 --> 00:07:35,220
the nodes here and I had to swear a lot

00:07:33,210 --> 00:07:39,710
during the last days I'm setting this

00:07:35,220 --> 00:07:42,389
whole thing up and my colleagues began

00:07:39,710 --> 00:07:46,440
changing class stuff to class to fake

00:07:42,389 --> 00:07:48,660
and yeah that's how I'm calling it right

00:07:46,440 --> 00:07:52,979
now and because it's not really a safe

00:07:48,660 --> 00:07:56,520
cluster and okay at the top of each rack

00:07:52,979 --> 00:07:58,139
there's a monitor nodes there are three

00:07:56,520 --> 00:08:00,990
monitor nodes because they always need a

00:07:58,139 --> 00:08:03,210
majority to decide and if you have for

00:08:00,990 --> 00:08:07,860
example four nodes and one node fails

00:08:03,210 --> 00:08:09,810
then three nodes can still decide about

00:08:07,860 --> 00:08:12,510
a new version of the map but if the

00:08:09,810 --> 00:08:14,610
second second one fails then there's no

00:08:12,510 --> 00:08:17,639
majority and therefore there's no real

00:08:14,610 --> 00:08:19,139
advantage over having just three nodes

00:08:17,639 --> 00:08:22,889
so you can always have just one failure

00:08:19,139 --> 00:08:26,729
in any case so therefore I'm using three

00:08:22,889 --> 00:08:29,669
monitor nodes and in my theoretical

00:08:26,729 --> 00:08:32,039
setup I wanted to put them each in one

00:08:29,669 --> 00:08:35,219
rack and maybe distribute the racks in

00:08:32,039 --> 00:08:37,459
different rooms in my data center and

00:08:35,219 --> 00:08:39,900
therefore have a pretty good

00:08:37,459 --> 00:08:43,320
availability in case of power failures

00:08:39,900 --> 00:08:46,410
in one room for example okay and then

00:08:43,320 --> 00:08:49,500
the on the lower left two nodes are

00:08:46,410 --> 00:08:52,350
currently unused and the upper-left note

00:08:49,500 --> 00:08:54,250
is used as an administrator node m and

00:08:52,350 --> 00:08:57,819
you're going to see

00:08:54,250 --> 00:09:00,220
what is going on later on there and then

00:08:57,819 --> 00:09:02,889
there are some nice LEDs I wanted to

00:09:00,220 --> 00:09:04,779
show on the webcam I'm hope I'm hoping

00:09:02,889 --> 00:09:07,720
that is working later on and you can see

00:09:04,779 --> 00:09:12,389
the LEDs they are currently three

00:09:07,720 --> 00:09:16,269
different yeah

00:09:12,389 --> 00:09:18,430
traffic lights basically and you see the

00:09:16,269 --> 00:09:22,000
there are always three LEDs on the top

00:09:18,430 --> 00:09:23,560
notes and they like any traffic light

00:09:22,000 --> 00:09:26,790
red is not good

00:09:23,560 --> 00:09:29,290
yellow is okayish and green is good and

00:09:26,790 --> 00:09:32,199
for the admin note that means the whole

00:09:29,290 --> 00:09:35,379
cluster is fine and for the other three

00:09:32,199 --> 00:09:38,889
that means yellow day accrete just

00:09:35,379 --> 00:09:41,079
agreed on a new map and or when they are

00:09:38,889 --> 00:09:43,149
when one is red that basically means

00:09:41,079 --> 00:09:44,139
that something is wrong and green

00:09:43,149 --> 00:09:46,000
everything is fine

00:09:44,139 --> 00:09:48,850
and it will show a heartbeat all the

00:09:46,000 --> 00:09:53,319
time and then there are other LEDs on

00:09:48,850 --> 00:09:57,100
each storage node red may currently mean

00:09:53,319 --> 00:10:00,129
so so it's correct red means right and

00:09:57,100 --> 00:10:03,970
korean-led means currently the device is

00:10:00,129 --> 00:10:07,420
reading and yeah I'm hoping you can see

00:10:03,970 --> 00:10:09,550
the colors later on and before I'm going

00:10:07,420 --> 00:10:10,779
into the details about how a self is

00:10:09,550 --> 00:10:13,569
handing all the data and going to

00:10:10,779 --> 00:10:16,139
demonstrate it and a few other things

00:10:13,569 --> 00:10:21,399
about how you can access the data and

00:10:16,139 --> 00:10:25,449
and how safe is basically handling all

00:10:21,399 --> 00:10:28,920
that and all our data and Saif is

00:10:25,449 --> 00:10:32,680
basically start in objects objects

00:10:28,920 --> 00:10:34,689
reside somewhere called pools and you

00:10:32,680 --> 00:10:37,029
can access the objects in the pools we

00:10:34,689 --> 00:10:40,269
arrest for example the red escape way

00:10:37,029 --> 00:10:44,250
earlier mentioned you can also use safe

00:10:40,269 --> 00:10:47,500
file system and you can also export

00:10:44,250 --> 00:10:49,870
extra block devices and there are other

00:10:47,500 --> 00:10:53,230
tools that allow exporting I sky and so

00:10:49,870 --> 00:10:54,819
on and I'm going to use first

00:10:53,230 --> 00:10:56,470
demonstration a command line two we'll

00:10:54,819 --> 00:10:59,379
call it Redis that is directly

00:10:56,470 --> 00:11:01,930
interacting with the cluster and there

00:10:59,379 --> 00:11:03,579
are also libraries available in many

00:11:01,930 --> 00:11:05,980
programming languages that allow you to

00:11:03,579 --> 00:11:07,570
write programs that directly interact

00:11:05,980 --> 00:11:10,770
with the cluster

00:11:07,570 --> 00:11:13,120
okay how does safe manage all the things

00:11:10,770 --> 00:11:15,490
scaling into petabytes and not have any

00:11:13,120 --> 00:11:17,740
single point of failure and I think this

00:11:15,490 --> 00:11:20,440
there are many talks about this and I'm

00:11:17,740 --> 00:11:22,420
having another run at it and yeah the

00:11:20,440 --> 00:11:25,750
thing responsible for this is called

00:11:22,420 --> 00:11:29,620
crash control scalable decentralized

00:11:25,750 --> 00:11:31,690
placement of replicated data and crush

00:11:29,620 --> 00:11:35,770
is an algorithm based approach to

00:11:31,690 --> 00:11:37,840
storing data and it does not require a

00:11:35,770 --> 00:11:40,660
lot of information to calculate where

00:11:37,840 --> 00:11:42,490
the data should be stored and it just

00:11:40,660 --> 00:11:45,880
needs the name of the object to be

00:11:42,490 --> 00:11:48,070
stored it needs to know the pool the

00:11:45,880 --> 00:11:51,340
logical partition where to store the

00:11:48,070 --> 00:11:52,590
data and it requires a current status of

00:11:51,340 --> 00:11:55,270
the cluster

00:11:52,590 --> 00:11:58,450
which nodes are available and if they're

00:11:55,270 --> 00:12:00,880
online or offline and then you can also

00:11:58,450 --> 00:12:03,310
place some constraints on the cluster

00:12:00,880 --> 00:12:06,430
for example in the set up I would like

00:12:03,310 --> 00:12:09,610
to have one replication of each object

00:12:06,430 --> 00:12:11,560
on each rec so if one room goes offline

00:12:09,610 --> 00:12:15,730
oven rack goes offline for example when

00:12:11,560 --> 00:12:19,930
I plug off power supply then one records

00:12:15,730 --> 00:12:22,120
offline and yeah I'm going to let it run

00:12:19,930 --> 00:12:23,320
for a cup of milk because best berry

00:12:22,120 --> 00:12:25,720
pies are slow and they will take some

00:12:23,320 --> 00:12:28,030
time to realize that some nodes are gone

00:12:25,720 --> 00:12:31,660
okay you already see the oh no you can't

00:12:28,030 --> 00:12:33,820
see it sorry now you can see it still

00:12:31,660 --> 00:12:35,830
you can see it you see that the yellow

00:12:33,820 --> 00:12:37,390
LED is running right now the blinking

00:12:35,830 --> 00:12:39,670
that means that they realize something

00:12:37,390 --> 00:12:45,610
is wrong but it will still take some

00:12:39,670 --> 00:12:47,590
time okay let's continue then there are

00:12:45,610 --> 00:12:51,390
the user-defined constraints on the

00:12:47,590 --> 00:12:53,440
placement as mentioned and they are

00:12:51,390 --> 00:12:56,860
written in something called the crash

00:12:53,440 --> 00:13:00,460
map and yeah so I already started with

00:12:56,860 --> 00:13:04,570
the first demonstration I unplugged one

00:13:00,460 --> 00:13:08,020
of the racks and what you probably can

00:13:04,570 --> 00:13:15,340
see you know I can't because okay so now

00:13:08,020 --> 00:13:17,980
you can see that the raspberries here

00:13:15,340 --> 00:13:21,490
and those racks are not doing a lot

00:13:17,980 --> 00:13:23,709
that's because another

00:13:21,490 --> 00:13:27,760
no can you see it that they are blinking

00:13:23,709 --> 00:13:29,050
here yeah sort of okay now they start

00:13:27,760 --> 00:13:31,860
blinking yeah that's what should happen

00:13:29,050 --> 00:13:35,800
and now they realize that the all the

00:13:31,860 --> 00:13:38,440
notes here are gone and the applications

00:13:35,800 --> 00:13:41,320
are not they're not real plication

00:13:38,440 --> 00:13:44,110
szávay labore so they start copying the

00:13:41,320 --> 00:13:47,170
still existing replications of data to

00:13:44,110 --> 00:13:49,360
other nodes so that I regain my three

00:13:47,170 --> 00:13:51,910
total replications I wanted to have in

00:13:49,360 --> 00:13:54,640
the cluster and the cluster status

00:13:51,910 --> 00:13:58,060
during that time is not very healthy

00:13:54,640 --> 00:14:04,560
and that is a signal butter currently

00:13:58,060 --> 00:14:07,630
red LED yeah so there are two different

00:14:04,560 --> 00:14:10,240
statuses up after XF class though one is

00:14:07,630 --> 00:14:12,790
arrow one is warn that are not good but

00:14:10,240 --> 00:14:15,399
error does not mean that it's not red

00:14:12,790 --> 00:14:17,890
it's crashing error can also mean it

00:14:15,399 --> 00:14:20,790
just takes some time to recover so red

00:14:17,890 --> 00:14:28,899
does not mean that it's totally failing

00:14:20,790 --> 00:14:32,860
okay um now you want to store the data

00:14:28,899 --> 00:14:35,079
in something like partitions having all

00:14:32,860 --> 00:14:36,520
data objects and one partition is not

00:14:35,079 --> 00:14:38,140
very useful for example you want to

00:14:36,520 --> 00:14:41,230
create user rights to different parts of

00:14:38,140 --> 00:14:43,750
the cluster and that is achieved by the

00:14:41,230 --> 00:14:47,740
things called pools as mentioned

00:14:43,750 --> 00:14:50,829
previously and now you may think well I

00:14:47,740 --> 00:14:55,300
know partitions easy stuff and yeah

00:14:50,829 --> 00:14:59,110
basically this the pools are from the

00:14:55,300 --> 00:15:02,110
outside acting like mo yeah very similar

00:14:59,110 --> 00:15:08,490
to petitions you can for example create

00:15:02,110 --> 00:15:11,440
snapshots and yeah you can define rules

00:15:08,490 --> 00:15:12,820
for each pool for example the

00:15:11,440 --> 00:15:14,680
replication level you can say this pool

00:15:12,820 --> 00:15:16,839
should have three replication the other

00:15:14,680 --> 00:15:18,490
one only two and then there are

00:15:16,839 --> 00:15:21,180
different modes for replication so I

00:15:18,490 --> 00:15:21,180
will talk about this later

00:15:22,529 --> 00:15:28,959
now that's not all inside there are

00:15:27,370 --> 00:15:31,600
something called placement groups and

00:15:28,959 --> 00:15:34,420
the placement groups are basically

00:15:31,600 --> 00:15:35,020
helping safe to organize the data within

00:15:34,420 --> 00:15:37,990
the

00:15:35,020 --> 00:15:41,380
Lester and if you have thousands or

00:15:37,990 --> 00:15:45,520
millions of objects in one pool and each

00:15:41,380 --> 00:15:47,710
note has to know or should in theory be

00:15:45,520 --> 00:15:50,290
able to calculate where each object is

00:15:47,710 --> 00:15:54,760
currently stored and this is very

00:15:50,290 --> 00:15:56,410
expensive in case EPU time therefore the

00:15:54,760 --> 00:16:00,570
complexity is reduced with the placement

00:15:56,410 --> 00:16:04,330
groups and the placement groups are just

00:16:00,570 --> 00:16:07,480
groups of objects packed together and

00:16:04,330 --> 00:16:10,360
they are put together based on their

00:16:07,480 --> 00:16:12,640
names so and the there's something like

00:16:10,360 --> 00:16:15,010
a hash function that is deciding which

00:16:12,640 --> 00:16:18,220
placement group and object resides in

00:16:15,010 --> 00:16:22,440
and each placement group then uses a

00:16:18,220 --> 00:16:25,090
defined set of OSDs to store the data

00:16:22,440 --> 00:16:28,480
yeah and placement groups are one of the

00:16:25,090 --> 00:16:35,200
most important maybe the most important

00:16:28,480 --> 00:16:37,540
a performance factor okay and so when I

00:16:35,200 --> 00:16:39,670
want to store an object in a cluster I

00:16:37,540 --> 00:16:42,520
decide I want to put it in pull a or

00:16:39,670 --> 00:16:44,470
pool B and the pools then are divided

00:16:42,520 --> 00:16:47,470
into a number of placement groups there

00:16:44,470 --> 00:16:49,750
are some yeah you have to calculate it

00:16:47,470 --> 00:16:51,040
the depending on the number of notes you

00:16:49,750 --> 00:16:52,840
are having in your cluster but I'm not

00:16:51,040 --> 00:16:58,690
going into this right now or not in this

00:16:52,840 --> 00:17:01,030
talk and then the chef is creating the

00:16:58,690 --> 00:17:04,150
hash of the filename deciding well this

00:17:01,030 --> 00:17:06,430
file goes into placement one of the pool

00:17:04,150 --> 00:17:08,680
or placement group two of the pool or

00:17:06,430 --> 00:17:11,520
maybe another placement group and when

00:17:08,680 --> 00:17:13,780
I'm putting it in another pool and

00:17:11,520 --> 00:17:15,610
they're always these can be used by

00:17:13,780 --> 00:17:18,550
several placement groups at the same

00:17:15,610 --> 00:17:22,060
time so there's no need that just one

00:17:18,550 --> 00:17:24,760
placement group is using one node iOS D

00:17:22,060 --> 00:17:28,000
and it for performance reasons it's even

00:17:24,760 --> 00:17:30,270
better to have multiple placement groups

00:17:28,000 --> 00:17:35,350
on one node the average should be around

00:17:30,270 --> 00:17:38,440
115 okay and then there are some neat

00:17:35,350 --> 00:17:41,380
things going on when I begin writing of

00:17:38,440 --> 00:17:45,160
an object into one placement group or

00:17:41,380 --> 00:17:47,050
two specifically one OSD the client

00:17:45,160 --> 00:17:48,490
knows which use the OSD it needs to

00:17:47,050 --> 00:17:51,760
contact and write a story up

00:17:48,490 --> 00:17:54,070
checked on and then the OSD knows that

00:17:51,760 --> 00:17:56,260
it has to create two other replications

00:17:54,070 --> 00:17:58,570
of the object and begins writing those

00:17:56,260 --> 00:18:02,020
replications to other notes and as soon

00:17:58,570 --> 00:18:06,000
as the epic replications are finished

00:18:02,020 --> 00:18:08,620
and writing on of all three objects or

00:18:06,000 --> 00:18:10,510
replications is finished then the

00:18:08,620 --> 00:18:13,300
primary OSD that's the one that was

00:18:10,510 --> 00:18:15,070
contacted at the beginning and sends

00:18:13,300 --> 00:18:18,030
signal back to the client that the

00:18:15,070 --> 00:18:22,660
object has been successfully stored and

00:18:18,030 --> 00:18:25,570
so this is not that fast because I first

00:18:22,660 --> 00:18:27,460
need to contact one oh s D this one it's

00:18:25,570 --> 00:18:30,070
a contact to other may be more or less

00:18:27,460 --> 00:18:31,990
depends on your configuration and needs

00:18:30,070 --> 00:18:35,110
to write all the stuff and then

00:18:31,990 --> 00:18:38,290
afterwards it can send the finished

00:18:35,110 --> 00:18:41,890
signal and on the other hand reading is

00:18:38,290 --> 00:18:44,260
much better because when you your client

00:18:41,890 --> 00:18:47,679
does not only know what the primary note

00:18:44,260 --> 00:18:49,420
it can contact but also knows all the

00:18:47,679 --> 00:18:52,650
other nodes that start the replications

00:18:49,420 --> 00:18:56,320
of the object so they can by themselves

00:18:52,650 --> 00:18:58,660
just choose any other node contacted and

00:18:56,320 --> 00:19:05,170
read the object from there so that makes

00:18:58,660 --> 00:19:12,340
a parallel reading very efficient yeah

00:19:05,170 --> 00:19:14,559
so about placement groups as I mentioned

00:19:12,340 --> 00:19:16,750
before they just reduce the complexity

00:19:14,559 --> 00:19:19,390
of tracking each object and reduce

00:19:16,750 --> 00:19:24,820
calculating the time where where object

00:19:19,390 --> 00:19:27,040
resides and they are impacting the

00:19:24,820 --> 00:19:30,160
performance because as soon as one and

00:19:27,040 --> 00:19:31,809
node fails the data has to be duplicated

00:19:30,160 --> 00:19:34,630
again or replicated somewhere else and

00:19:31,809 --> 00:19:36,910
so and when the other nodes realized

00:19:34,630 --> 00:19:39,720
that one note is gone then they begin

00:19:36,910 --> 00:19:41,920
checking well I I'm also having

00:19:39,720 --> 00:19:44,679
replications of this specific placement

00:19:41,920 --> 00:19:46,360
group and then they are contacting other

00:19:44,679 --> 00:19:48,340
nodes and putting all the stuff they are

00:19:46,360 --> 00:19:51,250
they have stored in the placement groups

00:19:48,340 --> 00:19:53,350
that are now do not have enough for

00:19:51,250 --> 00:19:56,140
applications they begin storing this

00:19:53,350 --> 00:19:59,679
data these placement groups on other

00:19:56,140 --> 00:20:01,730
nodes as well and and depending on your

00:19:59,679 --> 00:20:04,309
cluster size

00:20:01,730 --> 00:20:08,419
well if you have few placement groups

00:20:04,309 --> 00:20:12,590
per note then you can this note can only

00:20:08,419 --> 00:20:14,179
contact five six seven are always these

00:20:12,590 --> 00:20:17,210
for example if you have only seven

00:20:14,179 --> 00:20:19,429
placement groups on one note it can just

00:20:17,210 --> 00:20:21,830
contact seven other at most seven other

00:20:19,429 --> 00:20:25,909
OSDs and begin writing and this data to

00:20:21,830 --> 00:20:28,880
seven other way notes for example if you

00:20:25,909 --> 00:20:32,779
have much more placement groups on the

00:20:28,880 --> 00:20:34,340
other hand then at some point it's not

00:20:32,779 --> 00:20:37,190
impair improving the performance anymore

00:20:34,340 --> 00:20:38,840
you know okay I have to mention that

00:20:37,190 --> 00:20:42,380
writing usually takes longer than

00:20:38,840 --> 00:20:43,880
reading so while one note is pushing all

00:20:42,380 --> 00:20:46,940
the data the other ones have to write

00:20:43,880 --> 00:20:49,700
and yeah so the bottleneck is usually

00:20:46,940 --> 00:20:53,659
the writing and the more - the more

00:20:49,700 --> 00:20:55,460
other notes the remaining OSD can write

00:20:53,659 --> 00:20:57,049
the faster the process is at some point

00:20:55,460 --> 00:20:59,240
of course the network is the bottleneck

00:20:57,049 --> 00:21:03,440
therefore having more placement groups

00:20:59,240 --> 00:21:05,929
does not improve performance but then

00:21:03,440 --> 00:21:08,769
you begin having other problems having

00:21:05,929 --> 00:21:12,340
too many placement groups is also again

00:21:08,769 --> 00:21:15,440
very expensive on the computing side

00:21:12,340 --> 00:21:17,450
okay now I'm hoping No okay so that's

00:21:15,440 --> 00:21:20,389
the the thing with live demos is

00:21:17,450 --> 00:21:25,370
beginning to start and usually it should

00:21:20,389 --> 00:21:34,020
not take that long - okay I have to plug

00:21:25,370 --> 00:21:37,320
it in again okay let's see

00:21:34,020 --> 00:21:41,130
to happen and now as soon as the

00:21:37,320 --> 00:21:43,560
raspberries come online again then okay

00:21:41,130 --> 00:21:45,150
maybe I can just explain currently

00:21:43,560 --> 00:21:48,210
what's going on I need to put aside the

00:21:45,150 --> 00:21:50,580
chameleon and I need to change the view

00:21:48,210 --> 00:21:52,110
and I can make maybe can put in

00:21:50,580 --> 00:21:55,410
fullscreen okay

00:21:52,110 --> 00:21:57,510
so currently there are two OSDs

00:21:55,410 --> 00:22:00,270
remaining the ones over here and you see

00:21:57,510 --> 00:22:03,510
that the two green LEDs are blinking

00:22:00,270 --> 00:22:06,270
that means and they are still more than

00:22:03,510 --> 00:22:08,970
50% of the monitors available and they

00:22:06,270 --> 00:22:13,110
can still decide what is going about

00:22:08,970 --> 00:22:16,140
what is going on in the cluster and yeah

00:22:13,110 --> 00:22:20,010
so as soon as the the now the new notes

00:22:16,140 --> 00:22:20,910
are coming online and you see they will

00:22:20,010 --> 00:22:24,180
soon

00:22:20,910 --> 00:22:28,500
contact the remaining notes and then it

00:22:24,180 --> 00:22:32,490
is a thing that one of the selling

00:22:28,500 --> 00:22:34,830
points of Ceph come in and now the

00:22:32,490 --> 00:22:37,470
cluster is beginning to redistribute the

00:22:34,830 --> 00:22:40,280
data so you can see that the notes in

00:22:37,470 --> 00:22:43,770
here are beginning to read and write and

00:22:40,280 --> 00:22:47,190
those notes over here will receive all

00:22:43,770 --> 00:22:48,960
the data that has been changed or yeah

00:22:47,190 --> 00:22:52,260
moved during the time they were offline

00:22:48,960 --> 00:22:55,740
so now the data is rebalanced of all

00:22:52,260 --> 00:22:57,090
notes and after a few minutes the

00:22:55,740 --> 00:23:00,270
cluster should be back to its original

00:22:57,090 --> 00:23:03,090
state because I did not change that mean

00:23:00,270 --> 00:23:05,070
not the original state state but the

00:23:03,090 --> 00:23:08,480
data basically should be back where it

00:23:05,070 --> 00:23:16,530
was before I took at the one rec offline

00:23:08,480 --> 00:23:22,320
okay and yeah while this is going on I

00:23:16,530 --> 00:23:25,110
can continue so there are two different

00:23:22,320 --> 00:23:28,520
modes of how staff handles the replicas

00:23:25,110 --> 00:23:31,530
replications one are one mode is called

00:23:28,520 --> 00:23:32,760
replicate yeah it's used in replicated

00:23:31,530 --> 00:23:35,310
pools that's basically what I was

00:23:32,760 --> 00:23:37,080
talking about the whole time and then

00:23:35,310 --> 00:23:39,030
there's another thing and because

00:23:37,080 --> 00:23:41,370
replications are expensive on the

00:23:39,030 --> 00:23:43,140
storage side you need if you have three

00:23:41,370 --> 00:23:45,600
replications you need three times the

00:23:43,140 --> 00:23:47,490
storage you need to purchase storing the

00:23:45,600 --> 00:23:50,460
file ones or object ones

00:23:47,490 --> 00:23:52,710
and yeah that just cost a lot of money

00:23:50,460 --> 00:23:54,779
if you have lot of data and then you can

00:23:52,710 --> 00:23:59,279
also do something that is similar to

00:23:54,779 --> 00:24:01,380
write AB to write five or six you split

00:23:59,279 --> 00:24:05,309
your data object into chunks and you

00:24:01,380 --> 00:24:07,620
create parity chunks for the data so you

00:24:05,309 --> 00:24:10,890
can for example split your object in two

00:24:07,620 --> 00:24:16,559
chunks and create one parity chunk then

00:24:10,890 --> 00:24:20,490
you can F one failure you can maintain

00:24:16,559 --> 00:24:22,890
the cluster with one failure so the data

00:24:20,490 --> 00:24:25,909
is still available when one cluster of a

00:24:22,890 --> 00:24:30,000
even wreck fails or one node fails but

00:24:25,909 --> 00:24:32,880
if another node fails then the data is

00:24:30,000 --> 00:24:35,370
not available anymore a in contrast to

00:24:32,880 --> 00:24:37,470
the replicated pool where one surviving

00:24:35,370 --> 00:24:42,149
copy is enough to have the data still

00:24:37,470 --> 00:24:45,059
available and that means in this case if

00:24:42,149 --> 00:24:48,000
I want to store the data in that

00:24:45,059 --> 00:24:52,679
replicated pools I will have to use

00:24:48,000 --> 00:24:55,919
three times the storage I the file

00:24:52,679 --> 00:24:59,760
usually uses and if I use an eraser

00:24:55,919 --> 00:25:01,500
coded pool it just uses when I use this

00:24:59,760 --> 00:25:03,720
congregation that I want to split the

00:25:01,500 --> 00:25:07,200
file into two parts I have one per chunk

00:25:03,720 --> 00:25:10,740
then I will use only a 50% overhead of

00:25:07,200 --> 00:25:13,740
my disks for storing the data but that

00:25:10,740 --> 00:25:16,620
comes at a cost if I want to read now

00:25:13,740 --> 00:25:18,690
this data from those d's I need to

00:25:16,620 --> 00:25:21,149
contact at least two OSDs

00:25:18,690 --> 00:25:23,130
and fetch the data from both and then

00:25:21,149 --> 00:25:26,850
calculate or put my file back together

00:25:23,130 --> 00:25:30,000
and with a CPU as though to cost CPU

00:25:26,850 --> 00:25:32,130
time I am reading data from much more Z

00:25:30,000 --> 00:25:38,580
butan reading data from erasure coded

00:25:32,130 --> 00:25:42,330
pools then from replicate tools when

00:25:38,580 --> 00:25:44,610
writing data to replicate pools it also

00:25:42,330 --> 00:25:49,679
takes a lot of bandwidth in the backend

00:25:44,610 --> 00:25:51,630
so the file is written to one OSD this

00:25:49,679 --> 00:25:53,909
has to contact to others or for others

00:25:51,630 --> 00:25:55,490
or maybe just one other OSD so it

00:25:53,909 --> 00:26:05,399
produces a large load on the back

00:25:55,490 --> 00:26:12,149
back-end Network and but yeah sorry I

00:26:05,399 --> 00:26:15,000
lost myself a to records

00:26:12,149 --> 00:26:17,720
yeah and on the other hand you can have

00:26:15,000 --> 00:26:22,980
more efficient data use a storage usage

00:26:17,720 --> 00:26:28,470
and try this off against CPU time okay

00:26:22,980 --> 00:26:34,700
now it's not working for some reason I'm

00:26:28,470 --> 00:26:37,110
not sure why and let's have a look and

00:26:34,700 --> 00:26:50,130
this is one of the reasons why you

00:26:37,110 --> 00:26:54,919
shouldn't do life okay and all my tests

00:26:50,130 --> 00:27:02,070
well it looks no it doesn't look fine

00:26:54,919 --> 00:27:03,960
yeah so it's still yeah okay some rules

00:27:02,070 --> 00:27:06,240
are broken I will just delete them for

00:27:03,960 --> 00:27:09,830
the sake of the demonstration and I need

00:27:06,240 --> 00:27:09,830
to put a beta microphone four seconds

00:27:37,700 --> 00:27:45,510
okay now I'm the pools should be removed

00:27:42,510 --> 00:27:47,010
and it should take another couple of

00:27:45,510 --> 00:27:49,500
minutes and now okay now the clusters

00:27:47,010 --> 00:27:52,200
screen again so there it's restored I

00:27:49,500 --> 00:27:57,779
can now create new pools okay and

00:27:52,200 --> 00:28:01,289
failure handling so in order to do all

00:27:57,779 --> 00:28:03,720
the things safe promises it can do and

00:28:01,289 --> 00:28:06,780
you need some user interaction you need

00:28:03,720 --> 00:28:10,139
to when when you just install Zef it

00:28:06,780 --> 00:28:12,690
treats every node similar to each edge

00:28:10,139 --> 00:28:13,470
when any other node it just puts the

00:28:12,690 --> 00:28:15,330
data on

00:28:13,470 --> 00:28:17,490
we know that is available if you want

00:28:15,330 --> 00:28:19,080
for example to have data distributed

00:28:17,490 --> 00:28:21,360
over different drags you have to

00:28:19,080 --> 00:28:23,490
manually tell the cluster well these

00:28:21,360 --> 00:28:25,320
nodes reside in one reg the other nodes

00:28:23,490 --> 00:28:28,470
another wreck and maybe you have a third

00:28:25,320 --> 00:28:30,390
wreck and those wrecks can be in rows of

00:28:28,470 --> 00:28:32,100
wrecks and the rows can again be in

00:28:30,390 --> 00:28:34,919
rooms or data centers so you can

00:28:32,100 --> 00:28:39,860
basically define the full structure you

00:28:34,919 --> 00:28:42,480
have in your data center and yeah

00:28:39,860 --> 00:28:44,820
therefore distribute or the data

00:28:42,480 --> 00:28:48,780
everywhere in your different rooms and

00:28:44,820 --> 00:28:52,500
guarantee a high availability and this

00:28:48,780 --> 00:28:55,289
is done with the crash map and the crash

00:28:52,500 --> 00:28:59,010
map that's when you d compile it it's

00:28:55,289 --> 00:29:03,270
just a text file where you can say well

00:28:59,010 --> 00:29:05,070
these always T's belong to one thing

00:29:03,270 --> 00:29:07,919
called packet and these buckets are in a

00:29:05,070 --> 00:29:10,020
hierarchy so one can reside in another

00:29:07,919 --> 00:29:13,049
bucket and these are basically the

00:29:10,020 --> 00:29:16,110
things that are the racks the notes and

00:29:13,049 --> 00:29:22,260
upper level our data center rooms and so

00:29:16,110 --> 00:29:24,000
on okay now what's going on when no

00:29:22,260 --> 00:29:31,640
sorry not what's going on I already

00:29:24,000 --> 00:29:34,260
mentioned and now when devices fail in I

00:29:31,640 --> 00:29:40,740
already mentioned that I can jump over

00:29:34,260 --> 00:29:43,470
that slide and now I can put data on

00:29:40,740 --> 00:29:46,200
different pools and I can define for

00:29:43,470 --> 00:29:47,669
each pool well and for the state and

00:29:46,200 --> 00:29:50,309
this pool is not that important I just

00:29:47,669 --> 00:29:52,650
want to replications because I don't

00:29:50,309 --> 00:29:55,260
want to waste my storage for them and I

00:29:52,650 --> 00:29:58,140
can decide well other objects are much

00:29:55,260 --> 00:30:01,020
more important maybe I want to have four

00:29:58,140 --> 00:30:04,320
replications and then on the other hand

00:30:01,020 --> 00:30:07,559
I can also use the writer coded pools to

00:30:04,320 --> 00:30:17,490
do this whole thing more storage

00:30:07,559 --> 00:30:20,940
efficient now if I use if I can don't

00:30:17,490 --> 00:30:22,980
define any structure for the cluster the

00:30:20,940 --> 00:30:24,720
advantage is that every other node can

00:30:22,980 --> 00:30:27,180
be used for applications so if one node

00:30:24,720 --> 00:30:29,640
fails any other node can jump in

00:30:27,180 --> 00:30:34,680
used for replacing the failed note and

00:30:29,640 --> 00:30:38,130
if I under hand begin defining another

00:30:34,680 --> 00:30:41,400
structure maybe Rex in this case and I'm

00:30:38,130 --> 00:30:42,900
telling the class - well put one object

00:30:41,400 --> 00:30:44,130
in the first rag one object run

00:30:42,900 --> 00:30:45,780
replication and second rack and the

00:30:44,130 --> 00:30:48,930
third replication replication and third

00:30:45,780 --> 00:30:51,030
wreck and one wreck fails then basically

00:30:48,930 --> 00:30:53,550
not all nodes are available for

00:30:51,030 --> 00:30:55,770
replication or if maybe just one node

00:30:53,550 --> 00:30:57,450
fails in the dirt wreck only the two

00:30:55,770 --> 00:31:00,180
remaining nodes in the third wreck are

00:30:57,450 --> 00:31:05,700
available for a replication therefore

00:31:00,180 --> 00:31:07,620
there are some disadvantages for using

00:31:05,700 --> 00:31:09,450
this but disadvantage with disadvantages

00:31:07,620 --> 00:31:12,150
may be going a little too far and you

00:31:09,450 --> 00:31:14,880
have to provision your cluster and

00:31:12,150 --> 00:31:17,490
taking this into account and so you need

00:31:14,880 --> 00:31:20,280
enough remaining nodes available and

00:31:17,490 --> 00:31:22,620
more important is are the monitor nodes

00:31:20,280 --> 00:31:27,090
you usually don't have that many monitor

00:31:22,620 --> 00:31:29,640
notes in your cluster and one scenario

00:31:27,090 --> 00:31:32,190
might be that you have to start several

00:31:29,640 --> 00:31:36,330
rooms in your building and you have

00:31:32,190 --> 00:31:38,070
maybe two M monitors in one room and

00:31:36,330 --> 00:31:40,740
three monitors in the other room

00:31:38,070 --> 00:31:43,560
now if the room with the three monitors

00:31:40,740 --> 00:31:45,990
it goes offline then basically your

00:31:43,560 --> 00:31:50,070
cluster stack it cannot operate anymore

00:31:45,990 --> 00:31:53,940
and so you usually should have a third

00:31:50,070 --> 00:31:56,040
side way well some monitors reside and

00:31:53,940 --> 00:31:59,460
you have two and two in each server room

00:31:56,040 --> 00:32:01,800
and maybe a fifth monitor in another

00:31:59,460 --> 00:32:04,530
location that is available or connected

00:32:01,800 --> 00:32:07,350
to both other locations so it's acting

00:32:04,530 --> 00:32:15,030
as a tie breaker in case of one room

00:32:07,350 --> 00:32:17,370
fails okay so if I'm using if I'm

00:32:15,030 --> 00:32:20,970
distributing my objects over different

00:32:17,370 --> 00:32:22,800
drags I'm also having advantage that for

00:32:20,970 --> 00:32:24,180
example if infrastructure fails to

00:32:22,800 --> 00:32:25,980
switch that is connecting the whole rack

00:32:24,180 --> 00:32:28,140
or the power supply unit that is

00:32:25,980 --> 00:32:32,130
supplying the whole rack is going

00:32:28,140 --> 00:32:35,010
offline then the cluster is still able

00:32:32,130 --> 00:32:38,520
to operate normally and of course it

00:32:35,010 --> 00:32:40,920
takes some time to redistribute the data

00:32:38,520 --> 00:32:46,110
use already saw that when I

00:32:40,920 --> 00:32:48,870
lack of the power supply for the third

00:32:46,110 --> 00:32:51,660
rack it takes may well in this case it

00:32:48,870 --> 00:32:55,410
took much more because the cluster

00:32:51,660 --> 00:32:57,300
yeah failed totally and I can try again

00:32:55,410 --> 00:32:59,250
maybe it's going to work this time but

00:32:57,300 --> 00:33:06,120
now I'm not going to do this I'm just

00:32:59,250 --> 00:33:08,550
going to take away one I'm I'm yeah not

00:33:06,120 --> 00:33:11,340
that hard to it not right now okay so I

00:33:08,550 --> 00:33:15,510
just took one note of line and what

00:33:11,340 --> 00:33:19,950
should happen is I took the upper note

00:33:15,510 --> 00:33:21,570
here and the rack off line and at some

00:33:19,950 --> 00:33:23,190
point the other notes again should

00:33:21,570 --> 00:33:25,800
realize that this notice gun and you

00:33:23,190 --> 00:33:28,950
will see that the red and green LEDs

00:33:25,800 --> 00:33:30,600
start notice is not the thing that now

00:33:28,950 --> 00:33:34,400
it's going to start now you see that

00:33:30,600 --> 00:33:36,330
they begin blinking a lot and they begin

00:33:34,400 --> 00:33:39,990
redistributing the data that was stored

00:33:36,330 --> 00:33:43,140
on the third note a first note on a

00:33:39,990 --> 00:33:46,830
class the track and yeah storing it on

00:33:43,140 --> 00:33:48,960
the remaining nodes and of course and

00:33:46,830 --> 00:33:52,620
they have to read it from the notes in

00:33:48,960 --> 00:34:00,600
the other racks a oh and yeah talking

00:33:52,620 --> 00:34:04,110
it's fine okay and then there's one

00:34:00,600 --> 00:34:07,470
thing that is important when using um

00:34:04,110 --> 00:34:09,960
those different pools erasure coded and

00:34:07,470 --> 00:34:11,940
replicated pools and if you you if

00:34:09,960 --> 00:34:14,580
you're using in a replicated pool it

00:34:11,940 --> 00:34:16,590
does not matter which right does matter

00:34:14,580 --> 00:34:19,590
which note replaces the note that is

00:34:16,590 --> 00:34:21,030
gone and if you want if the first node

00:34:19,590 --> 00:34:23,399
fails that is the primary node that is

00:34:21,030 --> 00:34:27,570
contacted in case someone wants to ride

00:34:23,399 --> 00:34:30,030
an object to the pool and then you want

00:34:27,570 --> 00:34:31,740
if this first note goes goes offline you

00:34:30,030 --> 00:34:34,350
want another node that already has all

00:34:31,740 --> 00:34:36,360
copies to immediately jump in so what

00:34:34,350 --> 00:34:38,399
you're doing is you're queuing all the

00:34:36,360 --> 00:34:44,460
OSDs that are used for the placement

00:34:38,399 --> 00:34:48,590
group and as soon as one OSD fails like

00:34:44,460 --> 00:34:52,589
in here em they are all jumping in one

00:34:48,590 --> 00:34:56,219
place ahead in the queue and then the

00:34:52,589 --> 00:34:59,460
next OSD is used to replicate the data

00:34:56,219 --> 00:35:01,559
there and this is especially important

00:34:59,460 --> 00:35:03,210
if the first node fails and then the

00:35:01,559 --> 00:35:05,009
second notice immediately Daren can

00:35:03,210 --> 00:35:07,079
continue operating the cluster or the

00:35:05,009 --> 00:35:08,579
pool sorry and more specific the

00:35:07,079 --> 00:35:11,249
placement group this is done the

00:35:08,579 --> 00:35:14,339
placement group level and on the other

00:35:11,249 --> 00:35:16,969
hand if you are using a racial coded

00:35:14,339 --> 00:35:21,989
pool the position of the OSD s and the

00:35:16,969 --> 00:35:25,019
is important that means if the object is

00:35:21,989 --> 00:35:27,479
split split into different into chunks

00:35:25,019 --> 00:35:29,039
then the first part is started first OSD

00:35:27,479 --> 00:35:32,670
is second part in a second OSD and so on

00:35:29,039 --> 00:35:34,380
and now if one node fails they should

00:35:32,670 --> 00:35:37,200
not change the position because then all

00:35:34,380 --> 00:35:40,589
the chunks have to be moved to the new

00:35:37,200 --> 00:35:43,170
two new nodes in the placement group and

00:35:40,589 --> 00:35:45,900
therefore what you would like to do is

00:35:43,170 --> 00:35:47,969
you just replace the failed node with

00:35:45,900 --> 00:35:49,619
any other node that is somewhere in the

00:35:47,969 --> 00:35:52,529
cluster and you begin replicating the

00:35:49,619 --> 00:35:54,089
data from on this new node so it you do

00:35:52,529 --> 00:35:56,339
not want to change the order of the

00:35:54,089 --> 00:35:59,430
remaining nodes because that would mean

00:35:56,339 --> 00:36:04,579
additional time moving data within the

00:35:59,430 --> 00:36:12,749
placement group or placement groups okay

00:36:04,579 --> 00:36:16,410
I think I'm already pretty much yeah

00:36:12,749 --> 00:36:20,249
okay and I will just black that in red

00:36:16,410 --> 00:36:27,420
again and sorry the demonstration part

00:36:20,249 --> 00:36:28,920
came a little short because it took too

00:36:27,420 --> 00:36:33,390
much time to recover from the first

00:36:28,920 --> 00:36:35,700
failure and yeah so just to give you

00:36:33,390 --> 00:36:40,019
some insight what the raspberries are

00:36:35,700 --> 00:36:41,999
capable of I ran the parents and

00:36:40,019 --> 00:36:46,829
performance tests and you can write

00:36:41,999 --> 00:36:49,559
around 1.6 megabytes per second into the

00:36:46,829 --> 00:36:52,380
cluster I did not do any read tests

00:36:49,559 --> 00:36:56,309
because I did not have time

00:36:52,380 --> 00:36:58,829
and it's nice for displaying on the LEDs

00:36:56,309 --> 00:37:01,769
what is happening but performance wise

00:36:58,829 --> 00:37:03,150
it's really bad so you do not want to

00:37:01,769 --> 00:37:14,400
use it even at home for playing around

00:37:03,150 --> 00:37:18,269
and yeah what you can also do with the

00:37:14,400 --> 00:37:19,680
different sorts of pools you can do

00:37:18,269 --> 00:37:23,670
something that is called cash gearing

00:37:19,680 --> 00:37:27,180
you can use an easy pool in the

00:37:23,670 --> 00:37:29,250
background and put a replicated pool in

00:37:27,180 --> 00:37:32,460
front of it and all objects that are

00:37:29,250 --> 00:37:35,579
written to this cash at your pool and

00:37:32,460 --> 00:37:39,390
first will be put into the replicated

00:37:35,579 --> 00:37:43,140
pool and they reside there for some

00:37:39,390 --> 00:37:45,660
period of time and then at some point

00:37:43,140 --> 00:37:49,019
they will be written to the cold storage

00:37:45,660 --> 00:37:51,900
and to the EC array circle a racially

00:37:49,019 --> 00:37:54,990
coded pool so if you when you want to

00:37:51,900 --> 00:37:58,500
access this object and it's already in

00:37:54,990 --> 00:38:01,650
the replicated pool and it is residing

00:37:58,500 --> 00:38:04,019
above the a erasure coded pool and you

00:38:01,650 --> 00:38:07,230
can access this file again with several

00:38:04,019 --> 00:38:10,259
clients from several OSDs at the same

00:38:07,230 --> 00:38:13,609
time and only if you're not using this

00:38:10,259 --> 00:38:16,680
object anymore it's flushed from the

00:38:13,609 --> 00:38:19,619
from the replicated pool and move to the

00:38:16,680 --> 00:38:22,140
well it's always moved to the EC rater

00:38:19,619 --> 00:38:23,700
correct pool but then as soon as it's

00:38:22,140 --> 00:38:26,039
not important not that important anymore

00:38:23,700 --> 00:38:27,630
and you need it again then you have to

00:38:26,039 --> 00:38:30,690
retrieve it from the erasure coded pool

00:38:27,630 --> 00:38:35,039
so that's basically a compromise between

00:38:30,690 --> 00:38:38,460
both worlds you get some improvements in

00:38:35,039 --> 00:38:40,559
performance at these sometimes it

00:38:38,460 --> 00:38:44,759
depends heavily on what you actually

00:38:40,559 --> 00:38:46,890
want to do with that and you also get

00:38:44,759 --> 00:38:52,200
some efficiency out of the erasure coded

00:38:46,890 --> 00:38:54,390
pool storage mechanism okay M yeah I'm

00:38:52,200 --> 00:38:57,059
going to jump over that and I'm going to

00:38:54,390 --> 00:38:59,460
stop here because I wanted to leave 20

00:38:57,059 --> 00:39:01,380
minutes for question and answers and if

00:38:59,460 --> 00:39:04,559
there are no questions and answers I

00:39:01,380 --> 00:39:05,920
could do some try again to do some

00:39:04,559 --> 00:39:07,569
demonstrations but for

00:39:05,920 --> 00:39:19,720
I want to take questions if there are

00:39:07,569 --> 00:39:23,440
any yeah there's one yes microphone when

00:39:19,720 --> 00:39:23,890
you'll find the third drag this sorry

00:39:23,440 --> 00:39:27,369
what

00:39:23,890 --> 00:39:30,460
Wow it's probably but when you offline

00:39:27,369 --> 00:39:33,309
the star drag and you put it online

00:39:30,460 --> 00:39:36,940
again it started writing data but it

00:39:33,309 --> 00:39:40,500
already had it and the OS dist should

00:39:36,940 --> 00:39:43,150
have should have known they have it I'm

00:39:40,500 --> 00:39:46,900
I do not exactly know how that is

00:39:43,150 --> 00:39:49,869
handled in detail and but usually you

00:39:46,900 --> 00:39:51,760
have to assume that during the time it

00:39:49,869 --> 00:39:55,720
was offline at least some of the data

00:39:51,760 --> 00:39:58,660
changed so at least needs to compare

00:39:55,720 --> 00:40:00,520
what happened and probably it's doing

00:39:58,660 --> 00:40:01,960
that but I'm not sure about I don't know

00:40:00,520 --> 00:40:04,299
basically I don't know the answer to

00:40:01,960 --> 00:40:06,400
that question maybe someone else in room

00:40:04,299 --> 00:40:11,250
dance there are some people involved and

00:40:06,400 --> 00:40:11,250
that could know it but yeah I do not

00:40:12,089 --> 00:40:29,829
other questions and rec and notes so if

00:40:26,260 --> 00:40:31,660
I have two rooms look maybe I understood

00:40:29,829 --> 00:40:34,030
it wrong but like with that also mean

00:40:31,660 --> 00:40:36,400
like the replication in that rooms like

00:40:34,030 --> 00:40:39,940
would they cross replicate or if like

00:40:36,400 --> 00:40:42,460
yes so if I I need to manually define

00:40:39,940 --> 00:40:44,890
that so I can I have notes and one room

00:40:42,460 --> 00:40:48,579
notes in the other room and by itself

00:40:44,890 --> 00:40:52,240
ACEF will treat each notes with equally

00:40:48,579 --> 00:40:55,450
so it could by accident all replications

00:40:52,240 --> 00:40:57,549
could land in one room but if I tell

00:40:55,450 --> 00:40:59,799
safe well these notes are in one room

00:40:57,549 --> 00:41:01,540
and these nodes on the other room and I

00:40:59,799 --> 00:41:03,520
tell it I want one right brick at least

00:41:01,540 --> 00:41:05,650
one replication in each room then it

00:41:03,520 --> 00:41:10,490
will do that so but I have manually I

00:41:05,650 --> 00:41:15,470
have to tell it manually to the soul

00:41:10,490 --> 00:41:18,680
okay any other questions no okay

00:41:15,470 --> 00:41:21,430
I will maybe go back a few slides and

00:41:18,680 --> 00:41:32,570
then try to if you are interested and

00:41:21,430 --> 00:41:36,080
yeah to do the where did I lose okay I

00:41:32,570 --> 00:41:38,839
think what I want to do first is again

00:41:36,080 --> 00:41:40,790
create I am not sure whether there is

00:41:38,839 --> 00:42:03,500
still existing pool in the cluster I

00:41:40,790 --> 00:42:05,510
first need to check that and so I

00:42:03,500 --> 00:42:07,460
deleted all the pools so all the

00:42:05,510 --> 00:42:11,660
demonstration I did after deleting the

00:42:07,460 --> 00:42:13,430
pools was basically bogus so I first

00:42:11,660 --> 00:42:31,570
need to create another pool and I will

00:42:13,430 --> 00:42:46,070
just use a replicated pool right now and

00:42:31,570 --> 00:42:48,530
of course okay now

00:42:46,070 --> 00:42:50,690
to create a pool and you will see that

00:42:48,530 --> 00:42:53,120
the no you don't because you cannot see

00:42:50,690 --> 00:42:55,960
the webcam and you see now that the

00:42:53,120 --> 00:42:59,030
nodes are starting to work and just

00:42:55,960 --> 00:43:01,070
creating the placement groups on all the

00:42:59,030 --> 00:43:03,650
nodes already takes some time in the

00:43:01,070 --> 00:43:06,640
cluster and during that time I'm not

00:43:03,650 --> 00:43:09,320
sure no you cannot really see it the

00:43:06,640 --> 00:43:13,160
here the yellow LED is on that means

00:43:09,320 --> 00:43:17,540
currently the cluster is telling me that

00:43:13,160 --> 00:43:21,790
it's not really fully available right

00:43:17,540 --> 00:43:24,830
now it's it's working on creating all

00:43:21,790 --> 00:43:30,470
placement groups that I want to have so

00:43:24,830 --> 00:43:34,910
I told it to have 128 placement groups

00:43:30,470 --> 00:43:37,430
in the cluster which is not the best

00:43:34,910 --> 00:43:40,610
thing for this amount of nodes you

00:43:37,430 --> 00:43:43,760
should use more like 512 but for

00:43:40,610 --> 00:43:47,030
performance reasons with the recipes and

00:43:43,760 --> 00:43:49,700
yeah it's just faster creating fewer

00:43:47,030 --> 00:43:50,810
placement groups in my experience

00:43:49,700 --> 00:43:59,930
amateur butts

00:43:50,810 --> 00:44:03,170
and first tests yeah yes okay yeah okay

00:43:59,930 --> 00:44:04,820
what I'm running on the recipes I'm

00:44:03,170 --> 00:44:09,580
running on the raspberry PI's I'm

00:44:04,820 --> 00:44:11,810
running and the 42.2 a truce image and

00:44:09,580 --> 00:44:17,120
initially I wanted to use the full

00:44:11,810 --> 00:44:20,570
openSUSE safe packages and I had some

00:44:17,120 --> 00:44:22,670
trouble and config configuring safe but

00:44:20,570 --> 00:44:26,270
I'm not sure whether that was due to me

00:44:22,670 --> 00:44:30,080
or due to packages and the end I decided

00:44:26,270 --> 00:44:34,730
to use the sleep packages and then it

00:44:30,080 --> 00:44:38,410
worked fine now I'm quite confident that

00:44:34,730 --> 00:44:40,580
it was my okay that I was not capable of

00:44:38,410 --> 00:44:43,220
configuring it correctly the first time

00:44:40,580 --> 00:44:45,950
so I'm pretty confident that it should

00:44:43,220 --> 00:44:49,520
work with the openSUSE packages as well

00:44:45,950 --> 00:44:53,150
so yeah no differences and I use the

00:44:49,520 --> 00:44:55,220
safe deploy scripts to set up the

00:44:53,150 --> 00:44:56,360
cluster because and there's another

00:44:55,220 --> 00:44:58,340
thing available called

00:44:56,360 --> 00:44:59,599
deep sea that's there are some scripts

00:44:58,340 --> 00:45:04,220
for salt

00:44:59,599 --> 00:45:06,680
and that takes a long time to execute on

00:45:04,220 --> 00:45:09,170
the recipes so and have it using the

00:45:06,680 --> 00:45:10,849
safe deploys just a little faster in the

00:45:09,170 --> 00:45:13,729
end and they are not that many notes

00:45:10,849 --> 00:45:19,509
that I really need to use salt to manage

00:45:13,729 --> 00:45:19,509
all the notes okay yes another question

00:45:20,589 --> 00:45:25,849
and yeah you're using thusly

00:45:23,720 --> 00:45:28,700
repositories on openSUSE and you are

00:45:25,849 --> 00:45:31,309
putting the recipes with PXE or no and

00:45:28,700 --> 00:45:33,799
the operating though I used the choose

00:45:31,309 --> 00:45:38,059
image that's available for the SD card

00:45:33,799 --> 00:45:41,359
for raspberry PI's and flash the images

00:45:38,059 --> 00:45:43,130
on the SD cards put them and then I

00:45:41,359 --> 00:45:46,460
added this little posit ori afterwards

00:45:43,130 --> 00:45:53,839
and it used them only to install safe

00:45:46,460 --> 00:45:58,329
and yeah safe deploy okay any other

00:45:53,839 --> 00:46:01,160
questions or okay okay and now the

00:45:58,329 --> 00:46:03,950
cluster is finished with creating the

00:46:01,160 --> 00:46:09,410
place placement groups and the pool and

00:46:03,950 --> 00:46:11,989
now i can write an object into the pool

00:46:09,410 --> 00:46:15,289
i will just write string hello world and

00:46:11,989 --> 00:46:18,140
an object called test object and when i

00:46:15,289 --> 00:46:20,150
hit enter you see it already takes a lot

00:46:18,140 --> 00:46:22,640
of time just to store the string hello

00:46:20,150 --> 00:46:29,539
world it took maybe three seconds to do

00:46:22,640 --> 00:46:31,279
that and i will just move the command

00:46:29,539 --> 00:46:36,440
line over here and the webcam over here

00:46:31,279 --> 00:46:39,680
and okay it's still working on things i

00:46:36,440 --> 00:46:41,479
do not know exactly but now when I write

00:46:39,680 --> 00:46:44,329
the object again you see the red lights

00:46:41,479 --> 00:46:46,430
jumping or switching on on some know

00:46:44,329 --> 00:46:49,880
whatever maybe turn it a little to the

00:46:46,430 --> 00:46:52,369
side and you can see the LEDs better and

00:46:49,880 --> 00:46:56,450
you see it then there's one LED coming

00:46:52,369 --> 00:47:00,109
on line here and a reverse one here and

00:46:56,450 --> 00:47:03,049
here I will just do it again and you see

00:47:00,109 --> 00:47:07,119
that they are stored actually on the

00:47:03,049 --> 00:47:09,979
bottom notes of each rec so currently

00:47:07,119 --> 00:47:11,539
the bottom notes of each rec are used to

00:47:09,979 --> 00:47:13,130
start a to replica or the three

00:47:11,539 --> 00:47:17,000
applications of the object so

00:47:13,130 --> 00:47:22,670
I will just pull the plaque on this and

00:47:17,000 --> 00:47:25,790
this node over here and that could be a

00:47:22,670 --> 00:47:29,210
problem because maybe I switched off the

00:47:25,790 --> 00:47:37,910
primary node and when I want to now read

00:47:29,210 --> 00:47:41,840
the object get test object test object

00:47:37,910 --> 00:47:45,200
and write it to the command line okay

00:47:41,840 --> 00:47:48,410
now yeah I probably took offline the

00:47:45,200 --> 00:47:53,650
primary node and the cluster first needs

00:47:48,410 --> 00:47:55,730
to realize that the node is gone and or

00:47:53,650 --> 00:47:57,890
sorry not the primary node but the notes

00:47:55,730 --> 00:48:00,980
that the client wanted to contact right

00:47:57,890 --> 00:48:06,620
now so any case it takes some time for

00:48:00,980 --> 00:48:09,050
the pool to placement groups to get

00:48:06,620 --> 00:48:12,260
up-to-date and then the data should

00:48:09,050 --> 00:48:15,950
basically be available again let's wait

00:48:12,260 --> 00:48:18,680
and see what is going to happen and we

00:48:15,950 --> 00:48:22,100
can during that time just run safe

00:48:18,680 --> 00:48:33,680
status and see what is going on and

00:48:22,100 --> 00:48:37,610
detail so okay yeah 61 placement groups

00:48:33,680 --> 00:48:41,660
are still totally fine another 67 are

00:48:37,610 --> 00:48:46,120
well not good right now and over time

00:48:41,660 --> 00:48:50,890
this should changed I should change and

00:48:46,120 --> 00:48:54,370
more and more placement groups should be

00:48:50,890 --> 00:48:59,120
should be back up running yeah so now

00:48:54,370 --> 00:49:03,050
most of them are activating that means

00:48:59,120 --> 00:49:12,700
there yeah now 74 are already back up

00:49:03,050 --> 00:49:15,850
again sorry on the raspberry PI's I

00:49:12,700 --> 00:49:15,850
haven't tried

00:49:19,580 --> 00:49:27,360
yeah because and 42.3 calamari will not

00:49:24,900 --> 00:49:32,580
be supported anymore so except deploy

00:49:27,360 --> 00:49:40,340
neither so you should use deep sea with

00:49:32,580 --> 00:49:42,900
the next release anyway okay now all the

00:49:40,340 --> 00:49:45,060
basement groups are fine again and I

00:49:42,900 --> 00:49:47,550
should be able to read the object yeah

00:49:45,060 --> 00:49:49,500
there it is so it's working again and

00:49:47,550 --> 00:49:52,310
now you see the reading is actually much

00:49:49,500 --> 00:49:55,800
faster than writing and I do not have

00:49:52,310 --> 00:50:00,840
more clients here but yeah well in the

00:49:55,800 --> 00:50:04,530
end it should scale okay now let's jump

00:50:00,840 --> 00:50:07,560
back Oh what I want to do next and I

00:50:04,530 --> 00:50:17,430
have to okay and I've got a few minutes

00:50:07,560 --> 00:50:19,140
left and maybe I will just mmm oh I

00:50:17,430 --> 00:50:21,750
think I will stop here and if you're

00:50:19,140 --> 00:50:23,970
interested just I will go over there and

00:50:21,750 --> 00:50:26,100
and if you have any questions you can

00:50:23,970 --> 00:50:28,500
contact me afterwards and I will free

00:50:26,100 --> 00:50:30,430
the stage for the next talk okay thank

00:50:28,500 --> 00:50:38,130
you

00:50:30,430 --> 00:50:38,130

YouTube URL: https://www.youtube.com/watch?v=9jjUygE8Wk4


