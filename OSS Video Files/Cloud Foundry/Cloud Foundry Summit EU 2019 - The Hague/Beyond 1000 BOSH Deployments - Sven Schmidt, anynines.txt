Title: Beyond 1000 BOSH Deployments - Sven Schmidt, anynines
Publication date: 2019-09-13
Playlist: Cloud Foundry Summit EU 2019 - The Hague
Description: 
	Beyond 1000 BOSH Deployments - Sven Schmidt, anynines 

How do you operate over 1,200 deployments on a single BOSH Director? In the past many talks have had the Topic of Cloud Foundry at scale. But how about the underlying automation layer? BOSH has its own set of challenges and limits for running VMs and Deployments at scale. Learn which obstacles and limits came up and how we solved them with the help of the BOSH core development team. Learn how we monitor the directors, be it via logging and metrics or performance indicators. Weâ€™ll also show you how we automate BOSH itself to ensure the best experience for end users, and to keep them blissfully unaware of the complexity of the processes working on their behalf After this talk you will also be able to run at least 1,200 deployments on your directors. 

For more info: https://www.cloudfoundry.org/
Captions: 
	00:00:00,060 --> 00:00:08,130
hello I don't I hope people are still

00:00:05,759 --> 00:00:09,750
coming and don't my and if they come

00:00:08,130 --> 00:00:17,340
they don't mind that we started without

00:00:09,750 --> 00:00:20,060
them let's talk about wait wait a second

00:00:17,340 --> 00:00:25,050
I think I killed my mouse is it um

00:00:20,060 --> 00:00:31,460
welcome to my talk beyond 1000 Bosch

00:00:25,050 --> 00:00:31,460
deployments Andy clickers empty

00:00:35,270 --> 00:00:42,360
sorry it's Google slides company

00:00:37,980 --> 00:00:45,300
mandatory so I welcome you to the talk

00:00:42,360 --> 00:00:47,070
it's nice that you have all come it's my

00:00:45,300 --> 00:00:52,949
first conference talk so I hope I don't

00:00:47,070 --> 00:00:55,739
rush through it so the title is stolen

00:00:52,949 --> 00:00:59,129
from Julian our CEO which a few years

00:00:55,739 --> 00:01:01,469
ago made the theoretical our thoughts

00:00:59,129 --> 00:01:04,949
about how it do you deploy 1000 data

00:01:01,469 --> 00:01:06,750
services and last year we reached 1000

00:01:04,949 --> 00:01:10,110
data services on a single port director

00:01:06,750 --> 00:01:13,979
and this is how the the issues we faced

00:01:10,110 --> 00:01:17,909
and the fun we had and let's start with

00:01:13,979 --> 00:01:21,689
introducing one second that was one too

00:01:17,909 --> 00:01:23,930
many I'm suin I'm at any nine since 2015

00:01:21,689 --> 00:01:28,140
I'm an ordained minister

00:01:23,930 --> 00:01:29,939
in New Zealand I'm working with cloud

00:01:28,140 --> 00:01:33,210
foundry Porsche and lately kubernetes

00:01:29,939 --> 00:01:37,380
and enhance and you find me on twitter

00:01:33,210 --> 00:01:40,619
as hallelujah which is also where I'm

00:01:37,380 --> 00:01:43,290
also from so I mentor in EVE Online and

00:01:40,619 --> 00:01:44,820
it's also where this slack avatar comes

00:01:43,290 --> 00:01:46,950
from and the name comes from so it's

00:01:44,820 --> 00:01:49,020
basically the only twitter account i

00:01:46,950 --> 00:01:51,360
ever had for a longer time because it

00:01:49,020 --> 00:01:53,340
was just something I like and I'm

00:01:51,360 --> 00:01:56,880
responsible for all the office plants in

00:01:53,340 --> 00:02:01,520
the ananias office I got a complaint

00:01:56,880 --> 00:02:04,530
once to HR the office is too green so

00:02:01,520 --> 00:02:07,200
but enough of the introductions let's

00:02:04,530 --> 00:02:08,940
start with the road to 1200 deployments

00:02:07,200 --> 00:02:12,810
because I have like we don't we don't

00:02:08,940 --> 00:02:13,710
only have 2000 we have we have 1214 was

00:02:12,810 --> 00:02:18,870
our mo

00:02:13,710 --> 00:02:21,330
our eyes peak and so first is probably

00:02:18,870 --> 00:02:24,270
the question what the hell did you do to

00:02:21,330 --> 00:02:26,550
get 1200 Bosch deployments on a single

00:02:24,270 --> 00:02:28,380
brush director well it's pretty simple

00:02:26,550 --> 00:02:31,170
we are the users of the ananias data

00:02:28,380 --> 00:02:32,970
service framework and as anyone who has

00:02:31,170 --> 00:02:35,100
have heard about it we are using

00:02:32,970 --> 00:02:37,710
dedicated instances and this is an

00:02:35,100 --> 00:02:40,820
architectural diagram so basically if

00:02:37,710 --> 00:02:45,720
you do create service the broker

00:02:40,820 --> 00:02:49,050
instructs a deployer which deploys the

00:02:45,720 --> 00:02:53,070
VMS on Bosh so each and every one of

00:02:49,050 --> 00:02:58,740
these 1200 deployments is one data

00:02:53,070 --> 00:03:02,460
service that a customer uses and it

00:02:58,740 --> 00:03:04,620
works pretty well so far so first in our

00:03:02,460 --> 00:03:06,450
usual environments we don't only have

00:03:04,620 --> 00:03:07,830
one Bosh because we were thinking at the

00:03:06,450 --> 00:03:10,500
beginning yeah but if we have a Bosch

00:03:07,830 --> 00:03:14,370
with 200 deployments what happens if 50

00:03:10,500 --> 00:03:16,500
databases and the runtime breaks what

00:03:14,370 --> 00:03:20,070
should we repair first so we decided we

00:03:16,500 --> 00:03:23,520
have three Portia's so first we have the

00:03:20,070 --> 00:03:25,620
over Bosch which is there to deploy to

00:03:23,520 --> 00:03:28,050
run time and only dear on term entity

00:03:25,620 --> 00:03:30,240
under Bosch we have the under Bush which

00:03:28,050 --> 00:03:32,700
deploys the service brokers and all

00:03:30,240 --> 00:03:35,430
service VMs this is also the the one

00:03:32,700 --> 00:03:38,370
with 1200 deployments the other ones up

00:03:35,430 --> 00:03:39,900
below 20 and the utility Bosch which is

00:03:38,370 --> 00:03:42,270
there for all the utilities like

00:03:39,900 --> 00:03:45,000
inception building and this and the

00:03:42,270 --> 00:03:50,700
Prometheus monitoring I'm sorry we don't

00:03:45,000 --> 00:03:52,590
use the CF metric store already so with

00:03:50,700 --> 00:03:54,300
those three posh directors it's pretty

00:03:52,590 --> 00:03:57,330
straightforward but here we always

00:03:54,300 --> 00:04:01,290
already learned some lessons in our trip

00:03:57,330 --> 00:04:02,760
so first the over NTU toothbrush are

00:04:01,290 --> 00:04:04,770
created with create ends because the

00:04:02,760 --> 00:04:06,780
over Bush was the first and then after

00:04:04,770 --> 00:04:09,990
why we thought we should probably put

00:04:06,780 --> 00:04:12,060
the monitoring somewhere else so we have

00:04:09,990 --> 00:04:13,880
two characters which were created with

00:04:12,060 --> 00:04:16,890
create and vent

00:04:13,880 --> 00:04:18,840
today we che and we in the meantime we

00:04:16,890 --> 00:04:21,390
changed our process so we deployed a

00:04:18,840 --> 00:04:24,210
youtube push with create M and then we

00:04:21,390 --> 00:04:26,040
deployed the other two directors or as

00:04:24,210 --> 00:04:27,600
we now have it we have multiple young

00:04:26,040 --> 00:04:28,950
under pushes because we

00:04:27,600 --> 00:04:33,330
want to see what happens if you have

00:04:28,950 --> 00:04:35,670
2,000 on it and so so the youtubes

00:04:33,330 --> 00:04:37,470
pushes all so now they're responsible to

00:04:35,670 --> 00:04:41,070
deploy all other Bosch directors which

00:04:37,470 --> 00:04:43,380
we use and we we got the great idea hey

00:04:41,070 --> 00:04:45,060
we have cret up let's merge those Crudup

00:04:43,380 --> 00:04:46,860
so we only have one crate up and we used

00:04:45,060 --> 00:04:50,090
the one which was co located on the

00:04:46,860 --> 00:04:52,860
other posh but there's the problem that

00:04:50,090 --> 00:04:54,510
suddenly nobody can create a service if

00:04:52,860 --> 00:04:57,000
the ov approach is updating because the

00:04:54,510 --> 00:04:59,850
over boss has the credible okay turd so

00:04:57,000 --> 00:05:02,400
this is why we went over and now the you

00:04:59,850 --> 00:05:06,150
to its Bosch deploys H a deployment

00:05:02,400 --> 00:05:07,980
consisting of UAA and crap which is then

00:05:06,150 --> 00:05:12,540
used by the Bosch directors for their

00:05:07,980 --> 00:05:14,610
store and this way we still have it in a

00:05:12,540 --> 00:05:17,040
way that all directors can access it

00:05:14,610 --> 00:05:20,430
without any director upgrade interfering

00:05:17,040 --> 00:05:23,940
and we decided back then to use an

00:05:20,430 --> 00:05:26,190
external RDS and while you think yeah we

00:05:23,940 --> 00:05:28,560
just take an Amazon RDS it solves all

00:05:26,190 --> 00:05:30,000
our problems no it doesn't it just moves

00:05:28,560 --> 00:05:32,550
to problems somewhere else

00:05:30,000 --> 00:05:36,510
but more about that later because that's

00:05:32,550 --> 00:05:39,330
at least two of the issues we had so

00:05:36,510 --> 00:05:41,190
let's start it so we have to push setup

00:05:39,330 --> 00:05:43,460
we have everything running to run times

00:05:41,190 --> 00:05:47,760
ready the first customers hit in January

00:05:43,460 --> 00:05:51,660
2018 let's start running it and here's a

00:05:47,760 --> 00:05:53,790
and as first let's bring bring what the

00:05:51,660 --> 00:05:56,880
issue issues we had at the beginning I

00:05:53,790 --> 00:06:00,120
Oh credits they are extremely fun so

00:05:56,880 --> 00:06:03,150
Amazon new system gzp also uses them but

00:06:00,120 --> 00:06:05,340
since we don't use TCP it's about AWS

00:06:03,150 --> 00:06:09,750
and AWS has the biggest system so it

00:06:05,340 --> 00:06:14,670
like we fell over it like 10 12 times so

00:06:09,750 --> 00:06:17,430
on Amazon they use the default this type

00:06:14,670 --> 00:06:19,620
GP 2 which is an SSD volume and the one

00:06:17,430 --> 00:06:23,370
and at volume those volumes are limited

00:06:19,620 --> 00:06:25,680
by I ups and you get 3i ops per gigabyte

00:06:23,370 --> 00:06:27,750
of disk size so the bigger your disk the

00:06:25,680 --> 00:06:31,140
more you are allowed to do on that disk

00:06:27,750 --> 00:06:33,650
so you don't have it on magnetic stores

00:06:31,140 --> 00:06:37,380
there is the bandwidth to limitation and

00:06:33,650 --> 00:06:40,260
we have on stage the director teh RDS

00:06:37,380 --> 00:06:41,520
database runs on GP 2 and on protection

00:06:40,260 --> 00:06:44,340
it runs on standard

00:06:41,520 --> 00:06:46,730
which is basically magnet disks low

00:06:44,340 --> 00:06:48,810
Amazon doesn't recommend it anymore and

00:06:46,730 --> 00:06:52,889
deprecated it and you shouldn't should

00:06:48,810 --> 00:06:56,850
never even touch it and you can see the

00:06:52,889 --> 00:06:58,590
disk I ops budget in cloud watch but

00:06:56,850 --> 00:07:00,449
there is one problem with it you cannot

00:06:58,590 --> 00:07:02,669
see it when it's an RDS instance because

00:07:00,449 --> 00:07:05,310
then they don't show you so if you want

00:07:02,669 --> 00:07:07,650
to check it then we started making an

00:07:05,310 --> 00:07:10,380
alert on the queue length

00:07:07,650 --> 00:07:12,540
okay how many i/o operations are queued

00:07:10,380 --> 00:07:14,790
and when they go over more than three

00:07:12,540 --> 00:07:17,190
operations queued on average it's either

00:07:14,790 --> 00:07:20,010
that's not happening or the director or

00:07:17,190 --> 00:07:22,260
the disk doesn't have I hope I hope so

00:07:20,010 --> 00:07:24,389
anymore and the alerts are also great

00:07:22,260 --> 00:07:26,220
you cannot just make an alert like yeah

00:07:24,389 --> 00:07:28,350
look at all disks and tell me whenever

00:07:26,220 --> 00:07:30,090
one disk goes under 20% know you have to

00:07:28,350 --> 00:07:31,530
set it up for each disk separately and

00:07:30,090 --> 00:07:34,260
when you delete the disk you have to

00:07:31,530 --> 00:07:36,479
separately delete the alert so basically

00:07:34,260 --> 00:07:40,650
you need an automation to monitor your

00:07:36,479 --> 00:07:43,229
disk so what is the effect of what we

00:07:40,650 --> 00:07:45,840
have here so in the production the disk

00:07:43,229 --> 00:07:49,380
is slower but it's very consistent so we

00:07:45,840 --> 00:07:53,070
never have any variance and we in

00:07:49,380 --> 00:07:56,220
reaction time to database but on staging

00:07:53,070 --> 00:07:58,380
we had it go unresponsive at times not

00:07:56,220 --> 00:08:00,570
working and we scaled it and scaled it

00:07:58,380 --> 00:08:03,000
so and this is how we are at the moment

00:08:00,570 --> 00:08:06,000
so as you see it has a black difference

00:08:03,000 --> 00:08:08,160
and of four and a forty times higher

00:08:06,000 --> 00:08:11,060
cost for the staging Bosh director which

00:08:08,160 --> 00:08:16,560
only runs at peak with 600 deployments

00:08:11,060 --> 00:08:18,720
so you can see if you optimize your disk

00:08:16,560 --> 00:08:22,289
on Amazon you can also already start to

00:08:18,720 --> 00:08:25,800
save some money and and save yourself a

00:08:22,289 --> 00:08:28,229
lot of worries because we had like three

00:08:25,800 --> 00:08:31,050
weeks where it was daily like I think we

00:08:28,229 --> 00:08:33,060
got into stall again or not or is it

00:08:31,050 --> 00:08:35,099
just the Bosh director and it's bad if

00:08:33,060 --> 00:08:37,140
you have to wonder if the software is

00:08:35,099 --> 00:08:39,570
the issue or if it's just the disc not

00:08:37,140 --> 00:08:41,940
working so let's continue with things

00:08:39,570 --> 00:08:44,310
that drain your I ops for the Bosch

00:08:41,940 --> 00:08:46,440
director to daily snapshot tasks even if

00:08:44,310 --> 00:08:48,600
you disable snatch snapshots whereas so

00:08:46,440 --> 00:08:50,280
most times it still queues a snapshot

00:08:48,600 --> 00:08:52,770
tasks for each and every deployment and

00:08:50,280 --> 00:08:54,580
when you have 1,000 deployments then it

00:08:52,770 --> 00:08:57,370
goes for them all and could

00:08:54,580 --> 00:08:58,540
and asks for the deployment convicts as

00:08:57,370 --> 00:09:00,670
for the cloud convict as for the

00:08:58,540 --> 00:09:03,060
manifest as for everything and then to

00:09:00,670 --> 00:09:05,920
say like oh snap shots are disabled by

00:09:03,060 --> 00:09:08,320
Bosch VMs and the Bosch deployments

00:09:05,920 --> 00:09:10,930
which was originally also a problem it

00:09:08,320 --> 00:09:14,860
got better in the meantime I will also

00:09:10,930 --> 00:09:17,980
explain it later and if you get drained

00:09:14,860 --> 00:09:19,900
repeatedly think about going to SC 1 or

00:09:17,980 --> 00:09:22,090
s T 1 which are the magnet tabs they're

00:09:19,900 --> 00:09:25,690
a bit slower than GP 2 but they cost

00:09:22,090 --> 00:09:27,850
half of GP 2 and they are consistent and

00:09:25,690 --> 00:09:30,760
you never have to worry about I ops so

00:09:27,850 --> 00:09:33,730
that's where we are going with with the

00:09:30,760 --> 00:09:36,070
staging direct database in the near

00:09:33,730 --> 00:09:39,160
future we just go to magnetic disk and

00:09:36,070 --> 00:09:40,930
like ok we just dealt with that problem

00:09:39,160 --> 00:09:42,610
it will never come again and we don't

00:09:40,930 --> 00:09:47,050
have to pay for a terabyte of disk from

00:09:42,610 --> 00:09:49,360
which we use 5 gigabyte so now let's

00:09:47,050 --> 00:09:51,850
talk about some issues I think Morgan

00:09:49,360 --> 00:09:54,640
still dislikes me from that time because

00:09:51,850 --> 00:09:57,340
there was like the fall of 2018 and

00:09:54,640 --> 00:09:59,770
winter so basically every one or two

00:09:57,340 --> 00:10:02,260
weeks he the Porsche core team probably

00:09:59,770 --> 00:10:05,200
woke up look at slag was like they broke

00:10:02,260 --> 00:10:08,950
it again there's still a channel in

00:10:05,200 --> 00:10:11,710
slack for issues we have because we had

00:10:08,950 --> 00:10:13,540
issues so let's start with with our

00:10:11,710 --> 00:10:17,470
first big problem so we were like in the

00:10:13,540 --> 00:10:20,410
September all nice and shiny is 670

00:10:17,470 --> 00:10:23,350
deployments the director VM is very slow

00:10:20,410 --> 00:10:26,230
so we wait like 2 or 3 minutes sometimes

00:10:23,350 --> 00:10:27,940
to come back some some clients even say

00:10:26,230 --> 00:10:31,480
like ok that posh director isn't coming

00:10:27,940 --> 00:10:33,370
back I'm disconnecting we we scaled

00:10:31,480 --> 00:10:36,790
Bosch we sketch database nothing works

00:10:33,370 --> 00:10:39,370
even an m4 2x large RDS doesn't make

00:10:36,790 --> 00:10:41,140
anything and we also added more disk IO

00:10:39,370 --> 00:10:43,480
ops we went to three tier terabytes in

00:10:41,140 --> 00:10:45,160
the meantime it's was nothing helping so

00:10:43,480 --> 00:10:48,130
what was the solution

00:10:45,160 --> 00:10:50,140
well this one was pretty simple thanks

00:10:48,130 --> 00:10:52,330
to s AP because they stumbled over the

00:10:50,140 --> 00:10:54,580
issue first they fixed it before we

00:10:52,330 --> 00:10:57,010
found it and the problem was that the

00:10:54,580 --> 00:10:59,800
director was just going whenever you do

00:10:57,010 --> 00:11:01,840
push from Posche VMs it also selected

00:10:59,800 --> 00:11:04,120
for each deployment the deployment

00:11:01,840 --> 00:11:07,750
conflicts even though it wasn't any part

00:11:04,120 --> 00:11:09,540
of the output so it just did join

00:11:07,750 --> 00:11:12,850
join required between two tables

00:11:09,540 --> 00:11:17,050
whenever you ask for VMS of a deployment

00:11:12,850 --> 00:11:19,990
and that was about 1,000 requests per

00:11:17,050 --> 00:11:23,230
second while we are doing bosch PM's and

00:11:19,990 --> 00:11:25,150
after that was gone it was pretty much

00:11:23,230 --> 00:11:26,710
faster so when we updated the Bosch

00:11:25,150 --> 00:11:29,140
directory we went down from like it

00:11:26,710 --> 00:11:31,810
average response time of 1 minute and 40

00:11:29,140 --> 00:11:34,120
seconds to 8 seconds to Porsche

00:11:31,810 --> 00:11:36,790
deployments because the probe mins did

00:11:34,120 --> 00:11:39,340
the same and thanks to as ap for fixing

00:11:36,790 --> 00:11:41,590
it before we had to issue so we solved

00:11:39,340 --> 00:11:45,270
this issue by simply just updating the

00:11:41,590 --> 00:11:50,230
software the next problem unfortunately

00:11:45,270 --> 00:11:53,110
as I didn't find first we found it first

00:11:50,230 --> 00:11:55,120
so the same issue we have a posture that

00:11:53,110 --> 00:11:56,560
is unresponsive or if it answers it's

00:11:55,120 --> 00:11:59,290
extremely slow

00:11:56,560 --> 00:12:02,320
we cannot upload anything we cannot

00:11:59,290 --> 00:12:05,380
start deploy tasks we get we had error

00:12:02,320 --> 00:12:08,950
messages so I look in push VMs for the

00:12:05,380 --> 00:12:10,870
under Porsche 50% disk free well it it

00:12:08,950 --> 00:12:15,220
looks like the disk is full but the disk

00:12:10,870 --> 00:12:18,310
is half empty so I H on to it and like

00:12:15,220 --> 00:12:20,680
nothing happens ok that's create a file

00:12:18,310 --> 00:12:23,110
like ok not create fire

00:12:20,680 --> 00:12:25,270
I know it's exhausted so the problem was

00:12:23,110 --> 00:12:27,460
that Porsche keeps the lock so when you

00:12:25,270 --> 00:12:30,280
to push locks it basically just reads

00:12:27,460 --> 00:12:33,670
two locks from the VM and just prints

00:12:30,280 --> 00:12:35,589
the mount out to you and the problem was

00:12:33,670 --> 00:12:38,620
that bosch tossed the task lock sent

00:12:35,589 --> 00:12:44,500
leads them regularly but it only deleted

00:12:38,620 --> 00:12:46,660
I think true to every every cycle so we

00:12:44,500 --> 00:12:50,350
created tasks just simply faster than

00:12:46,660 --> 00:12:52,390
they could be needed and thanks to the

00:12:50,350 --> 00:12:54,240
bosch prometheus exporter we did a lot

00:12:52,390 --> 00:12:57,700
of push VMs every 5 minutes

00:12:54,240 --> 00:13:00,100
943 m/s so we had one point eight

00:12:57,700 --> 00:13:02,740
million folders on the disk and each of

00:13:00,100 --> 00:13:04,630
them containing 0 2 3 5 so either it was

00:13:02,740 --> 00:13:07,270
already deleted there was a zip of it or

00:13:04,630 --> 00:13:12,250
whatever I like to see PID Park enter

00:13:07,270 --> 00:13:14,680
the output lock and so quick triage was

00:13:12,250 --> 00:13:18,610
pretty simple i delete some oh it locks

00:13:14,680 --> 00:13:21,000
files like 1.79 million of them i scaled

00:13:18,610 --> 00:13:23,970
it is kya so i cut more i

00:13:21,000 --> 00:13:26,819
more eye notes available

00:13:23,970 --> 00:13:30,360
I wrote a message to the posh core open

00:13:26,819 --> 00:13:33,540
an issue and we of course we set up an

00:13:30,360 --> 00:13:35,550
alert for the I note usage so usually

00:13:33,540 --> 00:13:37,290
the ops mantras you can do every mistake

00:13:35,550 --> 00:13:38,699
once but you shouldn't do it twice

00:13:37,290 --> 00:13:40,250
because the new alert should have warned

00:13:38,699 --> 00:13:43,129
you

00:13:40,250 --> 00:13:45,720
so and then we switched from the posh

00:13:43,129 --> 00:13:47,610
exported to the graphite ehm plugin from

00:13:45,720 --> 00:13:50,490
prometheus which is basically instead of

00:13:47,610 --> 00:13:52,529
doing posh VMs it just forwards all the

00:13:50,490 --> 00:13:54,629
data that the posh already gets from the

00:13:52,529 --> 00:13:56,160
health monitor and the and it contains

00:13:54,629 --> 00:13:58,259
the same data in the meantime at the

00:13:56,160 --> 00:14:02,250
beginning it didn't all contain all data

00:13:58,259 --> 00:14:04,860
and now it contains all data so use it

00:14:02,250 --> 00:14:06,660
though don't use the vm exporter as soon

00:14:04,860 --> 00:14:08,939
as you reach any amount of deployments

00:14:06,660 --> 00:14:12,240
you probably going to shoot yourself in

00:14:08,939 --> 00:14:15,660
the foot and the bosch core also did a

00:14:12,240 --> 00:14:18,810
fix they made porsche more aggressive at

00:14:15,660 --> 00:14:22,230
deleting so i tested last week and we

00:14:18,810 --> 00:14:24,930
only had 18,000 task locks folders on

00:14:22,230 --> 00:14:29,089
the disk which is a slight increase from

00:14:24,930 --> 00:14:29,089
1.6 to 1.8 million

00:14:29,149 --> 00:14:37,170
so then christmas came closer everyone

00:14:34,379 --> 00:14:39,629
was happy the bosch core team was

00:14:37,170 --> 00:14:41,040
probably like hey two months without an

00:14:39,629 --> 00:14:43,319
issue for maintenance and then we like

00:14:41,040 --> 00:14:47,309
if we have a very slow push directly

00:14:43,319 --> 00:14:49,379
again it was like yeah sometimes four

00:14:47,309 --> 00:14:52,500
minutes we couldn't do anything it was

00:14:49,379 --> 00:14:54,540
happening during an upgrade and we saw

00:14:52,500 --> 00:14:57,240
when when we checked the network locks

00:14:54,540 --> 00:14:59,430
it was like ok the push protects the

00:14:57,240 --> 00:15:02,490
database but then cancels two requests

00:14:59,430 --> 00:15:05,879
after one or two minutes what is

00:15:02,490 --> 00:15:10,050
happening so we I investigated further

00:15:05,879 --> 00:15:11,910
went into the debug logs and since it

00:15:10,050 --> 00:15:13,529
was always happening when you started

00:15:11,910 --> 00:15:15,540
the deploy when you tried to deploy a

00:15:13,529 --> 00:15:18,360
service or update it we investigated

00:15:15,540 --> 00:15:21,509
what happens so actually what happened

00:15:18,360 --> 00:15:24,569
so our data service framework before was

00:15:21,509 --> 00:15:26,550
using was following the Porsche locks to

00:15:24,569 --> 00:15:28,019
find out if a task worked or not but

00:15:26,550 --> 00:15:30,600
they changed behavior that it just

00:15:28,019 --> 00:15:33,059
checked with a recent task is the task

00:15:30,600 --> 00:15:34,610
see running or is it done to determine

00:15:33,059 --> 00:15:36,709
the last

00:15:34,610 --> 00:15:39,079
after lays last operation which was

00:15:36,709 --> 00:15:40,959
better for the CPU of the Bosch director

00:15:39,079 --> 00:15:43,910
because it didn't have in the worst case

00:15:40,959 --> 00:15:45,800
nine nine deployment tasks where

00:15:43,910 --> 00:15:50,149
everyone was just following with the CLI

00:15:45,800 --> 00:15:53,420
so what what does Bosch tasks - are do

00:15:50,149 --> 00:15:55,190
it listed last 30 tasks we had 3.5

00:15:53,420 --> 00:15:58,790
million tasks in the database and as you

00:15:55,190 --> 00:16:00,440
see this is a very nice query so

00:15:58,790 --> 00:16:02,269
basically we flipped the table around

00:16:00,440 --> 00:16:05,390
and then go from the top down and fire

00:16:02,269 --> 00:16:07,430
and look until we find as much as many

00:16:05,390 --> 00:16:09,920
tasks as we were asked for but what

00:16:07,430 --> 00:16:11,690
happens if you don't find enough well

00:16:09,920 --> 00:16:13,850
the statement doesn't have any condition

00:16:11,690 --> 00:16:16,220
that prevents you from just going

00:16:13,850 --> 00:16:18,890
through with all of it so what happened

00:16:16,220 --> 00:16:21,560
was each and every start even each and

00:16:18,890 --> 00:16:23,959
every Posche tasks that are costed array

00:16:21,560 --> 00:16:26,480
it cost the Postgres to go well 3.5

00:16:23,959 --> 00:16:29,240
million lines which if it's requested

00:16:26,480 --> 00:16:32,570
six times at the same time takes a

00:16:29,240 --> 00:16:34,670
moment to return so it was basically

00:16:32,570 --> 00:16:37,220
just that locking itself waiting for the

00:16:34,670 --> 00:16:39,079
database which in turn only answered the

00:16:37,220 --> 00:16:41,329
slow because Bosch was asking so often

00:16:39,079 --> 00:16:43,970
and then we had database response times

00:16:41,329 --> 00:16:45,920
of three minutes it was a very

00:16:43,970 --> 00:16:49,370
interesting ticket to make for Bosch

00:16:45,920 --> 00:16:51,649
because step to reproduce create a Bosch

00:16:49,370 --> 00:16:53,630
director make sure the tasks table is

00:16:51,649 --> 00:16:55,660
sufficiently filled about three million

00:16:53,630 --> 00:16:58,160
entries should suffice

00:16:55,660 --> 00:17:04,010
we could maybe help you with the script

00:16:58,160 --> 00:17:06,650
to do that if you require it and well so

00:17:04,010 --> 00:17:09,169
we did a triage fix for that why we

00:17:06,650 --> 00:17:10,790
waited so we changed the - our option -

00:17:09,169 --> 00:17:13,819
one in the brokers so we only get the

00:17:10,790 --> 00:17:17,209
last one tasks then to make sure because

00:17:13,819 --> 00:17:19,400
with 1200 deployments you have always

00:17:17,209 --> 00:17:22,400
some deployments which don't have a

00:17:19,400 --> 00:17:25,189
recent ask we made a script which

00:17:22,400 --> 00:17:27,410
basically did a Porsche manifest Bosh

00:17:25,189 --> 00:17:30,020
deployed that manifest so basically

00:17:27,410 --> 00:17:32,540
every each and every year service

00:17:30,020 --> 00:17:34,130
deployment had one posh task which was

00:17:32,540 --> 00:17:37,190
successfully run which didn't change

00:17:34,130 --> 00:17:39,950
anything but at least the the broker

00:17:37,190 --> 00:17:43,730
found tasks that was a successfully okay

00:17:39,950 --> 00:17:46,490
and he could continue so after the issue

00:17:43,730 --> 00:17:47,900
the Porsche core fixed it they fixed it

00:17:46,490 --> 00:17:48,530
together with another issue where

00:17:47,900 --> 00:17:51,770
someone had

00:17:48,530 --> 00:17:54,770
about the same problem so first the old

00:17:51,770 --> 00:17:57,020
test from database instead of two on

00:17:54,770 --> 00:18:00,590
every cycle it deletes ten every cycle

00:17:57,020 --> 00:18:02,210
and since we then finally got completely

00:18:00,590 --> 00:18:04,430
away from the porsche exporter we also

00:18:02,210 --> 00:18:07,460
created much less tasks like there were

00:18:04,430 --> 00:18:09,650
times when we had like half a million

00:18:07,460 --> 00:18:11,450
tasks the day because we had just every

00:18:09,650 --> 00:18:13,610
five minutes a thousand tasks every five

00:18:11,450 --> 00:18:15,080
minutes thousand tasks that customers

00:18:13,610 --> 00:18:19,630
create databases customers delete

00:18:15,080 --> 00:18:22,370
databases everything does a task and so

00:18:19,630 --> 00:18:25,700
we went down and so last week I checked

00:18:22,370 --> 00:18:27,920
we had 1100 tasks in the database of the

00:18:25,700 --> 00:18:30,050
production Cloud Foundry just because we

00:18:27,920 --> 00:18:32,720
don't do tests Bosch VMs anymore and

00:18:30,050 --> 00:18:38,990
because the boss is more aggressive at

00:18:32,720 --> 00:18:40,850
purging those old locks away so now now

00:18:38,990 --> 00:18:43,550
that we know those are some problems

00:18:40,850 --> 00:18:45,710
there was a more but that those were the

00:18:43,550 --> 00:18:48,140
most interesting ones and the ones where

00:18:45,710 --> 00:18:51,190
we basically had a nearly stop Bosch

00:18:48,140 --> 00:18:53,720
director let's go to monitoring like

00:18:51,190 --> 00:18:55,790
there's things you should monitor but

00:18:53,720 --> 00:18:57,920
probably won't because for example to be

00:18:55,790 --> 00:19:00,380
Prometheus as it is prepared for Cloud

00:18:57,920 --> 00:19:02,000
Foundry with a with all the cloud 400

00:19:00,380 --> 00:19:04,790
dashboards and all the stuff and alert

00:19:02,000 --> 00:19:07,640
manager is not set up to do that but you

00:19:04,790 --> 00:19:11,300
should do it anyways because those are

00:19:07,640 --> 00:19:13,640
things which can bite you so first you

00:19:11,300 --> 00:19:15,620
should check network IP exhaustion

00:19:13,640 --> 00:19:18,680
especially in our case where the

00:19:15,620 --> 00:19:20,540
customers basically creating the VMS you

00:19:18,680 --> 00:19:23,450
should always keep in you should always

00:19:20,540 --> 00:19:24,890
keep enough space free so people so the

00:19:23,450 --> 00:19:26,660
customer doesn't notice it first like

00:19:24,890 --> 00:19:28,520
you can have a lot of problems as long

00:19:26,660 --> 00:19:30,830
as the customer doesn't notice that

00:19:28,520 --> 00:19:34,880
there are problems in the operation

00:19:30,830 --> 00:19:37,760
world everything is fine it is is

00:19:34,880 --> 00:19:40,880
dependent so you have to probably write

00:19:37,760 --> 00:19:42,760
that script yourself as a shell script

00:19:40,880 --> 00:19:44,930
or Ruby script or whatever you are

00:19:42,760 --> 00:19:49,280
familiar with we use to push gateway to

00:19:44,930 --> 00:19:53,090
push those metrics and yeah then the

00:19:49,280 --> 00:19:55,130
disk guy up so we don't monitor all this

00:19:53,090 --> 00:19:57,170
guy ops we just monitor them on the

00:19:55,130 --> 00:19:59,750
important instances like the boss

00:19:57,170 --> 00:20:01,880
shouldn't run out the data the central

00:19:59,750 --> 00:20:05,930
database shouldn't run out because

00:20:01,880 --> 00:20:08,270
we lose using a lot of t2 instances we

00:20:05,930 --> 00:20:10,550
also have to check cpu credits on some

00:20:08,270 --> 00:20:12,680
instances even though I must say a lot

00:20:10,550 --> 00:20:14,780
of people say you should never deploy on

00:20:12,680 --> 00:20:17,120
t2 in census yes you can

00:20:14,780 --> 00:20:21,130
with most components of Cloud Foundry

00:20:17,120 --> 00:20:25,040
fit on a t2 small or t2 medium instance

00:20:21,130 --> 00:20:26,780
then quota limitations our record for

00:20:25,040 --> 00:20:29,420
requesting a quota limitation increase

00:20:26,780 --> 00:20:32,810
was once when we asked adder for 100

00:20:29,420 --> 00:20:34,490
more cpu quota and it took them nine

00:20:32,810 --> 00:20:37,220
days to approve it because they were

00:20:34,490 --> 00:20:40,190
basically can you piece is explicitly

00:20:37,220 --> 00:20:43,010
explain why you need 100 more CPUs by

00:20:40,190 --> 00:20:45,950
the way could you also take those 100

00:20:43,010 --> 00:20:50,750
CPUs in the USA or do it does it really

00:20:45,950 --> 00:20:52,370
have to be in Europe then the disk I

00:20:50,750 --> 00:20:54,050
know it usage after we had the issue

00:20:52,370 --> 00:20:57,670
with Bosch which was set up an alert

00:20:54,050 --> 00:21:00,860
miniature alert for older I note and I

00:20:57,670 --> 00:21:02,870
must admit it's it's a very rare problem

00:21:00,860 --> 00:21:06,380
that you run out of inodes but nobody

00:21:02,870 --> 00:21:08,120
expects it if it happens so in the worst

00:21:06,380 --> 00:21:09,620
case you have an alert that will never

00:21:08,120 --> 00:21:11,690
get triggered in the best case you have

00:21:09,620 --> 00:21:14,120
no thought that saves you from debugging

00:21:11,690 --> 00:21:15,620
for a day then the next thing is

00:21:14,120 --> 00:21:18,110
certificate expiration so the new

00:21:15,620 --> 00:21:20,000
approach me CL is tell you when the

00:21:18,110 --> 00:21:22,580
certificates on the off the director

00:21:20,000 --> 00:21:25,720
expire but not about all other

00:21:22,580 --> 00:21:30,170
certificates like you a Creta and that

00:21:25,720 --> 00:21:32,330
setup certificate expiration alerts it's

00:21:30,170 --> 00:21:34,280
better than just adding a calendar event

00:21:32,330 --> 00:21:36,430
when you create it because that alert

00:21:34,280 --> 00:21:40,550
will just trigger every day until you

00:21:36,430 --> 00:21:43,130
rotate the certificates and nobody can

00:21:40,550 --> 00:21:45,590
say oh it was on a second of September I

00:21:43,130 --> 00:21:49,070
did my calendar only showed me August

00:21:45,590 --> 00:21:52,250
all months and then you have the problem

00:21:49,070 --> 00:21:55,100
that you're looking ok we have fro we

00:21:52,250 --> 00:21:58,250
have certificates that are not invalid

00:21:55,100 --> 00:22:00,530
now and of course make checks that check

00:21:58,250 --> 00:22:02,150
if metrics are missing because if you

00:22:00,530 --> 00:22:04,460
have a metric that tells you if backups

00:22:02,150 --> 00:22:05,480
are working and there are no metrics

00:22:04,460 --> 00:22:07,640
coming in anymore

00:22:05,480 --> 00:22:11,090
are the backups working or are they not

00:22:07,640 --> 00:22:14,480
working before you think like Oh backups

00:22:11,090 --> 00:22:15,620
work we don't get alerts ok we don't see

00:22:14,480 --> 00:22:17,720
that we don't get any

00:22:15,620 --> 00:22:21,140
information about if those backups still

00:22:17,720 --> 00:22:25,610
work so who knows maybe we have backups

00:22:21,140 --> 00:22:27,980
maybe we don't so okay that's the things

00:22:25,610 --> 00:22:31,760
you should monitor and now about what

00:22:27,980 --> 00:22:35,059
1,200 deployments taught us as any nine

00:22:31,760 --> 00:22:36,950
said its operators so first if you have

00:22:35,059 --> 00:22:39,200
something that breaks the director the

00:22:36,950 --> 00:22:42,070
Boche team is usually pretty keen and

00:22:39,200 --> 00:22:44,510
fast on fixing it they don't really like

00:22:42,070 --> 00:22:46,820
someone to tell them that a director is

00:22:44,510 --> 00:22:49,640
broken Porsche itself is pretty stable

00:22:46,820 --> 00:22:52,160
so usually when we managed to get tasks

00:22:49,640 --> 00:22:54,710
running they run without issues so we

00:22:52,160 --> 00:22:56,720
didn't have any so at first we were we

00:22:54,710 --> 00:22:59,000
had to fear that maybe for that many

00:22:56,720 --> 00:23:00,830
deployments then we have maybe at some

00:22:59,000 --> 00:23:03,440
point state machine issues and

00:23:00,830 --> 00:23:06,530
inconsistent states but it never

00:23:03,440 --> 00:23:09,260
happened bosses I mean the worst case

00:23:06,530 --> 00:23:11,120
that can come out is if you restart a

00:23:09,260 --> 00:23:13,040
Bosch director cold while it's running a

00:23:11,120 --> 00:23:14,929
task you have like a deployment half

00:23:13,040 --> 00:23:17,450
finished that's like the worst that can

00:23:14,929 --> 00:23:19,460
happen so far but the database state and

00:23:17,450 --> 00:23:22,400
director state itself were never an

00:23:19,460 --> 00:23:24,530
issue and if you use the Prometheus

00:23:22,400 --> 00:23:28,970
projects border use the graph at HM plug

00:23:24,530 --> 00:23:31,250
in I repeated use it it's better and for

00:23:28,970 --> 00:23:32,330
smaller medium environments when it

00:23:31,250 --> 00:23:33,980
comes to the scale for the Bosch

00:23:32,330 --> 00:23:36,230
director because I see a lot of people

00:23:33,980 --> 00:23:39,170
just like yeah we need a Bosch director

00:23:36,230 --> 00:23:41,120
and it needs to be an m4 2x large on in

00:23:39,170 --> 00:23:44,120
all cases else it just breaks like

00:23:41,120 --> 00:23:46,970
petite or large which costs you like $60

00:23:44,120 --> 00:23:49,070
and Amazon is enough for at least 200 to

00:23:46,970 --> 00:23:51,920
300 deployments and if it's go if it

00:23:49,070 --> 00:23:54,830
goes larger you can think about an m5

00:23:51,920 --> 00:23:59,059
x-large or 2x large or your is

00:23:54,830 --> 00:24:03,020
equivalent so m5x lodges for CPU 16

00:23:59,059 --> 00:24:05,900
gigabytes of memory and at this point

00:24:03,020 --> 00:24:08,840
your i/o speed to the disk and the

00:24:05,900 --> 00:24:11,870
network speed will become the main issue

00:24:08,840 --> 00:24:16,970
not the CPU like all our Bosch directors

00:24:11,870 --> 00:24:18,950
just idle around at 5% CPU all day but

00:24:16,970 --> 00:24:20,929
there's a general advice I should give

00:24:18,950 --> 00:24:23,030
don't overdo the work account just don't

00:24:20,929 --> 00:24:25,190
think like I have a big boss director I

00:24:23,030 --> 00:24:28,220
will do a lot with it I make 20 workers

00:24:25,190 --> 00:24:29,600
because you have to scale your Bosch

00:24:28,220 --> 00:24:35,600
director so it's able

00:24:29,600 --> 00:24:38,240
to survive all the older workers working

00:24:35,600 --> 00:24:40,220
simultaneously so our biggest brush

00:24:38,240 --> 00:24:42,169
directed one with the 1200 deployments

00:24:40,220 --> 00:24:44,440
has nine workers and one of them is just

00:24:42,169 --> 00:24:47,630
basically only there for Bosch PMS and

00:24:44,440 --> 00:24:49,160
we never had issues because if you go if

00:24:47,630 --> 00:24:51,770
you have for example packing services

00:24:49,160 --> 00:24:54,860
people usually don't generate and delete

00:24:51,770 --> 00:24:59,299
packing services every 20 minutes apart

00:24:54,860 --> 00:25:01,160
from our tests and the chance that you

00:24:59,299 --> 00:25:02,870
use all those tasks is rather low but

00:25:01,160 --> 00:25:07,039
you have to keep in mind you have to be

00:25:02,870 --> 00:25:08,450
able to support all those tasks else and

00:25:07,039 --> 00:25:11,299
your Bosch director might become

00:25:08,450 --> 00:25:14,929
unresponsive or slow and you get the

00:25:11,299 --> 00:25:18,080
same problems as we have so this is the

00:25:14,929 --> 00:25:21,470
end if you want to keep in touch you can

00:25:18,080 --> 00:25:23,450
also come at a booth talk to me or find

00:25:21,470 --> 00:25:27,500
me on the conference I will be probably

00:25:23,450 --> 00:25:33,190
around till tomorrow evening and do you

00:25:27,500 --> 00:25:33,190
have any questions I have a microphone

00:25:35,230 --> 00:25:42,110
of course I get a question from the most

00:25:38,000 --> 00:25:45,650
back-end row I want to ask how many

00:25:42,110 --> 00:25:47,780
instances alignments there so the

00:25:45,650 --> 00:25:50,240
instances the deployments have depends

00:25:47,780 --> 00:25:53,120
on the data service flavor so the

00:25:50,240 --> 00:25:55,580
instance types are either it's a so each

00:25:53,120 --> 00:25:59,210
deployment is add a single nano instance

00:25:55,580 --> 00:26:01,549
or it's a cluster or in the case of our

00:25:59,210 --> 00:26:05,780
Lakme service it's either four or six

00:26:01,549 --> 00:26:09,320
VMs so on stage we have more nano

00:26:05,780 --> 00:26:12,010
services on production it's like 70 80

00:26:09,320 --> 00:26:15,260
percent clusters alright so you have two

00:26:12,010 --> 00:26:19,490
times four about or more to get the

00:26:15,260 --> 00:26:21,830
instance numbers yes so like make it

00:26:19,490 --> 00:26:24,500
like two two and a half for the amount

00:26:21,830 --> 00:26:26,690
of instances I mean I had an interesting

00:26:24,500 --> 00:26:29,690
discussion on the last CF operator per

00:26:26,690 --> 00:26:31,159
call where someone from your company

00:26:29,690 --> 00:26:32,929
even explained to me that you should

00:26:31,159 --> 00:26:36,260
never put more than 100 PM's on the

00:26:32,929 --> 00:26:38,960
Posche director and was not really that

00:26:36,260 --> 00:26:41,090
happy with my answer to you want to do

00:26:38,960 --> 00:26:43,250
you want to operate a fleet of t2 small

00:26:41,090 --> 00:26:45,679
portion tectors or do you want to

00:26:43,250 --> 00:26:47,900
to operate whatever comes out whatever

00:26:45,679 --> 00:26:52,190
is running on those Bosch directors so

00:26:47,900 --> 00:26:54,020
the thing is VMs is one thing which

00:26:52,190 --> 00:26:56,390
gives is the one thing that gives you

00:26:54,020 --> 00:27:00,320
the disk no no not to discover the

00:26:56,390 --> 00:27:01,909
network i/o while the this guy or often

00:27:00,320 --> 00:27:04,220
comes from the deployments because

00:27:01,909 --> 00:27:06,740
that's where it does put the task logs

00:27:04,220 --> 00:27:11,000
the VM law to BM information and all the

00:27:06,740 --> 00:27:12,590
stuff so I mean we also had seen pivotal

00:27:11,000 --> 00:27:16,070
cloud foundries which have more than

00:27:12,590 --> 00:27:19,309
those two to three thousand films on a

00:27:16,070 --> 00:27:20,539
single director so VMs is another metric

00:27:19,309 --> 00:27:22,340
but the bruster

00:27:20,539 --> 00:27:23,990
but with bosch deployments you have

00:27:22,340 --> 00:27:26,360
different problems than when you have

00:27:23,990 --> 00:27:27,740
more VMs because mobiums just mean the

00:27:26,360 --> 00:27:29,240
elf monitors more to two more

00:27:27,740 --> 00:27:30,830
deployments mean there's more

00:27:29,240 --> 00:27:33,559
interaction with the database which can

00:27:30,830 --> 00:27:36,169
cause issues so we had like more the

00:27:33,559 --> 00:27:38,600
database side of the issues so far maybe

00:27:36,169 --> 00:27:41,780
in the future we will become the we we

00:27:38,600 --> 00:27:45,740
get Network IO side a bit more if we

00:27:41,780 --> 00:27:48,620
have more clusters but currently I don't

00:27:45,740 --> 00:27:50,900
think we go much higher because we now

00:27:48,620 --> 00:27:53,480
starting to use more than one under Bush

00:27:50,900 --> 00:27:55,820
because we decided that it's probably a

00:27:53,480 --> 00:27:57,500
better idea to not find out what breaks

00:27:55,820 --> 00:27:59,990
with two thousand deployments or three

00:27:57,500 --> 00:28:01,880
thousand deployments and just say okay

00:27:59,990 --> 00:28:04,190
we we managed to run it at 1200

00:28:01,880 --> 00:28:07,640
deployments I mean every every problem

00:28:04,190 --> 00:28:11,780
we had was fixed in upstream so you all

00:28:07,640 --> 00:28:14,720
get a benefit of it and did that answer

00:28:11,780 --> 00:28:17,659
your question yeah that answered it yeah

00:28:14,720 --> 00:28:19,730
and yeah it's great to have this

00:28:17,659 --> 00:28:22,039
collaboration where we both fix issues

00:28:19,730 --> 00:28:24,950
and like I said be any nines other

00:28:22,039 --> 00:28:26,270
companies and profit from it can I ask

00:28:24,950 --> 00:28:31,789
another question or is there somebody

00:28:26,270 --> 00:28:33,530
else I don't see any ray Stantz if you

00:28:31,789 --> 00:28:34,870
have a question don't run out of the

00:28:33,530 --> 00:28:38,330
room right now

00:28:34,870 --> 00:28:40,580
okay everyone has questions the other

00:28:38,330 --> 00:28:43,010
thing is we currently see a problem when

00:28:40,580 --> 00:28:45,590
updating large deployment instances on

00:28:43,010 --> 00:28:48,950
large landscapes were we see it timed

00:28:45,590 --> 00:28:50,299
out we're nuts messages timeout we're

00:28:48,950 --> 00:28:53,149
currently working on that we also have

00:28:50,299 --> 00:28:55,250
an issue upstream open I was asking if

00:28:53,149 --> 00:28:56,509
you also see that no we haven't seen

00:28:55,250 --> 00:28:58,849
that yet

00:28:56,509 --> 00:29:00,739
so probably you will most likely fix it

00:28:58,849 --> 00:29:02,089
again before we have tissue so we update

00:29:00,739 --> 00:29:02,479
and never have to issue in the first

00:29:02,089 --> 00:29:07,959
place

00:29:02,479 --> 00:29:14,029
thank you okay any any other questions

00:29:07,959 --> 00:29:18,619
comments or okay it's a coffee break so

00:29:14,029 --> 00:29:21,169
and I just have half a minute left so

00:29:18,619 --> 00:29:24,199
I'm I'm letting you leave early to your

00:29:21,169 --> 00:29:26,359
coffee break and I wish you a nice rest

00:29:24,199 --> 00:29:29,029
of the summit and I hope you learned

00:29:26,359 --> 00:29:31,039
something in the talk if you have any if

00:29:29,029 --> 00:29:32,329
if you want to talk to me just find me

00:29:31,039 --> 00:29:34,489
in a conference I'll come to our booth

00:29:32,329 --> 00:29:40,459
they usually know where I am

00:29:34,489 --> 00:29:43,869
so a vinay summit and I hope it I

00:29:40,459 --> 00:29:43,869
thought I helped you somehow

00:29:45,070 --> 00:29:47,560

YouTube URL: https://www.youtube.com/watch?v=peMFKBdRi_o


