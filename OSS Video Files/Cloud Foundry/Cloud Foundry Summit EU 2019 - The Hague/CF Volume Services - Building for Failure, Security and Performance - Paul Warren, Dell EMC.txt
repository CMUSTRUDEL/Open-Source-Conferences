Title: CF Volume Services - Building for Failure, Security and Performance - Paul Warren, Dell EMC
Publication date: 2019-09-13
Playlist: Cloud Foundry Summit EU 2019 - The Hague
Description: 
	CF Volume Services - Building for Failure, Security and Performance - Paul Warren, Dell EMC 

CloudFoundry has had a volume services capability for several years now allowing the platform to run workloads that require a persistent disk. We have also laid the groundwork for the same capabilities in Project Eirini and contributed to industry standards defining how to attach volumes into containers.  Based on NFS and SMB it has been both an interesting and challenging journey to provide a viable and reliable service offering into a modern, Cloud Native platform, using protocols that were designed decades ago and to a different set of assumptions.  Join Paul and Julian for a guided tour of Volume Services in CF and Project Eirini and on their journey from the very beginning finding a viable solution through to present day and building a service offering that is effective, efficient and resilient against failure. 

For more info: https://www.cloudfoundry.org/
Captions: 
	00:00:00,199 --> 00:00:11,210
hi folks welcome to safe summit 2019 in

00:00:09,450 --> 00:00:15,059
Europe I hope you're having a good day

00:00:11,210 --> 00:00:18,410
welcome to my talk on volume services my

00:00:15,059 --> 00:00:20,430
name is Paul Warren from Dell EMC and

00:00:18,410 --> 00:00:23,990
I've been an engineer on the volume

00:00:20,430 --> 00:00:23,990
Services team since its inception

00:00:24,050 --> 00:00:30,210
standard disclaimer I'd like to take you

00:00:27,510 --> 00:00:31,710
on a journey from that beginning all the

00:00:30,210 --> 00:00:34,980
way through to where we are today

00:00:31,710 --> 00:00:39,079
so you can understand better the

00:00:34,980 --> 00:00:42,870
offering that we provide how it works

00:00:39,079 --> 00:00:44,760
why it works the way it does and some of

00:00:42,870 --> 00:00:45,329
the decisions that got us to where we

00:00:44,760 --> 00:00:47,700
are today

00:00:45,329 --> 00:00:50,460
so for those that aren't familiar what a

00:00:47,700 --> 00:00:52,410
volume services volume services in cloud

00:00:50,460 --> 00:00:54,090
foundry are the ability to bind one or

00:00:52,410 --> 00:00:58,620
more shared volumes into your

00:00:54,090 --> 00:01:01,440
application container I say shake

00:00:58,620 --> 00:01:05,129
and I say shared for a reason because

00:01:01,440 --> 00:01:07,380
you can only attach Nazz shared volumes

00:01:05,129 --> 00:01:11,490
into your into your application

00:01:07,380 --> 00:01:14,159
containers we don't do block the reason

00:01:11,490 --> 00:01:17,310
for that is that cloud foundry the heart

00:01:14,159 --> 00:01:19,619
has a diego-based scheduler and that is

00:01:17,310 --> 00:01:22,500
a jewel that's optimized for 12 factor

00:01:19,619 --> 00:01:24,509
applications and to cut a long story

00:01:22,500 --> 00:01:26,810
short the services you binds your

00:01:24,509 --> 00:01:28,680
applications running on this scheduler

00:01:26,810 --> 00:01:31,259
after we had a handle more than one

00:01:28,680 --> 00:01:33,299
thing connecting to it at once and

00:01:31,259 --> 00:01:33,900
that's why we don't do block with cloud

00:01:33,299 --> 00:01:37,170
foundry

00:01:33,900 --> 00:01:41,220
on Diego I'll get into a little bit of

00:01:37,170 --> 00:01:42,990
Block at the end so in the beginning

00:01:41,220 --> 00:01:46,439
there was just cloud foundry there were

00:01:42,990 --> 00:01:47,939
no volume services well of course there

00:01:46,439 --> 00:01:50,070
were volume services they existed you

00:01:47,939 --> 00:01:52,409
just couldn't connect to them from from

00:01:50,070 --> 00:01:53,340
cloud foundry but we wanted you to be

00:01:52,409 --> 00:01:55,530
able to connect to them from cloud

00:01:53,340 --> 00:02:00,329
foundry and we wanted to expose those

00:01:55,530 --> 00:02:05,640
services just as any other service so we

00:02:00,329 --> 00:02:07,740
created an NFS broker gave that thing a

00:02:05,640 --> 00:02:09,929
database so that we could persist

00:02:07,740 --> 00:02:12,490
service instances and service bindings

00:02:09,929 --> 00:02:14,950
across restarts of the broker

00:02:12,490 --> 00:02:16,540
which you have to be able to do for when

00:02:14,950 --> 00:02:18,880
you're deploying updates to your Cloud

00:02:16,540 --> 00:02:21,640
Foundry deployment that thing gets taken

00:02:18,880 --> 00:02:24,160
down and brought back up again okay

00:02:21,640 --> 00:02:26,440
we also modified the open service broker

00:02:24,160 --> 00:02:28,840
API to have a volume service construct

00:02:26,440 --> 00:02:31,120
construct so that service brokers could

00:02:28,840 --> 00:02:33,820
issue service bindings with volumes in

00:02:31,120 --> 00:02:35,530
them along the way we had to touch some

00:02:33,820 --> 00:02:38,040
of the existing Cloud Foundry components

00:02:35,530 --> 00:02:41,440
that's Dutch Cloud Controller BBS

00:02:38,040 --> 00:02:42,850
auctioneer and the rep make them all a

00:02:41,440 --> 00:02:48,070
little bit familiar with what the volume

00:02:42,850 --> 00:02:52,900
service is on the sell we had to put a

00:02:48,070 --> 00:02:55,180
volume manager and paired with the NFS

00:02:52,900 --> 00:02:56,560
broker we had to put an NFS driver the

00:02:55,180 --> 00:03:02,110
thing that's actually going to provision

00:02:56,560 --> 00:03:05,410
them out for you and we proved this

00:03:02,110 --> 00:03:08,170
architecture out with with asset based

00:03:05,410 --> 00:03:09,220
broker and Driver originally but we

00:03:08,170 --> 00:03:10,360
didn't want to take that for a couple of

00:03:09,220 --> 00:03:13,330
reasons so we didn't want to take that

00:03:10,360 --> 00:03:15,160
into production so it seemed to make

00:03:13,330 --> 00:03:16,720
sense to move in the direction of NFS

00:03:15,160 --> 00:03:21,540
given that we have a Linux based

00:03:16,720 --> 00:03:21,540
operating system at the heart and also

00:03:22,290 --> 00:03:25,930
and also because that was the signal we

00:03:24,490 --> 00:03:28,330
were getting from from you folks right

00:03:25,930 --> 00:03:32,560
we wanted something which that was

00:03:28,330 --> 00:03:35,410
generally useful so how does it actually

00:03:32,560 --> 00:03:38,770
work well in the developer pushes his

00:03:35,410 --> 00:03:41,770
app obviously it uploads the application

00:03:38,770 --> 00:03:43,660
bits to the blobstore and it creates a

00:03:41,770 --> 00:03:48,610
record of that new application in the

00:03:43,660 --> 00:03:51,730
cloud controller database that then

00:03:48,610 --> 00:03:54,520
allows the developer to create a new NFS

00:03:51,730 --> 00:03:55,930
service and specify the share that he

00:03:54,520 --> 00:04:01,000
wants to bind into his application

00:03:55,930 --> 00:04:02,140
containers what's actually happening

00:04:01,000 --> 00:04:03,610
here is we're not actually going and

00:04:02,140 --> 00:04:05,650
creating a share on the back-end for a

00:04:03,610 --> 00:04:06,730
couple of reasons right the signal we

00:04:05,650 --> 00:04:08,380
were getting from the community

00:04:06,730 --> 00:04:10,390
initially was that they wanted to take

00:04:08,380 --> 00:04:12,550
their legacy applications and lifts and

00:04:10,390 --> 00:04:13,630
shift and into Cloud Foundry yeah this

00:04:12,550 --> 00:04:15,010
was some application that had been

00:04:13,630 --> 00:04:18,190
running in the data center for years

00:04:15,010 --> 00:04:19,870
we've been reading and writing files to

00:04:18,190 --> 00:04:22,240
some share for years as a particular

00:04:19,870 --> 00:04:24,190
user but it was a pain to keep that

00:04:22,240 --> 00:04:25,570
thing Security patched an occasion it

00:04:24,190 --> 00:04:27,730
fell over and then your

00:04:25,570 --> 00:04:28,720
to get out of bed and restarted lift and

00:04:27,730 --> 00:04:30,880
shift that thing onto Cloud Foundry

00:04:28,720 --> 00:04:33,880
right and you kind of get all that for

00:04:30,880 --> 00:04:35,500
free the other reason we don't provision

00:04:33,880 --> 00:04:38,650
shares is that it's not actually part of

00:04:35,500 --> 00:04:40,090
the NFS protocol itself so NFS deals

00:04:38,650 --> 00:04:42,280
with the mount and reading writing of

00:04:40,090 --> 00:04:44,500
files but it doesn't actually manage the

00:04:42,280 --> 00:04:47,980
lifecycle of the shares and we as a team

00:04:44,500 --> 00:04:49,300
consider that we we support protocols so

00:04:47,980 --> 00:04:51,400
you specify the share that you want to

00:04:49,300 --> 00:04:53,230
associate with Cloud Controller towards

00:04:51,400 --> 00:04:54,850
the NFS and basically NFS was just

00:04:53,230 --> 00:04:58,720
recording the fact that you're going to

00:04:54,850 --> 00:05:01,120
buying that share later on develop and

00:04:58,720 --> 00:05:03,100
then binds his application to a service

00:05:01,120 --> 00:05:05,770
so it's looking just like any other

00:05:03,100 --> 00:05:09,250
Service Cloud Controller talks to the

00:05:05,770 --> 00:05:11,980
NFS broker the NFS broker issues a

00:05:09,250 --> 00:05:14,440
service binding with a funny amount in

00:05:11,980 --> 00:05:16,510
it and you can see this is the section

00:05:14,440 --> 00:05:18,700
of the service binding for the specific

00:05:16,510 --> 00:05:22,090
to the volumes it's targeting the NFS

00:05:18,700 --> 00:05:28,270
drive one themself and here's the share

00:05:22,090 --> 00:05:29,920
that we want to to bind in and really we

00:05:28,270 --> 00:05:32,260
haven't placed anything yet so that's

00:05:29,920 --> 00:05:35,800
just recording associating the share

00:05:32,260 --> 00:05:38,890
with the application so then the

00:05:35,800 --> 00:05:41,650
developer starts his app Cal patroller

00:05:38,890 --> 00:05:44,080
talks to the VBS BBS wants to know where

00:05:41,650 --> 00:05:47,020
to run this workload so it's going to

00:05:44,080 --> 00:05:48,850
hold an auction one of the cells and

00:05:47,020 --> 00:05:49,990
depict the cells here right there's

00:05:48,850 --> 00:05:51,640
three of them but it could be any number

00:05:49,990 --> 00:05:55,420
one of the cells is going to win that

00:05:51,640 --> 00:05:57,940
auction sell one in this case BBS talks

00:05:55,420 --> 00:05:59,860
to the rep on that cell and says hey see

00:05:57,940 --> 00:06:02,620
these application bits over here can you

00:05:59,860 --> 00:06:04,330
run them in a container for me the rep

00:06:02,620 --> 00:06:05,770
looks at that specification and says

00:06:04,330 --> 00:06:08,140
well actually there's a service binding

00:06:05,770 --> 00:06:09,880
there and it's called a volume in it so

00:06:08,140 --> 00:06:12,400
it asks the vol manager to provision

00:06:09,880 --> 00:06:14,560
that volume the Volm annajura cc's the

00:06:12,400 --> 00:06:16,960
NFS driver knows that the driver it

00:06:14,560 --> 00:06:18,940
wants to provision the share and it

00:06:16,960 --> 00:06:24,340
forwards that onto the NFS driver and

00:06:18,940 --> 00:06:26,680
the NFS driver does amount of that

00:06:24,340 --> 00:06:28,450
remote share so at this point you have

00:06:26,680 --> 00:06:31,720
the remote share mounted over the top of

00:06:28,450 --> 00:06:33,280
the local directory on the cell the NFS

00:06:31,720 --> 00:06:35,770
driver then returns this local mount

00:06:33,280 --> 00:06:38,680
point back to the rep the rep sister

00:06:35,770 --> 00:06:41,199
garden take those application bits

00:06:38,680 --> 00:06:42,490
run them in a container please and by

00:06:41,199 --> 00:06:45,880
the way could you buy in this directory

00:06:42,490 --> 00:06:47,979
into it and so it does and at this point

00:06:45,880 --> 00:06:50,530
in time you have an application that's

00:06:47,979 --> 00:06:52,990
free to read and write files to that

00:06:50,530 --> 00:06:55,270
remote share and that was the very first

00:06:52,990 --> 00:07:00,070
version of what we got working but there

00:06:55,270 --> 00:07:02,800
was a problem for every single build

00:07:00,070 --> 00:07:05,860
pack that you run or you push to Cloud

00:07:02,800 --> 00:07:09,820
Foundry yeah the user inside the

00:07:05,860 --> 00:07:13,419
containers always has a UID of 2000 and

00:07:09,820 --> 00:07:16,150
that maps to a UID on the cell outside

00:07:13,419 --> 00:07:19,360
the container of a thousand and because

00:07:16,150 --> 00:07:20,949
NFS is a POSIX based security model user

00:07:19,360 --> 00:07:22,870
a thousand is going to be the guy that's

00:07:20,949 --> 00:07:25,870
fiddling with the files on your remote

00:07:22,870 --> 00:07:27,910
share now bear in mind this is in the

00:07:25,870 --> 00:07:30,210
early days as well was all about lifting

00:07:27,910 --> 00:07:33,340
and shifting assisting applications

00:07:30,210 --> 00:07:34,720
99.99% chance that's not the user that

00:07:33,340 --> 00:07:36,520
has been fiddling with the files on the

00:07:34,720 --> 00:07:36,880
show right you need it to be someone

00:07:36,520 --> 00:07:40,150
else

00:07:36,880 --> 00:07:43,630
most likely user a thousand doesn't even

00:07:40,150 --> 00:07:45,280
have access to that share so we had to

00:07:43,630 --> 00:07:47,680
think about this and we thought well

00:07:45,280 --> 00:07:49,810
that's okay when you do binding will

00:07:47,680 --> 00:07:52,229
allow you to specify the UID of the user

00:07:49,810 --> 00:07:54,729
that you do want touching the files

00:07:52,229 --> 00:07:57,099
growl controller talks the NFS broker

00:07:54,729 --> 00:07:59,610
and if s broker responds with the

00:07:57,099 --> 00:08:01,990
service binding with the volume in it

00:07:59,610 --> 00:08:04,680
specifies the UID that you want to use

00:08:01,990 --> 00:08:08,289
and then we fast forward to placement

00:08:04,680 --> 00:08:12,010
the NFS driver does the kernel mount as

00:08:08,289 --> 00:08:13,840
it did before but we then do a second

00:08:12,010 --> 00:08:15,909
mount over the top of that mount with a

00:08:13,840 --> 00:08:19,060
special fuse module of our own devising

00:08:15,909 --> 00:08:22,000
called map FS and the sole purpose of

00:08:19,060 --> 00:08:26,560
that thing is to map current user

00:08:22,000 --> 00:08:29,560
UID a thousand in this case to the

00:08:26,560 --> 00:08:34,000
intended user with the idea of 999 in

00:08:29,560 --> 00:08:35,589
this case and then the NFS driver

00:08:34,000 --> 00:08:37,630
returns this to the rep and the rep

00:08:35,589 --> 00:08:38,200
binds that second mount into the

00:08:37,630 --> 00:08:40,029
container

00:08:38,200 --> 00:08:42,190
now when the application reads and

00:08:40,029 --> 00:08:44,529
writes files is doing so through that

00:08:42,190 --> 00:08:46,990
second mount which is mapping the user

00:08:44,529 --> 00:08:49,270
from a thousand to 999 in this case on

00:08:46,990 --> 00:08:51,670
the way out and mapping it back again on

00:08:49,270 --> 00:08:54,980
the way in

00:08:51,670 --> 00:08:58,790
so that's what we called NVP minimal

00:08:54,980 --> 00:09:00,610
Viable Product but of course it was a

00:08:58,790 --> 00:09:04,190
bit Rowsey because it wasn't very secure

00:09:00,610 --> 00:09:05,690
right a user could specify any UID

00:09:04,190 --> 00:09:07,670
and we would try and bind it for him

00:09:05,690 --> 00:09:09,650
moreover CF doesn't have any rate

00:09:07,670 --> 00:09:13,970
limiting on its API so if you knew there

00:09:09,650 --> 00:09:16,940
was share out there yeah so back to the

00:09:13,970 --> 00:09:19,460
drawing board we thought Kerberos is

00:09:16,940 --> 00:09:22,310
built into the NFS protocol this is okay

00:09:19,460 --> 00:09:26,150
we'll allow the user to specify username

00:09:22,310 --> 00:09:30,650
password on bind service and then we

00:09:26,150 --> 00:09:32,480
fast forward to at placement will then

00:09:30,650 --> 00:09:35,720
try and initialize this Kerberos

00:09:32,480 --> 00:09:37,070
security context and then on the mount

00:09:35,720 --> 00:09:39,770
were specified that we want to use

00:09:37,070 --> 00:09:41,779
Kerberos security but this was a bust

00:09:39,770 --> 00:09:43,910
didn't work for us we can initialize

00:09:41,779 --> 00:09:46,040
that first security context but we

00:09:43,910 --> 00:09:48,620
couldn't Alyssa lies multiple security

00:09:46,040 --> 00:09:52,279
contexts it was way to mount a bound to

00:09:48,620 --> 00:09:53,810
the host machine and bearing in mind

00:09:52,279 --> 00:09:55,130
that this was happening on itself we

00:09:53,810 --> 00:09:56,630
could have any number of applications

00:09:55,130 --> 00:09:59,810
placed on that cell all requiring

00:09:56,630 --> 00:10:01,670
different security contexts so back to

00:09:59,810 --> 00:10:05,120
the drawing board again we thought this

00:10:01,670 --> 00:10:06,800
is OK LDAP probably the most common IDP

00:10:05,120 --> 00:10:10,160
out there maybe there's something we can

00:10:06,800 --> 00:10:12,170
do that so we kept the ability to

00:10:10,160 --> 00:10:17,390
specify a username password on the

00:10:12,170 --> 00:10:19,070
buying service and then when issues

00:10:17,390 --> 00:10:22,820
amount we include the username and

00:10:19,070 --> 00:10:25,070
password and then when you start your

00:10:22,820 --> 00:10:29,030
application if we fast forward to app

00:10:25,070 --> 00:10:31,700
placement before we do any mounts the

00:10:29,030 --> 00:10:34,310
NFS driver contacts an LDAP server with

00:10:31,700 --> 00:10:36,380
those credentials and verifies them if

00:10:34,310 --> 00:10:40,910
they check out we exchanged those

00:10:36,380 --> 00:10:42,530
credentials for that user's UID and then

00:10:40,910 --> 00:10:44,959
we do the exact same double mounting

00:10:42,530 --> 00:10:46,130
process that we saw earlier and we bind

00:10:44,959 --> 00:10:48,140
mount that second mount into the

00:10:46,130 --> 00:10:51,529
application container and at this point

00:10:48,140 --> 00:10:53,959
we had a secure solution and by the way

00:10:51,529 --> 00:10:56,330
when we run with LDAP enabled we turn

00:10:53,959 --> 00:10:59,470
off the UID capability okay so this was

00:10:56,330 --> 00:10:59,470
reasonably secure

00:11:01,339 --> 00:11:07,139
the customers then started to say to us

00:11:03,569 --> 00:11:08,999
yeah that's good but we want to use

00:11:07,139 --> 00:11:10,709
application manifests and volume

00:11:08,999 --> 00:11:11,790
services and of course we just giving

00:11:10,709 --> 00:11:13,350
them a solution where you have to

00:11:11,790 --> 00:11:15,689
specify the username password in the

00:11:13,350 --> 00:11:19,970
bind service can't specify - see

00:11:15,689 --> 00:11:22,290
parameters in your application manifest

00:11:19,970 --> 00:11:23,489
so this meant that we had to capture the

00:11:22,290 --> 00:11:26,040
user name and password

00:11:23,489 --> 00:11:27,689
during the create service not bind

00:11:26,040 --> 00:11:29,279
service which meant that we had to

00:11:27,689 --> 00:11:30,809
remember the user name and password in

00:11:29,279 --> 00:11:32,850
between the create service and buying

00:11:30,809 --> 00:11:34,619
service and because it was a password

00:11:32,850 --> 00:11:39,029
that meant that we had to remember it

00:11:34,619 --> 00:11:41,369
securely so we added cred hub as an

00:11:39,029 --> 00:11:43,169
option to run the NFS broker on top off

00:11:41,369 --> 00:11:45,449
and that meant that we could remember

00:11:43,169 --> 00:11:49,859
these name passwords securely and that

00:11:45,449 --> 00:11:52,049
also meant that when you started your

00:11:49,859 --> 00:11:54,239
app you can specify your application

00:11:52,049 --> 00:11:58,139
manifest with your volume service in it

00:11:54,239 --> 00:12:02,480
and then during that start process your

00:11:58,139 --> 00:12:04,410
volume is bound automatically we can

00:12:02,480 --> 00:12:05,850
include the username password

00:12:04,410 --> 00:12:08,759
information in there because we

00:12:05,850 --> 00:12:10,290
remembered it securely in cred hub and

00:12:08,759 --> 00:12:13,829
we fast forward all the way through the

00:12:10,290 --> 00:12:18,269
same process right and the application

00:12:13,829 --> 00:12:20,790
is placed on the cell so that gave us a

00:12:18,269 --> 00:12:21,869
little bit more flexibility I think at

00:12:20,790 --> 00:12:23,939
this point we were fairly feature

00:12:21,869 --> 00:12:25,949
complete and we kind of graduated from

00:12:23,939 --> 00:12:27,239
experimental with a lot of new folks and

00:12:25,949 --> 00:12:29,879
you actually started using us for real

00:12:27,239 --> 00:12:33,110
and you said hey yeah like the

00:12:29,879 --> 00:12:36,540
performance of your stuff really sucks

00:12:33,110 --> 00:12:38,489
and this wasn't too much of a surprise

00:12:36,540 --> 00:12:41,759
to us because when we do that kernel

00:12:38,489 --> 00:12:43,379
mount we run the caching off we turn all

00:12:41,759 --> 00:12:44,850
crashing off and that was deliberate

00:12:43,379 --> 00:12:46,410
because when you push in app you can

00:12:44,850 --> 00:12:49,529
scale it on the platform to multiple

00:12:46,410 --> 00:12:53,429
instances and so we wanted we didn't

00:12:49,529 --> 00:12:55,499
want to affect data integrity so we're

00:12:53,429 --> 00:12:57,359
wrong with caching off to force every

00:12:55,499 --> 00:13:00,929
client back to the server for every

00:12:57,359 --> 00:13:03,660
request but of course that comes with a

00:13:00,929 --> 00:13:05,039
performance penalty so we talked to

00:13:03,660 --> 00:13:06,600
these people that we're trying to do

00:13:05,039 --> 00:13:10,410
this and they said well actually you

00:13:06,600 --> 00:13:12,839
know what the use case we've got is some

00:13:10,410 --> 00:13:14,899
other process drops a massive file and

00:13:12,839 --> 00:13:17,269
then a fair share and then all

00:13:14,899 --> 00:13:18,619
Ã§f try apps are doing is really reading

00:13:17,269 --> 00:13:21,490
those and doing some batch processing on

00:13:18,619 --> 00:13:24,559
that file we said all right fair enough

00:13:21,490 --> 00:13:28,879
that's okay then if you specify read

00:13:24,559 --> 00:13:32,209
only then we'll include that in the

00:13:28,879 --> 00:13:35,540
service binding if we fast forward to

00:13:32,209 --> 00:13:37,490
app placement when we do the mount we

00:13:35,540 --> 00:13:39,379
won't do the caching we won't turn the

00:13:37,490 --> 00:13:40,999
caching off sorry so we'll run with

00:13:39,379 --> 00:13:42,709
default caching and then you get a bit

00:13:40,999 --> 00:13:45,290
of a performance improvement there and

00:13:42,709 --> 00:13:47,420
they said great thanks very much by the

00:13:45,290 --> 00:13:51,050
way it still sucks

00:13:47,420 --> 00:13:52,790
we said ok hmm and they said well you

00:13:51,050 --> 00:13:54,490
know what these aren't lift and shift

00:13:52,790 --> 00:13:57,350
these cases for us this is net neat

00:13:54,490 --> 00:13:59,029
these shares the net new so we actually

00:13:57,350 --> 00:14:02,809
don't care who's reading the file we're

00:13:59,029 --> 00:14:05,600
quite happy to open up access to you

00:14:02,809 --> 00:14:20,360
idea thousand so we said them ok fair

00:14:05,600 --> 00:14:22,129
enough part of me so he said fine if you

00:14:20,360 --> 00:14:24,230
don't specify user name password or a

00:14:22,129 --> 00:14:27,439
UID and GID depending on how you're

00:14:24,230 --> 00:14:30,050
running then we'll get rid of the map FS

00:14:27,439 --> 00:14:31,910
here leaving you with just a kernel map

00:14:30,050 --> 00:14:33,949
and then you should be back to the

00:14:31,910 --> 00:14:35,720
performance that you get with just a

00:14:33,949 --> 00:14:38,660
regular old kernel man and they said

00:14:35,720 --> 00:14:43,850
great that's that that's fantastic thank

00:14:38,660 --> 00:14:45,980
you very much so that gave us some level

00:14:43,850 --> 00:14:49,970
of performance in in somewhat of a

00:14:45,980 --> 00:14:54,429
pinyin ated way then we had other

00:14:49,970 --> 00:14:54,429
customers who had unreliability issues

00:14:55,240 --> 00:15:00,559
we never quite got to the reason whether

00:14:58,549 --> 00:15:02,120
it was nodes in their NFS cluster that

00:15:00,559 --> 00:15:03,740
we're unreliable or if it was just the

00:15:02,120 --> 00:15:05,629
network that was unreliable but some

00:15:03,740 --> 00:15:07,790
think in there was unreliable and this

00:15:05,629 --> 00:15:12,619
customer sometimes had maps that took a

00:15:07,790 --> 00:15:14,660
long time well what happens there is if

00:15:12,619 --> 00:15:17,269
it takes longer than 10 seconds to place

00:15:14,660 --> 00:15:20,029
the the place the application or the

00:15:17,269 --> 00:15:21,529
workload then BBS is going to get bored

00:15:20,029 --> 00:15:23,259
and it's going to cancel that and go and

00:15:21,529 --> 00:15:26,360
try and place that workload elsewhere

00:15:23,259 --> 00:15:27,640
doing all the clever Diego stuff that it

00:15:26,360 --> 00:15:29,410
does

00:15:27,640 --> 00:15:31,120
but it does send us a cancellation when

00:15:29,410 --> 00:15:33,310
it's going to cancel and we knock code

00:15:31,120 --> 00:15:35,170
we've done a pretty good job of routing

00:15:33,310 --> 00:15:37,420
that cancellation signal all the way

00:15:35,170 --> 00:15:41,110
through to the NFS driver and we did

00:15:37,420 --> 00:15:43,630
what we thought was smart we cancelled

00:15:41,110 --> 00:15:45,310
the mount command but this customer was

00:15:43,630 --> 00:15:46,720
saying to us yeah but we still have like

00:15:45,310 --> 00:15:48,279
mounts on the cell and we have no

00:15:46,720 --> 00:15:51,279
Associated workload it's kind of like an

00:15:48,279 --> 00:15:53,529
orphan map we were like hmm doesn't

00:15:51,279 --> 00:15:56,589
sound quite right so we dug into it a

00:15:53,529 --> 00:15:58,420
little bit and what we discovered was

00:15:56,589 --> 00:16:00,370
when you do a kernel mount the kernel

00:15:58,420 --> 00:16:02,470
mount turns right back around and forks

00:16:00,370 --> 00:16:04,180
a sub process to do the specific type of

00:16:02,470 --> 00:16:07,000
mount in this case it was forking a

00:16:04,180 --> 00:16:09,370
mount NFS sub process so I guess you

00:16:07,000 --> 00:16:11,110
probably guess what happens we got that

00:16:09,370 --> 00:16:12,490
cancellation signal when the BBS got

00:16:11,110 --> 00:16:13,180
bored waiting and went to place the app

00:16:12,490 --> 00:16:15,279
elsewhere

00:16:13,180 --> 00:16:17,800
we've done a really nice job of

00:16:15,279 --> 00:16:20,050
canceling the parent mount process but

00:16:17,800 --> 00:16:22,839
leaving the sub process still running

00:16:20,050 --> 00:16:25,660
now if your unreliability went away and

00:16:22,839 --> 00:16:27,640
that mount then finished successfully at

00:16:25,660 --> 00:16:30,220
some point in time you're left with your

00:16:27,640 --> 00:16:33,640
orphaned mount on self so that wasn't

00:16:30,220 --> 00:16:36,070
ideal fairly easy fix though to modify

00:16:33,640 --> 00:16:40,209
the code to cancel the process group

00:16:36,070 --> 00:16:41,260
rather than the process right same deal

00:16:40,209 --> 00:16:43,779
with unmount right that could

00:16:41,260 --> 00:16:46,360
theoretically take a long time we get

00:16:43,779 --> 00:16:50,709
the cancellation signal we kill the on

00:16:46,360 --> 00:16:53,170
milk and then you probably left with an

00:16:50,709 --> 00:16:55,120
orphaned mount again the operating

00:16:53,170 --> 00:16:56,620
system came to our rescue there there's

00:16:55,120 --> 00:16:58,600
a - shell flag on the unmount which

00:16:56,620 --> 00:17:01,209
stands for lazy unmount what this does

00:16:58,600 --> 00:17:03,130
is it disassociates mount from the

00:17:01,209 --> 00:17:05,140
filesystem and returns success

00:17:03,130 --> 00:17:07,569
immediately allowing us to return

00:17:05,140 --> 00:17:09,970
success almost immediately to the rep

00:17:07,569 --> 00:17:12,419
whilst in the backgrounds the operating

00:17:09,970 --> 00:17:15,850
system will take care of unmounting that

00:17:12,419 --> 00:17:17,589
you know and shepherding that process

00:17:15,850 --> 00:17:20,290
through the completion so a nice easy

00:17:17,589 --> 00:17:23,140
solution for us in the end so at this

00:17:20,290 --> 00:17:24,939
point that gave us a nice fairly feature

00:17:23,140 --> 00:17:28,030
complete offering it was reasonably

00:17:24,939 --> 00:17:31,000
secure you've got reasonable performance

00:17:28,030 --> 00:17:33,700
in an opinionated way and we were fairly

00:17:31,000 --> 00:17:36,910
reliable in the face of flakiness so it

00:17:33,700 --> 00:17:38,650
worked but it's quite complicated the

00:17:36,910 --> 00:17:41,320
NFS driver has to know about the LDAP

00:17:38,650 --> 00:17:42,850
server usually the person who manages

00:17:41,320 --> 00:17:45,190
if deployment isn't the person who

00:17:42,850 --> 00:17:46,960
manages the LDAP deployment so this

00:17:45,190 --> 00:17:48,909
usually means tickets to get the

00:17:46,960 --> 00:17:51,970
connection details to go open up

00:17:48,909 --> 00:17:53,620
security groups for the LDAP server or

00:17:51,970 --> 00:17:56,710
the old up deployment allowing traffic

00:17:53,620 --> 00:18:02,080
from from the set of cells which are the

00:17:56,710 --> 00:18:03,610
IPS the set of the cell IPs moreover

00:18:02,080 --> 00:18:05,710
when the developer doesn't need a new

00:18:03,610 --> 00:18:07,480
share that probably means more tickets

00:18:05,710 --> 00:18:09,220
to because the developer it probably

00:18:07,480 --> 00:18:11,440
isn't the person managing the LDAP

00:18:09,220 --> 00:18:14,110
deployment either and this one's a

00:18:11,440 --> 00:18:15,879
little bit tricky because as part of the

00:18:14,110 --> 00:18:17,679
NFS protocol part of the security

00:18:15,879 --> 00:18:19,830
protocol for NFS you have to specify the

00:18:17,679 --> 00:18:22,419
set of IPs that have got access to this

00:18:19,830 --> 00:18:24,879
share developer doesn't normally know

00:18:22,419 --> 00:18:26,470
what the cell IPS are so that's more

00:18:24,879 --> 00:18:29,100
communication more tickets to try and

00:18:26,470 --> 00:18:29,100
get that hooked up

00:18:30,100 --> 00:18:33,789
we've got map FS on the cell so there's

00:18:32,200 --> 00:18:35,259
more mounts there than people expect

00:18:33,789 --> 00:18:37,679
there's more mounts there's more

00:18:35,259 --> 00:18:40,659
processes more things to go wrong and

00:18:37,679 --> 00:18:43,570
the final kicker is that even if things

00:18:40,659 --> 00:18:46,090
are working great the traffic for NFS

00:18:43,570 --> 00:18:49,629
goes across the wire in plain text

00:18:46,090 --> 00:18:51,009
it's totally unencrypted so whilst we

00:18:49,629 --> 00:18:53,740
were working on this we'd also been

00:18:51,009 --> 00:18:58,269
working on SMB and it works the same way

00:18:53,740 --> 00:18:59,950
right there's an SMB broker there's an

00:18:58,269 --> 00:19:03,220
SMB driver and usually have obviously

00:18:59,950 --> 00:19:04,720
both on the cell at the same time and

00:19:03,220 --> 00:19:06,789
this allows the developer to create an

00:19:04,720 --> 00:19:09,220
instance of an SMB service rather than

00:19:06,789 --> 00:19:10,659
NFS service it works the same way you

00:19:09,220 --> 00:19:12,149
specify your share it is just an

00:19:10,659 --> 00:19:17,889
association we're not provisioning it

00:19:12,149 --> 00:19:19,360
it's just in UNC format instead and then

00:19:17,889 --> 00:19:21,370
when you bind your service you specify a

00:19:19,360 --> 00:19:24,789
username and password just like you do

00:19:21,370 --> 00:19:26,529
with NFS that issues similar sort of

00:19:24,789 --> 00:19:30,610
binding but you start targeting the SMB

00:19:26,529 --> 00:19:33,700
driver not the NFS driver and then when

00:19:30,610 --> 00:19:36,159
you start your app we do a syst based

00:19:33,700 --> 00:19:41,639
mount not an NFS base mount and then

00:19:36,159 --> 00:19:45,190
oops and then we take that mount point

00:19:41,639 --> 00:19:46,450
and we hand it back to the rep and we

00:19:45,190 --> 00:19:48,250
ask the rep to bind that into the

00:19:46,450 --> 00:19:50,080
container so you'll notice there's no

00:19:48,250 --> 00:19:51,519
matter first mount on top of this and

00:19:50,080 --> 00:19:54,950
this is because authentication and

00:19:51,519 --> 00:19:57,200
authorization is built into mapper

00:19:54,950 --> 00:19:58,700
by default so we don't need the

00:19:57,200 --> 00:19:59,800
complexity that we have with the NFS

00:19:58,700 --> 00:20:04,430
solution

00:19:59,800 --> 00:20:07,220
moreover whoops moreover if you have

00:20:04,430 --> 00:20:10,700
anything resembling a fairly recent nas

00:20:07,220 --> 00:20:13,280
on your back-end and it supports SMB v3

00:20:10,700 --> 00:20:17,150
then the client on the cells will

00:20:13,280 --> 00:20:18,920
negotiate up to SMB v3 and your traffic

00:20:17,150 --> 00:20:22,160
across the wire will be encrypted for

00:20:18,920 --> 00:20:29,240
you by default as well so I think you

00:20:22,160 --> 00:20:32,930
agree over all simpler solution so a

00:20:29,240 --> 00:20:34,610
word on locking we support or we

00:20:32,930 --> 00:20:38,000
consider that we support the NFS

00:20:34,610 --> 00:20:41,990
protocols only those being in effect the

00:20:38,000 --> 00:20:44,120
protocols only those being NFS in SMB we

00:20:41,990 --> 00:20:46,670
do we don't as a team we do not certify

00:20:44,120 --> 00:20:50,840
against any backends any asses backends

00:20:46,670 --> 00:20:52,400
including Isilon we provision the mounts

00:20:50,840 --> 00:20:54,110
only on to the cell and into the

00:20:52,400 --> 00:20:58,190
container and then we take them away

00:20:54,110 --> 00:21:02,240
again afterwards right there is a tiny

00:20:58,190 --> 00:21:05,450
bit of code in the data plane for NFS in

00:21:02,240 --> 00:21:07,580
some situations that being map FS but

00:21:05,450 --> 00:21:10,520
that does not have any locking code in

00:21:07,580 --> 00:21:11,930
it right it is devoid of locking code it

00:21:10,520 --> 00:21:13,790
does one thing and one thing only which

00:21:11,930 --> 00:21:19,040
is to map that user on the way out and

00:21:13,790 --> 00:21:21,800
map it on the way back in so feel free

00:21:19,040 --> 00:21:23,510
to go ahead and use locking but if

00:21:21,800 --> 00:21:25,460
you're going to use locking you probably

00:21:23,510 --> 00:21:28,100
want to take a look at a few things you

00:21:25,460 --> 00:21:29,600
probably want to understand the locking

00:21:28,100 --> 00:21:31,550
calls that your application is going to

00:21:29,600 --> 00:21:32,900
make to the operating system and what I

00:21:31,550 --> 00:21:35,660
mean by that is you know if you have a

00:21:32,900 --> 00:21:36,980
java application and your developers are

00:21:35,660 --> 00:21:39,200
running some sort of locking code in

00:21:36,980 --> 00:21:40,550
java you need to understand what the JVM

00:21:39,200 --> 00:21:42,890
is gonna do down at the operating system

00:21:40,550 --> 00:21:44,560
level and then once you understand that

00:21:42,890 --> 00:21:47,780
you probably want to go and understand

00:21:44,560 --> 00:21:52,160
the locking calls that the operating

00:21:47,780 --> 00:21:54,280
system is making over the wire for the

00:21:52,160 --> 00:21:57,020
version of the mount that you're doing

00:21:54,280 --> 00:22:01,130
because it's different especially an NFS

00:21:57,020 --> 00:22:02,180
from different versions and then you

00:22:01,130 --> 00:22:03,860
probably want it's not on the board

00:22:02,180 --> 00:22:05,300
there but you probably want to start to

00:22:03,860 --> 00:22:08,510
have a think about what it means if

00:22:05,300 --> 00:22:11,210
you're using version 4 of NFS and SMB

00:22:08,510 --> 00:22:14,179
when a client opens file or take out a

00:22:11,210 --> 00:22:16,940
lease on that file that's a performance

00:22:14,179 --> 00:22:18,230
enhancement that allows the client side

00:22:16,940 --> 00:22:20,360
to read and write to that file without

00:22:18,230 --> 00:22:22,640
going back to the server because it

00:22:20,360 --> 00:22:23,270
knows the server will come and break

00:22:22,640 --> 00:22:24,860
that lock

00:22:23,270 --> 00:22:26,720
later on if it has another client that

00:22:24,860 --> 00:22:28,070
wants to write to that file giving the

00:22:26,720 --> 00:22:30,799
first client the opportunity to write

00:22:28,070 --> 00:22:32,630
write the changes back that's all great

00:22:30,799 --> 00:22:35,059
in all unless you have unreliability and

00:22:32,630 --> 00:22:38,270
server can't break that lock in those

00:22:35,059 --> 00:22:40,970
situations I think you can run into some

00:22:38,270 --> 00:22:43,370
data integrity issues so feel free to

00:22:40,970 --> 00:22:46,010
use locking but proceed with caution and

00:22:43,370 --> 00:22:48,380
if you do try we would love to hear some

00:22:46,010 --> 00:22:53,510
stories from you right we'd love to talk

00:22:48,380 --> 00:22:56,299
to you about that so takeaways use SMB

00:22:53,510 --> 00:22:58,250
when you can it's simpler and it's more

00:22:56,299 --> 00:23:00,200
secure our assumption is that most nas

00:22:58,250 --> 00:23:02,419
is nowadays can export share as both

00:23:00,200 --> 00:23:05,780
either NFS and SMB so again let us know

00:23:02,419 --> 00:23:09,770
if that's not the case using NFS by all

00:23:05,780 --> 00:23:12,020
means if you can't use SMB but hopefully

00:23:09,770 --> 00:23:15,169
at this point it's fairly resilient to

00:23:12,020 --> 00:23:17,419
failure and if performance is a key spy

00:23:15,169 --> 00:23:19,429
tech pay special attention to the amount

00:23:17,419 --> 00:23:21,620
options you're using if you can get away

00:23:19,429 --> 00:23:24,470
with read-only go for it if you don't

00:23:21,620 --> 00:23:25,700
need UID mapping don't use it and then

00:23:24,470 --> 00:23:27,110
you're going to get a regular old kernel

00:23:25,700 --> 00:23:30,220
mount and the performance and caching

00:23:27,110 --> 00:23:30,220
associated with that

00:23:30,820 --> 00:23:36,380
so a quick look forward I'd like to

00:23:34,270 --> 00:23:37,940
you'll you'll notice that I haven't

00:23:36,380 --> 00:23:40,970
mentioned Windows at all during this

00:23:37,940 --> 00:23:43,669
talk and that is because my p.m. Julian

00:23:40,970 --> 00:23:46,640
over here is about to do a talk with

00:23:43,669 --> 00:23:48,290
Daniella in Everest just across the way

00:23:46,640 --> 00:23:50,809
right after this and they're going to

00:23:48,290 --> 00:23:54,080
talk about volume services from Windows

00:23:50,809 --> 00:23:55,400
applications and then tomorrow Julian's

00:23:54,080 --> 00:23:58,520
going to do another talk on pushing

00:23:55,400 --> 00:24:01,820
 so this was about a recent POC we

00:23:58,520 --> 00:24:04,970
did we're running with CF on top of

00:24:01,820 --> 00:24:07,460
kubernetes we were able to push an image

00:24:04,970 --> 00:24:09,290
of the database scale up to three

00:24:07,460 --> 00:24:12,650
instances each one getting their own

00:24:09,290 --> 00:24:15,620
block device and then run that as a

00:24:12,650 --> 00:24:17,750
cluster and read yeah right for one node

00:24:15,620 --> 00:24:19,909
and read from the others which for us

00:24:17,750 --> 00:24:22,119
from Delhi MC was quite exciting because

00:24:19,909 --> 00:24:24,159
we've been trying to get find

00:24:22,119 --> 00:24:30,459
to do block for about three years and I

00:24:24,159 --> 00:24:31,689
think we found a route forward so what

00:24:30,459 --> 00:24:33,129
does the immediate future look for

00:24:31,689 --> 00:24:34,479
isolate that's our service offering

00:24:33,129 --> 00:24:35,019
today but what does the immediate future

00:24:34,479 --> 00:24:38,669
look like

00:24:35,019 --> 00:24:40,959
well now foundry on diego code line

00:24:38,669 --> 00:24:42,759
we're considering it going into

00:24:40,959 --> 00:24:46,179
maintenance we're going to do bug fixes

00:24:42,759 --> 00:24:47,409
and CVS on that but we're going to turn

00:24:46,179 --> 00:24:50,949
our attention to cloud foundry on

00:24:47,409 --> 00:24:53,649
kubernetes and in the first instance

00:24:50,949 --> 00:24:56,649
we're going to focus on our charter says

00:24:53,649 --> 00:25:00,189
we're going to focus on feature parity

00:24:56,649 --> 00:25:02,259
with cloud foundry on Diego and we're

00:25:00,189 --> 00:25:03,609
going to go after SMB first because we

00:25:02,259 --> 00:25:06,639
think that's a much better fit for this

00:25:03,609 --> 00:25:08,499
cloud native world again please let us

00:25:06,639 --> 00:25:11,799
know if you think those priorities

00:25:08,499 --> 00:25:15,429
should be done differently once we get

00:25:11,799 --> 00:25:17,019
through that the plan is open we don't

00:25:15,429 --> 00:25:21,009
have anything in our backlog or our

00:25:17,019 --> 00:25:22,929
Charter past that we think we would like

00:25:21,009 --> 00:25:24,489
to go after block but we would love to

00:25:22,929 --> 00:25:26,849
hear from you guys do you have use cases

00:25:24,489 --> 00:25:29,229
where you need to be developing

00:25:26,849 --> 00:25:32,259
application work based workloads that

00:25:29,229 --> 00:25:35,439
require block devices another area we

00:25:32,259 --> 00:25:37,359
could go to is brokers it's always bug

00:25:35,439 --> 00:25:39,999
me a little bit that it's an association

00:25:37,359 --> 00:25:42,369
only I'd like to have brokers that

00:25:39,999 --> 00:25:46,059
provisioned which I think would reduce

00:25:42,369 --> 00:25:47,589
the inertia of using volume services but

00:25:46,059 --> 00:25:49,119
we'd love to hear what you guys think

00:25:47,589 --> 00:25:51,339
that's just what I think what do you

00:25:49,119 --> 00:25:53,019
guys would like us to do and lastly

00:25:51,339 --> 00:25:57,309
locking as you probably can tell from

00:25:53,019 --> 00:25:58,809
the locking slide locking with NFS and

00:25:57,309 --> 00:26:00,789
SMB is a little bit fraught with danger

00:25:58,809 --> 00:26:05,409
it can work if you're careful about what

00:26:00,789 --> 00:26:07,629
you're doing if you are going down that

00:26:05,409 --> 00:26:09,999
road or you have the opinion that volume

00:26:07,629 --> 00:26:11,349
services to just make locking work we

00:26:09,999 --> 00:26:14,079
would also love to hear from you and

00:26:11,349 --> 00:26:19,179
we'd love to discuss further with you

00:26:14,079 --> 00:26:21,039
what working actually means so the plan

00:26:19,179 --> 00:26:23,319
is open we would love to hear from you

00:26:21,039 --> 00:26:26,109
guys about where where you think we

00:26:23,319 --> 00:26:29,259
should go next and at that point I think

00:26:26,109 --> 00:26:32,259
I'll open up for questions how did I do

00:26:29,259 --> 00:26:34,979
yeah all right well one minute over but

00:26:32,259 --> 00:26:34,979
yeah

00:26:39,940 --> 00:26:47,800
yes yeah yep

00:26:48,280 --> 00:26:58,850
right so the we don't have any code in

00:26:56,150 --> 00:27:01,600
volume services to manage locking so if

00:26:58,850 --> 00:27:07,370
your application is has that requirement

00:27:01,600 --> 00:27:09,860
then then I think it's best to be very

00:27:07,370 --> 00:27:12,820
cautious as you proceed and really

00:27:09,860 --> 00:27:15,350
understand the applications that you're

00:27:12,820 --> 00:27:18,800
what your application is going to be

00:27:15,350 --> 00:27:21,160
doing and take it from there we're not

00:27:18,800 --> 00:27:24,080
going to do anything other than provide

00:27:21,160 --> 00:27:25,640
access to the locking features that are

00:27:24,080 --> 00:27:28,010
supported by the operating system and

00:27:25,640 --> 00:27:42,710
and the protocol in question that you're

00:27:28,010 --> 00:27:45,620
using that is how it all accessing the

00:27:42,710 --> 00:27:48,410
same share you mean or yes that is

00:27:45,620 --> 00:27:52,520
actually that is actually how it works

00:27:48,410 --> 00:27:56,000
today so each workload each app instance

00:27:52,520 --> 00:27:58,040
will get its own mount in days gone by

00:27:56,000 --> 00:27:59,810
we were try and if the if the app

00:27:58,040 --> 00:28:04,150
instance got placed on the same cell in

00:27:59,810 --> 00:28:06,380
days gone by we tried to be clever and

00:28:04,150 --> 00:28:08,120
kind of reference you have one mount a

00:28:06,380 --> 00:28:10,130
reference count but we moved away from

00:28:08,120 --> 00:28:13,510
that for several reasons so so each app

00:28:10,130 --> 00:28:13,510
instance will get its own map

00:28:21,780 --> 00:28:27,440
Yeah right you still have access

00:28:39,820 --> 00:28:44,619
and you want for any other questions

00:28:43,440 --> 00:28:46,479
okay

00:28:44,619 --> 00:28:47,499
I'll be hanging around outside for a few

00:28:46,479 --> 00:28:48,909
minutes and then I'm probably going to

00:28:47,499 --> 00:28:49,670
Julian's talk you can come and find me

00:28:48,909 --> 00:28:54,150
at first I'll make you one

00:28:49,670 --> 00:28:54,150

YouTube URL: https://www.youtube.com/watch?v=6Hv76QUf_RE


