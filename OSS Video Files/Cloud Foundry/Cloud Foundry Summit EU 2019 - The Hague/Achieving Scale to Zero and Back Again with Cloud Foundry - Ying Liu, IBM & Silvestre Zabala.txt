Title: Achieving Scale to Zero and Back Again with Cloud Foundry - Ying Liu, IBM & Silvestre Zabala
Publication date: 2019-09-13
Playlist: Cloud Foundry Summit EU 2019 - The Hague
Description: 
	Achieving Scale to Zero and Back Again with Cloud Foundry - Ying Liu, IBM & Silvestre Zabala, SAP 

zero-scale” is an idea that an application can be reduced down to zero instances when idle and brought back to the required amount of instances when it is needed. It enables better resource efficiency by allowing idling workloads to automatically scale to zero and zero workloads to be automatically re-activated by coming requests.  This session will introduce an experimental project “scale2zero” which implements this feature for Cloud Foundry. We will introduce the user experience, how it works with other Cloud Foundry components like GoRouter, loggregator. We will also talk about future integration into app-autoscaler project and how it will affect and work with dynamic and scheduled scaling. 

For more info: https://www.cloudfoundry.org/
Captions: 
	00:00:00,030 --> 00:00:05,490
hello everyone yeah good afternoon thank

00:00:03,179 --> 00:00:07,350
you for joining our session and the

00:00:05,490 --> 00:00:10,860
session we'll talk about how to achieve

00:00:07,350 --> 00:00:14,849
to go to zero and bring the application

00:00:10,860 --> 00:00:18,390
back up again on Cal foundry and I mean

00:00:14,849 --> 00:00:21,570
email from IBM and this directory from I

00:00:18,390 --> 00:00:25,289
say P both of Earth's are working for

00:00:21,570 --> 00:00:27,810
the country I also get a project about

00:00:25,289 --> 00:00:30,019
scanner projects is used to help you to

00:00:27,810 --> 00:00:33,030
auto scale your co-founder applications

00:00:30,019 --> 00:00:36,480
according to the different workloads and

00:00:33,030 --> 00:00:39,809
this is the project report and the lab

00:00:36,480 --> 00:00:43,890
channel and a jetrel few weeks ago we

00:00:39,809 --> 00:00:47,340
released a post killer 2.0 release to

00:00:43,890 --> 00:00:50,640
leverage to the latest la verga charity

00:00:47,340 --> 00:00:53,699
ice and should make h more production

00:00:50,640 --> 00:00:56,760
ready and to the skill to the other

00:00:53,699 --> 00:00:58,410
feature is our next most interest topic

00:00:56,760 --> 00:01:00,629
that we would love to add into our

00:00:58,410 --> 00:01:04,890
future release so today let's do a

00:01:00,629 --> 00:01:05,820
preview this is the agenda first of all

00:01:04,890 --> 00:01:08,250
we will take some time

00:01:05,820 --> 00:01:10,380
oh it would take time to introduce what

00:01:08,250 --> 00:01:12,530
is to go to their role how to do it in

00:01:10,380 --> 00:01:15,380
country and our implementation details

00:01:12,530 --> 00:01:17,790
and then we will do a some brief

00:01:15,380 --> 00:01:20,520
introduction about the latest update of

00:01:17,790 --> 00:01:22,650
all scalar for the katherine matrix and

00:01:20,520 --> 00:01:25,049
the UI integration so that you can

00:01:22,650 --> 00:01:27,210
understand how to go to zero future will

00:01:25,049 --> 00:01:31,049
be present app or scalar

00:01:27,210 --> 00:01:36,240
after it is integrated into the official

00:01:31,049 --> 00:01:39,000
release okay so what is going to 0 so

00:01:36,240 --> 00:01:41,579
when you run application on car foundry

00:01:39,000 --> 00:01:43,950
you will notice it should be one or more

00:01:41,579 --> 00:01:46,560
instance and all this instance running

00:01:43,950 --> 00:01:47,159
it will cost it will consume some

00:01:46,560 --> 00:01:50,640
resources

00:01:47,159 --> 00:01:53,460
CPU memory and so on and all this result

00:01:50,640 --> 00:01:56,130
resources will take money yeah you will

00:01:53,460 --> 00:01:57,210
pay for that if you don't need your

00:01:56,130 --> 00:01:59,689
application anymore

00:01:57,210 --> 00:02:02,939
you can stop it or delete it completely

00:01:59,689 --> 00:02:06,409
or if you are just running a task our

00:02:02,939 --> 00:02:09,119
job you can use the F grunt tasks and

00:02:06,409 --> 00:02:11,160
with that command Carter will help you

00:02:09,119 --> 00:02:12,900
to terminate the application container

00:02:11,160 --> 00:02:14,730
whether the task

00:02:12,900 --> 00:02:17,190
finished so that you don't need to worry

00:02:14,730 --> 00:02:20,459
about the addition nor ever additional

00:02:17,190 --> 00:02:23,610
cost but the problem take is things is

00:02:20,459 --> 00:02:25,950
the web application which is which don't

00:02:23,610 --> 00:02:28,110
have a very heavy load is just a house

00:02:25,950 --> 00:02:30,659
um or requires occasionally but you

00:02:28,110 --> 00:02:31,049
can't stop it if you cause if you stop

00:02:30,659 --> 00:02:34,739
it

00:02:31,049 --> 00:02:37,290
your any other will get error message so

00:02:34,739 --> 00:02:38,430
that is where the skill to zero feature

00:02:37,290 --> 00:02:42,150
can kicks in

00:02:38,430 --> 00:02:44,959
yeah first of all there is to go to the

00:02:42,150 --> 00:02:47,750
engine to monitor your HTTP requests

00:02:44,959 --> 00:02:51,299
there are two applications for the

00:02:47,750 --> 00:02:53,400
application tool there are active OTP

00:02:51,299 --> 00:02:56,730
requests so we don't do nothing on that

00:02:53,400 --> 00:02:59,909
for my AP p1 there is no incoming

00:02:56,730 --> 00:03:01,829
requests over the defined period so

00:02:59,909 --> 00:03:05,519
secure to the other engine will stop

00:03:01,829 --> 00:03:08,609
that but after that it master have a

00:03:05,519 --> 00:03:11,430
chance to whip up if there is some new

00:03:08,609 --> 00:03:14,370
request you can see in the application

00:03:11,430 --> 00:03:16,590
need to be wake up so we add another

00:03:14,370 --> 00:03:19,590
component we named it as a daughter

00:03:16,590 --> 00:03:21,690
proxy once the applicant my application

00:03:19,590 --> 00:03:24,660
one I've stopped the daughter proxy

00:03:21,690 --> 00:03:28,019
hijack is rough and pretended it is my

00:03:24,660 --> 00:03:31,470
AP p1 to be ready to accept the incoming

00:03:28,019 --> 00:03:33,959
request once there is a TB requires to

00:03:31,470 --> 00:03:37,230
come to the Roger proxy for my IP P 1

00:03:33,959 --> 00:03:39,359
dot virtual item home it will notify the

00:03:37,230 --> 00:03:41,879
skill to end secure to 0 and gene to

00:03:39,359 --> 00:03:43,560
start the original application and once

00:03:41,879 --> 00:03:46,919
the application container is ready

00:03:43,560 --> 00:03:49,949
yeah the ATP request will be redirect to

00:03:46,919 --> 00:03:53,609
it so you won't get any error message

00:03:49,949 --> 00:03:56,250
but you will experience later a bit

00:03:53,609 --> 00:03:58,590
longer latency for the faulty request

00:03:56,250 --> 00:04:01,290
and after that everything back to normal

00:03:58,590 --> 00:04:05,400
the requester will be rotated to the

00:04:01,290 --> 00:04:07,139
application container directly yeah so

00:04:05,400 --> 00:04:09,989
you cannot see the scale to zero

00:04:07,139 --> 00:04:11,549
I'd here is very natural yeah in fact in

00:04:09,989 --> 00:04:13,769
the past years there are several

00:04:11,549 --> 00:04:16,799
differential attempts to achieve the po2

00:04:13,769 --> 00:04:20,190
zero alcohol boundary the most basic

00:04:16,799 --> 00:04:24,000
ones yeah is my giraffe to our

00:04:20,190 --> 00:04:24,490
application to justice it similar to the

00:04:24,000 --> 00:04:27,910
command

00:04:24,490 --> 00:04:30,970
they have me proud but the problem here

00:04:27,910 --> 00:04:34,060
is that uncle foundry the Ross is a

00:04:30,970 --> 00:04:36,900
resource related to space if you have a

00:04:34,060 --> 00:04:39,580
garage in space a then you can't my

00:04:36,900 --> 00:04:42,849
associates the drawers to applications

00:04:39,580 --> 00:04:46,449
are running in space B so that's require

00:04:42,849 --> 00:04:49,660
you must relocate co-locate your route

00:04:46,449 --> 00:04:53,229
properly with your target application in

00:04:49,660 --> 00:04:55,509
the same user space that is ridiculous

00:04:53,229 --> 00:04:58,270
yeah you want to scale to there to fill

00:04:55,509 --> 00:05:02,229
castor you need to add another one so

00:04:58,270 --> 00:05:02,889
yeah so there is another project named

00:05:02,229 --> 00:05:07,330
Auto sleep

00:05:02,889 --> 00:05:10,630
our Auto sleep is an open source open

00:05:07,330 --> 00:05:13,060
source project as well it leveraged the

00:05:10,630 --> 00:05:16,389
routing power routing service capability

00:05:13,060 --> 00:05:19,210
to achieve this so if you run with the

00:05:16,389 --> 00:05:21,190
auto scale or water or - sorry

00:05:19,210 --> 00:05:23,319
obviously project you need to create a

00:05:21,190 --> 00:05:26,860
service instance for that and then you

00:05:23,319 --> 00:05:30,219
can use append the job bank the route to

00:05:26,860 --> 00:05:33,070
the service instance I think it's much

00:05:30,219 --> 00:05:36,130
better but we still want you to want to

00:05:33,070 --> 00:05:39,400
provide or more or more virtual user

00:05:36,130 --> 00:05:41,770
experience to make the go to zero loss

00:05:39,400 --> 00:05:44,409
like building is furious looks like a

00:05:41,770 --> 00:05:48,610
platform extension so that even to use

00:05:44,409 --> 00:05:50,409
for you to achieve that we rebid it the

00:05:48,610 --> 00:05:54,280
routing architecture of curved boundary

00:05:50,409 --> 00:05:57,819
we notice that each in each Diego say oh

00:05:54,280 --> 00:06:00,219
there is no emitter to image the graph

00:05:57,819 --> 00:06:02,979
information should nav and then true

00:06:00,219 --> 00:06:06,009
lodging API and which is served for the

00:06:02,979 --> 00:06:09,039
culture and history Roger here although

00:06:06,009 --> 00:06:12,300
in this architecture the Rajan API so

00:06:09,039 --> 00:06:17,080
for the PCP Roger but in fact it has

00:06:12,300 --> 00:06:20,289
some experimental API to allow Earth to

00:06:17,080 --> 00:06:24,009
associate the ATP rod so that in our

00:06:20,289 --> 00:06:26,620
current implementation the George proxy

00:06:24,009 --> 00:06:29,949
will register the ATP rose to the rotten

00:06:26,620 --> 00:06:32,589
API and then keep sending their harvest

00:06:29,949 --> 00:06:35,099
to claim the land needs of the rod and

00:06:32,589 --> 00:06:37,260
if the application continue did we have

00:06:35,099 --> 00:06:41,490
we will already

00:06:37,260 --> 00:06:44,670
- production API but anyway it is still

00:06:41,490 --> 00:06:49,680
experimental a API is post-birth the

00:06:44,670 --> 00:06:53,100
release if it can't be GA yeah it mail

00:06:49,680 --> 00:06:56,580
will be not safely enough to introduce

00:06:53,100 --> 00:07:00,180
it to do it in the above killer so maybe

00:06:56,580 --> 00:07:02,010
we can we afford to the nerd solution to

00:07:00,180 --> 00:07:04,470
just a podcaster the Nath should

00:07:02,010 --> 00:07:07,890
register the nos - Nos

00:07:04,470 --> 00:07:09,770
such as what what is deeds by the dot

00:07:07,890 --> 00:07:13,590
register

00:07:09,770 --> 00:07:15,870
okay so we have it so the most important

00:07:13,590 --> 00:07:18,750
issue about how to hijack the RAF and

00:07:15,870 --> 00:07:21,750
the other issue and the other problem in

00:07:18,750 --> 00:07:25,230
implementation detail is how we can get

00:07:21,750 --> 00:07:28,230
the HTTP how we can monitor the ATP

00:07:25,230 --> 00:07:30,390
request so here we need to leverage what

00:07:28,230 --> 00:07:32,460
we have we done in the app or scarer

00:07:30,390 --> 00:07:36,060
yeah since we won

00:07:32,460 --> 00:07:39,120
yeah it was scary well we we need to

00:07:36,060 --> 00:07:42,450
scale applications according to is

00:07:39,120 --> 00:07:45,690
continuum matrix cpu memory or response

00:07:42,450 --> 00:07:46,680
time or throughput or casual matrix so

00:07:45,690 --> 00:07:51,000
for Earth

00:07:46,680 --> 00:07:51,600
no ATP request means there is the

00:07:51,000 --> 00:07:54,000
through pewds

00:07:51,600 --> 00:07:58,260
equal to zero over a period hands so we

00:07:54,000 --> 00:08:01,470
just need to redo that we we do the we

00:07:58,260 --> 00:08:05,340
will average the edge aggregator

00:08:01,470 --> 00:08:08,360
streaming API to get the continual

00:08:05,340 --> 00:08:13,380
matrix at HTTP start/stop event from the

00:08:08,360 --> 00:08:16,650
rewards Rock proxy yeah so here is the

00:08:13,380 --> 00:08:20,640
architecture of both all scalar and

00:08:16,650 --> 00:08:23,940
skill to zero all the components in the

00:08:20,640 --> 00:08:27,770
in the - box is the Rancho Earth

00:08:23,940 --> 00:08:30,420
there are the orange ones is the public

00:08:27,770 --> 00:08:31,680
component which is kernel component with

00:08:30,420 --> 00:08:34,710
is if post

00:08:31,680 --> 00:08:37,919
public API and you know once it's a

00:08:34,710 --> 00:08:41,130
internal component so for scheduled

00:08:37,919 --> 00:08:43,470
arrow the customer the and you don't

00:08:41,130 --> 00:08:46,320
need to define or to go to zero policy

00:08:43,470 --> 00:08:48,250
to the API server it's very easy it's

00:08:46,320 --> 00:08:50,949
just enable/disable

00:08:48,250 --> 00:08:53,949
and after that the magical way and

00:08:50,949 --> 00:08:56,740
aggregator will get the ATP start and

00:08:53,949 --> 00:08:59,439
stop event from the rockatuer and do the

00:08:56,740 --> 00:09:04,959
aggregation and if the if there is no

00:08:59,439 --> 00:09:08,230
ATP gtp request over a period of time

00:09:04,959 --> 00:09:10,660
just getting anything we are we will

00:09:08,230 --> 00:09:12,790
stop the application and also the jobs

00:09:10,660 --> 00:09:13,540
will be registered through the yachting

00:09:12,790 --> 00:09:15,970
API

00:09:13,540 --> 00:09:19,569
yeah the similar the similar workflow

00:09:15,970 --> 00:09:21,279
will apply to the both Kira as well the

00:09:19,569 --> 00:09:24,670
and you derm can be fun the zucchini

00:09:21,279 --> 00:09:28,930
policies through the APN server by say

00:09:24,670 --> 00:09:32,410
RI or UI dashboard and after that the

00:09:28,930 --> 00:09:34,329
matrix CPU memories through pool

00:09:32,410 --> 00:09:37,029
response Rama's about customer matrix

00:09:34,329 --> 00:09:39,129
will be aggregated and carrying event

00:09:37,029 --> 00:09:41,259
could be trigger okay

00:09:39,129 --> 00:09:43,810
we have talked too much and he returned

00:09:41,259 --> 00:09:47,290
to demo and let me see whether I have

00:09:43,810 --> 00:09:50,319
loved to to real demo today yes miss

00:09:47,290 --> 00:09:53,639
just now yeah just repeat for our

00:09:50,319 --> 00:09:56,379
session my polite is crushed so I just I

00:09:53,639 --> 00:10:01,899
just finished the installation over the

00:09:56,379 --> 00:10:05,139
40 minutes so here I have a demo

00:10:01,899 --> 00:10:12,339
application I just pushed you can see it

00:10:05,139 --> 00:10:16,449
is running and and now I have let me

00:10:12,339 --> 00:10:32,230
check our oh sorry

00:10:16,449 --> 00:10:34,000
yeah since I currently we are delivered

00:10:32,230 --> 00:10:36,699
the skill to zero as a separate

00:10:34,000 --> 00:10:39,250
Department yeah I think we owe it didn't

00:10:36,699 --> 00:10:43,990
integrated into the both Kara directly

00:10:39,250 --> 00:10:45,610
and to and to enable the skill to zero

00:10:43,990 --> 00:10:52,720
and the demo

00:10:45,610 --> 00:10:54,750
ABP I just need to do this let me watch

00:10:52,720 --> 00:10:54,750
it

00:10:55,549 --> 00:11:02,429
yeah we can watch the status of that and

00:10:59,099 --> 00:11:04,349
and during the waiting time we can share

00:11:02,429 --> 00:11:07,589
what I have done in the script

00:11:04,349 --> 00:11:10,979
it just really requests to the API

00:11:07,589 --> 00:11:13,949
server and to to claim the bridge with

00:11:10,979 --> 00:11:16,109
your version is 30 seconds 30 seconds so

00:11:13,949 --> 00:11:18,929
we need to wait for one minute when

00:11:16,109 --> 00:11:26,629
there is no request yeah it will be

00:11:18,929 --> 00:11:33,449
stopped or maybe 30 seconds is too long

00:11:26,629 --> 00:11:37,319
we need to wait for a while yeah and it

00:11:33,449 --> 00:11:40,970
normally 30 maybe I I forgot one thing

00:11:37,319 --> 00:11:40,970
to make a requester first

00:11:43,860 --> 00:11:47,399
[Music]

00:11:57,190 --> 00:12:03,610
so but and we will just crypt yeah I

00:12:00,550 --> 00:12:05,620
want you yeah there is no running

00:12:03,610 --> 00:12:10,139
instance for this application now so it

00:12:05,620 --> 00:12:17,160
is stopped so let's make a request for

00:12:10,139 --> 00:12:17,160
to this endpoint to wake it up again

00:12:17,519 --> 00:12:25,449
okay and make a request and now it is

00:12:20,529 --> 00:12:28,209
starting and yet the request is still

00:12:25,449 --> 00:12:31,149
waiting is it still holding with we will

00:12:28,209 --> 00:12:35,769
hold it until the application container

00:12:31,149 --> 00:12:37,660
is running and yeah now we get the

00:12:35,769 --> 00:12:40,329
HelloWorld response and the total

00:12:37,660 --> 00:12:45,069
latency is eighteen eighty seconds and

00:12:40,329 --> 00:12:50,050
now we can make a request again and this

00:12:45,069 --> 00:12:53,350
time is pretty fast okay so we and we

00:12:50,050 --> 00:12:57,720
are here and also we can do a query to

00:12:53,350 --> 00:13:00,910
check the audit log of what has happened

00:12:57,720 --> 00:13:04,750
yeah with top h1 there is no ingress

00:13:00,910 --> 00:13:09,490
request and and we can talk about us

00:13:04,750 --> 00:13:14,439
request received finish - we can disable

00:13:09,490 --> 00:13:16,480
that okay and you can see that as you

00:13:14,439 --> 00:13:18,579
can see that here after I push our

00:13:16,480 --> 00:13:20,920
application I just need to do some

00:13:18,579 --> 00:13:23,019
simple API calls you don't need to ban

00:13:20,920 --> 00:13:25,779
the service you don't need to do

00:13:23,019 --> 00:13:28,509
anything else so it is quite easy to

00:13:25,779 --> 00:13:31,449
consume we call it as a building service

00:13:28,509 --> 00:13:33,750
and in fact you can also consume the

00:13:31,449 --> 00:13:38,050
same experience in a path Keller

00:13:33,750 --> 00:13:41,559
yeah so that is the part of Apple scarer

00:13:38,050 --> 00:13:44,199
but sorry able to zero in but just now

00:13:41,559 --> 00:13:48,069
you can notice we just you trigger the

00:13:44,199 --> 00:13:51,370
skill to zero with incoming HTTP request

00:13:48,069 --> 00:13:53,529
so maybe but currently we have some

00:13:51,370 --> 00:13:57,610
custom metrics supporting Apple together

00:13:53,529 --> 00:14:00,850
so maybe if it is not a tree yeah it's

00:13:57,610 --> 00:14:04,059
listen yeah it depends on the evaluation

00:14:00,850 --> 00:14:06,189
yeah maybe we can try to trigger the

00:14:04,059 --> 00:14:08,300
superior according to your behavior

00:14:06,189 --> 00:14:11,569
according to the

00:14:08,300 --> 00:14:14,749
matrix that you report it so that ge-h

00:14:11,569 --> 00:14:19,660
can motor up to you too once the

00:14:14,749 --> 00:14:19,660
application could be to go to zero okay

00:14:19,900 --> 00:14:22,970
[Music]

00:14:23,940 --> 00:14:36,139
[Applause]

00:14:30,429 --> 00:14:38,449
let's get set up here so a feature we

00:14:36,139 --> 00:14:41,720
just saw is of course in development and

00:14:38,449 --> 00:14:43,999
part of the next one of the next

00:14:41,720 --> 00:14:47,920
upcoming releases the feature that I'm

00:14:43,999 --> 00:14:57,980
talking now is not really scary to zero

00:14:47,920 --> 00:15:02,569
its I have to switch with one hand sorry

00:14:57,980 --> 00:15:04,429
so but it's a feature that's coming

00:15:02,569 --> 00:15:07,100
I think definitely in the next big

00:15:04,429 --> 00:15:11,059
release of the autoscaler and that's the

00:15:07,100 --> 00:15:16,309
custom metrics feature so what is a

00:15:11,059 --> 00:15:18,110
custom metric in our case so we have an

00:15:16,309 --> 00:15:21,230
app out of scale always had support for

00:15:18,110 --> 00:15:24,410
the standard metrics or CPU usage memory

00:15:21,230 --> 00:15:26,720
usage of course response time or

00:15:24,410 --> 00:15:30,799
throughput which is basically the

00:15:26,720 --> 00:15:33,110
inverse of that and yesterday they were

00:15:30,799 --> 00:15:37,459
always supported and they come either

00:15:33,110 --> 00:15:40,879
directly from the runtime in the first

00:15:37,459 --> 00:15:43,519
two cases or they are basically derived

00:15:40,879 --> 00:15:47,360
from just listening to the locks and

00:15:43,519 --> 00:15:48,889
looking at HTTP start/stop events how

00:15:47,360 --> 00:15:52,869
they were called formally I guess

00:15:48,889 --> 00:15:56,959
nowadays they're a little bit different

00:15:52,869 --> 00:15:59,869
so custom metrics just allow you the

00:15:56,959 --> 00:16:03,949
application basically to submit metrics

00:15:59,869 --> 00:16:06,920
to us through push approach I mean

00:16:03,949 --> 00:16:09,110
that's the current state and if there

00:16:06,920 --> 00:16:11,329
are other requests we could also switch

00:16:09,110 --> 00:16:13,220
to them to other approaches but

00:16:11,329 --> 00:16:15,740
currently it's a brute push approach and

00:16:13,220 --> 00:16:17,689
of course in that zone not everybody can

00:16:15,740 --> 00:16:19,309
just submit metrics for your application

00:16:17,689 --> 00:16:22,140
it needs to be don't need to be

00:16:19,309 --> 00:16:25,050
credentials and these are the two

00:16:22,140 --> 00:16:27,000
things that we need media to tackle so

00:16:25,050 --> 00:16:29,550
how do you use the custom metrics

00:16:27,000 --> 00:16:31,320
feature basically you just start by

00:16:29,550 --> 00:16:34,440
claiming a custom metric in their

00:16:31,320 --> 00:16:37,140
scaling policy so this is how enormous

00:16:34,440 --> 00:16:40,500
scaling policy in Oporto scaler looks

00:16:37,140 --> 00:16:44,100
like there's one rule in this scaling

00:16:40,500 --> 00:16:53,520
policy and the one special thing is

00:16:44,100 --> 00:16:57,060
basically that let's see is that we here

00:16:53,520 --> 00:16:59,010
in the custom we just define a metric

00:16:57,060 --> 00:17:01,710
type in this this time it's called

00:16:59,010 --> 00:17:03,780
custom that is not one of the standard

00:17:01,710 --> 00:17:06,660
metrics so normally there would be CPU

00:17:03,780 --> 00:17:09,209
or Ruppert or something like that from

00:17:06,660 --> 00:17:11,459
the standard metric types there but in

00:17:09,209 --> 00:17:13,610
this case we just you just write in your

00:17:11,459 --> 00:17:17,070
string in and that's new custom metric

00:17:13,610 --> 00:17:18,750
and for those that don't know how a

00:17:17,070 --> 00:17:20,459
scaling rule looks like it's basically

00:17:18,750 --> 00:17:23,520
at the moment like that that you give it

00:17:20,459 --> 00:17:25,829
a threshold you say okay over the

00:17:23,520 --> 00:17:29,060
threshold with that operator you need to

00:17:25,829 --> 00:17:31,200
adjust by plus one by adding one

00:17:29,060 --> 00:17:33,180
application instance if that rule is

00:17:31,200 --> 00:17:35,340
triggered and then you need of course

00:17:33,180 --> 00:17:38,280
that policy to attach your application

00:17:35,340 --> 00:17:40,380
either using the CLI with attach policy

00:17:38,280 --> 00:17:42,390
or if you are not running the built-in

00:17:40,380 --> 00:17:45,060
mode so at the app autoscaler can be

00:17:42,390 --> 00:17:47,910
provided in two modes a built-in mode

00:17:45,060 --> 00:17:49,860
and there's always brokered mode

00:17:47,910 --> 00:17:52,140
basically and if you have a service

00:17:49,860 --> 00:17:55,050
broker and you can of course attach the

00:17:52,140 --> 00:17:58,350
policy with fine policy every spined

00:17:55,050 --> 00:18:01,130
service sorry so then you need of course

00:17:58,350 --> 00:18:04,830
a credential to start submitting the

00:18:01,130 --> 00:18:07,230
credentials to the autoscaler

00:18:04,830 --> 00:18:09,180
and that's well it depends on how you

00:18:07,230 --> 00:18:10,320
set up if you are using the service

00:18:09,180 --> 00:18:12,540
broker mode of course you get the

00:18:10,320 --> 00:18:15,030
credential value obvious planning pretty

00:18:12,540 --> 00:18:17,820
standard if you're using the built-in

00:18:15,030 --> 00:18:21,540
mode and you need to you either use the

00:18:17,820 --> 00:18:24,570
command line or the dashboard to create

00:18:21,540 --> 00:18:27,210
credentials explicitly and then you

00:18:24,570 --> 00:18:31,500
start basically submitting those metrics

00:18:27,210 --> 00:18:32,870
to the metric forward component as it's

00:18:31,500 --> 00:18:34,430
called in the autoscaler

00:18:32,870 --> 00:18:36,110
and

00:18:34,430 --> 00:18:37,580
it's a very simple payload that you have

00:18:36,110 --> 00:18:40,940
to give you have to give the instance

00:18:37,580 --> 00:18:43,880
index for which this custom metric

00:18:40,940 --> 00:18:46,640
applies and then the custom metric

00:18:43,880 --> 00:18:50,900
itself telling us the value that you

00:18:46,640 --> 00:18:54,140
want to submit so in that picture that

00:18:50,900 --> 00:18:57,320
we saw before we already saw that the

00:18:54,140 --> 00:19:00,530
components are that are now used for

00:18:57,320 --> 00:19:02,900
that and that's on the one hand that

00:19:00,530 --> 00:19:06,140
metric for water that's publicly exposed

00:19:02,900 --> 00:19:07,520
to your application there you submit by

00:19:06,140 --> 00:19:11,120
the dressed interface that I just

00:19:07,520 --> 00:19:13,100
outlined there custom metrics the metric

00:19:11,120 --> 00:19:15,470
for water will do exactly what the name

00:19:13,100 --> 00:19:17,810
says it will forward the matrixyl

00:19:15,470 --> 00:19:20,740
aggregator and then it will explore

00:19:17,810 --> 00:19:24,290
exactly like the standard metrics flow

00:19:20,740 --> 00:19:26,660
through the app autoscaler and we will

00:19:24,290 --> 00:19:30,890
or the a part of scaler will do scaling

00:19:26,660 --> 00:19:33,110
decisions based on those metrics yes

00:19:30,890 --> 00:19:44,630
okay that's basically it for that and

00:19:33,110 --> 00:19:47,450
now a small demo for that so let's start

00:19:44,630 --> 00:19:50,510
from the start so I just one thing that

00:19:47,450 --> 00:19:53,600
I wanted you to show you we are now

00:19:50,510 --> 00:19:57,290
running on u-20 which is a live

00:19:53,600 --> 00:20:15,100
landscape of sa peak load so let's okay

00:19:57,290 --> 00:20:15,100
completely missed and sorry okay sorry

00:20:15,880 --> 00:20:20,660
so we are running on u-20 which is a

00:20:18,950 --> 00:20:23,180
life landscape of the ASAP cloud

00:20:20,660 --> 00:20:24,920
platform cloud foundry environment we

00:20:23,180 --> 00:20:27,520
said I want to show you that even though

00:20:24,920 --> 00:20:31,010
that feature is not currently in a

00:20:27,520 --> 00:20:33,230
released version of the autoscaler it's

00:20:31,010 --> 00:20:35,960
a stable feature already it's already in

00:20:33,230 --> 00:20:38,890
GA for s AP customers so can already

00:20:35,960 --> 00:20:41,860
built on that it's already in use and

00:20:38,890 --> 00:20:45,080
one demo didn't I want to show right now

00:20:41,860 --> 00:20:47,260
so when we started offering that feature

00:20:45,080 --> 00:20:50,950
of course we got

00:20:47,260 --> 00:20:53,620
yeah request for help how to use it new

00:20:50,950 --> 00:20:56,500
feature and funnily enough almost all

00:20:53,620 --> 00:20:58,270
requests outlined one specific scenario

00:20:56,500 --> 00:20:59,860
and that one did they wanted to use and

00:20:58,270 --> 00:21:02,110
that's also cinahl reddit i want to show

00:20:59,860 --> 00:21:03,970
you it right now it's basically that

00:21:02,110 --> 00:21:05,530
they want to have a front-end

00:21:03,970 --> 00:21:07,929
application of multiple front-end

00:21:05,530 --> 00:21:14,410
applications and then they will they

00:21:07,929 --> 00:21:16,660
want to submit basically work tasks to a

00:21:14,410 --> 00:21:20,020
queue and that they want to have worker

00:21:16,660 --> 00:21:23,110
nodes that are applications also that

00:21:20,020 --> 00:21:26,080
pull work from a queue work on that and

00:21:23,110 --> 00:21:27,640
then somehow maybe put the result back

00:21:26,080 --> 00:21:32,350
into the queue and it they won't of

00:21:27,640 --> 00:21:34,809
course there there's a worker nodes to

00:21:32,350 --> 00:21:36,400
be scaled based on the current length of

00:21:34,809 --> 00:21:40,900
the queue of the depth of the queue so

00:21:36,400 --> 00:21:44,230
that it's executed as fast as possible

00:21:40,900 --> 00:21:46,750
but not with too many idle worker nodes

00:21:44,230 --> 00:21:49,390
let's say and that's basically the set

00:21:46,750 --> 00:21:52,780
up that I have now I've simplified it we

00:21:49,390 --> 00:21:55,179
have only one app of course and that is

00:21:52,780 --> 00:21:58,090
a spine bound on the one side on the

00:21:55,179 --> 00:22:01,929
autoscaler service on the other side to

00:21:58,090 --> 00:22:06,460
a queue RabbitMQ in this but this is

00:22:01,929 --> 00:22:10,600
just a detail and we will start

00:22:06,460 --> 00:22:12,429
basically by okay we see already see

00:22:10,600 --> 00:22:15,460
it's only one instance running right at

00:22:12,429 --> 00:22:19,390
the moment so we start by submitting

00:22:15,460 --> 00:22:24,570
some work into that queue and that is of

00:22:19,390 --> 00:22:27,400
course a very that's right thousand

00:22:24,570 --> 00:22:31,510
packages into it queue I think it's a

00:22:27,400 --> 00:22:36,070
very simple API that I offering so let's

00:22:31,510 --> 00:22:41,080
see it's just just quick and let's see

00:22:36,070 --> 00:22:42,450
what this call actually did so I wrote

00:22:41,080 --> 00:22:44,830
it in nodejs

00:22:42,450 --> 00:22:47,080
basically everything is just setting up

00:22:44,830 --> 00:22:48,760
the connection to the app autoscaler

00:22:47,080 --> 00:22:52,150
we're in the service program mode so we

00:22:48,760 --> 00:22:54,490
get there we get credentials from the a

00:22:52,150 --> 00:22:58,740
perm from the environment of course this

00:22:54,490 --> 00:23:01,070
time and also the UL that we need to

00:22:58,740 --> 00:23:02,899
call to when submitting has

00:23:01,070 --> 00:23:05,210
some credentials and we do our set up

00:23:02,899 --> 00:23:08,240
with the cure not very interesting but

00:23:05,210 --> 00:23:11,360
he is the most important and also very

00:23:08,240 --> 00:23:13,070
simple part that we set up an function

00:23:11,360 --> 00:23:16,659
that is called in this case every 10

00:23:13,070 --> 00:23:19,309
seconds and in that function we will

00:23:16,659 --> 00:23:21,799
basically submit the queue length so we

00:23:19,309 --> 00:23:24,320
check the queue and we check how many

00:23:21,799 --> 00:23:26,480
messages are in that queue and with from

00:23:24,320 --> 00:23:29,779
that construct basically the message

00:23:26,480 --> 00:23:32,840
that we are sending to the app

00:23:29,779 --> 00:23:35,929
autoscaler that's a unit if you look

00:23:32,840 --> 00:23:38,539
closely that's not in use basically and

00:23:35,929 --> 00:23:39,980
it's something that's going away but in

00:23:38,539 --> 00:23:42,350
that version it's still there but it's

00:23:39,980 --> 00:23:45,169
not used of course we need to get the

00:23:42,350 --> 00:23:47,450
credentials from also from the app

00:23:45,169 --> 00:23:55,509
environment and then we start submitting

00:23:47,450 --> 00:24:00,129
those those custom metrics just look

00:23:55,509 --> 00:24:00,129
quickly if it's doing that

00:24:07,580 --> 00:24:12,440
so every 10 seconds it's already

00:24:10,019 --> 00:24:15,779
submitting it at the moment a very of

00:24:12,440 --> 00:24:17,580
970 let's quickly look at the auto

00:24:15,779 --> 00:24:21,269
scaling policy that we have actually

00:24:17,580 --> 00:24:23,009
bound to that app I did it through

00:24:21,269 --> 00:24:25,409
service binding that you of course you

00:24:23,009 --> 00:24:27,630
can use the CLI for that so here's a

00:24:25,409 --> 00:24:29,880
little bit more in the in the in the

00:24:27,630 --> 00:24:32,039
auto scaling policy and we say that if

00:24:29,880 --> 00:24:35,039
these queue lengths this custom metric

00:24:32,039 --> 00:24:37,559
that we defined is greater than 100 we

00:24:35,039 --> 00:24:40,769
do a plus to adjustment if it's greater

00:24:37,559 --> 00:24:43,169
10 plus 1 if it's lower equals intend

00:24:40,769 --> 00:24:48,950
and we start scaling down let's look if

00:24:43,169 --> 00:24:54,990
that actually did something already not

00:24:48,950 --> 00:24:58,919
that's hope now of course it's GI

00:24:54,990 --> 00:25:01,350
software so it works yeah we started we

00:24:58,919 --> 00:25:04,350
already did one scaling event so we

00:25:01,350 --> 00:25:07,019
scaled up by plus 2 since we were over

00:25:04,350 --> 00:25:11,370
the threshold of 100 and the rules are

00:25:07,019 --> 00:25:14,580
evaluated from the top so in 60 seconds

00:25:11,370 --> 00:25:17,429
more 80 seconds more will probably scale

00:25:14,580 --> 00:25:24,509
up a little bit more so that's basically

00:25:17,429 --> 00:25:36,809
for that demo that's the a custom

00:25:24,509 --> 00:25:39,470
metrics feature and Stratos autoscaler

00:25:36,809 --> 00:25:39,470
extensions

00:25:44,299 --> 00:25:51,750
it's perience we also have a gift engine

00:25:47,690 --> 00:25:54,690
Stratus to display all the ocular staffs

00:25:51,750 --> 00:25:57,240
in the stresses in the future once the

00:25:54,690 --> 00:26:00,659
skill to the particle in it will be

00:25:57,240 --> 00:26:03,710
displayed in the same flames yeah

00:26:00,659 --> 00:26:06,210
once you install straight holes and view

00:26:03,710 --> 00:26:09,809
application in that there will be

00:26:06,210 --> 00:26:12,659
automatic auto scale or tap inside you

00:26:09,809 --> 00:26:15,900
can you can check the scale you can add

00:26:12,659 --> 00:26:21,330
is getting policies schedules and matrix

00:26:15,900 --> 00:26:23,549
here yes and also for the for the cosmic

00:26:21,330 --> 00:26:26,220
matrix since you need to generate your

00:26:23,549 --> 00:26:28,950
credential for that so there is a manual

00:26:26,220 --> 00:26:31,020
to a future to allow you to generate the

00:26:28,950 --> 00:26:33,030
credential the APR q then you

00:26:31,020 --> 00:26:35,850
communicate with the measure for water

00:26:33,030 --> 00:26:39,299
and this is the matrix for the Catherine

00:26:35,850 --> 00:26:41,850
matrix magic is plate view and this is

00:26:39,299 --> 00:26:44,820
the scaling history that just are the

00:26:41,850 --> 00:26:49,679
same this showed by the virtue of the

00:26:44,820 --> 00:26:53,520
COI so that it was natural for about

00:26:49,679 --> 00:26:55,679
scarer you so oh definitely yeah we will

00:26:53,520 --> 00:26:58,799
add to go to the ER into the obscure

00:26:55,679 --> 00:27:01,530
spot and also here and also another

00:26:58,799 --> 00:27:03,570
topic is to simplify this getting the

00:27:01,530 --> 00:27:06,419
definition just now you can see some

00:27:03,570 --> 00:27:09,330
wrong choices so maybe in the future in

00:27:06,419 --> 00:27:11,700
the future we will change it to attack

00:27:09,330 --> 00:27:13,350
it to orange it approach the current is

00:27:11,700 --> 00:27:16,980
getting you will still work about you

00:27:13,350 --> 00:27:19,559
will make you will simplify the

00:27:16,980 --> 00:27:24,419
policy definition is periods and also

00:27:19,559 --> 00:27:26,220
all the other yeah yes provide more

00:27:24,419 --> 00:27:29,700
there I suppose to do this

00:27:26,220 --> 00:27:33,409
implementation and also we will do some

00:27:29,700 --> 00:27:36,929
internal sharding or a more AJ

00:27:33,409 --> 00:27:40,080
enhancement to gadgets ready and also

00:27:36,929 --> 00:27:42,230
some documents updates thank you that's

00:27:40,080 --> 00:27:42,230

YouTube URL: https://www.youtube.com/watch?v=GlehuGJzpV0


