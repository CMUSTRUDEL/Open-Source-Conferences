Title: The Internals of Metric Store - Todd Persen, Pivotal
Publication date: 2019-09-13
Playlist: Cloud Foundry Summit EU 2019 - The Hague
Description: 
	The Internals of Metric Store - Todd Persen, Pivotal 

In this talk, Todd will walk through the high-level source code of Metric Store and highlight its key components - ingress via Loggregator, persistence with the InfluxDB storage engine, querying using PromQL, and authorization with the CF Auth Proxy. After a thorough tour of this codebase, attendees should have a good grasp on the architecture and be able to build new applications that leverage the features of Metric Store and contribute to the open source project if they so desire. 

For more info: https://www.cloudfoundry.org/
Captions: 
	00:00:00,000 --> 00:00:05,279
my name is Todd Pearson I'm an engineer

00:00:03,360 --> 00:00:09,240
on the data observability team at

00:00:05,279 --> 00:00:10,620
pivotal working on specifically metrics

00:00:09,240 --> 00:00:15,330
tour but we have a couple other products

00:00:10,620 --> 00:00:18,480
we work on in our Denver office so this

00:00:15,330 --> 00:00:20,070
talk is is gonna be kind of just going

00:00:18,480 --> 00:00:21,810
through the metrics tour project I

00:00:20,070 --> 00:00:23,820
talked a little bit about some of the

00:00:21,810 --> 00:00:24,960
ways you can use it for observability in

00:00:23,820 --> 00:00:26,460
a previous talk but if you have any

00:00:24,960 --> 00:00:28,920
questions specifically about metrics

00:00:26,460 --> 00:00:30,689
tour or how you can use it or what it

00:00:28,920 --> 00:00:32,669
does after this talk feel free to grab

00:00:30,689 --> 00:00:35,190
me afterwards or you know drop me a line

00:00:32,669 --> 00:00:37,200
after the talk but the goal basically

00:00:35,190 --> 00:00:38,790
with this talk is kind of go over how

00:00:37,200 --> 00:00:41,430
magics are structured kind of what the

00:00:38,790 --> 00:00:44,280
internal design is like so that

00:00:41,430 --> 00:00:45,989
theoretically you could contribute to it

00:00:44,280 --> 00:00:47,070
in the open source world if you wanted

00:00:45,989 --> 00:00:49,050
to

00:00:47,070 --> 00:00:51,899
ideally you know we've kind of taken

00:00:49,050 --> 00:00:54,449
this thing and made it pretty easy to

00:00:51,899 --> 00:00:55,980
use but you should be able to be able to

00:00:54,449 --> 00:00:57,120
build applications on top of it somebody

00:00:55,980 --> 00:00:59,160
and the previous talk was asking just

00:00:57,120 --> 00:01:00,809
about you know sorry about that how they

00:00:59,160 --> 00:01:01,829
could add their own alerting scripts and

00:01:00,809 --> 00:01:04,229
things like that on top of it for a

00:01:01,829 --> 00:01:05,790
custom deployment so hopefully this kind

00:01:04,229 --> 00:01:07,979
of gives you a sense of how it works

00:01:05,790 --> 00:01:10,650
that you can interact with it and then

00:01:07,979 --> 00:01:12,180
also you know boundaries constantly

00:01:10,650 --> 00:01:13,710
evolving logger gator itself is going

00:01:12,180 --> 00:01:15,600
through a bunch of different changes so

00:01:13,710 --> 00:01:17,430
this will kind of help you think about

00:01:15,600 --> 00:01:20,939
how much exert will continue to evolve

00:01:17,430 --> 00:01:24,600
as we go forward and as the longer Gator

00:01:20,939 --> 00:01:27,210
subsystem changes okay so what is

00:01:24,600 --> 00:01:30,960
metrics store it's a time series

00:01:27,210 --> 00:01:33,450
database that has persistent storage so

00:01:30,960 --> 00:01:36,240
unlike log cache which I'll talk about a

00:01:33,450 --> 00:01:37,950
little bit in just a second it actually

00:01:36,240 --> 00:01:40,500
stores data on disk is durable between

00:01:37,950 --> 00:01:42,090
restarts and lets you take all the

00:01:40,500 --> 00:01:43,740
metrics that are coming through your

00:01:42,090 --> 00:01:45,899
system and store them so that you can

00:01:43,740 --> 00:01:48,600
query them over long periods of time so

00:01:45,899 --> 00:01:50,220
if you've kind of been using Cloud

00:01:48,600 --> 00:01:51,450
Foundry for a while you've probably seen

00:01:50,220 --> 00:01:53,009
some of the evolution that led up to

00:01:51,450 --> 00:01:56,430
this but you know initially there was

00:01:53,009 --> 00:01:57,780
kind of just the firehose and you know

00:01:56,430 --> 00:02:00,450
you either had to be listening to it

00:01:57,780 --> 00:02:02,100
with a nozzle or a CLI tool to see

00:02:00,450 --> 00:02:03,060
what's going through it and then you

00:02:02,100 --> 00:02:05,490
know probably about a year and a half

00:02:03,060 --> 00:02:06,750
ago we started working on log cache and

00:02:05,490 --> 00:02:08,580
then as that came out it gave you the

00:02:06,750 --> 00:02:10,349
ability to cache things for shorter

00:02:08,580 --> 00:02:12,040
durations of time but it still wasn't

00:02:10,349 --> 00:02:14,019
durable and

00:02:12,040 --> 00:02:15,489
you still had a limited amount of time

00:02:14,019 --> 00:02:18,250
that you could actually store metrics

00:02:15,489 --> 00:02:19,659
for because we were only using in memory

00:02:18,250 --> 00:02:21,340
storage at that point so about a year

00:02:19,659 --> 00:02:22,959
ago we kind of took a lot of the

00:02:21,340 --> 00:02:24,400
feedback about log cache and people who

00:02:22,959 --> 00:02:26,319
wanted to have a little bit more

00:02:24,400 --> 00:02:29,010
duration for their metrics and we

00:02:26,319 --> 00:02:31,720
started making that kind of same design

00:02:29,010 --> 00:02:33,220
persistent and that codebase has has

00:02:31,720 --> 00:02:35,260
become metric stores so originally it

00:02:33,220 --> 00:02:36,700
was just a fork of log - we just like

00:02:35,260 --> 00:02:39,510
took the whole thing forked it and

00:02:36,700 --> 00:02:42,489
started modifying it in place and that

00:02:39,510 --> 00:02:43,599
has now diverged quite a bit you

00:02:42,489 --> 00:02:45,340
probably wouldn't recognize the two code

00:02:43,599 --> 00:02:46,450
bases as being very similar but there

00:02:45,340 --> 00:02:48,730
are still some patterns and

00:02:46,450 --> 00:02:50,560
authentication design patterns that

00:02:48,730 --> 00:02:53,109
we've used that still are the same as

00:02:50,560 --> 00:02:55,959
what log cache uses it's written

00:02:53,109 --> 00:02:57,939
entirely and go and for the storage side

00:02:55,959 --> 00:02:59,680
of it we actually just used the storage

00:02:57,939 --> 00:03:02,230
engine from in flux TB directly so

00:02:59,680 --> 00:03:05,079
basically everything below the influx

00:03:02,230 --> 00:03:07,810
query engine we just took all that in

00:03:05,079 --> 00:03:09,280
and that's how we're using that's our

00:03:07,810 --> 00:03:11,109
actually storing data on disk and how it

00:03:09,280 --> 00:03:12,970
decided to destructure the project cuz

00:03:11,109 --> 00:03:15,150
it gave us basically the hardest piece

00:03:12,970 --> 00:03:18,310
of it which is you know compressed

00:03:15,150 --> 00:03:20,169
sharded time series storage we basically

00:03:18,310 --> 00:03:24,639
got that part for free from a relatively

00:03:20,169 --> 00:03:27,310
mature project and then we kind of as we

00:03:24,639 --> 00:03:28,629
were we're evolving log cache a lot of

00:03:27,310 --> 00:03:30,729
people wanted to see if there was a way

00:03:28,629 --> 00:03:31,870
that they could use law cache the same

00:03:30,729 --> 00:03:34,030
way that they would use prometheus would

00:03:31,870 --> 00:03:36,340
be able to support prom QL we started

00:03:34,030 --> 00:03:38,620
adding from ql to log cache initially

00:03:36,340 --> 00:03:40,060
and it ended up being kind of tricky

00:03:38,620 --> 00:03:43,629
with the way that law cache was designed

00:03:40,060 --> 00:03:44,799
because the in memory store tried to

00:03:43,629 --> 00:03:47,019
make things fast but at the same time

00:03:44,799 --> 00:03:49,479
make querying with filters really

00:03:47,019 --> 00:03:51,340
difficult so when we did metric start we

00:03:49,479 --> 00:03:53,560
actually took the entire Prometheus

00:03:51,340 --> 00:03:55,689
query engine and and stuck it in there

00:03:53,560 --> 00:03:57,220
so our goal with that was to try and be

00:03:55,689 --> 00:03:58,569
a hundred percent from through all

00:03:57,220 --> 00:04:01,870
compatible there are still some cases

00:03:58,569 --> 00:04:03,250
where kind of authentication or

00:04:01,870 --> 00:04:05,049
authorization makes it a little bit

00:04:03,250 --> 00:04:06,939
tricky but I'll talk a little bit about

00:04:05,049 --> 00:04:08,620
how that works and that's kind of that

00:04:06,939 --> 00:04:10,689
we'll get into the the role based access

00:04:08,620 --> 00:04:12,579
control model that we took from log

00:04:10,689 --> 00:04:16,840
cache which tries to work natively with

00:04:12,579 --> 00:04:18,310
the boundary as well so metric store has

00:04:16,840 --> 00:04:20,769
has three main processes if you look

00:04:18,310 --> 00:04:22,779
inside of the of the package so the

00:04:20,769 --> 00:04:24,070
nozzle is the first component so that's

00:04:22,779 --> 00:04:25,570
the one that actually connects to the

00:04:24,070 --> 00:04:28,210
reverse log proxy

00:04:25,570 --> 00:04:30,760
for a vlog regatta and essentially takes

00:04:28,210 --> 00:04:32,890
everything that is a timer counter or

00:04:30,760 --> 00:04:35,020
gauge envelope leaves all the logs

00:04:32,890 --> 00:04:37,000
behind but it takes anything that's

00:04:35,020 --> 00:04:38,770
that's an actual metric that has a value

00:04:37,000 --> 00:04:41,320
associated with it and pulls those in

00:04:38,770 --> 00:04:43,540
and the nozzle is what handles the

00:04:41,320 --> 00:04:45,070
formatting from envelopes to kind of our

00:04:43,540 --> 00:04:47,110
native type and I'll talk about that a

00:04:45,070 --> 00:04:48,760
little bit in a second and the off

00:04:47,110 --> 00:04:51,130
proxies kind of on the other end so

00:04:48,760 --> 00:04:53,110
that's the piece that actually you know

00:04:51,130 --> 00:04:55,930
gets connected to by go router works

00:04:53,110 --> 00:04:57,310
with UA and Kathy and figures out who

00:04:55,930 --> 00:04:58,810
you are and what you can see into some

00:04:57,310 --> 00:05:00,400
filtering there so there are some there

00:04:58,810 --> 00:05:02,500
are some rules that we have to enforce

00:05:00,400 --> 00:05:07,060
with which you can see that make some of

00:05:02,500 --> 00:05:08,740
the edge cases for for queries weird so

00:05:07,060 --> 00:05:10,060
I could talk to you more about that

00:05:08,740 --> 00:05:11,740
offline if you run into problems with it

00:05:10,060 --> 00:05:13,360
but essentially because of what you can

00:05:11,740 --> 00:05:15,160
see sometimes there are some wild card

00:05:13,360 --> 00:05:16,900
queries with prom QL that you want to be

00:05:15,160 --> 00:05:19,000
able to run that you can't because it's

00:05:16,900 --> 00:05:20,770
difficult to tell if you're not an admin

00:05:19,000 --> 00:05:22,840
specifically what you're actually

00:05:20,770 --> 00:05:24,730
allowed to see and then the metrics or

00:05:22,840 --> 00:05:27,490
process is the the actual storage layer

00:05:24,730 --> 00:05:29,590
itself it takes the writes in from the

00:05:27,490 --> 00:05:31,210
nozzle stores them and then pulls them

00:05:29,590 --> 00:05:34,900
back off disk and retrieves them when a

00:05:31,210 --> 00:05:37,000
query comes in on the other side okay so

00:05:34,900 --> 00:05:39,550
the nozzle as I said it reads from the

00:05:37,000 --> 00:05:42,790
reverse log proxy so that communication

00:05:39,550 --> 00:05:46,030
stream is G RPC and if you look at a lot

00:05:42,790 --> 00:05:48,790
of the the kind of current law greater

00:05:46,030 --> 00:05:50,980
design and the old law caches on there

00:05:48,790 --> 00:05:52,600
was a lot of G RPC everywhere sooths

00:05:50,980 --> 00:05:54,580
actually we've kind of started getting

00:05:52,600 --> 00:05:57,700
away from that because G RPC is a very

00:05:54,580 --> 00:05:59,650
robust framework but it's also it comes

00:05:57,700 --> 00:06:01,720
with a lot of overhead and as we started

00:05:59,650 --> 00:06:03,040
looking at what we were basically all we

00:06:01,720 --> 00:06:05,050
were doing with the nozzles we're taking

00:06:03,040 --> 00:06:07,360
points from one place or writing them to

00:06:05,050 --> 00:06:09,840
another one so G RPC is built on top of

00:06:07,360 --> 00:06:12,160
protocol buffers which is a great

00:06:09,840 --> 00:06:13,630
durable serialization format but when

00:06:12,160 --> 00:06:15,310
you have two processes that always get

00:06:13,630 --> 00:06:16,780
updated at the same time it doesn't

00:06:15,310 --> 00:06:18,670
really buy you a lot you just need to

00:06:16,780 --> 00:06:20,920
get data from one place to another so we

00:06:18,670 --> 00:06:22,720
actually got rid of all the GRP see that

00:06:20,920 --> 00:06:24,340
we're using internally we have to use G

00:06:22,720 --> 00:06:26,860
RPC with the reverse log proxy but

00:06:24,340 --> 00:06:31,420
inside of metrics or all the rights

00:06:26,860 --> 00:06:33,100
between processes happen as just type

00:06:31,420 --> 00:06:35,320
length value encoded frames we just

00:06:33,100 --> 00:06:37,350
essentially writing it over TCP we have

00:06:35,320 --> 00:06:38,920
some minimal serialization in place

00:06:37,350 --> 00:06:42,610
there's a package

00:06:38,920 --> 00:06:44,740
go tiny it's very easy to probably you

00:06:42,610 --> 00:06:47,410
know I can't actually be mostly read me

00:06:44,740 --> 00:06:49,030
cuz it's not in English but it's works

00:06:47,410 --> 00:06:50,530
really well it's great for just go

00:06:49,030 --> 00:06:51,430
structure so you have to go processes

00:06:50,530 --> 00:06:53,890
that are talking to each other and you

00:06:51,430 --> 00:06:55,030
don't need the data to be durable at

00:06:53,890 --> 00:06:57,070
rest you just want to be able to

00:06:55,030 --> 00:07:01,240
communicate the serialization overhead

00:06:57,070 --> 00:07:04,300
is really minimal and then the one thing

00:07:01,240 --> 00:07:05,530
that we do that's that sort of changes

00:07:04,300 --> 00:07:08,440
the way that the data in the envelopes

00:07:05,530 --> 00:07:10,120
is structured is we actually take timer

00:07:08,440 --> 00:07:11,410
metrics which essentially the only place

00:07:10,120 --> 00:07:15,490
that that really ends up getting used

00:07:11,410 --> 00:07:17,140
are the the go router has this HTTP

00:07:15,490 --> 00:07:19,090
start stop event that it emits every

00:07:17,140 --> 00:07:21,670
time a process or every time the request

00:07:19,090 --> 00:07:23,890
comes in and finishes so if you have a

00:07:21,670 --> 00:07:26,320
foundation with a lot of traffic this

00:07:23,890 --> 00:07:27,490
could potentially be you know tens or

00:07:26,320 --> 00:07:30,760
hundreds of thousands of requests per

00:07:27,490 --> 00:07:32,230
second you're getting two metrics for

00:07:30,760 --> 00:07:33,490
each one of those and when we started

00:07:32,230 --> 00:07:35,950
looking at the data that we were storing

00:07:33,490 --> 00:07:38,380
we realized in some cases timer metrics

00:07:35,950 --> 00:07:39,910
could be like 90 to 95 percent of all

00:07:38,380 --> 00:07:41,590
the data that we were storing and no one

00:07:39,910 --> 00:07:43,540
first of all no one looked at it and

00:07:41,590 --> 00:07:44,710
secondly doing queries across it because

00:07:43,540 --> 00:07:46,420
it's so much data was actually

00:07:44,710 --> 00:07:49,600
problematic so what we decided to do is

00:07:46,420 --> 00:07:50,980
upfront we actually do some down

00:07:49,600 --> 00:07:55,240
sampling in the nozzle so we'll actually

00:07:50,980 --> 00:07:57,640
just collect metrics for each each

00:07:55,240 --> 00:07:59,140
different requests and request type so

00:07:57,640 --> 00:08:00,100
we get different status codes will group

00:07:59,140 --> 00:08:02,050
those but we essentially do some

00:08:00,100 --> 00:08:04,120
bucketing upfront and then write out the

00:08:02,050 --> 00:08:05,170
sort of the downsampled metrics so if

00:08:04,120 --> 00:08:06,910
you look at the way the timer metrics

00:08:05,170 --> 00:08:08,020
are structured inside metrics so it's a

00:08:06,910 --> 00:08:10,390
little bit different than the way they

00:08:08,020 --> 00:08:12,550
come through the nozzle but so far

00:08:10,390 --> 00:08:14,650
that's actually cut down on storage size

00:08:12,550 --> 00:08:17,860
and made it much more usable queryable

00:08:14,650 --> 00:08:19,180
data format and then the nozzle also

00:08:17,860 --> 00:08:20,710
does some batching just to make it as

00:08:19,180 --> 00:08:22,450
efficient as possible to write data into

00:08:20,710 --> 00:08:25,450
metrics or so by default we kind of

00:08:22,450 --> 00:08:27,190
limit the payload size to 64 kilobytes

00:08:25,450 --> 00:08:28,990
but we also you know have like a

00:08:27,190 --> 00:08:30,580
periodic flush so then the nozzle won't

00:08:28,990 --> 00:08:31,900
hold data for more than I think it's

00:08:30,580 --> 00:08:33,160
like 200 milliseconds it'll just

00:08:31,900 --> 00:08:37,060
automatically write whatever whatever is

00:08:33,160 --> 00:08:38,080
in the buffer and then so I talked a

00:08:37,060 --> 00:08:39,640
little bit about I don't know if if

00:08:38,080 --> 00:08:40,630
you're not familiar with the envelope

00:08:39,640 --> 00:08:42,729
type you can look at it but there are

00:08:40,630 --> 00:08:44,470
some some protobuf definitions that tell

00:08:42,729 --> 00:08:46,750
you what a law greater envelope looks

00:08:44,470 --> 00:08:47,830
like but we actually try to get as far

00:08:46,750 --> 00:08:49,990
away from that as possible because

00:08:47,830 --> 00:08:51,220
there's a lot of once again a lot of

00:08:49,990 --> 00:08:52,950
overhead that comes with protobuf it

00:08:51,220 --> 00:08:55,080
gives you some some great

00:08:52,950 --> 00:08:57,390
abilities but for us we just kind of

00:08:55,080 --> 00:08:59,280
needed to be able to make it as simple

00:08:57,390 --> 00:09:01,230
as possible so really this is just a

00:08:59,280 --> 00:09:03,510
basic go structure so we define a point

00:09:01,230 --> 00:09:05,010
and it's got a name so that's going to

00:09:03,510 --> 00:09:07,680
be the metric name that gets written in

00:09:05,010 --> 00:09:10,200
a time stamp which is an n64 in

00:09:07,680 --> 00:09:12,090
nanoseconds and then a value so all the

00:09:10,200 --> 00:09:13,680
values that we store our floats and that

00:09:12,090 --> 00:09:15,600
makes it pretty easy because everything

00:09:13,680 --> 00:09:17,450
that's coming to the firehose has some

00:09:15,600 --> 00:09:19,380
sort of a value associated and then

00:09:17,450 --> 00:09:20,790
labels which I'll get into a little bit

00:09:19,380 --> 00:09:24,150
more but those are essentially just key

00:09:20,790 --> 00:09:25,770
value pairs of the the labels that are

00:09:24,150 --> 00:09:28,860
coming through on the envelope as well

00:09:25,770 --> 00:09:30,930
and then over the wire we just group

00:09:28,860 --> 00:09:32,490
things into batches as I said so that's

00:09:30,930 --> 00:09:34,170
kind of what the the batch struct looks

00:09:32,490 --> 00:09:36,780
like so it's just a slice of points and

00:09:34,170 --> 00:09:38,970
that we basically took probably four or

00:09:36,780 --> 00:09:40,830
five protobuf files that were previously

00:09:38,970 --> 00:09:43,050
being used in blogger Gator and log cash

00:09:40,830 --> 00:09:44,520
and stripped it down to just that so

00:09:43,050 --> 00:09:45,810
that's really all the all the types that

00:09:44,520 --> 00:09:47,910
we're sending over the wire now which

00:09:45,810 --> 00:09:50,850
makes working with the code base a lot

00:09:47,910 --> 00:09:52,440
easier okay so going back to the other

00:09:50,850 --> 00:09:56,310
side of metric store for a minute so the

00:09:52,440 --> 00:09:59,120
auth proxy like I said it actually talks

00:09:56,310 --> 00:10:02,370
to both UA a and Cloud Controller and

00:09:59,120 --> 00:10:03,900
figures out who can see what so we

00:10:02,370 --> 00:10:06,120
actually recently implemented offline

00:10:03,900 --> 00:10:07,500
token validation in the off proxy so it

00:10:06,120 --> 00:10:09,540
actually hits the token keys end point

00:10:07,500 --> 00:10:13,140
gets the signing keys and can actually

00:10:09,540 --> 00:10:14,580
do the authentication side of it offline

00:10:13,140 --> 00:10:17,190
without having to hit you AAA for every

00:10:14,580 --> 00:10:18,660
user old versions of log cash used to do

00:10:17,190 --> 00:10:20,190
that which could be it could be

00:10:18,660 --> 00:10:21,450
problematic for UA a could be

00:10:20,190 --> 00:10:24,440
problematic for cloud controller so

00:10:21,450 --> 00:10:27,300
we've got the offline token validation

00:10:24,440 --> 00:10:29,970
that works now and then we also do some

00:10:27,300 --> 00:10:32,910
basic caching with cloud controller so

00:10:29,970 --> 00:10:34,470
that we can try not to hit glide

00:10:32,910 --> 00:10:37,980
controller for every every request to

00:10:34,470 --> 00:10:39,420
get list of who can see what and

00:10:37,980 --> 00:10:43,050
especially as we started moving away

00:10:39,420 --> 00:10:45,710
from some of the simpler log cache use

00:10:43,050 --> 00:10:48,210
cases and started moving to more of a

00:10:45,710 --> 00:10:49,380
like I guess we've moved to a world

00:10:48,210 --> 00:10:50,760
where people are doing more frequent

00:10:49,380 --> 00:10:52,650
requests so the less we can hit those

00:10:50,760 --> 00:10:54,960
external surfaces the less overhead

00:10:52,650 --> 00:10:57,270
there is on each request the less we

00:10:54,960 --> 00:10:58,650
like stress those external services but

00:10:57,270 --> 00:11:00,930
also the less that we need to spend

00:10:58,650 --> 00:11:02,040
spend time doing so between those two

00:11:00,930 --> 00:11:03,410
the offline token validation the cache

00:11:02,040 --> 00:11:05,430
and we've got to reduce the amount of

00:11:03,410 --> 00:11:07,560
external work that

00:11:05,430 --> 00:11:11,040
the auth proxy needs to do on every

00:11:07,560 --> 00:11:12,960
request and then we use we check for two

00:11:11,040 --> 00:11:14,730
different scopes there's logs admin and

00:11:12,960 --> 00:11:16,260
da pit out firehose and if your user has

00:11:14,730 --> 00:11:18,089
either of those then you'll you'll be

00:11:16,260 --> 00:11:19,500
granted like admin level access

00:11:18,089 --> 00:11:21,480
basically to the off proxies so we need

00:11:19,500 --> 00:11:23,160
the any of the filtering any of the

00:11:21,480 --> 00:11:24,870
source ID limitations that I was talking

00:11:23,160 --> 00:11:26,339
about those will be dropped so you

00:11:24,870 --> 00:11:28,260
basically be able to query query on

00:11:26,339 --> 00:11:29,490
anything and all of your prompt you'll

00:11:28,260 --> 00:11:32,250
query it should basically behave like

00:11:29,490 --> 00:11:34,440
like normal prompt QL queries and then

00:11:32,250 --> 00:11:35,970
the kind of the way I was talking about

00:11:34,440 --> 00:11:37,110
the the work that we have to do that

00:11:35,970 --> 00:11:39,120
make some problem to all queries

00:11:37,110 --> 00:11:41,760
difficult as well actually we use the

00:11:39,120 --> 00:11:43,440
Prometheus Cree engine abstract syntax

00:11:41,760 --> 00:11:45,660
tree so we actually break down the

00:11:43,440 --> 00:11:47,339
queries if you're not an admin look at

00:11:45,660 --> 00:11:49,950
the things that you're requesting and if

00:11:47,339 --> 00:11:51,990
we have to we'll add some some labels to

00:11:49,950 --> 00:11:54,180
make sure that you're not requesting it

00:11:51,990 --> 00:11:55,230
you're not allowed to and we'll if

00:11:54,180 --> 00:11:56,790
you're looking for something that you

00:11:55,230 --> 00:11:59,640
shouldn't be able to will actually

00:11:56,790 --> 00:12:00,480
reject the query so we're able to do

00:11:59,640 --> 00:12:02,700
some of that stuff because we're using

00:12:00,480 --> 00:12:04,589
like I said we're using as much of the

00:12:02,700 --> 00:12:06,330
problem from QL engine as possible and

00:12:04,589 --> 00:12:11,250
that lets us actually process the great

00:12:06,330 --> 00:12:14,220
just like Prometheus would okay so

00:12:11,250 --> 00:12:15,660
getting into the metric store itself for

00:12:14,220 --> 00:12:18,360
the persistence side as I said we're

00:12:15,660 --> 00:12:20,870
using the in Flex DB storage engine so

00:12:18,360 --> 00:12:23,820
in Flex DB recently started releasing

00:12:20,870 --> 00:12:24,900
version 2.0 the storage engine is

00:12:23,820 --> 00:12:27,540
relatively the same but we've kind of

00:12:24,900 --> 00:12:28,709
stuck with the the 1x branch so we're

00:12:27,540 --> 00:12:31,230
actually just pulling from whatever the

00:12:28,709 --> 00:12:33,900
most recent releases of that and the

00:12:31,230 --> 00:12:35,250
reason is you know influx 2.0 x' is

00:12:33,900 --> 00:12:37,770
still it's been in alpha I think might

00:12:35,250 --> 00:12:39,300
be in beta now it's still changing and

00:12:37,770 --> 00:12:41,610
we didn't want to bring in something

00:12:39,300 --> 00:12:42,900
that was you know moving quickly when

00:12:41,610 --> 00:12:45,630
all we really wanted was a relatively

00:12:42,900 --> 00:12:47,880
stable mature package for storage so

00:12:45,630 --> 00:12:49,980
that's kind of what we're well what

00:12:47,880 --> 00:12:51,600
we've got and the great things about

00:12:49,980 --> 00:12:52,589
using the in flexi storage engine it's

00:12:51,600 --> 00:12:54,810
got good compression performance

00:12:52,589 --> 00:12:56,220
especially on floats which like I said

00:12:54,810 --> 00:12:58,020
if you look look at our points that

00:12:56,220 --> 00:13:00,779
we're writing in we're only writing

00:12:58,020 --> 00:13:02,790
points in Flex DB also supports I think

00:13:00,779 --> 00:13:04,230
like in spools and strings we're not

00:13:02,790 --> 00:13:06,000
using any of those but floating point

00:13:04,230 --> 00:13:09,270
compression is probably about as good as

00:13:06,000 --> 00:13:11,190
you can get there's a new index format

00:13:09,270 --> 00:13:13,260
that they launched recently called TSI

00:13:11,190 --> 00:13:15,209
it's it's pretty good when we were using

00:13:13,260 --> 00:13:17,459
the old one before the TSI was released

00:13:15,209 --> 00:13:18,850
we occasionally have some out of memory

00:13:17,459 --> 00:13:21,579
problems if we didn't have box

00:13:18,850 --> 00:13:24,459
is that we're big enough but now with

00:13:21,579 --> 00:13:27,220
that it's relatively good it handles it

00:13:24,459 --> 00:13:29,500
handles some stale tags well so

00:13:27,220 --> 00:13:31,810
basically the the short version of it is

00:13:29,500 --> 00:13:35,380
it lets you get away with using smaller

00:13:31,810 --> 00:13:38,709
boxes for the same amount of data of

00:13:35,380 --> 00:13:41,199
storage and so far it's been it's been

00:13:38,709 --> 00:13:42,550
really stable for us the storage format

00:13:41,199 --> 00:13:44,889
itself isn't changing which is a nice

00:13:42,550 --> 00:13:46,329
property we want to be able to you know

00:13:44,889 --> 00:13:47,920
keep adding features to metric store and

00:13:46,329 --> 00:13:49,449
not have to worry about the storage

00:13:47,920 --> 00:13:51,009
format of in Flex DB changing much of

00:13:49,449 --> 00:13:53,079
her time and basically for the entire

00:13:51,009 --> 00:13:55,990
one dot X line it's basically been the

00:13:53,079 --> 00:13:59,110
same format under the hood and then easy

00:13:55,990 --> 00:14:01,420
data eviction so by that I mean that we

00:13:59,110 --> 00:14:04,240
we took in flux DB and if you if you're

00:14:01,420 --> 00:14:07,360
using in Flex DB proper you can define

00:14:04,240 --> 00:14:09,310
how how big your retention windows are

00:14:07,360 --> 00:14:11,230
how large your shards are on disk so

00:14:09,310 --> 00:14:14,769
like what what unit of time does each

00:14:11,230 --> 00:14:16,089
physical shard take up and we've kind of

00:14:14,769 --> 00:14:17,860
just gone with the decision that each

00:14:16,089 --> 00:14:19,660
shard is gonna be one day because based

00:14:17,860 --> 00:14:21,970
on most of the people that we've talked

00:14:19,660 --> 00:14:23,769
with we're usually measuring the amount

00:14:21,970 --> 00:14:26,079
of data that we're storing in in weeks

00:14:23,769 --> 00:14:27,699
maybe months but we want to be able to

00:14:26,079 --> 00:14:30,459
drop data relatively easily so in this

00:14:27,699 --> 00:14:33,519
case we sort of default to six weeks of

00:14:30,459 --> 00:14:36,279
data which is 42 days and then as each

00:14:33,519 --> 00:14:37,959
day expires so as data gets older we'll

00:14:36,279 --> 00:14:39,870
just drop the oldest day so that kind of

00:14:37,959 --> 00:14:42,759
gives you a good rolling window of

00:14:39,870 --> 00:14:44,259
storage and it's it's a thing that you

00:14:42,759 --> 00:14:46,089
know users then don't have to decide

00:14:44,259 --> 00:14:47,319
theoretically we could expose that but

00:14:46,089 --> 00:14:48,880
it's it's also something that you don't

00:14:47,319 --> 00:14:51,040
want to have to change after you started

00:14:48,880 --> 00:14:53,110
writing data into it and in Flex DB

00:14:51,040 --> 00:14:54,910
because it's stores data by time period

00:14:53,110 --> 00:14:58,360
it makes it pretty easy for us to drop

00:14:54,910 --> 00:15:00,399
data easily as it expires and then

00:14:58,360 --> 00:15:03,639
indexing is another interesting property

00:15:00,399 --> 00:15:05,139
of influx TV so when when data comes

00:15:03,639 --> 00:15:06,970
through the firehouses an envelope it's

00:15:05,139 --> 00:15:10,480
just got a bunch of labels and they're

00:15:06,970 --> 00:15:12,579
you know relatively arbitrary and in

00:15:10,480 --> 00:15:14,439
Flex DB kind of has two different

00:15:12,579 --> 00:15:17,649
concepts for how you want to think about

00:15:14,439 --> 00:15:19,930
data one is writing it as a field which

00:15:17,649 --> 00:15:20,889
is an onion xed like columnstore and

00:15:19,930 --> 00:15:23,709
that's kind of where I was talking about

00:15:20,889 --> 00:15:28,120
it stores floats in spools and strings

00:15:23,709 --> 00:15:31,240
those are fields and then tags are

00:15:28,120 --> 00:15:33,399
actually they're not stored they're just

00:15:31,240 --> 00:15:36,279
of the index that influx DB builds

00:15:33,399 --> 00:15:38,290
around your data so the upside is if you

00:15:36,279 --> 00:15:40,480
make something a tag it's relatively

00:15:38,290 --> 00:15:42,160
fast because you can run queries on it

00:15:40,480 --> 00:15:43,839
because it's part of the index it also

00:15:42,160 --> 00:15:45,760
uses up more memory because the entire

00:15:43,839 --> 00:15:48,160
index kind of has to fit in memory and

00:15:45,760 --> 00:15:49,660
that's where the the TSI that I was

00:15:48,160 --> 00:15:51,240
talking about the new index format makes

00:15:49,660 --> 00:15:53,649
that a little bit more memory efficient

00:15:51,240 --> 00:15:54,850
so you can kind of choose as your as

00:15:53,649 --> 00:15:56,410
you're running data into in flex DB

00:15:54,850 --> 00:15:58,480
whether it's going to be a tag or field

00:15:56,410 --> 00:16:01,350
and we sort of looked looked at what we

00:15:58,480 --> 00:16:03,310
had coming through the firehose and said

00:16:01,350 --> 00:16:04,510
pretty much everything should be tags

00:16:03,310 --> 00:16:06,430
because most people want to be able to

00:16:04,510 --> 00:16:08,350
filter on it and that works well with

00:16:06,430 --> 00:16:10,980
with the memory kind of that we use

00:16:08,350 --> 00:16:12,930
normally on a box but we saw a few

00:16:10,980 --> 00:16:14,890
different things coming through

00:16:12,930 --> 00:16:18,040
envelopes that were sort of terrible

00:16:14,890 --> 00:16:20,470
candidates for tagging so these six

00:16:18,040 --> 00:16:22,089
things so you are a Content length user

00:16:20,470 --> 00:16:24,480
agent requests a TV for word in a remote

00:16:22,089 --> 00:16:27,339
address these are basically things like

00:16:24,480 --> 00:16:29,170
so content length is just the size of

00:16:27,339 --> 00:16:32,140
every HTTP request that comes through

00:16:29,170 --> 00:16:33,430
the system it is not something that's

00:16:32,140 --> 00:16:35,260
worth indexing because it's essentially

00:16:33,430 --> 00:16:37,899
you could have any possible value from

00:16:35,260 --> 00:16:39,310
like 0 to infinity it's also not

00:16:37,899 --> 00:16:40,750
something that you're ever gonna query

00:16:39,310 --> 00:16:42,100
on you're not gonna say give me

00:16:40,750 --> 00:16:43,450
everything that's content length of a

00:16:42,100 --> 00:16:45,070
thousand like it's just not something

00:16:43,450 --> 00:16:47,829
that people want to do with that data so

00:16:45,070 --> 00:16:50,050
by leaving these out I think we've kind

00:16:47,829 --> 00:16:51,640
of removed some of the things that add a

00:16:50,050 --> 00:16:53,950
lot of condensing overhead and would

00:16:51,640 --> 00:16:55,450
require more memory they still get

00:16:53,950 --> 00:16:57,040
stored as field so if you wanted to be

00:16:55,450 --> 00:16:58,720
able to query them this data is still

00:16:57,040 --> 00:17:01,600
available we still write as a disk but

00:16:58,720 --> 00:17:02,709
we don't we don't sort in the index so

00:17:01,600 --> 00:17:04,329
and I'll talk about this on another

00:17:02,709 --> 00:17:05,770
slide as you're creating that data these

00:17:04,329 --> 00:17:08,500
are actually things that you would not

00:17:05,770 --> 00:17:10,030
be able to use if you were doing like an

00:17:08,500 --> 00:17:12,040
autocomplete or if using the labels and

00:17:10,030 --> 00:17:15,100
pointed Prometheus these won't you can't

00:17:12,040 --> 00:17:16,750
actually look for these directly and

00:17:15,100 --> 00:17:18,040
that's that last bullet point so I'll

00:17:16,750 --> 00:17:21,220
talk about those implications in a

00:17:18,040 --> 00:17:22,569
minute so on the querying side like I

00:17:21,220 --> 00:17:24,160
said we're using the query engine

00:17:22,569 --> 00:17:26,050
directly from Prometheus we just import

00:17:24,160 --> 00:17:28,900
those libraries directly and we just

00:17:26,050 --> 00:17:31,990
have some sort of adapters that make it

00:17:28,900 --> 00:17:33,130
easy for us to get get that piece hooked

00:17:31,990 --> 00:17:34,480
up to the storage engine and I'll talk

00:17:33,130 --> 00:17:38,290
about kind of how that works in just a

00:17:34,480 --> 00:17:41,830
second and I was mentioned the G RPC

00:17:38,290 --> 00:17:43,929
problem a little bit and we probably

00:17:41,830 --> 00:17:45,850
prior to about a month ago we had

00:17:43,929 --> 00:17:47,379
another process called the Gateway and

00:17:45,850 --> 00:17:49,480
if you look at log cache you'll still

00:17:47,379 --> 00:17:52,629
see this process so the Gateway was

00:17:49,480 --> 00:17:54,580
basically just it sat in between metric

00:17:52,629 --> 00:17:57,970
store and the auth proxy and it was just

00:17:54,580 --> 00:17:59,379
a G RPC to JSON converter because the

00:17:57,970 --> 00:18:00,999
decision had been made previously that

00:17:59,379 --> 00:18:03,429
everything spoke to your PC and that's

00:18:00,999 --> 00:18:05,860
not something that you can easily talk

00:18:03,429 --> 00:18:07,629
to from the outside world so essentially

00:18:05,860 --> 00:18:10,539
all that gateway process did is just

00:18:07,629 --> 00:18:13,720
convert the G RPC responses coming from

00:18:10,539 --> 00:18:17,259
log - or metric store to JSON that was

00:18:13,720 --> 00:18:18,669
prom QL compatible ish and as we started

00:18:17,259 --> 00:18:19,509
looking at more of the integrations

00:18:18,669 --> 00:18:21,789
we're doing with the Prometheus

00:18:19,509 --> 00:18:24,009
libraries we kind of realized gee our PC

00:18:21,789 --> 00:18:25,840
was offering us nothing and what we did

00:18:24,009 --> 00:18:27,429
is just remove that gateway process and

00:18:25,840 --> 00:18:30,970
made it so that the metric store itself

00:18:27,429 --> 00:18:33,399
now just uses the the API libraries from

00:18:30,970 --> 00:18:35,230
Prometheus so it can speak JSON natively

00:18:33,399 --> 00:18:36,580
so it actually just acts like a

00:18:35,230 --> 00:18:39,220
Prometheus now and that means the auth

00:18:36,580 --> 00:18:44,470
proxy can just talk directly to that

00:18:39,220 --> 00:18:46,840
that JSON subject on api's so let's see

00:18:44,470 --> 00:18:48,100
yeah so now as basically as of the next

00:18:46,840 --> 00:18:49,299
release of you looking master at the

00:18:48,100 --> 00:18:50,470
Gateway process will be gone the next

00:18:49,299 --> 00:18:52,210
release won't have it any more

00:18:50,470 --> 00:18:53,980
so if you see any changes if you've been

00:18:52,210 --> 00:18:55,330
using metric store already and you saw

00:18:53,980 --> 00:18:57,369
that that process went away that that

00:18:55,330 --> 00:18:59,350
was the reason why and in general this

00:18:57,369 --> 00:19:00,970
just makes it easier for us to reuse

00:18:59,350 --> 00:19:06,460
Prometheus components a little bit more

00:19:00,970 --> 00:19:10,210
directly ok so I was talking about how

00:19:06,460 --> 00:19:12,999
we kind of made the Prometheus query

00:19:10,210 --> 00:19:14,740
engine talk to talk to the in Flex DB

00:19:12,999 --> 00:19:17,200
storage engine so there are a couple key

00:19:14,740 --> 00:19:18,129
interfaces that Prometheus defines and

00:19:17,200 --> 00:19:20,440
they've done a really good job of kind

00:19:18,129 --> 00:19:22,690
of making these as as straightforward as

00:19:20,440 --> 00:19:24,009
possible and is easy to like extract to

00:19:22,690 --> 00:19:26,409
implement yourself so essentially

00:19:24,009 --> 00:19:29,470
there's a storage interface which is a

00:19:26,409 --> 00:19:32,409
queryable type but essentially all needs

00:19:29,470 --> 00:19:34,629
to be able to do is handle these three

00:19:32,409 --> 00:19:36,309
functions so start time a pender and

00:19:34,629 --> 00:19:38,950
close so a pender will return you an

00:19:36,309 --> 00:19:40,749
ependymal object so essentially if you

00:19:38,950 --> 00:19:42,549
have a storage interface you can ask for

00:19:40,749 --> 00:19:44,889
an append ur write data to it and tell

00:19:42,549 --> 00:19:46,330
it to store it so that's what Prometheus

00:19:44,889 --> 00:19:49,029
expects so as long as we fulfill that

00:19:46,330 --> 00:19:50,710
interface behind the scenes then we we

00:19:49,029 --> 00:19:53,470
can use any back-end we want so in this

00:19:50,710 --> 00:19:55,200
case we just have kind of like a storage

00:19:53,470 --> 00:19:56,640
Shin that we've written that takes

00:19:55,200 --> 00:19:58,220
that has this interface on the front end

00:19:56,640 --> 00:20:00,330
and then knows how to take those

00:19:58,220 --> 00:20:03,810
appenders and write the map to the

00:20:00,330 --> 00:20:05,310
influx DB storage engine and then on the

00:20:03,810 --> 00:20:08,250
query side of the per medias interfaces

00:20:05,310 --> 00:20:09,870
so a queryable is just a thing that

00:20:08,250 --> 00:20:12,300
route that you can you can call query

00:20:09,870 --> 00:20:13,680
iran and get a query ur object so

00:20:12,300 --> 00:20:17,220
essentially once again we've had we have

00:20:13,680 --> 00:20:18,660
our own shims for these as well and this

00:20:17,220 --> 00:20:20,820
will just return a thing that knows how

00:20:18,660 --> 00:20:23,130
to use the low-level kind of bindings to

00:20:20,820 --> 00:20:26,640
the in Flex DB storage engine but also

00:20:23,130 --> 00:20:28,500
handles the key prom key well types so

00:20:26,640 --> 00:20:31,020
there's a select query and that's used

00:20:28,500 --> 00:20:33,780
by both instant and range queries label

00:20:31,020 --> 00:20:35,640
values and label names and basically

00:20:33,780 --> 00:20:38,220
every everything that you can do with

00:20:35,640 --> 00:20:39,600
Prometheus from a query perspective can

00:20:38,220 --> 00:20:41,430
be done through that interface so we've

00:20:39,600 --> 00:20:43,620
essentially built our own version of

00:20:41,430 --> 00:20:45,090
these interfaces as well for me this

00:20:43,620 --> 00:20:46,980
just talk to those and then those know

00:20:45,090 --> 00:20:48,930
how to do all the work with the influx

00:20:46,980 --> 00:20:50,310
DB storage engine so that was really all

00:20:48,930 --> 00:20:52,440
that was required to get us to be able

00:20:50,310 --> 00:20:56,970
to to use the storage engine under the

00:20:52,440 --> 00:21:00,440
hood and so there been some other kind

00:20:56,970 --> 00:21:03,780
of interesting byproducts of getting

00:21:00,440 --> 00:21:05,570
more closely aligned with the Prometheus

00:21:03,780 --> 00:21:08,640
interfaces that I was just talking about

00:21:05,570 --> 00:21:12,990
so there's a package inside Prometheus

00:21:08,640 --> 00:21:15,360
called rules manager one of those is the

00:21:12,990 --> 00:21:17,400
thing that manages alerting rules which

00:21:15,360 --> 00:21:20,820
is cool because it lets us also be able

00:21:17,400 --> 00:21:22,200
to feed alerting rules into metrics

00:21:20,820 --> 00:21:25,530
store and out if you haven't worked with

00:21:22,200 --> 00:21:27,450
the the alert manager component of

00:21:25,530 --> 00:21:29,880
Prometheus it's essentially a separate

00:21:27,450 --> 00:21:31,470
process that's designed to be highly

00:21:29,880 --> 00:21:34,500
available so you can run two of them in

00:21:31,470 --> 00:21:36,480
parallel and as Prometheus or in this

00:21:34,500 --> 00:21:38,250
case metric store processes those

00:21:36,480 --> 00:21:40,080
alerting rules it can actually send them

00:21:38,250 --> 00:21:42,780
to the alert manager process it'll

00:21:40,080 --> 00:21:43,860
handle deduplication and rate limiting

00:21:42,780 --> 00:21:46,610
and things like that so it's essentially

00:21:43,860 --> 00:21:49,110
given us the ability to natively run

00:21:46,610 --> 00:21:50,700
alerting inside a metric store once

00:21:49,110 --> 00:21:54,360
again using all Prometheus components

00:21:50,700 --> 00:21:56,100
and that's something that I think we've

00:21:54,360 --> 00:21:57,870
just about finalized the work on so a

00:21:56,100 --> 00:22:00,960
basic version of that should be in the

00:21:57,870 --> 00:22:02,640
metric store or code base soon and then

00:22:00,960 --> 00:22:04,530
the other thing that we can do is is

00:22:02,640 --> 00:22:06,690
recording rules so I don't know if if

00:22:04,530 --> 00:22:07,980
you're familiar with in Flex DV or other

00:22:06,690 --> 00:22:08,710
systems that we had something called

00:22:07,980 --> 00:22:11,200
continuous query

00:22:08,710 --> 00:22:13,360
in Flex DB the idea here is basically

00:22:11,200 --> 00:22:15,730
you can have rules that tell you how you

00:22:13,360 --> 00:22:17,679
want to down sample your data so if we

00:22:15,730 --> 00:22:19,090
have really noisy data or you just have

00:22:17,679 --> 00:22:21,309
something that people query a lot on a

00:22:19,090 --> 00:22:22,720
long time range recording rules let you

00:22:21,309 --> 00:22:25,419
take data and sort of rewrite it in a

00:22:22,720 --> 00:22:26,590
lower resolution and because we're

00:22:25,419 --> 00:22:28,960
essentially using all these interfaces

00:22:26,590 --> 00:22:31,360
we were able to kind of adopt that

00:22:28,960 --> 00:22:32,559
ability as well that's not something

00:22:31,360 --> 00:22:35,679
we've done a lot of work on yet but

00:22:32,559 --> 00:22:37,870
theoretically it should just work we may

00:22:35,679 --> 00:22:39,159
need to do some sort of usability stuff

00:22:37,870 --> 00:22:40,899
to figure out how you feed these in

00:22:39,159 --> 00:22:42,880
because by default these are CMO files

00:22:40,899 --> 00:22:44,110
that you would feed Prometheus and it

00:22:42,880 --> 00:22:46,750
would you just restart it and it will

00:22:44,110 --> 00:22:48,610
reload them I think we can do some more

00:22:46,750 --> 00:22:49,840
interesting things with the rest of sort

00:22:48,610 --> 00:22:52,000
of the cloud boundary ecosystem to

00:22:49,840 --> 00:22:53,380
figure out how we want to provide and

00:22:52,000 --> 00:22:54,610
manage these and some other teams are

00:22:53,380 --> 00:22:56,320
charting to work with these as well so

00:22:54,610 --> 00:22:58,179
there may be some more evolution on this

00:22:56,320 --> 00:23:01,360
but essentially we've gotten those for

00:22:58,179 --> 00:23:03,070
free from Prometheus and then I kind of

00:23:01,360 --> 00:23:06,340
mentioned this so we've been doing some

00:23:03,070 --> 00:23:10,059
more work internally for a multi node

00:23:06,340 --> 00:23:12,909
metric store and those same interfaces

00:23:10,059 --> 00:23:14,950
interestingly can just be made sort of

00:23:12,909 --> 00:23:17,039
remote aware so if we have like a

00:23:14,950 --> 00:23:19,210
storage interface that we've implemented

00:23:17,039 --> 00:23:21,100
we can do a little bit of work and make

00:23:19,210 --> 00:23:22,630
it like a remote storage interface so

00:23:21,100 --> 00:23:24,880
essentially that can satisfy the same

00:23:22,630 --> 00:23:27,909
storage interface but under the hood

00:23:24,880 --> 00:23:30,610
maybe it's doing a remote read from

00:23:27,909 --> 00:23:32,830
another node or a remote write to make

00:23:30,610 --> 00:23:35,110
some of these multi node processes work

00:23:32,830 --> 00:23:37,330
and to you know to the Prometheus query

00:23:35,110 --> 00:23:38,860
engine side it doesn't really it doesn't

00:23:37,330 --> 00:23:40,570
care it's just it's just talking to an

00:23:38,860 --> 00:23:41,890
interface but under the hood we've been

00:23:40,570 --> 00:23:43,059
able to do some of this stuff so that we

00:23:41,890 --> 00:23:45,549
can actually make some of these multi

00:23:43,059 --> 00:23:46,630
node queries work so that's something

00:23:45,549 --> 00:23:47,890
we've been working on

00:23:46,630 --> 00:23:49,330
I think I'll talk about a little bit

00:23:47,890 --> 00:23:50,679
towards the end but we're trying to

00:23:49,330 --> 00:23:52,000
figure out if there's a way we can start

00:23:50,679 --> 00:23:53,890
to open-source some of this work to make

00:23:52,000 --> 00:23:55,090
it so that we can have a multi node

00:23:53,890 --> 00:23:59,289
metric store available in the open

00:23:55,090 --> 00:24:00,970
source so if you've used prompt QL

00:23:59,289 --> 00:24:02,230
before you probably are familiar with

00:24:00,970 --> 00:24:04,000
all these but essentially these are the

00:24:02,230 --> 00:24:07,630
the query endpoints that we've been able

00:24:04,000 --> 00:24:09,039
to make available to support prana QL so

00:24:07,630 --> 00:24:10,779
we've got the instant queries and range

00:24:09,039 --> 00:24:12,309
queries which are just queries at a

00:24:10,779 --> 00:24:14,409
given point in time in queries over a

00:24:12,309 --> 00:24:16,330
time range so the first one would be

00:24:14,409 --> 00:24:18,669
more like what you'd see on like a

00:24:16,330 --> 00:24:19,630
single gauge the range queries are more

00:24:18,669 --> 00:24:22,240
like what you would draw on a graph

00:24:19,630 --> 00:24:24,010
series names let's you look at kind of

00:24:22,240 --> 00:24:26,290
it's more like some metadata exploration

00:24:24,010 --> 00:24:27,370
to see what's available and you can give

00:24:26,290 --> 00:24:29,170
it some labels to kind of figure out

00:24:27,370 --> 00:24:32,050
what data is available in your metric

00:24:29,170 --> 00:24:35,140
store and then label names label values

00:24:32,050 --> 00:24:36,370
let you just kind of see what what

00:24:35,140 --> 00:24:37,540
labels you've written into the system

00:24:36,370 --> 00:24:39,520
and this is the place where I was kind

00:24:37,540 --> 00:24:43,360
of describing we had to make a slight

00:24:39,520 --> 00:24:46,090
design trade-off and that's the the the

00:24:43,360 --> 00:24:48,880
labels end point will only return to you

00:24:46,090 --> 00:24:51,730
the things that we've made tags so in

00:24:48,880 --> 00:24:54,309
that case it's everything that is not

00:24:51,730 --> 00:24:57,460
one of these six will come back in the

00:24:54,309 --> 00:24:59,980
label labels end point and then

00:24:57,460 --> 00:25:01,330
furthermore when you create a label

00:24:59,980 --> 00:25:03,429
values end point it'll tell you all the

00:25:01,330 --> 00:25:05,950
values that we know about for a given

00:25:03,429 --> 00:25:07,300
label name and once again if you tried

00:25:05,950 --> 00:25:09,610
to query one of those other ones like

00:25:07,300 --> 00:25:11,260
URI or content like we would just tell

00:25:09,610 --> 00:25:12,990
you it doesn't exist because the

00:25:11,260 --> 00:25:15,190
likelihood that you would get back an

00:25:12,990 --> 00:25:19,120
unprocessed bull amount of data is very

00:25:15,190 --> 00:25:20,110
high for something like URI or things

00:25:19,120 --> 00:25:21,670
like that it's just not gonna be

00:25:20,110 --> 00:25:23,380
something that like Griffin or any

00:25:21,670 --> 00:25:25,360
visualization tool would know what to do

00:25:23,380 --> 00:25:28,270
with so that's the reason for those

00:25:25,360 --> 00:25:29,830
asterisks there's a lot more

00:25:28,270 --> 00:25:31,480
documentation about the prom qll API and

00:25:29,830 --> 00:25:34,870
how it works so if you're interested

00:25:31,480 --> 00:25:39,370
just google that you will get a ton of

00:25:34,870 --> 00:25:42,220
stuff to read sorry keep doing that

00:25:39,370 --> 00:25:44,380
so future work we've gotten a two point

00:25:42,220 --> 00:25:45,820
where a metric store is pretty solid

00:25:44,380 --> 00:25:47,290
right now and I think we've got some

00:25:45,820 --> 00:25:48,730
internal teams that are using it it's

00:25:47,290 --> 00:25:51,400
open source so you can you can give it a

00:25:48,730 --> 00:25:53,080
try now there's some query performance

00:25:51,400 --> 00:25:55,929
work that we've identified just kind of

00:25:53,080 --> 00:25:57,100
ways that we've connected things poorly

00:25:55,929 --> 00:25:59,020
and maybe we're not doing them in the

00:25:57,100 --> 00:26:00,940
most efficient way so special especially

00:25:59,020 --> 00:26:05,320
on queries they either pull back a lot

00:26:00,940 --> 00:26:06,730
of data or things that that look over

00:26:05,320 --> 00:26:08,290
long periods of time I guess in

00:26:06,730 --> 00:26:09,850
particular that pull back a lot of f2

00:26:08,290 --> 00:26:12,340
hit a lot of those day long shards that

00:26:09,850 --> 00:26:13,960
I was describing those can be a little

00:26:12,340 --> 00:26:17,559
bit slow so we're working on optimizing

00:26:13,960 --> 00:26:20,050
those and then I talked a little bit

00:26:17,559 --> 00:26:22,240
about how logger Gators changing so

00:26:20,050 --> 00:26:24,160
logger Gator is kind of trying to get

00:26:22,240 --> 00:26:27,970
away from the the firehose concept a

00:26:24,160 --> 00:26:31,300
little bit and moving a little bit more

00:26:27,970 --> 00:26:32,770
towards a world where all of the all the

00:26:31,300 --> 00:26:35,050
services are going to expose some sort

00:26:32,770 --> 00:26:35,740
of a metrics endpoint once again more in

00:26:35,050 --> 00:26:39,460
line with kind of

00:26:35,740 --> 00:26:40,149
being Prometheus compatible and in that

00:26:39,460 --> 00:26:42,190
world

00:26:40,149 --> 00:26:44,440
our current nozzle design is going to

00:26:42,190 --> 00:26:47,080
make less sense so we're going to need

00:26:44,440 --> 00:26:48,610
to build the nozzle that either can can

00:26:47,080 --> 00:26:50,409
take a list of metrics endpoints and do

00:26:48,610 --> 00:26:52,330
the scraping itself or like push some of

00:26:50,409 --> 00:26:54,070
that nozzle work elsewhere but ideally

00:26:52,330 --> 00:26:56,200
that nozzle process will start to evolve

00:26:54,070 --> 00:26:58,809
over the next probably six months or so

00:26:56,200 --> 00:27:01,029
as the log reader design continues to

00:26:58,809 --> 00:27:02,260
evolve so that'll be something that

00:27:01,029 --> 00:27:03,580
we're going to be working on it may be

00:27:02,260 --> 00:27:05,470
something where we make it kind of like

00:27:03,580 --> 00:27:06,700
a modular maybe we have like a couple

00:27:05,470 --> 00:27:07,840
different nozzles and you just deploy

00:27:06,700 --> 00:27:10,149
the one that makes sense for your use

00:27:07,840 --> 00:27:12,190
case but in general that that work is

00:27:10,149 --> 00:27:14,110
going to kind of be going on and then as

00:27:12,190 --> 00:27:17,110
I mentioned we've been doing some some

00:27:14,110 --> 00:27:19,929
work on a multi node metric store so the

00:27:17,110 --> 00:27:21,850
goals there would be being able to have

00:27:19,929 --> 00:27:23,590
high availability so as you're you're

00:27:21,850 --> 00:27:25,779
you know restarting your deployment

00:27:23,590 --> 00:27:28,330
you'll always have at least one node up

00:27:25,779 --> 00:27:30,549
or hopefully one node with with any copy

00:27:28,330 --> 00:27:33,399
of the data up so ideally queries will

00:27:30,549 --> 00:27:36,010
always be be able to be fulfilled even

00:27:33,399 --> 00:27:37,870
if you're restarting some bills and

00:27:36,010 --> 00:27:39,669
replications so as we're taking data in

00:27:37,870 --> 00:27:41,110
from the nozzle being able to you know

00:27:39,669 --> 00:27:42,580
write two copies of it within the

00:27:41,110 --> 00:27:43,990
cluster so at any at any point if you

00:27:42,580 --> 00:27:46,059
completely lose a node or node is

00:27:43,990 --> 00:27:48,700
offline you'll still be able to have a

00:27:46,059 --> 00:27:50,890
copy of that data online and then

00:27:48,700 --> 00:27:52,559
dynamic scaling something that would be

00:27:50,890 --> 00:27:54,700
I think relatively tricky to implement

00:27:52,559 --> 00:27:56,470
with the the current design but

00:27:54,700 --> 00:27:58,210
something we've been talking about is if

00:27:56,470 --> 00:27:59,980
you deploy a multi node metric store

00:27:58,210 --> 00:28:01,570
what is what is the scaling strategy

00:27:59,980 --> 00:28:03,220
like other than just scaling up like is

00:28:01,570 --> 00:28:04,419
there a way that we could make it so

00:28:03,220 --> 00:28:06,520
that it was possible to add or remove

00:28:04,419 --> 00:28:07,809
nodes relatively easily not something we

00:28:06,520 --> 00:28:08,950
have an answer for yet but it something

00:28:07,809 --> 00:28:13,120
we've been talking about just trying to

00:28:08,950 --> 00:28:14,350
figure out you know how can we make that

00:28:13,120 --> 00:28:16,179
can we make a multi node metric store

00:28:14,350 --> 00:28:18,970
that's that's as usable as possible and

00:28:16,179 --> 00:28:21,970
you know possibly at some point it may

00:28:18,970 --> 00:28:23,950
be that log cache itself stopped

00:28:21,970 --> 00:28:25,809
servicing the prompt Q endpoints that it

00:28:23,950 --> 00:28:27,850
has it stops being there for metrics at

00:28:25,809 --> 00:28:29,470
all and metrics or it just becomes the

00:28:27,850 --> 00:28:31,720
single place that you go to to get any

00:28:29,470 --> 00:28:33,039
of your metrics and then maybe the you

00:28:31,720 --> 00:28:35,590
know the logger gate or subsystem

00:28:33,039 --> 00:28:37,059
focuses purely on on logs and some sort

00:28:35,590 --> 00:28:39,370
of like syslog support for that stuff

00:28:37,059 --> 00:28:42,399
and then you know metrics or is just a

00:28:39,370 --> 00:28:47,010
metric store yeah so the lots of

00:28:42,399 --> 00:28:47,010
interesting things coming up questions

00:29:33,130 --> 00:29:38,930
sure so the question was why like you

00:29:36,260 --> 00:29:40,370
know why why metric store why does it

00:29:38,930 --> 00:29:42,260
make sense not necessarily as a

00:29:40,370 --> 00:29:43,520
progression of the things that we kind

00:29:42,260 --> 00:29:45,830
of had before what what are we really

00:29:43,520 --> 00:29:48,200
adding so one of the things like I said

00:29:45,830 --> 00:29:49,730
when we were we had logged cash I think

00:29:48,200 --> 00:29:51,680
the biggest question was like how can we

00:29:49,730 --> 00:29:53,300
make it possible to store more data so

00:29:51,680 --> 00:29:54,890
people can actually run queries because

00:29:53,300 --> 00:29:56,180
a lot of these things that you're

00:29:54,890 --> 00:29:58,550
looking at whether it's for

00:29:56,180 --> 00:30:00,350
observability or just being able to

00:29:58,550 --> 00:30:01,700
provide user value for the metrics that

00:30:00,350 --> 00:30:03,230
are coming through the system you need

00:30:01,700 --> 00:30:05,450
to be able to look over a certain time

00:30:03,230 --> 00:30:06,830
range to be able to do some of these

00:30:05,450 --> 00:30:08,420
things if you're doing comparisons a

00:30:06,830 --> 00:30:10,550
week over week you need to actually be

00:30:08,420 --> 00:30:12,140
able to persist that data so that was

00:30:10,550 --> 00:30:14,480
kind of the reason from from log cash

00:30:12,140 --> 00:30:17,090
which was purely in memory and not

00:30:14,480 --> 00:30:18,800
persisted at all you wanted to be able

00:30:17,090 --> 00:30:20,150
to have something that actually can can

00:30:18,800 --> 00:30:22,580
store data and so there are other

00:30:20,150 --> 00:30:23,900
time-series databases out there but none

00:30:22,580 --> 00:30:26,030
of them really work natively with cloud

00:30:23,900 --> 00:30:28,070
battery none of them you know you'd have

00:30:26,030 --> 00:30:29,750
to go write your own nozzle you'd have

00:30:28,070 --> 00:30:31,160
to write your own off proxy so there are

00:30:29,750 --> 00:30:32,930
a lot of these components that we had in

00:30:31,160 --> 00:30:34,790
in log cache that made it really

00:30:32,930 --> 00:30:37,970
convenient for these for this type of

00:30:34,790 --> 00:30:39,430
thing to exist and be usable you know

00:30:37,970 --> 00:30:43,070
both from an ingress and egress

00:30:39,430 --> 00:30:45,740
perspective but we didn't have that that

00:30:43,070 --> 00:30:48,710
retention story solved until we we did

00:30:45,740 --> 00:30:50,510
this and then I think another thing that

00:30:48,710 --> 00:30:51,920
is interesting is alerting is a thing

00:30:50,510 --> 00:30:54,020
that people ask about all the time there

00:30:51,920 --> 00:30:56,650
are lots of external services that you

00:30:54,020 --> 00:30:59,360
can use for alerting but being able to

00:30:56,650 --> 00:31:01,250
kind of have this data available in a

00:30:59,360 --> 00:31:03,530
way that's that's durable and reliable

00:31:01,250 --> 00:31:04,520
now gives you the ability to be able to

00:31:03,530 --> 00:31:06,350
start using some of that to drive

00:31:04,520 --> 00:31:07,790
alerting decisions so as we start to

00:31:06,350 --> 00:31:09,090
pull some of the alert manager work in

00:31:07,790 --> 00:31:10,800
and

00:31:09,090 --> 00:31:12,450
to build that integration it makes it

00:31:10,800 --> 00:31:14,670
possible for us to actually like have a

00:31:12,450 --> 00:31:17,010
single place that can be the place that

00:31:14,670 --> 00:31:18,840
people go to we can always say metric

00:31:17,010 --> 00:31:20,520
stores where the metrics live if you

00:31:18,840 --> 00:31:21,810
want to do querying if you want to do

00:31:20,520 --> 00:31:24,120
alerting if you want to do any kind of

00:31:21,810 --> 00:31:26,220
like roll-up or aggregations all the

00:31:24,120 --> 00:31:27,780
data is there and it's I think it's a

00:31:26,220 --> 00:31:29,970
story that we just we didn't really have

00:31:27,780 --> 00:31:31,620
a whole picture for law cash was useful

00:31:29,970 --> 00:31:33,390
in some ways but also was kind of

00:31:31,620 --> 00:31:34,500
terrible because you know you had no

00:31:33,390 --> 00:31:35,670
reliability that your data would be

00:31:34,500 --> 00:31:37,800
there if you do a deployment it's all

00:31:35,670 --> 00:31:40,020
gone so I think us being able to put

00:31:37,800 --> 00:31:41,220
together a picture where persistence is

00:31:40,020 --> 00:31:43,050
something that you can just expect

00:31:41,220 --> 00:31:44,790
natively you don't have to have a large

00:31:43,050 --> 00:31:47,310
VM running this initially it can just be

00:31:44,790 --> 00:31:48,480
a single node you know that's got a

00:31:47,310 --> 00:31:51,030
relatively small footprint and only

00:31:48,480 --> 00:31:52,320
holds like maybe a week of data at least

00:31:51,030 --> 00:31:53,820
you have something now that you can

00:31:52,320 --> 00:31:55,470
start to build on top of and if we get

00:31:53,820 --> 00:31:57,420
to a world where there's an open-source

00:31:55,470 --> 00:31:59,610
metric open-source metrics where that's

00:31:57,420 --> 00:32:01,260
a multi node then you start to have like

00:31:59,610 --> 00:32:02,610
that full H a picture and there's

00:32:01,260 --> 00:32:04,110
there's a lot more that you can do with

00:32:02,610 --> 00:32:06,530
it that you just couldn't do easily

00:32:04,110 --> 00:32:06,530
before

00:32:40,480 --> 00:32:46,039
sure yeah so the question is if you if

00:32:44,750 --> 00:32:47,509
you already have sort of a Prometheus

00:32:46,039 --> 00:32:49,070
based solution like where does where

00:32:47,509 --> 00:32:50,720
does that fit with metric store do you

00:32:49,070 --> 00:32:52,730
just not use metric store so I think

00:32:50,720 --> 00:32:54,590
Prometheus has a lot of lot of

00:32:52,730 --> 00:32:55,759
trade-offs like you know it's designed

00:32:54,590 --> 00:32:57,889
to scrape things so if you're if you're

00:32:55,759 --> 00:32:59,029
using it with an open source cloud times

00:32:57,889 --> 00:33:00,440
you already you've had to you have to

00:32:59,029 --> 00:33:02,480
have something that's pulling all this

00:33:00,440 --> 00:33:04,250
firehose data in and making it scrape

00:33:02,480 --> 00:33:06,110
Abul for Prometheus so that's kind of

00:33:04,250 --> 00:33:07,580
trade-off number one is you've had to

00:33:06,110 --> 00:33:08,480
figure out how to implement that for

00:33:07,580 --> 00:33:10,970
some people

00:33:08,480 --> 00:33:12,740
Prometheus can't physically can't handle

00:33:10,970 --> 00:33:14,539
all the data because they don't have the

00:33:12,740 --> 00:33:16,220
design they don't have the ability to do

00:33:14,539 --> 00:33:18,289
the design trade-off that we did where

00:33:16,220 --> 00:33:19,970
we said some of these noisy things are

00:33:18,289 --> 00:33:22,580
just going to be filled we're not going

00:33:19,970 --> 00:33:23,960
to index them so in sums some places

00:33:22,580 --> 00:33:25,490
because you can only have like a single

00:33:23,960 --> 00:33:27,350
Prometheus there's no sharding with

00:33:25,490 --> 00:33:29,120
Prometheus really I mean you can you can

00:33:27,350 --> 00:33:30,710
find a way to shard it but if you just

00:33:29,120 --> 00:33:32,480
have one Prometheus there's a certain

00:33:30,710 --> 00:33:34,539
point where it can't it can't handle all

00:33:32,480 --> 00:33:36,710
that data just from an index perspective

00:33:34,539 --> 00:33:38,419
so you'll you'll see it either run out

00:33:36,710 --> 00:33:41,539
of memory and that's that's kind of like

00:33:38,419 --> 00:33:43,190
a fundamental design choice of

00:33:41,539 --> 00:33:44,659
Prometheus but I think there are some

00:33:43,190 --> 00:33:46,549
solutions some some people trying to

00:33:44,659 --> 00:33:48,350
solve this that haven't had success with

00:33:46,549 --> 00:33:51,820
Prometheus yet and I think we're kind of

00:33:48,350 --> 00:33:54,080
trying to step in with a slightly more

00:33:51,820 --> 00:33:55,940
it's opinionated in a different way I

00:33:54,080 --> 00:33:57,110
think we're trying to say this is kind

00:33:55,940 --> 00:33:59,149
of the way that Cloud Foundry metrics

00:33:57,110 --> 00:34:01,159
look and work here are some things that

00:33:59,149 --> 00:34:03,500
we think make it easy to to handle all

00:34:01,159 --> 00:34:05,870
of them it's totally possible that we

00:34:03,500 --> 00:34:09,040
could also just if you have a Prometheus

00:34:05,870 --> 00:34:12,530
in maybe maybe there's a way that we can

00:34:09,040 --> 00:34:14,899
like let your Prometheus scrape some

00:34:12,530 --> 00:34:15,889
subset of the metrics store data like I

00:34:14,899 --> 00:34:17,629
think there's a world where they can

00:34:15,889 --> 00:34:19,550
they can live together but I think

00:34:17,629 --> 00:34:21,290
there's a lot of overlap but it's not

00:34:19,550 --> 00:34:22,760
they're not the same so I think

00:34:21,290 --> 00:34:23,540
depending on your use case I'm happy to

00:34:22,760 --> 00:34:24,710
talk to you more about this because

00:34:23,540 --> 00:34:26,629
we've seen a lot of different use cases

00:34:24,710 --> 00:34:29,599
I think Prometheus is actually a great

00:34:26,629 --> 00:34:30,980
tool for 80% of the stuff out there like

00:34:29,599 --> 00:34:31,670
I think if it works for you and you can

00:34:30,980 --> 00:34:32,840
deploy it

00:34:31,670 --> 00:34:34,400
great but you still figure out how to

00:34:32,840 --> 00:34:35,690
get data into it and yourself to figure

00:34:34,400 --> 00:34:36,950
out how you're gonna secure it if you're

00:34:35,690 --> 00:34:38,600
going to expose it to end-users there's

00:34:36,950 --> 00:34:40,160
not a lot like for me this doesn't have

00:34:38,600 --> 00:34:41,270
a security story and that's kind of

00:34:40,160 --> 00:34:43,520
where we wanted to bring something that

00:34:41,270 --> 00:34:45,530
had that full picture from how you get

00:34:43,520 --> 00:34:47,900
the data in how you get the data out and

00:34:45,530 --> 00:34:50,110
make sure that it's relatively secure so

00:34:47,900 --> 00:34:51,920
that we could have that as the default

00:34:50,110 --> 00:34:54,080
once again there are some times where

00:34:51,920 --> 00:34:56,150
Prometheus is totally fine and I don't

00:34:54,080 --> 00:34:58,040
think you should replace it maybe in

00:34:56,150 --> 00:34:59,030
that case you don't use metric store but

00:34:58,040 --> 00:35:01,010
I think there are some cases where

00:34:59,030 --> 00:35:02,720
having these extra components and having

00:35:01,010 --> 00:35:05,000
this ability to be a little bit more

00:35:02,720 --> 00:35:07,070
granular with how data is stored gives

00:35:05,000 --> 00:35:12,970
us a bit of an advantage with metric

00:35:07,070 --> 00:35:12,970
store cool thanks everybody

00:35:13,430 --> 00:35:16,550

YouTube URL: https://www.youtube.com/watch?v=7XuhOca69yU


