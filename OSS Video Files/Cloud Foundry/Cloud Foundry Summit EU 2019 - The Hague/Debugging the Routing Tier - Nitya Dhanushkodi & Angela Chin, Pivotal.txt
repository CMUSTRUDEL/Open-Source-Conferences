Title: Debugging the Routing Tier - Nitya Dhanushkodi & Angela Chin, Pivotal
Publication date: 2019-09-13
Playlist: Cloud Foundry Summit EU 2019 - The Hague
Description: 
	Debugging the Routing Tier - Nitya Dhanushkodi & Angela Chin, Pivotal

As the entry point to Cloud Foundry, routing-tier related errors come from a variety of places. While some of these errors can indicate actual bugs in Cloud Foundry source code, there can be a number of root causes-- misbehaving applications, misconfigured load balancers, and infrastructure issues, just to name a few. To isolate and debug these problems, it becomes important to know which data to collect, how to collect it, and what in the data can indicate the root cause.

In this talk, Angela and Nitya will cover what information is useful to collect and what to look for in the data to eliminate possible causes of problems. Drawing on their experiences as members of the CF Networking team, they will go through issues that operators have seen in the past, and how they use tools such as routing logs, pprof, flamegraphs, and wireshark to debug, isolate, and work around these issues.

For more info: https://www.cloudfoundry.org/
Captions: 
	00:00:00,030 --> 00:00:05,009
thank you all for coming to this session

00:00:03,149 --> 00:00:07,319
my name is Angela I'm a software

00:00:05,009 --> 00:00:10,050
engineer at pivotal I'm Nydia I'm a

00:00:07,319 --> 00:00:10,889
software engineer at awesome and today

00:00:10,050 --> 00:00:13,559
we're going to be talking about

00:00:10,889 --> 00:00:17,760
debugging the routing tier so the goal

00:00:13,559 --> 00:00:20,510
of the next 30 minutes or so together is

00:00:17,760 --> 00:00:24,060
that by the end of this presentation

00:00:20,510 --> 00:00:25,619
everyone in the audience has a shared

00:00:24,060 --> 00:00:32,520
understanding of what the routing tier

00:00:25,619 --> 00:00:35,219
is and has tools and tips in their bag

00:00:32,520 --> 00:00:38,399
now for debugging the routing cheer and

00:00:35,219 --> 00:00:42,320
maybe even just debugging problems in

00:00:38,399 --> 00:00:46,289
general in order to achieve this goal

00:00:42,320 --> 00:00:50,370
our plan is to start by level setting on

00:00:46,289 --> 00:00:52,379
what exactly is the routing tier before

00:00:50,370 --> 00:00:57,570
looking at to actual case studies that

00:00:52,379 --> 00:01:00,149
me and the TIA have debugged in relation

00:00:57,570 --> 00:01:02,430
to problems that manifested in the

00:01:00,149 --> 00:01:04,920
routing tier as members of the CF

00:01:02,430 --> 00:01:06,659
networking program and then we're going

00:01:04,920 --> 00:01:08,310
to cap it off with some additional

00:01:06,659 --> 00:01:11,460
general debugging tips that will

00:01:08,310 --> 00:01:12,900
hopefully allow you to see errors and

00:01:11,460 --> 00:01:15,360
get to the root cause

00:01:12,900 --> 00:01:18,390
so without further ado I'm going to hand

00:01:15,360 --> 00:01:22,970
it over to Nisha to talk about what the

00:01:18,390 --> 00:01:22,970
routing tier is if this clicker ever

00:01:24,350 --> 00:01:30,360
works - yeah cool

00:01:27,360 --> 00:01:31,799
so what is the routing tier and how does

00:01:30,360 --> 00:01:34,049
it fit into the rest of your Cloud

00:01:31,799 --> 00:01:35,850
Foundry we want to give you a brief

00:01:34,049 --> 00:01:37,920
overview of what it is so that we can

00:01:35,850 --> 00:01:40,710
look at the case studies and debugging

00:01:37,920 --> 00:01:43,829
tips later so if we look at this diagram

00:01:40,710 --> 00:01:46,920
here the gray box represents all the

00:01:43,829 --> 00:01:49,490
system components of CF like your API

00:01:46,920 --> 00:01:51,960
Diego UAA and your routers and the

00:01:49,490 --> 00:01:53,490
components in the gray box are all the

00:01:51,960 --> 00:01:55,530
stuff that makes it possible for you to

00:01:53,490 --> 00:01:58,500
see f.push and run your applications and

00:01:55,530 --> 00:02:01,649
all the colorful components are the

00:01:58,500 --> 00:02:04,439
components of the routing tier so in a

00:02:01,649 --> 00:02:06,659
typical CF deployment your routing tier

00:02:04,439 --> 00:02:08,910
may consist of a load balancer H a

00:02:06,659 --> 00:02:13,110
proxies the routers and your

00:02:08,910 --> 00:02:13,890
applications so if we look at just the

00:02:13,110 --> 00:02:16,230
left side of the

00:02:13,890 --> 00:02:19,260
diagram we can see all the components

00:02:16,230 --> 00:02:21,780
that an HTTP request might hit before

00:02:19,260 --> 00:02:25,290
being finally routed to an HTTP

00:02:21,780 --> 00:02:27,540
application so the load balancer is the

00:02:25,290 --> 00:02:29,130
front facing part of your CF and it'll

00:02:27,540 --> 00:02:31,020
be the component that receives the

00:02:29,130 --> 00:02:34,410
request and will forward it on to the AJ

00:02:31,020 --> 00:02:36,390
proxy the H a proxy would be configured

00:02:34,410 --> 00:02:38,580
to point to the go routers as its back

00:02:36,390 --> 00:02:41,000
ends you tend to use H a proxy in a

00:02:38,580 --> 00:02:44,190
deployment when you need features

00:02:41,000 --> 00:02:46,050
offered by H a proxy that aren't offered

00:02:44,190 --> 00:02:49,020
by your load balancers or your CF

00:02:46,050 --> 00:02:52,830
routers so for example H a proxy allows

00:02:49,020 --> 00:02:53,940
you to define access control lists so

00:02:52,830 --> 00:02:57,180
then you have gerado

00:02:53,940 --> 00:03:00,330
go router is a layer seven HTTP proxy

00:02:57,180 --> 00:03:03,090
router that handles connections to the

00:03:00,330 --> 00:03:05,610
application backends so it has a series

00:03:03,090 --> 00:03:08,010
of handlers that will run and process

00:03:05,610 --> 00:03:11,489
your request before it then makes a new

00:03:08,010 --> 00:03:13,500
request to the backend it can also route

00:03:11,489 --> 00:03:17,850
you to other CF components like your API

00:03:13,500 --> 00:03:22,410
and then ultimate you li you reach your

00:03:17,850 --> 00:03:25,860
App cool so now that you have an

00:03:22,410 --> 00:03:28,530
overview we'll talk about problems that

00:03:25,860 --> 00:03:30,299
people have run into when running Cloud

00:03:28,530 --> 00:03:32,160
Foundry in production so I'll give it

00:03:30,299 --> 00:03:38,310
over to Angela and we'll kick off the

00:03:32,160 --> 00:03:41,519
case studies ok the clicker is working

00:03:38,310 --> 00:03:43,760
again magical so again we're going to

00:03:41,519 --> 00:03:48,630
dive into the first two to case studies

00:03:43,760 --> 00:03:50,370
these are actual problems that we have

00:03:48,630 --> 00:03:52,890
debugged as members of the networking

00:03:50,370 --> 00:03:55,320
program and so hopefully we can share

00:03:52,890 --> 00:03:59,100
with you our methodology and how we

00:03:55,320 --> 00:04:01,019
approach debugging and offer some

00:03:59,100 --> 00:04:03,329
insight as these are real problems

00:04:01,019 --> 00:04:05,040
they're definitely super interesting and

00:04:03,329 --> 00:04:06,780
we didn't do everything perfectly but

00:04:05,040 --> 00:04:09,060
hopefully following on the path that we

00:04:06,780 --> 00:04:13,320
did offers insight into how you might

00:04:09,060 --> 00:04:15,660
debug in the future so oftentimes when

00:04:13,320 --> 00:04:17,970
problems are manifesting in the routing

00:04:15,660 --> 00:04:20,940
tier that means that there's a problem

00:04:17,970 --> 00:04:23,250
with your data path in production this

00:04:20,940 --> 00:04:25,560
means that your production apps might no

00:04:23,250 --> 00:04:27,510
longer be reachable so it's pretty easy

00:04:25,560 --> 00:04:31,230
as

00:04:27,510 --> 00:04:33,690
a person as a customer a user of Cloud

00:04:31,230 --> 00:04:36,000
Foundry when you run into this to say

00:04:33,690 --> 00:04:39,330
that the problem is simply that the go

00:04:36,000 --> 00:04:41,360
router is broken your production system

00:04:39,330 --> 00:04:45,300
is failing you should be panicking

00:04:41,360 --> 00:04:47,130
but to be on the receiving end of this

00:04:45,300 --> 00:04:49,230
information it's not exactly the most

00:04:47,130 --> 00:04:52,200
helpful and debugging what the actual

00:04:49,230 --> 00:04:54,630
problem is if you're the one who's

00:04:52,200 --> 00:04:57,240
actively debugging the issue and run

00:04:54,630 --> 00:04:59,100
into it yourself or if somebody else is

00:04:57,240 --> 00:05:01,530
providing you this information even if

00:04:59,100 --> 00:05:03,690
it's presented to you instead of the go

00:05:01,530 --> 00:05:06,510
router is broken but in a very detailed

00:05:03,690 --> 00:05:09,300
form and a very clear explanation the

00:05:06,510 --> 00:05:11,190
first thing we always like to state to

00:05:09,300 --> 00:05:14,130
do is to state the problem in your own

00:05:11,190 --> 00:05:15,660
words this allows you to make sure that

00:05:14,130 --> 00:05:17,880
you have the same shared common

00:05:15,660 --> 00:05:20,280
understanding as whoever is reporting

00:05:17,880 --> 00:05:21,930
the issue to you or if you're the one

00:05:20,280 --> 00:05:24,120
who's discovered the issue stating the

00:05:21,930 --> 00:05:25,920
problem in your own words gives you this

00:05:24,120 --> 00:05:28,530
baseline understanding to then share

00:05:25,920 --> 00:05:30,780
with others and provide sort of the

00:05:28,530 --> 00:05:35,160
basis of what you're trying to find the

00:05:30,780 --> 00:05:38,370
root cause of so for case study number

00:05:35,160 --> 00:05:40,680
one the problem that we faced was that

00:05:38,370 --> 00:05:43,050
the customer had to micro services

00:05:40,680 --> 00:05:45,210
deployed one was an externally facing

00:05:43,050 --> 00:05:47,490
front-end application that would send

00:05:45,210 --> 00:05:50,520
crud requests to the second back-end

00:05:47,490 --> 00:05:53,130
application the micro service on the

00:05:50,520 --> 00:05:55,650
front end would send a gift followed

00:05:53,130 --> 00:05:57,810
immediately by a put request the get

00:05:55,650 --> 00:06:00,930
request would always succeed but the put

00:05:57,810 --> 00:06:03,510
would always error I realized that this

00:06:00,930 --> 00:06:07,170
is a wall a text so let's visualize what

00:06:03,510 --> 00:06:09,690
this problem statement is saying in this

00:06:07,170 --> 00:06:12,180
case we see that there's an incoming get

00:06:09,690 --> 00:06:13,560
request from an external client the go

00:06:12,180 --> 00:06:16,050
router forwards this request to the

00:06:13,560 --> 00:06:18,960
front end which will then make a get

00:06:16,050 --> 00:06:20,820
request to the back end the get request

00:06:18,960 --> 00:06:23,460
will actually in this case hairpin back

00:06:20,820 --> 00:06:25,470
out through the go router and go to the

00:06:23,460 --> 00:06:27,830
back end and this succeeds so the

00:06:25,470 --> 00:06:30,690
information gets passed back along and

00:06:27,830 --> 00:06:33,810
then the front end makes the media

00:06:30,690 --> 00:06:36,630
follow-up of a put request and that what

00:06:33,810 --> 00:06:39,540
happens we know an error is occurring

00:06:36,630 --> 00:06:40,460
but what we don't know is is the problem

00:06:39,540 --> 00:06:43,160
happening

00:06:40,460 --> 00:06:45,170
at the go router or is the put request

00:06:43,160 --> 00:06:49,520
successfully going through the go router

00:06:45,170 --> 00:06:51,800
to the back end and then airing gal who

00:06:49,520 --> 00:06:54,770
really knows that's why we're here to

00:06:51,800 --> 00:06:57,860
debug the problem this is further

00:06:54,770 --> 00:06:59,480
complicated by the fact that well we've

00:06:57,860 --> 00:07:01,820
been looking at the go router front end

00:06:59,480 --> 00:07:04,880
and back end as the only components in

00:07:01,820 --> 00:07:08,360
this system they're only a part of our

00:07:04,880 --> 00:07:09,980
Cloud Foundry topology we also have to

00:07:08,360 --> 00:07:12,410
contend with other parts of the routing

00:07:09,980 --> 00:07:15,500
tier as Nydia described earlier such as

00:07:12,410 --> 00:07:17,300
the a qi proxy and load balancer so

00:07:15,500 --> 00:07:20,630
instead of there being only four network

00:07:17,300 --> 00:07:23,660
connections we have to consider up to

00:07:20,630 --> 00:07:25,730
eight if not more and that's a lot more

00:07:23,660 --> 00:07:29,090
places where the error could actually be

00:07:25,730 --> 00:07:32,030
occurring so the second thing that we

00:07:29,090 --> 00:07:34,700
want to do in terms of debugging it's

00:07:32,030 --> 00:07:36,920
the level set on the architecture how

00:07:34,700 --> 00:07:39,050
exactly is the routing tier set up in

00:07:36,920 --> 00:07:41,600
this case it was set up pretty

00:07:39,050 --> 00:07:44,810
straightforward a very typical CF

00:07:41,600 --> 00:07:47,300
routing topology with a load balancer H

00:07:44,810 --> 00:07:50,390
a proxy go router and then applications

00:07:47,300 --> 00:07:53,720
and so you might say great we know the

00:07:50,390 --> 00:07:56,090
problem we know the components it's easy

00:07:53,720 --> 00:07:57,740
to want to jump in jump the gun and try

00:07:56,090 --> 00:08:01,040
and figure out the connection that's

00:07:57,740 --> 00:08:03,920
failing but let's stay in the

00:08:01,040 --> 00:08:06,020
information collection face and in

00:08:03,920 --> 00:08:08,900
addition to gathering and level setting

00:08:06,020 --> 00:08:11,360
on the architecture also gather

00:08:08,900 --> 00:08:14,030
information about the configuration for

00:08:11,360 --> 00:08:15,860
each of these components as well by

00:08:14,030 --> 00:08:19,400
gathering all of this information up

00:08:15,860 --> 00:08:21,410
front it allows us access to more

00:08:19,400 --> 00:08:23,840
knowledge and information so that in the

00:08:21,410 --> 00:08:25,610
future if we think a problem might be

00:08:23,840 --> 00:08:27,920
happening for one component or another

00:08:25,610 --> 00:08:31,160
we don't have to go back and collect

00:08:27,920 --> 00:08:32,900
information again so the configuration

00:08:31,160 --> 00:08:35,480
that you'd want to gather would be

00:08:32,900 --> 00:08:39,080
information about the load balancers H a

00:08:35,480 --> 00:08:40,970
proxy go router and usually involves

00:08:39,080 --> 00:08:43,730
including it from gathering information

00:08:40,970 --> 00:08:49,490
about a whole slew of different time

00:08:43,730 --> 00:08:51,500
outs after you gather configuration the

00:08:49,490 --> 00:08:53,840
third thing we want to do and this might

00:08:51,500 --> 00:08:57,130
not be the most helpful thing to say

00:08:53,840 --> 00:08:59,150
this gathering even more information I

00:08:57,130 --> 00:09:00,830
realize that this is pretty broad right

00:08:59,150 --> 00:09:02,840
like before we were being a little bit

00:09:00,830 --> 00:09:04,850
more narrow by saying we want to learn

00:09:02,840 --> 00:09:06,529
the architecture we want to gather

00:09:04,850 --> 00:09:09,140
configuration but what is this quote

00:09:06,529 --> 00:09:11,660
unquote more information right it really

00:09:09,140 --> 00:09:14,300
depends on what the problem statement is

00:09:11,660 --> 00:09:17,050
and what issue you're trying to debug

00:09:14,300 --> 00:09:19,279
but this additional information

00:09:17,050 --> 00:09:22,160
generally includes things like

00:09:19,279 --> 00:09:24,290
application logs which in this case was

00:09:22,160 --> 00:09:24,980
what alerted us to this problem in the

00:09:24,290 --> 00:09:27,760
first place

00:09:24,980 --> 00:09:31,310
seeing an i/o error on the put request

00:09:27,760 --> 00:09:32,690
but can include other logs as well so

00:09:31,310 --> 00:09:35,180
you can gather logs from the load

00:09:32,690 --> 00:09:37,010
balancer if possible the H a proxy and

00:09:35,180 --> 00:09:40,850
go router in addition to your

00:09:37,010 --> 00:09:42,950
applications in this situation as well

00:09:40,850 --> 00:09:46,160
given that we know that a network

00:09:42,950 --> 00:09:47,779
connection is failing somewhere the

00:09:46,160 --> 00:09:50,000
other set of information that we want to

00:09:47,779 --> 00:09:52,130
gather is what what's actually happening

00:09:50,000 --> 00:09:53,870
with our network connections and so this

00:09:52,130 --> 00:09:55,520
is where the first tool that we're going

00:09:53,870 --> 00:10:00,520
to talk about today comes into play

00:09:55,520 --> 00:10:04,070
which is TCP dump so what is TCP dump

00:10:00,520 --> 00:10:06,500
TCP dump is a packet analyzer that you

00:10:04,070 --> 00:10:09,440
can use via the command line

00:10:06,500 --> 00:10:11,510
there's no GUI for it but what you can

00:10:09,440 --> 00:10:14,360
do with it often is use it to capture

00:10:11,510 --> 00:10:16,730
information on a remote server write the

00:10:14,360 --> 00:10:19,220
output to a file and then actually

00:10:16,730 --> 00:10:23,140
visualize and understand the file with

00:10:19,220 --> 00:10:27,650
some other packet analyzer the has a GUI

00:10:23,140 --> 00:10:29,630
so in this case we ran TCP dump on every

00:10:27,650 --> 00:10:32,630
single VM involved in the connection

00:10:29,630 --> 00:10:34,670
path that we had access to so we ran TCP

00:10:32,630 --> 00:10:36,860
dump on the Diego cell which our

00:10:34,670 --> 00:10:40,690
applications are running on on the hea

00:10:36,860 --> 00:10:44,150
proxy and on the go routers we did so

00:10:40,690 --> 00:10:47,089
via you know a very simple command TCP

00:10:44,150 --> 00:10:50,510
dump right that I'll put to this out dot

00:10:47,089 --> 00:10:52,850
pcap file and then listen on the e0

00:10:50,510 --> 00:10:54,920
interface so looking for all network

00:10:52,850 --> 00:10:59,180
packets coming in and out of that

00:10:54,920 --> 00:11:02,089
virtual machine doing so we now had all

00:10:59,180 --> 00:11:05,320
the information about the network

00:11:02,089 --> 00:11:07,440
connections that were occurring but

00:11:05,320 --> 00:11:09,550
again it can be sort of hard to discern

00:11:07,440 --> 00:11:11,200
what network traffic was actually

00:11:09,550 --> 00:11:13,060
happening by looking at the file itself

00:11:11,200 --> 00:11:14,530
and so this is where the second tool

00:11:13,060 --> 00:11:14,890
we're going to talk about comes into

00:11:14,530 --> 00:11:16,810
play

00:11:14,890 --> 00:11:21,370
I'm sorry we're really is this a

00:11:16,810 --> 00:11:25,720
clarifying question or okay and so this

00:11:21,370 --> 00:11:28,890
is where we come to Wireshark so why are

00:11:25,720 --> 00:11:34,500
sharp an open sourced packet analyzer

00:11:28,890 --> 00:11:37,690
with a GUI so you can use Wireshark to

00:11:34,500 --> 00:11:39,940
look at network traffic in real time or

00:11:37,690 --> 00:11:42,430
you can load a packet or you can load a

00:11:39,940 --> 00:11:45,550
file into it to analyze and so this is

00:11:42,430 --> 00:11:47,890
what we typically tend to do is use TCP

00:11:45,550 --> 00:11:50,020
dump to actually gather information

00:11:47,890 --> 00:11:52,810
about packets load it into a file and

00:11:50,020 --> 00:11:56,170
then load that file into Wireshark to

00:11:52,810 --> 00:11:57,850
then understand what's going on

00:11:56,170 --> 00:12:00,130
Wireshark is also nice because in

00:11:57,850 --> 00:12:02,440
addition to providing visualizations it

00:12:00,130 --> 00:12:04,870
also allows you to filter and inspect

00:12:02,440 --> 00:12:06,430
packets so you can ignore any packets

00:12:04,870 --> 00:12:07,900
that might not be relevant to the

00:12:06,430 --> 00:12:11,710
network connection that you currently

00:12:07,900 --> 00:12:15,490
want to be viewing so in this case we'll

00:12:11,710 --> 00:12:18,850
look at an example of a Wireshark in use

00:12:15,490 --> 00:12:20,860
so here we can see that this is how we

00:12:18,850 --> 00:12:22,930
can visualize and Wireshark we have

00:12:20,860 --> 00:12:25,510
information including the timestamp

00:12:22,930 --> 00:12:29,740
we have the source and destination IP

00:12:25,510 --> 00:12:33,460
for the packet we have the type of

00:12:29,740 --> 00:12:35,650
connection we have the content length

00:12:33,460 --> 00:12:38,800
and then at the very end additional

00:12:35,650 --> 00:12:40,660
information about your packet so in this

00:12:38,800 --> 00:12:43,540
case we had filtered on a specific

00:12:40,660 --> 00:12:45,310
connection between the Diego cell where

00:12:43,540 --> 00:12:47,890
the front end was living and the h8

00:12:45,310 --> 00:12:51,910
proxy and what was interesting in this

00:12:47,890 --> 00:12:55,150
was that we saw that the hea proxy was

00:12:51,910 --> 00:12:57,370
actually sending a thin act packet the

00:12:55,150 --> 00:13:00,040
Diego cell so what the hea proxy is

00:12:57,370 --> 00:13:03,070
saying by sending a fin a packet is that

00:13:00,040 --> 00:13:05,920
i want to close this connection i no

00:13:03,070 --> 00:13:10,180
longer will be respectful to this

00:13:05,920 --> 00:13:12,820
network connection but immediately after

00:13:10,180 --> 00:13:16,030
this fin act packet was sent from the hg

00:13:12,820 --> 00:13:17,640
proxy to the Diego self the Diego cell

00:13:16,030 --> 00:13:20,760
was sending a whole

00:13:17,640 --> 00:13:27,300
lieu of new packets and this is what was

00:13:20,760 --> 00:13:30,560
causing our put to era so at this point

00:13:27,300 --> 00:13:33,290
in time as we're actively debugging the

00:13:30,560 --> 00:13:35,730
customer is probably also trying a whole

00:13:33,290 --> 00:13:38,700
slew of different things to try and get

00:13:35,730 --> 00:13:40,980
their system to stop failing right and

00:13:38,700 --> 00:13:42,720
so we get this new information not

00:13:40,980 --> 00:13:47,190
disabling keep alive from the go router

00:13:42,720 --> 00:13:50,910
to the back end fixes the issue which is

00:13:47,190 --> 00:13:53,520
weird because disabling keep allies from

00:13:50,910 --> 00:13:56,730
the go router is it back-end is the red

00:13:53,520 --> 00:13:59,520
arrows right here but we have seen that

00:13:56,730 --> 00:14:01,590
the connection was failing between the

00:13:59,520 --> 00:14:07,200
front and an H a proxy connection number

00:14:01,590 --> 00:14:10,550
five what is going on how can keep alive

00:14:07,200 --> 00:14:15,090
these red arrows impact what's happening

00:14:10,550 --> 00:14:16,260
with connection five here well at this

00:14:15,090 --> 00:14:17,730
point we had gathered all the

00:14:16,260 --> 00:14:21,630
information that we thought was

00:14:17,730 --> 00:14:23,520
necessary and so the next step in our

00:14:21,630 --> 00:14:26,610
debugging process is to come up with a

00:14:23,520 --> 00:14:28,860
hypothesis and a plan of attack so

00:14:26,610 --> 00:14:31,230
honestly we were pretty biased by this

00:14:28,860 --> 00:14:33,900
recent information that had fixed the

00:14:31,230 --> 00:14:36,090
problem and so our initial hypothesis

00:14:33,900 --> 00:14:39,630
was that some value was being propagated

00:14:36,090 --> 00:14:41,580
from the go router that makes the

00:14:39,630 --> 00:14:45,540
connection between the H a proxy and

00:14:41,580 --> 00:14:48,240
front end fail right like if the go

00:14:45,540 --> 00:14:50,820
router like having keepalive enabled

00:14:48,240 --> 00:14:52,890
fixes the issue then maybe some values

00:14:50,820 --> 00:14:56,490
being propagated that would cause it to

00:14:52,890 --> 00:14:59,910
occur that is again some values being

00:14:56,490 --> 00:15:04,740
propagated from the red arrows here that

00:14:59,910 --> 00:15:05,910
would impact connection five but while

00:15:04,740 --> 00:15:08,460
we were able to come up with this

00:15:05,910 --> 00:15:10,410
hypothesis we couldn't come up with a

00:15:08,460 --> 00:15:13,380
plan of attack to either prove or

00:15:10,410 --> 00:15:16,950
disprove that this hypothesis was true

00:15:13,380 --> 00:15:19,650
and so because we couldn't do that

00:15:16,950 --> 00:15:21,450
looking back at all the information we

00:15:19,650 --> 00:15:24,150
had collected by a TCP dump and

00:15:21,450 --> 00:15:26,550
Wireshark we decided to discard the

00:15:24,150 --> 00:15:30,060
hypothesis for now and come up with a

00:15:26,550 --> 00:15:32,300
new one and this is sort of in line with

00:15:30,060 --> 00:15:34,650
our general process

00:15:32,300 --> 00:15:36,780
repeating coming up with a hypothesis

00:15:34,650 --> 00:15:40,440
and plan of attack until the root cause

00:15:36,780 --> 00:15:41,940
is that so really trying to zero in on

00:15:40,440 --> 00:15:44,340
the information we had a hand

00:15:41,940 --> 00:15:45,900
our second hypothesis was that something

00:15:44,340 --> 00:15:48,450
about the httpclient

00:15:45,900 --> 00:15:51,810
is causing the clothes packet to be

00:15:48,450 --> 00:15:53,880
ignored and after we came up with this

00:15:51,810 --> 00:15:57,480
hypothesis we got another interesting

00:15:53,880 --> 00:15:59,640
tidbit from the customer now if we wait

00:15:57,480 --> 00:16:01,170
five seconds between making the get

00:15:59,640 --> 00:16:04,500
requests from the front end to back end

00:16:01,170 --> 00:16:05,600
and then the put request it would always

00:16:04,500 --> 00:16:09,750
succeed

00:16:05,600 --> 00:16:13,920
five seconds sounds sort of like some

00:16:09,750 --> 00:16:16,500
timeout it fit pretty in line with

00:16:13,920 --> 00:16:19,680
thinking about the HTTP client and its

00:16:16,500 --> 00:16:22,140
configuration so he asked to see the

00:16:19,680 --> 00:16:23,400
application source code to know how the

00:16:22,140 --> 00:16:27,570
front-end application was actually

00:16:23,400 --> 00:16:30,660
making its request doing so we saw that

00:16:27,570 --> 00:16:33,650
in the application itself which was a

00:16:30,660 --> 00:16:38,640
spring application it was simply calling

00:16:33,650 --> 00:16:43,320
HTTP yet it was using the default HTTP

00:16:38,640 --> 00:16:45,510
client no overrides so luckily a pivotal

00:16:43,320 --> 00:16:48,300
we have lots of spring experts so you

00:16:45,510 --> 00:16:51,150
want to slack one to Google and Stack

00:16:48,300 --> 00:16:54,660
Overflow and what we learned is the

00:16:51,150 --> 00:16:57,570
default HTTP client in spring does not

00:16:54,660 --> 00:17:00,380
respect closed packets it will try to

00:16:57,570 --> 00:17:04,260
reuse the existing connection and

00:17:00,380 --> 00:17:07,950
furthermore the default timeout for the

00:17:04,260 --> 00:17:10,350
HTTP client is five seconds so after

00:17:07,950 --> 00:17:12,330
five seconds the HTTP client will close

00:17:10,350 --> 00:17:13,860
the connection open a new one and so

00:17:12,330 --> 00:17:17,700
that's why if you wait a five seconds

00:17:13,860 --> 00:17:19,260
you'd always see it succeed this is also

00:17:17,700 --> 00:17:20,880
confirmed by looking at the

00:17:19,260 --> 00:17:23,910
configuration that we had gathered about

00:17:20,880 --> 00:17:27,180
the H a proxy previously in which we saw

00:17:23,910 --> 00:17:29,790
that the keepalive timeout was set to

00:17:27,180 --> 00:17:32,100
half a second so after half a second the

00:17:29,790 --> 00:17:35,450
H a proxy was closing the connection but

00:17:32,100 --> 00:17:37,640
our clients simply wouldn't respect it

00:17:35,450 --> 00:17:40,290
salt

00:17:37,640 --> 00:17:42,150
obviously this took a lot longer to

00:17:40,290 --> 00:17:44,820
debug than the ten minutes I'm

00:17:42,150 --> 00:17:47,340
describing up here but we can take away

00:17:44,820 --> 00:17:50,460
some valuable lessons from this first of

00:17:47,340 --> 00:17:51,330
all clients have configuration too so

00:17:50,460 --> 00:17:53,340
when we're looking at the configuration

00:17:51,330 --> 00:17:54,900
together in addition to looking at the

00:17:53,340 --> 00:17:58,020
system components you should also gather

00:17:54,900 --> 00:18:00,140
information about any HTTP clients if

00:17:58,020 --> 00:18:02,490
they're in the data path

00:18:00,140 --> 00:18:05,040
we also learned to ignore the red

00:18:02,490 --> 00:18:06,929
herrings and short term fixes in this

00:18:05,040 --> 00:18:08,790
example keep a lives lettuce astray at

00:18:06,929 --> 00:18:10,760
first to form an incorrect first

00:18:08,790 --> 00:18:14,610
hypothesis that we couldn't actually

00:18:10,760 --> 00:18:16,530
prove or disprove only after we found

00:18:14,610 --> 00:18:19,890
the root cause were we then able to go

00:18:16,530 --> 00:18:22,520
back and say why exactly what keep alive

00:18:19,890 --> 00:18:26,970
fix issue which was a whole nother

00:18:22,520 --> 00:18:29,669
debugging session in and of itself given

00:18:26,970 --> 00:18:31,440
that we only have ten or so minutes left

00:18:29,669 --> 00:18:33,360
we're not going to go into that and

00:18:31,440 --> 00:18:35,250
instead I'm gonna hand it over to net

00:18:33,360 --> 00:18:44,179
again to talk about case study number

00:18:35,250 --> 00:18:47,190
two all right so let's look at this one

00:18:44,179 --> 00:18:50,280
so the problem statement here is that

00:18:47,190 --> 00:18:52,230
the API becomes responsive unresponsive

00:18:50,280 --> 00:18:54,990
when trying to see if login other

00:18:52,230 --> 00:18:57,650
requests succeed and some of the go

00:18:54,990 --> 00:19:02,370
router instances have high memory usage

00:18:57,650 --> 00:19:03,929
so first let's visualize this even

00:19:02,370 --> 00:19:06,179
though we were told that the go router

00:19:03,929 --> 00:19:07,950
has high memory usage let's remember

00:19:06,179 --> 00:19:09,900
that there are several components that

00:19:07,950 --> 00:19:12,900
the CF login request needs to go through

00:19:09,900 --> 00:19:15,169
in this case here's a diagram of what

00:19:12,900 --> 00:19:18,000
this deployment architecture looks like

00:19:15,169 --> 00:19:20,220
requests travel into the load balancer

00:19:18,000 --> 00:19:22,020
through one of the H a proxy instances

00:19:20,220 --> 00:19:25,350
into one of the go router instances and

00:19:22,020 --> 00:19:26,970
then it finally reaches the API and then

00:19:25,350 --> 00:19:29,549
the response needs to go back through

00:19:26,970 --> 00:19:33,210
all these components so now we can start

00:19:29,549 --> 00:19:36,059
to gather some information here's a

00:19:33,210 --> 00:19:38,370
couple of places that you can look there

00:19:36,059 --> 00:19:41,190
roughly ordered from broad to more

00:19:38,370 --> 00:19:43,440
focused and as you move from top to

00:19:41,190 --> 00:19:46,110
bottom you may find some indicators that

00:19:43,440 --> 00:19:48,179
let you skip stuff that's irrelevant but

00:19:46,110 --> 00:19:50,470
right now the only information we have

00:19:48,179 --> 00:19:52,899
relates to high memory usage

00:19:50,470 --> 00:19:58,389
let's start broad and look at the VM

00:19:52,899 --> 00:20:01,690
vitals so here's a snippet of some of

00:19:58,389 --> 00:20:04,419
the VM vitals you can get all of this

00:20:01,690 --> 00:20:08,049
information from Bosch and it can tell

00:20:04,419 --> 00:20:10,389
you about the health of the VMS so this

00:20:08,049 --> 00:20:13,450
snippet happens to be only from router

00:20:10,389 --> 00:20:15,279
instance zero and it's one of the VMS

00:20:13,450 --> 00:20:21,100
that was experiencing the problem the

00:20:15,279 --> 00:20:22,779
problem statement the boxes are off but

00:20:21,100 --> 00:20:25,870
you can kind of tell which cones they're

00:20:22,779 --> 00:20:29,080
supposed to be on so you can observe

00:20:25,870 --> 00:20:31,720
here that the memory usage on router 0

00:20:29,080 --> 00:20:35,320
was pretty high but that the CPU usage

00:20:31,720 --> 00:20:37,210
is actually fairly low and another

00:20:35,320 --> 00:20:40,210
interesting observation is that the

00:20:37,210 --> 00:20:46,029
memory being swapped is also increasing

00:20:40,210 --> 00:20:48,399
over time on this router so based on

00:20:46,029 --> 00:20:51,039
what we've kind of seen so far a

00:20:48,399 --> 00:20:52,659
reasonable hypothesis could be that go

00:20:51,039 --> 00:20:54,429
router isn't scaled enough and there

00:20:52,659 --> 00:20:59,860
aren't enough instances to handle the

00:20:54,429 --> 00:21:02,139
load but we've also learned from looking

00:20:59,860 --> 00:21:04,360
around on the VMS that even though we

00:21:02,139 --> 00:21:07,029
see these symptoms on router instance 0

00:21:04,360 --> 00:21:09,429
this wasn't actually happening on all of

00:21:07,029 --> 00:21:12,039
the router instances just a few of them

00:21:09,429 --> 00:21:14,379
and in fact after restarting the routers

00:21:12,039 --> 00:21:16,740
with the high memory usage the problem

00:21:14,379 --> 00:21:19,389
would return often on another router vm

00:21:16,740 --> 00:21:20,860
so the routers that did experience these

00:21:19,389 --> 00:21:22,720
symptoms still reported that they were

00:21:20,860 --> 00:21:24,519
healthy and so there was actually no

00:21:22,720 --> 00:21:29,049
indication to the load bouncer to direct

00:21:24,519 --> 00:21:30,460
traffic elsewhere so so far there's a

00:21:29,049 --> 00:21:32,649
couple of things that are telling us

00:21:30,460 --> 00:21:36,070
that horizontally scaling the go router

00:21:32,649 --> 00:21:38,350
isn't actually going to help so the CPU

00:21:36,070 --> 00:21:41,409
usage was low and a majority of the CPU

00:21:38,350 --> 00:21:43,330
time was spent waiting also the load

00:21:41,409 --> 00:21:45,549
bouncer health or check requests

00:21:43,330 --> 00:21:48,129
themselves are just fine

00:21:45,549 --> 00:21:51,070
so only some requests are unresponsive

00:21:48,129 --> 00:21:52,990
so adding more go router instances won't

00:21:51,070 --> 00:21:57,610
help since traffic won't be directed

00:21:52,990 --> 00:21:59,220
away from them anyway the bad ones so

00:21:57,610 --> 00:22:01,720
now we should try to zoom in further

00:21:59,220 --> 00:22:04,590
look at the logs and see if there is

00:22:01,720 --> 00:22:07,240
anything useful there

00:22:04,590 --> 00:22:11,410
so here's a snippet of the go router

00:22:07,240 --> 00:22:13,810
error logs one error sticks out here the

00:22:11,410 --> 00:22:16,180
go router is having trouble with

00:22:13,810 --> 00:22:17,740
connections to other components because

00:22:16,180 --> 00:22:21,460
it was hitting the maximum number of

00:22:17,740 --> 00:22:23,380
open files so now we have something to

00:22:21,460 --> 00:22:25,390
start to get into why are we hitting

00:22:23,380 --> 00:22:28,660
these limits are the limits themselves

00:22:25,390 --> 00:22:30,850
too low or which connections incoming or

00:22:28,660 --> 00:22:35,160
outgoing are causing us to hit this

00:22:30,850 --> 00:22:35,160
limit so let's look at the diagram again

00:22:35,940 --> 00:22:43,600
open files to the go router could be

00:22:38,860 --> 00:22:45,820
caused by any one of these red arrows so

00:22:43,600 --> 00:22:48,520
it could be AJ proxy to the go router go

00:22:45,820 --> 00:22:53,890
router to app backends or go router to

00:22:48,520 --> 00:22:56,260
system components there's actually one

00:22:53,890 --> 00:22:58,720
more place that we could have open files

00:22:56,260 --> 00:23:01,540
on the go router so if you have a route

00:22:58,720 --> 00:23:03,610
service as part of handling the request

00:23:01,540 --> 00:23:06,580
that the go router gets from each a

00:23:03,610 --> 00:23:09,100
proxy go router will also send a request

00:23:06,580 --> 00:23:12,670
to the route service and keep that

00:23:09,100 --> 00:23:15,040
connection open while while the route

00:23:12,670 --> 00:23:17,890
service is still doing its processing so

00:23:15,040 --> 00:23:20,530
connections from H a proxy to go router

00:23:17,890 --> 00:23:25,660
will stay open while the route service

00:23:20,530 --> 00:23:27,550
is still processing its request so now

00:23:25,660 --> 00:23:29,440
we want to zoom into these red arrows

00:23:27,550 --> 00:23:32,830
where we can to gather some more

00:23:29,440 --> 00:23:35,380
information and we actually learned that

00:23:32,830 --> 00:23:38,140
the number of connections between H I

00:23:35,380 --> 00:23:42,430
proxy and Luco router are going growing

00:23:38,140 --> 00:23:43,270
but not go router to the back ends so

00:23:42,430 --> 00:23:46,030
that's really helpful

00:23:43,270 --> 00:23:49,780
we know that it's this highlighted arrow

00:23:46,030 --> 00:23:51,480
that's growing and we can start to

00:23:49,780 --> 00:23:55,480
reason about why that might be

00:23:51,480 --> 00:23:58,120
so connections from H a proxy to the go

00:23:55,480 --> 00:24:00,120
router stay open for as long as go

00:23:58,120 --> 00:24:02,650
router is processing that request

00:24:00,120 --> 00:24:05,470
including if it's processing it in a

00:24:02,650 --> 00:24:07,570
route service so a slower misbehaving

00:24:05,470 --> 00:24:11,020
route service could cause connections

00:24:07,570 --> 00:24:14,140
from H a proxy to go router to stay open

00:24:11,020 --> 00:24:17,700
so that could be an arrow a reason why

00:24:14,140 --> 00:24:19,320
this circled arrow is growing

00:24:17,700 --> 00:24:21,180
so given what we know so far I'm

00:24:19,320 --> 00:24:24,600
misbehaving route service could be a

00:24:21,180 --> 00:24:26,400
reasonable hypothesis but we ended up

00:24:24,600 --> 00:24:28,410
ruling it out by killing route services

00:24:26,400 --> 00:24:33,090
that were so and seeing the problem

00:24:28,410 --> 00:24:35,730
persist and if you recall from before it

00:24:33,090 --> 00:24:38,400
did only seem like some type of requests

00:24:35,730 --> 00:24:39,870
we're experiencing the bottleneck since

00:24:38,400 --> 00:24:41,640
the load balancer health checks were

00:24:39,870 --> 00:24:44,460
still okay and we've ruled out route

00:24:41,640 --> 00:24:46,530
services and we need more information

00:24:44,460 --> 00:24:48,960
about requests going into the go router

00:24:46,530 --> 00:24:51,540
so that Gore adders access logs can give

00:24:48,960 --> 00:24:54,000
us that and what we learned from the

00:24:51,540 --> 00:24:56,760
access logs was that there are many 404s

00:24:54,000 --> 00:24:59,220
but also that the request time for the

00:24:56,760 --> 00:25:03,630
404 is is much higher than for other

00:24:59,220 --> 00:25:05,670
response codes so our new hypothesis is

00:25:03,630 --> 00:25:09,360
that 404s are causing the connections to

00:25:05,670 --> 00:25:12,080
stay alive and we have a lot of outside

00:25:09,360 --> 00:25:14,730
information at this point but our

00:25:12,080 --> 00:25:17,430
observations seem to be a symptom of a

00:25:14,730 --> 00:25:21,060
deeper problem so what we really need to

00:25:17,430 --> 00:25:25,410
do is dig into the 404s and see how go

00:25:21,060 --> 00:25:27,870
router itself is dealing with them to

00:25:25,410 --> 00:25:31,310
help us do this we can profile the code

00:25:27,870 --> 00:25:34,950
so a good tool for doing that is P prof

00:25:31,310 --> 00:25:38,550
p prof. allows you to collect and

00:25:34,950 --> 00:25:41,700
visualize CPU profiles traces heat

00:25:38,550 --> 00:25:44,160
profiles goroutine profiles etc all

00:25:41,700 --> 00:25:46,650
these things to profile a go program go

00:25:44,160 --> 00:25:48,720
router has a P prof endpoint for

00:25:46,650 --> 00:25:52,050
collecting profile and trace data and

00:25:48,720 --> 00:25:55,740
flame graphed flame graphs are a way to

00:25:52,050 --> 00:25:58,380
visualize profile data that contains

00:25:55,740 --> 00:26:01,140
stack traces so we'll look at a flame

00:25:58,380 --> 00:26:04,800
graph of profile data of the go router

00:26:01,140 --> 00:26:07,890
while the problems occurring so here's

00:26:04,800 --> 00:26:09,420
this flame graph of the CPU profile you

00:26:07,890 --> 00:26:12,120
can't read the text but we'll talk about

00:26:09,420 --> 00:26:16,170
that in a sec first let's see how to

00:26:12,120 --> 00:26:19,140
read the flame graph so the text in all

00:26:16,170 --> 00:26:21,780
of these boxes are function names when

00:26:19,140 --> 00:26:24,780
you use CPU profiling the profiler

00:26:21,780 --> 00:26:28,050
interrupts the program execution at

00:26:24,780 --> 00:26:31,110
specified intervals and logs the state

00:26:28,050 --> 00:26:33,090
of the programs call stack the length of

00:26:31,110 --> 00:26:36,330
the boxes tells you how many samples

00:26:33,090 --> 00:26:38,940
during the profile were spent in this

00:26:36,330 --> 00:26:42,690
function while the function is executing

00:26:38,940 --> 00:26:46,470
so I very very roughly think of this as

00:26:42,690 --> 00:26:48,390
time spent in this function so generally

00:26:46,470 --> 00:26:50,910
you'd want to look at the wider boxes

00:26:48,390 --> 00:26:54,720
and the last couple of calls in the

00:26:50,910 --> 00:26:56,520
stack which are towards the bottom so

00:26:54,720 --> 00:27:00,840
there's a lot of samples in this stack a

00:26:56,520 --> 00:27:03,210
lot of time and the interesting thing

00:27:00,840 --> 00:27:06,570
about this stack is that it's the stack

00:27:03,210 --> 00:27:08,309
that calls that to the logger so there's

00:27:06,570 --> 00:27:10,530
some kind of bottleneck in logging and

00:27:08,309 --> 00:27:12,330
we know that when profiling this code

00:27:10,530 --> 00:27:15,020
there were a lot of four of fours and

00:27:12,330 --> 00:27:19,040
the 404 is likely resulted in logging

00:27:15,020 --> 00:27:21,900
and further down the stack we see that

00:27:19,040 --> 00:27:25,110
during the write for the log it's

00:27:21,900 --> 00:27:27,030
calling to a locking function so we

00:27:25,110 --> 00:27:31,460
finally found our bottleneck which is

00:27:27,030 --> 00:27:31,460
lock contention when writing to the disk

00:27:31,880 --> 00:27:39,390
so lock contention when writing to the

00:27:35,880 --> 00:27:42,390
disk could be caused by any combo of

00:27:39,390 --> 00:27:45,750
many writes to the disk and an actual

00:27:42,390 --> 00:27:49,470
slow disk and the router does attempt to

00:27:45,750 --> 00:27:51,419
log route unknown for all 404s which we

00:27:49,470 --> 00:27:53,010
know is happening very frequently in

00:27:51,419 --> 00:27:55,740
this case when the problem was

00:27:53,010 --> 00:27:57,690
manifesting so this could be what causes

00:27:55,740 --> 00:28:01,440
the connections to persist because they

00:27:57,690 --> 00:28:03,960
were stuck waiting to write and this was

00:28:01,440 --> 00:28:06,540
verified by isolating the go router on a

00:28:03,960 --> 00:28:08,730
VM on an empty hypervisor with no other

00:28:06,540 --> 00:28:10,679
VMs and seeing the problem go away

00:28:08,730 --> 00:28:17,580
because there was no more storage

00:28:10,679 --> 00:28:19,530
contention so yay so here's what we can

00:28:17,580 --> 00:28:21,210
take away from this case there were a

00:28:19,530 --> 00:28:23,190
lot of VM metrics we looked at at the

00:28:21,210 --> 00:28:24,929
beginning but none really had to do with

00:28:23,190 --> 00:28:27,299
the disk so maybe if we had looked at

00:28:24,929 --> 00:28:29,040
something like IO stat earlier that

00:28:27,299 --> 00:28:32,010
could have geared us in the right

00:28:29,040 --> 00:28:33,840
direction a little earlier and also it's

00:28:32,010 --> 00:28:35,640
kind of useful to start broad and gather

00:28:33,840 --> 00:28:39,990
as much information as you can and kind

00:28:35,640 --> 00:28:42,179
of zoom in because it gives you more

00:28:39,990 --> 00:28:44,070
context for when you look at data from

00:28:42,179 --> 00:28:44,999
things like p prof if you just kind of

00:28:44,070 --> 00:28:46,409
look at a flame graph

00:28:44,999 --> 00:28:52,799
straight off the bat you're not really

00:28:46,409 --> 00:28:55,319
sure what you're looking for so yeah so

00:28:52,799 --> 00:28:56,759
now let's go into some general debugging

00:28:55,319 --> 00:28:59,609
tips in the last couple of minutes we

00:28:56,759 --> 00:29:01,529
have together so the first thing we

00:28:59,609 --> 00:29:04,079
always like to recommend is to check

00:29:01,529 --> 00:29:05,789
your time out configurations so in

00:29:04,079 --> 00:29:07,709
general you want your load balancers to

00:29:05,789 --> 00:29:09,719
have the highest timeouts then your go

00:29:07,709 --> 00:29:11,069
writer and then your application this is

00:29:09,719 --> 00:29:12,629
because you don't want something higher

00:29:11,069 --> 00:29:14,819
up in your writing tier to be

00:29:12,629 --> 00:29:16,649
prematurely closing the connection while

00:29:14,819 --> 00:29:19,799
something further on may still be able

00:29:16,649 --> 00:29:21,959
to serve it additionally in regards to

00:29:19,799 --> 00:29:23,909
logging we want to know that application

00:29:21,959 --> 00:29:25,529
logs have timestamps and so you can look

00:29:23,909 --> 00:29:29,309
at what's happening at the application

00:29:25,529 --> 00:29:31,619
level and try to map those logs to

00:29:29,309 --> 00:29:33,289
what's happening in the go router or H a

00:29:31,619 --> 00:29:35,759
proxy logs as well

00:29:33,289 --> 00:29:37,919
additionally endpoint failure requests

00:29:35,759 --> 00:29:40,709
in the access logs have a corresponding

00:29:37,919 --> 00:29:43,199
lock in the go router and so you can

00:29:40,709 --> 00:29:47,009
correlate the two using the vcap request

00:29:43,199 --> 00:29:50,669
ID header lastly some other tips we have

00:29:47,009 --> 00:29:52,769
is if you are in doubt and you're able

00:29:50,669 --> 00:29:54,929
to you should pull the load balancer out

00:29:52,769 --> 00:29:56,309
of the equation it just removes one more

00:29:54,929 --> 00:29:57,659
component that could be causing the

00:29:56,309 --> 00:30:00,149
problem

00:29:57,659 --> 00:30:02,159
also testing each router individually

00:30:00,149 --> 00:30:05,339
depending on the problem statement might

00:30:02,159 --> 00:30:06,869
be good as well and lastly we always

00:30:05,339 --> 00:30:08,789
recommend checking the certs on each

00:30:06,869 --> 00:30:11,389
level a certificate management and

00:30:08,789 --> 00:30:13,979
generation can definitely be a challenge

00:30:11,389 --> 00:30:15,779
so we're gonna post all these slides on

00:30:13,979 --> 00:30:17,279
the schedule if you're interested in

00:30:15,779 --> 00:30:19,559
digging deeper into any of these tools

00:30:17,279 --> 00:30:23,549
we have references to all of them listed

00:30:19,559 --> 00:30:25,439
here otherwise I'm sorry we're happy to

00:30:23,549 --> 00:30:26,879
answer questions outside the door

00:30:25,439 --> 00:30:28,979
because I think there's another talk

00:30:26,879 --> 00:30:32,969
scheduled after and we're at time but

00:30:28,979 --> 00:30:35,629
thank you so much and have a great rest

00:30:32,969 --> 00:30:35,629
of your summit

00:30:36,160 --> 00:30:38,349

YouTube URL: https://www.youtube.com/watch?v=0Bfj2eepO9g


