Title: Cloud Foundry Community Advisory Board Call [May 2020]
Publication date: 2020-05-20
Playlist: Community Advisory Board
Description: 
	Agenda available here: https://docs.google.com/document/d/1SCOlAquyUmNM-AQnekCOXiwhLs6gveTxAcduvDcW_xI/edit?usp=sharing
Captions: 
	00:00:00,290 --> 00:00:11,759
let you go okay welcome to some months

00:00:07,950 --> 00:00:17,430
cab cults um I believe it's the month of

00:00:11,759 --> 00:00:19,199
May May that's it's not April IIIi can't

00:00:17,430 --> 00:00:22,680
really keep track I actually literally

00:00:19,199 --> 00:00:26,130
have to check the County April is

00:00:22,680 --> 00:00:28,949
anyways welcome thank you all for coming

00:00:26,130 --> 00:00:31,369
to the Cloud Foundry community advisory

00:00:28,949 --> 00:00:34,200
board meeting for the month of May I

00:00:31,369 --> 00:00:36,930
thank you for coming we've got the usual

00:00:34,200 --> 00:00:39,239
updates first from the Cloud Foundry

00:00:36,930 --> 00:00:42,480
foundation we have chip here maybe

00:00:39,239 --> 00:00:46,590
others that I can't see and we will go

00:00:42,480 --> 00:00:49,440
through the pmc project highlights and

00:00:46,590 --> 00:00:52,320
then we have two presentations from

00:00:49,440 --> 00:00:54,000
t-mobile and I believe we have the

00:00:52,320 --> 00:00:56,250
people from t-mobile and excuse me I

00:00:54,000 --> 00:00:57,750
don't have the the names of the people

00:00:56,250 --> 00:00:59,010
from t-mobile in front of me so if you

00:00:57,750 --> 00:01:01,379
could introduce yourself say let's we

00:00:59,010 --> 00:01:04,559
kick off first dealing with noisy

00:01:01,379 --> 00:01:06,240
neighbor problems on CF and also another

00:01:04,559 --> 00:01:08,340
presentation on automating the lifecycle

00:01:06,240 --> 00:01:11,820
management of large-scale CF

00:01:08,340 --> 00:01:14,640
environments so to kick things off can I

00:01:11,820 --> 00:01:16,650
turn this over to you chip and let us

00:01:14,640 --> 00:01:18,900
know what's happening with Cloud Foundry

00:01:16,650 --> 00:01:21,750
foundation yeah we're gonna keep it

00:01:18,900 --> 00:01:23,939
quick sometimes I feel like these

00:01:21,750 --> 00:01:27,960
updates are just about events so this

00:01:23,939 --> 00:01:31,860
updates just about an event we we not

00:01:27,960 --> 00:01:35,070
the schedule for the in really big

00:01:31,860 --> 00:01:37,950
air-quote North American summit just

00:01:35,070 --> 00:01:40,979
happens to be centered on the US central

00:01:37,950 --> 00:01:43,950
time zone but it'll be clustered in the

00:01:40,979 --> 00:01:45,590
morning of two days right so June 24th

00:01:43,950 --> 00:01:48,600
June 25th

00:01:45,590 --> 00:01:50,520
hopefully that's two not that's not too

00:01:48,600 --> 00:01:53,939
late for our friends that are based in

00:01:50,520 --> 00:01:55,799
Europe and might be a little bit early

00:01:53,939 --> 00:01:59,070
for those of you that are based on the

00:01:55,799 --> 00:02:01,590
north american west coast but we're

00:01:59,070 --> 00:02:04,530
trying to kind of do our best to pick a

00:02:01,590 --> 00:02:07,409
time zone that's gonna work you know

00:02:04,530 --> 00:02:10,110
across as many many geographies as

00:02:07,409 --> 00:02:13,230
possible sometimes i also wish that we

00:02:10,110 --> 00:02:13,860
were actually a flat earth so that we

00:02:13,230 --> 00:02:17,040
could just

00:02:13,860 --> 00:02:17,610
truly eliminate time zones and that

00:02:17,040 --> 00:02:21,020
would be great

00:02:17,610 --> 00:02:24,360
so anyway register schedules out

00:02:21,020 --> 00:02:25,290
registrations open anybody on the CF dev

00:02:24,360 --> 00:02:27,030
mailing list should have seen a

00:02:25,290 --> 00:02:29,490
contributor code feel free to use it if

00:02:27,030 --> 00:02:32,070
you're on that mailing list any member

00:02:29,490 --> 00:02:34,790
or all of our primary member contacts

00:02:32,070 --> 00:02:39,470
that receive receive codes to register

00:02:34,790 --> 00:02:41,370
so lots of opportunities to get a get

00:02:39,470 --> 00:02:44,010
registered and see some really

00:02:41,370 --> 00:02:45,959
interesting content I'll say it I've

00:02:44,010 --> 00:02:46,980
said it before I'll say it again this is

00:02:45,959 --> 00:02:49,200
a virtual event

00:02:46,980 --> 00:02:51,240
everybody's going virtual nobody knows

00:02:49,200 --> 00:02:52,620
how to actually do it we're all learning

00:02:51,240 --> 00:02:54,570
from each other both the commercial

00:02:52,620 --> 00:02:57,239
companies and Susa is an example I think

00:02:54,570 --> 00:03:00,030
is actively running a event right now

00:02:57,239 --> 00:03:00,570
virtually using the same platform that

00:03:00,030 --> 00:03:02,520
we'll be using

00:03:00,570 --> 00:03:04,320
so there's there's quite a lot of

00:03:02,520 --> 00:03:07,800
interesting things that that we're going

00:03:04,320 --> 00:03:10,350
to learn from that so with that I'm done

00:03:07,800 --> 00:03:11,850
talking about events everything else

00:03:10,350 --> 00:03:13,680
that we do is in support of the project

00:03:11,850 --> 00:03:15,980
so we should roll over to project

00:03:13,680 --> 00:03:15,980
updates

00:03:16,209 --> 00:03:25,709
I wondered um I wanted to highlight that

00:03:20,140 --> 00:03:28,689
spring one also has its CFP closing

00:03:25,709 --> 00:03:30,430
today and there is a Cloud Foundry Trek

00:03:28,689 --> 00:03:32,769
for anyone who's interested in

00:03:30,430 --> 00:03:37,950
submitting a talk to the Cloud Foundry

00:03:32,769 --> 00:03:39,909
track for spring one dot IO awesome

00:03:37,950 --> 00:03:42,150
that's a great point

00:03:39,909 --> 00:03:42,150
Ely

00:03:44,099 --> 00:03:50,489
and like everything else would be

00:03:46,650 --> 00:03:54,200
virtual right yes it'll be a virtual I

00:03:50,489 --> 00:03:54,200
believe in in August

00:03:57,490 --> 00:03:59,550
you

00:04:00,470 --> 00:04:08,120
you if you could drop that in the in the

00:04:04,430 --> 00:04:10,960
agenda we'll use that for notes to make

00:04:08,120 --> 00:04:10,960
sure that would be great

00:04:11,170 --> 00:04:16,970
have we got someone actually is it you

00:04:14,150 --> 00:04:20,019
you if for a prime-time PMC or that be

00:04:16,970 --> 00:04:23,060
Eric it's right yeah I can cover that

00:04:20,019 --> 00:04:25,520
yeah a few few highlights from the

00:04:23,060 --> 00:04:28,520
runtime PMC I've had some big releases

00:04:25,520 --> 00:04:30,710
from the integration projects release

00:04:28,520 --> 00:04:34,040
integration role DCF deployment version

00:04:30,710 --> 00:04:36,410
13 and version 0.2 of CF frigates over

00:04:34,040 --> 00:04:40,130
the past month and not to be outdone

00:04:36,410 --> 00:04:41,960
Cube CF released its version 2.0 and I

00:04:40,130 --> 00:04:44,300
know they've been busy integrating some

00:04:41,960 --> 00:04:47,150
of the helm charts like Iranian ireenie

00:04:44,300 --> 00:04:50,120
X into that release to replace the

00:04:47,150 --> 00:04:52,640
bas-reliefs derived artifacts and then

00:04:50,120 --> 00:04:54,830
also there's something that I think will

00:04:52,640 --> 00:04:56,390
have more and more information and

00:04:54,830 --> 00:04:59,150
communication about over the next few

00:04:56,390 --> 00:05:02,570
months but the CLI team is getting ready

00:04:59,150 --> 00:05:05,720
to release the initial GA version of the

00:05:02,570 --> 00:05:07,840
v7 CLI so this is going to have some

00:05:05,720 --> 00:05:10,310
breaking changes compared to v6

00:05:07,840 --> 00:05:13,220
primarily in support of things like

00:05:10,310 --> 00:05:15,169
rolling updates and other commands that

00:05:13,220 --> 00:05:17,090
relate to the v3 API is the claw

00:05:15,169 --> 00:05:18,500
controllers been developing underneath

00:05:17,090 --> 00:05:19,669
that to support those kinds of

00:05:18,500 --> 00:05:22,010
operations over the years

00:05:19,669 --> 00:05:28,250
quick question is the breakage only in

00:05:22,010 --> 00:05:30,890
the v3 API calls well I think the the v7

00:05:28,250 --> 00:05:33,040
refers to just some of the commands

00:05:30,890 --> 00:05:35,840
having breaking changes in the CFC Li

00:05:33,040 --> 00:05:38,150
I'm not sure exactly which ones off the

00:05:35,840 --> 00:05:40,700
top of my head but it's like they'll

00:05:38,150 --> 00:05:42,410
have more details about any commands

00:05:40,700 --> 00:05:44,360
that are changing code one of them are

00:05:42,410 --> 00:05:50,210
just things and yeah you do have an

00:05:44,360 --> 00:05:52,640
extensive document great okay and then

00:05:50,210 --> 00:05:55,010
in terms of other project updates so

00:05:52,640 --> 00:05:58,570
Kathy's been moving along with its

00:05:55,010 --> 00:06:01,910
integration with kpac in the gate space

00:05:58,570 --> 00:06:04,550
to allow for operations like updating

00:06:01,910 --> 00:06:06,530
the build packs and root of s that key

00:06:04,550 --> 00:06:08,570
pack is managing FRAP Gatien images and

00:06:06,530 --> 00:06:09,740
then figuring out how those rid of s

00:06:08,570 --> 00:06:11,800
updates for example are going to

00:06:09,740 --> 00:06:14,990
propagate out to application instances

00:06:11,800 --> 00:06:16,940
in that context

00:06:14,990 --> 00:06:18,260
so other improvements ireenie now

00:06:16,940 --> 00:06:20,510
supports rolling deploys for

00:06:18,260 --> 00:06:24,280
applications and they are working to

00:06:20,510 --> 00:06:26,360
support application tasks as well and

00:06:24,280 --> 00:06:28,400
networking I know we've discussed this

00:06:26,360 --> 00:06:30,080
in some other arenas but they've been

00:06:28,400 --> 00:06:31,820
driving out some work around having a C

00:06:30,080 --> 00:06:33,710
or D for CF France and so they're

00:06:31,820 --> 00:06:35,290
continuing work to integrate with that

00:06:33,710 --> 00:06:40,160
and to support that as a way of

00:06:35,290 --> 00:06:43,820
translating routing information into the

00:06:40,160 --> 00:06:46,190
underlying kubernetes cluster and then

00:06:43,820 --> 00:06:49,510
the logging and metrics team which we've

00:06:46,190 --> 00:06:52,270
recently renamed for logger gator to

00:06:49,510 --> 00:06:54,800
correctly denote their full scope of

00:06:52,270 --> 00:06:57,170
responsibilities they've recently

00:06:54,800 --> 00:06:59,260
finished working with a Cappy team to

00:06:57,170 --> 00:07:02,650
get Cloud Controller metrics exposed via

00:06:59,260 --> 00:07:05,990
prometheus exporter again in a

00:07:02,650 --> 00:07:09,740
kubernetes deployment and they've also

00:07:05,990 --> 00:07:11,120
been working on how to inject the logs

00:07:09,740 --> 00:07:14,060
from control plant components such as

00:07:11,120 --> 00:07:17,740
cloud controller or the routing tier

00:07:14,060 --> 00:07:21,320
into application log streams and

00:07:17,740 --> 00:07:23,210
indicates distributions so you know

00:07:21,320 --> 00:07:24,950
those are some of the highlights but as

00:07:23,210 --> 00:07:27,440
always people are welcome to drop into

00:07:24,950 --> 00:07:29,540
the app runtime PMC meetings which

00:07:27,440 --> 00:07:35,480
happen over two weeks and ask questions

00:07:29,540 --> 00:07:37,100
there too thanks so much Eric just a

00:07:35,480 --> 00:07:38,780
tiny thing about cube see if I know that

00:07:37,100 --> 00:07:41,330
you actually released the 2.2 recently

00:07:38,780 --> 00:07:46,190
to fix some some breakage in 2.0 that

00:07:41,330 --> 00:07:51,040
people noticed have we got someone

00:07:46,190 --> 00:07:51,040
representing the Boche PMC here today

00:07:51,800 --> 00:07:56,260
and I'm gonna take

00:07:53,230 --> 00:07:59,680
wild guests that we also don't have dr.

00:07:56,260 --> 00:08:03,160
max for extensions because he is trying

00:07:59,680 --> 00:08:04,980
to vacate that position and there is I

00:08:03,160 --> 00:08:11,350
wanted to call a people's attention to

00:08:04,980 --> 00:08:13,060
varnas post from a few weeks ago seeking

00:08:11,350 --> 00:08:18,790
nominations for the Cloud Foundry

00:08:13,060 --> 00:08:19,960
extensions PMC lead Souza has been I've

00:08:18,790 --> 00:08:22,600
been looking around to see if there was

00:08:19,960 --> 00:08:24,970
some volunteers at Sousa we may not have

00:08:22,600 --> 00:08:27,030
someone that I can nominate for that

00:08:24,970 --> 00:08:29,890
position so I would like to suggest that

00:08:27,030 --> 00:08:31,480
this is a good opportunity from for

00:08:29,890 --> 00:08:32,410
another organization in the community or

00:08:31,480 --> 00:08:34,510
another individual in the community

00:08:32,410 --> 00:08:38,919
who's interested in seeing the

00:08:34,510 --> 00:08:41,050
extensions BMC grow just up and the

00:08:38,919 --> 00:08:44,590
volunteer for this you can talk to any

00:08:41,050 --> 00:08:46,810
of the foundation people or myself about

00:08:44,590 --> 00:08:49,140
that on slack if you don't feel

00:08:46,810 --> 00:08:52,180
comfortable speaking up now or

00:08:49,140 --> 00:08:56,020
nominating yourself in the mailing list

00:08:52,180 --> 00:08:57,100
like I did for this job so just calling

00:08:56,020 --> 00:08:58,960
your attention to that there is a

00:08:57,100 --> 00:09:00,430
vacancy we'd love to see someone who

00:08:58,960 --> 00:09:01,960
there's a passion for all of the

00:09:00,430 --> 00:09:03,400
extensions in the community and making

00:09:01,960 --> 00:09:05,500
that community grow a little bit so

00:09:03,400 --> 00:09:07,720
please have a look at that post and have

00:09:05,500 --> 00:09:09,630
a think about if you'd like to to leave

00:09:07,720 --> 00:09:13,630
that and I'll continue looking around

00:09:09,630 --> 00:09:18,790
for for for vol and telling people at

00:09:13,630 --> 00:09:22,960
Sousa with that if there's nothing else

00:09:18,790 --> 00:09:26,800
from the the pmc projects can I hand it

00:09:22,960 --> 00:09:30,160
off to somebody from t-mobile and I

00:09:26,800 --> 00:09:33,700
believe we have it Roy

00:09:30,160 --> 00:09:37,960
hi honey hi I'm sorry about the t-mobile

00:09:33,700 --> 00:09:40,360
we do have a bunch of folks here we have

00:09:37,960 --> 00:09:43,060
Amy McGuire who will be covering noisy

00:09:40,360 --> 00:09:45,820
neighbors and then Brandon in track who

00:09:43,060 --> 00:09:49,750
will be covering automation lifecycle so

00:09:45,820 --> 00:09:54,540
we're all here okay excellent

00:09:49,750 --> 00:09:54,540
oh no downer name so it's Eamon

00:10:01,480 --> 00:10:14,270
and we should we passed presenter to you

00:10:04,970 --> 00:10:20,480
even I sure Ashley can you do that I'm

00:10:14,270 --> 00:10:36,770
due up Pass presenter or share screen

00:10:20,480 --> 00:10:39,190
yeah excuse me so I am a warrior I'm

00:10:36,770 --> 00:10:41,480
with the t-mobile on the platform it

00:10:39,190 --> 00:10:43,040
excuse me platforming infrastructure

00:10:41,480 --> 00:10:45,320
engineering team and I'm going to be

00:10:43,040 --> 00:10:50,360
doing a presentation about how we

00:10:45,320 --> 00:10:53,960
managed our log losses at scale so when

00:10:50,360 --> 00:10:57,500
I say scale our scale is pretty big in

00:10:53,960 --> 00:11:00,160
terms of PCI futures got over 20

00:10:57,500 --> 00:11:03,110
foundations over 28 individual

00:11:00,160 --> 00:11:07,220
deployments of PCF in various stages

00:11:03,110 --> 00:11:10,370
centers regions across the country yeah

00:11:07,220 --> 00:11:13,610
over 70,000 containers we're 700 million

00:11:10,370 --> 00:11:15,440
daily transactions 3,000 individual

00:11:13,610 --> 00:11:17,620
applications and over a hundred

00:11:15,440 --> 00:11:22,010
different application teams so it's

00:11:17,620 --> 00:11:24,830
really big really a complicated problem

00:11:22,010 --> 00:11:27,890
to solve especially given the size of

00:11:24,830 --> 00:11:29,900
our team which is right right now we're

00:11:27,890 --> 00:11:31,490
about four or five people that kind of

00:11:29,900 --> 00:11:33,530
concentrate on this we have some

00:11:31,490 --> 00:11:37,100
ancillary help from other teams but it's

00:11:33,530 --> 00:11:39,110
it's a big challenge so the issue that

00:11:37,100 --> 00:11:42,620
we had been running into was that we

00:11:39,110 --> 00:11:50,540
were seeing frequent excuse me frequency

00:11:42,620 --> 00:11:52,100
to your log loss across excuse me

00:11:50,540 --> 00:11:56,900
frequent to your log loss across

00:11:52,100 --> 00:11:58,610
applications on PCF so this could be in

00:11:56,900 --> 00:12:00,290
any step in the chain essentially it

00:11:58,610 --> 00:12:03,080
could be an hour longer Gator components

00:12:00,290 --> 00:12:05,240
are longer Gator components analysis log

00:12:03,080 --> 00:12:07,040
agent and the newer version feeds into

00:12:05,240 --> 00:12:09,060
or syslog components are so sly

00:12:07,040 --> 00:12:11,640
components

00:12:09,060 --> 00:12:14,430
pull off by a swung forwarder

00:12:11,640 --> 00:12:16,980
and I'm us alongside we've got indexers

00:12:14,430 --> 00:12:18,900
and various components so and any step

00:12:16,980 --> 00:12:22,410
in the chain there we could see issues

00:12:18,900 --> 00:12:24,030
with you know CPU or memory or just cues

00:12:22,410 --> 00:12:25,920
filling up and so on and so forth and

00:12:24,030 --> 00:12:28,310
what we found out was that more often

00:12:25,920 --> 00:12:31,740
than not the cause of the problem was

00:12:28,310 --> 00:12:33,870
the noisy neighbor so an application

00:12:31,740 --> 00:12:36,300
that was just logging really excessively

00:12:33,870 --> 00:12:38,070
and since it's a shared platform we've

00:12:36,300 --> 00:12:40,500
got the containers on the Diego cell

00:12:38,070 --> 00:12:44,190
sharing sharing the components to get

00:12:40,500 --> 00:12:45,840
the logs off of the off of the node same

00:12:44,190 --> 00:12:48,620
thing with the syslog nodes and was one

00:12:45,840 --> 00:12:51,990
forwarder said um there was a typically

00:12:48,620 --> 00:12:53,760
a single or just a few applications that

00:12:51,990 --> 00:12:57,090
were responsible for flooding everything

00:12:53,760 --> 00:12:59,340
and causing the log loss so how much

00:12:57,090 --> 00:13:03,780
lagos were releasing at some points we

00:12:59,340 --> 00:13:06,120
were seeing up to 75% log loss so this

00:13:03,780 --> 00:13:08,760
was typically somebody would be doing a

00:13:06,120 --> 00:13:10,380
load test or they they left info on or

00:13:08,760 --> 00:13:12,510
or there was some kind of a stack trace

00:13:10,380 --> 00:13:15,750
printing out a thousand line stack trace

00:13:12,510 --> 00:13:17,340
line by line you know millions of times

00:13:15,750 --> 00:13:20,250
a second or something ridiculous like

00:13:17,340 --> 00:13:22,860
that so we could see Peaks like that on

00:13:20,250 --> 00:13:26,130
a sustained basis we could see in excess

00:13:22,860 --> 00:13:28,050
of 15% log loss even when we weren't

00:13:26,130 --> 00:13:30,630
seeing log loss we were seeing

00:13:28,050 --> 00:13:33,600
substantial delays just the components

00:13:30,630 --> 00:13:35,910
have various cues and so on that fill up

00:13:33,600 --> 00:13:37,920
with the messages and then those

00:13:35,910 --> 00:13:39,480
messages or you know pull off of the

00:13:37,920 --> 00:13:42,390
queue and eventually sent down the

00:13:39,480 --> 00:13:44,340
pipeline so those components were all

00:13:42,390 --> 00:13:45,810
becoming overwhelmed so even when they

00:13:44,340 --> 00:13:51,690
weren't being dropped we would see

00:13:45,810 --> 00:13:54,930
delays of 15 30 sometimes an hour due to

00:13:51,690 --> 00:13:57,450
various issues along the chain another

00:13:54,930 --> 00:13:59,040
issue that we had was just in terms of

00:13:57,450 --> 00:14:00,600
the physical retention after logs

00:13:59,040 --> 00:14:01,860
actually get to slunk out we're

00:14:00,600 --> 00:14:04,830
retaining them of course you know as

00:14:01,860 --> 00:14:07,890
SSDs and platter drives and so on and

00:14:04,830 --> 00:14:09,690
our retention is determined by how much

00:14:07,890 --> 00:14:13,020
space we have left and how quickly we're

00:14:09,690 --> 00:14:14,370
using that space so because of how noisy

00:14:13,020 --> 00:14:16,440
these neighbors were because of how much

00:14:14,370 --> 00:14:18,420
they were logging on these disks they

00:14:16,440 --> 00:14:18,750
were filling up really quickly and it's

00:14:18,420 --> 00:14:20,190
um

00:14:18,750 --> 00:14:22,980
we had you know under four days

00:14:20,190 --> 00:14:25,680
retention which is not really helpful

00:14:22,980 --> 00:14:27,150
for teams that figure out you know they

00:14:25,680 --> 00:14:28,470
had to have a bug today and they're

00:14:27,150 --> 00:14:30,780
trying to trace it back to their last

00:14:28,470 --> 00:14:32,700
release you know a week ago or something

00:14:30,780 --> 00:14:34,230
to that effect and the logs don't exist

00:14:32,700 --> 00:14:36,630
or they're trying to do some kind of

00:14:34,230 --> 00:14:38,100
reporting or reconciliation or just

00:14:36,630 --> 00:14:41,310
trying to track something down you know

00:14:38,100 --> 00:14:43,440
it's it's unusual to catch something

00:14:41,310 --> 00:14:45,690
unless it's really bad then you know

00:14:43,440 --> 00:14:48,480
that that time window so wasn't a good

00:14:45,690 --> 00:14:50,850
experience some of the challenges that

00:14:48,480 --> 00:14:53,190
we had and just kind of tackling the

00:14:50,850 --> 00:14:55,350
issue was the the complexity of the of

00:14:53,190 --> 00:14:57,780
the infrastructure the fact that we

00:14:55,350 --> 00:14:59,490
didn't necessarily own every component

00:14:57,780 --> 00:15:01,920
the Splunk team was an entirely

00:14:59,490 --> 00:15:04,700
different team we didn't have a good

00:15:01,920 --> 00:15:07,260
view from the application perspective

00:15:04,700 --> 00:15:09,090
because we didn't know and all of the

00:15:07,260 --> 00:15:13,710
components we didn't have a consistent

00:15:09,090 --> 00:15:15,630
alerting approach and because of the

00:15:13,710 --> 00:15:18,540
various points in the chain where we

00:15:15,630 --> 00:15:20,490
could see this issue it was ended up

00:15:18,540 --> 00:15:22,860
being basically an ad hoc process and we

00:15:20,490 --> 00:15:25,860
have our daytime on-call would be

00:15:22,860 --> 00:15:27,660
getting complaints from customers or

00:15:25,860 --> 00:15:30,810
they'd be seeing an alert on a longer

00:15:27,660 --> 00:15:34,560
Gator component or Splunk component

00:15:30,810 --> 00:15:36,780
would a trip and they contact us so then

00:15:34,560 --> 00:15:38,850
we'd kind of get in and just poke around

00:15:36,780 --> 00:15:41,430
and try and figure out okay where's the

00:15:38,850 --> 00:15:43,500
noisy neighbor because that's more often

00:15:41,430 --> 00:15:44,850
than not that was the issue we'd be able

00:15:43,500 --> 00:15:47,520
to track them down with you know some ad

00:15:44,850 --> 00:15:49,230
hoc queries then we'd need to go and

00:15:47,520 --> 00:15:51,480
manually look up okay who owns this

00:15:49,230 --> 00:15:54,510
application and then generate the email

00:15:51,480 --> 00:15:57,420
and try and get that team you know on

00:15:54,510 --> 00:15:59,520
the hunter bridge if we needed to and it

00:15:57,420 --> 00:16:01,740
just it took a long time and people were

00:15:59,520 --> 00:16:04,370
losing logs all the while so you know

00:16:01,740 --> 00:16:07,980
it's just not a good customer experience

00:16:04,370 --> 00:16:09,600
so in order to solve it we started out

00:16:07,980 --> 00:16:11,790
by saying okay what's our you know

00:16:09,600 --> 00:16:14,460
service level objective you know what do

00:16:11,790 --> 00:16:16,650
we want to accomplish okay how do we get

00:16:14,460 --> 00:16:18,330
the users to help us accomplish this and

00:16:16,650 --> 00:16:20,490
the Terms of Service from the users and

00:16:18,330 --> 00:16:22,710
how do we figure out when users or

00:16:20,490 --> 00:16:24,870
customers are violating those terms of

00:16:22,710 --> 00:16:27,300
service and what's the mechanism we can

00:16:24,870 --> 00:16:31,140
use to inform them so that they can take

00:16:27,300 --> 00:16:32,259
action so the objective SLO that we

00:16:31,140 --> 00:16:35,329
ended up coming up with

00:16:32,259 --> 00:16:37,339
90% we felt 90% was reasonable at all

00:16:35,329 --> 00:16:38,569
times you know what I ideally more than

00:16:37,339 --> 00:16:40,790
that most of the time but that we should

00:16:38,569 --> 00:16:43,069
never have you know fewer than 90% of

00:16:40,790 --> 00:16:45,860
our logs reach the end destination with

00:16:43,069 --> 00:16:47,509
trusses long you know within a

00:16:45,860 --> 00:16:49,809
reasonable time frame shouldn't seem big

00:16:47,509 --> 00:16:53,509
delays we shouldn't be dropping anything

00:16:49,809 --> 00:16:55,670
and in terms of excuse me the terms of

00:16:53,509 --> 00:16:58,910
service for the user is just based on

00:16:55,670 --> 00:17:01,329
our observations about when the the

00:16:58,910 --> 00:17:04,189
component started to see issues and

00:17:01,329 --> 00:17:06,589
based on some of our conversations with

00:17:04,189 --> 00:17:08,870
pivotal we decided that an individual

00:17:06,589 --> 00:17:10,419
application incident shouldn't exceed a

00:17:08,870 --> 00:17:13,640
hundred thousand logs per minute and

00:17:10,419 --> 00:17:15,140
that an individual application instance

00:17:13,640 --> 00:17:17,209
should never exceed you know a burst

00:17:15,140 --> 00:17:19,760
rate of more than a million logs for

00:17:17,209 --> 00:17:22,250
Mina which sounds ridiculous but we've

00:17:19,760 --> 00:17:24,980
had a applications log a lot more than

00:17:22,250 --> 00:17:29,000
alex per minute and it's a wonder they

00:17:24,980 --> 00:17:30,799
stay running but we do see that so in

00:17:29,000 --> 00:17:34,039
terms of the implicate implementation

00:17:30,799 --> 00:17:39,169
that we came up with the solution was a

00:17:34,039 --> 00:17:41,960
set of micro services and we have a job

00:17:39,169 --> 00:17:45,770
that runs on concourse by a cron right

00:17:41,960 --> 00:17:47,750
now it's every hour so we post a query

00:17:45,770 --> 00:17:50,840
and a callback URL to or Splunk query

00:17:47,750 --> 00:17:52,700
service and thus one query service has a

00:17:50,840 --> 00:17:54,230
knowledge and credentials of this one

00:17:52,700 --> 00:17:57,020
clusters that needs to interact with

00:17:54,230 --> 00:17:58,970
it'll execute that query against each of

00:17:57,020 --> 00:18:01,630
those clusters we have one for NP our

00:17:58,970 --> 00:18:04,070
non production one for our payment cards

00:18:01,630 --> 00:18:06,500
concerns and then one for production

00:18:04,070 --> 00:18:08,450
traffic from E to those that determines

00:18:06,500 --> 00:18:10,220
which applications are violating the

00:18:08,450 --> 00:18:12,620
Terms of Service just based on you know

00:18:10,220 --> 00:18:14,659
some query language and it gets back the

00:18:12,620 --> 00:18:18,649
metadata that we have about those works

00:18:14,659 --> 00:18:20,480
some of that is added from excuse me

00:18:18,649 --> 00:18:23,630
from our syslog infrastructure and some

00:18:20,480 --> 00:18:24,649
of that's just there by default so it'll

00:18:23,630 --> 00:18:28,039
collect that information

00:18:24,649 --> 00:18:29,779
it'll bundle that up and some JSON and

00:18:28,039 --> 00:18:31,250
it'll post that to our noisy neighbor

00:18:29,779 --> 00:18:35,299
microservice the noisy neighbor

00:18:31,250 --> 00:18:36,529
microservice takes those organs and

00:18:35,299 --> 00:18:38,960
looks in our internal

00:18:36,529 --> 00:18:41,960
metadata store about those works which

00:18:38,960 --> 00:18:43,130
for us right now is excuse me is

00:18:41,960 --> 00:18:45,660
bitbucket

00:18:43,130 --> 00:18:49,890
it'll get back the configs they show we

00:18:45,660 --> 00:18:51,810
tory's map - to which own nursed which

00:18:49,890 --> 00:18:54,420
in our internal infrastructure will be

00:18:51,810 --> 00:18:57,120
employee IDs so gathers up those

00:18:54,420 --> 00:18:59,270
employee IDs sends that to our email

00:18:57,120 --> 00:19:01,800
lookup service the email lookup service

00:18:59,270 --> 00:19:03,540
interacts with our LDAP service converts

00:19:01,800 --> 00:19:05,250
those employee IDs to email addresses

00:19:03,540 --> 00:19:07,560
sends that back to the noisy neighbor

00:19:05,250 --> 00:19:09,770
service and then finally the noisy

00:19:07,560 --> 00:19:12,630
neighbor service bundles that altogether

00:19:09,770 --> 00:19:16,170
puts it into a templated email and

00:19:12,630 --> 00:19:20,760
shifts that out to our customers via our

00:19:16,170 --> 00:19:24,630
SMTP server in terms of the email itself

00:19:20,760 --> 00:19:26,400
we have some helpful tips on there you

00:19:24,630 --> 00:19:28,290
know some things that we we've seen in

00:19:26,400 --> 00:19:31,680
the past I've mentioned few giving of

00:19:28,290 --> 00:19:34,950
the talk so logging stack traces with

00:19:31,680 --> 00:19:36,720
line breaks so you could see thousand

00:19:34,950 --> 00:19:38,700
line stack trace but if there's line

00:19:36,720 --> 00:19:40,740
breaks that's a thousand individual

00:19:38,700 --> 00:19:43,410
events per stack trace and if you've got

00:19:40,740 --> 00:19:45,060
some you know recurrent error we've seen

00:19:43,410 --> 00:19:47,460
this with for example interactions with

00:19:45,060 --> 00:19:49,590
Costco or people trying to reinitiate

00:19:47,460 --> 00:19:51,900
sessions you know it's it's thousands

00:19:49,590 --> 00:19:55,280
and thousands of lines you know every

00:19:51,900 --> 00:19:57,390
every second which is it's not ideal

00:19:55,280 --> 00:20:00,060
there are some other cases where people

00:19:57,390 --> 00:20:02,520
will leave on info or debug because they

00:20:00,060 --> 00:20:05,700
were debugging something locally or in a

00:20:02,520 --> 00:20:09,270
non production environment or you know

00:20:05,700 --> 00:20:11,940
those sorts of things so we give them

00:20:09,270 --> 00:20:15,450
tips on how to address in it then kind

00:20:11,940 --> 00:20:17,700
of the idea here is we say hey you know

00:20:15,450 --> 00:20:19,500
if this isn't addressed in mix X hours

00:20:17,700 --> 00:20:20,850
and that's you know that fluctuates

00:20:19,500 --> 00:20:23,310
depending on the environment and the

00:20:20,850 --> 00:20:24,750
severity of the logging then we'll need

00:20:23,310 --> 00:20:26,010
to unfortunately

00:20:24,750 --> 00:20:28,830
disconnect you from logging

00:20:26,010 --> 00:20:30,390
infrastructure because you know we need

00:20:28,830 --> 00:20:34,320
to preserve the health of the platform

00:20:30,390 --> 00:20:36,690
for our other tenants as well so in

00:20:34,320 --> 00:20:39,720
terms of how effective it is this is a

00:20:36,690 --> 00:20:42,570
chart of when we first released this and

00:20:39,720 --> 00:20:45,750
it does really work which we're happy

00:20:42,570 --> 00:20:48,870
with so we've we've been consistently

00:20:45,750 --> 00:20:50,460
meeting our SLO since we released the

00:20:48,870 --> 00:20:53,190
noisy neighbor and for the most part

00:20:50,460 --> 00:20:55,080
customers have been you know actually

00:20:53,190 --> 00:20:57,789
glad to hear that they they have no idea

00:20:55,080 --> 00:20:59,859
that they're logging that much or

00:20:57,789 --> 00:21:02,979
you end up catching an error that they

00:20:59,859 --> 00:21:04,749
weren't even aware of you know it's it's

00:21:02,979 --> 00:21:06,609
it's been helpful for the customer

00:21:04,749 --> 00:21:08,649
helpful for the health of the platform

00:21:06,609 --> 00:21:11,139
in getting all of our logs to their

00:21:08,649 --> 00:21:13,869
destination it's improved our retention

00:21:11,139 --> 00:21:16,269
of the salong side of things and in

00:21:13,869 --> 00:21:18,249
terms of our support or on call during

00:21:16,269 --> 00:21:20,200
the day we're saving on the order of you

00:21:18,249 --> 00:21:22,029
know some days a couple of hours we'd

00:21:20,200 --> 00:21:24,940
have to spend on this that we don't have

00:21:22,029 --> 00:21:29,859
to spend now so it's it's been really

00:21:24,940 --> 00:21:36,359
helpful and that is the last slide so if

00:21:29,859 --> 00:21:39,639
anyone has any questions I think the

00:21:36,359 --> 00:21:41,979
example any patterns that you described

00:21:39,639 --> 00:21:44,950
that some of the developers were doing

00:21:41,979 --> 00:21:47,320
was really useful so maybe there's more

00:21:44,950 --> 00:21:49,779
comment thank you for that looks like my

00:21:47,320 --> 00:21:56,820
question coming into it it's very

00:21:49,779 --> 00:21:59,950
insightful yes was putting the

00:21:56,820 --> 00:22:01,450
application in debug mode by accident

00:21:59,950 --> 00:22:06,190
and that took up half our loading

00:22:01,450 --> 00:22:07,720
capacity so but I thought that foundry

00:22:06,190 --> 00:22:09,549
itself already had some kind of

00:22:07,720 --> 00:22:11,679
mechanism to prevent this kind of thing

00:22:09,549 --> 00:22:15,549
or is there am I mistaken on that point

00:22:11,679 --> 00:22:17,769
I mean into nine there is an emerging

00:22:15,549 --> 00:22:21,519
feature we already mentioned that

00:22:17,769 --> 00:22:23,349
somewhere yes this is yeah so looking

00:22:21,519 --> 00:22:25,779
forward to trying that out we're we're a

00:22:23,349 --> 00:22:27,009
couple of minor versions behind and

00:22:25,779 --> 00:22:28,960
since we've got it's such a big

00:22:27,009 --> 00:22:43,509
infrastructure it's gonna take us a

00:22:28,960 --> 00:22:46,570
while to get there and also from from

00:22:43,509 --> 00:22:50,109
t-mobile about you are also using Splunk

00:22:46,570 --> 00:22:52,059
for the reporting I always say that the

00:22:50,109 --> 00:22:56,979
the smoke test results were also going

00:22:52,059 --> 00:23:00,190
to Splunk I believe I mean wouldn't you

00:22:56,979 --> 00:23:01,960
lose that kind of thing if Splunk gets

00:23:00,190 --> 00:23:02,909
overwhelmed or spunk is not the problem

00:23:01,960 --> 00:23:07,059
here

00:23:02,909 --> 00:23:09,009
on occasion spunk has has been the

00:23:07,059 --> 00:23:11,349
problem when it comes to log loss in

00:23:09,009 --> 00:23:11,559
total that typically would have been in

00:23:11,349 --> 00:23:13,240
our

00:23:11,559 --> 00:23:15,970
locker gaiter infrastructure you have

00:23:13,240 --> 00:23:17,529
seen instances where the indexers and

00:23:15,970 --> 00:23:19,539
other components on Splunk we're

00:23:17,529 --> 00:23:22,499
becoming overwhelmed we would see delays

00:23:19,539 --> 00:23:26,950
there are are metrics or in a separate

00:23:22,499 --> 00:23:39,039
index so okay yeah a bit of a buffer

00:23:26,950 --> 00:23:41,080
there I'm not yet no it would definitely

00:23:39,039 --> 00:23:44,249
be interesting we are similar problems

00:23:41,080 --> 00:23:46,570
there and even though we're not large

00:23:44,249 --> 00:23:54,490
this kind of thing happen can happen to

00:23:46,570 --> 00:23:58,629
anyone I guess you said not yet so we

00:23:54,490 --> 00:24:01,360
smell all new projects coming up well I

00:23:58,629 --> 00:24:05,499
can imagine this is in many places very

00:24:01,360 --> 00:24:07,419
specific to t-mobile but if you have any

00:24:05,499 --> 00:24:09,159
inclination I know this community would

00:24:07,419 --> 00:24:10,049
appreciate having a look at definitely

00:24:09,159 --> 00:24:16,450
yeah

00:24:10,049 --> 00:24:21,429
these parts of it alright great thanks

00:24:16,450 --> 00:24:23,440
yeah the Cloud Foundry community repo is

00:24:21,429 --> 00:24:24,840
happy to host any and all sorts of

00:24:23,440 --> 00:24:31,320
things

00:24:24,840 --> 00:24:36,009
particularly in the extensions yeah okay

00:24:31,320 --> 00:24:39,629
squeeze next that would be me I give me

00:24:36,009 --> 00:24:39,629
just one moment and I'll share my screen

00:24:55,120 --> 00:25:02,330
all right can everyone see my screen

00:24:57,110 --> 00:25:03,860
okay yes excellent my name is Brendan

00:25:02,330 --> 00:25:05,750
Indra I'm also on the platform

00:25:03,860 --> 00:25:07,250
infrastructure engineering team and this

00:25:05,750 --> 00:25:12,830
presentation is about lifecycle

00:25:07,250 --> 00:25:14,660
automation and obviously the team oh so

00:25:12,830 --> 00:25:17,840
refresher on our scale has even already

00:25:14,660 --> 00:25:18,380
covered this 20 plus foundations 70,000

00:25:17,840 --> 00:25:20,170
containers

00:25:18,380 --> 00:25:22,820
some hundred million daily transactions

00:25:20,170 --> 00:25:25,070
3,000 plus applications and 100 very

00:25:22,820 --> 00:25:28,550
different dev teams fairly large-scale

00:25:25,070 --> 00:25:31,429
that we have to deal with in this matter

00:25:28,550 --> 00:25:34,070
so challenges before automation

00:25:31,429 --> 00:25:36,140
consistency this was a big one every

00:25:34,070 --> 00:25:37,700
time we brought a foundation online the

00:25:36,140 --> 00:25:39,350
parameter changes are the tiles that we

00:25:37,700 --> 00:25:41,630
used or the versions that we use would

00:25:39,350 --> 00:25:43,370
sometimes vary and the more foundations

00:25:41,630 --> 00:25:45,700
we brought online the greater that

00:25:43,370 --> 00:25:48,040
variance became made it very hard to

00:25:45,700 --> 00:25:50,300
predictably upgrade or update things

00:25:48,040 --> 00:25:52,370
also we didn't really have any change

00:25:50,300 --> 00:25:54,050
tracking mechanism so if an engineer

00:25:52,370 --> 00:25:55,880
made a little change to test something

00:25:54,050 --> 00:25:57,710
and saved it we weren't aware of that

00:25:55,880 --> 00:25:59,330
there wasn't necessarily everyone on the

00:25:57,710 --> 00:26:01,730
team wasn't as fun the same page or

00:25:59,330 --> 00:26:04,040
stuff was not noted or documentation

00:26:01,730 --> 00:26:06,830
wasn't updated as you can see this would

00:26:04,040 --> 00:26:07,940
also cause problems during upgrades to

00:26:06,830 --> 00:26:09,380
foundations should have the same

00:26:07,940 --> 00:26:10,790
settings but don't you go to do an

00:26:09,380 --> 00:26:13,940
upgrade you get pretty unpredictable

00:26:10,790 --> 00:26:16,010
results this resulted in lengthy and

00:26:13,940 --> 00:26:17,690
chaotic upgrades it would take a couple

00:26:16,010 --> 00:26:20,660
of weeks to get a single foundational

00:26:17,690 --> 00:26:23,320
point upgrade very very challenging in

00:26:20,660 --> 00:26:26,390
the early days before we had automation

00:26:23,320 --> 00:26:29,000
so our solution we use pivotal platform

00:26:26,390 --> 00:26:31,640
automation and concourse pipelines we've

00:26:29,000 --> 00:26:33,860
stored the foundation configurations and

00:26:31,640 --> 00:26:35,929
source control all of those

00:26:33,860 --> 00:26:39,860
documentation zarnow living documents as

00:26:35,929 --> 00:26:41,270
in whenever we go to use them we're

00:26:39,860 --> 00:26:43,370
updating the configuration files

00:26:41,270 --> 00:26:45,410
themselves and then staging the changes

00:26:43,370 --> 00:26:47,420
so that anything that is in those config

00:26:45,410 --> 00:26:48,830
is actively being used we don't really

00:26:47,420 --> 00:26:50,120
have to worry about them going out of

00:26:48,830 --> 00:26:51,830
date so anytime we need to look at

00:26:50,120 --> 00:26:53,030
settings or previous history anything

00:26:51,830 --> 00:26:54,830
along those lines we can look in source

00:26:53,030 --> 00:26:57,230
control and see what exactly took place

00:26:54,830 --> 00:26:59,360
there we also settled on no manual

00:26:57,230 --> 00:27:01,730
deployments ideally everything should be

00:26:59,360 --> 00:27:02,400
run through automation it's reduces

00:27:01,730 --> 00:27:04,170
errors really

00:27:02,400 --> 00:27:06,390
copy/paste problems those sort of things

00:27:04,170 --> 00:27:08,310
and it also provides a paper trail if

00:27:06,390 --> 00:27:10,590
you will for any changes that we make

00:27:08,310 --> 00:27:12,630
this also required a bit of team process

00:27:10,590 --> 00:27:14,520
people were used to doing things in ops

00:27:12,630 --> 00:27:16,080
manner directly even by hand this

00:27:14,520 --> 00:27:18,120
required a little bit of a mindset

00:27:16,080 --> 00:27:20,160
change to make that happen but through

00:27:18,120 --> 00:27:22,800
team agreements we managed to push

00:27:20,160 --> 00:27:25,170
through and do that this helped maintain

00:27:22,800 --> 00:27:27,660
consistency we also have a peer review

00:27:25,170 --> 00:27:29,520
process that requires at least a minimum

00:27:27,660 --> 00:27:31,320
of two engineers to look at any changes

00:27:29,520 --> 00:27:32,640
that are going to be made and approve

00:27:31,320 --> 00:27:36,060
them before we actually do them this

00:27:32,640 --> 00:27:37,740
helps you know reduce fatigue when

00:27:36,060 --> 00:27:40,170
you're making lots of changes helps

00:27:37,740 --> 00:27:42,090
reduce mistakes those sort of things we

00:27:40,170 --> 00:27:43,500
also use slack to make a little bar

00:27:42,090 --> 00:27:46,310
communications for these things and I'll

00:27:43,500 --> 00:27:49,530
show an example of that in a little bit

00:27:46,310 --> 00:27:50,910
this is our pipeline flow overview so

00:27:49,530 --> 00:27:52,350
I'll be going through this a little bit

00:27:50,910 --> 00:27:56,160
more in detail later but just to give

00:27:52,350 --> 00:27:59,580
you a rough idea each one of these is

00:27:56,160 --> 00:28:02,040
for an individual tile and the entire

00:27:59,580 --> 00:28:04,230
tile process from will check for pending

00:28:02,040 --> 00:28:07,290
changes will perform the initial backup

00:28:04,230 --> 00:28:09,420
will stage those changes will then

00:28:07,290 --> 00:28:10,980
perform another backup and then we will

00:28:09,420 --> 00:28:12,780
compare those backups and look at any

00:28:10,980 --> 00:28:15,210
drifts that are there this helps us

00:28:12,780 --> 00:28:16,620
eliminate potentially introducing any

00:28:15,210 --> 00:28:19,880
unwanted variables and also make sure

00:28:16,620 --> 00:28:19,880
that we're putting the right stuff in

00:28:20,330 --> 00:28:24,720
this is what we use for our slack

00:28:22,380 --> 00:28:27,180
notifications for our automation we

00:28:24,720 --> 00:28:29,190
cover stuff such as job starts succeeds

00:28:27,180 --> 00:28:31,140
failures job aborted pretty much any

00:28:29,190 --> 00:28:33,540
state that you can get in Concours we've

00:28:31,140 --> 00:28:36,420
got a cover here an example to the right

00:28:33,540 --> 00:28:38,460
these are jobs starting and then also

00:28:36,420 --> 00:28:39,990
their statuses as they continue or reach

00:28:38,460 --> 00:28:41,520
rigor as they go through the pipeline

00:28:39,990 --> 00:28:43,890
and at the very bottom there you can see

00:28:41,520 --> 00:28:45,300
that there's a job failure it's kind of

00:28:43,890 --> 00:28:46,770
different emoji different color to bring

00:28:45,300 --> 00:28:49,050
attention to us this is something that

00:28:46,770 --> 00:28:50,820
the entire team keeps an eye on in a

00:28:49,050 --> 00:28:52,350
regular basis so we know pretty much

00:28:50,820 --> 00:28:54,000
what's going on at any given foundation

00:28:52,350 --> 00:28:58,800
with the automation just by looking in

00:28:54,000 --> 00:29:02,280
this flat shell so the actual process we

00:28:58,800 --> 00:29:04,200
make our changes locally so we create a

00:29:02,280 --> 00:29:05,520
branch based off a master it allows for

00:29:04,200 --> 00:29:07,440
multiple engineers to work on individual

00:29:05,520 --> 00:29:08,970
foundations at the same time once

00:29:07,440 --> 00:29:10,260
shaders have been prepared they are

00:29:08,970 --> 00:29:12,930
checked into source control and pushed

00:29:10,260 --> 00:29:14,610
upstream by having configuration changes

00:29:12,930 --> 00:29:16,260
in source control we have a snapshot in

00:29:14,610 --> 00:29:18,360
time of who applied what

00:29:16,260 --> 00:29:19,770
Changez went into what foundation this

00:29:18,360 --> 00:29:22,500
allows for change accountability and

00:29:19,770 --> 00:29:27,570
tracking and if needed we were reflow

00:29:22,500 --> 00:29:29,910
the pipelines so this is our backup job

00:29:27,570 --> 00:29:32,760
we have select notifications as you can

00:29:29,910 --> 00:29:33,840
see in the top there if we pull down the

00:29:32,760 --> 00:29:35,190
images and tasks from platform

00:29:33,840 --> 00:29:38,160
automation we pull in our main

00:29:35,190 --> 00:29:39,240
repository we then run the tasks and

00:29:38,160 --> 00:29:41,420
then we send the follow-up /

00:29:39,240 --> 00:29:41,420
notifications

00:29:42,269 --> 00:29:47,339
this is the pending change job this will

00:29:44,879 --> 00:29:51,059
go through and check and see if there

00:29:47,339 --> 00:29:52,950
are ending changes basically the idea is

00:29:51,059 --> 00:29:54,059
if somebody makes a change in option and

00:29:52,950 --> 00:29:56,159
then they save it but they don't

00:29:54,059 --> 00:29:57,719
actually apply those changes or in the

00:29:56,159 --> 00:29:59,519
event that an apply changes failed and

00:29:57,719 --> 00:30:01,919
it got left in that state this will

00:29:59,519 --> 00:30:03,330
check each product in opps man to make

00:30:01,919 --> 00:30:05,700
sure there are no pending changes if

00:30:03,330 --> 00:30:07,979
there are it will hold in its process

00:30:05,700 --> 00:30:10,049
and then you have to go in and remediate

00:30:07,979 --> 00:30:12,059
it figure out if what what had changed

00:30:10,049 --> 00:30:15,049
or you know in the case revert off spin

00:30:12,059 --> 00:30:15,049
and then move forward

00:30:16,240 --> 00:30:20,450
this is a backup this will go through

00:30:19,100 --> 00:30:22,940
and back up all the parameters in the

00:30:20,450 --> 00:30:27,290
tile and then it will check those as a

00:30:22,940 --> 00:30:29,570
source control for future use this is

00:30:27,290 --> 00:30:32,000
where the tile staging happens configure

00:30:29,570 --> 00:30:33,770
a configuration files are interpolated

00:30:32,000 --> 00:30:35,600
and output for use later in the pipeline

00:30:33,770 --> 00:30:37,160
in short this process replaces any

00:30:35,600 --> 00:30:40,460
pipeline VARs and a tile set of

00:30:37,160 --> 00:30:42,320
configuration files the tile specified

00:30:40,460 --> 00:30:44,720
in the version config is examined and is

00:30:42,320 --> 00:30:47,720
its requirements are printed and console

00:30:44,720 --> 00:30:48,919
for future reference that's the the

00:30:47,720 --> 00:30:51,230
configure product portion of it it looks

00:30:48,919 --> 00:30:53,840
like I had just slightly there we also

00:30:51,230 --> 00:30:55,429
go through and upload the stem cell we

00:30:53,840 --> 00:31:01,340
upload the tile and then finally we

00:30:55,429 --> 00:31:02,630
configure the product so the tile is

00:31:01,340 --> 00:31:04,040
then configured with the insert plate

00:31:02,630 --> 00:31:04,700
configurations from one of the previous

00:31:04,040 --> 00:31:06,290
steps

00:31:04,700 --> 00:31:08,330
if all goes well we move forward onto

00:31:06,290 --> 00:31:09,049
the next step if there's a problem with

00:31:08,330 --> 00:31:11,179
the staging params

00:31:09,049 --> 00:31:12,860
it is reviewed updated in the configure

00:31:11,179 --> 00:31:14,510
in the config and then the job is kicked

00:31:12,860 --> 00:31:16,490
off again we'd like to feel forward in

00:31:14,510 --> 00:31:16,940
this sense it's very easy for this to

00:31:16,490 --> 00:31:20,419
happen

00:31:16,940 --> 00:31:22,790
a simple typo in the configuration file

00:31:20,419 --> 00:31:24,590
or a small change or we forgot to add a

00:31:22,790 --> 00:31:26,450
pram for a new tiled version something

00:31:24,590 --> 00:31:27,710
along those lines it doesn't stop us

00:31:26,450 --> 00:31:30,080
really we just figure out what the

00:31:27,710 --> 00:31:32,450
problem is fixes and configs restage it

00:31:30,080 --> 00:31:33,770
and off we go we don't have to rerun the

00:31:32,450 --> 00:31:34,730
backup or any of that stuff since it's

00:31:33,770 --> 00:31:38,929
already been done and it was in the

00:31:34,730 --> 00:31:40,730
clean state this is the post aged backup

00:31:38,929 --> 00:31:41,210
step pretty much the same as the first

00:31:40,730 --> 00:31:43,820
one

00:31:41,210 --> 00:31:45,340
when you take a backup of the tiles it

00:31:43,820 --> 00:31:47,600
gets whatever the current state is it

00:31:45,340 --> 00:31:48,950
and so whatever is currently stages

00:31:47,600 --> 00:31:51,970
those parameters that's what it's going

00:31:48,950 --> 00:31:51,970
to grab and it's going to back that up

00:31:52,010 --> 00:31:54,950
this is where we do the drift validation

00:31:53,630 --> 00:31:58,250
and this right here has been kind of the

00:31:54,950 --> 00:32:00,860
key to our success we go through and

00:31:58,250 --> 00:32:02,840
once this process is done we post links

00:32:00,860 --> 00:32:05,150
to our pending changes but I have a

00:32:02,840 --> 00:32:06,890
separate job that actually runs that you

00:32:05,150 --> 00:32:08,330
can see the pending changes so we know

00:32:06,890 --> 00:32:10,820
exactly what we're supposed to be

00:32:08,330 --> 00:32:12,080
looking at we post links to the drift

00:32:10,820 --> 00:32:13,490
which as you can see on the right-hand

00:32:12,080 --> 00:32:16,700
side here is a small example of what

00:32:13,490 --> 00:32:18,049
that looks like and then there's a

00:32:16,700 --> 00:32:19,940
couple of the jobs that we run before we

00:32:18,049 --> 00:32:22,250
kick anything off we push that in slack

00:32:19,940 --> 00:32:23,360
we get approvals on it we review them we

00:32:22,250 --> 00:32:24,710
take a look at the changes make sure

00:32:23,360 --> 00:32:27,320
this is what we want that anything

00:32:24,710 --> 00:32:31,309
didn't slip in cetera and then we move

00:32:27,320 --> 00:32:33,110
forward this is what applying changes

00:32:31,309 --> 00:32:35,240
actually looks like sure everyone's

00:32:33,110 --> 00:32:38,390
familiar with the Bosch or the sorry the

00:32:35,240 --> 00:32:41,210
option apply changes screen this is how

00:32:38,390 --> 00:32:43,309
we communicate stuff in slack once we

00:32:41,210 --> 00:32:45,410
kick off the changes we post what it's

00:32:43,309 --> 00:32:47,419
doing and then we update and update the

00:32:45,410 --> 00:32:49,220
emojis accordingly so that's we have a

00:32:47,419 --> 00:32:52,309
good idea of where we're at

00:32:49,220 --> 00:32:54,470
anybody on the team management anybody

00:32:52,309 --> 00:32:55,940
really that is part of our slack channel

00:32:54,470 --> 00:32:59,059
for this can take a look and see where

00:32:55,940 --> 00:33:04,490
we are with any deployment at any given

00:32:59,059 --> 00:33:06,410
time so as a reminder before automation

00:33:04,490 --> 00:33:08,900
we had consistency issues we had

00:33:06,410 --> 00:33:10,400
challenges tracking any sort of changes

00:33:08,900 --> 00:33:12,790
or updates or anything along those lines

00:33:10,400 --> 00:33:16,340
and it was lengthy and chaotic upgrades

00:33:12,790 --> 00:33:18,520
here's where we are today all of our

00:33:16,340 --> 00:33:20,870
tiles are backed by configuration files

00:33:18,520 --> 00:33:22,429
foundation consistency it was all over

00:33:20,870 --> 00:33:24,140
the board now it's at an all-time high

00:33:22,429 --> 00:33:26,169
from no change tracking to full change

00:33:24,140 --> 00:33:28,580
tracking verification and accountability

00:33:26,169 --> 00:33:30,679
moving from one foundation single point

00:33:28,580 --> 00:33:32,840
upgrade every two weeks to 3 plus

00:33:30,679 --> 00:33:34,730
multi-point foundations per week and

00:33:32,840 --> 00:33:36,350
what I mean by that we are currently in

00:33:34,730 --> 00:33:38,179
the process of upgrading from two for

00:33:36,350 --> 00:33:39,860
two to seven it had been running two to

00:33:38,179 --> 00:33:41,020
three foundation blitz per week from

00:33:39,860 --> 00:33:43,250
start to finish

00:33:41,020 --> 00:33:44,870
huge difference in the amount of time

00:33:43,250 --> 00:33:47,840
that it takes to actually get upgrades

00:33:44,870 --> 00:33:50,929
and not to mention this is pretty much

00:33:47,840 --> 00:33:54,470
due to lack of failures along the way

00:33:50,929 --> 00:33:55,760
and what have you

00:33:54,470 --> 00:33:59,120
I'd like to give a special thanks to the

00:33:55,760 --> 00:34:00,890
urge and JP two of my team members all

00:33:59,120 --> 00:34:02,570
of them worked very hard on the initial

00:34:00,890 --> 00:34:03,919
path to getting us rolling with

00:34:02,570 --> 00:34:05,809
automation upgrades of deployment and

00:34:03,919 --> 00:34:08,300
just want to say thank you guys I really

00:34:05,809 --> 00:34:09,530
appreciated it also I'd like to give an

00:34:08,300 --> 00:34:10,550
additional thanks to everyone on the

00:34:09,530 --> 00:34:12,139
platform team without your support

00:34:10,550 --> 00:34:13,280
testing and willingness to change we

00:34:12,139 --> 00:34:17,149
would still be doing things the hard way

00:34:13,280 --> 00:34:25,010
and I'm very happy that we are dots with

00:34:17,149 --> 00:34:28,820
that odd questions well I have a few if

00:34:25,010 --> 00:34:31,250
I may actually we're also you looking

00:34:28,820 --> 00:34:33,590
into the platform automation tooling

00:34:31,250 --> 00:34:39,470
which was like follow up on the PCF

00:34:33,590 --> 00:34:42,050
pipelines if I'm correct as I understand

00:34:39,470 --> 00:34:43,609
that you basically use it to upgrade the

00:34:42,050 --> 00:34:46,700
platform or make changes in

00:34:43,609 --> 00:34:49,790
configuration you do not use it for like

00:34:46,700 --> 00:34:51,740
initial deployment I assume the platform

00:34:49,790 --> 00:34:56,740
was already existing before you started

00:34:51,740 --> 00:35:00,020
to automate right well so because yeah

00:34:56,740 --> 00:35:01,400
to answer your question sorry we did

00:35:00,020 --> 00:35:03,050
have we did have foundations in place

00:35:01,400 --> 00:35:06,170
before we started doing the automation

00:35:03,050 --> 00:35:09,140
that is correct however we roll out any

00:35:06,170 --> 00:35:11,930
new foundations with using platform

00:35:09,140 --> 00:35:13,250
automation oh I this presentation was

00:35:11,930 --> 00:35:14,599
summed up there is actually a lot of

00:35:13,250 --> 00:35:17,869
other jobs that I have running on

00:35:14,599 --> 00:35:19,310
platform automation specifically that I

00:35:17,869 --> 00:35:21,170
wasn't really able to cover here but

00:35:19,310 --> 00:35:22,940
yeah we are using it and also those

00:35:21,170 --> 00:35:24,349
foundations that we initially brought

00:35:22,940 --> 00:35:26,839
online though they had all the different

00:35:24,349 --> 00:35:29,510
changes those got folded in to

00:35:26,839 --> 00:35:30,290
automation and now as we've gone through

00:35:29,510 --> 00:35:32,839
several upgrades they've all

00:35:30,290 --> 00:35:35,330
consistently updated and become very

00:35:32,839 --> 00:35:37,940
similar as we as we move forward with

00:35:35,330 --> 00:35:39,859
that so the there were only real

00:35:37,940 --> 00:35:43,010
difference for installing a new

00:35:39,859 --> 00:35:45,500
foundation versus upgrading foundation

00:35:43,010 --> 00:35:46,820
is one job we have an install option

00:35:45,500 --> 00:35:50,810
instead of an upgrade option and it

00:35:46,820 --> 00:35:52,369
operates very similar ok cool because I

00:35:50,810 --> 00:35:54,290
have in this formation age we were

00:35:52,369 --> 00:35:57,619
working on this for like most of the

00:35:54,290 --> 00:36:01,460
time they spent on creating the initial

00:35:57,619 --> 00:36:03,109
setup basically so the way we handle it

00:36:01,460 --> 00:36:04,640
is that we run the install spend job

00:36:03,109 --> 00:36:06,080
which as I said it's very similar

00:36:04,640 --> 00:36:08,510
there's a couple of things you have to

00:36:06,080 --> 00:36:10,340
setup first basically you need

00:36:08,510 --> 00:36:11,960
a state file for tracking that sort of

00:36:10,340 --> 00:36:13,880
thing what version you want to run got

00:36:11,960 --> 00:36:16,520
to make sure your tile for that and all

00:36:13,880 --> 00:36:18,440
that's in as3 repository we're with the

00:36:16,520 --> 00:36:20,240
rest of our tiles once you run that you

00:36:18,440 --> 00:36:22,430
install it and then from there we treat

00:36:20,240 --> 00:36:23,810
it just like a regular foundation in the

00:36:22,430 --> 00:36:25,070
upgrade process we don't put the tiles

00:36:23,810 --> 00:36:26,840
on there ahead of time or any of that

00:36:25,070 --> 00:36:29,150
thing we treat it just like we were

00:36:26,840 --> 00:36:30,740
running an upgrade on it and through the

00:36:29,150 --> 00:36:33,140
nature of how this is done it's just

00:36:30,740 --> 00:36:35,030
going to install those instead of does

00:36:33,140 --> 00:36:38,540
that make sense yeah it makes sense

00:36:35,030 --> 00:36:40,670
thank you very much we also we also go

00:36:38,540 --> 00:36:42,290
through and we have a template for each

00:36:40,670 --> 00:36:43,850
foundation that we have because each

00:36:42,290 --> 00:36:46,100
foundation does have its own set of

00:36:43,850 --> 00:36:47,570
template and config files we have a we

00:36:46,100 --> 00:36:49,160
have a template for that we just copy

00:36:47,570 --> 00:36:50,390
that over and when we update the values

00:36:49,160 --> 00:36:51,800
to reflect what we want for that

00:36:50,390 --> 00:36:54,050
foundation and it's gonna be all unique

00:36:51,800 --> 00:36:58,150
things like IP addresses you know main

00:36:54,050 --> 00:36:58,150
source information I can imagine yeah

00:37:00,070 --> 00:37:09,110
anything else thank you all right thank

00:37:06,920 --> 00:37:12,710
you very much thanks so much Brandon

00:37:09,110 --> 00:37:16,730
it's a lot to absorb I'm wondering if we

00:37:12,710 --> 00:37:19,160
can see a copy of your slides or if

00:37:16,730 --> 00:37:22,370
you've blogged this anywhere that we can

00:37:19,160 --> 00:37:25,100
reference yeah I'm happy to share copy

00:37:22,370 --> 00:37:26,900
of the slides and so the other thing too

00:37:25,100 --> 00:37:28,220
there there is a lot to cover and I'm if

00:37:26,900 --> 00:37:29,470
anyone has any questions feel free to

00:37:28,220 --> 00:37:32,180
reach out to me I'm more than happy to

00:37:29,470 --> 00:37:33,680
assist where I can it's it's a bit of an

00:37:32,180 --> 00:37:36,800
undertaking if you are not familiar

00:37:33,680 --> 00:37:40,280
haven't worked on it before so I began

00:37:36,800 --> 00:37:43,280
working I know I was trying to map what

00:37:40,280 --> 00:37:48,020
what is applicable here from

00:37:43,280 --> 00:37:50,510
PCF tiles to just country application

00:37:48,020 --> 00:37:53,060
runtime open source or another distro a

00:37:50,510 --> 00:37:54,800
lot of really good best practices there

00:37:53,060 --> 00:37:57,620
that you've implemented I just need to

00:37:54,800 --> 00:38:00,440
pick it apart and actually read read

00:37:57,620 --> 00:38:02,240
some stuff and think about it I think it

00:38:00,440 --> 00:38:03,830
can be more general because borsch is

00:38:02,240 --> 00:38:07,310
doing all the heavy lifting here so

00:38:03,830 --> 00:38:10,160
exactly just manage is just at all it is

00:38:07,310 --> 00:38:18,380
just like a yeoman generator as far as I

00:38:10,160 --> 00:38:20,300
see it it's taking parameters on llamo

00:38:18,380 --> 00:38:22,250
father Spitzer something bushes in the

00:38:20,300 --> 00:38:25,280
stands

00:38:22,250 --> 00:38:27,920
one there is a little bit community at

00:38:25,280 --> 00:38:29,420
the processing gamble yeah it's lots of

00:38:27,920 --> 00:38:30,530
um oh and there's a there's a couple of

00:38:29,420 --> 00:38:32,090
things under the hood that's going on

00:38:30,530 --> 00:38:33,470
you'll notice there's three slides

00:38:32,090 --> 00:38:38,000
towards the end that talk about it we're

00:38:33,470 --> 00:38:39,950
using a hierarchy of global contexts and

00:38:38,000 --> 00:38:41,900
local changes basically I've stripped it

00:38:39,950 --> 00:38:43,660
out so that all the global changes that

00:38:41,900 --> 00:38:46,220
are ubiquitous across all foundations

00:38:43,660 --> 00:38:47,690
exist at the global level and then

00:38:46,220 --> 00:38:49,490
you've got a context which are network

00:38:47,690 --> 00:38:51,080
specific certain networks have certain

00:38:49,490 --> 00:38:52,730
requirements and then those fall there

00:38:51,080 --> 00:38:55,400
and then we have our local files free

00:38:52,730 --> 00:38:57,710
amyl that are all the stuff like IP data

00:38:55,400 --> 00:38:59,090
source that sort of thing basically when

00:38:57,710 --> 00:39:00,350
we do our upgrades once everything is

00:38:59,090 --> 00:39:04,130
set in place you don't really don't

00:39:00,350 --> 00:39:05,660
touch the global or context files during

00:39:04,130 --> 00:39:07,460
the upgrade you'll do it once for the

00:39:05,660 --> 00:39:09,680
initial upgrade and then you just update

00:39:07,460 --> 00:39:11,150
the little local files something about

00:39:09,680 --> 00:39:12,650
effective there's around I don't know

00:39:11,150 --> 00:39:15,530
four hundred lines in your standard

00:39:12,650 --> 00:39:17,510
patch config file this shortens it down

00:39:15,530 --> 00:39:20,350
to 50 lines at most that you're looking

00:39:17,510 --> 00:39:22,910
at at any given time per foundation so

00:39:20,350 --> 00:39:24,050
yeah I have notes on that I'll send that

00:39:22,910 --> 00:39:25,340
along with a slide if anyone has

00:39:24,050 --> 00:39:30,290
questions about that feel free to reject

00:39:25,340 --> 00:39:32,420
me oh sorry go ahead doc sorry

00:39:30,290 --> 00:39:33,890
especially that multi foundation thing I

00:39:32,420 --> 00:39:37,070
mean interested in as far as I remember

00:39:33,890 --> 00:39:41,300
the PC automation tooling was basically

00:39:37,070 --> 00:39:43,100
designed to upgrade a single site so I'm

00:39:41,300 --> 00:39:44,840
also wondering how you tackle this the

00:39:43,100 --> 00:39:47,480
relations a star like a dependency

00:39:44,840 --> 00:39:50,450
between sites or is that each site is

00:39:47,480 --> 00:39:52,430
updated independently well so the way

00:39:50,450 --> 00:39:55,760
we're kind of handling that is is

00:39:52,430 --> 00:39:58,760
twofold so I'm the concourse side of

00:39:55,760 --> 00:40:00,740
things we have a we have regional

00:39:58,760 --> 00:40:02,570
deployments for all of our major regions

00:40:00,740 --> 00:40:05,600
and inside of those regions we have

00:40:02,570 --> 00:40:08,240
context specific foundation appointments

00:40:05,600 --> 00:40:09,950
so we'll have will basically use the

00:40:08,240 --> 00:40:12,890
teams in concourse to manage which

00:40:09,950 --> 00:40:14,900
pipeline is runs for what so we have a

00:40:12,890 --> 00:40:16,910
our staging Foundation for example a CT

00:40:14,900 --> 00:40:18,260
stage and TD stage of two just for

00:40:16,910 --> 00:40:20,030
examples those will each have their own

00:40:18,260 --> 00:40:21,500
semen inside of them those teams will

00:40:20,030 --> 00:40:24,560
have pipeline specifics of those

00:40:21,500 --> 00:40:26,270
foundations we also have in repress

00:40:24,560 --> 00:40:27,890
structure we have it split out in such a

00:40:26,270 --> 00:40:31,430
way that the out all the configuration

00:40:27,890 --> 00:40:33,380
files are split in there and through the

00:40:31,430 --> 00:40:35,780
use of parameter files when flying the

00:40:33,380 --> 00:40:37,010
pipeline specifies all the rel

00:40:35,780 --> 00:40:41,230
information for that individual

00:40:37,010 --> 00:40:41,230
foundation so

00:40:41,270 --> 00:40:45,250
we basically sidestep that whole problem

00:40:45,280 --> 00:40:53,600
okay thank you for the feedback it's

00:40:47,869 --> 00:40:58,300
very interesting anyone else

00:40:53,600 --> 00:40:58,300
any other questions for Brandon

00:40:59,440 --> 00:41:03,279
the t-mobile team for for proposing

00:41:01,599 --> 00:41:05,680
these great talks and for delivering

00:41:03,279 --> 00:41:08,349
them and I'll figure out with you guys

00:41:05,680 --> 00:41:11,319
offline what the best way is to share

00:41:08,349 --> 00:41:13,450
this with the wider organization will

00:41:11,319 --> 00:41:15,299
they get from see if that posters up or

00:41:13,450 --> 00:41:17,470
something I'm not sure where we keep

00:41:15,299 --> 00:41:21,490
things like this but maybe we've got to

00:41:17,470 --> 00:41:24,700
create a place where we can share past

00:41:21,490 --> 00:41:27,970
and present cab presentations but thank

00:41:24,700 --> 00:41:29,109
you very much for that there's a little

00:41:27,970 --> 00:41:30,519
bit of time left over for open

00:41:29,109 --> 00:41:32,829
discussion items if we want to do that

00:41:30,519 --> 00:41:36,809
or we can take back a bit of our day and

00:41:32,829 --> 00:41:36,809
and do other things

00:41:42,420 --> 00:41:46,710
okay well

00:41:44,580 --> 00:41:50,730
everyone very much for joining joining

00:41:46,710 --> 00:41:52,670
us today thanks to everyone Eric for

00:41:50,730 --> 00:41:54,900
presenting the apps run time updates

00:41:52,670 --> 00:41:57,270
thanks chip for the foundation updates

00:41:54,900 --> 00:41:59,550
and of course thank you very much to the

00:41:57,270 --> 00:42:01,860
t-mobile team we're presenting today and

00:41:59,550 --> 00:42:07,800
making this a really really information

00:42:01,860 --> 00:42:10,460
packed cab meeting thanks everyone thank

00:42:07,800 --> 00:42:10,460

YouTube URL: https://www.youtube.com/watch?v=S0kKCYn3JWc


