Title: Diego Runtime Overview - Eric Malm, Pivotal
Publication date: 2016-06-06
Playlist: Containers 101
Description: 
	Have you heard about Diego for a while now, but wonder what it means to you as an application developer or operator? Have you experienced the magic of pushing an application to Cloud Foundry and want to know what makes it all possible behind the scenes? 

Over the past year, the Diego container runtime has made tremendous strides, reaching feature parity with the existing DEA runtime, making its deployments easily upgradable, and scaling to manage larger and larger container workloads. In this talk, the project lead for the Diego team will review the components of the Diego system, how they interact with each other, and how they integrate with existing Cloud Foundry deployments to allow a straightforward transition to the new runtime. We will also discuss how the Diego runtime enables many powerful new platform features, such as CF Tasks, TCP routing, and SSH access to containers, and preview other upcoming features that CF teams are actively working on and exploring today. 

After watching this talk youâ€™ll be ready to upgrade to Diego with confidence and take advantage of the many improvements it brings to Cloud Foundry.

Eric Malm
Pivotal Software
Eric works at Pivotal Software as the Product Manager for the CF Runtime Diego team, and prior to that was a software engineer on the Diego team and on the CF Runtime team. He also holds a Ph.D. in Mathematics from Stanford University.
Captions: 
	00:00:00,000 --> 00:00:04,290
all right good afternoon everyone thanks

00:00:03,179 --> 00:00:07,259
for coming I hope you all had a great

00:00:04,290 --> 00:00:09,750
lunch break my name is Eric Maugham from

00:00:07,259 --> 00:00:11,670
pivitol and I'm the product manager for

00:00:09,750 --> 00:00:14,250
the CF front I'm Diego team and today

00:00:11,670 --> 00:00:17,789
I'd like to give you an overview of the

00:00:14,250 --> 00:00:20,550
new Diego runtime system so what is

00:00:17,789 --> 00:00:23,340
Diego Diego is cloud foundries new

00:00:20,550 --> 00:00:24,689
container runtime and as you may have

00:00:23,340 --> 00:00:28,289
noticed if you've looked at it it

00:00:24,689 --> 00:00:32,790
introduces a few new components well

00:00:28,289 --> 00:00:35,940
maybe a few more than that actually

00:00:32,790 --> 00:00:37,200
there's a lot more components so I'd

00:00:35,940 --> 00:00:40,710
like to give you some sense of how we

00:00:37,200 --> 00:00:42,300
can make sense of this entire new system

00:00:40,710 --> 00:00:45,360
that's going to live at the heart of

00:00:42,300 --> 00:00:48,360
cloud foundry so even though there are a

00:00:45,360 --> 00:00:51,210
lot more components they each have very

00:00:48,360 --> 00:00:54,410
specific responsibilities and we can

00:00:51,210 --> 00:00:58,280
organize them into coherent subsystems

00:00:54,410 --> 00:01:01,500
so what we think of as the core of Diego

00:00:58,280 --> 00:01:03,510
actually is composed of a few different

00:01:01,500 --> 00:01:07,020
components collaborating together and

00:01:03,510 --> 00:01:08,220
they have a few dependencies and then a

00:01:07,020 --> 00:01:10,710
lot of the other components that we've

00:01:08,220 --> 00:01:12,780
introduced as part of Diego to integrate

00:01:10,710 --> 00:01:14,760
it into cloud foundry have been to

00:01:12,780 --> 00:01:17,460
translate the rest of the system to

00:01:14,760 --> 00:01:18,960
operate with this new core so for

00:01:17,460 --> 00:01:21,509
example there's a whole set of services

00:01:18,960 --> 00:01:24,210
that collaborate with this core of Diego

00:01:21,509 --> 00:01:26,490
and cloud controller to make sure that

00:01:24,210 --> 00:01:32,280
CF can run its workloads in this new

00:01:26,490 --> 00:01:34,049
system and as part of that there are a

00:01:32,280 --> 00:01:35,820
number of modules that know about how to

00:01:34,049 --> 00:01:38,369
run very specific types of applications

00:01:35,820 --> 00:01:40,740
so build packs but also dr. images and

00:01:38,369 --> 00:01:43,100
windows apps and integrate with that cc

00:01:40,740 --> 00:01:45,060
bridge to help us run that workload and

00:01:43,100 --> 00:01:48,720
then some of the other components that

00:01:45,060 --> 00:01:50,790
we've introduced have been to interact

00:01:48,720 --> 00:01:54,390
with and to extend the notions of

00:01:50,790 --> 00:01:55,710
routing on the platform so let me give

00:01:54,390 --> 00:01:58,229
you a visual depiction of how these

00:01:55,710 --> 00:02:01,640
subsystems are connected together in an

00:01:58,229 --> 00:02:01,640
existing cloud foundry deployment

00:02:06,100 --> 00:02:10,700
so you can see on the edge here we've

00:02:08,840 --> 00:02:12,200
incorporated all of these subsystems

00:02:10,700 --> 00:02:13,700
with the existing ones that we're used

00:02:12,200 --> 00:02:17,200
to from Cloud Foundry in this case the

00:02:13,700 --> 00:02:17,200
cloud controller and the go routers I

00:02:17,379 --> 00:02:20,959
mentioned the Diego core it's

00:02:19,220 --> 00:02:23,390
responsibilities are to take

00:02:20,959 --> 00:02:25,459
containerized workloads and determine

00:02:23,390 --> 00:02:27,349
determine how best to place those across

00:02:25,459 --> 00:02:30,049
the distributed system and how to manage

00:02:27,349 --> 00:02:33,349
their lifecycle and to do that

00:02:30,049 --> 00:02:35,510
effectively they need a consistent

00:02:33,349 --> 00:02:39,080
distributed store to collaborate their

00:02:35,510 --> 00:02:41,239
components across but the core of Diego

00:02:39,080 --> 00:02:43,700
itself is not responsible for creating

00:02:41,239 --> 00:02:47,000
these containers instead it delegates

00:02:43,700 --> 00:02:49,400
that to its garden dependency so that is

00:02:47,000 --> 00:02:51,620
the component that knows how to actually

00:02:49,400 --> 00:02:55,610
create a container and how to execute

00:02:51,620 --> 00:02:57,860
inside of it so that started off with

00:02:55,610 --> 00:03:00,560
Garden Linux but it's also a point of

00:02:57,860 --> 00:03:02,329
extension in the system so that we can

00:03:00,560 --> 00:03:05,870
support a notion of containerization for

00:03:02,329 --> 00:03:07,250
Windows applications as well and then

00:03:05,870 --> 00:03:10,670
last of these dependencies that the

00:03:07,250 --> 00:03:12,440
Diego core has is on console to allow

00:03:10,670 --> 00:03:16,970
its own components to coordinate in

00:03:12,440 --> 00:03:19,220
sophisticated ways so all of this

00:03:16,970 --> 00:03:20,959
subsystem is operable entirely

00:03:19,220 --> 00:03:23,540
independent of the rest of Cloud Foundry

00:03:20,959 --> 00:03:26,090
and then these other layers help us

00:03:23,540 --> 00:03:29,299
integrate it into the existing Cloud

00:03:26,090 --> 00:03:30,980
Foundry deployments so I mentioned the

00:03:29,299 --> 00:03:33,709
CC bridge and these app lifecycle

00:03:30,980 --> 00:03:36,980
extension points they cooperate with

00:03:33,709 --> 00:03:38,870
Cloud Controller to allow it to run its

00:03:36,980 --> 00:03:42,670
existing notions of build pack based

00:03:38,870 --> 00:03:45,650
applications Ruby node Java but also to

00:03:42,670 --> 00:03:47,630
extend that to support workloads based

00:03:45,650 --> 00:03:52,069
on docker images and based on running

00:03:47,630 --> 00:03:54,500
net apps on Windows and then likewise

00:03:52,069 --> 00:03:56,599
the router emitter component we think of

00:03:54,500 --> 00:03:59,239
as a bridge to the existing go router

00:03:56,599 --> 00:04:00,590
tier to allow it to receive the same

00:03:59,239 --> 00:04:02,930
kinds of route registration messages

00:04:00,590 --> 00:04:05,329
that it is expected from the DA's and

00:04:02,930 --> 00:04:07,209
the previous architecture while still

00:04:05,329 --> 00:04:09,590
operating with this new back-end and

00:04:07,209 --> 00:04:11,329
then likewise this has given us the

00:04:09,590 --> 00:04:13,220
opportunity to introduce entirely new

00:04:11,329 --> 00:04:15,650
routing layers to Cloud Foundry

00:04:13,220 --> 00:04:17,479
one of those is SSH access to containers

00:04:15,650 --> 00:04:19,329
which we just view as an ssh routing

00:04:17,479 --> 00:04:21,550
system

00:04:19,329 --> 00:04:24,650
okay so I'd like to give you a picture

00:04:21,550 --> 00:04:26,180
in more detail of that Diego core and

00:04:24,650 --> 00:04:30,890
how we arrange it in a typical

00:04:26,180 --> 00:04:32,900
deployment scenario so in that core

00:04:30,890 --> 00:04:35,270
there's really three types of VMs

00:04:32,900 --> 00:04:37,400
and down to the bottom we have a large

00:04:35,270 --> 00:04:39,770
group of VMs that we call the cells in

00:04:37,400 --> 00:04:41,000
the Diego deployment and these are where

00:04:39,770 --> 00:04:42,800
we're actually going to be creating

00:04:41,000 --> 00:04:46,850
containers and executing workloads in

00:04:42,800 --> 00:04:49,610
them so on each cell there's a local

00:04:46,850 --> 00:04:51,530
garden process running but that can be

00:04:49,610 --> 00:04:55,970
garden Linux a green garden windows and

00:04:51,530 --> 00:04:57,980
we're all looking forward to the coming

00:04:55,970 --> 00:05:00,740
garden run see as the next generation of

00:04:57,980 --> 00:05:02,660
what garden Linux means but garden

00:05:00,740 --> 00:05:04,580
itself although it knows how to create

00:05:02,660 --> 00:05:04,880
containers and how to run processes in

00:05:04,580 --> 00:05:06,800
them

00:05:04,880 --> 00:05:09,710
it doesn't participate in this broader

00:05:06,800 --> 00:05:11,330
distributed system and so the other

00:05:09,710 --> 00:05:13,910
Diego component that lives on the cells

00:05:11,330 --> 00:05:15,980
is something that we call the rep and

00:05:13,910 --> 00:05:17,390
this is responsible for broadcasting the

00:05:15,980 --> 00:05:21,590
presence of that cell to the rest of the

00:05:17,390 --> 00:05:23,360
system to receive work from that system

00:05:21,590 --> 00:05:25,550
and to report it back to the rest of the

00:05:23,360 --> 00:05:29,419
system as well as to drive garden to

00:05:25,550 --> 00:05:32,419
actually make the containers okay so

00:05:29,419 --> 00:05:34,580
each one of those cells is ready to

00:05:32,419 --> 00:05:36,229
receive some work but we also need to

00:05:34,580 --> 00:05:39,470
know how to specify that so the system

00:05:36,229 --> 00:05:41,570
itself so that's where this top VM the

00:05:39,470 --> 00:05:44,150
database comes in and that hosts another

00:05:41,570 --> 00:05:46,760
Diego service that we call the BBS or

00:05:44,150 --> 00:05:48,979
the bulletin boards store so this is

00:05:46,760 --> 00:05:51,740
what presents the public API for clients

00:05:48,979 --> 00:05:53,419
of the Diego core to specify the

00:05:51,740 --> 00:05:56,120
workloads that they want to run in the

00:05:53,419 --> 00:05:57,740
cluster and the BBS also knows about

00:05:56,120 --> 00:06:00,020
different types of lifecycle policies

00:05:57,740 --> 00:06:04,130
for those workloads and enforces them

00:06:00,020 --> 00:06:06,140
and that we've also co-located the VBS

00:06:04,130 --> 00:06:10,940
on these database nodes with the sed

00:06:06,140 --> 00:06:12,979
that uses for persistence okay so even

00:06:10,940 --> 00:06:14,810
though the BBS knows a lot about the

00:06:12,979 --> 00:06:17,510
lifecycle policy for these individual

00:06:14,810 --> 00:06:19,250
units of work it in and of itself is not

00:06:17,510 --> 00:06:21,530
responsible for making the placement

00:06:19,250 --> 00:06:23,360
decisions instead it delegates that

00:06:21,530 --> 00:06:26,210
responsibility to a service that we call

00:06:23,360 --> 00:06:27,650
the auctioneer so this is responsible

00:06:26,210 --> 00:06:28,880
for communicating with all those cells

00:06:27,650 --> 00:06:31,610
that are registered in the deployment

00:06:28,880 --> 00:06:33,439
and making optimal placement decisions

00:06:31,610 --> 00:06:38,330
telling them what work to do in what

00:06:33,439 --> 00:06:40,250
order and then last but not least we

00:06:38,330 --> 00:06:43,069
have the converter and this is

00:06:40,250 --> 00:06:44,810
responsible for periodically analyzing

00:06:43,069 --> 00:06:46,879
the global state of the work that's

00:06:44,810 --> 00:06:48,710
running in the diego core and taking any

00:06:46,879 --> 00:06:50,629
corrective actions if there are missing

00:06:48,710 --> 00:06:53,060
instances then it will recreate them

00:06:50,629 --> 00:06:53,479
it'll tell the BBS you're missing some

00:06:53,060 --> 00:06:55,400
stuff

00:06:53,479 --> 00:06:56,990
please make this if there are extra

00:06:55,400 --> 00:07:00,469
instances it'll tell the system to stop

00:06:56,990 --> 00:07:01,370
them and if there is work that is ready

00:07:00,469 --> 00:07:02,930
to be rescheduled

00:07:01,370 --> 00:07:06,860
it'll sweep through until the system to

00:07:02,930 --> 00:07:08,180
do that as well okay so I mentioned that

00:07:06,860 --> 00:07:10,699
the VBS is in charge of enforcing

00:07:08,180 --> 00:07:12,530
different types of workloads and let me

00:07:10,699 --> 00:07:16,580
tell you about the two that we support

00:07:12,530 --> 00:07:18,770
in Diego so the first of those is the

00:07:16,580 --> 00:07:21,229
notion of a long-running process or an

00:07:18,770 --> 00:07:23,419
lrp for short so these are the

00:07:21,229 --> 00:07:27,080
characteristics that we desire for these

00:07:23,419 --> 00:07:29,300
kinds of processes so the first aspect

00:07:27,080 --> 00:07:31,879
is that they should be continually

00:07:29,300 --> 00:07:34,580
running we expect this workload always

00:07:31,879 --> 00:07:39,020
to be running when it's desired so if it

00:07:34,580 --> 00:07:41,210
stops even with a successful exit

00:07:39,020 --> 00:07:44,479
condition we consider it to be failed

00:07:41,210 --> 00:07:45,979
and we reschedule it to restart it we've

00:07:44,479 --> 00:07:48,289
also structured these LR PS to be

00:07:45,979 --> 00:07:50,150
scalable so you have one specification

00:07:48,289 --> 00:07:51,620
of the unit of work you want to run but

00:07:50,150 --> 00:07:55,759
you can scale it up to an arbitrary

00:07:51,620 --> 00:07:59,449
number of instances and then in terms of

00:07:55,759 --> 00:08:02,120
its operation Diego wants to make these

00:07:59,449 --> 00:08:03,919
instances as available as possible so it

00:08:02,120 --> 00:08:06,080
may temporarily run more than the

00:08:03,919 --> 00:08:08,360
desired number of instances in order to

00:08:06,080 --> 00:08:12,589
have a seamless transition say during an

00:08:08,360 --> 00:08:14,389
update to the system itself in contrast

00:08:12,589 --> 00:08:15,979
to that the other type of work that we

00:08:14,389 --> 00:08:18,289
scheduled on Diego is the notion of a

00:08:15,979 --> 00:08:20,409
one-off task and this is almost the

00:08:18,289 --> 00:08:23,150
exact opposite of a long-running process

00:08:20,409 --> 00:08:26,479
these are expected to terminate at some

00:08:23,150 --> 00:08:28,669
point and the data that we track around

00:08:26,479 --> 00:08:30,620
those tasks will detect whether or not

00:08:28,669 --> 00:08:32,539
they've terminated successfully or not

00:08:30,620 --> 00:08:34,459
and if they've terminated successfully

00:08:32,539 --> 00:08:36,010
can convey a small results back to the

00:08:34,459 --> 00:08:38,479
client

00:08:36,010 --> 00:08:41,990
these are inherently non-scalable we

00:08:38,479 --> 00:08:43,250
intend them to be one offs and we're

00:08:41,990 --> 00:08:45,140
also more careful about trying to

00:08:43,250 --> 00:08:47,090
schedule them consistently

00:08:45,140 --> 00:08:48,560
to make sure that we're only running

00:08:47,090 --> 00:08:50,060
them once so there are more

00:08:48,560 --> 00:08:52,240
opportunities for the system to say I

00:08:50,060 --> 00:08:55,430
can't be sure that I scheduled this

00:08:52,240 --> 00:08:57,020
exactly once so maybe I'll defer to

00:08:55,430 --> 00:09:00,740
scheduling it zero times and let the

00:08:57,020 --> 00:09:02,750
client deal with that so these two types

00:09:00,740 --> 00:09:04,850
of work are abstracted from the types of

00:09:02,750 --> 00:09:07,220
work that we know that we run already in

00:09:04,850 --> 00:09:09,320
Cloud Foundry so these long running

00:09:07,220 --> 00:09:11,210
processes are an abstraction of the

00:09:09,320 --> 00:09:13,880
notion of the app process that Cloud

00:09:11,210 --> 00:09:16,100
Controller wants to run to run a build

00:09:13,880 --> 00:09:19,430
pack app but also now a doctor after a

00:09:16,100 --> 00:09:21,440
Windows app and these one-off tasks are

00:09:19,430 --> 00:09:23,600
a representation of the kinds of staging

00:09:21,440 --> 00:09:26,000
tasks the Cloud Controller often wants

00:09:23,600 --> 00:09:28,280
to run as a compilation or vendor in

00:09:26,000 --> 00:09:30,230
step before running a build pack app or

00:09:28,280 --> 00:09:34,460
to extract some metadata from a docker

00:09:30,230 --> 00:09:36,470
application but as these concepts have

00:09:34,460 --> 00:09:39,260
solidified in the Diego runtime we found

00:09:36,470 --> 00:09:41,840
other use cases for them as well and in

00:09:39,260 --> 00:09:43,550
particular these one-off tasks are very

00:09:41,840 --> 00:09:47,270
appropriate for running one-off tasks in

00:09:43,550 --> 00:09:49,840
the context of your own application so

00:09:47,270 --> 00:09:52,190
this is a new type of workload that the

00:09:49,840 --> 00:09:59,030
v3 version of the Cloud Controller API

00:09:52,190 --> 00:10:01,190
is exposing ok so let me show you in a

00:09:59,030 --> 00:10:03,920
little more detail some of the contents

00:10:01,190 --> 00:10:05,810
of the long-running processes that arise

00:10:03,920 --> 00:10:10,370
as we're trying to run CF applications

00:10:05,810 --> 00:10:12,230
on Diego so let's take the example of a

00:10:10,370 --> 00:10:16,040
build pack app let's say it's a rails

00:10:12,230 --> 00:10:17,570
app so one of the things that we need to

00:10:16,040 --> 00:10:21,410
know to run this type of work as a

00:10:17,570 --> 00:10:24,130
container on the backend is what are the

00:10:21,410 --> 00:10:27,500
file system contents of this application

00:10:24,130 --> 00:10:30,380
so if you're a built pack app then all

00:10:27,500 --> 00:10:32,210
of them are based on the same common

00:10:30,380 --> 00:10:33,680
consistent root filesystem this is

00:10:32,210 --> 00:10:36,920
something that cloud before refers to as

00:10:33,680 --> 00:10:40,070
a stack so it knows it's only as CF

00:10:36,920 --> 00:10:43,370
Linux FS 2 for the trusty based stack

00:10:40,070 --> 00:10:46,160
and when it's running on Diego we have

00:10:43,370 --> 00:10:48,560
an option to preload those onto some of

00:10:46,160 --> 00:10:50,030
the cells and so when we're identifying

00:10:48,560 --> 00:10:52,460
that root filesystem

00:10:50,030 --> 00:10:54,110
we use this pre-loaded schema to

00:10:52,460 --> 00:10:55,550
indicate that this should already be

00:10:54,110 --> 00:10:58,160
present on some of your cells and they

00:10:55,550 --> 00:10:59,059
will know that but that's not sufficient

00:10:58,160 --> 00:11:00,769
just to run the

00:10:59,059 --> 00:11:03,109
particular type of application we also

00:11:00,769 --> 00:11:05,029
need to load in the contents of that

00:11:03,109 --> 00:11:07,309
application into this base root file

00:11:05,029 --> 00:11:10,519
system so that's the notion for a build

00:11:07,309 --> 00:11:12,769
pack app of a droplet so for this build

00:11:10,519 --> 00:11:14,419
back out the file system contents of the

00:11:12,769 --> 00:11:18,199
container are the union of those two

00:11:14,419 --> 00:11:19,849
sets of information but then we also

00:11:18,199 --> 00:11:22,249
need to know what to run in that file

00:11:19,849 --> 00:11:24,139
system context and in this case for a

00:11:22,249 --> 00:11:27,789
rails app maybe we want to run rails and

00:11:24,139 --> 00:11:29,929
server mode with the correct gem context

00:11:27,789 --> 00:11:32,509
but finally we need to put some limits

00:11:29,929 --> 00:11:34,249
on this to run it efficiently and

00:11:32,509 --> 00:11:37,759
effectively in a multi-tenant container

00:11:34,249 --> 00:11:39,679
environment so that's where we can

00:11:37,759 --> 00:11:41,029
specify that maybe this application

00:11:39,679 --> 00:11:44,419
should have no more than half a gigabyte

00:11:41,029 --> 00:11:46,129
of memory but in terms of its disk usage

00:11:44,419 --> 00:11:48,379
we only need to account for that extra

00:11:46,129 --> 00:11:50,239
droplet layer because we already have

00:11:48,379 --> 00:11:52,579
that pre-loaded file system available on

00:11:50,239 --> 00:11:55,309
the cells that can run so maybe we only

00:11:52,579 --> 00:11:57,949
need half a quarter of a gig of disk

00:11:55,309 --> 00:12:01,339
space for the differentiation factor for

00:11:57,949 --> 00:12:04,879
this app and then finally maybe we want

00:12:01,339 --> 00:12:06,559
to run three instances of this where we

00:12:04,879 --> 00:12:08,329
can use the same set of language to

00:12:06,559 --> 00:12:12,049
specify how to run a docker image as a

00:12:08,329 --> 00:12:13,909
container on the platform so for example

00:12:12,049 --> 00:12:15,699
let's say we want to run the library

00:12:13,909 --> 00:12:18,049
Redis image from dr. hub

00:12:15,699 --> 00:12:19,519
well that tells us all of the

00:12:18,049 --> 00:12:21,469
information we need for the contents of

00:12:19,519 --> 00:12:24,529
that container the whole thing comes

00:12:21,469 --> 00:12:26,209
from docker hub and then the action will

00:12:24,529 --> 00:12:27,799
be whatever the entry point specifies in

00:12:26,209 --> 00:12:29,929
that docker image which we can extract

00:12:27,799 --> 00:12:32,899
during a staging task before we run it

00:12:29,929 --> 00:12:34,369
as an app and then likewise maybe we

00:12:32,899 --> 00:12:35,929
have different limits that we want to

00:12:34,369 --> 00:12:36,259
place on this container when it's

00:12:35,929 --> 00:12:38,179
running

00:12:36,259 --> 00:12:39,949
we might need less memory for this

00:12:38,179 --> 00:12:41,989
because we know how much data we intend

00:12:39,949 --> 00:12:43,699
to store in this Redis instance but it

00:12:41,989 --> 00:12:45,289
might need more disk because we're

00:12:43,699 --> 00:12:47,569
having to take the entire contents of

00:12:45,289 --> 00:12:48,949
that docker image and we don't know that

00:12:47,569 --> 00:12:51,469
it's necessarily shared with any other

00:12:48,949 --> 00:12:54,559
containers like we do for the preloaded

00:12:51,469 --> 00:12:56,059
route offenses and then finally maybe we

00:12:54,559 --> 00:12:57,319
only want to run one instance of this

00:12:56,059 --> 00:12:58,999
because these registers are not

00:12:57,319 --> 00:13:00,529
clustered together and so if we run

00:12:58,999 --> 00:13:04,149
multiple copies and load balance across

00:13:00,529 --> 00:13:06,889
them we would get inconsistent behavior

00:13:04,149 --> 00:13:08,689
okay so let's focus on this rails based

00:13:06,889 --> 00:13:11,179
app and let me walk you through how

00:13:08,689 --> 00:13:13,020
Diego actually schedules it to run in

00:13:11,179 --> 00:13:15,420
the core

00:13:13,020 --> 00:13:17,760
so here's a sample small deployment of

00:13:15,420 --> 00:13:19,589
Diego we've got the BBFS and the

00:13:17,760 --> 00:13:21,600
auctioneer and then we have a few cells

00:13:19,589 --> 00:13:24,360
that already exist in the deployment

00:13:21,600 --> 00:13:26,160
so in particular we have two Linux based

00:13:24,360 --> 00:13:27,930
cells they're running Linux operating

00:13:26,160 --> 00:13:30,420
system and then garden Linux on top of

00:13:27,930 --> 00:13:32,010
that or maybe garden run C and they're

00:13:30,420 --> 00:13:34,350
already running some existing work a

00:13:32,010 --> 00:13:35,910
couple containers on the first one and a

00:13:34,350 --> 00:13:38,760
single container on the second one and

00:13:35,910 --> 00:13:41,010
then below that we have a Windows VM

00:13:38,760 --> 00:13:45,630
running garden windows and it too has

00:13:41,010 --> 00:13:47,490
some work that it's performing okay so

00:13:45,630 --> 00:13:49,320
the first step is for cloud controller

00:13:47,490 --> 00:13:51,870
through the bridge to specify that the

00:13:49,320 --> 00:13:53,550
DVS I'd like to run this Ruby based

00:13:51,870 --> 00:13:54,620
application and give me three instances

00:13:53,550 --> 00:13:57,330
of it

00:13:54,620 --> 00:13:59,130
so the VBS records that into Stata base

00:13:57,330 --> 00:14:01,830
it records the specification of that

00:13:59,130 --> 00:14:04,170
work entirely but it also makes slots

00:14:01,830 --> 00:14:07,380
reach one of those instances index zero

00:14:04,170 --> 00:14:09,000
one and two and as them greyed out

00:14:07,380 --> 00:14:10,589
because it hasn't assigned them to

00:14:09,000 --> 00:14:13,980
anything yet nothing in the system has

00:14:10,589 --> 00:14:15,930
claimed them as running so it's next

00:14:13,980 --> 00:14:17,880
step is to communicate to the auctioneer

00:14:15,930 --> 00:14:19,470
there's some outstanding work that we

00:14:17,880 --> 00:14:22,770
need to place somewhere to try to start

00:14:19,470 --> 00:14:26,730
running and the auctioneer receives that

00:14:22,770 --> 00:14:28,589
request and batches up requests for new

00:14:26,730 --> 00:14:30,149
work to place so it could be in the

00:14:28,589 --> 00:14:31,740
middle of placing some existing work and

00:14:30,149 --> 00:14:33,270
it's not going to step over itself

00:14:31,740 --> 00:14:35,610
trying to place a new work in the middle

00:14:33,270 --> 00:14:37,350
of that let's say this is the complete

00:14:35,610 --> 00:14:40,560
batch of work that the auctioneer has

00:14:37,350 --> 00:14:44,130
been told to run so here's how it places

00:14:40,560 --> 00:14:45,390
them its first step is to contact all

00:14:44,130 --> 00:14:48,000
the cells that are registered in the

00:14:45,390 --> 00:14:51,000
deployment and to get a snapshot of

00:14:48,000 --> 00:14:52,829
their current state and from there it

00:14:51,000 --> 00:14:55,920
can make decisions about optimal

00:14:52,829 --> 00:15:00,420
placement and update those in real time

00:14:55,920 --> 00:15:01,649
as it makes its decisions so its first

00:15:00,420 --> 00:15:04,589
going to decide where to place that

00:15:01,649 --> 00:15:07,380
index 0 instance of this new lrp and

00:15:04,589 --> 00:15:08,790
it's looking across the capacity of

00:15:07,380 --> 00:15:11,160
those cells and it identifies that

00:15:08,790 --> 00:15:13,890
second cell as the best place to put

00:15:11,160 --> 00:15:15,420
that instance because it's running the

00:15:13,890 --> 00:15:18,360
fewest containers maybe it's also using

00:15:15,420 --> 00:15:19,709
the least amount of memory ok and now

00:15:18,360 --> 00:15:22,320
it's going to place that index 1

00:15:19,709 --> 00:15:23,910
instance and it's looking now and it's

00:15:22,320 --> 00:15:26,310
updated representation of what these

00:15:23,910 --> 00:15:26,760
cells are running and one of the

00:15:26,310 --> 00:15:28,440
constrain

00:15:26,760 --> 00:15:30,300
that the auctioneer operates by is that

00:15:28,440 --> 00:15:33,050
it wants to spread these instances of

00:15:30,300 --> 00:15:35,070
the same lrp out as widely as possible

00:15:33,050 --> 00:15:37,260
so even though it looks like those two

00:15:35,070 --> 00:15:38,940
linux cells are tied it's going to

00:15:37,260 --> 00:15:40,380
prefer the first one for placement

00:15:38,940 --> 00:15:44,340
because it's not running any other

00:15:40,380 --> 00:15:46,200
instances of this LRP okay so now it's

00:15:44,340 --> 00:15:49,710
got one final instance to place the

00:15:46,200 --> 00:15:51,240
index to one if we were going solely by

00:15:49,710 --> 00:15:52,350
anti affinity it would want to place it

00:15:51,240 --> 00:15:54,780
on that last cell because it's not

00:15:52,350 --> 00:15:56,370
running any instances of it but that

00:15:54,780 --> 00:15:58,080
cell is ineligible to run this workload

00:15:56,370 --> 00:16:00,600
because it's a Windows cell and not a

00:15:58,080 --> 00:16:02,790
Linux cell so instead it decides to put

00:16:00,600 --> 00:16:06,870
that second instance on that second cell

00:16:02,790 --> 00:16:08,160
or that index to instance so now that

00:16:06,870 --> 00:16:10,620
the auctioneer has made all of those

00:16:08,160 --> 00:16:13,880
decisions it transmits them just to the

00:16:10,620 --> 00:16:16,320
cells that need to run new work and

00:16:13,880 --> 00:16:19,200
those cells immediately reserve space

00:16:16,320 --> 00:16:21,390
for these individual instances and now

00:16:19,200 --> 00:16:23,280
the auctioneer is done and the cells are

00:16:21,390 --> 00:16:25,980
in charge of managing the local work

00:16:23,280 --> 00:16:27,900
that they've been assigned so their next

00:16:25,980 --> 00:16:30,450
task is to communicate back to the BBS

00:16:27,900 --> 00:16:32,040
and to double-check that nobody else has

00:16:30,450 --> 00:16:34,760
claimed that workload while this is

00:16:32,040 --> 00:16:37,560
happening so let's say cell number two

00:16:34,760 --> 00:16:39,380
does that first and it tells the BBS I'm

00:16:37,560 --> 00:16:42,000
trying to claim these two units of work

00:16:39,380 --> 00:16:44,340
nobody else has so the BBS says great

00:16:42,000 --> 00:16:46,230
you're good together and at that point

00:16:44,340 --> 00:16:48,030
this cell can start creating those

00:16:46,230 --> 00:16:50,450
containers and then once they're created

00:16:48,030 --> 00:16:52,860
run these processes inside of them so

00:16:50,450 --> 00:16:55,130
cell number one does the same thing

00:16:52,860 --> 00:16:58,590
claims that in index one instance and

00:16:55,130 --> 00:17:01,110
starts it up so let's say that cell

00:16:58,590 --> 00:17:03,540
starts that container really fast and a

00:17:01,110 --> 00:17:05,400
detects that this process is healthy so

00:17:03,540 --> 00:17:06,810
then reports back to the BBS not only

00:17:05,400 --> 00:17:09,480
have I claimed that but I'm actively

00:17:06,810 --> 00:17:12,930
running and the BBS updates its

00:17:09,480 --> 00:17:14,670
representation of that okay let's say

00:17:12,930 --> 00:17:17,880
cell number two does not have such a

00:17:14,670 --> 00:17:20,760
good time and ends up crashing that

00:17:17,880 --> 00:17:22,560
index to instance so it also reports

00:17:20,760 --> 00:17:25,949
that back to the BBS to say this one

00:17:22,560 --> 00:17:28,410
crashed sorry and then it clears up its

00:17:25,949 --> 00:17:30,360
container so if this is the first time

00:17:28,410 --> 00:17:33,270
that that instance is crashed the BBS is

00:17:30,360 --> 00:17:34,950
policy around how it schedules LR PS

00:17:33,270 --> 00:17:36,270
will say all right you'll get a free

00:17:34,950 --> 00:17:38,510
restart we'll give you three free

00:17:36,270 --> 00:17:40,230
restarts but if you start crashing

00:17:38,510 --> 00:17:46,380
continually then we'll

00:17:40,230 --> 00:17:50,280
back off from that okay so we've seen

00:17:46,380 --> 00:17:52,770
that many of these components even in

00:17:50,280 --> 00:17:54,870
the core of Diego are crucial for it to

00:17:52,770 --> 00:17:55,950
operate successfully we can't get any

00:17:54,870 --> 00:17:57,660
work placed if we don't have the

00:17:55,950 --> 00:17:58,169
auctioneer available to make decisions

00:17:57,660 --> 00:18:00,000
about it

00:17:58,169 --> 00:18:01,950
so that means in any kind of real

00:18:00,000 --> 00:18:04,590
deployment we want to run multiple

00:18:01,950 --> 00:18:05,880
copies of that but we also need to

00:18:04,590 --> 00:18:07,950
recognize that it has this important

00:18:05,880 --> 00:18:10,010
global responsibility and we wouldn't

00:18:07,950 --> 00:18:11,940
want multiple copies of it running

00:18:10,010 --> 00:18:13,919
simultaneously and independently and

00:18:11,940 --> 00:18:16,200
making decisions that conflict with each

00:18:13,919 --> 00:18:17,970
other so this is where these components

00:18:16,200 --> 00:18:19,650
coordinate with each other to make sure

00:18:17,970 --> 00:18:21,630
that even though we have multiple ones

00:18:19,650 --> 00:18:24,150
deployed only one is active and the rest

00:18:21,630 --> 00:18:25,429
are dormant and they do that via our

00:18:24,150 --> 00:18:28,710
console dependency

00:18:25,429 --> 00:18:30,330
so these auctioneers contend over a

00:18:28,710 --> 00:18:32,910
particular key and console that

00:18:30,330 --> 00:18:34,530
represents a lock so the both try to

00:18:32,910 --> 00:18:35,790
grab it but only one of them will

00:18:34,530 --> 00:18:39,059
succeed that's where we use the

00:18:35,790 --> 00:18:41,760
consistency of the RAF based key value

00:18:39,059 --> 00:18:43,110
store the console presents so now

00:18:41,760 --> 00:18:46,020
auctioneering umber two is grabbed that

00:18:43,110 --> 00:18:49,140
lock and it's going to broadcast to the

00:18:46,020 --> 00:18:51,419
rest of the system that any requests for

00:18:49,140 --> 00:18:53,730
placement should go to it so when the

00:18:51,419 --> 00:18:59,040
VBS shows up and has Newark to place it

00:18:53,730 --> 00:19:01,110
talks to the active one well let's say

00:18:59,040 --> 00:19:02,910
that we're now updating the deployment

00:19:01,110 --> 00:19:04,470
maybe this is a controlled update we're

00:19:02,910 --> 00:19:05,160
using Bosch to update all the software

00:19:04,470 --> 00:19:07,350
in the cluster

00:19:05,160 --> 00:19:10,260
so we're gracefully shutting down the

00:19:07,350 --> 00:19:12,480
auctioneer and as part of that it will

00:19:10,260 --> 00:19:15,179
release its lock in consoles key value

00:19:12,480 --> 00:19:16,770
store so now the standby auctioneer

00:19:15,179 --> 00:19:22,140
which is continuous trying to grab it

00:19:16,770 --> 00:19:23,429
will succeed and become active so then

00:19:22,140 --> 00:19:25,049
when Bosch brings back that second

00:19:23,429 --> 00:19:28,410
instance of the auctioneer it'll be in

00:19:25,049 --> 00:19:29,880
that dormant mode okay well let's say

00:19:28,410 --> 00:19:33,049
that something more catastrophic happens

00:19:29,880 --> 00:19:36,450
and that first auctioneer just crashes

00:19:33,049 --> 00:19:38,580
well eventually within seconds console

00:19:36,450 --> 00:19:42,150
will release that lock from the system

00:19:38,580 --> 00:19:43,830
because nobody's maintaining it and then

00:19:42,150 --> 00:19:46,110
that second auctioneer can come in and

00:19:43,830 --> 00:19:49,440
grab it again and become active within

00:19:46,110 --> 00:19:50,970
just a few seconds so we use this

00:19:49,440 --> 00:19:53,700
pattern for the auctioneer to coordinate

00:19:50,970 --> 00:19:54,840
them it turns out that the BBS and the

00:19:53,700 --> 00:19:57,360
burger also have that global

00:19:54,840 --> 00:19:58,860
responsibility and so they also all hold

00:19:57,360 --> 00:20:01,380
blocks that are represented in consul

00:19:58,860 --> 00:20:05,399
and then some of the components that are

00:20:01,380 --> 00:20:06,899
in these bridge layers also hold these

00:20:05,399 --> 00:20:09,960
locks because they have similar global

00:20:06,899 --> 00:20:11,279
responsibilities so in general if you

00:20:09,960 --> 00:20:12,899
want to know how to run an H a

00:20:11,279 --> 00:20:14,549
deployment of these services you need at

00:20:12,899 --> 00:20:16,200
least two of these to be sure that one

00:20:14,549 --> 00:20:18,029
can fail over to the other appropriately

00:20:16,200 --> 00:20:19,679
and in general if you're deploying by

00:20:18,029 --> 00:20:22,399
availability zone we recommend that you

00:20:19,679 --> 00:20:25,590
have at least one per availability Xin

00:20:22,399 --> 00:20:28,049
so we also use consul for other purposes

00:20:25,590 --> 00:20:30,870
we use it to broadcast stateless

00:20:28,049 --> 00:20:32,460
services through consul DNS and also the

00:20:30,870 --> 00:20:35,750
cells store their presence records in

00:20:32,460 --> 00:20:35,750
there so the auctioneer can find them

00:20:35,840 --> 00:20:41,460
okay so that's a detailed tour of the

00:20:39,600 --> 00:20:42,690
core of Diego and let me give you some

00:20:41,460 --> 00:20:45,179
context on what some of these

00:20:42,690 --> 00:20:49,230
intermediate layers do as well the CC

00:20:45,179 --> 00:20:51,090
bridge and the routing layers so if we

00:20:49,230 --> 00:20:52,470
look at the CC bridge there's a lot of

00:20:51,090 --> 00:20:53,370
little components on there but they each

00:20:52,470 --> 00:20:56,159
have very tightly focused

00:20:53,370 --> 00:20:58,380
responsibilities so for example the

00:20:56,159 --> 00:21:00,240
stager component is responsible for

00:20:58,380 --> 00:21:02,370
receiving staging requests from cloud

00:21:00,240 --> 00:21:04,620
controller and translating them into

00:21:02,370 --> 00:21:09,409
those Diego tasks to submit to the BBS

00:21:04,620 --> 00:21:11,730
and then it also registers itself as the

00:21:09,409 --> 00:21:15,029
place to receive a call back when that

00:21:11,730 --> 00:21:16,980
task is done so if that task succeeds or

00:21:15,029 --> 00:21:21,200
fails the stager is notified about it

00:21:16,980 --> 00:21:21,200
and then let's see si know about it

00:21:21,889 --> 00:21:26,909
likewise when cloud controller wants to

00:21:24,690 --> 00:21:28,500
run an app on the Diego back end it

00:21:26,909 --> 00:21:29,760
doesn't talk directly to the BBS it

00:21:28,500 --> 00:21:32,010
talks through this instinct listener

00:21:29,760 --> 00:21:34,649
component and it tells it to start the

00:21:32,010 --> 00:21:36,419
application and that translates it into

00:21:34,649 --> 00:21:39,299
a long-running process for Diego to

00:21:36,419 --> 00:21:41,460
manage and then what collector for wants

00:21:39,299 --> 00:21:43,679
to know how that process is doing how

00:21:41,460 --> 00:21:46,919
its instances are running it'll talk to

00:21:43,679 --> 00:21:51,870
the TPS listener and that will ask the

00:21:46,919 --> 00:21:53,309
BBS about the lrpc state okay so there

00:21:51,870 --> 00:21:54,480
are also a couple of components on this

00:21:53,309 --> 00:21:56,730
bridge that have more global

00:21:54,480 --> 00:21:58,580
responsibilities and the most important

00:21:56,730 --> 00:22:02,460
one is what we call the instinct volker

00:21:58,580 --> 00:22:04,590
so it will periodically ask the BBS what

00:22:02,460 --> 00:22:06,900
el RPS are you running on behalf of

00:22:04,590 --> 00:22:08,460
Cloud Controller

00:22:06,900 --> 00:22:10,590
and then it'll also ask hot controller

00:22:08,460 --> 00:22:12,920
what apps should Diego be running on its

00:22:10,590 --> 00:22:15,000
back end and it will reconcile those

00:22:12,920 --> 00:22:17,850
treating cloud controller as the source

00:22:15,000 --> 00:22:19,920
of truth so if there are any missing LR

00:22:17,850 --> 00:22:22,200
PS it'll make them if there are any

00:22:19,920 --> 00:22:23,580
extra ones it will delete them and if

00:22:22,200 --> 00:22:28,920
there are any ones that should be scaled

00:22:23,580 --> 00:22:30,840
differently it will rescale so because

00:22:28,920 --> 00:22:32,970
this is a global responsibility it's a

00:22:30,840 --> 00:22:34,440
lot of data and it could be potentially

00:22:32,970 --> 00:22:36,960
making decisions that conflict with

00:22:34,440 --> 00:22:38,520
itself if it's running in parallel this

00:22:36,960 --> 00:22:41,910
also has one of these global locks

00:22:38,520 --> 00:22:44,730
that's managed through console and then

00:22:41,910 --> 00:22:47,850
last and maybe least we have the TPS

00:22:44,730 --> 00:22:49,880
watcher and it has one job it's going to

00:22:47,850 --> 00:22:52,110
watch for instances the crash and

00:22:49,880 --> 00:22:54,680
translate those into crash events for

00:22:52,110 --> 00:22:56,790
cloud controller to report to end-users

00:22:54,680 --> 00:22:58,620
so because we don't want duplicate

00:22:56,790 --> 00:23:01,460
events this also has a lock that's

00:22:58,620 --> 00:23:01,460
managed in console

00:23:01,580 --> 00:23:05,850
ok so in the last few minutes let me

00:23:04,710 --> 00:23:07,530
give you an overview of how we've

00:23:05,850 --> 00:23:09,870
integrated the routing tears into this

00:23:07,530 --> 00:23:11,310
new Diego back-end so we have the

00:23:09,870 --> 00:23:13,050
familiar go routers down here in the

00:23:11,310 --> 00:23:14,510
lower right and then we have this

00:23:13,050 --> 00:23:18,150
component we call the route emitter

00:23:14,510 --> 00:23:20,160
that's deployed alongside that so the

00:23:18,150 --> 00:23:23,730
router meter is paying attention to all

00:23:20,160 --> 00:23:26,190
the work that's scheduled in the BBS so

00:23:23,730 --> 00:23:27,420
when we specify a new lrp not only can

00:23:26,190 --> 00:23:28,620
we specify what should run in the

00:23:27,420 --> 00:23:30,690
container but we can specify some

00:23:28,620 --> 00:23:33,450
networking and information associated to

00:23:30,690 --> 00:23:36,390
it so in particular we can tell Garden I

00:23:33,450 --> 00:23:40,650
want to expose ports in this case say

00:23:36,390 --> 00:23:43,800
8080 and 9999 and then as part of that

00:23:40,650 --> 00:23:45,780
specification Cloud Controller or any

00:23:43,800 --> 00:23:47,610
other client can record that it wants to

00:23:45,780 --> 00:23:50,010
associate some of those ports with

00:23:47,610 --> 00:23:52,080
certain types of routers so in this case

00:23:50,010 --> 00:23:54,300
for the go router we have a tag called

00:23:52,080 --> 00:23:56,040
the CF router and it can specify the

00:23:54,300 --> 00:23:58,740
domain name that that's associated with

00:23:56,040 --> 00:24:01,770
and then the port the traffic should

00:23:58,740 --> 00:24:03,720
come into the container on so the router

00:24:01,770 --> 00:24:05,790
meter detects this new work is desired

00:24:03,720 --> 00:24:09,030
and starts building up its routing table

00:24:05,790 --> 00:24:10,590
in memory so it knows at this point that

00:24:09,030 --> 00:24:14,790
something is interested in routing

00:24:10,590 --> 00:24:17,880
foo.com requests see instances of this

00:24:14,790 --> 00:24:20,110
app or of this lrp but it doesn't have

00:24:17,880 --> 00:24:22,779
any instances yet

00:24:20,110 --> 00:24:24,220
so when one shows up some additional

00:24:22,779 --> 00:24:26,860
information that's present when it's

00:24:24,220 --> 00:24:28,840
running is the address of the cell that

00:24:26,860 --> 00:24:32,200
that instance is running on and the port

00:24:28,840 --> 00:24:33,549
mapping so if something outside of the

00:24:32,200 --> 00:24:36,399
cell wants to communicate with this

00:24:33,549 --> 00:24:38,320
container on container side port 8080 it

00:24:36,399 --> 00:24:41,380
should instead talk to this other port

00:24:38,320 --> 00:24:43,899
that's exposed on the Sun so the router

00:24:41,380 --> 00:24:46,419
matter gets that update too and now it

00:24:43,899 --> 00:24:49,870
effectively doesn't inner join on the LR

00:24:46,419 --> 00:24:52,779
P's ID and that port to register that

00:24:49,870 --> 00:24:54,279
new endpoint with this domain and then

00:24:52,779 --> 00:24:55,720
we get when it gets these updates it

00:24:54,279 --> 00:25:00,370
sends out of information out to all the

00:24:55,720 --> 00:25:02,110
go routers well this is now an

00:25:00,370 --> 00:25:04,510
extensible mechanism that supports other

00:25:02,110 --> 00:25:06,549
types of routing on the platform if we

00:25:04,510 --> 00:25:09,100
want to do TCP routing now we can

00:25:06,549 --> 00:25:13,210
associate some TCP routing data see that

00:25:09,100 --> 00:25:14,740
9999 port instead and so there's an

00:25:13,210 --> 00:25:17,110
analogous component in the routing

00:25:14,740 --> 00:25:19,779
release called the TCP emitter that will

00:25:17,110 --> 00:25:22,690
do the same kind of watching for desired

00:25:19,779 --> 00:25:24,570
routing data and instance back-end

00:25:22,690 --> 00:25:27,130
information and make those associations

00:25:24,570 --> 00:25:31,299
so in this case it knows it wants to

00:25:27,130 --> 00:25:33,820
associate with this lrp traffic destined

00:25:31,299 --> 00:25:35,649
for port 9999 with a particular routing

00:25:33,820 --> 00:25:37,990
group and a particular external port and

00:25:35,649 --> 00:25:39,700
then I can update those TCP routers

00:25:37,990 --> 00:25:41,980
dynamically with the back-end

00:25:39,700 --> 00:25:49,500
information as those instances come

00:25:41,980 --> 00:25:51,460
online or die off okay so that is a

00:25:49,500 --> 00:25:52,899
large part of the functionality of

00:25:51,460 --> 00:25:54,519
what's in Diego release and how it works

00:25:52,899 --> 00:25:55,720
and let me tell you a little bit about

00:25:54,519 --> 00:25:57,580
the next steps that we're planning on

00:25:55,720 --> 00:26:01,090
taking so if you went to gym and

00:25:57,580 --> 00:26:02,260
Luanne's presentation this morning they

00:26:01,090 --> 00:26:03,820
were telling you about what we've done

00:26:02,260 --> 00:26:06,340
in terms of performance analysis of

00:26:03,820 --> 00:26:09,190
Diego and we know that it runs great at

00:26:06,340 --> 00:26:12,610
10,000 instances it will run adequately

00:26:09,190 --> 00:26:14,679
at say 50,000 instances but we really

00:26:12,610 --> 00:26:16,240
want to hit is 250,000 instances or

00:26:14,679 --> 00:26:19,149
maybe more maybe up to a million

00:26:16,240 --> 00:26:20,769
containers managed in order to do that

00:26:19,149 --> 00:26:24,250
one of the bottlenecks that we've

00:26:20,769 --> 00:26:27,070
identified as fcd itself in version 2 so

00:26:24,250 --> 00:26:29,350
we're almost done switching Diego over

00:26:27,070 --> 00:26:30,850
to run with a relational store which is

00:26:29,350 --> 00:26:33,350
a much better fit for the kinds of data

00:26:30,850 --> 00:26:36,140
operations we need to do on the workload

00:26:33,350 --> 00:26:38,539
we're managing you've also heard a lot

00:26:36,140 --> 00:26:40,039
about the persistence effort so in fact

00:26:38,539 --> 00:26:42,320
we saw a demo this morning of WordPress

00:26:40,039 --> 00:26:44,419
running scaled up on Cloud Foundry which

00:26:42,320 --> 00:26:49,400
is pretty amazing and the persistence

00:26:44,419 --> 00:26:51,440
team is now working on ways to attach

00:26:49,400 --> 00:26:52,970
different types of storage instances

00:26:51,440 --> 00:26:55,640
such as individual block storage for

00:26:52,970 --> 00:26:57,169
instances we also have a container to

00:26:55,640 --> 00:26:59,660
container networking team that's

00:26:57,169 --> 00:27:01,970
integrating with Diego to provide richer

00:26:59,660 --> 00:27:05,450
types of networking between containers

00:27:01,970 --> 00:27:07,010
and policy to enforce so these are all

00:27:05,450 --> 00:27:09,350
things that are happening on teams that

00:27:07,010 --> 00:27:10,880
are adjacent to Diego but on the core

00:27:09,350 --> 00:27:14,659
Diego team itself we're very interested

00:27:10,880 --> 00:27:16,370
in exploring notions of active

00:27:14,659 --> 00:27:18,710
rebalancing within the cluster so we can

00:27:16,370 --> 00:27:20,929
make greater use of the available space

00:27:18,710 --> 00:27:22,640
on cells by doing more sophisticated

00:27:20,929 --> 00:27:24,799
placement decisions right now we give

00:27:22,640 --> 00:27:26,840
absolute primacy to running instances we

00:27:24,799 --> 00:27:29,059
never move those except when we're

00:27:26,840 --> 00:27:33,770
draining the cells in say a Bosh

00:27:29,059 --> 00:27:35,539
lifecycle event but in some cases that

00:27:33,770 --> 00:27:37,820
could mean that we temporarily end up

00:27:35,539 --> 00:27:39,289
with poor anti affinity for instances

00:27:37,820 --> 00:27:41,840
just based on the current capacity of

00:27:39,289 --> 00:27:44,000
the cells and if we can gradually start

00:27:41,840 --> 00:27:45,980
massaging those to make better

00:27:44,000 --> 00:27:47,630
guarantees long-term about the health of

00:27:45,980 --> 00:27:50,419
these instances in their placement that

00:27:47,630 --> 00:27:52,429
we're very interested in that and then

00:27:50,419 --> 00:27:55,520
we're also committed to making Diego an

00:27:52,429 --> 00:27:57,559
operable system so improving the metrics

00:27:55,520 --> 00:27:59,059
that are coming out of it and improving

00:27:57,559 --> 00:28:00,710
any other additional tooling that may be

00:27:59,059 --> 00:28:05,090
useful for operators as they interact

00:28:00,710 --> 00:28:06,169
with and inspect the system so we've got

00:28:05,090 --> 00:28:14,200
a couple minutes left I'd be happy to

00:28:06,169 --> 00:28:14,200
field any questions yeah

00:28:16,420 --> 00:28:19,670
[Music]

00:28:20,299 --> 00:28:24,029
yeah so the question is what are the

00:28:22,559 --> 00:28:25,860
release plans in terms of organizing

00:28:24,029 --> 00:28:27,840
these so we've intentionally avoided

00:28:25,860 --> 00:28:30,269
incorporating Diego release directly

00:28:27,840 --> 00:28:34,019
into CF release and our manifest

00:28:30,269 --> 00:28:35,340
generation scripts are the primary uses

00:28:34,019 --> 00:28:39,210
to create a Diego deployment that

00:28:35,340 --> 00:28:41,100
integrates against a CF and there's a

00:28:39,210 --> 00:28:42,899
broader plan to take CF release and

00:28:41,100 --> 00:28:44,460
break it into these kinds of subsystems

00:28:42,899 --> 00:28:47,820
Cloud Controller UA

00:28:44,460 --> 00:28:49,499
the routing releases and so the release

00:28:47,820 --> 00:28:52,289
integration team is working on their

00:28:49,499 --> 00:28:53,759
efforts to produce a better system for

00:28:52,289 --> 00:28:56,340
manifest generation that will

00:28:53,759 --> 00:28:58,679
incorporate the existing CF release

00:28:56,340 --> 00:29:00,239
components as well as the Diego ones so

00:28:58,679 --> 00:29:01,830
we'll have a unified manifest generation

00:29:00,239 --> 00:29:06,350
system coming soon but for now it's a

00:29:01,830 --> 00:29:06,350
separate deployment manifest yeah

00:29:19,889 --> 00:29:25,399
so right now we don't support any notion

00:29:22,830 --> 00:29:27,450
of cross locality for that scheduling

00:29:25,399 --> 00:29:29,999
all those scheduling decisions are

00:29:27,450 --> 00:29:31,470
independent I think that's another

00:29:29,999 --> 00:29:33,570
interesting placement direction to go in

00:29:31,470 --> 00:29:35,220
so now maybe you have a system

00:29:33,570 --> 00:29:37,320
especially in the presence of this

00:29:35,220 --> 00:29:39,599
container networking policy where two

00:29:37,320 --> 00:29:41,909
different LR PS should be communicating

00:29:39,599 --> 00:29:43,379
with each other so that's one direction

00:29:41,909 --> 00:29:51,389
that we could look at taking placement

00:29:43,379 --> 00:29:52,859
policy depending on the use cases okay

00:29:51,389 --> 00:29:54,299
well I think we're about out of time so

00:29:52,859 --> 00:29:54,630
thank you very much if you're interested

00:29:54,299 --> 00:30:00,459
in more

00:29:54,630 --> 00:30:00,459

YouTube URL: https://www.youtube.com/watch?v=iv5EpheLLh0


