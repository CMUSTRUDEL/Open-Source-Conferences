Title: Simplified Kubernetes User Experience with TAS4K8S - Boskey Savla, VMware
Publication date: 2020-06-20
Playlist: Cloud Foundry Summit NA 2020 - Virtual
Description: 
	Simplified Kubernetes User Experience with TAS4K8S - Boskey Savla, VMware 

For more info: https://www.cloudfoundry.org/

Kubernetes solves a lot of challenges that come with managing an infrastructure stack. At the same time giving development teams direct access to the Kubernetes API for workload deployment and maintenance isnâ€™t an optimal user experience. In this talk, we look at how VMware Tanzu is helping create an infrastructure stack that supports Kubernetes yet gives Development teams the CF experience via Tanzu Application Service Powered by cf-for-k8s.
Captions: 
	00:00:00,030 --> 00:00:03,870
hi everybody my name is Bosque szabla

00:00:01,949 --> 00:00:06,080
and I'm a technical marketing manager

00:00:03,870 --> 00:00:08,910
working on modern applications at VMware

00:00:06,080 --> 00:00:11,070
today I'm going to talk about how VM er

00:00:08,910 --> 00:00:13,349
is helping simplify the cube gratis user

00:00:11,070 --> 00:00:17,010
experience based on console application

00:00:13,349 --> 00:00:19,439
service with kubernetes now if you think

00:00:17,010 --> 00:00:21,060
about you know a life a little bit go

00:00:19,439 --> 00:00:23,640
back in history and think about how

00:00:21,060 --> 00:00:27,510
applications were built and deployed

00:00:23,640 --> 00:00:30,029
before kubernetes came along essentially

00:00:27,510 --> 00:00:31,529
you'll see a degree or variation of

00:00:30,029 --> 00:00:34,350
something like this right there's a

00:00:31,529 --> 00:00:36,570
development team that is working on

00:00:34,350 --> 00:00:39,000
creating that application recording it

00:00:36,570 --> 00:00:40,440
etc and then there's a team that is

00:00:39,000 --> 00:00:42,690
actually trying to figure out how to

00:00:40,440 --> 00:00:45,690
push that application to a given

00:00:42,690 --> 00:00:48,840
infrastructure now sometimes these

00:00:45,690 --> 00:00:50,570
processes could be very aware in you

00:00:48,840 --> 00:00:53,760
know every step is manual and you're

00:00:50,570 --> 00:00:55,440
trying to somebody's actually physically

00:00:53,760 --> 00:00:57,780
sitting there clicking washing machines

00:00:55,440 --> 00:01:00,930
deploying integrating but load balancers

00:00:57,780 --> 00:01:03,270
and things like that and at times we

00:01:00,930 --> 00:01:05,909
also start seeing organization do this

00:01:03,270 --> 00:01:08,220
with some configuration management tools

00:01:05,909 --> 00:01:11,520
like maybe chef you know right chef

00:01:08,220 --> 00:01:16,920
recipes or puppet manifest files around

00:01:11,520 --> 00:01:18,659
symbol in cetera now with a completely

00:01:16,920 --> 00:01:20,580
automated system where you know a

00:01:18,659 --> 00:01:22,950
configuration management system is going

00:01:20,580 --> 00:01:25,860
to automate the deployment and the

00:01:22,950 --> 00:01:27,960
maintenance of that application even

00:01:25,860 --> 00:01:30,390
when you're doing so you know apart from

00:01:27,960 --> 00:01:33,000
writing the application code itself the

00:01:30,390 --> 00:01:35,579
team now has to start deploying or

00:01:33,000 --> 00:01:38,070
working on coding this configuration

00:01:35,579 --> 00:01:39,810
management tool and a lot of times these

00:01:38,070 --> 00:01:41,700
config management tools have a one on

00:01:39,810 --> 00:01:44,460
one bad thing to a particular

00:01:41,700 --> 00:01:47,159
infrastructure so if you write a chef

00:01:44,460 --> 00:01:48,930
recipe for AWS it may not work on a

00:01:47,159 --> 00:01:51,810
shore or vice versa so it may not work

00:01:48,930 --> 00:01:53,909
on vSphere and hence like you'll have to

00:01:51,810 --> 00:01:57,270
you know portability also is not that

00:01:53,909 --> 00:01:59,729
you know easy plus developers don't

00:01:57,270 --> 00:02:01,829
necessarily see where their application

00:01:59,729 --> 00:02:04,469
is being pushed it's obvious there's

00:02:01,829 --> 00:02:07,079
this manual wall where things have to be

00:02:04,469 --> 00:02:10,200
pushed to a particular team to get it

00:02:07,079 --> 00:02:12,690
done I think you bananas changed a lot

00:02:10,200 --> 00:02:14,520
of this paradigm by

00:02:12,690 --> 00:02:16,170
you know abstracting away infrastructure

00:02:14,520 --> 00:02:19,050
we all know that you know kubernetes

00:02:16,170 --> 00:02:20,940
just this very well now communities is

00:02:19,050 --> 00:02:23,670
abstracting away infrastructure so it's

00:02:20,940 --> 00:02:25,920
making things a lot more simpler so

00:02:23,670 --> 00:02:28,230
rather than having to talk to every

00:02:25,920 --> 00:02:30,630
infrastructure provider that you may

00:02:28,230 --> 00:02:33,090
have for example talking to AWS talking

00:02:30,630 --> 00:02:35,100
to be scaling to Google Cloud I sure you

00:02:33,090 --> 00:02:37,140
just know you know just talk to the

00:02:35,100 --> 00:02:39,650
Cuban Aires API and then the kubernetes

00:02:37,140 --> 00:02:41,760
api is going to handle a lot of the

00:02:39,650 --> 00:02:43,680
workload creation and deployment

00:02:41,760 --> 00:02:46,290
workflows on that particular

00:02:43,680 --> 00:02:48,510
infrastructure so this simplifies a lot

00:02:46,290 --> 00:02:52,620
of things in terms of how applications

00:02:48,510 --> 00:02:54,570
get deployed and maintained now another

00:02:52,620 --> 00:02:58,410
thing that cuban ids is doing pretty

00:02:54,570 --> 00:03:00,390
good is it is a lot more application

00:02:58,410 --> 00:03:02,940
aware than a lot of these tools and

00:03:00,390 --> 00:03:05,730
systems so for example you know every

00:03:02,940 --> 00:03:08,250
application has specific needs like

00:03:05,730 --> 00:03:10,560
every application or a workload needs to

00:03:08,250 --> 00:03:12,840
be deployed it needs to be scalable it

00:03:10,560 --> 00:03:15,140
needs to have load balancing it needs to

00:03:12,840 --> 00:03:18,690
have ingress it needs to have a way to

00:03:15,140 --> 00:03:21,720
you know manage configuration elements

00:03:18,690 --> 00:03:24,030
at runtime etc and so cuban at ease if

00:03:21,720 --> 00:03:26,430
you think about it has a resource object

00:03:24,030 --> 00:03:28,170
of find for each of these app of this

00:03:26,430 --> 00:03:31,010
particular application needs so it has

00:03:28,170 --> 00:03:33,450
four deployments replica sets services

00:03:31,010 --> 00:03:36,360
ingress etc to take care of that

00:03:33,450 --> 00:03:38,550
particular application and this is where

00:03:36,360 --> 00:03:41,370
the power comes in right but the wall is

00:03:38,550 --> 00:03:43,560
broken and application teams are now the

00:03:41,370 --> 00:03:46,530
development teams themself can define

00:03:43,560 --> 00:03:48,750
very simply what the end state of that

00:03:46,530 --> 00:03:51,540
workload needs to be and push it to a

00:03:48,750 --> 00:03:53,340
communities API and the API or the

00:03:51,540 --> 00:03:54,959
kubernetes cluster in the backend works

00:03:53,340 --> 00:03:58,800
with your infrastructure provider to

00:03:54,959 --> 00:04:00,780
make all of this magic happen and I feel

00:03:58,800 --> 00:04:03,989
like this is really the crux of like

00:04:00,780 --> 00:04:05,880
that is is so popular the way it works

00:04:03,989 --> 00:04:09,690
for developers in the world the way it

00:04:05,880 --> 00:04:12,030
really abstracts infrastructure now

00:04:09,690 --> 00:04:14,459
let's take a look at what that developer

00:04:12,030 --> 00:04:17,100
experience feels like for example the

00:04:14,459 --> 00:04:20,190
deployer a simple two-tier or three tier

00:04:17,100 --> 00:04:22,560
application which has a need to deploy

00:04:20,190 --> 00:04:25,570
persistent storage and hence some

00:04:22,560 --> 00:04:27,220
runtime configurations to manage

00:04:25,570 --> 00:04:29,920
now typically I would you know as a

00:04:27,220 --> 00:04:32,830
developer you know one would start

00:04:29,920 --> 00:04:35,320
building or writing that code they would

00:04:32,830 --> 00:04:37,570
then create a docker file and

00:04:35,320 --> 00:04:40,870
containerize that build a container push

00:04:37,570 --> 00:04:42,970
it into a registry and then finally one

00:04:40,870 --> 00:04:45,760
before pushing it to a kubernetes are

00:04:42,970 --> 00:04:48,430
giving it to communities the team will

00:04:45,760 --> 00:04:51,250
have to define what kind of kubernetes

00:04:48,430 --> 00:04:53,470
objects are needed to run that

00:04:51,250 --> 00:04:55,120
particular application so some of the

00:04:53,470 --> 00:04:57,520
common objects would be I need a

00:04:55,120 --> 00:05:00,100
deployment type maybe you need a replica

00:04:57,520 --> 00:05:01,450
set maybe you need a service so these

00:05:00,100 --> 00:05:04,210
are some of the common elements that

00:05:01,450 --> 00:05:06,190
need to be created but then if you're

00:05:04,210 --> 00:05:07,960
talking to a database server for example

00:05:06,190 --> 00:05:10,120
then you don't need to define you know a

00:05:07,960 --> 00:05:12,820
way to talk to the database servers you

00:05:10,120 --> 00:05:17,140
probably end up using secrets or config

00:05:12,820 --> 00:05:21,010
maps to do so so the dev team then has

00:05:17,140 --> 00:05:23,740
to define a lot of these manifest files

00:05:21,010 --> 00:05:26,440
that are needed for that application to

00:05:23,740 --> 00:05:28,530
effectively run and that is just for the

00:05:26,440 --> 00:05:30,880
you know a single tier of that adds

00:05:28,530 --> 00:05:33,070
similarly for every other service or a

00:05:30,880 --> 00:05:35,170
for example a persistent storage that

00:05:33,070 --> 00:05:37,120
their application needs you'll have to

00:05:35,170 --> 00:05:38,920
go ahead and do the same you'll have to

00:05:37,120 --> 00:05:41,980
define secrets you'll have to define

00:05:38,920 --> 00:05:46,390
deployment persistent you know volume

00:05:41,980 --> 00:05:50,260
etc and all this for a two-tier app may

00:05:46,390 --> 00:05:52,120
feel like a little bit of an overhead in

00:05:50,260 --> 00:05:55,870
terms of your overall development

00:05:52,120 --> 00:05:57,790
lifecycle and we definitely feel that we

00:05:55,870 --> 00:05:58,750
can make this process a lot more

00:05:57,790 --> 00:06:00,580
efficient

00:05:58,750 --> 00:06:03,340
I mean kubernetes is already making

00:06:00,580 --> 00:06:06,130
things so simple for I know dev teams

00:06:03,340 --> 00:06:08,470
but they don't have to think about you

00:06:06,130 --> 00:06:11,320
know what is an AWS load balancer versus

00:06:08,470 --> 00:06:13,720
what is a vSphere load balancer etc but

00:06:11,320 --> 00:06:16,000
then we feel like there is a scope to

00:06:13,720 --> 00:06:19,300
you know or they define the experience

00:06:16,000 --> 00:06:22,690
of even talking to the communities API

00:06:19,300 --> 00:06:26,110
and that is where you know we are

00:06:22,690 --> 00:06:28,390
working to create this new integration

00:06:26,110 --> 00:06:30,880
where we understand that Cloud Foundry

00:06:28,390 --> 00:06:35,020
which was a pass or platform as a

00:06:30,880 --> 00:06:36,880
service is very popular with development

00:06:35,020 --> 00:06:39,230
teams because of the simple user

00:06:36,880 --> 00:06:41,750
experience that I'd give developers

00:06:39,230 --> 00:06:44,060
and you can simply write your code to a

00:06:41,750 --> 00:06:47,000
CF push and then Cloud Foundry

00:06:44,060 --> 00:06:48,950
completely takes over and maintains your

00:06:47,000 --> 00:06:51,200
application for you without you having

00:06:48,950 --> 00:06:54,920
to you know understand much and how it's

00:06:51,200 --> 00:06:57,380
done so what we are doing is we are

00:06:54,920 --> 00:07:00,440
taking elements or what we talked about

00:06:57,380 --> 00:07:03,290
you is kubernetes has so much to offer

00:07:00,440 --> 00:07:05,300
and the deliver experience of from

00:07:03,290 --> 00:07:07,400
foundry is amazing and people love that

00:07:05,300 --> 00:07:10,160
experience why don't we integrate the

00:07:07,400 --> 00:07:13,220
two together to make this even more

00:07:10,160 --> 00:07:15,260
rock-solid of the solution and that's

00:07:13,220 --> 00:07:17,690
what we did with tons of application

00:07:15,260 --> 00:07:19,340
service for kubernetes what turns your

00:07:17,690 --> 00:07:21,770
application service for kubernetes does

00:07:19,340 --> 00:07:24,140
is it gives you that familiar CF push

00:07:21,770 --> 00:07:26,330
experience at the same time the

00:07:24,140 --> 00:07:29,930
abstraction the capabilities for Kaveri

00:07:26,330 --> 00:07:34,340
Springs essentially what we are doing is

00:07:29,930 --> 00:07:36,110
we are basically integrating or we are

00:07:34,340 --> 00:07:39,380
connecting to our foundry runtime

00:07:36,110 --> 00:07:42,410
components to be running as pods and

00:07:39,380 --> 00:07:44,030
containers within Kuban at ease and I'm

00:07:42,410 --> 00:07:47,180
not going to get into the architecture a

00:07:44,030 --> 00:07:49,730
lot because it's a very in-depth it was

00:07:47,180 --> 00:07:52,600
it might take an hour but let's say you

00:07:49,730 --> 00:07:55,670
do a push a CF push of an application

00:07:52,600 --> 00:07:58,850
workflow you know but one that we have

00:07:55,670 --> 00:08:01,670
already coded once you do a CF push what

00:07:58,850 --> 00:08:03,080
effectively happens is the CF runtime

00:08:01,670 --> 00:08:03,860
components that are running on that

00:08:03,080 --> 00:08:05,780
cluster

00:08:03,860 --> 00:08:08,120
for example the build service and

00:08:05,780 --> 00:08:10,580
particularly are going is going to you

00:08:08,120 --> 00:08:12,830
know take all the files needed for that

00:08:10,580 --> 00:08:16,370
application workload create a docker

00:08:12,830 --> 00:08:19,070
file run a docker build on that and

00:08:16,370 --> 00:08:21,920
create an image and will then push that

00:08:19,070 --> 00:08:25,070
image to a registry of your choice or

00:08:21,920 --> 00:08:27,590
container registry wonder once it has

00:08:25,070 --> 00:08:30,500
done that it will start you know talking

00:08:27,590 --> 00:08:33,770
to the kubernetes api to deploy that

00:08:30,500 --> 00:08:36,560
application into deployment and pods and

00:08:33,770 --> 00:08:38,810
then finally it will to create services

00:08:36,560 --> 00:08:41,780
and ingress for that specific

00:08:38,810 --> 00:08:44,560
application so effectively a single CF

00:08:41,780 --> 00:08:47,570
push command in turn is going to talk to

00:08:44,560 --> 00:08:50,120
communities and to multiple different

00:08:47,570 --> 00:08:52,340
tasks that a dev team would have done

00:08:50,120 --> 00:08:53,990
anyways

00:08:52,340 --> 00:08:56,990
let's take a quick demo of how this

00:08:53,990 --> 00:08:59,570
works so here's my kubernetes cluster

00:08:56,990 --> 00:09:03,920
I'm running this on tons of kubernetes

00:08:59,570 --> 00:09:05,840
squid and the Scolari's cluster also has

00:09:03,920 --> 00:09:08,810
the cloud foundry forgiving at ease

00:09:05,840 --> 00:09:11,210
components pre-installed so if you look

00:09:08,810 --> 00:09:14,180
at the namespaces it will have like the

00:09:11,210 --> 00:09:16,280
build service saw the namespaces that

00:09:14,180 --> 00:09:19,420
are not you know needed for Cloud

00:09:16,280 --> 00:09:22,580
Foundry to operate on kubernetes and

00:09:19,420 --> 00:09:26,330
you'll see some components like you'll

00:09:22,580 --> 00:09:29,330
see this casting API server the KPAC the

00:09:26,330 --> 00:09:31,550
build service etc running these parts

00:09:29,330 --> 00:09:33,740
and then are running on the kubernetes

00:09:31,550 --> 00:09:38,510
cluster essentially going to help us

00:09:33,740 --> 00:09:40,340
deploy an app so the platform do you run

00:09:38,510 --> 00:09:42,020
time is essentially running as an

00:09:40,340 --> 00:09:45,680
application but on discriminatees

00:09:42,020 --> 00:09:48,500
cluster now let's take a look at a

00:09:45,680 --> 00:09:52,670
specific namespace called C F workloads

00:09:48,500 --> 00:09:55,510
now within the CF close you'll see that

00:09:52,670 --> 00:09:58,610
this is where a lot of the applications

00:09:55,510 --> 00:10:01,730
that will be pushing into this cluster

00:09:58,610 --> 00:10:03,680
will get deployed we also have a store

00:10:01,730 --> 00:10:06,020
running on this cluster which will help

00:10:03,680 --> 00:10:10,270
you know which Cloud Foundry uses to

00:10:06,020 --> 00:10:13,550
build in grassroots so let's jump in and

00:10:10,270 --> 00:10:16,010
start deploying an application and see

00:10:13,550 --> 00:10:19,100
what the experience looks like so have a

00:10:16,010 --> 00:10:21,290
very sample go based application that's

00:10:19,100 --> 00:10:24,470
just going to show me the index at a

00:10:21,290 --> 00:10:26,990
given point in time and I'm already

00:10:24,470 --> 00:10:29,390
logged in to South laundry that is

00:10:26,990 --> 00:10:32,480
running on the kubernetes cluster over

00:10:29,390 --> 00:10:34,340
here so as you can see if I do my list

00:10:32,480 --> 00:10:37,040
of applications there are no apps

00:10:34,340 --> 00:10:39,770
currently running and my goal is to push

00:10:37,040 --> 00:10:43,190
this go based application into cloud

00:10:39,770 --> 00:10:46,010
foundry so what I'm gonna do is I'm just

00:10:43,190 --> 00:10:48,800
getting going to get into that directory

00:10:46,010 --> 00:10:51,860
that my application sits so it's called

00:10:48,800 --> 00:10:54,620
I'm calling it a test app here are all

00:10:51,860 --> 00:10:59,360
the files for that application and I'm

00:10:54,620 --> 00:11:01,040
just simply going to do CF push and I'm

00:10:59,360 --> 00:11:04,640
going to give it a specific application

00:11:01,040 --> 00:11:07,240
name or a ingress root so I'm going to

00:11:04,640 --> 00:11:07,240
call it test

00:11:07,630 --> 00:11:16,940
so in the back and if you will if we go

00:11:11,240 --> 00:11:21,350
back to the cluster you'll start seeing

00:11:16,940 --> 00:11:23,540
that the sia workload staging area

00:11:21,350 --> 00:11:25,070
starts just you know starts building up

00:11:23,540 --> 00:11:27,170
it will you'll start seeing some

00:11:25,070 --> 00:11:30,230
workloads getting created over here and

00:11:27,170 --> 00:11:33,740
the build service is going to basically

00:11:30,230 --> 00:11:36,320
take the application files push them in

00:11:33,740 --> 00:11:38,750
a docker file and upload it to my

00:11:36,320 --> 00:11:41,180
registry in my case I'm going to use

00:11:38,750 --> 00:11:43,190
docker hub as my system registry and

00:11:41,180 --> 00:11:46,370
you'll you know you know in a bit you'll

00:11:43,190 --> 00:11:50,150
start to see that one on my talker hub

00:11:46,370 --> 00:11:57,890
you know this new container image will

00:11:50,150 --> 00:12:01,990
be published so it takes a little maybe

00:11:57,890 --> 00:12:01,990
a minute or two to finish this set up

00:12:02,170 --> 00:12:07,280
okay so let's take a look at the

00:12:05,420 --> 00:12:10,120
applications you'll see that the test

00:12:07,280 --> 00:12:14,830
tab has been pushed into the cluster

00:12:10,120 --> 00:12:14,830
let's take a look at the cluster itself

00:12:15,700 --> 00:12:27,830
so if I go to my namespaces if I go to

00:12:21,620 --> 00:12:30,230
CF workloads you'll see the test app pod

00:12:27,830 --> 00:12:32,120
getting created you'll also see some of

00:12:30,230 --> 00:12:35,000
the stateful sets needed for that

00:12:32,120 --> 00:12:37,640
application to run you'll also see the

00:12:35,000 --> 00:12:40,550
services that were built as part of that

00:12:37,640 --> 00:12:43,400
application and an ingress and a network

00:12:40,550 --> 00:12:44,990
policy all of this happened without us

00:12:43,400 --> 00:12:47,480
telling communities what to do it

00:12:44,990 --> 00:12:50,120
created the defaults needed to

00:12:47,480 --> 00:12:53,900
effectively run this application on the

00:12:50,120 --> 00:12:57,760
communities classroom if I go back to my

00:12:53,900 --> 00:13:00,380
docker hub and just refresh this page

00:12:57,760 --> 00:13:03,380
you'll see that this particular image

00:13:00,380 --> 00:13:05,780
that I just you know the Cloud Foundry

00:13:03,380 --> 00:13:08,810
running all that Cuba dairies cluster

00:13:05,780 --> 00:13:11,000
actually push this image into my daugher

00:13:08,810 --> 00:13:14,660
hub two minutes ago without me having to

00:13:11,000 --> 00:13:17,510
build any docker files or create any you

00:13:14,660 --> 00:13:19,050
know or build any images all of this was

00:13:17,510 --> 00:13:22,560
automated within

00:13:19,050 --> 00:13:30,540
the californian instance let's take a

00:13:22,560 --> 00:13:32,930
look at the application itself she'll

00:13:30,540 --> 00:13:35,459
see this is a simple app that's running

00:13:32,930 --> 00:13:39,720
with a link with an ingress route

00:13:35,459 --> 00:13:43,589
running on that particular question so

00:13:39,720 --> 00:13:46,230
this is a very simple example of how CF

00:13:43,589 --> 00:13:48,300
push is going to help us build all these

00:13:46,230 --> 00:13:51,080
different communities components needed

00:13:48,300 --> 00:13:53,250
to effectively run that application

00:13:51,080 --> 00:13:55,649
apart from just you know pushing

00:13:53,250 --> 00:13:59,610
application running the container images

00:13:55,649 --> 00:14:01,380
building it deploying services etc the

00:13:59,610 --> 00:14:04,589
foundry is also going to help us manage

00:14:01,380 --> 00:14:06,330
logging systems so if I need to see the

00:14:04,589 --> 00:14:10,770
logs for that particular port or

00:14:06,330 --> 00:14:16,410
container I simply do CF logs for my app

00:14:10,770 --> 00:14:20,880
and we are going to start seeing the pod

00:14:16,410 --> 00:14:22,440
logs running in our terminal similarly

00:14:20,880 --> 00:14:25,589
scaling in our application is also

00:14:22,440 --> 00:14:29,779
pretty simple I can simply say CF scale

00:14:25,589 --> 00:14:34,080
and my instance count is going to be 2

00:14:29,779 --> 00:14:38,790
sorry I forgot to give it the name of

00:14:34,080 --> 00:14:40,050
the app and if we go back to our what

00:14:38,790 --> 00:14:42,720
this is going to do is basically create

00:14:40,050 --> 00:14:44,459
another pod for that particular it's

00:14:42,720 --> 00:14:47,490
going to pull the image from my talker

00:14:44,459 --> 00:14:50,310
hub and create a new pause so if you go

00:14:47,490 --> 00:14:52,170
back to workloads you'll now see that

00:14:50,310 --> 00:14:55,560
there are two pods running because we

00:14:52,170 --> 00:14:59,279
just scale it using CFS scale so this is

00:14:55,560 --> 00:15:02,370
just a few examples how utilizing the CF

00:14:59,279 --> 00:15:05,070
push or the CFA PIV are essentially

00:15:02,370 --> 00:15:07,860
wrapping multiple kubernetes api is

00:15:05,070 --> 00:15:10,560
behind a single CF api to run our

00:15:07,860 --> 00:15:15,440
application and effectively simplifying

00:15:10,560 --> 00:15:15,440

YouTube URL: https://www.youtube.com/watch?v=U9SGETf-A6A


