Title: CF Server-Less: CloudFoundry as a Platform for Server-less Computing
Publication date: 2016-09-30
Playlist: Cloud Foundry Summit Europe 2016
Description: 
	CF Server-Less: CloudFoundry as a Platform for Server-less Computing - Nima Kaviani & Michael Maximilien, IBM

As CloudFoundry matures to become the de-facto platform for enterprise cloud computing, it is no surprise that various types of computing services need to be supported. Server-less computing is a new name for various previous concepts or technologies, e.g. functors, functional programming. In this model of computing, applications are written as a series of small functions with clear input and output and can be executed in the cloud on demand. The difference between this approach and eg. micro-services is that the backend part only executes when invoked. In this talk we compare and contrast possible approaches to make CF a first class platform for server-less computing. We explore retrofitting the current CF app push model, leverage open source server less platforms like OpenWhisk, and investigate how the CF runtime (Diego) could be improved to support server less apps as on-off tasks.

About Nima Kaviani
Nima Kaviani is a Cloud Engineer with IBM Cloud Labs and also a member of Cloud Foundry's Diego Team. Nima has a PhD in computer science, and has been working in the area of multi-cloud and hybrid cloud as part of his PhD work. He has also been the developer of Cloud Foundry BlueDocs, and has been actively writing about the experience of developing Cloud Foundry.

About Michael (aka dr.max) Maximilien
My name is Michael Maximilien, better known as max or dr.max, and I am a computer scientist with IBM. At IBM Research Triangle Park, I was a principal engineer for the worldwide industry point-of-sale standard: JavaPOS. At IBM Research, some highlights include pioneering research on semantic Web services, mashups, and cloud computing, and platform-as-a-service. I joined the IBM Cloud Labs in 2014 and work closely with Pivotal Inc., to help make the Cloud Found the best PaaS.
Captions: 
	00:00:00,030 --> 00:00:10,530
okay welcome to comparing CF serverless

00:00:04,250 --> 00:00:11,820
environments this talk is gonna be

00:00:10,530 --> 00:00:13,349
interesting in the sense that it's

00:00:11,820 --> 00:00:18,090
different from what you've heard from

00:00:13,349 --> 00:00:20,910
serverless it's also something that

00:00:18,090 --> 00:00:23,090
started out of conversation in the

00:00:20,910 --> 00:00:28,380
hallways at pivotal like a lot of things

00:00:23,090 --> 00:00:31,349
tend to happen with CF I'm Max and this

00:00:28,380 --> 00:00:32,550
is my colleague Nima he actually

00:00:31,349 --> 00:00:36,540
probably could present everything

00:00:32,550 --> 00:00:42,329
because he did do most of the work he

00:00:36,540 --> 00:00:46,200
just happened to to be stuck with me so

00:00:42,329 --> 00:00:48,860
I'll try to make it short and sweet what

00:00:46,200 --> 00:00:51,770
we want to try to do is to discuss a

00:00:48,860 --> 00:00:56,160
series of question around service and

00:00:51,770 --> 00:00:59,460
also at the end kind of discuss also

00:00:56,160 --> 00:01:02,000
where we're heading as IBM you probably

00:00:59,460 --> 00:01:05,580
heard of us discussing open whisk and

00:01:02,000 --> 00:01:08,130
different service environments that that

00:01:05,580 --> 00:01:09,810
we are essentially trying to sell this

00:01:08,130 --> 00:01:11,070
is not gonna be one of those talk where

00:01:09,810 --> 00:01:14,040
we selling that as a matter of fact

00:01:11,070 --> 00:01:15,750
that's part of the challenge here so

00:01:14,040 --> 00:01:17,759
we'll get started and then you'll see at

00:01:15,750 --> 00:01:19,970
the end where we are trying to encourage

00:01:17,759 --> 00:01:24,479
you to participate

00:01:19,970 --> 00:01:26,759
so what is surveillance computing there

00:01:24,479 --> 00:01:29,939
are different definitions but at the end

00:01:26,759 --> 00:01:32,039
of the day it's just a web function

00:01:29,939 --> 00:01:36,170
right it's the ability to execute some

00:01:32,039 --> 00:01:39,450
kind of a remote function from your

00:01:36,170 --> 00:01:41,820
program hopefully it would be something

00:01:39,450 --> 00:01:44,369
that reacts to some event so for

00:01:41,820 --> 00:01:47,490
instance you have an IOT like some of

00:01:44,369 --> 00:01:48,869
the common demos around surveillance so

00:01:47,490 --> 00:01:50,729
you have a some kind of an internet of

00:01:48,869 --> 00:01:52,560
thing that's like telling your friend

00:01:50,729 --> 00:01:57,229
since the weather and then you want to

00:01:52,560 --> 00:01:59,600
execute some computation or on that and

00:01:57,229 --> 00:02:02,520
typically it would be lightweight

00:01:59,600 --> 00:02:04,860
stateless and hopefully cheap because

00:02:02,520 --> 00:02:07,229
you don't have to keep the container or

00:02:04,860 --> 00:02:08,819
the VM wherever that code is executing

00:02:07,229 --> 00:02:12,620
you can just execute it when you need it

00:02:08,819 --> 00:02:16,200
so it's very reactive even based and

00:02:12,620 --> 00:02:20,790
hopefully short and sweet sort of like

00:02:16,200 --> 00:02:22,530
CGI few people at pivotal when were

00:02:20,790 --> 00:02:24,269
chatting kind of compared it with that

00:02:22,530 --> 00:02:25,859
and I'm sure in the web they do the same

00:02:24,269 --> 00:02:28,590
I think it's a little bit different but

00:02:25,859 --> 00:02:30,570
it's similar so what are some questions

00:02:28,590 --> 00:02:33,450
around serving us and why did we start

00:02:30,570 --> 00:02:35,819
this work to compare them well obviously

00:02:33,450 --> 00:02:37,829
the first question being part of cf

00:02:35,819 --> 00:02:39,900
should be well why don't you just use

00:02:37,829 --> 00:02:41,790
Cloud Foundry all right right I mean we

00:02:39,900 --> 00:02:43,620
provide all these things and then people

00:02:41,790 --> 00:02:45,660
would say well with Cloud Foundry you

00:02:43,620 --> 00:02:47,519
spin up an app you can even do better

00:02:45,660 --> 00:02:49,260
because you'll have a log you'll have

00:02:47,519 --> 00:02:51,750
tons of information about what's going

00:02:49,260 --> 00:02:53,730
on why do you reinvent what we've

00:02:51,750 --> 00:02:55,799
already done would pass I see some

00:02:53,730 --> 00:02:59,549
people smiling because that's a common

00:02:55,799 --> 00:03:01,680
question I think it's a fair point but

00:02:59,549 --> 00:03:02,970
the people doing surveillance will tell

00:03:01,680 --> 00:03:05,790
you it's a little bit different to write

00:03:02,970 --> 00:03:07,620
because you can build a workflow around

00:03:05,790 --> 00:03:12,180
some of those web functions so if you

00:03:07,620 --> 00:03:14,970
look at our solution open wisk and even

00:03:12,180 --> 00:03:16,980
amazon's or as euros you'll see that the

00:03:14,970 --> 00:03:18,480
UI allows you to sort of set up the

00:03:16,980 --> 00:03:21,510
workflow around different functions

00:03:18,480 --> 00:03:23,069
that's a little bit harder to do with CF

00:03:21,510 --> 00:03:24,540
because then you'll have to have

00:03:23,069 --> 00:03:25,920
different apps and so on so it's a

00:03:24,540 --> 00:03:27,580
little bit different a different model

00:03:25,920 --> 00:03:29,110
for programming I think

00:03:27,580 --> 00:03:30,250
but there are still questions around

00:03:29,110 --> 00:03:32,770
that right like so what's the advantage

00:03:30,250 --> 00:03:34,990
what are the advantages of course cost

00:03:32,770 --> 00:03:36,310
performance from the perspective of the

00:03:34,990 --> 00:03:39,630
person operating a surveillance

00:03:36,310 --> 00:03:41,530
environment as well as the person using

00:03:39,630 --> 00:03:42,100
surveillance environment is a big

00:03:41,530 --> 00:03:44,910
question

00:03:42,100 --> 00:03:48,580
so can we can you kind of compare those

00:03:44,910 --> 00:03:51,940
and then of course could you just use CF

00:03:48,580 --> 00:03:55,300
right that's a typical question so our

00:03:51,940 --> 00:03:58,330
goals is not to make another CF proposal

00:03:55,300 --> 00:04:00,010
because in some ways we want this to be

00:03:58,330 --> 00:04:02,650
separate from CF although there's some

00:04:00,010 --> 00:04:04,570
relationship and we also can't possibly

00:04:02,650 --> 00:04:07,480
explore all service environment even

00:04:04,570 --> 00:04:11,410
though the solution that we have once we

00:04:07,480 --> 00:04:13,480
make it open source and we intend to we

00:04:11,410 --> 00:04:15,610
will you will be able to try other

00:04:13,480 --> 00:04:17,650
service environments okay so that's

00:04:15,610 --> 00:04:19,780
that's part of it and again of course

00:04:17,650 --> 00:04:22,210
you know the big problem with this talk

00:04:19,780 --> 00:04:24,940
I can tell you up to right before

00:04:22,210 --> 00:04:27,720
walking to this door I had people from

00:04:24,940 --> 00:04:29,620
IBM needing to wanna see our slides

00:04:27,720 --> 00:04:31,840
because they want to make sure that

00:04:29,620 --> 00:04:33,700
we're not trying to favor things and of

00:04:31,840 --> 00:04:35,290
course since we're IDM we're comparing

00:04:33,700 --> 00:04:37,090
things how does it look

00:04:35,290 --> 00:04:39,070
so you'll see we have to dumb down

00:04:37,090 --> 00:04:41,169
things a little bit so you'll have to go

00:04:39,070 --> 00:04:43,360
run the experiments yourself so that you

00:04:41,169 --> 00:04:45,250
can compare them if you want we actually

00:04:43,360 --> 00:04:47,230
look pretty good but we're not the best

00:04:45,250 --> 00:04:50,440
in every category so it's kind of like

00:04:47,230 --> 00:04:53,370
okay yeah I can't you can say that so

00:04:50,440 --> 00:04:53,370
maybe I'm in trouble already

00:04:53,400 --> 00:05:00,070
so just hurt you just at the best for it

00:04:56,290 --> 00:05:03,340
you know it's also enough not a full

00:05:00,070 --> 00:05:06,340
suite of tests I think that would take a

00:05:03,340 --> 00:05:08,770
longer effort but it's a it's a good

00:05:06,340 --> 00:05:12,669
beginning so the way I like to say it is

00:05:08,770 --> 00:05:14,830
you know we used to be as a company and

00:05:12,669 --> 00:05:17,229
we're trying to get there where you know

00:05:14,830 --> 00:05:19,479
we not only participated but we led we

00:05:17,229 --> 00:05:21,250
were the leaders and we would help lead

00:05:19,479 --> 00:05:23,290
efforts and I think that's where we

00:05:21,250 --> 00:05:26,050
always formalize we want to help lead

00:05:23,290 --> 00:05:27,910
the movement so think of this as

00:05:26,050 --> 00:05:29,900
potentially a contribution in that

00:05:27,910 --> 00:05:32,270
direction

00:05:29,900 --> 00:05:34,160
so the approach that we're taking is to

00:05:32,270 --> 00:05:37,190
essentially try to figure out which is

00:05:34,160 --> 00:05:39,500
best but by defining some experiments

00:05:37,190 --> 00:05:41,540
and then running those experiments and

00:05:39,500 --> 00:05:45,530
sharing the results in repeating so very

00:05:41,540 --> 00:05:49,400
simple very classic you know in some

00:05:45,530 --> 00:05:53,120
ways a scientific method of which both

00:05:49,400 --> 00:05:55,310
Nia and I have background and you know

00:05:53,120 --> 00:05:58,400
academia so we're trying to do the same

00:05:55,310 --> 00:06:02,180
but not fully with the rigor of a

00:05:58,400 --> 00:06:03,770
scientific paper so with that I'll pass

00:06:02,180 --> 00:06:05,750
it to NEMA but first thing I want to

00:06:03,770 --> 00:06:08,780
mention is that we've looked at all of

00:06:05,750 --> 00:06:11,360
these environments so as you're

00:06:08,780 --> 00:06:13,850
obviously is the Microsoft functions

00:06:11,360 --> 00:06:16,790
it's in beta open whisk which is our

00:06:13,850 --> 00:06:18,410
solution which isn't in beta also lambda

00:06:16,790 --> 00:06:22,100
is the only one that's public right now

00:06:18,410 --> 00:06:23,570
iron IO I'm not sure they are fully beta

00:06:22,100 --> 00:06:25,580
but they're certainly part of it and

00:06:23,570 --> 00:06:28,520
then this thing in the middle called C

00:06:25,580 --> 00:06:30,620
observable s that a couple of people at

00:06:28,520 --> 00:06:34,880
pivotal one in particular he doesn't

00:06:30,620 --> 00:06:38,780
want to be named I work with him on

00:06:34,880 --> 00:06:40,850
Bosch so you know that actually hacked

00:06:38,780 --> 00:06:42,950
up this version so with that I'll pass

00:06:40,850 --> 00:06:44,180
it to Nima who will talk about the

00:06:42,950 --> 00:06:49,160
result but he actually you're gonna do a

00:06:44,180 --> 00:06:55,670
cool demo to herself thank you Thank You

00:06:49,160 --> 00:06:57,950
max so Nima and cavion and i'm a

00:06:55,670 --> 00:07:01,190
contributor to diego run time full time

00:06:57,950 --> 00:07:03,230
and this is what we did on the side to

00:07:01,190 --> 00:07:06,380
understand socio serverless or basically

00:07:03,230 --> 00:07:08,000
several systems as a whole and so as max

00:07:06,380 --> 00:07:09,530
mentioned when we started thinking about

00:07:08,000 --> 00:07:11,960
understanding these different several

00:07:09,530 --> 00:07:13,400
systems we decided that we wanted to

00:07:11,960 --> 00:07:15,680
have a way to see what the underlying

00:07:13,400 --> 00:07:19,070
architecture is like and that's one of

00:07:15,680 --> 00:07:20,720
the reasons we went about implementing

00:07:19,070 --> 00:07:22,880
CF surveillance so out of the

00:07:20,720 --> 00:07:23,510
discussions that we had with folks at

00:07:22,880 --> 00:07:25,040
people

00:07:23,510 --> 00:07:27,530
we decided that okay let's implement

00:07:25,040 --> 00:07:29,450
something on top of Cloud Foundry that

00:07:27,530 --> 00:07:31,370
more or less emulates the behavior of a

00:07:29,450 --> 00:07:33,740
surveillance system and then based on

00:07:31,370 --> 00:07:35,390
the lessons that we learn we go and

00:07:33,740 --> 00:07:39,320
understand what is the overall behavior

00:07:35,390 --> 00:07:41,330
of several systems in general so we

00:07:39,320 --> 00:07:42,800
implemented CF surveillance as an

00:07:41,330 --> 00:07:45,560
application on top

00:07:42,800 --> 00:07:48,139
Cloud Foundry which can actually turn

00:07:45,560 --> 00:07:50,090
off and turn on other applications so if

00:07:48,139 --> 00:07:52,190
you have a function that you want to run

00:07:50,090 --> 00:07:53,389
and if you want it to be server list you

00:07:52,190 --> 00:07:55,340
can deploy it as a Cloud Foundry

00:07:53,389 --> 00:07:57,889
application and then there's a layer on

00:07:55,340 --> 00:07:59,870
top that manages that application when

00:07:57,889 --> 00:08:01,879
the requests come to that application it

00:07:59,870 --> 00:08:04,039
goes on and starts the application bring

00:08:01,879 --> 00:08:06,080
the application up responds to your

00:08:04,039 --> 00:08:08,300
request and then if the application is

00:08:06,080 --> 00:08:10,729
ideal for some some time it takes the

00:08:08,300 --> 00:08:12,379
application down and by doing it by just

00:08:10,729 --> 00:08:14,599
turning on and turning off of the

00:08:12,379 --> 00:08:17,419
application it saves on resources and

00:08:14,599 --> 00:08:19,250
essentially saves on mine so once we did

00:08:17,419 --> 00:08:20,870
it then we realized okay so that's a

00:08:19,250 --> 00:08:22,370
very basic implementation of

00:08:20,870 --> 00:08:24,860
surveillance on top of client foundry

00:08:22,370 --> 00:08:26,780
and now what is the lessons that we

00:08:24,860 --> 00:08:28,310
learned from it and what experiments can

00:08:26,780 --> 00:08:31,909
we conduct in order to understand how

00:08:28,310 --> 00:08:33,740
the other systems behave so we went on

00:08:31,909 --> 00:08:35,750
basically thought about the possible

00:08:33,740 --> 00:08:37,550
experiments and we built a system that

00:08:35,750 --> 00:08:40,399
would allow us to run those experiments

00:08:37,550 --> 00:08:42,800
so I'm gonna show you a demo of what the

00:08:40,399 --> 00:08:44,360
system is but basically before that I'm

00:08:42,800 --> 00:08:46,130
gonna tell you what experiments we came

00:08:44,360 --> 00:08:47,870
up with so the first thing that we

00:08:46,130 --> 00:08:49,940
decided to do was to understand the

00:08:47,870 --> 00:08:51,860
behavior of these several systems on

00:08:49,940 --> 00:08:54,290
their high throughput and low throughput

00:08:51,860 --> 00:08:55,970
loads and so basically if you have a

00:08:54,290 --> 00:08:59,180
function that is constantly receiving

00:08:55,970 --> 00:09:01,070
calls then what is the behavior of the

00:08:59,180 --> 00:09:02,570
system and if it's only frequently

00:09:01,070 --> 00:09:04,370
receiving calls then what is the

00:09:02,570 --> 00:09:06,470
behavior system then why is this

00:09:04,370 --> 00:09:08,029
important it's because essentially when

00:09:06,470 --> 00:09:10,070
you have a function that runs as a

00:09:08,029 --> 00:09:11,870
surrealist function what happens is that

00:09:10,070 --> 00:09:14,170
the code gets loaded into a container

00:09:11,870 --> 00:09:17,660
right and that container has a lifetime

00:09:14,170 --> 00:09:19,940
usually CF cell like several systems are

00:09:17,660 --> 00:09:21,920
short-lived containers because they want

00:09:19,940 --> 00:09:23,500
to save on resources they kill the

00:09:21,920 --> 00:09:26,630
in the container as soon as the function

00:09:23,500 --> 00:09:28,130
has done with this job right so the

00:09:26,630 --> 00:09:31,220
question is how long this container

00:09:28,130 --> 00:09:33,529
stays around for it to be responsive and

00:09:31,220 --> 00:09:35,630
at the same time be cheap there will be

00:09:33,529 --> 00:09:37,850
implemented our approach as I mentioned

00:09:35,630 --> 00:09:40,730
we kill the container if it sits idle

00:09:37,850 --> 00:09:42,320
immediately the problem with it is that

00:09:40,730 --> 00:09:44,450
the next time you call that function

00:09:42,320 --> 00:09:46,339
because you have to bring the container

00:09:44,450 --> 00:09:48,500
back up and redeploy the code on all of

00:09:46,339 --> 00:09:50,240
that it's going to take time right so

00:09:48,500 --> 00:09:52,070
the question for high throughput versus

00:09:50,240 --> 00:09:54,680
low throughput was what is the approach

00:09:52,070 --> 00:09:55,950
that other sterilized technologies are

00:09:54,680 --> 00:09:57,660
using the other

00:09:55,950 --> 00:09:59,670
that we did were like typical

00:09:57,660 --> 00:10:02,300
performance measurements and memory

00:09:59,670 --> 00:10:06,120
intensive computation and we did it by

00:10:02,300 --> 00:10:09,120
doing matrix multiplications to 300 by

00:10:06,120 --> 00:10:12,690
300 matrix we multiplied them and we

00:10:09,120 --> 00:10:14,730
this is unknown memory intensive problem

00:10:12,690 --> 00:10:16,650
and so we measured the amount of time

00:10:14,730 --> 00:10:19,140
that it actually takes for different

00:10:16,650 --> 00:10:22,410
functions to respond for CPU intensive

00:10:19,140 --> 00:10:25,350
we did finding of prime numbers and

00:10:22,410 --> 00:10:27,030
that's a very common and CPU intensive

00:10:25,350 --> 00:10:28,830
function we tried the across all the

00:10:27,030 --> 00:10:30,720
environments that we mentioned we also

00:10:28,830 --> 00:10:32,640
looked into container managed management

00:10:30,720 --> 00:10:34,710
behavior basically the same thing on I

00:10:32,640 --> 00:10:36,240
mentioned whether the containers stay

00:10:34,710 --> 00:10:37,770
around whether they get killed

00:10:36,240 --> 00:10:40,260
immediately or whether there's another

00:10:37,770 --> 00:10:42,500
strategy involved so for all the

00:10:40,260 --> 00:10:45,860
experiments that we did we actually

00:10:42,500 --> 00:10:48,450
deployed because we wanted the

00:10:45,860 --> 00:10:50,400
deployments to be similar and close so

00:10:48,450 --> 00:10:52,800
we gave every single function in every

00:10:50,400 --> 00:10:55,680
environment 512 megabytes of memory and

00:10:52,800 --> 00:10:57,540
we deployed all of them in US East

00:10:55,680 --> 00:10:59,130
because when you go to this cloud

00:10:57,540 --> 00:11:02,600
platforms obviously you can choose where

00:10:59,130 --> 00:11:05,190
your functions are deployed except for

00:11:02,600 --> 00:11:07,740
openness which is I think only available

00:11:05,190 --> 00:11:10,200
in Dallas so we dope a risk we didn't

00:11:07,740 --> 00:11:11,820
have that much option so then we started

00:11:10,200 --> 00:11:13,260
launching requests to these functions

00:11:11,820 --> 00:11:15,300
and we did two ways of launching

00:11:13,260 --> 00:11:18,180
requests sequentially so that every

00:11:15,300 --> 00:11:19,980
request hits the end point after the

00:11:18,180 --> 00:11:21,810
previous one is finished and then in

00:11:19,980 --> 00:11:23,640
parallel so all of the requests hit the

00:11:21,810 --> 00:11:26,610
endpoint at the same time and we did it

00:11:23,640 --> 00:11:28,650
with 100 requests and we basically did

00:11:26,610 --> 00:11:30,300
it three times and we did a ramp up

00:11:28,650 --> 00:11:32,370
period so that we actually have

00:11:30,300 --> 00:11:35,880
everything up and running and before

00:11:32,370 --> 00:11:38,640
starting to collect it okay so for the

00:11:35,880 --> 00:11:42,120
demo this is basically what you need to

00:11:38,640 --> 00:11:44,970
supply to the to the basic code that

00:11:42,120 --> 00:11:47,850
runs the experiment you define all the

00:11:44,970 --> 00:11:49,950
different environments you define the

00:11:47,850 --> 00:11:53,310
endpoints and you define the type of

00:11:49,950 --> 00:11:55,170
headers and the type of bodies that are

00:11:53,310 --> 00:11:57,570
gonna be passed to these HTML calls

00:11:55,170 --> 00:12:00,000
because at the end of the day DCF these

00:11:57,570 --> 00:12:03,840
sterilized functions are just HTML like

00:12:00,000 --> 00:12:05,970
HTTP endpoints that you hit and once you

00:12:03,840 --> 00:12:08,040
provide everything in the right format

00:12:05,970 --> 00:12:09,989
for those functions it's just a simple

00:12:08,040 --> 00:12:11,699
HTTP call

00:12:09,989 --> 00:12:15,429
so once you provide all those

00:12:11,699 --> 00:12:18,109
information that what happens is

00:12:15,429 --> 00:12:19,819
and you can define the number of times

00:12:18,109 --> 00:12:21,109
that actually a function is called and

00:12:19,819 --> 00:12:22,819
you can define the time between

00:12:21,109 --> 00:12:25,759
intervals basically every two

00:12:22,819 --> 00:12:26,989
consecutive calls and it just runs it

00:12:25,759 --> 00:12:29,269
against all the environments that you

00:12:26,989 --> 00:12:31,399
have and collects the data and reports

00:12:29,269 --> 00:12:33,709
the data back so as you can see it comes

00:12:31,399 --> 00:12:35,559
up with some numbers on how much time it

00:12:33,709 --> 00:12:38,629
takes for every one of the environments

00:12:35,559 --> 00:12:40,069
so with that we started running all

00:12:38,629 --> 00:12:41,329
these experiments against all the

00:12:40,069 --> 00:12:43,669
environments that I mentioned and we

00:12:41,329 --> 00:12:46,730
went to all of these environment open

00:12:43,669 --> 00:12:48,230
whiskers or lambda I ran IO and deployed

00:12:46,730 --> 00:12:49,669
these functions the functions that we

00:12:48,230 --> 00:12:52,609
implemented we implemented the functions

00:12:49,669 --> 00:12:55,639
in Python and I believe the code for the

00:12:52,609 --> 00:12:56,959
experiments and also for the test suite

00:12:55,639 --> 00:12:59,239
is going to be released hopefully

00:12:56,959 --> 00:13:02,299
sometime soon we are planning to make it

00:12:59,239 --> 00:13:04,879
open source so let's look into their

00:13:02,299 --> 00:13:07,910
results and I basically for each of the

00:13:04,879 --> 00:13:09,739
experiments I'm only showing one of the

00:13:07,910 --> 00:13:13,129
environments the one that I thought was

00:13:09,739 --> 00:13:14,299
more interesting if you look at it for a

00:13:13,129 --> 00:13:16,279
high-throughput function this is

00:13:14,299 --> 00:13:18,379
basically a simple echo function you hit

00:13:16,279 --> 00:13:21,049
the endpoint and it responds with a

00:13:18,379 --> 00:13:23,720
simple like hello world you see that the

00:13:21,049 --> 00:13:25,459
all the 100 requests were successful and

00:13:23,720 --> 00:13:27,589
the average time it took for every

00:13:25,459 --> 00:13:30,649
request to come back was around like 180

00:13:27,589 --> 00:13:34,369
milliseconds or 200 milliseconds and

00:13:30,649 --> 00:13:36,259
there are spikes in the in the

00:13:34,369 --> 00:13:39,019
experiments and those are the ones that

00:13:36,259 --> 00:13:41,329
actually took longer for the for the

00:13:39,019 --> 00:13:43,429
container to be set up so the spikes

00:13:41,329 --> 00:13:45,470
that you see are the ones that the

00:13:43,429 --> 00:13:47,149
container wasn't there so it took

00:13:45,470 --> 00:13:48,649
probably like close to seven minutes to

00:13:47,149 --> 00:13:50,540
create the container put the code on it

00:13:48,649 --> 00:13:52,579
and then respond to the first call but

00:13:50,540 --> 00:13:53,929
once that first goal was through the

00:13:52,579 --> 00:13:57,259
remaining calls were actually much

00:13:53,929 --> 00:14:02,389
faster that's why they're the average as

00:13:57,259 --> 00:14:04,369
there are 200 milliseconds so that was

00:14:02,389 --> 00:14:06,529
the sequential one right where every

00:14:04,369 --> 00:14:09,049
request came after the end the previous

00:14:06,529 --> 00:14:11,239
one then we did it in parallel and azure

00:14:09,049 --> 00:14:14,989
was very interesting because as you see

00:14:11,239 --> 00:14:16,549
there is like the Steep basically

00:14:14,989 --> 00:14:19,309
response time like the response time

00:14:16,549 --> 00:14:21,739
increases kind of linearly as the number

00:14:19,309 --> 00:14:24,980
of requests as as we basically push more

00:14:21,739 --> 00:14:28,700
more requests or basically make more

00:14:24,980 --> 00:14:31,550
calls and it seems like azure does now

00:14:28,700 --> 00:14:33,500
good job bringing up new containers in

00:14:31,550 --> 00:14:35,510
order to be able to equally balanced the

00:14:33,500 --> 00:14:37,910
look I don't know exactly what is behind

00:14:35,510 --> 00:14:39,320
it but if they had enough resources to

00:14:37,910 --> 00:14:41,690
respond to these calls this should be

00:14:39,320 --> 00:14:42,860
like a flatline this is what we observed

00:14:41,690 --> 00:14:45,800
in some of the other the better

00:14:42,860 --> 00:14:47,540
environments usually no matter how many

00:14:45,800 --> 00:14:49,370
how many requests review launched

00:14:47,540 --> 00:14:51,410
towards an endpoint it was kind of a

00:14:49,370 --> 00:14:55,370
flatline but for a juror it actually

00:14:51,410 --> 00:14:57,320
spiked up so for low throughput we

00:14:55,370 --> 00:15:00,350
actually made it so that the same echo

00:14:57,320 --> 00:15:01,700
function rather than calling it and

00:15:00,350 --> 00:15:03,680
basically one after another

00:15:01,700 --> 00:15:05,690
exactly we made it so that there was

00:15:03,680 --> 00:15:07,370
like a five minute delay or one minute

00:15:05,690 --> 00:15:09,320
delay between every every two

00:15:07,370 --> 00:15:10,790
consecutive costs their way we

00:15:09,320 --> 00:15:14,210
implemented CF serverless

00:15:10,790 --> 00:15:16,430
was that if the the endpoint was

00:15:14,210 --> 00:15:18,470
inactive for 30 seconds we would kill

00:15:16,430 --> 00:15:20,270
the container we wanted to see what the

00:15:18,470 --> 00:15:22,220
other platforms do and we actually

00:15:20,270 --> 00:15:24,080
played with the time here I'm I'm

00:15:22,220 --> 00:15:25,850
talking specifically about the one

00:15:24,080 --> 00:15:27,560
minute delay but we played with five

00:15:25,850 --> 00:15:29,270
minute delay and ten minute delay to see

00:15:27,560 --> 00:15:30,980
whether changing the time would actually

00:15:29,270 --> 00:15:34,040
affect the time that it takes for the

00:15:30,980 --> 00:15:36,020
container to respond that and what CF

00:15:34,040 --> 00:15:37,520
serverless we knew what the behavior was

00:15:36,020 --> 00:15:39,290
the container was killed so every

00:15:37,520 --> 00:15:41,330
request took around like 10 second to

00:15:39,290 --> 00:15:43,040
respond but we realized that for other

00:15:41,330 --> 00:15:44,870
environments they're actually not

00:15:43,040 --> 00:15:46,340
killing the container what they do is

00:15:44,870 --> 00:15:48,500
that they freeze the container so they

00:15:46,340 --> 00:15:51,200
use the C group freezing and it allows

00:15:48,500 --> 00:15:53,150
them to free up resources but keep the

00:15:51,200 --> 00:15:54,650
container on the disk so the next time

00:15:53,150 --> 00:15:57,530
it's called it's actually slower

00:15:54,650 --> 00:15:59,810
slightly slower but not drastically slow

00:15:57,530 --> 00:16:01,430
so it's not in the order of like 7 min

00:15:59,810 --> 00:16:04,640
like seconds it's usually in the earth

00:16:01,430 --> 00:16:07,460
in the upper end of you know hundreds of

00:16:04,640 --> 00:16:09,080
milliseconds before for all the other

00:16:07,460 --> 00:16:11,150
environments that we observe open misc

00:16:09,080 --> 00:16:12,980
lambda and nature for irony oh it's

00:16:11,150 --> 00:16:16,210
different the thing with iron IO is that

00:16:12,980 --> 00:16:19,670
it's specifically designed for running

00:16:16,210 --> 00:16:22,280
tasks that are supposed to run only once

00:16:19,670 --> 00:16:24,410
but are like heavy in computation so

00:16:22,280 --> 00:16:25,940
their implementation was very similar to

00:16:24,410 --> 00:16:28,370
several s they would bring up a

00:16:25,940 --> 00:16:29,990
container launch everything into it do

00:16:28,370 --> 00:16:31,940
that thing and then kill it and throw it

00:16:29,990 --> 00:16:34,880
away so it would take a lot longer for

00:16:31,940 --> 00:16:36,440
every single call from our nao to come

00:16:34,880 --> 00:16:38,630
back and that was an interesting

00:16:36,440 --> 00:16:41,149
comparison or observation compared to

00:16:38,630 --> 00:16:44,749
the other environments

00:16:41,149 --> 00:16:47,749
so we did the memory intensive function

00:16:44,749 --> 00:16:49,220
experiment and in a sequential Casey

00:16:47,749 --> 00:16:50,480
this is the usual behavior that you

00:16:49,220 --> 00:16:52,399
would expect to see from myself and

00:16:50,480 --> 00:16:56,300
several is function a flat line

00:16:52,399 --> 00:16:58,129
the average was around you know 1.3

00:16:56,300 --> 00:17:03,860
seconds and all the requests were

00:16:58,129 --> 00:17:06,650
successful now we did it in parallel and

00:17:03,860 --> 00:17:08,419
the interesting one was again Azra one

00:17:06,650 --> 00:17:10,220
thing we noticed was that out of the

00:17:08,419 --> 00:17:12,709
calls all the calls that we made only

00:17:10,220 --> 00:17:15,650
15% of them were successful and the

00:17:12,709 --> 00:17:18,049
other ones basically faked resources got

00:17:15,650 --> 00:17:19,819
congested for some reason and they

00:17:18,049 --> 00:17:21,350
couldn't respond immediately it was

00:17:19,819 --> 00:17:23,329
usually a time out or out of memory

00:17:21,350 --> 00:17:25,789
error that we received when we hit the

00:17:23,329 --> 00:17:27,740
app for the other environments it

00:17:25,789 --> 00:17:31,720
usually was better it was again kind of

00:17:27,740 --> 00:17:31,720
like a flat line

00:17:32,009 --> 00:17:37,120
the other experiment was CPU intensive

00:17:34,929 --> 00:17:39,100
function so calculating prime numbers

00:17:37,120 --> 00:17:41,499
and what I'm showing you here is a

00:17:39,100 --> 00:17:43,210
sequential case for CF server list and

00:17:41,499 --> 00:17:45,039
as you can see except for the sparks

00:17:43,210 --> 00:17:48,330
that I explained earlier it's pretty

00:17:45,039 --> 00:17:53,700
much a flatline 100 percent success and

00:17:48,330 --> 00:17:57,519
each calculation took around 2.5 seconds

00:17:53,700 --> 00:18:00,220
and we did it in parallel for open risk

00:17:57,519 --> 00:18:02,230
and you see that as requests coming it

00:18:00,220 --> 00:18:04,389
is it goes up like this

00:18:02,230 --> 00:18:06,190
this pattern kinda is similar in all

00:18:04,389 --> 00:18:08,679
platforms it goes up but not that much

00:18:06,190 --> 00:18:11,950
the overall average was like around I

00:18:08,679 --> 00:18:15,220
think this one is actually 26 seconds so

00:18:11,950 --> 00:18:17,320
I think I am I have a typo here in the

00:18:15,220 --> 00:18:19,690
case of sequential it actually took 26

00:18:17,320 --> 00:18:21,970
seconds and then in the case of parallel

00:18:19,690 --> 00:18:24,039
the response was around 8 seconds to 9

00:18:21,970 --> 00:18:28,659
seconds for the platforms that we

00:18:24,039 --> 00:18:30,309
experimented with so general

00:18:28,659 --> 00:18:32,470
observations about the experiments that

00:18:30,309 --> 00:18:34,590
we ran one thing that we realized was

00:18:32,470 --> 00:18:36,549
timeout was an issue and a lot of these

00:18:34,590 --> 00:18:37,960
conveyor ments even though they claimed

00:18:36,549 --> 00:18:38,529
that they actually have configurable

00:18:37,960 --> 00:18:41,139
timeouts

00:18:38,529 --> 00:18:44,710
they really don't for example in Amazon

00:18:41,139 --> 00:18:46,360
that default is 30 seconds but if you

00:18:44,710 --> 00:18:49,179
change it to 5 minutes it's still 30

00:18:46,360 --> 00:18:52,090
seconds like whatever you do it's 30

00:18:49,179 --> 00:18:55,330
seconds for forager

00:18:52,090 --> 00:18:57,490
it's not configurable yet I think in

00:18:55,330 --> 00:18:59,259
iteration 22 that came out like a few

00:18:57,490 --> 00:19:00,999
days ago they made it so that you can

00:18:59,259 --> 00:19:03,490
configure it they claim that the default

00:19:00,999 --> 00:19:05,289
is 5 minutes but usually what we observe

00:19:03,490 --> 00:19:06,789
was that but as we hit these end points

00:19:05,289 --> 00:19:08,070
it would usually time out a lot faster

00:19:06,789 --> 00:19:11,919
than 5 minutes

00:19:08,070 --> 00:19:14,200
so yes still that kind of a sketchy the

00:19:11,919 --> 00:19:18,759
other thing is that there is this notion

00:19:14,200 --> 00:19:20,980
of money versus time right and the

00:19:18,759 --> 00:19:22,570
general idea is that if you spend more

00:19:20,980 --> 00:19:24,549
money obviously you're going to be able

00:19:22,570 --> 00:19:26,740
to do things faster and if you think

00:19:24,549 --> 00:19:28,840
about several s even though there are

00:19:26,740 --> 00:19:31,450
like subtle differences between several

00:19:28,840 --> 00:19:33,369
s and a long-running long-running

00:19:31,450 --> 00:19:34,990
program at the end they are more or less

00:19:33,369 --> 00:19:37,749
the same so if you keep the container

00:19:34,990 --> 00:19:39,340
around for longer and if you provide

00:19:37,749 --> 00:19:40,960
more containers and more resources

00:19:39,340 --> 00:19:43,620
obviously things are going to run fast

00:19:40,960 --> 00:19:45,240
so if you're willing to to

00:19:43,620 --> 00:19:48,750
basically spend more money than

00:19:45,240 --> 00:19:50,730
essentially you can have good number of

00:19:48,750 --> 00:19:54,390
containers running your functions in a

00:19:50,730 --> 00:19:57,360
long-running model and you can basically

00:19:54,390 --> 00:19:58,650
create boilerplate code along all the

00:19:57,360 --> 00:20:00,270
orchestration that you need between

00:19:58,650 --> 00:20:02,400
these functions and you can get

00:20:00,270 --> 00:20:04,380
something similar the the point which

00:20:02,400 --> 00:20:05,190
the observer several is in general is

00:20:04,380 --> 00:20:07,710
that it's going to help you

00:20:05,190 --> 00:20:10,529
significantly in terms of resources that

00:20:07,710 --> 00:20:13,409
in terms of costs of using resources

00:20:10,529 --> 00:20:15,510
that you would use so the last thing is

00:20:13,409 --> 00:20:18,659
that the experiments that we are running

00:20:15,510 --> 00:20:20,850
we're actually moving towards creating

00:20:18,659 --> 00:20:22,049
it something that everybody can use and

00:20:20,850 --> 00:20:24,110
everybody can run against their

00:20:22,049 --> 00:20:26,880
environments we're hoping to be able to

00:20:24,110 --> 00:20:29,360
release both the code for running the

00:20:26,880 --> 00:20:32,580
experiments and also the actual

00:20:29,360 --> 00:20:34,020
benchmarks open source so that in case

00:20:32,580 --> 00:20:36,809
you're interested you can try it with

00:20:34,020 --> 00:20:40,679
your with your own environments I think

00:20:36,809 --> 00:20:43,799
with that yeah so one thing we want to

00:20:40,679 --> 00:20:45,000
discuss they I think we maybe I should

00:20:43,799 --> 00:20:46,620
have stress is that a lot of these

00:20:45,000 --> 00:20:48,570
environment is still in beta so like

00:20:46,620 --> 00:20:53,610
Pocono a schism beta Azure is in beta

00:20:48,570 --> 00:20:56,190
and IO I'm not so sure we try to get

00:20:53,610 --> 00:20:58,200
Google functions and those not only send

00:20:56,190 --> 00:21:00,990
beta but you can't even use it so the

00:20:58,200 --> 00:21:03,809
point is numbers we're showing you here

00:21:00,990 --> 00:21:06,870
or beta except for lambda so keep that

00:21:03,809 --> 00:21:09,450
in mind but what we want to do in terms

00:21:06,870 --> 00:21:11,760
of you know where do we go from here is

00:21:09,450 --> 00:21:14,940
to release something like a spec server

00:21:11,760 --> 00:21:17,700
less so to that extent what we did is to

00:21:14,940 --> 00:21:21,110
we've got a draft it is actually a

00:21:17,700 --> 00:21:23,250
document to you shut down that Google

00:21:21,110 --> 00:21:26,399
noise send us email we'll send it to you

00:21:23,250 --> 00:21:30,000
and this is to essentially say there's

00:21:26,399 --> 00:21:32,309
specs over les is something we think

00:21:30,000 --> 00:21:34,710
could be useful and there's gonna be a

00:21:32,309 --> 00:21:37,289
way to run some experiments around it

00:21:34,710 --> 00:21:38,940
and obviously part of the idea is not

00:21:37,289 --> 00:21:41,340
only to release the experiment runners

00:21:38,940 --> 00:21:43,649
and the experiments publicly but to get

00:21:41,340 --> 00:21:45,960
everybody tried on their own

00:21:43,649 --> 00:21:47,580
environments so that you know maybe you

00:21:45,960 --> 00:21:50,250
can optimize it and part of the idea of

00:21:47,580 --> 00:21:55,799
creating and I like a spec is you know

00:21:50,250 --> 00:21:57,400
how when Intel releases new CPUs then

00:21:55,799 --> 00:22:01,450
you can run

00:21:57,400 --> 00:22:05,650
or different benchmarks to kind of

00:22:01,450 --> 00:22:07,980
compare the the CPUs and I think server

00:22:05,650 --> 00:22:10,810
less environment might have a similar

00:22:07,980 --> 00:22:13,570
potential so we'd love to hear your

00:22:10,810 --> 00:22:16,630
opinion about this if you are interested

00:22:13,570 --> 00:22:19,720
we'd love to have you know collaboration

00:22:16,630 --> 00:22:21,790
this is very early and you know in the

00:22:19,720 --> 00:22:24,670
stage of where a server less computing

00:22:21,790 --> 00:22:28,600
is gonna go but this is potentially a

00:22:24,670 --> 00:22:31,240
way for you to get engage in and produce

00:22:28,600 --> 00:22:32,350
something that would be valuable so with

00:22:31,240 --> 00:22:34,480
that we thank you

00:22:32,350 --> 00:22:37,290
you'll find us on Twitter here and then

00:22:34,480 --> 00:22:37,290
we can take some questions

00:22:37,950 --> 00:22:43,190

YouTube URL: https://www.youtube.com/watch?v=dTOfCu3aoLM


