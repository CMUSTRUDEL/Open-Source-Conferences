Title: Update on Cloud Foundry Isolation Segments (formerly Elastic Clusters) - Dieu Cao, Pivotal
Publication date: 2016-09-30
Playlist: Cloud Foundry Summit Europe 2016
Description: 
	Update on Cloud Foundry Isolation Segments (formerly Elastic Clusters) - Dieu Cao, Pivotal

Motivation: CF Operators want the ability to isolate workloads across separate networks, iaasâ€™s, and datacenters to meet compliance, availability, and cost requirements. CF Operators expect to achieve this isolation without the added overhead of managing multiple full installs of Cloud Foundry.

Goal: The goal of this set of features is to enable the cloud controller to address workloads to isolated compute, routing, and logging components and assign application placement by mapping a Cloud Foundry Space to an Isolation Segment. We intend to support a new bosh deployment, a Cloud Foundry Isolation Segment, that may include compute, routing, and logging components. The cloud controller will have added functionality to allow operators to manage segments of CF capacity via the api.

About Dieu Cao
Dieu Cao is Director of Cloud Foundry Product Management at Pivotal focused on the strategic direction of Pivotal's Elastic Runtime product. | | She is also currently serving as the PMC Lead for the Cloud Foundry Foundation's Runtime PMC. | Prior to that, Dieu served as Product Manager at Greenplum working with Pivotal Labs on applications for use by data scientists.
Captions: 
	00:00:00,030 --> 00:00:04,290
hey um I'm Yui Cal I work for pivotal

00:00:03,689 --> 00:00:06,870
software

00:00:04,290 --> 00:00:09,480
I'm also the runtime PMC lead in the

00:00:06,870 --> 00:00:11,550
Cloud Foundry Foundation as you can see

00:00:09,480 --> 00:00:13,110
from the title of the talk I'll be

00:00:11,550 --> 00:00:15,630
giving an update on Cloud Foundry

00:00:13,110 --> 00:00:22,130
isolation segments formerly known as

00:00:15,630 --> 00:00:22,130
elastic clusters so let's get started

00:00:22,279 --> 00:00:33,980
mic microphone is this better how do I

00:00:30,539 --> 00:00:38,700
go back I can't go back but I'm Yui cap

00:00:33,980 --> 00:00:41,579
and I work for a pivotal software

00:00:38,700 --> 00:00:43,230
I'm also the runtime PMC lead in the

00:00:41,579 --> 00:00:45,930
cloud foundry foundation so you might

00:00:43,230 --> 00:00:47,760
know me from from that and as you can

00:00:45,930 --> 00:00:49,620
see from the talk I'll be giving an

00:00:47,760 --> 00:00:51,989
update on cloud foundry isolation

00:00:49,620 --> 00:00:57,870
segments formerly known as elastic

00:00:51,989 --> 00:00:59,190
clusters so let's get started um I'd

00:00:57,870 --> 00:01:01,590
like to frame the problem we're trying

00:00:59,190 --> 00:01:03,300
to solve now a lot of you out there you

00:01:01,590 --> 00:01:06,390
might start with one cloud foundry and

00:01:03,300 --> 00:01:09,600
that's great it might look a lot like

00:01:06,390 --> 00:01:11,820
this and although this doesn't really

00:01:09,600 --> 00:01:14,880
mention the data stores or all the other

00:01:11,820 --> 00:01:17,009
components required for coordination you

00:01:14,880 --> 00:01:22,020
may have even striped them across three

00:01:17,009 --> 00:01:23,820
AZ's for availability and and as more

00:01:22,020 --> 00:01:26,040
apps come to the platform you can just

00:01:23,820 --> 00:01:28,259
scale the appropriate chair you know

00:01:26,040 --> 00:01:31,920
whether it's the routers or the cells or

00:01:28,259 --> 00:01:35,970
the logging tier and that's great but

00:01:31,920 --> 00:01:38,579
then this happens you end up with more

00:01:35,970 --> 00:01:41,159
deployments of Cloud Foundry for it for

00:01:38,579 --> 00:01:44,549
one reason or another or maybe even this

00:01:41,159 --> 00:01:47,210
happens I do know of an organization

00:01:44,549 --> 00:01:49,320
that has 15 deployments of cloud foundry

00:01:47,210 --> 00:01:49,920
and there are a lot of different reasons

00:01:49,320 --> 00:01:52,740
for that

00:01:49,920 --> 00:01:55,729
perhaps for a disaster recovery or h.a

00:01:52,740 --> 00:01:58,579
or dev versus prod

00:01:55,729 --> 00:02:01,530
but there are some operational concerns

00:01:58,579 --> 00:02:03,630
to maintaining so many cloud foundries

00:02:01,530 --> 00:02:06,649
like keeping roles and permissions in

00:02:03,630 --> 00:02:09,630
sync across all of those cloud foundries

00:02:06,649 --> 00:02:11,910
the base cost of running a cloud

00:02:09,630 --> 00:02:15,600
Andre I think we're at somewhere about

00:02:11,910 --> 00:02:17,820
20 VMs or so of different sizes for at

00:02:15,600 --> 00:02:20,880
first single Cloud Foundry and as you

00:02:17,820 --> 00:02:22,320
have more VMs and Cloud Foundry is the

00:02:20,880 --> 00:02:26,850
deployment complexity and the

00:02:22,320 --> 00:02:29,730
maintenance costs are high so so then

00:02:26,850 --> 00:02:33,060
it's freedom to ask you know can we

00:02:29,730 --> 00:02:34,650
reduce the overhead right for each of

00:02:33,060 --> 00:02:38,100
those additional deployments that you

00:02:34,650 --> 00:02:41,520
have or wish to have we can start with

00:02:38,100 --> 00:02:45,270
asking two questions is it okay to have

00:02:41,520 --> 00:02:47,910
a shared single management tier between

00:02:45,270 --> 00:02:51,210
those CF deployments right one set of

00:02:47,910 --> 00:02:53,100
admins for the whole deployment one set

00:02:51,210 --> 00:02:56,340
of data stores a cloud controller

00:02:53,100 --> 00:02:58,800
database or a blob store might be some

00:02:56,340 --> 00:03:02,280
concerns you have there and if those are

00:02:58,800 --> 00:03:03,660
okay great and the second question is is

00:03:02,280 --> 00:03:06,120
the latency low between those

00:03:03,660 --> 00:03:09,270
deployments we ask that right now

00:03:06,120 --> 00:03:11,760
because in this feature set we're not

00:03:09,270 --> 00:03:13,380
specifically solving for instability

00:03:11,760 --> 00:03:17,720
that's introduced in between the shared

00:03:13,380 --> 00:03:19,740
components as you as latency increases

00:03:17,720 --> 00:03:22,920
although that may be something that

00:03:19,740 --> 00:03:25,470
could be addressed later but for now if

00:03:22,920 --> 00:03:27,450
yes to both of those then I think

00:03:25,470 --> 00:03:31,650
isolations could help reduce some of

00:03:27,450 --> 00:03:33,750
your overhead so what's an isolation

00:03:31,650 --> 00:03:36,060
segment and I'll just read this here

00:03:33,750 --> 00:03:39,380
it's a group of Cloud Foundry resources

00:03:36,060 --> 00:03:41,340
compute network and or logging to which

00:03:39,380 --> 00:03:44,130
applications can be directed for

00:03:41,340 --> 00:03:47,700
deployment and I'll do digress here for

00:03:44,130 --> 00:03:50,220
a moment to talk about naming so so

00:03:47,700 --> 00:03:52,980
what's in a name we've renamed and

00:03:50,220 --> 00:03:55,590
reworked this proposal in future set in

00:03:52,980 --> 00:03:56,010
architecture a few times run into a few

00:03:55,590 --> 00:03:58,470
walls

00:03:56,010 --> 00:04:00,410
it's been called placement pools and

00:03:58,470 --> 00:04:03,959
isolation groups and elastic clusters

00:04:00,410 --> 00:04:06,030
we've now settled on isolation segments

00:04:03,959 --> 00:04:09,540
thanks to a suggestion from Sandy cash

00:04:06,030 --> 00:04:12,180
from IBM it derives from a discussion of

00:04:09,540 --> 00:04:15,600
isolation and segmentation requirements

00:04:12,180 --> 00:04:19,140
as they pertain to PCI DSS which a fair

00:04:15,600 --> 00:04:21,120
amount of you care about I'm also fond

00:04:19,140 --> 00:04:21,950
of the word segment as it's not quite as

00:04:21,120 --> 00:04:24,230
overloaded

00:04:21,950 --> 00:04:29,840
as some of the other terms like groups

00:04:24,230 --> 00:04:32,480
or clusters or zones and so let's get

00:04:29,840 --> 00:04:35,900
back to it you've got you know for Claud

00:04:32,480 --> 00:04:38,660
foundries pictured here developers have

00:04:35,900 --> 00:04:40,280
to target four different end points and

00:04:38,660 --> 00:04:42,800
hopefully push their app to the correct

00:04:40,280 --> 00:04:45,710
one your admins may have difficulty

00:04:42,800 --> 00:04:48,710
keeping the permissions correct so given

00:04:45,710 --> 00:04:50,330
an organist face should that user have

00:04:48,710 --> 00:04:52,190
the same permissions across all of them

00:04:50,330 --> 00:04:55,700
or does it vary because maybe one spot

00:04:52,190 --> 00:04:58,190
and once divert test the operator

00:04:55,700 --> 00:05:00,080
management they have to think about what

00:04:58,190 --> 00:05:02,450
the patch levels are you you might end

00:05:00,080 --> 00:05:03,830
up not patching one of them for a really

00:05:02,450 --> 00:05:08,570
long time it just gets really out of

00:05:03,830 --> 00:05:11,170
sync so but so let's assume but for

00:05:08,570 --> 00:05:14,420
these deployments an isolation segment

00:05:11,170 --> 00:05:17,240
might help to to reduce this this

00:05:14,420 --> 00:05:18,590
overhead and so let's take a look at

00:05:17,240 --> 00:05:22,310
what that might look like to create an

00:05:18,590 --> 00:05:24,440
isolation segment this is really the

00:05:22,310 --> 00:05:26,720
simplest type of a isolation segment

00:05:24,440 --> 00:05:31,150
we're showing here it involves deploying

00:05:26,720 --> 00:05:34,880
an additional set of cells where and

00:05:31,150 --> 00:05:37,000
you've got shared management routing and

00:05:34,880 --> 00:05:40,160
logging but you can have dedicated

00:05:37,000 --> 00:05:42,470
compute resources in this architecture

00:05:40,160 --> 00:05:44,840
you could associate different you know

00:05:42,470 --> 00:05:48,050
different VM types to different

00:05:44,840 --> 00:05:50,660
isolation segments perhaps the blue

00:05:48,050 --> 00:05:53,330
segment has lots of CPU and salt state

00:05:50,660 --> 00:05:55,670
drives or perhaps you have an

00:05:53,330 --> 00:05:59,060
organization that has quality of service

00:05:55,670 --> 00:06:01,460
guarantees such as CPU availability or

00:05:59,060 --> 00:06:03,110
perhaps you're offering Cloud Foundry as

00:06:01,460 --> 00:06:05,050
a service you'd like to be able to

00:06:03,110 --> 00:06:07,340
charge an organization for a dedicated

00:06:05,050 --> 00:06:11,180
usage of a set of cells like a kind of

00:06:07,340 --> 00:06:13,070
premium premium usage additionally you

00:06:11,180 --> 00:06:15,950
can consider using this as a way to add

00:06:13,070 --> 00:06:18,710
additional runtime capabilities and in a

00:06:15,950 --> 00:06:22,490
smaller set such as like an NFS driver

00:06:18,710 --> 00:06:25,010
to a set of cells in a segment thanks to

00:06:22,490 --> 00:06:27,760
the recent persistence work which you

00:06:25,010 --> 00:06:30,530
can learn more about in a talk tomorrow

00:06:27,760 --> 00:06:34,910
NFS and shared file systems as a service

00:06:30,530 --> 00:06:35,750
you could then enable a specific service

00:06:34,910 --> 00:06:38,570
broker

00:06:35,750 --> 00:06:41,690
with plans that are just for that org or

00:06:38,570 --> 00:06:44,890
space so really just kind of adding

00:06:41,690 --> 00:06:49,400
premium capabilities for particular

00:06:44,890 --> 00:06:51,800
deployments another possibility might be

00:06:49,400 --> 00:06:54,230
creating segments that are known to spin

00:06:51,800 --> 00:06:57,500
down at nights on weekends so that's

00:06:54,230 --> 00:07:02,540
that some other possibilities may come

00:06:57,500 --> 00:07:05,890
to you as you think through these so

00:07:02,540 --> 00:07:09,080
let's take a look at a possible UX for

00:07:05,890 --> 00:07:12,880
for this I have not run this by the CLI

00:07:09,080 --> 00:07:15,890
team but this is the general intention

00:07:12,880 --> 00:07:17,960
so as a cloud controller admin you can

00:07:15,890 --> 00:07:21,140
create an isolation segment let's name

00:07:17,960 --> 00:07:23,270
it blue here and then you can bind it to

00:07:21,140 --> 00:07:26,810
a space in a particular organization and

00:07:23,270 --> 00:07:28,910
your space developers all they have to

00:07:26,810 --> 00:07:30,800
do is CF push they don't have to know

00:07:28,910 --> 00:07:36,230
about that and or think about that in

00:07:30,800 --> 00:07:37,880
this scenario in a future milestone and

00:07:36,230 --> 00:07:41,410
I believe that some some work is also

00:07:37,880 --> 00:07:44,240
underway to introduce this you exit

00:07:41,410 --> 00:07:47,870
we've proposed that an admin can

00:07:44,240 --> 00:07:49,880
associate multiple isolation segments to

00:07:47,870 --> 00:07:54,080
an org and then an org manager could

00:07:49,880 --> 00:08:00,979
self-service and associate particular

00:07:54,080 --> 00:08:02,630
segments to particular spaces the next

00:08:00,979 --> 00:08:07,460
type of isolation segment that I'd like

00:08:02,630 --> 00:08:09,890
to talk about is routers and cells from

00:08:07,460 --> 00:08:11,720
what I've heard this is perhaps the most

00:08:09,890 --> 00:08:14,240
compelling use case for a fair amount of

00:08:11,720 --> 00:08:17,419
customers and most likely to help reduce

00:08:14,240 --> 00:08:20,150
overhead with this you can isolate

00:08:17,419 --> 00:08:22,250
application traffic for a particular set

00:08:20,150 --> 00:08:24,169
of apps in a segment you need to

00:08:22,250 --> 00:08:26,960
configure DNS and your edge load

00:08:24,169 --> 00:08:29,210
balancer to direct routes to the correct

00:08:26,960 --> 00:08:31,160
set of go routers and the grow routers

00:08:29,210 --> 00:08:33,800
for that segment would need to be

00:08:31,160 --> 00:08:37,430
configured to only forward traffic for

00:08:33,800 --> 00:08:39,710
routes with with that segment so so from

00:08:37,430 --> 00:08:41,510
the edge load balancer through the go

00:08:39,710 --> 00:08:44,060
router and on to the cells the entire

00:08:41,510 --> 00:08:48,380
application request is contained within

00:08:44,060 --> 00:08:51,130
the segment and and which is quite quite

00:08:48,380 --> 00:08:56,930
nice if you have certain

00:08:51,130 --> 00:08:59,660
segmentation requirements there with

00:08:56,930 --> 00:09:01,730
each of these segments operators would

00:08:59,660 --> 00:09:05,330
need to select how strict the networking

00:09:01,730 --> 00:09:08,030
separation is for each segment now we do

00:09:05,330 --> 00:09:11,990
have a specific requirement for consul

00:09:08,030 --> 00:09:14,630
to be able to talk across segments on a

00:09:11,990 --> 00:09:17,150
specific set of ports and IPS in the

00:09:14,630 --> 00:09:21,620
initial milestones but otherwise the

00:09:17,150 --> 00:09:23,900
only required communication is between

00:09:21,620 --> 00:09:28,220
the individual segments and back up to

00:09:23,900 --> 00:09:30,350
the management here we're also hopeful

00:09:28,220 --> 00:09:32,120
that in future the requirement for

00:09:30,350 --> 00:09:35,330
console communication across the

00:09:32,120 --> 00:09:37,160
segment's will no longer be required as

00:09:35,330 --> 00:09:40,820
console allows for the kind of

00:09:37,160 --> 00:09:42,560
hub-and-spoke model there's an

00:09:40,820 --> 00:09:45,080
additional nuance about how to deal with

00:09:42,560 --> 00:09:47,330
domains and routes in a self-service way

00:09:45,080 --> 00:09:51,470
in this model which may or may not apply

00:09:47,330 --> 00:09:53,900
to your organization perhaps there's a

00:09:51,470 --> 00:09:56,330
shared sub domain for each isolation

00:09:53,900 --> 00:09:58,370
segment that requires routing and the

00:09:56,330 --> 00:10:00,800
CFCA I could be enhanced to highlight

00:09:58,370 --> 00:10:04,130
which shared domains are dedicated to a

00:10:00,800 --> 00:10:06,200
particular segment there's an additional

00:10:04,130 --> 00:10:09,770
proposal plan to deal with how to

00:10:06,200 --> 00:10:11,630
elegantly handle domains and routing but

00:10:09,770 --> 00:10:13,610
if you have ideas or suggestions in this

00:10:11,630 --> 00:10:15,770
area I would welcome your feedback you

00:10:13,610 --> 00:10:20,720
can comment right on the the existing

00:10:15,770 --> 00:10:22,870
proposal right now now a third type of

00:10:20,720 --> 00:10:26,360
isolation segment that I'm showing here

00:10:22,870 --> 00:10:28,700
involves adding logging to that perhaps

00:10:26,360 --> 00:10:31,130
your you have an organization that has

00:10:28,700 --> 00:10:34,250
compliance or security requirements that

00:10:31,130 --> 00:10:36,740
require the application logs from a set

00:10:34,250 --> 00:10:38,960
of apps deployed to one segment are

00:10:36,740 --> 00:10:41,750
never commingled with logs from another

00:10:38,960 --> 00:10:44,600
segment and if you have this particular

00:10:41,750 --> 00:10:47,050
use case I'm very much interested in

00:10:44,600 --> 00:10:50,270
talking to you so you can understand

00:10:47,050 --> 00:10:51,920
which aspects of logging need to be

00:10:50,270 --> 00:10:53,630
isolated a little better there there's

00:10:51,920 --> 00:10:55,640
actually many components and logging

00:10:53,630 --> 00:10:58,060
system so there's many choices we could

00:10:55,640 --> 00:10:58,060
make there

00:11:00,850 --> 00:11:05,500
there is another related aspect that I'd

00:11:03,220 --> 00:11:08,080
like to touch upon briefly before

00:11:05,500 --> 00:11:11,280
wrapping up that involves trust between

00:11:08,080 --> 00:11:13,990
components you know could could we add

00:11:11,280 --> 00:11:16,270
authentication and authorization between

00:11:13,990 --> 00:11:18,940
components so ensuring workloads

00:11:16,270 --> 00:11:20,800
intended for blue components cannot be

00:11:18,940 --> 00:11:23,530
accidentally sent or received by

00:11:20,800 --> 00:11:26,020
components in in the red segment which

00:11:23,530 --> 00:11:28,600
may be of a lower trust classification

00:11:26,020 --> 00:11:31,270
right maybe those logs go into a public

00:11:28,600 --> 00:11:34,840
domain somewhere now I'm not a security

00:11:31,270 --> 00:11:36,130
expert and we'd not need to run this by

00:11:34,840 --> 00:11:38,860
someone with more security background

00:11:36,130 --> 00:11:41,290
but a possibility we've discussed is

00:11:38,860 --> 00:11:43,780
perhaps using a set of certs for each

00:11:41,290 --> 00:11:45,250
component and as each component

00:11:43,780 --> 00:11:47,080
communicates with the management here

00:11:45,250 --> 00:11:49,360
the management tier could do some

00:11:47,080 --> 00:11:52,450
additional verification on the OU's of

00:11:49,360 --> 00:11:54,820
the cert where each ot maps to a segment

00:11:52,450 --> 00:11:57,580
and if this trust mode is enabled for a

00:11:54,820 --> 00:12:00,340
segment only components barring the

00:11:57,580 --> 00:12:02,170
appropriate certificate issued by the

00:12:00,340 --> 00:12:05,200
trusted CA from the management here

00:12:02,170 --> 00:12:13,540
could receive that particular segments

00:12:05,200 --> 00:12:15,250
workload so to wrap up I hope for those

00:12:13,540 --> 00:12:17,830
of you with multiple Cloud Foundry

00:12:15,250 --> 00:12:20,710
deployments or planned growth of Cloud

00:12:17,830 --> 00:12:22,780
Foundry isolation segments could help

00:12:20,710 --> 00:12:26,260
reduce the total number of deployments

00:12:22,780 --> 00:12:28,630
under management your feedback is is

00:12:26,260 --> 00:12:31,720
really valuable and needed on the

00:12:28,630 --> 00:12:35,860
proposal which can be found here at this

00:12:31,720 --> 00:12:38,140
URL I am really optimistic that we'll

00:12:35,860 --> 00:12:40,240
reach the first milestone right at the

00:12:38,140 --> 00:12:45,730
end of the year which includes compute

00:12:40,240 --> 00:12:48,370
and the CLI commands associated with

00:12:45,730 --> 00:12:51,820
that right now we've got the initial

00:12:48,370 --> 00:12:54,250
work for this first milestone done in

00:12:51,820 --> 00:12:56,290
Diego the cred for it and Cloud

00:12:54,250 --> 00:12:59,170
Controller Adan and we just need to flow

00:12:56,290 --> 00:13:03,400
that information through and then also

00:12:59,170 --> 00:13:05,530
implement the CLI commands you can find

00:13:03,400 --> 00:13:08,100
progress updates and links to relevant

00:13:05,530 --> 00:13:11,380
trackers in that proposal at the bottom

00:13:08,100 --> 00:13:12,880
and additional proposals for some of the

00:13:11,380 --> 00:13:16,660
future milestones

00:13:12,880 --> 00:13:19,840
as we get as as we add more capabilities

00:13:16,660 --> 00:13:21,640
to this will will be posted so be on the

00:13:19,840 --> 00:13:27,450
lookout for those and love your feedback

00:13:21,640 --> 00:13:27,450
for that questions

00:13:28,040 --> 00:13:31,220
[Applause]

00:13:33,510 --> 00:13:47,230
Johannes yes there is a stack hack it's

00:13:44,170 --> 00:13:51,090
not great there's a the build packs

00:13:47,230 --> 00:13:51,090
don't like the stack hack for example

00:14:05,340 --> 00:14:13,360
the stack hack well for for one your

00:14:10,480 --> 00:14:18,940
your hacking it's not it's not it's not

00:14:13,360 --> 00:14:21,340
supported so it's in a fork too as you

00:14:18,940 --> 00:14:22,900
can see from the UX right if you're

00:14:21,340 --> 00:14:25,630
doing the stack hack you actually have

00:14:22,900 --> 00:14:28,060
to think about what stack you're going

00:14:25,630 --> 00:14:30,790
to write and and whether that's

00:14:28,060 --> 00:14:33,000
appropriate another issue with the stack

00:14:30,790 --> 00:14:34,900
hack is the build packs aren't actually

00:14:33,000 --> 00:14:37,090
depending if you handled it correctly

00:14:34,900 --> 00:14:39,250
the build packs are looking for a

00:14:37,090 --> 00:14:42,010
particular stack to be able to determine

00:14:39,250 --> 00:14:44,400
what binaries are the right binaries to

00:14:42,010 --> 00:14:47,290
use when you're staging your app and so

00:14:44,400 --> 00:14:50,170
it looks at that CF underscore stack

00:14:47,290 --> 00:14:53,410
variable during staging to see like oh

00:14:50,170 --> 00:14:56,650
you're on CF looting safe s2 will get

00:14:53,410 --> 00:14:59,020
get those binaries so I haven't looked

00:14:56,650 --> 00:15:00,670
too deeply into the stack hack so I

00:14:59,020 --> 00:15:02,910
don't know how well it it addresses that

00:15:00,670 --> 00:15:02,910
issue

00:15:05,650 --> 00:15:08,130
yes

00:15:14,520 --> 00:15:21,490
yes so this would we think would also be

00:15:19,510 --> 00:15:26,770
possible you might need a separate Bosch

00:15:21,490 --> 00:15:28,320
director right now and and you share the

00:15:26,770 --> 00:15:31,510
appropriate things in the manifest

00:15:28,320 --> 00:15:35,830
because right now a single boss director

00:15:31,510 --> 00:15:38,980
can't target multiple CP is and you'd

00:15:35,830 --> 00:15:41,080
also have to consider that latency right

00:15:38,980 --> 00:15:43,120
if you're wanting to burst to Amazon

00:15:41,080 --> 00:15:46,690
maybe that would be okay maybe it would

00:15:43,120 --> 00:15:49,470
work but I I don't know you'd have to

00:15:46,690 --> 00:15:53,230
test it to see if the latency issues are

00:15:49,470 --> 00:15:55,390
large enough to to cause issues and

00:15:53,230 --> 00:15:58,000
that's something I'm hoping one of the

00:15:55,390 --> 00:16:03,640
teams can explore about you know how

00:15:58,000 --> 00:16:10,260
much latency is is is this architecture

00:16:03,640 --> 00:16:10,260
tolerant to yes

00:16:16,630 --> 00:16:25,270
they I don't believe it has to be it can

00:16:21,500 --> 00:16:25,270
be a separate Bosch deployment

00:16:46,929 --> 00:16:51,470
that's true I haven't actually looked at

00:16:49,429 --> 00:16:54,079
what the manifest changes look like that

00:16:51,470 --> 00:16:55,879
that eric has done on the Diego team so

00:16:54,079 --> 00:17:06,620
it's possible he's done it as a separate

00:16:55,879 --> 00:17:08,029
job Jim you would know the the isolation

00:17:06,620 --> 00:17:10,569
segment does it have to be a separate

00:17:08,029 --> 00:17:10,569
deployment

00:17:17,750 --> 00:17:21,919
so if it's a property on the sell then

00:17:19,970 --> 00:17:24,829
you could deploy just yourselves and

00:17:21,919 --> 00:17:27,579
then associate the property then right

00:17:24,829 --> 00:17:27,579
and then you could choose

00:17:43,390 --> 00:17:50,540
as we're looking to get this out as

00:17:48,170 --> 00:17:53,560
quickly as possible so the the simplest

00:17:50,540 --> 00:17:56,830
thing is as an admin I can bind to it

00:17:53,560 --> 00:18:01,160
again we're hoping to introduce some

00:17:56,830 --> 00:18:04,880
self-service UX where an admin can can

00:18:01,160 --> 00:18:07,250
map particular isolation segments to an

00:18:04,880 --> 00:18:10,100
org perhaps and then an org manager

00:18:07,250 --> 00:18:11,690
could self-service those isolation

00:18:10,100 --> 00:18:32,840
segments to the spaces that he's

00:18:11,690 --> 00:18:37,460
managing does that make sense yes sorry

00:18:32,840 --> 00:18:41,600
the the question game asked is in in the

00:18:37,460 --> 00:18:45,170
proposal he had asked a question of a at

00:18:41,600 --> 00:18:48,530
the go routers you could associate

00:18:45,170 --> 00:18:50,960
multiple isolation segments to them and

00:18:48,530 --> 00:18:53,930
again this kind of depends on what type

00:18:50,960 --> 00:18:55,940
of isolation you need at that level

00:18:53,930 --> 00:19:01,580
right if you're okay

00:18:55,940 --> 00:19:03,290
sharing a set of go routers across maybe

00:19:01,580 --> 00:19:05,600
two of your segments and you have five

00:19:03,290 --> 00:19:07,880
segments right there's two segments that

00:19:05,600 --> 00:19:13,280
that you that you're okay with sharing

00:19:07,880 --> 00:19:15,890
that routing tier with then the that go

00:19:13,280 --> 00:19:19,210
router own can can just contain the

00:19:15,890 --> 00:19:21,740
routing table for those two sets of

00:19:19,210 --> 00:19:24,230
segments and then it can direct the

00:19:21,740 --> 00:19:26,570
traffic to the appropriate segment and

00:19:24,230 --> 00:19:29,060
again like the the sticky point here is

00:19:26,570 --> 00:19:35,330
around how you set up your network you

00:19:29,060 --> 00:19:38,450
know does it allow if you've done the

00:19:35,330 --> 00:19:40,130
segment's in separate networks does it

00:19:38,450 --> 00:19:42,560
allow communication from this set of

00:19:40,130 --> 00:19:45,830
routers to the other side to the other

00:19:42,560 --> 00:19:47,510
set of cells but it's possible depending

00:19:45,830 --> 00:19:50,450
on how you've configured your deployment

00:19:47,510 --> 00:19:53,020
that you could have a shared shared set

00:19:50,450 --> 00:20:05,270
of routers across

00:19:53,020 --> 00:20:18,080
multiple compute is it likely to expand

00:20:05,270 --> 00:20:20,630
across I think it's possible for the

00:20:18,080 --> 00:20:22,850
logging system but the logging system is

00:20:20,630 --> 00:20:25,610
quite a bit more complicated than the

00:20:22,850 --> 00:20:28,370
other components so there's still some

00:20:25,610 --> 00:20:30,590
design to be done in terms of what that

00:20:28,370 --> 00:20:32,990
would look like in terms of how you

00:20:30,590 --> 00:20:34,970
discover the appropriate traffic

00:20:32,990 --> 00:20:39,770
controller to hit to find out where your

00:20:34,970 --> 00:20:42,679
apps where your logs at to to get at the

00:20:39,770 --> 00:20:44,630
logs for your app right there's some

00:20:42,679 --> 00:20:46,280
amount of coordination you have got some

00:20:44,630 --> 00:20:48,140
logs from the central management tier

00:20:46,280 --> 00:20:56,450
that need to get to the right place to

00:20:48,140 --> 00:20:58,400
be able to read them out all right no

00:20:56,450 --> 00:21:04,349
other questions thank you very much

00:20:58,400 --> 00:21:04,349

YouTube URL: https://www.youtube.com/watch?v=Yt-fH2YYl8E


