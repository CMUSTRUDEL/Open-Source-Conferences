Title: Lessons Learnt Operating Multiple Cloud Foundry on OpenStack Deployments - Tyler Britten, IBM
Publication date: 2016-09-30
Playlist: Cloud Foundry Summit Europe 2016
Description: 
	Lessons Learnt Operating Multiple Cloud Foundry on OpenStack Deployments - Tyler Britten, IBM

According to OpenStack users survey, Cloud Foundry is the 2nd most popular workload on OpenStack. You want to deploy Cloud Foundry on OpenStack or already have. What's next?

Cloud Foundry continues to evolve, often with revolutionary changes, e.g move from bosh-micro to bosh-init, using the new eCPI, move to Diego etc. Same with OpenStack, e.g changes from Keystone v2 to v3, from Kilo to Mitaka, network plugins changes etc. Both IaaS and PaaS layers are changing frequently.

In this talk will discuss our lessons learnt operating hybrid Cloud Foundry deployments on top of OpenStack over the last years. We will discuss how we have used underlying technologies like BOSH, Ansible, Jenkins, Razor, Rally etc to seamlessly operate Cloud Foundry on OpenStack. We will also detail community tools to validate whether Cloud Foundry deployment will work on your OpenStack etc.
Captions: 
	00:00:00,030 --> 00:00:05,899
all right we'll get we're going start a

00:00:01,709 --> 00:00:09,210
little late here last session right over

00:00:05,899 --> 00:00:10,920
but let's get going so we're not wasting

00:00:09,210 --> 00:00:15,120
any time all we're going to talk about

00:00:10,920 --> 00:00:17,490
is Cloud Foundry on OpenStack and kind

00:00:15,120 --> 00:00:20,070
of what we've gone through as IBM

00:00:17,490 --> 00:00:21,990
running our clip on three platforms on

00:00:20,070 --> 00:00:24,240
OpenStack kind of challenges we ran into

00:00:21,990 --> 00:00:26,760
how we work through them and I went from

00:00:24,240 --> 00:00:29,810
there so if not familiar with OpenStack

00:00:26,760 --> 00:00:33,899
and I'd be surprised but it's AI as

00:00:29,810 --> 00:00:36,170
cloud operating system open-source so we

00:00:33,899 --> 00:00:38,070
use it internally for our

00:00:36,170 --> 00:00:39,809
infrastructures or service platforms

00:00:38,070 --> 00:00:43,020
there are other companies that use it

00:00:39,809 --> 00:00:45,690
the key pieces to OpenStack we call the

00:00:43,020 --> 00:00:49,289
core services Nova neutrons with cinder

00:00:45,690 --> 00:00:52,340
Keystone glance their compute networking

00:00:49,289 --> 00:00:56,010
stored object storage block storage

00:00:52,340 --> 00:00:57,750
authentication and image storage so

00:00:56,010 --> 00:00:59,609
these are the core components to

00:00:57,750 --> 00:01:01,309
OpenStack that you would need if you

00:00:59,609 --> 00:01:03,329
were running Cloud Foundry on it

00:01:01,309 --> 00:01:05,220
obviously we're I Cloud Foundry summit

00:01:03,329 --> 00:01:09,479
so you have a pretty good idea hopefully

00:01:05,220 --> 00:01:11,270
what Cloud Foundry is so we IBM as a

00:01:09,479 --> 00:01:13,890
whole has been involved in cloud foundry

00:01:11,270 --> 00:01:16,380
for quite some time now

00:01:13,890 --> 00:01:19,799
both on our public bluemix platform as

00:01:16,380 --> 00:01:20,759
well as our private platforms so if

00:01:19,799 --> 00:01:25,170
you're familiar with the way cloud

00:01:20,759 --> 00:01:27,450
foundry works use bash as the as the

00:01:25,170 --> 00:01:29,100
deployment and lifecycle management tool

00:01:27,450 --> 00:01:32,220
for cloud foundry so this is how you

00:01:29,100 --> 00:01:34,590
deploy and run client foundry and the

00:01:32,220 --> 00:01:36,509
way you deploy it on different platforms

00:01:34,590 --> 00:01:38,369
is called the CPI a cloud provider

00:01:36,509 --> 00:01:40,680
interface it's a cloud provider

00:01:38,369 --> 00:01:43,829
interface tells bosh how to talk to the

00:01:40,680 --> 00:01:46,649
underlying components so there's CP is

00:01:43,829 --> 00:01:49,409
for AWS CP is for VMware there's also

00:01:46,649 --> 00:01:51,270
CPI for OpenStack there's some other

00:01:49,409 --> 00:01:53,399
ones IBM we've created one specifically

00:01:51,270 --> 00:01:54,780
for software and I know there's there's

00:01:53,399 --> 00:01:56,490
some other newer ones out there to

00:01:54,780 --> 00:01:58,500
deploy directly on bare metal and things

00:01:56,490 --> 00:02:00,360
like that so the the CPI is really what

00:01:58,500 --> 00:02:02,990
tells bosch how to talk to the

00:02:00,360 --> 00:02:04,970
underlying layer

00:02:02,990 --> 00:02:05,869
so when you're doing a Cloud Foundry to

00:02:04,970 --> 00:02:07,340
play there's a bunch of different

00:02:05,869 --> 00:02:09,649
components of washes the the mechanism

00:02:07,340 --> 00:02:11,720
that does it one of the key things you

00:02:09,649 --> 00:02:13,459
need is with this called a stem cell so

00:02:11,720 --> 00:02:15,290
there's your base OS that you're going

00:02:13,459 --> 00:02:16,430
to deploy on to that cloud that all

00:02:15,290 --> 00:02:18,080
those other pieces are going to be put

00:02:16,430 --> 00:02:20,690
on so in that release is the version

00:02:18,080 --> 00:02:22,130
which software packages configurations

00:02:20,690 --> 00:02:23,750
all the components that are gonna make

00:02:22,130 --> 00:02:25,580
up that Cloud Foundry release that

00:02:23,750 --> 00:02:27,050
you're going to deploy and then it's all

00:02:25,580 --> 00:02:29,840
tied together in a manifest so the

00:02:27,050 --> 00:02:31,700
manifest basically tells bosh what to

00:02:29,840 --> 00:02:33,830
deploy where what the networks are all

00:02:31,700 --> 00:02:37,760
those type of components so all those

00:02:33,830 --> 00:02:41,930
together is how Bosch deploys Cloud

00:02:37,760 --> 00:02:43,730
Foundry on to underlying ayahs so

00:02:41,930 --> 00:02:46,250
starting with the problems that we ran

00:02:43,730 --> 00:02:48,140
into not just with IBM but what we've

00:02:46,250 --> 00:02:51,080
seen and from talking to other cloud

00:02:48,140 --> 00:02:53,870
foundry teams they ran into when trying

00:02:51,080 --> 00:02:56,000
to deploy on OpenStack key ones

00:02:53,870 --> 00:02:58,370
instability especially with the earlier

00:02:56,000 --> 00:03:01,550
releases of OpenStack different API API

00:02:58,370 --> 00:03:04,190
changes API performances all those types

00:03:01,550 --> 00:03:07,519
of things and they change between

00:03:04,190 --> 00:03:09,049
releases some people would deploy their

00:03:07,519 --> 00:03:10,700
OpenStack it wouldn't be a continued

00:03:09,049 --> 00:03:12,709
contiguous release so they use some

00:03:10,700 --> 00:03:16,280
services from one version somewhere some

00:03:12,709 --> 00:03:19,070
from another capacity so this is where

00:03:16,280 --> 00:03:22,160
we've seen OpenStack uses flavors so we

00:03:19,070 --> 00:03:24,290
learned AWS so we have like vm sizes and

00:03:22,160 --> 00:03:26,209
they're not always ideal for Cloud

00:03:24,290 --> 00:03:27,200
Foundry out of the box so what happens

00:03:26,209 --> 00:03:28,850
is you going to put a lot of wasted

00:03:27,200 --> 00:03:33,620
resources because you're using the next

00:03:28,850 --> 00:03:35,360
size flavor up networking how you know

00:03:33,620 --> 00:03:36,709
your knee if you're doing something like

00:03:35,360 --> 00:03:40,459
OpenStack you're probably doing some

00:03:36,709 --> 00:03:41,959
sort of software-defined networking or

00:03:40,459 --> 00:03:43,850
network encapsulation then you throwing

00:03:41,959 --> 00:03:46,820
Cloud Foundry on top of that so how does

00:03:43,850 --> 00:03:50,900
that all play together well the other

00:03:46,820 --> 00:03:53,030
thing is enterprise software so hey cool

00:03:50,900 --> 00:03:56,510
we deployed our Cloud Foundry you know

00:03:53,030 --> 00:03:58,910
our DEA Zoran Diego or whatever but we

00:03:56,510 --> 00:04:00,730
need these other pieces say a commercial

00:03:58,910 --> 00:04:04,250
software package we want to run Oracle

00:04:00,730 --> 00:04:06,260
for a for a data service behind it well

00:04:04,250 --> 00:04:07,880
that may not been supported on OpenStack

00:04:06,260 --> 00:04:09,650
that was another another challenge we

00:04:07,880 --> 00:04:11,450
saw

00:04:09,650 --> 00:04:13,550
and really what you're trying to do it

00:04:11,450 --> 00:04:15,320
Andy the idea with CPI's and way Bosch

00:04:13,550 --> 00:04:17,299
works is to be able to make them really

00:04:15,320 --> 00:04:19,609
generic so we can just swap out the CPI

00:04:17,299 --> 00:04:22,580
and deploy the Seine to different

00:04:19,609 --> 00:04:25,220
different Aya's layers but the problem

00:04:22,580 --> 00:04:28,520
is they're different enough where you

00:04:25,220 --> 00:04:29,990
can't really get to that layer and then

00:04:28,520 --> 00:04:32,090
the other thing we're into with is if

00:04:29,990 --> 00:04:34,160
we've had we have customers and had

00:04:32,090 --> 00:04:35,600
customers who say well we want both so

00:04:34,160 --> 00:04:36,710
we need AI as for some of our stuff and

00:04:35,600 --> 00:04:38,450
we want Cloud Foundry for the other

00:04:36,710 --> 00:04:40,310
stuff so we want them all on the same

00:04:38,450 --> 00:04:42,710
layer and then it's how you handle

00:04:40,310 --> 00:04:44,930
permissioning so for example the

00:04:42,710 --> 00:04:46,760
crunches you gave to Bosch can't step

00:04:44,930 --> 00:04:49,210
all over the VMS that the users are

00:04:46,760 --> 00:04:52,280
praying individually and vice-versa

00:04:49,210 --> 00:04:54,620
h.a what works what doesn't how do we

00:04:52,280 --> 00:04:57,800
provide availability for all the cloud

00:04:54,620 --> 00:04:59,630
foundry components and you know as long

00:04:57,800 --> 00:05:01,970
and as well as the OpenStack components

00:04:59,630 --> 00:05:03,530
and then the constant release cycle so

00:05:01,970 --> 00:05:05,120
OpenStack does major releases twice a

00:05:03,530 --> 00:05:06,919
year Cloud Foundry it has a pretty

00:05:05,120 --> 00:05:09,440
aggressive release cycle as well so how

00:05:06,919 --> 00:05:11,290
do we you know get to deploy and then be

00:05:09,440 --> 00:05:15,070
able to keep up with those releases

00:05:11,290 --> 00:05:18,290
alright so there was a survey someone

00:05:15,070 --> 00:05:19,900
from IBM did ask me people kind of what

00:05:18,290 --> 00:05:22,160
they were running into with OpenStack

00:05:19,900 --> 00:05:24,950
with Cloud Foundry so what's your level

00:05:22,160 --> 00:05:27,500
experience you can see most have have

00:05:24,950 --> 00:05:29,540
had some experience and you can see you

00:05:27,500 --> 00:05:30,770
know what how much difficulty did you

00:05:29,540 --> 00:05:33,020
have with it you see the biggest one was

00:05:30,770 --> 00:05:34,120
significant difficulty deploying Cloud

00:05:33,020 --> 00:05:37,250
Foundry on OpenStack

00:05:34,120 --> 00:05:39,380
as we dug into that a little bit more do

00:05:37,250 --> 00:05:41,450
you have to customize anything you know

00:05:39,380 --> 00:05:43,940
it's almost 50/50 you know making

00:05:41,450 --> 00:05:46,340
customizations to to get deploying their

00:05:43,940 --> 00:05:48,290
environment and the two biggest ones

00:05:46,340 --> 00:05:50,810
here you can see from a were the biggest

00:05:48,290 --> 00:05:52,190
issues with Bosch instability OpenStack

00:05:50,810 --> 00:05:54,500
environments that they were working on

00:05:52,190 --> 00:05:56,639
as well as the difficulty getting the

00:05:54,500 --> 00:05:58,710
initial setup going

00:05:56,639 --> 00:06:00,330
so then we asked more like what what

00:05:58,710 --> 00:06:01,860
version do you using this the survey's a

00:06:00,330 --> 00:06:05,219
little but you can see it's a couple

00:06:01,860 --> 00:06:06,870
releases behind and then what type of

00:06:05,219 --> 00:06:09,180
OpenStack is it locally managed remotely

00:06:06,870 --> 00:06:12,629
manage all kind of things so most these

00:06:09,180 --> 00:06:15,780
environments were their local OpenStack

00:06:12,629 --> 00:06:18,689
environment so talking about that real

00:06:15,780 --> 00:06:21,300
quick what we what we use at IBM are our

00:06:18,689 --> 00:06:24,599
OpenStack is is called blue box

00:06:21,300 --> 00:06:26,180
it's a managed OpenStack offering so we

00:06:24,599 --> 00:06:28,770
take care of all that for the clients we

00:06:26,180 --> 00:06:30,240
operate it all they do is consume it so

00:06:28,770 --> 00:06:33,569
whether you do it in a software data

00:06:30,240 --> 00:06:35,460
center or in any customers own data

00:06:33,569 --> 00:06:38,159
center we remotely manage it upgrade it

00:06:35,460 --> 00:06:39,449
keep it keep it running troubleshoot at

00:06:38,159 --> 00:06:41,639
all those types of things and we provide

00:06:39,449 --> 00:06:44,370
all the the usual OpenStack services and

00:06:41,639 --> 00:06:44,789
a highly available fashion how do we do

00:06:44,370 --> 00:06:46,979
that

00:06:44,789 --> 00:06:48,930
we have a tool called Ursula which is

00:06:46,979 --> 00:06:51,509
open source that it's kind of a wrapper

00:06:48,930 --> 00:06:53,090
around ansible that allows us to deploy

00:06:51,509 --> 00:06:56,759
these OpenStack environments very

00:06:53,090 --> 00:06:58,529
quickly in a very controlled manner we

00:06:56,759 --> 00:07:01,710
get very good repeatability of them and

00:06:58,529 --> 00:07:05,310
also upgradability so I for example the

00:07:01,710 --> 00:07:07,440
the latest full release of OpenStack is

00:07:05,310 --> 00:07:09,240
Mitaka which came out earlier this year

00:07:07,440 --> 00:07:12,000
all of our customers clouds already

00:07:09,240 --> 00:07:13,319
upgraded to Mitaka which you know that

00:07:12,000 --> 00:07:14,729
you can see some people are even just

00:07:13,319 --> 00:07:16,110
starting to look at doing that and the

00:07:14,729 --> 00:07:18,599
only way we've been able to do that is

00:07:16,110 --> 00:07:21,629
automation and orchestration through our

00:07:18,599 --> 00:07:23,490
tooling so by doing that you know when

00:07:21,629 --> 00:07:27,479
when blue box got acquired by IBM last

00:07:23,490 --> 00:07:29,400
year and the bluemix team got to see you

00:07:27,479 --> 00:07:30,479
know a very good stable OpenStack

00:07:29,400 --> 00:07:33,210
environment that they could start to

00:07:30,479 --> 00:07:35,370
deploy to that's only started mixing

00:07:33,210 --> 00:07:39,569
that in so if you're not familiar with

00:07:35,370 --> 00:07:41,009
bluemix which when I first came to IBM I

00:07:39,569 --> 00:07:43,500
I always just thought well bluemix is

00:07:41,009 --> 00:07:45,240
IBM's cloud foundry but then there's a

00:07:43,500 --> 00:07:47,009
lot of other components to it so all the

00:07:45,240 --> 00:07:49,349
backing services and additional things

00:07:47,009 --> 00:07:52,639
like that but but the core of it is

00:07:49,349 --> 00:07:55,889
still cloud foundry so it's a very large

00:07:52,639 --> 00:07:59,099
environment we offer as a public facing

00:07:55,889 --> 00:08:00,599
cloud foundry environment as well as

00:07:59,099 --> 00:08:02,129
where we're focusing here with the

00:08:00,599 --> 00:08:04,199
OpenStack is dedicated in private so

00:08:02,129 --> 00:08:06,899
single tenant either in a software data

00:08:04,199 --> 00:08:08,399
center or customers data center and and

00:08:06,899 --> 00:08:09,870
bluemix can get pretty complex once you

00:08:08,399 --> 00:08:12,060
start pulling in those other

00:08:09,870 --> 00:08:14,010
on Cloud Foundry things so besides the

00:08:12,060 --> 00:08:17,880
bass Cloud Foundry stuff we have things

00:08:14,010 --> 00:08:19,350
like container service and you know we

00:08:17,880 --> 00:08:21,360
can do github Enterprise and all these

00:08:19,350 --> 00:08:23,670
other pieces so that it's not just

00:08:21,360 --> 00:08:25,650
beyond the basic Bosch install of the

00:08:23,670 --> 00:08:28,830
the Cloud Foundry components but it's

00:08:25,650 --> 00:08:32,100
all the other pieces around it so what

00:08:28,830 --> 00:08:34,950
did what did we what are we learned well

00:08:32,100 --> 00:08:36,000
the first thing here is the best place

00:08:34,950 --> 00:08:38,420
to start is if you have a cloud foundry

00:08:36,000 --> 00:08:41,130
environment is there's a there's a good

00:08:38,420 --> 00:08:42,980
information on the foundation's website

00:08:41,130 --> 00:08:46,280
to validate your OpenStack environment

00:08:42,980 --> 00:08:48,510
yeah some basic things you can find out

00:08:46,280 --> 00:08:51,990
to check to make sure you have in place

00:08:48,510 --> 00:08:57,060
before you even try to deploy and

00:08:51,990 --> 00:08:59,130
there's also a newer project called the

00:08:57,060 --> 00:09:00,420
OpenStack validator which you can

00:08:59,130 --> 00:09:03,150
actually run against your OpenStack

00:09:00,420 --> 00:09:05,910
environment and I'll test a bunch of the

00:09:03,150 --> 00:09:07,950
API calls that the cloud CPI would make

00:09:05,910 --> 00:09:09,120
anyway and make sure they work again

00:09:07,950 --> 00:09:10,650
before you go and deploy because

00:09:09,120 --> 00:09:12,210
depending on the size of your

00:09:10,650 --> 00:09:12,900
environment the performance of your

00:09:12,210 --> 00:09:14,010
equipment

00:09:12,900 --> 00:09:15,990
you know Cloud Foundry can take a little

00:09:14,010 --> 00:09:17,790
while to deploy so you don't want to

00:09:15,990 --> 00:09:19,440
start a deployment and then you know

00:09:17,790 --> 00:09:21,450
it's going to fail anyway so you can run

00:09:19,440 --> 00:09:22,920
this tool first and Oh check all your

00:09:21,450 --> 00:09:24,960
settings and tell you kind of like a

00:09:22,920 --> 00:09:26,580
pre-flight check to say hey this is we

00:09:24,960 --> 00:09:28,140
we couldn't delete a disk or we couldn't

00:09:26,580 --> 00:09:30,330
delete a VM or you know there's some bad

00:09:28,140 --> 00:09:34,010
permission something like that this tool

00:09:30,330 --> 00:09:34,010
gives you the ability to to see that

00:09:35,639 --> 00:09:40,990
sizing so I mentioned earlier the the

00:09:37,690 --> 00:09:43,959
default flavors what we've done is we've

00:09:40,990 --> 00:09:46,750
gone with custom flavors to match the

00:09:43,959 --> 00:09:50,019
different components of Cloud Foundry so

00:09:46,750 --> 00:09:52,899
you can see here the de A's router core

00:09:50,019 --> 00:09:55,180
nodes service gateways we had different

00:09:52,899 --> 00:09:57,250
and that's what we start our flavors

00:09:55,180 --> 00:10:00,970
with CSV so we have these how many gigs

00:09:57,250 --> 00:10:02,290
of RAM how much disk so we use those and

00:10:00,970 --> 00:10:07,360
then you can see the different counts of

00:10:02,290 --> 00:10:09,160
all the other ones and then on the right

00:10:07,360 --> 00:10:10,420
side to the amount of persistent disk so

00:10:09,160 --> 00:10:12,579
this is our I'll beat this is our

00:10:10,420 --> 00:10:15,250
internal we'll use for base sizing for

00:10:12,579 --> 00:10:18,190
starting the Cloud Foundry core

00:10:15,250 --> 00:10:21,370
deployment so it uses a good bit of

00:10:18,190 --> 00:10:24,100
resources to get started but we found

00:10:21,370 --> 00:10:26,829
using these settings and these sizes has

00:10:24,100 --> 00:10:30,370
has given us the most efficient use of

00:10:26,829 --> 00:10:32,880
the resources we could we can use under

00:10:30,370 --> 00:10:32,880
based on the

00:10:35,240 --> 00:10:39,090
so now we get into to scale the

00:10:37,800 --> 00:10:40,500
scalability pieces usually the next

00:10:39,090 --> 00:10:42,780
question cool we got it up and running

00:10:40,500 --> 00:10:44,940
initially how do we scale from here so

00:10:42,780 --> 00:10:46,620
on the public cloud side we've scaled

00:10:44,940 --> 00:10:49,380
you know caught we've proven cloud

00:10:46,620 --> 00:10:51,090
factor can scale extremely large to the

00:10:49,380 --> 00:10:53,100
tens of thousands of machines and and

00:10:51,090 --> 00:10:56,310
things like that but what about in a

00:10:53,100 --> 00:10:59,520
private OpenStack environment so this is

00:10:56,310 --> 00:11:01,350
where we found that starting with that

00:10:59,520 --> 00:11:02,520
base consumption right there you see in

00:11:01,350 --> 00:11:05,070
the blue that's kind of where we get

00:11:02,520 --> 00:11:07,620
started you know how many unique 32

00:11:05,070 --> 00:11:09,930
cores 700 gigs of ram six terabytes of

00:11:07,620 --> 00:11:11,820
disk and 1.5 terabytes of being

00:11:09,930 --> 00:11:13,980
persistent disk so this is the your base

00:11:11,820 --> 00:11:15,330
starting and as we've scaled some of

00:11:13,980 --> 00:11:18,450
these customer environments we've found

00:11:15,330 --> 00:11:20,820
like what those blocks are as they go so

00:11:18,450 --> 00:11:22,050
you can see based on that initial

00:11:20,820 --> 00:11:26,160
starting point that gets you one

00:11:22,050 --> 00:11:27,600
terabyte of application memory so you

00:11:26,160 --> 00:11:31,470
know depending on how many gigs or

00:11:27,600 --> 00:11:32,820
megabytes you give to each runtime you

00:11:31,470 --> 00:11:35,220
know that's how you could chop that up

00:11:32,820 --> 00:11:38,010
but that will get you a terabyte right

00:11:35,220 --> 00:11:40,890
there so each the assumption we make and

00:11:38,010 --> 00:11:43,200
how we've been playing each DEA and this

00:11:40,890 --> 00:11:46,320
is all based on the EI is not Diego at

00:11:43,200 --> 00:11:48,870
forque or 32 gig machines so every time

00:11:46,320 --> 00:11:50,550
we add twenty eight additional machines

00:11:48,870 --> 00:11:53,220
we get an additional terabyte of

00:11:50,550 --> 00:11:54,720
application capacity so that's kind of

00:11:53,220 --> 00:11:56,790
that's like our sizing numbers we've

00:11:54,720 --> 00:12:00,360
worked off of where every time we want

00:11:56,790 --> 00:12:02,310
to add so we brand it to two and even 32

00:12:00,360 --> 00:12:04,740
so we say every terabyte about patient

00:12:02,310 --> 00:12:07,050
memory 32 DEA s so it makes it pretty

00:12:04,740 --> 00:12:10,320
easy when we do sizing to work off those

00:12:07,050 --> 00:12:14,100
numbers and each of those 32 DEA s17 use

00:12:10,320 --> 00:12:15,720
12 terabytes of data store capacity then

00:12:14,100 --> 00:12:16,830
as we get larger we start you know

00:12:15,720 --> 00:12:19,020
growing you know things more log

00:12:16,830 --> 00:12:20,490
relators more every every three

00:12:19,020 --> 00:12:24,000
terabytes editor we had another API

00:12:20,490 --> 00:12:25,830
worker and another go router once you

00:12:24,000 --> 00:12:28,610
get into the services those are really

00:12:25,830 --> 00:12:31,410
service dependent so if it's something

00:12:28,610 --> 00:12:32,640
smaller you know or larger that's really

00:12:31,410 --> 00:12:35,550
depends on the service whether it's a

00:12:32,640 --> 00:12:37,020
Redis or my sequel or whatever you're

00:12:35,550 --> 00:12:38,820
using for your backbone services

00:12:37,020 --> 00:12:40,080
it's totally based on that individual

00:12:38,820 --> 00:12:41,880
service so some of our services are

00:12:40,080 --> 00:12:44,540
lightweight that we use and some of them

00:12:41,880 --> 00:12:44,540
are really heavy weight

00:12:45,960 --> 00:12:53,070
another thing we've seen is the

00:12:49,470 --> 00:12:56,290
especially during a deploy Bosch can be

00:12:53,070 --> 00:12:57,970
can hit the API really hard asking for

00:12:56,290 --> 00:13:01,150
resources and configuring things there's

00:12:57,970 --> 00:13:05,890
no sort of rate limiting on it so we

00:13:01,150 --> 00:13:08,320
need to make sure the the API settings

00:13:05,890 --> 00:13:10,390
for OpenStack turned up to increase the

00:13:08,320 --> 00:13:11,830
limits so that way we don't get timeouts

00:13:10,390 --> 00:13:14,080
and we because we don't want the Box

00:13:11,830 --> 00:13:18,760
deployment failing doing due to the lack

00:13:14,080 --> 00:13:20,860
of of API resources so oh this is

00:13:18,760 --> 00:13:24,430
another thing we've run into is name

00:13:20,860 --> 00:13:25,990
based security groups we're in OpenStack

00:13:24,430 --> 00:13:28,260
you can create security groups they have

00:13:25,990 --> 00:13:32,110
a UUID but you can also give them a name

00:13:28,260 --> 00:13:33,340
so when they does if I name when you

00:13:32,110 --> 00:13:35,320
give it a name the first thing I ask to

00:13:33,340 --> 00:13:37,990
do is go across the message bus to the

00:13:35,320 --> 00:13:39,580
database to find out the UID of that

00:13:37,990 --> 00:13:41,170
security group before you do anything

00:13:39,580 --> 00:13:43,420
with it so it just adds a lot of extra

00:13:41,170 --> 00:13:45,640
overhead where if you can reference the

00:13:43,420 --> 00:13:48,400
security groups by UUID you're skipping

00:13:45,640 --> 00:13:51,820
an additional transaction so that that's

00:13:48,400 --> 00:13:55,990
we saw reduced a lot of overhead on the

00:13:51,820 --> 00:13:58,750
back end this is the next one Neutron so

00:13:55,990 --> 00:14:03,100
we use in on the private side for blue

00:13:58,750 --> 00:14:05,830
box we use Linux bridge with provider

00:14:03,100 --> 00:14:07,900
networks and the X lands but if you're

00:14:05,830 --> 00:14:09,640
using open V switch depending on the

00:14:07,900 --> 00:14:11,920
networking technology using you have to

00:14:09,640 --> 00:14:13,690
be careful of your MT use because by

00:14:11,920 --> 00:14:15,760
default if you're using a standard empty

00:14:13,690 --> 00:14:17,170
you say on the physical host of 1500 and

00:14:15,760 --> 00:14:18,640
you haven't changed it and then you're

00:14:17,170 --> 00:14:29,710
using the X lanes you already cut that

00:14:18,640 --> 00:14:31,780
down so you start you can use the you

00:14:29,710 --> 00:14:34,660
need to turn it down because you can see

00:14:31,780 --> 00:14:37,390
1400 or 1460 depending of its GRE or the

00:14:34,660 --> 00:14:39,130
X land you'll run into problems with

00:14:37,390 --> 00:14:40,870
that anyway if you don't you know

00:14:39,130 --> 00:14:43,570
outside of just these with Cloud Foundry

00:14:40,870 --> 00:14:45,720
once you start getting you know networks

00:14:43,570 --> 00:14:49,600
inside the networks inside the networks

00:14:45,720 --> 00:14:52,240
also the you can change this compute

00:14:49,600 --> 00:14:55,360
scheduler driver as well to try and

00:14:52,240 --> 00:14:56,710
balance it based on instead of the the

00:14:55,360 --> 00:14:58,050
default scheduler so you want to balance

00:14:56,710 --> 00:15:00,390
it based on

00:14:58,050 --> 00:15:02,790
unload which is obviously when you're

00:15:00,390 --> 00:15:06,900
standing up a lot of de asel this is

00:15:02,790 --> 00:15:09,440
this is a key piece of that on the Bosch

00:15:06,900 --> 00:15:12,660
side on the Cloud Foundry side

00:15:09,440 --> 00:15:15,450
Nats timeouts are can be a challenge

00:15:12,660 --> 00:15:16,650
anyway when you have a lot of components

00:15:15,450 --> 00:15:19,170
that are that are talking if you know

00:15:16,650 --> 00:15:21,180
from attacks is the message bus for for

00:15:19,170 --> 00:15:25,170
Cloud Foundry so increasing that

00:15:21,180 --> 00:15:27,270
timeouts gives a little bit more it's a

00:15:25,170 --> 00:15:28,110
little bit more resilient than in

00:15:27,270 --> 00:15:29,990
dealing with some of these issues

00:15:28,110 --> 00:15:32,430
because it'll wait a little longer for

00:15:29,990 --> 00:15:34,860
for these pieces to come back to it so

00:15:32,430 --> 00:15:37,110
as that grows that that's something that

00:15:34,860 --> 00:15:41,580
can get these can start running into a

00:15:37,110 --> 00:15:43,680
problem with pre pretty quickly isolated

00:15:41,580 --> 00:15:45,060
components with multiple networks so

00:15:43,680 --> 00:15:46,440
this is where you can kind of get more

00:15:45,060 --> 00:15:48,030
efficient with your network allocation

00:15:46,440 --> 00:15:49,860
again it's a smaller side snot is

00:15:48,030 --> 00:15:51,390
important but as its scales this is

00:15:49,860 --> 00:15:54,390
where you start to run into problems and

00:15:51,390 --> 00:15:56,040
delays and congestion that can cause you

00:15:54,390 --> 00:15:57,360
know kind of this all these pieces that

00:15:56,040 --> 00:15:59,630
need to talk to each other start to have

00:15:57,360 --> 00:15:59,630
challenges

00:16:01,179 --> 00:16:07,249
also any of the stuff that is

00:16:05,469 --> 00:16:08,179
communicating inside the OpenStack

00:16:07,249 --> 00:16:09,979
environment so if you're not familiar

00:16:08,179 --> 00:16:11,629
OpenStack it is the concept of private

00:16:09,979 --> 00:16:13,519
networks and then floating IPs floating

00:16:11,629 --> 00:16:16,939
a piece of the public IPS to talk to the

00:16:13,519 --> 00:16:18,799
outside world so generally the the best

00:16:16,939 --> 00:16:20,389
practice is anything any of the pieces

00:16:18,799 --> 00:16:22,909
communicating with each other even your

00:16:20,389 --> 00:16:24,979
own so like let's say you're using ELQ

00:16:22,909 --> 00:16:27,199
as you're logging destination so you're

00:16:24,979 --> 00:16:30,769
gonna log everything to ELQ you need to

00:16:27,199 --> 00:16:32,809
have the you know the logs coming from

00:16:30,769 --> 00:16:34,519
longer Gator and from Cloud Foundry to

00:16:32,809 --> 00:16:35,869
go there if you go out through the

00:16:34,519 --> 00:16:37,789
floating IPs

00:16:35,869 --> 00:16:39,339
you're going through the net Neutron

00:16:37,789 --> 00:16:41,269
router so you're adding additional

00:16:39,339 --> 00:16:43,399
processing that needs to happen and more

00:16:41,269 --> 00:16:45,289
load on your cloud versus if they can go

00:16:43,399 --> 00:16:47,659
if they can be directly connected with

00:16:45,289 --> 00:16:49,609
inside the private network you'll you'll

00:16:47,659 --> 00:16:50,689
cut down a lot of extra overhead that's

00:16:49,609 --> 00:16:55,149
that way they can communicate directly

00:16:50,689 --> 00:16:55,149
without adding an additional routing

00:16:56,049 --> 00:17:00,409
especially if they're both on your one

00:16:58,549 --> 00:17:01,669
less if you're both in the OpenStack

00:17:00,409 --> 00:17:03,439
environment you know floating ip's on

00:17:01,669 --> 00:17:04,519
both now you're doing it both ways so

00:17:03,439 --> 00:17:05,209
it's hitting the router in both

00:17:04,519 --> 00:17:06,829
directions

00:17:05,209 --> 00:17:08,809
so if you can have them talk over the

00:17:06,829 --> 00:17:11,399
private network you'll save yourself a

00:17:08,809 --> 00:17:16,179
lot of overhead

00:17:11,399 --> 00:17:18,100
this stuff is you know it is where you

00:17:16,179 --> 00:17:19,779
start getting into some it is kind of

00:17:18,100 --> 00:17:21,640
common sense like hey don't open ports

00:17:19,779 --> 00:17:23,819
that you don't need shouldn't keep to

00:17:21,640 --> 00:17:29,590
the minimum that's that's kind of basic

00:17:23,819 --> 00:17:31,299
computer hygiene type stuff but the the

00:17:29,590 --> 00:17:32,440
other thing is we get to certificate

00:17:31,299 --> 00:17:33,760
certificates can always be a challenge

00:17:32,440 --> 00:17:35,860
with anything if you are using

00:17:33,760 --> 00:17:37,899
self-signed cert s-- you have to include

00:17:35,860 --> 00:17:41,740
in your manifest the location of the CA

00:17:37,899 --> 00:17:43,630
that sign the cert I recommend not using

00:17:41,740 --> 00:17:45,039
self-signed cert because they're so easy

00:17:43,630 --> 00:17:48,250
to get certs now with stuff like let's

00:17:45,039 --> 00:17:51,070
encrypt using self-signed cert so just

00:17:48,250 --> 00:17:53,049
asking for problems the other thing

00:17:51,070 --> 00:17:54,190
don't use full admin credentials on your

00:17:53,049 --> 00:17:55,720
Bosch manifest so when you give the

00:17:54,190 --> 00:17:56,980
manifest you have to give it OpenStack

00:17:55,720 --> 00:17:59,169
credentials so it can go build all this

00:17:56,980 --> 00:18:00,519
stuff if you're giving it full admin

00:17:59,169 --> 00:18:03,340
credentials it can literally do anything

00:18:00,519 --> 00:18:04,809
to your underlying OpenStack cloud you

00:18:03,340 --> 00:18:09,010
know delete entire networks the entire

00:18:04,809 --> 00:18:10,419
machines whatever and yeah minimize your

00:18:09,010 --> 00:18:13,330
use of floating IPS as much as possible

00:18:10,419 --> 00:18:15,399
every single node doesn't need to have a

00:18:13,330 --> 00:18:17,230
floating IP only the things that need to

00:18:15,399 --> 00:18:19,029
actually communicate with the external

00:18:17,230 --> 00:18:20,139
network so if you're using you know

00:18:19,029 --> 00:18:22,750
whatever kind of load balancer for

00:18:20,139 --> 00:18:24,490
example if using f5 or H a proxy or data

00:18:22,750 --> 00:18:26,230
power or one of those obviously they

00:18:24,490 --> 00:18:29,230
need to be have a public-facing IP

00:18:26,230 --> 00:18:31,840
address but all the individual CF

00:18:29,230 --> 00:18:34,510
components don't need to have public IP

00:18:31,840 --> 00:18:35,889
addresses on the when I say public I

00:18:34,510 --> 00:18:37,750
mean just outside the OpenStack club

00:18:35,889 --> 00:18:40,409
that could be still on your private

00:18:37,750 --> 00:18:40,409
internal network

00:18:41,250 --> 00:18:46,570
so this also another challenge too is

00:18:44,440 --> 00:18:50,670
how some of these components will get

00:18:46,570 --> 00:18:53,110
out to the Internet to get new releases

00:18:50,670 --> 00:18:55,299
or even during the initial deployment so

00:18:53,110 --> 00:18:56,590
the Bosch when you in the manifest you

00:18:55,299 --> 00:18:58,660
tell it which release you want to use

00:18:56,590 --> 00:19:01,360
it's going to go out pull it in and then

00:18:58,660 --> 00:19:03,250
deploy it so if you're behind a firewall

00:19:01,360 --> 00:19:05,620
or proxy and it's limited in its ability

00:19:03,250 --> 00:19:06,700
get to the Internet you're gonna have a

00:19:05,620 --> 00:19:08,140
problem because Bosch is gonna be able

00:19:06,700 --> 00:19:09,340
pull its components in and then

00:19:08,140 --> 00:19:10,870
depending on the component or deploying

00:19:09,340 --> 00:19:12,880
some of them are being compiled and

00:19:10,870 --> 00:19:14,890
built locally - so again it has to pull

00:19:12,880 --> 00:19:19,210
that pull those things down

00:19:14,890 --> 00:19:20,830
so from an OpenStack perspective you

00:19:19,210 --> 00:19:22,720
don't need to have a public floating IP

00:19:20,830 --> 00:19:24,460
to get to the Internet but what kept

00:19:22,720 --> 00:19:26,830
what we've seen catch a lot of customers

00:19:24,460 --> 00:19:28,750
up is they say oh we need to give this

00:19:26,830 --> 00:19:31,059
VM access and it has this private IP

00:19:28,750 --> 00:19:33,220
address and we give it outbound for four

00:19:31,059 --> 00:19:35,200
three access and it still doesn't work

00:19:33,220 --> 00:19:38,590
and the reason is is if it does not have

00:19:35,200 --> 00:19:39,940
its floating IP all this source from the

00:19:38,590 --> 00:19:41,350
rest of your networks perspective all

00:19:39,940 --> 00:19:43,210
the source traffic is coming from that

00:19:41,350 --> 00:19:47,640
gateway IP so you're actually letting

00:19:43,210 --> 00:19:52,120
the gateway IP out over 443 or whatever

00:19:47,640 --> 00:19:55,650
also Cloud Foundry out of the box does

00:19:52,120 --> 00:19:58,510
not support SSL packet inspection so

00:19:55,650 --> 00:20:00,630
what we've seen a lot of larger

00:19:58,510 --> 00:20:03,910
especially larger companies they'll do

00:20:00,630 --> 00:20:06,250
have a different certificate that

00:20:03,910 --> 00:20:08,650
they've trusted to inspect all the SSL

00:20:06,250 --> 00:20:09,340
traffic going off their network so

00:20:08,650 --> 00:20:11,890
basically they're doing a

00:20:09,340 --> 00:20:15,520
man-in-the-middle Cloud Foundry does not

00:20:11,890 --> 00:20:16,720
handle that so in these cases when they

00:20:15,520 --> 00:20:18,669
need to go out to the Internet to get

00:20:16,720 --> 00:20:20,110
those things we generally customers have

00:20:18,669 --> 00:20:23,650
had to whitelist the Cloud Foundry

00:20:20,110 --> 00:20:26,320
environment to say don't do as a cell

00:20:23,650 --> 00:20:29,130
packet inspection on these IPS because

00:20:26,320 --> 00:20:31,120
it just won't work that so break it

00:20:29,130 --> 00:20:32,950
the only thing we'll work with is one

00:20:31,120 --> 00:20:36,429
that's if it's an Internet Authority

00:20:32,950 --> 00:20:39,490
signed certificate so from my experience

00:20:36,429 --> 00:20:40,809
most customers don't they have a self

00:20:39,490 --> 00:20:42,520
sign internal server because they can

00:20:40,809 --> 00:20:43,780
push that CA cert to all their laptops

00:20:42,520 --> 00:20:44,919
or whatever so they're like that time

00:20:43,780 --> 00:20:46,690
they don't want to pay for an external

00:20:44,919 --> 00:20:48,390
cert and this is the type of stuff that

00:20:46,690 --> 00:20:51,550
breaks

00:20:48,390 --> 00:20:56,770
as I mentioned earlier optimizing

00:20:51,550 --> 00:20:59,590
capacity so making sure that your

00:20:56,770 --> 00:21:02,380
OpenStack flavors are right to match

00:20:59,590 --> 00:21:05,530
your Cloud Foundry deploy so as you see

00:21:02,380 --> 00:21:07,450
the defaults in OpenStack come are very

00:21:05,530 --> 00:21:09,430
similar to the AWS ones they start with

00:21:07,450 --> 00:21:11,950
M you know and one small you know all

00:21:09,430 --> 00:21:15,700
that kind of stuff is we create a set of

00:21:11,950 --> 00:21:17,800
flavors specifically for for Cloud

00:21:15,700 --> 00:21:19,660
Foundry so we start with CF and then the

00:21:17,800 --> 00:21:21,520
different size machines for the

00:21:19,660 --> 00:21:25,570
different roles so that way we can get

00:21:21,520 --> 00:21:26,890
the sizes down as much as possible the

00:21:25,570 --> 00:21:29,200
other thing you'll run into if you're

00:21:26,890 --> 00:21:30,370
not familiar with this part of OpenStack

00:21:29,200 --> 00:21:32,680
opens I guess the concept of the

00:21:30,370 --> 00:21:35,740
metadata service so cloud in it when a

00:21:32,680 --> 00:21:39,610
VM boots it actually connects to a you

00:21:35,740 --> 00:21:40,930
know internal HTTP address and pulls

00:21:39,610 --> 00:21:43,300
adds configure information that's where

00:21:40,930 --> 00:21:45,370
you can feed it additional details of

00:21:43,300 --> 00:21:46,840
what to do when it boots up so hey when

00:21:45,370 --> 00:21:49,540
you boot up then run this script or

00:21:46,840 --> 00:21:51,430
things like that some people have that

00:21:49,540 --> 00:21:53,200
turned off and they use a thing called

00:21:51,430 --> 00:21:55,630
config Drive you eat instead which is

00:21:53,200 --> 00:21:58,810
instead of it going to an HTTP address

00:21:55,630 --> 00:22:00,550
it mounts and I so that you know bounce

00:21:58,810 --> 00:22:02,980
what looks like - it is a cd-rom drive

00:22:00,550 --> 00:22:04,840
so I actually have to configure that in

00:22:02,980 --> 00:22:06,580
your Bosh manifest otherwise it'll fail

00:22:04,840 --> 00:22:08,050
when it goes they deploy because it

00:22:06,580 --> 00:22:10,420
can't figure out how to tell the VMS

00:22:08,050 --> 00:22:12,670
what to do so that's something that we

00:22:10,420 --> 00:22:13,780
ran into and we saw a customer and while

00:22:12,670 --> 00:22:16,830
I wasn't working it turned out there the

00:22:13,780 --> 00:22:16,830
metadata service turned off

00:22:16,859 --> 00:22:20,769
this is this is one of the bigger

00:22:18,969 --> 00:22:24,940
challenges is if you're gonna share this

00:22:20,769 --> 00:22:26,799
environment with a with non Cloud

00:22:24,940 --> 00:22:28,509
Foundry workloads so we have some

00:22:26,799 --> 00:22:29,739
customers there the cloud fat the open

00:22:28,509 --> 00:22:31,959
stacks only there just to run Cloud

00:22:29,739 --> 00:22:33,609
Foundry in other cases they they want to

00:22:31,959 --> 00:22:36,219
run other workloads and have cloud

00:22:33,609 --> 00:22:38,499
foundry also so the challenges can be

00:22:36,219 --> 00:22:41,320
here are the going back to the

00:22:38,499 --> 00:22:43,089
credentials so admin is too much but

00:22:41,320 --> 00:22:46,629
some of the individual tenant admins are

00:22:43,089 --> 00:22:47,950
not enough so what you really have to do

00:22:46,629 --> 00:22:50,019
is you can't use anything out of the

00:22:47,950 --> 00:22:52,209
bottles what we generally recommend is

00:22:50,019 --> 00:22:54,729
specialized roles just for Bosch to use

00:22:52,209 --> 00:22:58,690
for Cloud Foundry where hey it's a

00:22:54,729 --> 00:23:00,299
tenant admin but it also has some some

00:22:58,690 --> 00:23:03,429
higher-level stuff like the ability to

00:23:00,299 --> 00:23:05,799
to change flavors and quotas are a big

00:23:03,429 --> 00:23:07,779
one modifying quotas is really an admin

00:23:05,799 --> 00:23:09,700
thing not a tenant admin thing so that's

00:23:07,779 --> 00:23:11,200
that's something we've run into too so

00:23:09,700 --> 00:23:15,570
you end up having to create a specific

00:23:11,200 --> 00:23:15,570
role just for deploying Cloud Foundry

00:23:15,839 --> 00:23:21,429
like I mentioned OpenStack we have tuned

00:23:18,609 --> 00:23:22,690
releases a year at a minimum right the

00:23:21,429 --> 00:23:25,149
major releases and there's point

00:23:22,690 --> 00:23:26,289
releases in between and and and then

00:23:25,149 --> 00:23:28,959
there's it's a continuous deployment

00:23:26,289 --> 00:23:30,999
model so there may be hot fixes or bugs

00:23:28,959 --> 00:23:33,940
or patches that need to go in as well so

00:23:30,999 --> 00:23:34,899
how do we keep that rolling without

00:23:33,940 --> 00:23:39,279
taking down the Cloud Foundry

00:23:34,899 --> 00:23:42,339
environment moving from one release to

00:23:39,279 --> 00:23:45,999
another in the earlier days people would

00:23:42,339 --> 00:23:48,099
do mic so they do like a new environment

00:23:45,999 --> 00:23:49,509
and then migrate luckily we've gotten

00:23:48,099 --> 00:23:50,799
past that and OpenStack from it for a

00:23:49,509 --> 00:23:53,619
number of releases where you can do

00:23:50,799 --> 00:23:57,519
in-place upgrades and what we basically

00:23:53,619 --> 00:23:59,619
found is we can stagger the upgrades and

00:23:57,519 --> 00:24:01,479
with the controller's I've redundant

00:23:59,619 --> 00:24:03,249
services we can upgrade one flip over to

00:24:01,479 --> 00:24:06,039
the other so we can keep the keep the

00:24:03,249 --> 00:24:09,489
upgrades to a minimum the other place

00:24:06,039 --> 00:24:10,899
where we run into is the hypervisors

00:24:09,489 --> 00:24:13,089
themselves that's when you end up having

00:24:10,899 --> 00:24:16,839
to reboot nodes because there's a kernel

00:24:13,089 --> 00:24:20,440
patch or a hypervisor change so that's

00:24:16,839 --> 00:24:22,329
where you know making sure the the Cloud

00:24:20,440 --> 00:24:24,489
Foundry services and the nodes are on

00:24:22,329 --> 00:24:26,249
what order those come back up is kind of

00:24:24,489 --> 00:24:28,389
important to make sure that you can keep

00:24:26,249 --> 00:24:30,879
keep your instances up and running

00:24:28,389 --> 00:24:33,700
without having application level

00:24:30,879 --> 00:24:35,799
if you have enough de A's or Diego nodes

00:24:33,700 --> 00:24:37,450
the idea is you you know even if you

00:24:35,799 --> 00:24:39,580
lose an app instance you have other ones

00:24:37,450 --> 00:24:41,369
still running behind the go router so

00:24:39,580 --> 00:24:44,860
that shouldn't even be be noticeable

00:24:41,369 --> 00:24:46,509
most of the ones are for the minor

00:24:44,860 --> 00:24:47,649
upgrades or just a little fix here

00:24:46,509 --> 00:24:50,740
little fix there kind of thing that's

00:24:47,649 --> 00:24:52,419
just a service restart basically and

00:24:50,740 --> 00:24:54,850
it's usually only a couple seconds and

00:24:52,419 --> 00:24:59,200
it's generally unnoticed by my Cloud

00:24:54,850 --> 00:25:01,389
Foundry and then if there's ain't config

00:24:59,200 --> 00:25:03,340
changes so something fine later that

00:25:01,389 --> 00:25:04,929
like a parameter has to be changed like

00:25:03,340 --> 00:25:07,119
for example your first ran into that API

00:25:04,929 --> 00:25:10,179
limiting issue you're making those

00:25:07,119 --> 00:25:14,320
changes again restart is just a restart

00:25:10,179 --> 00:25:15,879
of the service so it's not as bad again

00:25:14,320 --> 00:25:18,879
it's it's a couple seconds and surely

00:25:15,879 --> 00:25:20,639
not noticed so the key takeaway I would

00:25:18,879 --> 00:25:22,720
say here is automate everything

00:25:20,639 --> 00:25:24,639
automation automation automation it's

00:25:22,720 --> 00:25:27,460
the only way we're able to keep our

00:25:24,639 --> 00:25:30,850
OpenStack releases upgradable

00:25:27,460 --> 00:25:32,350
supportable you know as we as we support

00:25:30,850 --> 00:25:34,899
you know the number of environments we

00:25:32,350 --> 00:25:36,820
have and it's the same thing with Cloud

00:25:34,899 --> 00:25:38,200
Foundry right using Bosh well we put it

00:25:36,820 --> 00:25:40,059
we put all those pieces together so our

00:25:38,200 --> 00:25:41,860
úrsula tool that we use to deploy the

00:25:40,059 --> 00:25:43,509
OpenStack and then we're using open

00:25:41,860 --> 00:25:44,919
seconds tool called rally to run testing

00:25:43,509 --> 00:25:47,289
against it once it's done to make sure

00:25:44,919 --> 00:25:50,860
it's it's ready to go and run all those

00:25:47,289 --> 00:25:56,409
validation pieces and then the Cloud

00:25:50,860 --> 00:25:58,720
Foundry we there's a a fog this is ruby

00:25:56,409 --> 00:26:00,850
gem called fog which will basically

00:25:58,720 --> 00:26:04,450
discover OpenStack and feed that into

00:26:00,850 --> 00:26:06,309
our bluemix cloud foundry deployment

00:26:04,450 --> 00:26:08,200
tool which basically builds them Bosch

00:26:06,309 --> 00:26:10,960
manifest so that way you know there's no

00:26:08,200 --> 00:26:12,610
chances for mistyped credentials or

00:26:10,960 --> 00:26:14,789
anything like that it's discovering it

00:26:12,610 --> 00:26:14,789
live

00:26:14,920 --> 00:26:18,940
you know making sure that we're refining

00:26:17,110 --> 00:26:22,060
and checking all those all those

00:26:18,940 --> 00:26:24,040
components and that way when we go to do

00:26:22,060 --> 00:26:25,810
the deploys it'll go and create all the

00:26:24,040 --> 00:26:26,980
VM configs create all those on the fly

00:26:25,810 --> 00:26:30,970
so those things have talked about like

00:26:26,980 --> 00:26:32,410
the the the you know specific flavor

00:26:30,970 --> 00:26:35,170
size and everything like that using this

00:26:32,410 --> 00:26:38,260
this automation tool builds all that

00:26:35,170 --> 00:26:40,900
stuff for us automatically pulls down

00:26:38,260 --> 00:26:42,930
the stem cell for the OpenStack

00:26:40,900 --> 00:26:45,280
environment generates the manifest

00:26:42,930 --> 00:26:46,180
deploys micro bosh does the whole thing

00:26:45,280 --> 00:26:48,370
in any way it goes

00:26:46,180 --> 00:26:50,590
so that's kind of how we've been able to

00:26:48,370 --> 00:26:53,260
do this in a very repeatable fashion so

00:26:50,590 --> 00:26:54,580
in customer environments sometimes was

00:26:53,260 --> 00:26:57,540
challenging depending on their

00:26:54,580 --> 00:26:59,500
underlying I as they were giving us

00:26:57,540 --> 00:27:01,810
presented challenges with deploying

00:26:59,500 --> 00:27:04,330
Cloud Foundry now we get this very

00:27:01,810 --> 00:27:07,090
repeatable understandable OpenStack

00:27:04,330 --> 00:27:12,010
environment it's made it much bit make

00:27:07,090 --> 00:27:13,750
deploys much much quicker so to wrap up

00:27:12,010 --> 00:27:16,030
here you know this is kind of how we

00:27:13,750 --> 00:27:18,400
deploy it we use so we do managed

00:27:16,030 --> 00:27:20,650
OpenStack and manage Cloud Foundry so we

00:27:18,400 --> 00:27:21,700
use this on the blue box level we use a

00:27:20,650 --> 00:27:23,440
thing called site controller that

00:27:21,700 --> 00:27:25,840
deploys and manages and upgrades and

00:27:23,440 --> 00:27:27,910
handles all the OpenStack stuff and then

00:27:25,840 --> 00:27:29,920
the bluemix I it's a similar concept we

00:27:27,910 --> 00:27:31,570
call relay so there's basically one

00:27:29,920 --> 00:27:33,310
local machine that then kicks off all

00:27:31,570 --> 00:27:35,140
these other pieces so once we just have

00:27:33,310 --> 00:27:36,520
these base components and we can build

00:27:35,140 --> 00:27:38,320
the entire OpenStack environment and

00:27:36,520 --> 00:27:39,850
build the entire Cloud Foundry

00:27:38,320 --> 00:27:41,990
environment on top of it in a fully

00:27:39,850 --> 00:27:44,630
automated fashion

00:27:41,990 --> 00:27:46,760
so again why customers like Cloud

00:27:44,630 --> 00:27:48,919
Foundry on OpenStack is the fact that

00:27:46,760 --> 00:27:51,679
you're getting 100 open pads environment

00:27:48,919 --> 00:27:53,899
with Cloud Foundry and an open eye as

00:27:51,679 --> 00:27:56,029
environment with OpenStack both of them

00:27:53,899 --> 00:27:58,130
have very strong communities around them

00:27:56,029 --> 00:28:02,260
a lot of contributors a lot of sponsors

00:27:58,130 --> 00:28:04,279
you know besides IBM in both spaces just

00:28:02,260 --> 00:28:05,600
the ability to open-source community

00:28:04,279 --> 00:28:07,789
work together and build these things you

00:28:05,600 --> 00:28:09,740
know like that like the OpenStack CPI is

00:28:07,789 --> 00:28:12,289
now being leveraged by by not just blue

00:28:09,740 --> 00:28:13,490
makes but pivotal and a toss and a whole

00:28:12,289 --> 00:28:14,390
bunch of other companies that are

00:28:13,490 --> 00:28:16,399
deploying on OpenStack

00:28:14,390 --> 00:28:18,590
gives them that capability to work

00:28:16,399 --> 00:28:22,880
together and get you that that fully

00:28:18,590 --> 00:28:24,169
open I as and pass solution so meeting

00:28:22,880 --> 00:28:25,789
those installation requirements for

00:28:24,169 --> 00:28:27,950
Cloud Foundry is very straightforward

00:28:25,789 --> 00:28:29,450
once you kind of once you learn some of

00:28:27,950 --> 00:28:31,970
these lessons and as you're doing your

00:28:29,450 --> 00:28:33,710
deploys it's it gets much easier so

00:28:31,970 --> 00:28:36,529
we've seen the two of them put together

00:28:33,710 --> 00:28:38,690
make a real difference in our private

00:28:36,529 --> 00:28:41,809
cloud foundry environments getting this

00:28:38,690 --> 00:28:43,429
predictable OpenStack layer for our

00:28:41,809 --> 00:28:46,490
clients environments so that way we can

00:28:43,429 --> 00:28:47,929
get a good clean Cloud Foundry installs

00:28:46,490 --> 00:28:49,399
and then a good customer experience all

00:28:47,929 --> 00:28:50,600
right at the end it's about the users a

00:28:49,399 --> 00:28:52,399
developer's experience so they want a

00:28:50,600 --> 00:28:54,020
predictable Cloud Foundry environment

00:28:52,399 --> 00:28:56,299
and clock factory needs a predictable

00:28:54,020 --> 00:29:00,289
environment and we get all that in a in

00:28:56,299 --> 00:29:08,110
a open source easy to use kind of

00:29:00,289 --> 00:29:08,110
environment any questions

00:29:09,130 --> 00:29:11,549

YouTube URL: https://www.youtube.com/watch?v=9D4r5jGSXco


