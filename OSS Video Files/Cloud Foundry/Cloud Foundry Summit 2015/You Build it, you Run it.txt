Title: You Build it, you Run it
Publication date: 2015-05-12
Playlist: Cloud Foundry Summit 2015
Description: 
	You Build it, you Run it - 01 Rene Welches 720p
Captions: 
	00:00:00,060 --> 00:00:06,230
hello and welcome I just got designed to

00:00:02,970 --> 00:00:09,120
start welcome to you build it you run it

00:00:06,230 --> 00:00:11,490
we're we're going to talk about our

00:00:09,120 --> 00:00:15,210
hybrids is a service or yes operational

00:00:11,490 --> 00:00:17,420
model my name is Ronnie Welch's and in

00:00:15,210 --> 00:00:20,430
the second part Johannes will talk about

00:00:17,420 --> 00:00:22,320
how we use monitoring and logging to

00:00:20,430 --> 00:00:26,820
support our product operational model

00:00:22,320 --> 00:00:28,619
and we are working on high precision

00:00:26,820 --> 00:00:32,880
service at the site of cloud foundry and

00:00:28,619 --> 00:00:36,000
monitoring and locking first some word

00:00:32,880 --> 00:00:39,750
about Hyper's we're an SMP company since

00:00:36,000 --> 00:00:41,489
2013 and our main product is Hybris

00:00:39,750 --> 00:00:45,000
commerce suite so we are really focused

00:00:41,489 --> 00:00:49,079
on e-commerce and commerce products and

00:00:45,000 --> 00:00:51,780
I have to rush a little bit of the

00:00:49,079 --> 00:00:54,750
slides because we only have like 30

00:00:51,780 --> 00:00:58,170
minutes and I want to give you a brief

00:00:54,750 --> 00:00:59,760
overview what yes is or better what it's

00:00:58,170 --> 00:01:01,800
not because there's always a little bit

00:00:59,760 --> 00:01:03,420
of confusion about what is Huibers is a

00:01:01,800 --> 00:01:06,540
service and how does it relate to the

00:01:03,420 --> 00:01:08,130
commerce as a service suite Hybris as a

00:01:06,540 --> 00:01:10,500
service or yes is not the Hybris

00:01:08,130 --> 00:01:13,290
commerce suite it's not the future of

00:01:10,500 --> 00:01:15,270
the commerce suite that we have and it's

00:01:13,290 --> 00:01:16,799
new and it doesn't run on Cloud Foundry

00:01:15,270 --> 00:01:18,180
because there was often the questions

00:01:16,799 --> 00:01:19,860
like hey we have customers that are

00:01:18,180 --> 00:01:21,479
having the Hybris commerce suite that's

00:01:19,860 --> 00:01:24,390
the running cloud front we can we run it

00:01:21,479 --> 00:01:26,250
on pivotal cloud foundry no we can't so

00:01:24,390 --> 00:01:29,400
Hybris is a service is a real new

00:01:26,250 --> 00:01:31,439
product so what is Hybris as a service

00:01:29,400 --> 00:01:33,509
iris as a service is a cloud platform

00:01:31,439 --> 00:01:35,909
that allows everyone to easily develop

00:01:33,509 --> 00:01:38,070
intense extents and sell services and

00:01:35,909 --> 00:01:40,650
application it's not necessarily

00:01:38,070 --> 00:01:43,710
restricted to e-commerce so it's really

00:01:40,650 --> 00:01:46,649
an open platform and of course it's

00:01:43,710 --> 00:01:49,590
built on micro services like you can see

00:01:46,649 --> 00:01:51,420
here we have core micro services we have

00:01:49,590 --> 00:01:55,909
a storefront we have common services

00:01:51,420 --> 00:01:58,680
mashups but it's easily to extent and

00:01:55,909 --> 00:02:00,689
therefore I will show you a little bit

00:01:58,680 --> 00:02:04,380
about the Hybris as a service

00:02:00,689 --> 00:02:06,509
architecture on the on the bottom we

00:02:04,380 --> 00:02:09,270
have an infrastructure layer from SVP

00:02:06,509 --> 00:02:11,280
and we are running on Cloud Foundry

00:02:09,270 --> 00:02:12,790
which is operated by the Hana cloud

00:02:11,280 --> 00:02:16,780
platform team

00:02:12,790 --> 00:02:18,549
and what we built on top of this is we

00:02:16,780 --> 00:02:21,400
have core services which are domain

00:02:18,549 --> 00:02:26,290
agnostic like a document or service for

00:02:21,400 --> 00:02:28,299
persistence or an account service and on

00:02:26,290 --> 00:02:31,030
top of these teams build their own

00:02:28,299 --> 00:02:33,489
domain specific services like product or

00:02:31,030 --> 00:02:35,860
cart and then we have a mashup layer

00:02:33,489 --> 00:02:39,060
which I will briefly explain in the next

00:02:35,860 --> 00:02:41,500
slide and then we have the connecting

00:02:39,060 --> 00:02:43,329
application clients which is really

00:02:41,500 --> 00:02:45,190
interesting for for you or for anybody

00:02:43,329 --> 00:02:47,310
that want to use the Hybris as a service

00:02:45,190 --> 00:02:50,470
platform is that you can build your own

00:02:47,310 --> 00:02:52,720
domain agnostic or the mind specific

00:02:50,470 --> 00:02:58,090
services like loyalty so if you go later

00:02:52,720 --> 00:03:00,670
to the SCP booth I we probably can show

00:02:58,090 --> 00:03:02,829
you like a loyalty demo case that some

00:03:00,670 --> 00:03:07,599
of the other ICP team to develop on top

00:03:02,829 --> 00:03:11,680
of cloud phone repair so yes sorry to

00:03:07,599 --> 00:03:15,579
the mashups why do we use the mashups if

00:03:11,680 --> 00:03:18,609
you don't use mashups the client will

00:03:15,579 --> 00:03:21,040
always have to implement error logic

00:03:18,609 --> 00:03:23,380
handling requires a lot of skulls and

00:03:21,040 --> 00:03:26,079
every client has to implement basically

00:03:23,380 --> 00:03:28,480
the same logic which couldn't we'll end

00:03:26,079 --> 00:03:31,930
up in like a lot of calls to the to the

00:03:28,480 --> 00:03:33,780
backend to the services and this will

00:03:31,930 --> 00:03:37,569
also create a lot of network latency

00:03:33,780 --> 00:03:40,480
that's why we introduced the concept of

00:03:37,569 --> 00:03:42,819
mashups and a measure of basically

00:03:40,480 --> 00:03:46,120
bundles the calls to the back end for

00:03:42,819 --> 00:03:49,989
for a client and the mashup is also

00:03:46,120 --> 00:03:52,120
therefore creating resilience so if one

00:03:49,989 --> 00:03:54,370
of the Beck Beck services like let's say

00:03:52,120 --> 00:03:55,989
the video service is down the mashup

00:03:54,370 --> 00:03:58,599
should be so resilient to still send

00:03:55,989 --> 00:04:03,690
back a result to the client and the

00:03:58,599 --> 00:04:03,690
client can handle then the output

00:04:04,190 --> 00:04:09,480
so much to the to the architectural

00:04:07,020 --> 00:04:11,700
background and now I want to go a little

00:04:09,480 --> 00:04:13,770
bit deeper into the and what we actually

00:04:11,700 --> 00:04:17,480
wanted to talk about it's that you built

00:04:13,770 --> 00:04:19,650
your run at paradigm that we have and

00:04:17,480 --> 00:04:23,730
they I want to start like a little bit

00:04:19,650 --> 00:04:25,560
in the past as a headline already says

00:04:23,730 --> 00:04:27,480
experience is what you get when you

00:04:25,560 --> 00:04:29,910
didn't get what you wanted and we

00:04:27,480 --> 00:04:32,010
started our first iteration of of our

00:04:29,910 --> 00:04:36,360
software as-a-service approached about

00:04:32,010 --> 00:04:38,520
four years ago and our first approach

00:04:36,360 --> 00:04:41,790
had like a couple of flaws which

00:04:38,520 --> 00:04:45,570
basically yeah you can see it as the

00:04:41,790 --> 00:04:47,430
first iteration and what we had was we

00:04:45,570 --> 00:04:49,890
had business service teams and course of

00:04:47,430 --> 00:04:52,290
his teams as we have it now but we had a

00:04:49,890 --> 00:04:55,220
specific DevOps team which was

00:04:52,290 --> 00:04:58,290
responsible for packaging the code and

00:04:55,220 --> 00:05:00,300
create puppet script and M collective

00:04:58,290 --> 00:05:04,350
and was responsible for deploying this

00:05:00,300 --> 00:05:06,270
to the tune deafened so the DevOps team

00:05:04,350 --> 00:05:08,070
as soon as it was done with with with

00:05:06,270 --> 00:05:09,420
its part it would hand it over to a so

00:05:08,070 --> 00:05:11,970
called team that's called the delivery

00:05:09,420 --> 00:05:14,100
and the deliver each team was basically

00:05:11,970 --> 00:05:17,640
responsible to roll that out to stage

00:05:14,100 --> 00:05:19,890
test or prod and on the other side we

00:05:17,640 --> 00:05:22,290
had our infrastructure team which was

00:05:19,890 --> 00:05:24,390
basically providing us with virtual

00:05:22,290 --> 00:05:27,030
machines unfortunately it was not like a

00:05:24,390 --> 00:05:28,970
real infrastructure as a service we

00:05:27,030 --> 00:05:34,800
really had to create a request tickets

00:05:28,970 --> 00:05:40,950
on tickets based VMs and all this led to

00:05:34,800 --> 00:05:44,130
to a couple of issues or combined with

00:05:40,950 --> 00:05:46,620
the architecture that we had like really

00:05:44,130 --> 00:05:49,830
long release and deployment size like

00:05:46,620 --> 00:05:52,190
life cycles and if I if I want to break

00:05:49,830 --> 00:05:55,980
them down to the architecture and the

00:05:52,190 --> 00:05:58,169
organizational setup it's in the

00:05:55,980 --> 00:06:00,570
architecture we had we had already a

00:05:58,169 --> 00:06:02,850
none micro approach but it was micro

00:06:00,570 --> 00:06:05,580
applications they were connected through

00:06:02,850 --> 00:06:08,700
an SDK SDK and not a stable API as we

00:06:05,580 --> 00:06:11,789
have it now and independent deployments

00:06:08,700 --> 00:06:14,940
were possible but you could only release

00:06:11,789 --> 00:06:17,370
the software as a whole package which

00:06:14,940 --> 00:06:20,190
ended up in like complex

00:06:17,370 --> 00:06:21,570
and long-running releases the

00:06:20,190 --> 00:06:23,690
architecture also had stickiness

00:06:21,570 --> 00:06:27,090
stickiness which means like we had

00:06:23,690 --> 00:06:28,949
session bound to the services and to the

00:06:27,090 --> 00:06:30,750
applications so we couldn't use like

00:06:28,949 --> 00:06:33,240
zero downtime deployment paradigms like

00:06:30,750 --> 00:06:36,840
Bluegreen deployment and everything was

00:06:33,240 --> 00:06:38,669
only restricted to the Java stack on the

00:06:36,840 --> 00:06:40,530
organizational side I already showed

00:06:38,669 --> 00:06:44,460
that in a previous slide we had like

00:06:40,530 --> 00:06:49,530
this separation between operations and

00:06:44,460 --> 00:06:51,990
delivery and this cost like long

00:06:49,530 --> 00:06:53,580
deployment and release cycles that the

00:06:51,990 --> 00:06:55,139
difference between the development

00:06:53,580 --> 00:06:57,000
environment which was handled actually

00:06:55,139 --> 00:06:58,320
by the development teams and the test

00:06:57,000 --> 00:07:00,090
and stage environments and the

00:06:58,320 --> 00:07:04,880
production environments where sometimes

00:07:00,090 --> 00:07:07,830
like two or three versions behind so

00:07:04,880 --> 00:07:10,830
there was clearly a disconnect between

00:07:07,830 --> 00:07:12,930
the whole organizations and one of the

00:07:10,830 --> 00:07:14,729
the main factors also was that there was

00:07:12,930 --> 00:07:17,039
no real infrastructure as a service

00:07:14,729 --> 00:07:19,199
provider that we could leverage because

00:07:17,039 --> 00:07:23,729
we were always depending we couldn't do

00:07:19,199 --> 00:07:25,500
like any kind of zero downtime

00:07:23,729 --> 00:07:29,160
deployments as we saw in the previous

00:07:25,500 --> 00:07:31,380
talk with Porsche so what did we change

00:07:29,160 --> 00:07:34,530
first of all we decided we have to go

00:07:31,380 --> 00:07:36,030
for it infrastructure as a service at

00:07:34,530 --> 00:07:38,160
the beginning we did that with AWS

00:07:36,030 --> 00:07:40,289
because it was the easiest and fastest

00:07:38,160 --> 00:07:42,360
way to set up and later we switch to a

00:07:40,289 --> 00:07:44,070
SAP in monsoon which is our s ap

00:07:42,360 --> 00:07:46,770
internal infrastructure as a service

00:07:44,070 --> 00:07:49,520
provider this is really essential if you

00:07:46,770 --> 00:07:53,760
want to use a platform as a service like

00:07:49,520 --> 00:07:56,430
Cloud Foundry and we use this also for

00:07:53,760 --> 00:08:02,280
our Beca service deployment which I will

00:07:56,430 --> 00:08:05,070
show later then we introduced a platform

00:08:02,280 --> 00:08:06,870
as a service we started with days for

00:08:05,070 --> 00:08:08,940
prototyping and playing around and

00:08:06,870 --> 00:08:12,539
getting experience but we pretty soon

00:08:08,940 --> 00:08:14,370
switched to cloud foundry and this gave

00:08:12,539 --> 00:08:16,229
our developers basically the freedom to

00:08:14,370 --> 00:08:18,630
self deploying self operate their

00:08:16,229 --> 00:08:20,550
services and also to choose that the

00:08:18,630 --> 00:08:24,240
freedom of of programming liners that

00:08:20,550 --> 00:08:26,430
they want to use then we did a lot of

00:08:24,240 --> 00:08:29,219
changes in the architecture we

00:08:26,430 --> 00:08:29,889
introduced the paradigms that are given

00:08:29,219 --> 00:08:32,589
by the to us

00:08:29,889 --> 00:08:35,950
Danette like one of them is no sticky

00:08:32,589 --> 00:08:38,560
sessions restless state this way we

00:08:35,950 --> 00:08:40,899
could do Bluegreen deployment for zero

00:08:38,560 --> 00:08:44,589
downtime we followed the reactive

00:08:40,899 --> 00:08:47,320
manifesto for resilient services for

00:08:44,589 --> 00:08:48,930
example for scalability and we started

00:08:47,320 --> 00:08:52,510
introducing a micro service architecture

00:08:48,930 --> 00:08:55,810
which resulted like in stable api's so

00:08:52,510 --> 00:08:59,050
we really have now stable api's and

00:08:55,810 --> 00:09:03,940
every change to the api will reflect in

00:08:59,050 --> 00:09:06,880
an increase of a API version number this

00:09:03,940 --> 00:09:10,120
all helped us to introduce basically one

00:09:06,880 --> 00:09:13,149
of our factors the utility run it so the

00:09:10,120 --> 00:09:15,130
dev team that develops the service also

00:09:13,149 --> 00:09:18,310
operates the service through all stages

00:09:15,130 --> 00:09:19,890
even in production they deployed they

00:09:18,310 --> 00:09:22,240
they set up their continuous integration

00:09:19,890 --> 00:09:26,140
they set up their continuous deployment

00:09:22,240 --> 00:09:28,029
process and the micro service

00:09:26,140 --> 00:09:30,490
architecture and the way we use Cloud

00:09:28,029 --> 00:09:32,709
Foundry enables us to do that to have

00:09:30,490 --> 00:09:34,449
independent release and deploy cycles

00:09:32,709 --> 00:09:38,740
between the services because there is no

00:09:34,449 --> 00:09:41,560
dependency between the services and we

00:09:38,740 --> 00:09:43,839
also extended this whole you build your

00:09:41,560 --> 00:09:47,649
on a paradigm to the banking service so

00:09:43,839 --> 00:09:49,600
if a team requires a database for their

00:09:47,649 --> 00:09:54,370
micro service they have to operate and

00:09:49,600 --> 00:09:56,800
deploy this as well so this came then -

00:09:54,370 --> 00:09:59,440
more or less like a view of this how we

00:09:56,800 --> 00:10:03,899
structured our teams so we have on the

00:09:59,440 --> 00:10:06,040
botton we have our service teams which

00:10:03,899 --> 00:10:07,269
working on the core teams and then we

00:10:06,040 --> 00:10:09,579
have the higher level the

00:10:07,269 --> 00:10:11,290
domain-specific teams that build their

00:10:09,579 --> 00:10:15,760
services on top of these core services

00:10:11,290 --> 00:10:18,250
and then we have our UI UX teams which

00:10:15,760 --> 00:10:20,230
are currently separated but I think

00:10:18,250 --> 00:10:27,130
we're going to move then partly into the

00:10:20,230 --> 00:10:29,019
teams I mentioned that also our teams

00:10:27,130 --> 00:10:32,829
managing their banking service on their

00:10:29,019 --> 00:10:35,320
own so in the previous talk we we

00:10:32,829 --> 00:10:39,100
learned that in order to deploy cloud

00:10:35,320 --> 00:10:40,810
foundry you have to use Porsche and we

00:10:39,100 --> 00:10:42,460
extended the whole concept to our teams

00:10:40,810 --> 00:10:46,190
and

00:10:42,460 --> 00:10:47,660
yeah they're using Bosch as well to

00:10:46,190 --> 00:10:51,230
manage and maintain their packing

00:10:47,660 --> 00:10:53,300
services and what we did is like the

00:10:51,230 --> 00:10:55,550
micro Bosch we have Micro boy super team

00:10:53,300 --> 00:10:57,470
set up so each team has their own micro

00:10:55,550 --> 00:11:00,050
Bosch and can deploy independently from

00:10:57,470 --> 00:11:03,710
each other and that's the way how they

00:11:00,050 --> 00:11:09,800
manage their yeah there's two of packing

00:11:03,710 --> 00:11:11,480
services that's it from my side and

00:11:09,800 --> 00:11:13,940
Johannes is gonna now tell a little bit

00:11:11,480 --> 00:11:27,110
how we use monitoring and logging to

00:11:13,940 --> 00:11:28,730
support our model yes hello yeah I guess

00:11:27,110 --> 00:11:32,930
I will skip the introduction of myself

00:11:28,730 --> 00:11:36,520
because yeah we planned to we thought we

00:11:32,930 --> 00:11:38,840
had 40 minutes but 30 minutes yeah will

00:11:36,520 --> 00:11:40,640
we have to speed up now so I'm basically

00:11:38,840 --> 00:11:43,460
one of the guys living in the dark and

00:11:40,640 --> 00:11:50,300
doing some computer stuff so I guess

00:11:43,460 --> 00:11:52,390
basically the same same like you if we

00:11:50,300 --> 00:11:53,840
talk about logging and monitoring in

00:11:52,390 --> 00:11:56,030
micro-service are you take two

00:11:53,840 --> 00:12:00,950
architectures we are facing a lot of

00:11:56,030 --> 00:12:05,540
issues or challenges if we have looked

00:12:00,950 --> 00:12:06,890
into the old pair old days it's really

00:12:05,540 --> 00:12:11,960
hard to keep an overview what's

00:12:06,890 --> 00:12:13,820
basically going on in my system and my

00:12:11,960 --> 00:12:15,770
first employee I had just said one box

00:12:13,820 --> 00:12:16,370
right if there was something wrong I had

00:12:15,770 --> 00:12:19,250
a look at it

00:12:16,370 --> 00:12:20,600
and could fix it now I have to at least

00:12:19,250 --> 00:12:23,450
in Cloud Foundry I don't know how many

00:12:20,600 --> 00:12:25,339
components are there about 10 so at

00:12:23,450 --> 00:12:27,470
least to Deepak Cloud Foundry itself

00:12:25,339 --> 00:12:31,100
it's more like a hell if you do it with

00:12:27,470 --> 00:12:32,900
a old-school ways and if we have a look

00:12:31,100 --> 00:12:38,660
onto our services deployed on top of

00:12:32,900 --> 00:12:40,610
Cloud Foundry it's going even worth on

00:12:38,660 --> 00:12:43,160
top of that each team can pick whatever

00:12:40,610 --> 00:12:45,050
they like to so they can change try it

00:12:43,160 --> 00:12:47,570
which is whatever technologies they like

00:12:45,050 --> 00:12:50,600
to use we got some teams using go others

00:12:47,570 --> 00:12:53,690
are using a lot of them using Java

00:12:50,600 --> 00:12:57,080
I guess somebody is using a car I don't

00:12:53,690 --> 00:12:59,750
even know what it is so we have to find

00:12:57,080 --> 00:13:03,340
a way basically to monitor and lock all

00:12:59,750 --> 00:13:07,820
this through one single pipeline and to

00:13:03,340 --> 00:13:11,090
somehow get a get a sim you have view on

00:13:07,820 --> 00:13:13,130
the different technologies then there

00:13:11,090 --> 00:13:15,140
are different app scopes so let's

00:13:13,130 --> 00:13:17,270
imagine we have a product service if you

00:13:15,140 --> 00:13:20,260
click in your shop you'll like to see

00:13:17,270 --> 00:13:22,850
immediately the the details of a product

00:13:20,260 --> 00:13:25,670
on the others hand side we get some

00:13:22,850 --> 00:13:27,260
really slowly responding services like

00:13:25,670 --> 00:13:28,880
the checkout service the checkout

00:13:27,260 --> 00:13:30,830
services in the backend checking if your

00:13:28,880 --> 00:13:31,910
credit card is valid is if your address

00:13:30,830 --> 00:13:34,550
is correct

00:13:31,910 --> 00:13:37,910
and so on and so forth so the checkout

00:13:34,550 --> 00:13:39,830
service is probably really much slower

00:13:37,910 --> 00:13:42,890
responding than a fast responding

00:13:39,830 --> 00:13:46,370
product service so you can't apply the

00:13:42,890 --> 00:13:47,810
same rules to each app and last but not

00:13:46,370 --> 00:13:50,450
least you always get some kind of

00:13:47,810 --> 00:13:51,740
platform influence so if our

00:13:50,450 --> 00:13:54,110
infrastructure is the service provider

00:13:51,740 --> 00:13:57,460
is not going running well or if we are

00:13:54,110 --> 00:14:00,260
doing a bad deployment you will see

00:13:57,460 --> 00:14:04,040
probably on up level some some influence

00:14:00,260 --> 00:14:06,080
and to somehow get all of these

00:14:04,040 --> 00:14:10,360
different points into to one big

00:14:06,080 --> 00:14:12,890
overview we sketched out in the first

00:14:10,360 --> 00:14:16,670
draft or in the first try the following

00:14:12,890 --> 00:14:19,100
architecture starting from Cloud Foundry

00:14:16,670 --> 00:14:22,280
we are sending metric and event data

00:14:19,100 --> 00:14:24,770
over to Riemann and from riemann we are

00:14:22,280 --> 00:14:29,180
starting the alerting the we are doing

00:14:24,770 --> 00:14:30,650
some some data a combination check if

00:14:29,180 --> 00:14:34,630
one services up then should the other

00:14:30,650 --> 00:14:36,740
run as well and so on and at least

00:14:34,630 --> 00:14:39,620
alerting the team's through big drops

00:14:36,740 --> 00:14:42,730
and storing all the metric data into

00:14:39,620 --> 00:14:45,680
graphite one of the downsides of riemann

00:14:42,730 --> 00:14:49,100
since then at least i am not the real

00:14:45,680 --> 00:14:54,290
big enclosure it's hard to configure it

00:14:49,100 --> 00:14:57,950
and at least it is not really scalable

00:14:54,290 --> 00:14:59,870
there is for for women you can program

00:14:57,950 --> 00:15:03,950
some scaling in there but

00:14:59,870 --> 00:15:05,779
it's not really scaling well on the

00:15:03,950 --> 00:15:08,750
other hand side we are running the the

00:15:05,779 --> 00:15:11,600
lock locks for a different approach we

00:15:08,750 --> 00:15:14,720
are using locks - with the dis lock end

00:15:11,600 --> 00:15:16,130
point where you can you can configure in

00:15:14,720 --> 00:15:18,020
cloud for users this lock drain URL

00:15:16,130 --> 00:15:21,680
sending all the data over to locks -

00:15:18,020 --> 00:15:24,070
when it's sort or cashed in Redis send

00:15:21,680 --> 00:15:28,430
over into a lock - indexer and finally

00:15:24,070 --> 00:15:34,220
end up in elasticsearch mmm that's quite

00:15:28,430 --> 00:15:38,029
ok but yeah locks - is not really

00:15:34,220 --> 00:15:44,180
comfortable to use like the thing we we

00:15:38,029 --> 00:15:46,460
are doing now in October cloud foundry

00:15:44,180 --> 00:15:50,360
released a new cool feature call it's a

00:15:46,460 --> 00:15:52,130
fire hose so this is basically the the

00:15:50,360 --> 00:15:56,029
underlying architecture you get some

00:15:52,130 --> 00:15:57,950
some sources where locks are sent over

00:15:56,029 --> 00:16:01,190
to the Metron age and installed on the

00:15:57,950 --> 00:16:06,230
VM the Metron agents are shipping over

00:16:01,190 --> 00:16:10,279
the the lock messages to do table where

00:16:06,230 --> 00:16:12,790
it's buffered if we you have configured

00:16:10,279 --> 00:16:16,370
this lock drain URL the the locks are

00:16:12,790 --> 00:16:18,290
sent to this lock directly otherwise the

00:16:16,370 --> 00:16:22,160
local Gator traffic controller is taking

00:16:18,290 --> 00:16:25,100
care that if you you asked for some

00:16:22,160 --> 00:16:27,529
locks you will get them and that's

00:16:25,100 --> 00:16:30,709
basically the way how the CFC LIF use

00:16:27,529 --> 00:16:32,720
run CF locks is working and on a much

00:16:30,709 --> 00:16:36,350
bigger scale the fire hose is doing the

00:16:32,720 --> 00:16:38,600
same job if you request or if you

00:16:36,350 --> 00:16:42,140
connect to the fire hose you will get

00:16:38,600 --> 00:16:44,959
the locks of the entire system and one

00:16:42,140 --> 00:16:47,870
of the main benefits of the the fire

00:16:44,959 --> 00:16:51,800
hose is it's also scalable so if you

00:16:47,870 --> 00:16:54,709
connect different client was the same

00:16:51,800 --> 00:16:58,160
client ID several times the log messages

00:16:54,709 --> 00:17:01,570
are spread equally over the whole fire

00:16:58,160 --> 00:17:04,339
hose and this is a really great thing

00:17:01,570 --> 00:17:06,050
because now you can can run a smaller

00:17:04,339 --> 00:17:08,390
build a small app deploy to Cloud

00:17:06,050 --> 00:17:10,730
Foundry connected to the firehouse and

00:17:08,390 --> 00:17:12,069
if this is not working anymore you don't

00:17:10,730 --> 00:17:14,679
have to do a whole

00:17:12,069 --> 00:17:19,600
deployment like Cornelia told us you

00:17:14,679 --> 00:17:24,100
just have run cf scale and you're on the

00:17:19,600 --> 00:17:26,169
safe side what we so what we are

00:17:24,100 --> 00:17:29,830
currently building based on this this

00:17:26,169 --> 00:17:32,140
poplar in mind we still have all the the

00:17:29,830 --> 00:17:35,380
backing services and the application and

00:17:32,140 --> 00:17:37,840
Cloud Foundry component locks itself but

00:17:35,380 --> 00:17:39,970
what we are doing now we are sending all

00:17:37,840 --> 00:17:42,460
these different sources of lock

00:17:39,970 --> 00:17:47,429
information and metrics information into

00:17:42,460 --> 00:17:50,409
Doppler just to as a note at the side

00:17:47,429 --> 00:17:53,740
there is a client available you can can

00:17:50,409 --> 00:17:55,390
use to integrate also making services

00:17:53,740 --> 00:17:58,570
deployed not into the Cloud Foundry

00:17:55,390 --> 00:18:01,059
cluster so yeah that's something we will

00:17:58,570 --> 00:18:02,740
try out soon but at least the

00:18:01,059 --> 00:18:06,940
application logs and Cloud Foundry locks

00:18:02,740 --> 00:18:10,240
itself are running through Doppler then

00:18:06,940 --> 00:18:12,820
we implemented a lock parser that's the

00:18:10,240 --> 00:18:14,529
thing I described before so the NOAA

00:18:12,820 --> 00:18:21,669
client is basically the component

00:18:14,529 --> 00:18:24,429
component pivotal is providing us to to

00:18:21,669 --> 00:18:28,390
get the locks out of the firehose and we

00:18:24,429 --> 00:18:30,580
build a lock parser which is basically

00:18:28,390 --> 00:18:32,830
checking which kind of messages message

00:18:30,580 --> 00:18:34,470
are you and are you a normal lock

00:18:32,830 --> 00:18:38,289
message then you will pass with the

00:18:34,470 --> 00:18:39,760
first rule if you're maybe a Ruta

00:18:38,289 --> 00:18:42,630
message here will be parsed with the

00:18:39,760 --> 00:18:46,330
Ruta rule and basically we are

00:18:42,630 --> 00:18:49,240
distinguishing on lock level between two

00:18:46,330 --> 00:18:50,919
different lock types

00:18:49,240 --> 00:18:53,220
first the normal lock type where we just

00:18:50,919 --> 00:18:58,299
get maybe a reef s ID and so on and

00:18:53,220 --> 00:19:00,309
second we got a metric lock type so we

00:18:58,299 --> 00:19:04,000
removed this whole riemann

00:19:00,309 --> 00:19:07,270
infrastructure sorry and we are now

00:19:04,000 --> 00:19:11,620
sending over the metric data through

00:19:07,270 --> 00:19:13,270
locks to because in the past we had the

00:19:11,620 --> 00:19:16,330
problem that if somebody is using some

00:19:13,270 --> 00:19:21,639
strange programming language or some

00:19:16,330 --> 00:19:24,850
some some unknown some some unknown

00:19:21,639 --> 00:19:25,570
technology they probably had to

00:19:24,850 --> 00:19:28,930
implement

00:19:25,570 --> 00:19:31,480
by their own doramin agent basically for

00:19:28,930 --> 00:19:34,630
standard in and sauna or for son and

00:19:31,480 --> 00:19:36,430
heir own standard oz every programming

00:19:34,630 --> 00:19:39,010
language was able to lock to standard

00:19:36,430 --> 00:19:40,330
error and standard out and that was the

00:19:39,010 --> 00:19:43,750
reason why we decide to send

00:19:40,330 --> 00:19:46,420
everything's through lock messages just

00:19:43,750 --> 00:19:48,400
to that nobody can complain I don't have

00:19:46,420 --> 00:19:52,570
a client for this specific monitoring

00:19:48,400 --> 00:19:55,330
tool from there we are handing over all

00:19:52,570 --> 00:20:00,340
the data into a spark cluster it's

00:19:55,330 --> 00:20:04,480
basically done to get a replacement for

00:20:00,340 --> 00:20:08,560
'man spark itself can do the same as

00:20:04,480 --> 00:20:12,190
reman from my perspective with a benefit

00:20:08,560 --> 00:20:16,780
of scaling so now we can even if there

00:20:12,190 --> 00:20:18,490
is a huge huge traffic on the interlock

00:20:16,780 --> 00:20:22,180
system we can just scale out in this

00:20:18,490 --> 00:20:24,610
Park and we are done on the output side

00:20:22,180 --> 00:20:26,950
yeah we are storing all the data and

00:20:24,610 --> 00:20:29,850
elasticsearch where we got where the

00:20:26,950 --> 00:20:33,460
teams can build nice dashboards on top

00:20:29,850 --> 00:20:38,340
and the alerting is done through victor

00:20:33,460 --> 00:20:40,450
ops which yeah kind of patriot duty and

00:20:38,340 --> 00:20:43,900
yeah there will will

00:20:40,450 --> 00:20:45,880
several more systems con consuming they

00:20:43,900 --> 00:20:50,830
lock data because yeah you can do some

00:20:45,880 --> 00:20:56,350
predictions on top of logging data for

00:20:50,830 --> 00:20:58,570
instance you can yeah maybe some some

00:20:56,350 --> 00:21:02,160
some cuspids customer related data can

00:20:58,570 --> 00:21:02,160
be fetched there and so on and so forth

00:21:04,200 --> 00:21:11,440
if we have a look back to the to the

00:21:08,770 --> 00:21:15,250
challenges we face in the on the

00:21:11,440 --> 00:21:17,350
beginning the overview yeah it's still a

00:21:15,250 --> 00:21:20,260
tricky thing because you don't get this

00:21:17,350 --> 00:21:23,170
nice UI every no okay here's flowing

00:21:20,260 --> 00:21:25,150
from here to there but i already

00:21:23,170 --> 00:21:28,000
mentioned the the breakfast ID we

00:21:25,150 --> 00:21:30,760
established in all our services and so

00:21:28,000 --> 00:21:32,800
if in lock message or if somebody is

00:21:30,760 --> 00:21:35,290
setting one of our endpoints this

00:21:32,800 --> 00:21:38,550
request ID is taken through the whole

00:21:35,290 --> 00:21:41,920
system and at the end of the day you can

00:21:38,550 --> 00:21:44,410
have a look in lesyk search for search

00:21:41,920 --> 00:21:47,110
for this single request ID and you get

00:21:44,410 --> 00:21:51,810
in basically all the end points this

00:21:47,110 --> 00:21:51,810
request from the outside heated

00:21:53,370 --> 00:21:59,590
maybe some some other yeah part where

00:21:57,100 --> 00:22:03,670
you can can leverage this output thing

00:21:59,590 --> 00:22:08,230
sinks build automatic graphs based on

00:22:03,670 --> 00:22:10,060
this data different technology I already

00:22:08,230 --> 00:22:13,360
mentioned we are shipping everything

00:22:10,060 --> 00:22:17,260
through locks so if there is somebody

00:22:13,360 --> 00:22:21,150
not able to send our locks is really for

00:22:17,260 --> 00:22:21,150
sure not taking the right technology

00:22:22,290 --> 00:22:28,900
different app scopes in remand we had a

00:22:27,430 --> 00:22:30,640
single Bosch deployment to install

00:22:28,900 --> 00:22:33,280
Rieman and configure women and so on

00:22:30,640 --> 00:22:35,110
with SPARC we are now able to allow

00:22:33,280 --> 00:22:40,060
teams to deploy their own monitoring

00:22:35,110 --> 00:22:42,280
rules and hopefully the teams will do

00:22:40,060 --> 00:22:46,000
this to cover their specific needs of

00:22:42,280 --> 00:22:48,640
monitoring and the platform and

00:22:46,000 --> 00:22:51,730
influence since we got all the lock

00:22:48,640 --> 00:22:54,160
messages about response time which error

00:22:51,730 --> 00:22:57,100
codes or which which shake HTTP response

00:22:54,160 --> 00:22:59,440
codes and so on and so forth and we are

00:22:57,100 --> 00:23:02,830
able to identify if the whole

00:22:59,440 --> 00:23:08,950
performance of the t platforms is going

00:23:02,830 --> 00:23:11,910
up or going down and so at the end we

00:23:08,950 --> 00:23:14,470
really get a good overview not about all

00:23:11,910 --> 00:23:16,330
specific components we can also get a

00:23:14,470 --> 00:23:27,550
really good overview of the the entire

00:23:16,330 --> 00:23:30,660
system I was faster than expected are

00:23:27,550 --> 00:23:30,660
the questions should we

00:23:44,690 --> 00:23:49,700
with CPU disk space and so on and so

00:23:48,200 --> 00:23:51,470
forth that's something we like to

00:23:49,700 --> 00:23:54,650
integrate there too at the moment we are

00:23:51,470 --> 00:23:57,110
we are doing this with Riemann there are

00:23:54,650 --> 00:23:59,240
basically this Riemann health tools so

00:23:57,110 --> 00:24:01,190
maybe I have not mentioned that we're in

00:23:59,240 --> 00:24:03,230
a transition phase at the moment so the

00:24:01,190 --> 00:24:05,810
app locks and metrics are going through

00:24:03,230 --> 00:24:09,620
the top loop through the top loop Roche

00:24:05,810 --> 00:24:13,730
the other things are partially still

00:24:09,620 --> 00:24:16,160
going through Riemann and yeah we will

00:24:13,730 --> 00:24:19,670
there is I guess in the NOAA project

00:24:16,160 --> 00:24:23,630
already some some small piece where you

00:24:19,670 --> 00:24:26,900
can some example file how to get system

00:24:23,630 --> 00:24:40,930
metrics through Doppler but now that's

00:24:26,900 --> 00:24:40,930
I'm not 100% sure yep oh that's

00:24:44,490 --> 00:24:47,630
[Music]

00:24:52,630 --> 00:24:57,410
definitely a learning curve for the

00:24:54,710 --> 00:24:59,600
teams but it was interesting you can

00:24:57,410 --> 00:25:02,570
find like in in in almost any team you

00:24:59,600 --> 00:25:05,270
can find somebody that that discovers

00:25:02,570 --> 00:25:07,850
his his interest in in operational tasks

00:25:05,270 --> 00:25:09,920
so what we did is like because we we had

00:25:07,850 --> 00:25:12,710
the Bosch experience we started like

00:25:09,920 --> 00:25:14,720
kind of one-day workshops with the teams

00:25:12,710 --> 00:25:17,480
where we told them the basics of Bosch

00:25:14,720 --> 00:25:19,970
and from there on I was like one or two

00:25:17,480 --> 00:25:21,530
guys in each team that was taking over

00:25:19,970 --> 00:25:23,060
the task and and doing the Bosch

00:25:21,530 --> 00:25:26,030
deployments and playing around and

00:25:23,060 --> 00:25:27,860
improving them because for example for a

00:25:26,030 --> 00:25:29,690
MongoDB our team we didn't have the

00:25:27,860 --> 00:25:31,850
experience that the developer teams has

00:25:29,690 --> 00:25:34,940
like what what we can how to improve the

00:25:31,850 --> 00:25:36,440
MongoDB but these teams had it and they

00:25:34,940 --> 00:25:39,100
just had to apply that then to their

00:25:36,440 --> 00:25:39,100
Bosh deployments

00:25:40,990 --> 00:25:51,850
uh both yes or no

00:25:46,179 --> 00:25:53,710
I guess to sum it up you have to

00:25:51,850 --> 00:25:55,090
automate it somehow and if you're using

00:25:53,710 --> 00:25:56,320
puppet you have to learn something and

00:25:55,090 --> 00:25:58,300
if you're using boss you have to learn

00:25:56,320 --> 00:26:00,850
something as well so at the end of the

00:25:58,300 --> 00:26:02,559
day we could provide them an in-house

00:26:00,850 --> 00:26:04,990
workshop where they only have to pray

00:26:02,559 --> 00:26:06,850
some travel costs instead of hiring

00:26:04,990 --> 00:26:17,830
somebody from the outside you could do

00:26:06,850 --> 00:26:19,890
this for instance okay thank you thank

00:26:17,830 --> 00:26:19,890

YouTube URL: https://www.youtube.com/watch?v=8ppOgcj5G3I


