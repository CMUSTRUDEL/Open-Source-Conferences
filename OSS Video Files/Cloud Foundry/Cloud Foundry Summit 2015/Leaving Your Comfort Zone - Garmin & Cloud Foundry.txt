Title: Leaving Your Comfort Zone - Garmin & Cloud Foundry
Publication date: 2015-05-12
Playlist: Cloud Foundry Summit 2015
Description: 
	Leaving Your Comfort Zone - Garmin & Cloud Foundry - 03 Alex Curtis, Brandon Henry, Jonathan Regehr 720p
Captions: 
	00:00:00,380 --> 00:00:08,460
good afternoon how's it going Wow more

00:00:05,520 --> 00:00:10,800
people we were expecting I'm Alex Curtis

00:00:08,460 --> 00:00:13,170
I'm a application a bin at Garmin

00:00:10,800 --> 00:00:14,639
International I'm joined here you guys

00:00:13,170 --> 00:00:17,279
will be here a minute Jonathan routier

00:00:14,639 --> 00:00:19,550
who is application development and burn

00:00:17,279 --> 00:00:22,230
and Henry's system administration and

00:00:19,550 --> 00:00:24,300
we're here to tell you guys a story

00:00:22,230 --> 00:00:27,000
about how we implemented a pass solution

00:00:24,300 --> 00:00:30,480
in four weeks and pass solution we

00:00:27,000 --> 00:00:31,590
obviously implements Cloud Foundry well

00:00:30,480 --> 00:00:33,630
go ahead we'll go through a little bit

00:00:31,590 --> 00:00:35,790
with you the decision why we went with

00:00:33,630 --> 00:00:37,020
Cloud Foundry some of the how we

00:00:35,790 --> 00:00:39,329
implement it and some of the learning

00:00:37,020 --> 00:00:41,280
opportunities we have along the way we

00:00:39,329 --> 00:00:42,450
do have one caveat though we're we

00:00:41,280 --> 00:00:44,520
actually are still in the middle of

00:00:42,450 --> 00:00:45,840
implementing it most of it or I guess

00:00:44,520 --> 00:00:47,070
we've got most of it in we're still

00:00:45,840 --> 00:00:48,870
getting some pieces in it so we don't

00:00:47,070 --> 00:00:50,430
actually have our apps in production yet

00:00:48,870 --> 00:00:52,320
but that is going to change here in a

00:00:50,430 --> 00:00:54,690
couple weeks so just want to make that

00:00:52,320 --> 00:00:55,770
really clear but we do brain that's

00:00:54,690 --> 00:00:57,930
going to go over here in a minute kind

00:00:55,770 --> 00:00:59,789
of what the status of the the actual

00:00:57,930 --> 00:01:01,949
layout of the environment is now and

00:00:59,789 --> 00:01:05,070
where it's going to be very soon so

00:01:01,949 --> 00:01:06,540
first a little bit about us garma were

00:01:05,070 --> 00:01:11,610
based in Olathe Kansas were founded in

00:01:06,540 --> 00:01:13,290
1989 but dr. min cow and Gary Burrell we

00:01:11,610 --> 00:01:15,030
have annual revenue 2.9 billion our

00:01:13,290 --> 00:01:17,159
markets include marine outdoor fitness

00:01:15,030 --> 00:01:19,520
aviation which is kind of a cornerstone

00:01:17,159 --> 00:01:22,860
for us as well as automotive and mobile

00:01:19,520 --> 00:01:24,689
the past were we're device company and

00:01:22,860 --> 00:01:26,280
we make hardware and there were all the

00:01:24,689 --> 00:01:28,759
all the little doodads you can you can

00:01:26,280 --> 00:01:30,750
think of most of which have GPS in them

00:01:28,759 --> 00:01:39,810
where hardware company probably always

00:01:30,750 --> 00:01:41,729
will be but and for the GPS joke so the

00:01:39,810 --> 00:01:43,170
present the president the future as I

00:01:41,729 --> 00:01:45,299
said we are a device company but however

00:01:43,170 --> 00:01:47,189
due to some ever-changing technology the

00:01:45,299 --> 00:01:49,350
way people are using their phones and

00:01:47,189 --> 00:01:50,729
other devices we're having to expand our

00:01:49,350 --> 00:01:53,280
product lines a little bit so I'm sure

00:01:50,729 --> 00:01:55,079
like everybody seemed like the vivo the

00:01:53,280 --> 00:01:58,799
health and wellness that the push we're

00:01:55,079 --> 00:02:00,060
making in that in that realm so this has

00:01:58,799 --> 00:02:01,829
not only expand the scope of our

00:02:00,060 --> 00:02:04,229
business it's also expanded the scope of

00:02:01,829 --> 00:02:06,270
our IT department so it's Hardware is

00:02:04,229 --> 00:02:07,710
still our thing however we're shifting a

00:02:06,270 --> 00:02:08,940
little bit more in the software we're

00:02:07,710 --> 00:02:12,989
taking a little bit better a little more

00:02:08,940 --> 00:02:13,440
of a look at that so with software that

00:02:12,989 --> 00:02:15,660
we

00:02:13,440 --> 00:02:18,180
look at both what we develop as well as

00:02:15,660 --> 00:02:20,280
what we purchase that software doesn't

00:02:18,180 --> 00:02:22,140
always impact directly on the bottom

00:02:20,280 --> 00:02:24,420
line of companies so efficiency and cost

00:02:22,140 --> 00:02:26,150
actually are real driving force on what

00:02:24,420 --> 00:02:29,220
we choose

00:02:26,150 --> 00:02:31,950
so all these health and fitness apps so

00:02:29,220 --> 00:02:34,350
we have coming in all these devices are

00:02:31,950 --> 00:02:36,440
phoning home and they're coming in to

00:02:34,350 --> 00:02:38,460
and basically these health and fitness

00:02:36,440 --> 00:02:40,530
devices that were building now we're

00:02:38,460 --> 00:02:42,120
hoping to fill the gap of the Nuvi and a

00:02:40,530 --> 00:02:44,220
lot of the AV or a lot of excuse me in

00:02:42,120 --> 00:02:46,650
the automobile pieces so the new video

00:02:44,220 --> 00:02:49,230
not and everybody's buying on on

00:02:46,650 --> 00:02:51,120
dashboard mounted GPS is anymore so we

00:02:49,230 --> 00:02:53,040
have to fill that gap and thankfully a

00:02:51,120 --> 00:02:54,810
few years ago they saw that and before

00:02:53,040 --> 00:02:56,100
it actually started trending down and so

00:02:54,810 --> 00:02:57,960
the health and fitness stuff is is

00:02:56,100 --> 00:03:00,840
really is really kicking off so all

00:02:57,960 --> 00:03:03,210
these devices are foaming back home to

00:03:00,840 --> 00:03:06,560
Olathe to our data center in Olathe our

00:03:03,210 --> 00:03:09,240
main data center to Garmin Connect which

00:03:06,560 --> 00:03:10,800
Garmin Connect is just an example of one

00:03:09,240 --> 00:03:12,150
of the many apps that we that we house

00:03:10,800 --> 00:03:14,220
add Garmin

00:03:12,150 --> 00:03:15,330
but it's probably arguably coming one of

00:03:14,220 --> 00:03:18,060
the biggest if not one of the most

00:03:15,330 --> 00:03:19,950
important so everything across all of

00:03:18,060 --> 00:03:23,459
our segments is phoning back into this

00:03:19,950 --> 00:03:24,989
into in connect so users can upload they

00:03:23,459 --> 00:03:27,270
can share they can monitor they can

00:03:24,989 --> 00:03:29,220
analyze what they're doing activity wise

00:03:27,270 --> 00:03:32,459
and that can be anything and with health

00:03:29,220 --> 00:03:34,050
and fitness or run a kayak trip or even

00:03:32,459 --> 00:03:37,650
flying if you go on a flight you can

00:03:34,050 --> 00:03:40,110
track that here as well and connect also

00:03:37,650 --> 00:03:42,300
integrates with most and most if not all

00:03:40,110 --> 00:03:43,890
of our devices coming off now so you

00:03:42,300 --> 00:03:47,220
know forerunner for running edge for

00:03:43,890 --> 00:03:49,650
cycling the Phoenix 3 everything that

00:03:47,220 --> 00:03:51,780
that we're building now can come back to

00:03:49,650 --> 00:03:54,300
Garmin and this is kind of an example

00:03:51,780 --> 00:03:56,280
what the dashboard looks so connect has

00:03:54,300 --> 00:03:57,989
come a long way when it first started it

00:03:56,280 --> 00:04:00,769
was a very it was a massive monolithic

00:03:57,989 --> 00:04:02,910
app I think it was one ear file and that

00:04:00,769 --> 00:04:05,370
obviously isn't sustainable

00:04:02,910 --> 00:04:06,780
so we're pushing more to go to a more

00:04:05,370 --> 00:04:09,209
segregated model and it's come a long

00:04:06,780 --> 00:04:11,070
way it still has a long way to go but

00:04:09,209 --> 00:04:13,290
the the back end is Oracle and also

00:04:11,070 --> 00:04:15,540
elastic searches it just brought online

00:04:13,290 --> 00:04:18,989
now and a lot of sequel lists database

00:04:15,540 --> 00:04:21,200
integration is coming as well it's also

00:04:18,989 --> 00:04:23,419
supported by several other IT

00:04:21,200 --> 00:04:25,470
infrastructure as well as applications

00:04:23,419 --> 00:04:28,470
and I'll go into that here in a little

00:04:25,470 --> 00:04:30,180
bit but like many of you who work for

00:04:28,470 --> 00:04:32,880
device companies or any-any companies

00:04:30,180 --> 00:04:34,500
actually sell products the holidays are

00:04:32,880 --> 00:04:36,600
really tough on us because connect

00:04:34,500 --> 00:04:37,710
really really gets beat up pretty pretty

00:04:36,600 --> 00:04:39,690
good and around the holidays so

00:04:37,710 --> 00:04:42,330
everybody's unwrapping their new devices

00:04:39,690 --> 00:04:43,830
and they're open them up Christmas Day

00:04:42,330 --> 00:04:45,030
or whatever it is and they're phoning

00:04:43,830 --> 00:04:47,490
home and that's when connect really

00:04:45,030 --> 00:04:50,550
starts getting stressed so we're looking

00:04:47,490 --> 00:04:52,380
at ways to to address this and a lot of

00:04:50,550 --> 00:04:54,570
it the direction we decided to go with

00:04:52,380 --> 00:04:56,370
splitting this out so splitting the app

00:04:54,570 --> 00:04:59,310
out even more segregating the the

00:04:56,370 --> 00:05:01,800
individual services and micro services

00:04:59,310 --> 00:05:03,030
or one way we're looking but Cloud

00:05:01,800 --> 00:05:05,070
Foundry we really believe is one of the

00:05:03,030 --> 00:05:08,130
keys to the strategy of us breaking up

00:05:05,070 --> 00:05:09,570
the app the first app that I mentioned

00:05:08,130 --> 00:05:11,250
before that's slated to go into Cloud

00:05:09,570 --> 00:05:12,750
Foundry into production is RSS so it's

00:05:11,250 --> 00:05:15,030
our single sign-on app and this is our

00:05:12,750 --> 00:05:17,789
enterprise-wide app we use it across not

00:05:15,030 --> 00:05:19,349
only connect but other apps that you may

00:05:17,789 --> 00:05:21,000
or may not be familiar with like the fly

00:05:19,349 --> 00:05:22,620
Garmin the my Garmin the by-the-bye

00:05:21,000 --> 00:05:25,830
Garmin the soar our main force or phone

00:05:22,620 --> 00:05:27,180
app this app here it will be the first

00:05:25,830 --> 00:05:29,550
and hopefully by the end of the month

00:05:27,180 --> 00:05:31,680
we're hoping we had some setbacks when

00:05:29,550 --> 00:05:33,180
we got Cloud Foundry installed not with

00:05:31,680 --> 00:05:35,460
claw foundry with the app itself it

00:05:33,180 --> 00:05:37,590
wasn't ready for it wasn't handling

00:05:35,460 --> 00:05:39,300
multiple data centers efficiently so we

00:05:37,590 --> 00:05:41,250
had to delay the release of it in Cloud

00:05:39,300 --> 00:05:43,289
Foundry but one of the requirements for

00:05:41,250 --> 00:05:45,030
us integrating integrating SSO in the

00:05:43,289 --> 00:05:47,160
indica vendors we still have a legacy

00:05:45,030 --> 00:05:50,010
piece of SSO still there so we have to

00:05:47,160 --> 00:05:51,510
be able to not only initially support

00:05:50,010 --> 00:05:53,610
SSO and Cloud Foundry we're gonna have

00:05:51,510 --> 00:05:55,950
to also integrate with the legacy piece

00:05:53,610 --> 00:05:58,110
as well but we're both working on that

00:05:55,950 --> 00:06:00,180
but so we get a little head of myself

00:05:58,110 --> 00:06:02,220
here so let's talk about how we got here

00:06:00,180 --> 00:06:04,320
first so murmuring up Jonathan or gear

00:06:02,220 --> 00:06:13,250
he's gonna show you the roadmap how we

00:06:04,320 --> 00:06:16,860
got here hi everybody

00:06:13,250 --> 00:06:17,880
alright so before I get into what we're

00:06:16,860 --> 00:06:21,060
doing I wanted to talk a little bit

00:06:17,880 --> 00:06:22,919
about why we need to change so deploying

00:06:21,060 --> 00:06:24,660
wars the servers it works

00:06:22,919 --> 00:06:26,700
why why why would want to change that

00:06:24,660 --> 00:06:28,950
well I came up with a couple reasons

00:06:26,700 --> 00:06:32,970
number one is is the build-out workload

00:06:28,950 --> 00:06:34,560
for for modern infrastructure it takes a

00:06:32,970 --> 00:06:36,570
long time sometimes to get it to get a

00:06:34,560 --> 00:06:38,070
server up and running after you handle

00:06:36,570 --> 00:06:40,770
it after the bare server is handed over

00:06:38,070 --> 00:06:43,380
for me an infrastructure team you still

00:06:40,770 --> 00:06:44,840
have to apply apply tomcat and things

00:06:43,380 --> 00:06:47,880
like that

00:06:44,840 --> 00:06:49,410
utilization isn't great on on regular

00:06:47,880 --> 00:06:50,760
infrastructure I logged into one of our

00:06:49,410 --> 00:06:54,480
servers the other day and the RAM on the

00:06:50,760 --> 00:06:56,250
server was pegged but it was sitting at

00:06:54,480 --> 00:06:57,480
2% CPU so we're obviously we're not

00:06:56,250 --> 00:06:59,190
using that infrastructure very well we

00:06:57,480 --> 00:07:03,540
need to do a better monitoring and

00:06:59,190 --> 00:07:05,970
security are they take too long in a box

00:07:03,540 --> 00:07:08,430
per box basis you think of your zero day

00:07:05,970 --> 00:07:09,810
vulnerabilities and you end up with

00:07:08,430 --> 00:07:11,790
things like I have a thousand servers

00:07:09,810 --> 00:07:14,070
that I have to get patched before this

00:07:11,790 --> 00:07:16,470
date because there's a vulnerability

00:07:14,070 --> 00:07:17,760
coming on and so hey I'm gonna get my

00:07:16,470 --> 00:07:19,440
infrastructure admins to drop everything

00:07:17,760 --> 00:07:22,620
they're doing and go solve that problem

00:07:19,440 --> 00:07:24,330
even with pooled infrastructure uptime

00:07:22,620 --> 00:07:26,490
goals can be difficult to reach if you

00:07:24,330 --> 00:07:27,780
need to scale up your pool it can be

00:07:26,490 --> 00:07:29,490
challenging to add things to the pool

00:07:27,780 --> 00:07:31,110
and get that done really quickly I guess

00:07:29,490 --> 00:07:34,820
it's it's easy to add to the pool doing

00:07:31,110 --> 00:07:39,750
it quickly is where you have a challenge

00:07:34,820 --> 00:07:42,740
so as far as developers go we end up

00:07:39,750 --> 00:07:45,090
doing things that aren't development and

00:07:42,740 --> 00:07:47,010
that takes away from the time we can be

00:07:45,090 --> 00:07:50,610
developing things like setting up new

00:07:47,010 --> 00:07:52,110
servers for projects doing those things

00:07:50,610 --> 00:07:54,419
instead of instead of responding

00:07:52,110 --> 00:07:55,710
directly to business needs the cost to a

00:07:54,419 --> 00:07:58,260
project where you have to add new

00:07:55,710 --> 00:08:00,720
servers things like that these are

00:07:58,260 --> 00:08:02,940
reasons why we wanted to change to drive

00:08:00,720 --> 00:08:04,560
this home a little closer I want to talk

00:08:02,940 --> 00:08:07,290
a little bit about a sample application

00:08:04,560 --> 00:08:09,110
outage I'm sure no one in the room has

00:08:07,290 --> 00:08:11,430
ever had this happen to them before but

00:08:09,110 --> 00:08:13,290
so this sample application outage you

00:08:11,430 --> 00:08:15,090
have a customer or facing application

00:08:13,290 --> 00:08:16,860
that's running a little slow you've got

00:08:15,090 --> 00:08:18,960
people that can't log in what's the

00:08:16,860 --> 00:08:20,610
solution you need to you need to scale

00:08:18,960 --> 00:08:22,560
up the application and so you end up

00:08:20,610 --> 00:08:26,220
having to add more app servers to your

00:08:22,560 --> 00:08:27,569
pool in this specific instance by the

00:08:26,220 --> 00:08:29,789
time we got to we need

00:08:27,569 --> 00:08:32,039
to add more servers to our pool we'd

00:08:29,789 --> 00:08:33,870
spent about 50 man hours we had a

00:08:32,039 --> 00:08:37,199
12-person troubleshooting team going on

00:08:33,870 --> 00:08:40,579
and our customer experience was impacted

00:08:37,199 --> 00:08:42,630
so those are things we all want to avoid

00:08:40,579 --> 00:08:46,680
another issue that you may or may not

00:08:42,630 --> 00:08:49,350
have we have a large app pool that has

00:08:46,680 --> 00:08:52,079
about 50 servers in it each server has

00:08:49,350 --> 00:08:53,519
about 50 Wars in it now don't judge we

00:08:52,079 --> 00:08:55,649
realize that this isn't the best thing

00:08:53,519 --> 00:08:57,899
and we're trying to get off of it but it

00:08:55,649 --> 00:09:00,120
is what it is today so if one of those

00:08:57,899 --> 00:09:02,220
apps in the in the pool needs to scale

00:09:00,120 --> 00:09:04,170
up guess what build a new server drop

00:09:02,220 --> 00:09:06,329
all those things on to the server that

00:09:04,170 --> 00:09:08,459
takes time you're scaling an entire

00:09:06,329 --> 00:09:10,829
suite rather than scaling in an app as a

00:09:08,459 --> 00:09:13,589
unit and to put it into a sports analogy

00:09:10,829 --> 00:09:15,120
hey I want to I want kicking practice

00:09:13,589 --> 00:09:16,500
okay bring the whole football team down

00:09:15,120 --> 00:09:18,329
to the field for kicking practice that's

00:09:16,500 --> 00:09:20,009
that's not good it's labor and

00:09:18,329 --> 00:09:24,720
infrastructure intensive to scale apps

00:09:20,009 --> 00:09:25,380
in that in that manner okay so how did

00:09:24,720 --> 00:09:28,589
we get here

00:09:25,380 --> 00:09:31,259
in late 2013 and one of our hackathons

00:09:28,589 --> 00:09:33,689
we had a promotional account with

00:09:31,259 --> 00:09:35,100
pivotal web services and in 24 hours

00:09:33,689 --> 00:09:38,430
myself and one of our architects was

00:09:35,100 --> 00:09:40,139
able to deploy an application dude zero

00:09:38,430 --> 00:09:43,380
downtime a B deployments we had it

00:09:40,139 --> 00:09:45,120
connected to a cloud data source and we

00:09:43,380 --> 00:09:46,980
really liked it so after that we

00:09:45,120 --> 00:09:48,930
attended a couple of cloud platform Road

00:09:46,980 --> 00:09:50,880
shows one of which we hosted at Garmin

00:09:48,930 --> 00:09:52,769
we decided it was time to dig even

00:09:50,880 --> 00:09:58,110
deeper and we decided to go for a proof

00:09:52,769 --> 00:10:00,509
of concept during our proof of concept

00:09:58,110 --> 00:10:02,160
we spent some time looking at data

00:10:00,509 --> 00:10:04,649
stores we know that Cloud Foundry

00:10:02,160 --> 00:10:06,630
connects really well to cloud hosted

00:10:04,649 --> 00:10:09,180
data but a Garmin we have some pretty

00:10:06,630 --> 00:10:10,829
huge databases already the connect one

00:10:09,180 --> 00:10:12,000
is one of the ones that was mentioned we

00:10:10,829 --> 00:10:13,709
needed to make sure we could connect to

00:10:12,000 --> 00:10:16,769
those one of them is my sequel we have a

00:10:13,709 --> 00:10:18,630
lot of Oracle also single data source

00:10:16,769 --> 00:10:21,959
applications user provided data sources

00:10:18,630 --> 00:10:24,000
worked really well for us I also ended

00:10:21,959 --> 00:10:25,410
up testing multiple application or

00:10:24,000 --> 00:10:27,959
multiple databases in one application

00:10:25,410 --> 00:10:29,040
not that I recommend it but for some of

00:10:27,959 --> 00:10:31,769
our legacy apps we wanted to make sure

00:10:29,040 --> 00:10:33,420
we could do it so that was easy now non

00:10:31,769 --> 00:10:37,910
JDBC is a little more challenging

00:10:33,420 --> 00:10:40,620
without the JDBC URL it's a little more

00:10:37,910 --> 00:10:41,310
tricky to make a user provided service

00:10:40,620 --> 00:10:42,840
connection

00:10:41,310 --> 00:10:44,370
but with a lot of great help from

00:10:42,840 --> 00:10:49,470
pivotal I was able to get that to work

00:10:44,370 --> 00:10:51,779
also so pivotal had set up weekly calls

00:10:49,470 --> 00:10:54,029
with us we had Andrew Ripken a few other

00:10:51,779 --> 00:10:55,680
guys on the phone weekly and they they

00:10:54,029 --> 00:10:57,060
did a lot of a lot a lot of

00:10:55,680 --> 00:10:58,680
troubleshooting we had a lot of a great

00:10:57,060 --> 00:11:00,690
help there the other thing that was

00:10:58,680 --> 00:11:03,150
going on during this POC is myself and

00:11:00,690 --> 00:11:04,800
our architects or debating about what do

00:11:03,150 --> 00:11:06,480
we put in the cloud foundry and and my

00:11:04,800 --> 00:11:08,460
initial thought was let's get everything

00:11:06,480 --> 00:11:09,990
in there as quickly as we can so that we

00:11:08,460 --> 00:11:12,510
can start to see the benefits of auto

00:11:09,990 --> 00:11:16,380
auto scaling better infrastructure usage

00:11:12,510 --> 00:11:21,420
etc so maybe I'm naive but this was what

00:11:16,380 --> 00:11:23,100
I was hoping for I think really if we

00:11:21,420 --> 00:11:27,720
had gone that way it probably would have

00:11:23,100 --> 00:11:29,550
looked more like this so we decided no

00:11:27,720 --> 00:11:30,750
we don't want to put everything in there

00:11:29,550 --> 00:11:32,730
because really what that would give us

00:11:30,750 --> 00:11:34,560
is a shiny new home for all of the

00:11:32,730 --> 00:11:36,420
problems we have today yes we have

00:11:34,560 --> 00:11:37,529
technical debt the company is only

00:11:36,420 --> 00:11:40,050
twenty years old but that's old enough

00:11:37,529 --> 00:11:41,400
to have technical debt and we have it so

00:11:40,050 --> 00:11:43,830
we decided that cloud foundry was our

00:11:41,400 --> 00:11:46,410
great opportunity to modernize so we're

00:11:43,830 --> 00:11:47,520
shooting to go 12 factor we needed we

00:11:46,410 --> 00:11:50,280
know we need to change some of our

00:11:47,520 --> 00:11:51,630
application development culture we need

00:11:50,280 --> 00:11:53,640
to change some parts of our

00:11:51,630 --> 00:11:55,920
implementations such as how we do Splunk

00:11:53,640 --> 00:11:57,810
I'm gonna let Alex talk about that a

00:11:55,920 --> 00:11:59,490
little more later and of course we know

00:11:57,810 --> 00:12:04,260
we need to do technical we need to

00:11:59,490 --> 00:12:06,150
reduce our technical debt we're really

00:12:04,260 --> 00:12:08,250
looking forward to using the spring

00:12:06,150 --> 00:12:11,220
stack we already use a lot of spring

00:12:08,250 --> 00:12:13,470
mainly the Spring Framework we use MVC

00:12:11,220 --> 00:12:16,610
the framework we're also going to start

00:12:13,470 --> 00:12:19,170
using cloud services I was able to get

00:12:16,610 --> 00:12:21,870
the spring cloud configuration server up

00:12:19,170 --> 00:12:24,300
and running within cloud foundry and so

00:12:21,870 --> 00:12:25,770
we're looking to externalize all of our

00:12:24,300 --> 00:12:28,500
configuration out of our applications

00:12:25,770 --> 00:12:30,720
host it within the spring cloud config

00:12:28,500 --> 00:12:32,730
server one of the things we do today

00:12:30,720 --> 00:12:35,100
each of our servers has a monitoring war

00:12:32,730 --> 00:12:36,810
on it which is great it allows us to

00:12:35,100 --> 00:12:38,820
monitor all of our servers but we all

00:12:36,810 --> 00:12:41,010
know with Cloud Foundry when you deploy

00:12:38,820 --> 00:12:42,570
a droplet it's just the droplet you

00:12:41,010 --> 00:12:44,670
can't put two droplets and call it a

00:12:42,570 --> 00:12:46,830
droplet so our monitoring war cannot be

00:12:44,670 --> 00:12:49,770
we can't deploy with apps anymore

00:12:46,830 --> 00:12:51,030
so spring actuator is going to is going

00:12:49,770 --> 00:12:53,520
to solve that nicely for us the

00:12:51,030 --> 00:12:54,870
out-of-the-box health indicators are

00:12:53,520 --> 00:12:57,150
pretty cool and I'm sure we're going

00:12:54,870 --> 00:12:59,279
end up writing some of our own so look

00:12:57,150 --> 00:13:01,080
for that from us in the future and we're

00:12:59,279 --> 00:13:05,220
also gonna be doing a lot more with cob

00:13:01,080 --> 00:13:07,339
connectors we've also been pretty pretty

00:13:05,220 --> 00:13:09,690
excited about this the Netflix OSS stack

00:13:07,339 --> 00:13:11,130
I'm not sure how many of you are aware

00:13:09,690 --> 00:13:13,650
of all the things that are there but

00:13:11,130 --> 00:13:15,720
it's it's well worth looking at we'll be

00:13:13,650 --> 00:13:17,520
using Eureka for service registration

00:13:15,720 --> 00:13:19,080
and discovery will be using histories

00:13:17,520 --> 00:13:21,930
for fault tolerance and circuit breakers

00:13:19,080 --> 00:13:25,440
will be using fain as a declarative rest

00:13:21,930 --> 00:13:28,050
client which allows us to address rest

00:13:25,440 --> 00:13:30,589
as it as a native Java interface so

00:13:28,050 --> 00:13:32,430
let's revisit why do we want to change

00:13:30,589 --> 00:13:34,050
there's no more build-out for

00:13:32,430 --> 00:13:37,589
applications after Cloud Foundry is up

00:13:34,050 --> 00:13:39,720
and running your infrastructure is much

00:13:37,589 --> 00:13:43,080
better utilized because the the DEA gets

00:13:39,720 --> 00:13:46,200
to decide who who's gonna run what and

00:13:43,080 --> 00:13:47,940
therefore your apps are more they're

00:13:46,200 --> 00:13:50,550
more spread out and you're using your

00:13:47,940 --> 00:13:53,040
infrastructure better zero downtime

00:13:50,550 --> 00:13:55,470
upgrades are gonna solve our labor

00:13:53,040 --> 00:13:58,500
intensive security and monitoring issues

00:13:55,470 --> 00:14:05,040
and the entire platform is designed to

00:13:58,500 --> 00:14:08,580
solve the uptime goal to cover this

00:14:05,040 --> 00:14:10,050
slide really quickly it'll give us the

00:14:08,580 --> 00:14:11,790
ability to do push-button deployments

00:14:10,050 --> 00:14:13,980
we're really looking forward for

00:14:11,790 --> 00:14:15,209
continuous delivery I wanted to talk

00:14:13,980 --> 00:14:18,270
about done a little earlier and I forgot

00:14:15,209 --> 00:14:20,220
to do that so how many developers here

00:14:18,270 --> 00:14:22,170
have said oh my codes checked in to get

00:14:20,220 --> 00:14:24,959
or I've got my pull request or I merged

00:14:22,170 --> 00:14:26,370
my code into develop I'm done well yes

00:14:24,959 --> 00:14:27,990
we're done when that happens but you

00:14:26,370 --> 00:14:30,050
know what nobody else gets to see the

00:14:27,990 --> 00:14:32,580
code and so with continuous deployment

00:14:30,050 --> 00:14:35,880
we're hoping that we can redefine done

00:14:32,580 --> 00:14:36,930
as it's in production and we're hoping

00:14:35,880 --> 00:14:41,850
that that will happen within the

00:14:36,930 --> 00:14:45,810
confines of a single sprint so going

00:14:41,850 --> 00:14:48,839
back to our outage prevention if we tell

00:14:45,810 --> 00:14:50,370
that same story with Cloud Foundry the

00:14:48,839 --> 00:14:52,620
real story is there's an imperceptible

00:14:50,370 --> 00:14:54,150
slowdown to the Apple we as consumers

00:14:52,620 --> 00:14:55,700
using our products we don't even notice

00:14:54,150 --> 00:14:57,990
it but Cloud Foundry notice is it

00:14:55,700 --> 00:15:00,300
because there's some heavier than normal

00:14:57,990 --> 00:15:02,070
load somewhere it scales up and before

00:15:00,300 --> 00:15:05,160
before a client even knows there's an

00:15:02,070 --> 00:15:07,140
issue the developers is notified that

00:15:05,160 --> 00:15:08,160
hey you're not running three instances

00:15:07,140 --> 00:15:08,820
of this app anymore now you're running

00:15:08,160 --> 00:15:10,950
five

00:15:08,820 --> 00:15:12,150
and you can go on Monday and go back

00:15:10,950 --> 00:15:14,820
through the logs and take a look at that

00:15:12,150 --> 00:15:16,260
rather than the long seven one outage

00:15:14,820 --> 00:15:19,470
call that you end up having on a Sunday

00:15:16,260 --> 00:15:21,390
afternoon because the app went down with

00:15:19,470 --> 00:15:22,650
cloud family manage apps you end up with

00:15:21,390 --> 00:15:24,960
each app having the number of instances

00:15:22,650 --> 00:15:26,310
it needs to have rather than having the

00:15:24,960 --> 00:15:27,990
number of instances that your monolith

00:15:26,310 --> 00:15:30,060
has to happen so going back to that

00:15:27,990 --> 00:15:31,980
football analogy when we want to take

00:15:30,060 --> 00:15:33,120
have kicking practice we just stick them

00:15:31,980 --> 00:15:34,320
on their own field and they go practice

00:15:33,120 --> 00:15:37,230
and they don't have to affect the rest

00:15:34,320 --> 00:15:39,270
of the football team the instances will

00:15:37,230 --> 00:15:41,520
grow and shrink is necessary the apps

00:15:39,270 --> 00:15:43,230
are nested in their containers so there

00:15:41,520 --> 00:15:45,870
there's there's no security issue and

00:15:43,230 --> 00:15:49,550
that zero day vulnerability you can you

00:15:45,870 --> 00:15:51,720
can solve that problem by upgrading the

00:15:49,550 --> 00:15:53,400
excuse me by upgrading Cloud Foundry

00:15:51,720 --> 00:15:55,710
while the apps are running and there's

00:15:53,400 --> 00:15:58,440
no downtime auto scaling is of course

00:15:55,710 --> 00:16:00,810
efficient and fast your your stuff as

00:15:58,440 --> 00:16:02,610
your your infrastructure is used much

00:16:00,810 --> 00:16:05,940
more efficiently and pass it off to

00:16:02,610 --> 00:16:07,820
Brandon he's going to tell us about some

00:16:05,940 --> 00:16:11,300
of the some of the bill that we've done

00:16:07,820 --> 00:16:11,300
thank you sir

00:16:13,770 --> 00:16:20,010
okay so I'm Brandon Henery emilynics

00:16:16,480 --> 00:16:22,450
admin with Garmin I did the initial

00:16:20,010 --> 00:16:25,000
implementation and design of Cloud

00:16:22,450 --> 00:16:26,460
Foundry at garmin and i'm gonna give you

00:16:25,000 --> 00:16:28,960
a high-level overview

00:16:26,460 --> 00:16:31,260
okay so Akamai is our global load

00:16:28,960 --> 00:16:33,610
balancer and web content caching system

00:16:31,260 --> 00:16:36,750
this is what we use to load balance

00:16:33,610 --> 00:16:39,040
globally to our different data centers

00:16:36,750 --> 00:16:40,660
the next thing to notice is we have

00:16:39,040 --> 00:16:42,910
nginx which is a lightweight load

00:16:40,660 --> 00:16:45,940
balancer behind an f5 which is a heavier

00:16:42,910 --> 00:16:47,410
more feature-rich load balancer the f5

00:16:45,940 --> 00:16:49,270
load balancer is used to balance

00:16:47,410 --> 00:16:52,270
application calls between our existing

00:16:49,270 --> 00:16:56,260
legacy applications and our shiny new

00:16:52,270 --> 00:16:57,940
Cloud Foundry environment whose pivotal

00:16:56,260 --> 00:17:00,220
to deploy cloud foundry in order to

00:16:57,940 --> 00:17:02,710
greatly reduce our implementation time

00:17:00,220 --> 00:17:05,680
time to production of our first app ease

00:17:02,710 --> 00:17:06,910
overall user and org management as you

00:17:05,680 --> 00:17:10,030
can see we actually have two different

00:17:06,910 --> 00:17:12,190
CF environments in production each each

00:17:10,030 --> 00:17:18,270
purple cloud it represents the David

00:17:12,190 --> 00:17:21,670
data center like many IT organ is is

00:17:18,270 --> 00:17:25,240
struggling to make our our data centers

00:17:21,670 --> 00:17:29,230
more disaster resilient and so this is a

00:17:25,240 --> 00:17:32,830
good way to do it ok so why do we have

00:17:29,230 --> 00:17:34,200
nginx as an intermediary for that you'll

00:17:32,830 --> 00:17:36,730
need to know a little background

00:17:34,200 --> 00:17:38,830
initially the scope of the project was

00:17:36,730 --> 00:17:40,660
to have little or no code rewrites for

00:17:38,830 --> 00:17:42,760
our first application

00:17:40,660 --> 00:17:44,080
many of our brownfield applications are

00:17:42,760 --> 00:17:45,420
interconnected and make calls back and

00:17:44,080 --> 00:17:48,010
forth to each other

00:17:45,420 --> 00:17:50,680
therefore when you have hard coded URLs

00:17:48,010 --> 00:17:53,440
and legacy application code like say SSO

00:17:50,680 --> 00:17:55,090
diagram and comm this doesn't work when

00:17:53,440 --> 00:17:56,950
you take into account cloud foundries

00:17:55,090 --> 00:17:59,980
requirement to route all application

00:17:56,950 --> 00:18:03,790
calls via the wild-card DNS entry star

00:17:59,980 --> 00:18:05,710
dot CF garmin.com for instance we

00:18:03,790 --> 00:18:08,530
decided to use nginx to do the URL and

00:18:05,710 --> 00:18:10,960
host how to rewrite f5 could do this via

00:18:08,530 --> 00:18:12,940
an IR rule but we wanted the to be able

00:18:10,960 --> 00:18:15,610
to store the configuration in source

00:18:12,940 --> 00:18:18,900
control automated deployment of the

00:18:15,610 --> 00:18:21,010
configuration to nginx via get hooks

00:18:18,900 --> 00:18:22,240
since cloud foundry will be a service

00:18:21,010 --> 00:18:23,650
used and shared by many different

00:18:22,240 --> 00:18:27,190
development teams we wanted to make sure

00:18:23,650 --> 00:18:27,690
that I was developer friendly safe his

00:18:27,190 --> 00:18:29,740
paw

00:18:27,690 --> 00:18:33,790
and allow them to make the changes that

00:18:29,740 --> 00:18:35,430
they need okay so here's what the nginx

00:18:33,790 --> 00:18:39,370
rewrite looks like in our architecture

00:18:35,430 --> 00:18:40,830
so you start with see if I can so you

00:18:39,370 --> 00:18:45,160
start with app garmin.com

00:18:40,830 --> 00:18:48,010
it comes down to the f5 layer and stop

00:18:45,160 --> 00:18:50,290
working okay it goes on the f5 layer

00:18:48,010 --> 00:18:52,840
where we basically do some sort of load

00:18:50,290 --> 00:18:55,690
balancing to route to our legacy

00:18:52,840 --> 00:18:57,940
applications and Cloud Foundry there

00:18:55,690 --> 00:19:00,190
canary in the coalmine a B deployments

00:18:57,940 --> 00:19:04,690
weighted load balancing something like

00:19:00,190 --> 00:19:07,290
that from there the requests that go to

00:19:04,690 --> 00:19:11,530
legacy are are just like they are today

00:19:07,290 --> 00:19:13,270
App garmin.com when they're routed to

00:19:11,530 --> 00:19:15,430
our Cloud Foundry environment it hits

00:19:13,270 --> 00:19:19,090
the nginx layer where it does the URL

00:19:15,430 --> 00:19:24,520
rewriting so for instance app dot oh la

00:19:19,090 --> 00:19:26,350
- cloud or app dot kcg cloud and then

00:19:24,520 --> 00:19:30,700
from there those requests are routed to

00:19:26,350 --> 00:19:32,500
Cloud Foundry so let's take a closer

00:19:30,700 --> 00:19:36,610
look at a single leg over Cloud Foundry

00:19:32,500 --> 00:19:38,500
implementation one thing I left out of

00:19:36,610 --> 00:19:42,010
the last slide is a callback to the f5

00:19:38,500 --> 00:19:44,080
after the nginx URL rewrite in order to

00:19:42,010 --> 00:19:48,670
simplify the image so it actually routes

00:19:44,080 --> 00:19:52,090
back to the same f5 and in reality we do

00:19:48,670 --> 00:19:56,080
that so that we can manage our Cloud

00:19:52,090 --> 00:19:58,390
Foundry routers through a f5 pool it

00:19:56,080 --> 00:19:59,950
just it makes it that level of

00:19:58,390 --> 00:20:04,330
abstraction a little bit better for

00:19:59,950 --> 00:20:05,590
scaling out in the future many of you

00:20:04,330 --> 00:20:11,560
may have noticed we only have one

00:20:05,590 --> 00:20:13,030
availability zone per datacenter so at

00:20:11,560 --> 00:20:14,770
the original time of implementation due

00:20:13,030 --> 00:20:15,850
to the quick pace of the project we

00:20:14,770 --> 00:20:18,990
don't have enough hardware for one

00:20:15,850 --> 00:20:21,310
availability zone because Cloud Foundry

00:20:18,990 --> 00:20:23,020
their load balancing logic actually

00:20:21,310 --> 00:20:25,870
requires two availability zones for a

00:20:23,020 --> 00:20:27,550
true h.a we've ordered additional

00:20:25,870 --> 00:20:30,250
hardware and we have it's scheduled for

00:20:27,550 --> 00:20:32,440
next week to actually add a different a

00:20:30,250 --> 00:20:35,590
new availability zone to each data

00:20:32,440 --> 00:20:37,120
center so let's take a look at the

00:20:35,590 --> 00:20:40,390
overview now removing the legacy app

00:20:37,120 --> 00:20:41,320
portion and adding our second

00:20:40,390 --> 00:20:44,700
availability zone

00:20:41,320 --> 00:20:47,500
data center this is our future state

00:20:44,700 --> 00:20:50,320
will remove the legacy infrastructure

00:20:47,500 --> 00:20:53,669
and rely on Cloud Foundry after it has

00:20:50,320 --> 00:20:56,940
been proven of production level up times

00:20:53,669 --> 00:20:59,320
so it looks good to me

00:20:56,940 --> 00:21:01,090
all right just gonna spend a slider to

00:20:59,320 --> 00:21:04,840
showing you that user interface for

00:21:01,090 --> 00:21:06,399
pivotal here we have the CF ops manager

00:21:04,840 --> 00:21:09,039
this is where we control some of our

00:21:06,399 --> 00:21:10,779
build pack installs it also acts as an

00:21:09,039 --> 00:21:13,539
integration point with VMware and Cloud

00:21:10,779 --> 00:21:15,159
Foundry infrastructure anyone who is

00:21:13,539 --> 00:21:17,169
installed Cloud Foundry manually is

00:21:15,159 --> 00:21:21,159
aware that navigating the maze of VM

00:21:17,169 --> 00:21:22,750
files is can be tedious the CF ops

00:21:21,159 --> 00:21:27,009
manager and VMware integration tile

00:21:22,750 --> 00:21:28,600
abstract a lot of that pain away within

00:21:27,009 --> 00:21:30,159
the elapsed elastic run time tile we

00:21:28,600 --> 00:21:31,509
have this handy resource tab that allows

00:21:30,159 --> 00:21:34,419
us to easily scale out or different

00:21:31,509 --> 00:21:36,879
Cloud Foundry components whether it's

00:21:34,419 --> 00:21:38,320
adding compute resources and the DA's or

00:21:36,879 --> 00:21:39,970
scaling out highly available load

00:21:38,320 --> 00:21:43,320
sensitive components like the router

00:21:39,970 --> 00:21:45,429
it's all managed usually from this page

00:21:43,320 --> 00:21:48,429
here's an example of the status page

00:21:45,429 --> 00:21:50,139
sorry that's a little blurry within the

00:21:48,429 --> 00:21:53,320
elastic runtime tile it's pretty handy

00:21:50,139 --> 00:21:55,149
for troubleshooting at a glance but to

00:21:53,320 --> 00:21:59,080
go in depth for production level support

00:21:55,149 --> 00:22:00,340
we need to go deeper for that I'll delve

00:21:59,080 --> 00:22:02,049
into how we are monitoring our caller

00:22:00,340 --> 00:22:05,139
finery environment as well as the

00:22:02,049 --> 00:22:06,100
applications that live within it so if

00:22:05,139 --> 00:22:07,330
you were paying attention one of the

00:22:06,100 --> 00:22:10,080
build packs we had installed in the

00:22:07,330 --> 00:22:12,610
office manager was called optometric s--

00:22:10,080 --> 00:22:15,190
this handy service will aggregate all of

00:22:12,610 --> 00:22:17,519
our data from Cloud Foundry environment

00:22:15,190 --> 00:22:21,039
to a nice little consumable JMX endpoint

00:22:17,519 --> 00:22:23,110
it is data about VM resources and health

00:22:21,039 --> 00:22:26,080
overall application management's

00:22:23,110 --> 00:22:28,419
statistics like number of crashes number

00:22:26,080 --> 00:22:31,840
of app instances expected vs. running

00:22:28,419 --> 00:22:35,850
total number of requests DEA metrics and

00:22:31,840 --> 00:22:35,850
many more valuable pieces of information

00:22:35,879 --> 00:22:40,929
in case you missed it there it is

00:22:38,850 --> 00:22:43,830
ok so what do we do when we have all

00:22:40,929 --> 00:22:46,000
these fancy metrics exposed we monitor

00:22:43,830 --> 00:22:48,580
we SolarWinds to monitor the overall

00:22:46,000 --> 00:22:50,620
health of our Cloud Foundry environments

00:22:48,580 --> 00:22:53,260
SolarWinds for those that don't know is

00:22:50,620 --> 00:22:54,910
a sort of all-in-one IT infrastructure

00:22:53,260 --> 00:22:56,830
monitoring suite

00:22:54,910 --> 00:22:59,200
through SolarWinds we implement a simple

00:22:56,830 --> 00:23:01,390
up-down health check for each component

00:22:59,200 --> 00:23:04,390
in cloud foundry this feeds directly

00:23:01,390 --> 00:23:06,160
into our existing 24 by 7 monitoring

00:23:04,390 --> 00:23:10,570
infrastructure and fits perfectly with

00:23:06,160 --> 00:23:13,420
our current Hong Kong call processes in

00:23:10,570 --> 00:23:15,340
theory the health manager within cloud

00:23:13,420 --> 00:23:16,660
foundry should keep all the components

00:23:15,340 --> 00:23:19,840
running but we know that sometimes

00:23:16,660 --> 00:23:23,860
reality doesn't meet expectations so we

00:23:19,840 --> 00:23:25,840
need to make sure this is not enough

00:23:23,860 --> 00:23:27,940
information to figure out exactly what's

00:23:25,840 --> 00:23:30,280
going wrong if components or

00:23:27,940 --> 00:23:33,430
applications start to fail so it leads

00:23:30,280 --> 00:23:35,950
us to our next tool view realized

00:23:33,430 --> 00:23:37,720
operations manager is VMware's

00:23:35,950 --> 00:23:39,250
integrated operation suite which we're

00:23:37,720 --> 00:23:41,470
basically just used for it's nifty -

00:23:39,250 --> 00:23:43,870
boarding system I'll show you the

00:23:41,470 --> 00:23:45,670
dashboards in a minute these metrics

00:23:43,870 --> 00:23:48,600
once again come from the JMX endpoint

00:23:45,670 --> 00:23:50,680
exposed by the ops metrics tile

00:23:48,600 --> 00:23:52,210
temporarily collects these metrics at

00:23:50,680 --> 00:23:54,010
our heart peak server which aggregates

00:23:52,210 --> 00:23:56,710
the information into a format that via

00:23:54,010 --> 00:23:58,870
lies ops manager likes unlike SolarWinds

00:23:56,710 --> 00:24:02,350
this allows us to delve very deeply into

00:23:58,870 --> 00:24:04,480
our Cloud Foundry infrastructure they're

00:24:02,350 --> 00:24:06,370
combined in two very helpful graphs with

00:24:04,480 --> 00:24:07,750
colored health indicators that can give

00:24:06,370 --> 00:24:10,360
you an at a glance view of Cloud Foundry

00:24:07,750 --> 00:24:13,330
environment and general CF application

00:24:10,360 --> 00:24:14,380
health this will also allow you to deep

00:24:13,330 --> 00:24:17,470
dive in for more thorough

00:24:14,380 --> 00:24:20,050
troubleshooting here's an example of one

00:24:17,470 --> 00:24:21,400
of our dashboards we can do things like

00:24:20,050 --> 00:24:23,440
monitor than I'm a number of

00:24:21,400 --> 00:24:25,570
applications requests how many routes

00:24:23,440 --> 00:24:28,120
the the Cloud Foundry routers have

00:24:25,570 --> 00:24:31,450
created a canary in the coalmine metric

00:24:28,120 --> 00:24:34,420
like registry update lag to test for

00:24:31,450 --> 00:24:36,760
network latency issues overall DEA

00:24:34,420 --> 00:24:40,180
utilization to warn of impending

00:24:36,760 --> 00:24:42,490
resource bottlenecks as things start to

00:24:40,180 --> 00:24:45,010
go wrong the green boxes will turn to

00:24:42,490 --> 00:24:48,130
yellow and then to red as things get

00:24:45,010 --> 00:24:51,250
worse we can monitor memory utilization

00:24:48,130 --> 00:24:53,650
over time of our DEA s which can help us

00:24:51,250 --> 00:24:55,450
plan for the future it can also help us

00:24:53,650 --> 00:24:58,540
see what impact new deployments have on

00:24:55,450 --> 00:25:00,190
our CF environment we can monitor

00:24:58,540 --> 00:25:02,800
application health from the perspective

00:25:00,190 --> 00:25:03,940
of the Cloud Foundry health monitor by

00:25:02,800 --> 00:25:05,920
default it can monitor application

00:25:03,940 --> 00:25:08,860
health to determine their state running

00:25:05,920 --> 00:25:11,590
stopped crashed their version

00:25:08,860 --> 00:25:13,929
number of instances and it has the

00:25:11,590 --> 00:25:15,460
ability to reconcile any problems am I

00:25:13,929 --> 00:25:16,960
find in the app within the applications

00:25:15,460 --> 00:25:20,230
or components running within Cloud

00:25:16,960 --> 00:25:22,120
Foundry this graph in particular would

00:25:20,230 --> 00:25:23,559
be helpful to monitor new application

00:25:22,120 --> 00:25:26,019
code releases to ensure that there

00:25:23,559 --> 00:25:29,230
aren't a bunch of new crashes after you

00:25:26,019 --> 00:25:32,320
deploy code this is only scratching the

00:25:29,230 --> 00:25:34,870
surface of what we can monitor there are

00:25:32,320 --> 00:25:38,559
a ton of metrics exposed I encourage you

00:25:34,870 --> 00:25:46,600
to go out there and and check out this

00:25:38,559 --> 00:25:48,490
blog post by Jamie and dig in so of

00:25:46,600 --> 00:25:53,529
course the goal of the operations team

00:25:48,490 --> 00:25:57,610
is 100% uptime Cloud Foundry VMware f5

00:25:53,529 --> 00:25:59,409
nginx and Akamai all the other

00:25:57,610 --> 00:26:01,960
components within our architecture are

00:25:59,409 --> 00:26:04,950
designed to support this goal they offer

00:26:01,960 --> 00:26:07,480
redundancy resiliency and visibility

00:26:04,950 --> 00:26:10,480
Akamai will load balance a clock across

00:26:07,480 --> 00:26:12,990
data centers f5 load balance across the

00:26:10,480 --> 00:26:15,100
nginx layer and the legacy architecture

00:26:12,990 --> 00:26:17,260
nginx will load balance to the Cloud

00:26:15,100 --> 00:26:18,820
Foundry routers the Cloud Foundry

00:26:17,260 --> 00:26:20,649
routers will then load balance to the

00:26:18,820 --> 00:26:23,440
DA's where the applications live within

00:26:20,649 --> 00:26:25,690
their respective containers that's not

00:26:23,440 --> 00:26:28,029
all we also have to keep in mind that

00:26:25,690 --> 00:26:29,769
this is all but backed by infrastructure

00:26:28,029 --> 00:26:31,480
and monitoring solutions designed to

00:26:29,769 --> 00:26:34,210
alert us and protect us in the unlikely

00:26:31,480 --> 00:26:37,929
event of all the previous redundancies

00:26:34,210 --> 00:26:39,610
fail the heartbeat health checks and

00:26:37,929 --> 00:26:42,100
self-healing provided by the cloud

00:26:39,610 --> 00:26:43,600
foundry health monitor the

00:26:42,100 --> 00:26:45,820
virtualization and infrastructure

00:26:43,600 --> 00:26:47,950
abstraction given to us by the VMware

00:26:45,820 --> 00:26:51,370
including the VM resurrector and V

00:26:47,950 --> 00:26:53,590
motion the in-depth analysis of CF

00:26:51,370 --> 00:26:57,190
components aggregated and visualized by

00:26:53,590 --> 00:26:58,510
Bureau eyes and high Pyrrhic the

00:26:57,190 --> 00:27:00,760
real-time monitoring alerting

00:26:58,510 --> 00:27:02,230
contributed by SolarWinds all of these

00:27:00,760 --> 00:27:03,940
components work together to provide a

00:27:02,230 --> 00:27:06,639
good customer experience and a hundred

00:27:03,940 --> 00:27:09,340
percent uptime which for an Operations

00:27:06,639 --> 00:27:11,440
guy that's why I'm here all right

00:27:09,340 --> 00:27:12,730
Alex will now go into a little greater

00:27:11,440 --> 00:27:16,110
detail about how we monitor the

00:27:12,730 --> 00:27:16,110
applications within cloud foundry

00:27:17,860 --> 00:27:26,140
yeah that's roomy yeah

00:27:23,490 --> 00:27:27,340
so we're doing application monitoring so

00:27:26,140 --> 00:27:29,679
totally separate than when we're doing a

00:27:27,340 --> 00:27:31,600
system on her you know be brief here we

00:27:29,679 --> 00:27:33,220
decided to go with two tools we had

00:27:31,600 --> 00:27:35,409
in-house already which were app dynamics

00:27:33,220 --> 00:27:36,940
for a JVM monitoring as well Splunk for

00:27:35,409 --> 00:27:39,820
just overall log monitoring

00:27:36,940 --> 00:27:41,830
so with app dynamics we we had a little

00:27:39,820 --> 00:27:44,260
bit of a hiccup with how we did this so

00:27:41,830 --> 00:27:46,960
just a quick overview the way we we

00:27:44,260 --> 00:27:48,929
actually load information in app

00:27:46,960 --> 00:27:51,760
dynamics with our current applications

00:27:48,929 --> 00:27:53,440
app dynamics has an application layer

00:27:51,760 --> 00:27:56,470
which you know obviously it overused

00:27:53,440 --> 00:27:58,360
term applications so that's an for us we

00:27:56,470 --> 00:28:00,190
use as an overall environment so connect

00:27:58,360 --> 00:28:02,559
prod connect stage internet fraud

00:28:00,190 --> 00:28:04,210
internet stage underneath that for the

00:28:02,559 --> 00:28:06,760
tier name we would actually define as a

00:28:04,210 --> 00:28:08,830
cluster so SSO being a great example of

00:28:06,760 --> 00:28:11,740
that so SSO would be its own cluster and

00:28:08,830 --> 00:28:13,240
then also as for the legacy piece the

00:28:11,740 --> 00:28:15,220
server's would come in and just populate

00:28:13,240 --> 00:28:17,139
underneath that so we grabbed the user

00:28:15,220 --> 00:28:20,769
provide service for app dynamics for the

00:28:17,139 --> 00:28:22,269
with the original build pack and we load

00:28:20,769 --> 00:28:23,740
it we add our credentials and then we're

00:28:22,269 --> 00:28:25,389
really sure what we're gonna see yet so

00:28:23,740 --> 00:28:28,750
Jonathan I just kind of jumped in feet

00:28:25,389 --> 00:28:30,159
first and we get this was initial so we

00:28:28,750 --> 00:28:32,019
get this because we weren't able to find

00:28:30,159 --> 00:28:33,909
our structure we get for our application

00:28:32,019 --> 00:28:36,610
config servers populated as the

00:28:33,909 --> 00:28:38,289
application the tier name is populated

00:28:36,610 --> 00:28:41,470
as Cloud Foundry and then the real

00:28:38,289 --> 00:28:43,559
confusing part we get that that string

00:28:41,470 --> 00:28:46,360
there is basically the Cloud Foundry

00:28:43,559 --> 00:28:48,010
instance gooood so we were you know this

00:28:46,360 --> 00:28:50,230
this was really close but it wasn't

00:28:48,010 --> 00:28:53,080
exactly what we wanted so we went back

00:28:50,230 --> 00:28:53,980
to app dynamics and in pivotal and just

00:28:53,080 --> 00:28:55,750
said you know guys were really close

00:28:53,980 --> 00:28:57,340
could you help us out and they they

00:28:55,750 --> 00:28:59,710
definitely did they helped us in a lot

00:28:57,340 --> 00:29:02,440
so again original bill pack so we

00:28:59,710 --> 00:29:04,600
grabbed Jaime O'Meara from pivotal and

00:29:02,440 --> 00:29:05,889
the icer map dynamics really stepped up

00:29:04,600 --> 00:29:07,480
to the plate for us and they got us a

00:29:05,889 --> 00:29:09,760
new build pack which provides a new user

00:29:07,480 --> 00:29:12,070
provides service wrap dynamics so we

00:29:09,760 --> 00:29:13,840
download it get it installed and we're

00:29:12,070 --> 00:29:15,970
able to define in that the application

00:29:13,840 --> 00:29:17,559
tier name and how we want the service to

00:29:15,970 --> 00:29:19,630
actually or the instances I should say

00:29:17,559 --> 00:29:20,380
to actually be to be brought in so you

00:29:19,630 --> 00:29:22,120
can see here

00:29:20,380 --> 00:29:24,010
Jonathan our this is a quick test so

00:29:22,120 --> 00:29:25,539
little app this is now Jonathan's

00:29:24,010 --> 00:29:28,210
infamous little app that we used for

00:29:25,539 --> 00:29:30,760
testing in Cloud Foundry caiman is the

00:29:28,210 --> 00:29:31,600
application name test CF came in as the

00:29:30,760 --> 00:29:33,400
tier name

00:29:31,600 --> 00:29:35,500
then the instances came in is zero and

00:29:33,400 --> 00:29:39,640
one now we're able to define that we

00:29:35,500 --> 00:29:41,230
could say SSO - zero SSO - one that that

00:29:39,640 --> 00:29:43,960
identifier at the end is an auto

00:29:41,230 --> 00:29:45,850
incrementing value so and we can edit

00:29:43,960 --> 00:29:47,470
that as well so you can see this is

00:29:45,850 --> 00:29:48,910
definitely a giant step in the right

00:29:47,470 --> 00:29:50,980
direction to kind of form it how we were

00:29:48,910 --> 00:29:53,080
doing a garment already and I kind of

00:29:50,980 --> 00:29:55,390
glossed over Splunk real quick so the

00:29:53,080 --> 00:29:56,770
way with spunk we're obviously not doing

00:29:55,390 --> 00:29:59,200
any system level we're doing all

00:29:56,770 --> 00:30:00,700
application levels so how it worked

00:29:59,200 --> 00:30:02,410
before for us and I'm sure how it works

00:30:00,700 --> 00:30:04,390
for a lot of you who use Splunk the

00:30:02,410 --> 00:30:06,429
applications are spewing off all these

00:30:04,390 --> 00:30:08,080
logs and they're going onto a file

00:30:06,429 --> 00:30:10,000
system in a lot of cases so you may

00:30:08,080 --> 00:30:12,549
write to a flat file and Splunk is then

00:30:10,000 --> 00:30:14,260
pulling all that in well without

00:30:12,549 --> 00:30:16,179
dynamics there's no or excuse me with

00:30:14,260 --> 00:30:18,159
Cloud Foundry there's no file system

00:30:16,179 --> 00:30:20,169
anymore so we have to integrate the

00:30:18,159 --> 00:30:22,120
Splunk user provide service called the

00:30:20,169 --> 00:30:23,679
Splunk drain so everything is coming off

00:30:22,120 --> 00:30:25,630
of these applications and it's just

00:30:23,679 --> 00:30:27,070
going into Splunk drain and at that

00:30:25,630 --> 00:30:28,929
point it becomes supplementary it's how

00:30:27,070 --> 00:30:30,549
you how you've done it before someone

00:30:28,929 --> 00:30:32,679
forwarders however you want to do it so

00:30:30,549 --> 00:30:34,720
and it comes in that way we're still

00:30:32,679 --> 00:30:36,760
getting into that so we're we're still

00:30:34,720 --> 00:30:37,960
this this piece these pieces I'm talking

00:30:36,760 --> 00:30:39,909
about here are still really in their

00:30:37,960 --> 00:30:41,470
infancy and we have a lot of process

00:30:39,909 --> 00:30:43,570
changes we have to make before we get to

00:30:41,470 --> 00:30:45,070
a lot of it so real quick on production

00:30:43,570 --> 00:30:45,429
management which is kind of where I come

00:30:45,070 --> 00:30:48,100
in

00:30:45,429 --> 00:30:49,270
I originally started talking about

00:30:48,100 --> 00:30:50,460
content I started hearing about

00:30:49,270 --> 00:30:53,409
continuous integration continuous

00:30:50,460 --> 00:30:55,539
deployments and this is kind of what I

00:30:53,409 --> 00:30:57,520
hear off the back so I'm I'm hearing

00:30:55,539 --> 00:30:59,200
testing and production is basically so I

00:30:57,520 --> 00:31:01,299
decided to approach it with an open mind

00:30:59,200 --> 00:31:03,460
and I've been learning more and more

00:31:01,299 --> 00:31:04,840
about it so we've realized that when we

00:31:03,460 --> 00:31:06,429
get in infrastructures like this we're

00:31:04,840 --> 00:31:08,730
gonna have to we're gonna have to adopt

00:31:06,429 --> 00:31:10,890
this this thing we're a release base

00:31:08,730 --> 00:31:13,150
currently mostly a release based

00:31:10,890 --> 00:31:14,350
deployment strategy so that process is

00:31:13,150 --> 00:31:15,700
going to have to become a lot more

00:31:14,350 --> 00:31:18,100
flexible a lot more continuous

00:31:15,700 --> 00:31:19,150
integration so the process itself is

00:31:18,100 --> 00:31:21,549
going to have mostly going to have to

00:31:19,150 --> 00:31:24,669
change before we can really adapt the we

00:31:21,549 --> 00:31:26,110
can adapt the the tool bamboo is kind of

00:31:24,669 --> 00:31:28,960
what we're looking at right now so but

00:31:26,110 --> 00:31:30,909
again all very preliminary so in summary

00:31:28,960 --> 00:31:32,470
I hope you guys got something out of

00:31:30,909 --> 00:31:34,450
this we're still a lot of it's in

00:31:32,470 --> 00:31:36,880
progress we're but we've we've come a

00:31:34,450 --> 00:31:38,470
long way and a lot with pivotal help and

00:31:36,880 --> 00:31:40,840
a lot with eff dynamics helped a lot of

00:31:38,470 --> 00:31:43,120
our vendors and we sincerely thank them

00:31:40,840 --> 00:31:44,590
but these are our seven the seven topics

00:31:43,120 --> 00:31:45,280
that Jonathan brought up this was the

00:31:44,590 --> 00:31:47,110
roadmap that

00:31:45,280 --> 00:31:49,150
brought us to this to this point so this

00:31:47,110 --> 00:31:50,650
is what we use to determine that Cloud

00:31:49,150 --> 00:31:52,300
Foundry really feel like is the way we

00:31:50,650 --> 00:31:54,880
that is really going to help us with

00:31:52,300 --> 00:31:56,260
getting our infrastructure going and

00:31:54,880 --> 00:31:58,270
getting getting to the next step

00:31:56,260 --> 00:32:00,730
so next the next steps for us we

00:31:58,270 --> 00:32:03,280
obviously get the SSO app integrated

00:32:00,730 --> 00:32:05,020
hopefully the end of this month maybe in

00:32:03,280 --> 00:32:06,700
a June we're really hoping that's that's

00:32:05,020 --> 00:32:07,990
soon we're gonna get the multi-site

00:32:06,700 --> 00:32:10,570
pieces with the multiple availability

00:32:07,990 --> 00:32:12,120
zones the Brandon talked about and we're

00:32:10,570 --> 00:32:17,410
gonna eliminate that that legacy

00:32:12,120 --> 00:32:19,450
interaction with SSO so I hope this was

00:32:17,410 --> 00:32:20,770
helpful we don't have a whole lot of

00:32:19,450 --> 00:32:22,360
information right now as much

00:32:20,770 --> 00:32:23,710
information as we like right now but you

00:32:22,360 --> 00:32:24,790
know who knows maybe next year we'll

00:32:23,710 --> 00:32:29,320
come back and give you guys an update

00:32:24,790 --> 00:32:29,740
and if you guys will have us so that's

00:32:29,320 --> 00:32:32,050
it

00:32:29,740 --> 00:32:33,220
and we'll be around we'll be around the

00:32:32,050 --> 00:32:36,450
whole weekend so they're gonna stuff

00:32:33,220 --> 00:32:36,450

YouTube URL: https://www.youtube.com/watch?v=7vd4_IVr2sg


