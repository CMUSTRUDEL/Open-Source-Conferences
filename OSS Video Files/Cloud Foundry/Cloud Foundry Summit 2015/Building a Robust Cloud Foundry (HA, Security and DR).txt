Title: Building a Robust Cloud Foundry (HA, Security and DR)
Publication date: 2015-05-13
Playlist: Cloud Foundry Summit 2015
Description: 
	Building a Robust Cloud Foundry (HA, Security and DR) - 02 Haydon Ryan, Duncan Winn 720p
Captions: 
	00:00:00,030 --> 00:00:05,460
so we're going to go ahead and get

00:00:01,890 --> 00:00:07,410
started so welcome to building a robust

00:00:05,460 --> 00:00:09,389
cloud foundry my name is Duncan Wynn I

00:00:07,410 --> 00:00:11,969
work for pivotal and I work for a cloud

00:00:09,389 --> 00:00:15,750
foundry services team my name is Hayden

00:00:11,969 --> 00:00:18,180
Ryan and I also work on the same team as

00:00:15,750 --> 00:00:19,740
Duncan essentially what our team does is

00:00:18,180 --> 00:00:22,590
we spend about half of our time with the

00:00:19,740 --> 00:00:24,269
engineers working with them and then the

00:00:22,590 --> 00:00:27,240
other half of the time working with

00:00:24,269 --> 00:00:29,369
customers taking them say from a POC and

00:00:27,240 --> 00:00:31,830
building them straight through to MVP 1

00:00:29,369 --> 00:00:33,480
now I must apologize if I do cough

00:00:31,830 --> 00:00:36,329
during this presentation I'm still

00:00:33,480 --> 00:00:40,620
really quite sick with the flu so just

00:00:36,329 --> 00:00:42,540
bear with me orders yeah so in this talk

00:00:40,620 --> 00:00:44,250
we're going to focus on our experience

00:00:42,540 --> 00:00:46,559
making cloud foundry robust so really

00:00:44,250 --> 00:00:48,660
hardening cloud foundry and that's

00:00:46,559 --> 00:00:50,250
looking at taking the four levels of H a

00:00:48,660 --> 00:00:52,860
further forward to make cloud foundry

00:00:50,250 --> 00:00:55,289
highly available looking at how you

00:00:52,860 --> 00:00:57,030
recover from a duct disaster situation

00:00:55,289 --> 00:00:59,190
so if the worst happens how do you bring

00:00:57,030 --> 00:01:02,100
back a cloud foundry and bot and Bush

00:00:59,190 --> 00:01:04,080
excuse me and also security so how do

00:01:02,100 --> 00:01:07,890
you lock cloud foundry down to make it

00:01:04,080 --> 00:01:09,360
more secure for your end-users so we're

00:01:07,890 --> 00:01:12,990
going to jump straight into high

00:01:09,360 --> 00:01:14,939
availability AJ so in most developers

00:01:12,990 --> 00:01:16,619
think about high availability they're

00:01:14,939 --> 00:01:18,960
really thinking about that apps and

00:01:16,619 --> 00:01:20,840
their services how you keep those

00:01:18,960 --> 00:01:23,430
services running in a performance

00:01:20,840 --> 00:01:25,740
reliable recoverable way so they're a

00:01:23,430 --> 00:01:27,600
joy to use for the end user they don't

00:01:25,740 --> 00:01:29,189
go down they just work and if something

00:01:27,600 --> 00:01:31,619
does go wrong there's timely error

00:01:29,189 --> 00:01:33,960
detection but when you look at cloud

00:01:31,619 --> 00:01:36,329
foundry cloud foundry does this for you

00:01:33,960 --> 00:01:38,280
especially around your applications it's

00:01:36,329 --> 00:01:40,520
got four levels of H a built in already

00:01:38,280 --> 00:01:43,680
so if you start with the application

00:01:40,520 --> 00:01:46,439
instances between the health manager and

00:01:43,680 --> 00:01:48,329
the Cloud Controller Cloud Foundry we'll

00:01:46,439 --> 00:01:49,770
look at the actual state versus the

00:01:48,329 --> 00:01:51,720
desired state and if an application

00:01:49,770 --> 00:01:54,960
instance goes away or bring it back for

00:01:51,720 --> 00:01:56,909
you if a platform process dies the

00:01:54,960 --> 00:01:59,340
platform processes are being monitored

00:01:56,909 --> 00:02:02,310
by Monnett and monitor will restart or

00:01:59,340 --> 00:02:04,409
try and restart that process for you if

00:02:02,310 --> 00:02:06,990
something more catastrophic goes wrong

00:02:04,409 --> 00:02:08,489
if a VM goes away then one of the many

00:02:06,990 --> 00:02:09,989
actions that Bosch can take is to

00:02:08,489 --> 00:02:12,810
actually resurrect that VM and bring

00:02:09,989 --> 00:02:13,650
that VM back for you and finally there's

00:02:12,810 --> 00:02:15,659
this concept

00:02:13,650 --> 00:02:17,189
availability zones where you can stripe

00:02:15,659 --> 00:02:19,860
your DEA is across different

00:02:17,189 --> 00:02:22,530
availability zones so if you lose an AZ

00:02:19,860 --> 00:02:24,900
or if you lose a disk store then you

00:02:22,530 --> 00:02:27,629
still have your applications running so

00:02:24,900 --> 00:02:29,010
what's left for us to do well really

00:02:27,629 --> 00:02:30,870
we're focusing on this concept of

00:02:29,010 --> 00:02:32,640
availability zones and extending it to

00:02:30,870 --> 00:02:34,769
beyond just DEA s

00:02:32,640 --> 00:02:36,870
to the actual Cloud Foundry components

00:02:34,769 --> 00:02:38,640
themselves so that we can keep Cloud

00:02:36,870 --> 00:02:41,819
Foundry running in a performant highly

00:02:38,640 --> 00:02:43,560
available way so if the companies who

00:02:41,819 --> 00:02:45,810
really need this and this means that

00:02:43,560 --> 00:02:47,370
their needs are beyond just a DR

00:02:45,810 --> 00:02:48,859
scenario they need to recover quicker

00:02:47,370 --> 00:02:51,299
than they can from a disaster scenario

00:02:48,859 --> 00:02:53,879
we see two patterns that have come up

00:02:51,299 --> 00:02:56,700
again and again the one on your left

00:02:53,879 --> 00:02:58,829
here is two Cloud Foundry deployments in

00:02:56,700 --> 00:03:00,420
two different data centers this could be

00:02:58,829 --> 00:03:03,000
one Cloud Foundry and Amazon and one on

00:03:00,420 --> 00:03:05,189
V Center it could be active passive or

00:03:03,000 --> 00:03:06,389
active us active active and you can then

00:03:05,189 --> 00:03:08,790
load balance between these two

00:03:06,389 --> 00:03:10,739
environments the one on the right is to

00:03:08,790 --> 00:03:12,569
take a single Cloud Foundry deployment

00:03:10,739 --> 00:03:14,670
and then split it across two different

00:03:12,569 --> 00:03:16,769
AZ's so we're going to look at the

00:03:14,670 --> 00:03:17,940
trade-offs between the two here we're

00:03:16,769 --> 00:03:19,410
going to start with the one on the left

00:03:17,940 --> 00:03:21,569
this does mean that you have to take

00:03:19,410 --> 00:03:23,669
your application and deploy it into two

00:03:21,569 --> 00:03:24,989
different locations so it's simple to

00:03:23,669 --> 00:03:27,449
deploy you're just deploying Cloud

00:03:24,989 --> 00:03:29,400
Foundry twice but the operation of that

00:03:27,449 --> 00:03:30,900
is more complex and you can deal with

00:03:29,400 --> 00:03:34,889
some of that complexity through a CI

00:03:30,900 --> 00:03:37,409
pipeline so how does it work in terms of

00:03:34,889 --> 00:03:39,180
traffic flow the end user they're

00:03:37,409 --> 00:03:40,709
agnostic to the underlying environment

00:03:39,180 --> 00:03:43,470
they don't care which data center they

00:03:40,709 --> 00:03:45,989
hit they just target my app at my Cloud

00:03:43,470 --> 00:03:48,540
Foundry comm and they then come into a

00:03:45,989 --> 00:03:50,040
GTM a global traffic manager typically

00:03:48,540 --> 00:03:53,099
for most companies we worked with us

00:03:50,040 --> 00:03:55,560
like an f5 load balancer that consults a

00:03:53,099 --> 00:03:58,440
DNS resolution service so through smarts

00:03:55,560 --> 00:04:00,840
like geo based IP routing and looking at

00:03:58,440 --> 00:04:02,970
various different load it works at which

00:04:00,840 --> 00:04:05,729
data center and which cloud found you to

00:04:02,970 --> 00:04:08,220
route the traffic onto so it comes in to

00:04:05,729 --> 00:04:10,440
a specific data center and hits an l TM

00:04:08,220 --> 00:04:11,819
a local traffic manager again for most

00:04:10,440 --> 00:04:13,699
companies that's something like an f5

00:04:11,819 --> 00:04:17,190
load balancer or an appliance that

00:04:13,699 --> 00:04:18,870
should have a VIP virtual IP and so if

00:04:17,190 --> 00:04:21,329
the appliance fails there's another

00:04:18,870 --> 00:04:23,610
appliance on standby to grab that VIP

00:04:21,329 --> 00:04:26,639
and then takeover so it's a very fast

00:04:23,610 --> 00:04:27,510
failover now in this scenario we had a

00:04:26,639 --> 00:04:29,790
problem and

00:04:27,510 --> 00:04:31,800
the appliance the physical appliance sat

00:04:29,790 --> 00:04:33,780
on the corporate network and it dealt

00:04:31,800 --> 00:04:35,430
with traffic from a number of other

00:04:33,780 --> 00:04:37,380
departments and they didn't want traffic

00:04:35,430 --> 00:04:39,630
to be decrypted or you know SSL

00:04:37,380 --> 00:04:41,820
terminated at that layer and then pass

00:04:39,630 --> 00:04:44,010
unencrypted traffic to this NSX boundary

00:04:41,820 --> 00:04:45,570
they were using this NSX boundary for

00:04:44,010 --> 00:04:46,770
their software-defined networking and

00:04:45,570 --> 00:04:49,200
they were using subnets behind the

00:04:46,770 --> 00:04:51,450
scenes so what they did is they recruit

00:04:49,200 --> 00:04:53,700
to the traffic passed it behind the NSX

00:04:51,450 --> 00:04:56,100
effectively firewall and then they

00:04:53,700 --> 00:04:57,750
decrypted it an H a proxy layer so it

00:04:56,100 --> 00:05:00,090
meant we had three layers of load

00:04:57,750 --> 00:05:02,010
balancers which isn't great but doesn't

00:05:00,090 --> 00:05:03,660
really matter too much and you can

00:05:02,010 --> 00:05:05,730
easily take out the third load balancer

00:05:03,660 --> 00:05:09,120
by putting a physical appliance in that

00:05:05,730 --> 00:05:11,070
NSX layer and one other point to notice

00:05:09,120 --> 00:05:13,950
that we had the search for both data

00:05:11,070 --> 00:05:15,840
centers at the LTM layer and this is

00:05:13,950 --> 00:05:17,430
because if something went wrong if you

00:05:15,840 --> 00:05:19,380
couldn't hit that Cloud Foundry instance

00:05:17,430 --> 00:05:23,400
then you could still route traffic over

00:05:19,380 --> 00:05:25,830
to the other data center a couple of

00:05:23,400 --> 00:05:27,990
other considerations we always advocate

00:05:25,830 --> 00:05:29,700
to have two different domains one for

00:05:27,990 --> 00:05:31,680
system and one for applications because

00:05:29,700 --> 00:05:34,200
you don't want your developers to start

00:05:31,680 --> 00:05:35,670
registering apps like UA a and cloud

00:05:34,200 --> 00:05:39,180
controller you want to keep those

00:05:35,670 --> 00:05:41,460
separation of concerns because we had to

00:05:39,180 --> 00:05:43,050
Cloud Foundry installations we did want

00:05:41,460 --> 00:05:45,240
to give the developers the ability to

00:05:43,050 --> 00:05:47,640
target specific Cloud Foundry Foundation

00:05:45,240 --> 00:05:49,590
and so we ended up with four domains two

00:05:47,640 --> 00:05:52,800
system and two application one for each

00:05:49,590 --> 00:05:54,330
data center but as I mentioned the NGS

00:05:52,800 --> 00:05:56,100
are they need to be agnostic so you end

00:05:54,330 --> 00:05:59,910
up with this generic my app and my Cloud

00:05:56,100 --> 00:06:01,650
Foundry com the last key consideration

00:05:59,910 --> 00:06:03,780
with this is services and this place a

00:06:01,650 --> 00:06:05,460
lot of people how do you keep your day

00:06:03,780 --> 00:06:07,470
two concurrent across two data centers

00:06:05,460 --> 00:06:09,090
and ideally you need something like a

00:06:07,470 --> 00:06:11,180
stretch layer to network with minimal

00:06:09,090 --> 00:06:14,460
latency someplace sub five milliseconds

00:06:11,180 --> 00:06:16,260
the last customer we worked with we

00:06:14,460 --> 00:06:18,090
looked and analyze their data usage and

00:06:16,260 --> 00:06:19,920
they were using an orca database but

00:06:18,090 --> 00:06:21,180
they had an application with a

00:06:19,920 --> 00:06:22,560
long-running session and they just

00:06:21,180 --> 00:06:25,290
needed to cache data for that session

00:06:22,560 --> 00:06:27,750
it's like a 45-minute session for a call

00:06:25,290 --> 00:06:29,880
center out so we looked at using Redis

00:06:27,750 --> 00:06:32,850
as a cache and it was really appropriate

00:06:29,880 --> 00:06:34,410
but this solution with one app targeting

00:06:32,850 --> 00:06:35,880
to Redis clusters to keep the data

00:06:34,410 --> 00:06:38,280
concurrent it's really ugly for the

00:06:35,880 --> 00:06:40,290
developer so next we explored

00:06:38,280 --> 00:06:41,340
instead of putting the onus on the

00:06:40,290 --> 00:06:42,889
developer to keep

00:06:41,340 --> 00:06:45,660
day two concurrent in two data centers

00:06:42,889 --> 00:06:47,340
want to have a service and have the

00:06:45,660 --> 00:06:49,680
service manage that concurrency and

00:06:47,340 --> 00:06:51,180
write to two data sources it's better

00:06:49,680 --> 00:06:52,650
because the developer doesn't need to

00:06:51,180 --> 00:06:54,330
deal with that complexity but you still

00:06:52,650 --> 00:06:57,060
have the latency going across two data

00:06:54,330 --> 00:06:58,380
centers and it's still not great so my

00:06:57,060 --> 00:06:59,880
preferred solution is to use some

00:06:58,380 --> 00:07:01,830
technology built for this

00:06:59,880 --> 00:07:03,600
something like gem fire or Cassandra

00:07:01,830 --> 00:07:06,419
which can propagate data synchronously

00:07:03,600 --> 00:07:09,030
or asynchronously synchronously over a

00:07:06,419 --> 00:07:10,680
wine and they have collision detection

00:07:09,030 --> 00:07:12,000
algorithms and all of that good stuff so

00:07:10,680 --> 00:07:14,460
they're really designed to keep data

00:07:12,000 --> 00:07:18,240
concurrent and consistent between two

00:07:14,460 --> 00:07:19,800
data centers thank you

00:07:18,240 --> 00:07:21,960
so the second deployment that we're

00:07:19,800 --> 00:07:24,210
going to look at is a single foundation

00:07:21,960 --> 00:07:27,300
of Cloud Foundry split across to

00:07:24,210 --> 00:07:29,940
availability zones now as Duncan has

00:07:27,300 --> 00:07:32,789
already mentioned quite a lot of the

00:07:29,940 --> 00:07:34,979
time what we end up deploying really

00:07:32,789 --> 00:07:37,020
depends on the customer so companies

00:07:34,979 --> 00:07:39,870
have very unique environments they have

00:07:37,020 --> 00:07:41,490
very unique requirements and one of the

00:07:39,870 --> 00:07:43,410
fantastic things about Cloud Foundry is

00:07:41,490 --> 00:07:49,410
it's very flexible in a way that you can

00:07:43,410 --> 00:07:52,229
deploy it so in my case study we have a

00:07:49,410 --> 00:07:54,889
few of the examples of unique

00:07:52,229 --> 00:07:57,270
requirements that companies may have

00:07:54,889 --> 00:07:59,610
with the particular customer I have in

00:07:57,270 --> 00:08:02,490
mind we had multiple multiple

00:07:59,610 --> 00:08:05,669
deployments of Cloud Foundry in a single

00:08:02,490 --> 00:08:08,580
VPC we had very restricted IP ranges so

00:08:05,669 --> 00:08:12,080
we had to override all the subnets or

00:08:08,580 --> 00:08:15,000
the IPS the ciders etc and we had some

00:08:12,080 --> 00:08:18,450
routing requirements as well so we had

00:08:15,000 --> 00:08:20,280
to use their internal corporate DNS we

00:08:18,450 --> 00:08:23,460
weren't allowed to use route 53 so this

00:08:20,280 --> 00:08:26,910
was on an Amazon and we also weren't

00:08:23,460 --> 00:08:29,460
allowed to use elastic load balancers so

00:08:26,910 --> 00:08:31,440
what this ended up meaning is that the

00:08:29,460 --> 00:08:32,760
way that we deployed Cloud Foundry was a

00:08:31,440 --> 00:08:34,380
little bit different to the way that we

00:08:32,760 --> 00:08:34,830
would normally recommend deploying Cloud

00:08:34,380 --> 00:08:40,440
Foundry

00:08:34,830 --> 00:08:45,990
in an AWS PPC so this is what we ended

00:08:40,440 --> 00:08:47,910
up deploying their arm their DNS was a

00:08:45,990 --> 00:08:49,890
bind DNS so it was actually in an

00:08:47,910 --> 00:08:53,310
external data center so it was not

00:08:49,890 --> 00:08:54,810
actually in Amazon itself we use Direct

00:08:53,310 --> 00:08:56,939
Connect to get into the veep

00:08:54,810 --> 00:08:59,610
see where the customer had a customer

00:08:56,939 --> 00:09:01,560
managed Bastion box they also had a

00:08:59,610 --> 00:09:04,980
customer managed NAT box so that kind of

00:09:01,560 --> 00:09:08,209
took that out from us managing things

00:09:04,980 --> 00:09:12,389
like security etc around that and then

00:09:08,209 --> 00:09:12,720
we have our - oops where's it gone here

00:09:12,389 --> 00:09:14,819
we go

00:09:12,720 --> 00:09:17,550
our to availability zones that we split

00:09:14,819 --> 00:09:20,069
Cloud Foundry over now I'm not going to

00:09:17,550 --> 00:09:21,779
go into too much of the detail with all

00:09:20,069 --> 00:09:23,699
the jobs in this particular slide but

00:09:21,779 --> 00:09:26,519
hopefully if you can't see the slide

00:09:23,699 --> 00:09:32,040
that well you can download the slides at

00:09:26,519 --> 00:09:34,529
the end so this is how like let's

00:09:32,040 --> 00:09:36,809
abstract it up a level this is what we

00:09:34,529 --> 00:09:40,079
ended up doing so using SSL termination

00:09:36,809 --> 00:09:42,600
at the H a proxy level using bind DNS

00:09:40,079 --> 00:09:45,749
round-robin now that was imposed on us

00:09:42,600 --> 00:09:48,480
from the customer this meant that if a

00:09:45,749 --> 00:09:50,550
an availability zone did go down they'd

00:09:48,480 --> 00:09:53,850
have to be a manual step to actually do

00:09:50,550 --> 00:09:55,589
register the IP for the H a proxy or the

00:09:53,850 --> 00:09:58,769
H a proxies for that availability zone

00:09:55,589 --> 00:10:01,379
so that did add kind of almost a single

00:09:58,769 --> 00:10:03,540
point of failure but um that was

00:10:01,379 --> 00:10:06,990
mitigated by a failure matrix which we

00:10:03,540 --> 00:10:08,819
provided to the customer now the reason

00:10:06,990 --> 00:10:11,639
that we have to H a proxies in each

00:10:08,819 --> 00:10:13,889
availability zone and to go routers

00:10:11,639 --> 00:10:16,889
backing them was so that if an

00:10:13,889 --> 00:10:18,600
availability zone did go down the

00:10:16,889 --> 00:10:20,519
instances that we already had deployed

00:10:18,600 --> 00:10:23,370
would be performed enough to handle the

00:10:20,519 --> 00:10:28,920
increased traffic load similarly we also

00:10:23,370 --> 00:10:31,800
scaled the da's so let's start looking a

00:10:28,920 --> 00:10:35,399
little bit further into this particular

00:10:31,800 --> 00:10:38,009
customers deployment of Cloud Foundry so

00:10:35,399 --> 00:10:39,360
what we started to look at was who does

00:10:38,009 --> 00:10:40,980
Cloud Foundry need to be highly

00:10:39,360 --> 00:10:42,000
available for there's really three

00:10:40,980 --> 00:10:44,189
classes of users

00:10:42,000 --> 00:10:46,559
so there's your end users that are

00:10:44,189 --> 00:10:48,839
consuming your applications there are

00:10:46,559 --> 00:10:50,790
your developers who are developing

00:10:48,839 --> 00:10:53,429
applications reading logs and pushing

00:10:50,790 --> 00:10:56,040
them and there are your operators as

00:10:53,429 --> 00:10:57,379
well so your operators are operating a

00:10:56,040 --> 00:10:59,189
Cloud Foundry environment and

00:10:57,379 --> 00:11:02,759
maintaining it keeping it highly

00:10:59,189 --> 00:11:04,620
available providing upgrades etc so with

00:11:02,759 --> 00:11:06,870
this particular customer their main

00:11:04,620 --> 00:11:07,570
priority that required 100% uptime was

00:11:06,870 --> 00:11:11,050
their end use

00:11:07,570 --> 00:11:12,940
so straightaway that makes all of the

00:11:11,050 --> 00:11:15,310
Cloud Foundry components that are

00:11:12,940 --> 00:11:17,260
related to getting applications running

00:11:15,310 --> 00:11:20,290
or keeping them running and providing

00:11:17,260 --> 00:11:21,610
data through extremely critical so we

00:11:20,290 --> 00:11:23,260
needed to make sure that all of these

00:11:21,610 --> 00:11:25,990
were deployed in a highly available

00:11:23,260 --> 00:11:29,580
manner with at least one instance in

00:11:25,990 --> 00:11:33,880
each availability zone some of the other

00:11:29,580 --> 00:11:35,820
jobs we defined as not so critical I

00:11:33,880 --> 00:11:37,990
mean still it very important obviously

00:11:35,820 --> 00:11:40,780
but if there was a little bit of

00:11:37,990 --> 00:11:42,670
downtime with these jobs the effects

00:11:40,780 --> 00:11:46,150
would not necessarily take down Cloud

00:11:42,670 --> 00:11:48,790
Foundry or the other users apps so these

00:11:46,150 --> 00:11:50,260
were things like logging the health

00:11:48,790 --> 00:11:52,150
monitor if the health monitor goes away

00:11:50,260 --> 00:11:53,890
it just means it's not monitoring your

00:11:52,150 --> 00:11:55,570
apps so if there's a short period of

00:11:53,890 --> 00:11:58,450
time where it's not monitoring that

00:11:55,570 --> 00:12:01,840
might be okay depending how many levels

00:11:58,450 --> 00:12:04,840
of h.a you've still got running as well

00:12:01,840 --> 00:12:09,220
as things like Bosch so again

00:12:04,840 --> 00:12:10,450
Bosch if Bosch goes away that means you

00:12:09,220 --> 00:12:12,790
don't have a resurrector you can't

00:12:10,450 --> 00:12:15,250
resurrect VMs it also means that you

00:12:12,790 --> 00:12:18,760
can't administer the Cloud Foundry

00:12:15,250 --> 00:12:20,770
environment but you know is that

00:12:18,760 --> 00:12:23,140
critical to the operation of Cloud

00:12:20,770 --> 00:12:26,280
Foundry it really depends on your use

00:12:23,140 --> 00:12:29,080
cases so in this case it just kind of

00:12:26,280 --> 00:12:30,820
removes one to two levels of the high

00:12:29,080 --> 00:12:33,070
availability that Duncan spoke about

00:12:30,820 --> 00:12:36,310
earlier and there were some things that

00:12:33,070 --> 00:12:39,430
we're not so critical so the clock

00:12:36,310 --> 00:12:40,500
global is essentially a cleanup job so

00:12:39,430 --> 00:12:43,330
it tells the Cloud Controller

00:12:40,500 --> 00:12:46,420
periodically to clean itself up this was

00:12:43,330 --> 00:12:47,920
not critical so we decided that that

00:12:46,420 --> 00:12:49,390
could only have one instance in one

00:12:47,920 --> 00:12:52,030
availability zone we didn't need to

00:12:49,390 --> 00:12:54,360
replicate it similarly having a jump box

00:12:52,030 --> 00:12:57,760
we did have a bastion box as well it was

00:12:54,360 --> 00:12:59,770
set by the customer so we decided that

00:12:57,760 --> 00:13:03,550
spinning up a jump box wouldn't actually

00:12:59,770 --> 00:13:05,710
take that long in a downtime situation

00:13:03,550 --> 00:13:10,650
it would just increase the time to

00:13:05,710 --> 00:13:13,900
recovery so we ended up deploying a

00:13:10,650 --> 00:13:16,060
minimum of one instance in each

00:13:13,900 --> 00:13:19,890
availability zone for all of these

00:13:16,060 --> 00:13:19,890
particular jobs for the customer

00:13:20,540 --> 00:13:28,110
so choosing a deployment topology really

00:13:25,559 --> 00:13:31,319
comes down to a lot of different factors

00:13:28,110 --> 00:13:33,929
so there's no one-size-fits-all you need

00:13:31,319 --> 00:13:36,959
to be very cognizant of the trade-offs

00:13:33,929 --> 00:13:39,269
between each of these apologies for a

00:13:36,959 --> 00:13:40,769
lot of customers and companies a single

00:13:39,269 --> 00:13:42,509
deployment in a single data center

00:13:40,769 --> 00:13:44,399
that's highly available within that data

00:13:42,509 --> 00:13:48,540
center and a really good disaster

00:13:44,399 --> 00:13:51,660
recovery procedure is enough to push

00:13:48,540 --> 00:13:54,209
your company forward other companies

00:13:51,660 --> 00:13:56,850
will require levels of pay che so you

00:13:54,209 --> 00:14:00,660
could either use a dual deployment so

00:13:56,850 --> 00:14:02,249
two single deployments across different

00:14:00,660 --> 00:14:05,369
Isis you could have them in different

00:14:02,249 --> 00:14:07,049
regions the issue with that is it's

00:14:05,369 --> 00:14:10,559
quite easy to deploy but it's quite hard

00:14:07,049 --> 00:14:12,959
to administer and develop for it's also

00:14:10,559 --> 00:14:15,720
quite hard to push apps to both of them

00:14:12,959 --> 00:14:19,429
but that can be mitigated by using a CI

00:14:15,720 --> 00:14:21,689
CD pipeline and the final deployment is

00:14:19,429 --> 00:14:23,579
quite complex because you have to be

00:14:21,689 --> 00:14:25,860
very cognizant of all the individual

00:14:23,579 --> 00:14:27,119
components of Cloud Foundry but once

00:14:25,860 --> 00:14:28,739
it's deployed it's quite straightforward

00:14:27,119 --> 00:14:34,499
it's just a single deployment so it's

00:14:28,739 --> 00:14:38,669
easier to deal with okay on to our

00:14:34,499 --> 00:14:40,110
disaster recovery so how should you

00:14:38,669 --> 00:14:42,569
backup bosh and how should you backup

00:14:40,110 --> 00:14:44,819
Cloud Foundry and it really comes down

00:14:42,569 --> 00:14:46,829
to these core components you need to

00:14:44,819 --> 00:14:48,929
back up your NFS server or your

00:14:46,829 --> 00:14:51,299
blobstore so that your comparators and

00:14:48,929 --> 00:14:53,339
your artifacts are backed up you need to

00:14:51,299 --> 00:14:55,709
make sure you backup your configuration

00:14:53,339 --> 00:14:58,049
and preferably source control that as

00:14:55,709 --> 00:14:59,759
well so your boss manifests and any

00:14:58,049 --> 00:15:01,290
vendor configuration as well so for

00:14:59,759 --> 00:15:03,689
example in our case pivotal Cloud

00:15:01,290 --> 00:15:05,850
Foundry your backup ops manager and you

00:15:03,689 --> 00:15:09,149
also need to backup your Cloud Foundry

00:15:05,850 --> 00:15:10,980
databases and your bosh databases when

00:15:09,149 --> 00:15:12,889
you look at this this is effectively

00:15:10,980 --> 00:15:14,879
your Cloud Foundry in its raw form

00:15:12,889 --> 00:15:16,470
everything else around that it's just

00:15:14,879 --> 00:15:18,449
wiring it's just processes and you can

00:15:16,470 --> 00:15:21,329
bring all that stuff back but this is

00:15:18,449 --> 00:15:22,439
the stuff you need to stay around so

00:15:21,329 --> 00:15:24,029
we're going to explore a couple of

00:15:22,439 --> 00:15:25,649
scenarios here and I'm going to take you

00:15:24,029 --> 00:15:27,869
through how we do it with pivotal Cloud

00:15:25,649 --> 00:15:30,569
Foundry and especially with ops manager

00:15:27,869 --> 00:15:32,300
and also how you do this with just open

00:15:30,569 --> 00:15:35,010
source cloud foundry and boss

00:15:32,300 --> 00:15:36,540
so the first scenario is what happens if

00:15:35,010 --> 00:15:39,089
you lose ops manager we use office

00:15:36,540 --> 00:15:40,769
manager to deploy Cloud Foundry so for

00:15:39,089 --> 00:15:42,660
us ops manager is really critical and

00:15:40,769 --> 00:15:44,160
what happens if you lose your Cloud

00:15:42,660 --> 00:15:48,420
Foundry deployment how did you get that

00:15:44,160 --> 00:15:50,100
stuff back for ops manager it's really

00:15:48,420 --> 00:15:53,399
simple you can export this configuration

00:15:50,100 --> 00:15:56,300
and then if everything goes up in flames

00:15:53,399 --> 00:15:58,920
you bring back a new ops manager and

00:15:56,300 --> 00:16:01,230
then you can import that configuration

00:15:58,920 --> 00:16:02,910
and providing you're using something

00:16:01,230 --> 00:16:04,860
like an external database and external

00:16:02,910 --> 00:16:07,279
blob store for Cloud Foundry your

00:16:04,860 --> 00:16:10,320
everything's fine

00:16:07,279 --> 00:16:12,089
to export ops manager you go into the UI

00:16:10,320 --> 00:16:13,769
and just download the installation

00:16:12,089 --> 00:16:16,170
settings it's really straightforward if

00:16:13,769 --> 00:16:18,029
you're really paranoid you can also copy

00:16:16,170 --> 00:16:20,250
the deployment manifests and with these

00:16:18,029 --> 00:16:21,690
deployment manifests with boss you can

00:16:20,250 --> 00:16:23,040
actually bring back your deployment if

00:16:21,690 --> 00:16:24,690
your deployment goes away but office

00:16:23,040 --> 00:16:27,930
manager will do that for you as well if

00:16:24,690 --> 00:16:29,910
you're not using office manager then you

00:16:27,930 --> 00:16:31,680
do just that you make sure you have a

00:16:29,910 --> 00:16:33,990
copy of those deployment manifests so

00:16:31,680 --> 00:16:36,149
you can download those from bosh and if

00:16:33,990 --> 00:16:37,680
your deployment goes away then you can

00:16:36,149 --> 00:16:39,149
use those manifests to bring back your

00:16:37,680 --> 00:16:41,940
deployment and everything's good so

00:16:39,149 --> 00:16:43,440
Bosch is critical for bringing back your

00:16:41,940 --> 00:16:46,560
deployments and the boss director is

00:16:43,440 --> 00:16:48,149
critical for that so what happens if you

00:16:46,560 --> 00:16:50,070
lose that boss director if someone goes

00:16:48,149 --> 00:16:52,050
and deletes that VM if it's a single VM

00:16:50,070 --> 00:16:54,149
what we call Micro Bosch or something

00:16:52,050 --> 00:16:57,270
else happens with it how do you recover

00:16:54,149 --> 00:16:58,800
from that scenario so we start with the

00:16:57,270 --> 00:17:01,050
same approach you backup your

00:16:58,800 --> 00:17:03,060
configuration and when you deploy the

00:17:01,050 --> 00:17:05,970
boss manifests a bot direktor you have

00:17:03,060 --> 00:17:08,100
this manifest bottle llamo and ideally

00:17:05,970 --> 00:17:10,829
you need that manifest to bring back

00:17:08,100 --> 00:17:13,230
Bosch but herein lies a perceived

00:17:10,829 --> 00:17:15,150
problem in that when we go to the

00:17:13,230 --> 00:17:17,520
directory where that manifest should

00:17:15,150 --> 00:17:20,069
exist it's been deleted and the office

00:17:17,520 --> 00:17:22,199
manager does this with good reason that

00:17:20,069 --> 00:17:24,360
manifest contains your AWS secret keys

00:17:22,199 --> 00:17:25,679
and other sensitive information so we

00:17:24,360 --> 00:17:27,270
don't want to leave that manifest lying

00:17:25,679 --> 00:17:30,750
around and plaintext on the file system

00:17:27,270 --> 00:17:32,070
so it deletes it so you don't have it

00:17:30,750 --> 00:17:34,559
and when everything else goes up in

00:17:32,070 --> 00:17:36,780
flames how do you recover so you bring

00:17:34,559 --> 00:17:39,090
back ops manager in the same way forward

00:17:36,780 --> 00:17:41,309
in the same way as our previously and

00:17:39,090 --> 00:17:42,960
then ops manager and this is the secret

00:17:41,309 --> 00:17:45,480
sauce it's got some capability to

00:17:42,960 --> 00:17:47,010
reconstruct that boss director

00:17:45,480 --> 00:17:49,320
with the same IP same set of credentials

00:17:47,010 --> 00:17:52,950
and so ops manager will actually bring

00:17:49,320 --> 00:17:54,090
that Bosch director back for you so how

00:17:52,950 --> 00:17:55,650
do you do this if you're not using

00:17:54,090 --> 00:17:58,140
pivotal Cloud Foundry an office manager

00:17:55,650 --> 00:18:01,440
so I would argue you absolutely need to

00:17:58,140 --> 00:18:02,910
keep that Bosch manifest around it's got

00:18:01,440 --> 00:18:04,500
all your credentials it's got all the

00:18:02,910 --> 00:18:06,210
information about your Bosch director

00:18:04,500 --> 00:18:07,860
and so ideally you need to source

00:18:06,210 --> 00:18:10,380
control that and you need to keep it and

00:18:07,860 --> 00:18:11,850
then with that Yama file with that

00:18:10,380 --> 00:18:13,919
manifest you can bring back your boss

00:18:11,850 --> 00:18:15,750
director but you need more than that

00:18:13,919 --> 00:18:17,070
because Bosch needs to be aware of all

00:18:15,750 --> 00:18:19,770
those deployments which you've already

00:18:17,070 --> 00:18:22,080
got out there so you have to you have a

00:18:19,770 --> 00:18:24,299
couple of options here if you go into

00:18:22,080 --> 00:18:25,980
Bosch and you use monitor stop to stop

00:18:24,299 --> 00:18:27,540
all the processes you can backup the

00:18:25,980 --> 00:18:29,760
Bosch database if you're using the

00:18:27,540 --> 00:18:31,440
internal database and then when you

00:18:29,760 --> 00:18:32,880
bring back a new Bosch you can do the

00:18:31,440 --> 00:18:36,030
same and then you can import that data

00:18:32,880 --> 00:18:37,410
into your new database if you need some

00:18:36,030 --> 00:18:39,480
more information and you're using the

00:18:37,410 --> 00:18:41,760
internal file system I'm sorry the NFS

00:18:39,480 --> 00:18:43,950
store then you can snapshot the disk

00:18:41,760 --> 00:18:45,660
again go into boss stop where the

00:18:43,950 --> 00:18:48,480
processes detach the new disk and

00:18:45,660 --> 00:18:50,340
reattach the snapshot my preferred

00:18:48,480 --> 00:18:52,350
solution is to use an external boss

00:18:50,340 --> 00:18:54,059
database and an external blob store and

00:18:52,350 --> 00:18:55,650
then when you bring back Bosch on the

00:18:54,059 --> 00:18:57,690
same ip you just connect it up to your

00:18:55,650 --> 00:19:01,500
datastore store your database in your

00:18:57,690 --> 00:19:03,179
blob store and where you go when you're

00:19:01,500 --> 00:19:05,160
backing up the Cloud Foundry databases

00:19:03,179 --> 00:19:06,540
you need the DB encryption key for the

00:19:05,160 --> 00:19:09,000
Cloud Controller because it's encrypted

00:19:06,540 --> 00:19:10,380
so in order to backup that and restore

00:19:09,000 --> 00:19:14,610
that database you need that encryption

00:19:10,380 --> 00:19:16,110
key in addition with the blobstore you

00:19:14,610 --> 00:19:19,140
need to think about how you back that up

00:19:16,110 --> 00:19:22,230
as well so in this case using s3 we set

00:19:19,140 --> 00:19:23,790
a policy to deny bucket deletion and

00:19:22,230 --> 00:19:25,020
that meant that that buckets never going

00:19:23,790 --> 00:19:27,360
to go away the GU is not going to change

00:19:25,020 --> 00:19:29,490
so you've always got that bucket and

00:19:27,360 --> 00:19:30,900
then you can turn on versioning for that

00:19:29,490 --> 00:19:33,000
bucket or for the contents of that

00:19:30,900 --> 00:19:34,650
bucket and if someone goes in and

00:19:33,000 --> 00:19:36,360
maliciously or accidentally deletes that

00:19:34,650 --> 00:19:43,130
content you can restore that content in

00:19:36,360 --> 00:19:46,200
your bucket still there so wrapping up

00:19:43,130 --> 00:19:48,030
quite a lot of the time companies think

00:19:46,200 --> 00:19:50,490
of security and they think of it as an

00:19:48,030 --> 00:19:51,750
afterthought security should not be an

00:19:50,490 --> 00:19:54,059
afterthought it's actually really

00:19:51,750 --> 00:19:55,440
important so what I want to do is

00:19:54,059 --> 00:19:57,270
provide you a little bit of a primer

00:19:55,440 --> 00:19:58,680
some basic things that we've seen out

00:19:57,270 --> 00:20:00,660
with customers

00:19:58,680 --> 00:20:02,730
that they've done you know potentially

00:20:00,660 --> 00:20:05,430
good things and just share them so that

00:20:02,730 --> 00:20:10,170
you can all kind of get a start in Cloud

00:20:05,430 --> 00:20:12,750
Foundry security so security is a hard

00:20:10,170 --> 00:20:14,940
problem it's not a problem that is

00:20:12,750 --> 00:20:17,250
solved once it is a process that needs

00:20:14,940 --> 00:20:21,180
to be continually solved to continually

00:20:17,250 --> 00:20:24,150
updated continually managed as well so

00:20:21,180 --> 00:20:26,340
iterating through security and security

00:20:24,150 --> 00:20:28,980
issues is really important

00:20:26,340 --> 00:20:31,620
similarly feedback from any issues that

00:20:28,980 --> 00:20:34,920
have arisen say security incident

00:20:31,620 --> 00:20:37,350
reports obtaining management support to

00:20:34,920 --> 00:20:39,830
hold meetings to discuss security etc is

00:20:37,350 --> 00:20:42,330
fundamental so you need that

00:20:39,830 --> 00:20:45,390
organizational backing before you can

00:20:42,330 --> 00:20:46,920
even start with security really so in

00:20:45,390 --> 00:20:50,030
security there are three main concepts

00:20:46,920 --> 00:20:53,040
the concept of restriction so using

00:20:50,030 --> 00:20:56,970
access authentication and authorization

00:20:53,040 --> 00:21:00,930
to restrict access to your VMs and your

00:20:56,970 --> 00:21:03,690
jobs limiting so limiting the scope if

00:21:00,930 --> 00:21:07,050
there has been a compromised ation of a

00:21:03,690 --> 00:21:09,870
VM or a job as well as mitigating any

00:21:07,050 --> 00:21:12,980
security breaches that - or potentially

00:21:09,870 --> 00:21:16,740
occur so let's let's dig down into them

00:21:12,980 --> 00:21:20,550
the first step is to restrict users so

00:21:16,740 --> 00:21:25,170
this is mostly talking about restricting

00:21:20,550 --> 00:21:27,990
users accessing your is level your cloud

00:21:25,170 --> 00:21:30,540
foundry installation as well as things

00:21:27,990 --> 00:21:32,460
like Bosch so the number one step that

00:21:30,540 --> 00:21:36,390
we recommend is use multi-factor

00:21:32,460 --> 00:21:38,430
authentication wherever possible so each

00:21:36,390 --> 00:21:40,410
user should have an individual account

00:21:38,430 --> 00:21:42,240
something that can be attributed to them

00:21:40,410 --> 00:21:45,900
something that can be put into an audit

00:21:42,240 --> 00:21:47,190
trail to potentially find any issues

00:21:45,900 --> 00:21:51,090
that have occurred or any unwanted

00:21:47,190 --> 00:21:53,160
changes and also lock that user out if

00:21:51,090 --> 00:21:54,450
their details have become compromised or

00:21:53,160 --> 00:21:56,790
if they're no longer with the company or

00:21:54,450 --> 00:21:58,650
disgruntled or whatever so a

00:21:56,790 --> 00:22:02,610
multi-factor authentication at a minimum

00:21:58,650 --> 00:22:05,550
on the IRS level similarly you can

00:22:02,610 --> 00:22:09,420
actually put MFA on to jump boxes so

00:22:05,550 --> 00:22:12,060
when you log in using SSH using an RSA

00:22:09,420 --> 00:22:12,530
key to actually provide a token takes it

00:22:12,060 --> 00:22:15,290
from

00:22:12,530 --> 00:22:18,020
just something they know to something

00:22:15,290 --> 00:22:22,460
that they have as well as something that

00:22:18,020 --> 00:22:24,680
they know in addition to that we also

00:22:22,460 --> 00:22:27,460
recommend that all Bosch users have

00:22:24,680 --> 00:22:30,970
their own separate accounts again this

00:22:27,460 --> 00:22:33,590
creates the ability to individually

00:22:30,970 --> 00:22:35,870
identify and target any compromised

00:22:33,590 --> 00:22:39,020
usernames and passwords that come up and

00:22:35,870 --> 00:22:40,940
finally github so you'll hear me kind of

00:22:39,020 --> 00:22:43,040
rabbit on a lot about audit trails etc

00:22:40,940 --> 00:22:45,080
when you check your manifests into

00:22:43,040 --> 00:22:48,860
github it's really important that you

00:22:45,080 --> 00:22:51,530
don't just use one username or say a

00:22:48,860 --> 00:22:54,050
group username but each person has their

00:22:51,530 --> 00:22:56,660
own individual accounts and they're very

00:22:54,050 --> 00:22:59,390
disciplined at actually sending that

00:22:56,660 --> 00:23:01,250
information to github one of the issues

00:22:59,390 --> 00:23:04,550
that we found on one customer's site was

00:23:01,250 --> 00:23:07,040
that they used a jump box with a single

00:23:04,550 --> 00:23:10,520
username and password and because of

00:23:07,040 --> 00:23:12,410
that whoever logged into github was

00:23:10,520 --> 00:23:14,450
getting all of the all the credits for

00:23:12,410 --> 00:23:16,640
all the changes that would be made so

00:23:14,450 --> 00:23:20,480
unfortunately that was not good in terms

00:23:16,640 --> 00:23:21,710
of having an audit trail the second step

00:23:20,480 --> 00:23:23,930
that you need to do is to restrict

00:23:21,710 --> 00:23:25,880
packets so this is best done at the

00:23:23,930 --> 00:23:29,180
Eyres level or is done at the Aya's

00:23:25,880 --> 00:23:31,070
level I should say so I focused mostly

00:23:29,180 --> 00:23:33,110
on Amazon security and Amazon

00:23:31,070 --> 00:23:34,520
deployments so a lot of my slides are

00:23:33,110 --> 00:23:36,620
going to be a little bit more targeted

00:23:34,520 --> 00:23:39,860
to that so on Amazon we have security

00:23:36,620 --> 00:23:42,230
groups these are security policies

00:23:39,860 --> 00:23:45,140
defined at the instance level so they

00:23:42,230 --> 00:23:47,360
can span subnets I can span availability

00:23:45,140 --> 00:23:51,170
zones etc and define ingress and egress

00:23:47,360 --> 00:23:53,600
rules similarly you can use access

00:23:51,170 --> 00:23:55,160
control lists which are at the subnet

00:23:53,600 --> 00:23:56,840
level we find that most customers don't

00:23:55,160 --> 00:23:57,080
actually use these but the option is

00:23:56,840 --> 00:23:59,930
there

00:23:57,080 --> 00:24:04,190
and finally routes restrict where data

00:23:59,930 --> 00:24:05,930
can go so data from an EOB may come in

00:24:04,190 --> 00:24:10,700
and it will only be able to go to say

00:24:05,930 --> 00:24:12,710
hey cho proxies or to the routers Cloud

00:24:10,700 --> 00:24:15,140
Foundry itself has security built into

00:24:12,710 --> 00:24:18,440
it you can set network properties to

00:24:15,140 --> 00:24:19,640
allow and deny IP ranges and sliders it

00:24:18,440 --> 00:24:21,470
is important to note though that the

00:24:19,640 --> 00:24:25,940
application security groups actually

00:24:21,470 --> 00:24:26,720
override this so if you're creating a

00:24:25,940 --> 00:24:28,250
new cloud

00:24:26,720 --> 00:24:30,650
found redeployment you're best off

00:24:28,250 --> 00:24:33,140
locking out using the design networks

00:24:30,650 --> 00:24:36,350
basically everything except for of

00:24:33,140 --> 00:24:40,100
course the the VMS for Cloud Foundry and

00:24:36,350 --> 00:24:42,080
then progressively allowing access using

00:24:40,100 --> 00:24:48,830
security groups at the application level

00:24:42,080 --> 00:24:52,330
through the CFC Li sorry okay so what I

00:24:48,830 --> 00:24:55,490
wanted to do is basically diagram out a

00:24:52,330 --> 00:24:57,710
the security diagram of pivotal Cloud

00:24:55,490 --> 00:24:59,600
Foundry 1.4 it just is a bit of a this

00:24:57,710 --> 00:25:00,919
is how area engineers have said that we

00:24:59,600 --> 00:25:03,530
should do it

00:25:00,919 --> 00:25:06,470
at a basic level so you'll notice

00:25:03,530 --> 00:25:08,809
straight away that we have a single V PC

00:25:06,470 --> 00:25:11,809
and this is all in one availability zone

00:25:08,809 --> 00:25:13,580
by the way we have a public and a

00:25:11,809 --> 00:25:15,500
private section so we've got a

00:25:13,580 --> 00:25:17,390
demilitarized zone which is accessible

00:25:15,500 --> 00:25:19,490
to the Internet and then we have our

00:25:17,390 --> 00:25:22,690
private subnet which is running all of

00:25:19,490 --> 00:25:27,289
our instances and all of our databases

00:25:22,690 --> 00:25:29,030
now one of the fundamental things when

00:25:27,289 --> 00:25:31,490
setting up security rules is to have

00:25:29,030 --> 00:25:35,150
separation of concerns so you'll notice

00:25:31,490 --> 00:25:37,700
that all of the vans essentially or the

00:25:35,150 --> 00:25:40,039
instance types has separate security

00:25:37,700 --> 00:25:41,929
groups so these red lines around them so

00:25:40,039 --> 00:25:45,890
ops manager is a separate concern so

00:25:41,929 --> 00:25:47,929
it's got a separate security group the

00:25:45,890 --> 00:25:51,799
NAT box has its own security group as

00:25:47,929 --> 00:25:57,169
does the alb and the Cloud Foundry VMs

00:25:51,799 --> 00:25:58,789
in the elastic runtime in terms of what

00:25:57,169 --> 00:26:01,250
you actually allow to come into this

00:25:58,789 --> 00:26:03,289
environment it is quite a good practice

00:26:01,250 --> 00:26:06,740
to restrict down access to the ops

00:26:03,289 --> 00:26:08,299
manager so by default had this security

00:26:06,740 --> 00:26:11,120
group to not allow any ingress

00:26:08,299 --> 00:26:14,480
whatsoever and force your users to

00:26:11,120 --> 00:26:18,110
actually go to AWS and enable access for

00:26:14,480 --> 00:26:20,900
themselves this also allows an audit

00:26:18,110 --> 00:26:22,370
trail using things like cloud trail

00:26:20,900 --> 00:26:26,960
which we'll talk about a little bit

00:26:22,370 --> 00:26:28,610
later now by default you'll notice that

00:26:26,960 --> 00:26:31,400
all of these security groups here are

00:26:28,610 --> 00:26:34,490
allowing ingress from anything in the

00:26:31,400 --> 00:26:37,159
VPC this is a default State you can

00:26:34,490 --> 00:26:38,690
actually lock it down further for

00:26:37,159 --> 00:26:40,310
instance a couple of things you might

00:26:38,690 --> 00:26:43,960
want to look at doing

00:26:40,310 --> 00:26:47,180
is having the elastic runtime only allow

00:26:43,960 --> 00:26:50,740
traffic from the EOB security group on

00:26:47,180 --> 00:26:54,320
so forth or for three or port 80

00:26:50,740 --> 00:26:56,930
similarly only allowing traffic from the

00:26:54,320 --> 00:26:59,420
ops manager security group on the Bosh

00:26:56,930 --> 00:27:04,430
CLI ports which there's a quite a few of

00:26:59,420 --> 00:27:06,800
them if an attack has been successful

00:27:04,430 --> 00:27:09,380
the next thing that you need to focus on

00:27:06,800 --> 00:27:12,290
is limiting scope if there has been a

00:27:09,380 --> 00:27:15,740
compromise so let's say that run has

00:27:12,290 --> 00:27:18,350
become compromised and attackers got a

00:27:15,740 --> 00:27:20,120
script on there we want to limit the

00:27:18,350 --> 00:27:22,940
ability for them to then jump on to

00:27:20,120 --> 00:27:26,810
other Cloud Foundry VMs and access the

00:27:22,940 --> 00:27:28,730
data behind them one of the most basic

00:27:26,810 --> 00:27:32,420
things that we see that customers don't

00:27:28,730 --> 00:27:34,580
always want to do is having different

00:27:32,420 --> 00:27:37,640
usernames and passwords for the

00:27:34,580 --> 00:27:39,770
different jobs on Cloud Foundry we

00:27:37,640 --> 00:27:42,320
recommend that both the usernames and

00:27:39,770 --> 00:27:43,910
the passwords be a random string of

00:27:42,320 --> 00:27:48,170
characters say about 20 characters long

00:27:43,910 --> 00:27:50,930
with the it uses both cases and that you

00:27:48,170 --> 00:27:52,640
avoid using the animal characters so

00:27:50,930 --> 00:27:54,470
Cloud Foundry is deployed using Yammer

00:27:52,640 --> 00:27:56,360
all you don't want it to get confused or

00:27:54,470 --> 00:27:58,310
you don't want Bosh to get confused if

00:27:56,360 --> 00:28:00,650
you using special characters that it

00:27:58,310 --> 00:28:03,230
interprets differently so I personally

00:28:00,650 --> 00:28:05,870
prefer not to use any special characters

00:28:03,230 --> 00:28:07,970
whatsoever and just have like uppercase

00:28:05,870 --> 00:28:10,130
and lowercase so what does that really

00:28:07,970 --> 00:28:15,800
mean avoid using cloud Cal for

00:28:10,130 --> 00:28:19,240
everything let's say there has been a

00:28:15,800 --> 00:28:21,470
security breach it is important to

00:28:19,240 --> 00:28:24,320
understand what has happened and how it

00:28:21,470 --> 00:28:25,970
has happened so standard security post

00:28:24,320 --> 00:28:28,280
breach rules apply

00:28:25,970 --> 00:28:32,120
you should isolate any VMs that have

00:28:28,280 --> 00:28:33,620
become infected or compromised in AWS

00:28:32,120 --> 00:28:35,450
the way that you do that is you attach a

00:28:33,620 --> 00:28:38,330
special security group to that VM that

00:28:35,450 --> 00:28:41,720
only allows ingress from you and allows

00:28:38,330 --> 00:28:44,510
no egress out of that security group the

00:28:41,720 --> 00:28:46,190
cool thing about Bosh is that Bosh won't

00:28:44,510 --> 00:28:48,310
be able to see that VM anymore so to

00:28:46,190 --> 00:28:50,510
resurrect a new one for you pretty cool

00:28:48,310 --> 00:28:52,880
but the most important part is it won't

00:28:50,510 --> 00:28:54,190
be compromised because it's resurrecting

00:28:52,880 --> 00:28:56,980
it from scratch

00:28:54,190 --> 00:29:00,520
then you need to investigate how the

00:28:56,980 --> 00:29:02,780
that particular box was compromised I

00:29:00,520 --> 00:29:07,610
would recommend rolling basically

00:29:02,780 --> 00:29:11,330
everything in your deployment for new

00:29:07,610 --> 00:29:14,210
passwords usernames hems and most

00:29:11,330 --> 00:29:15,740
importantly your ayahs credentials you

00:29:14,210 --> 00:29:18,950
don't want the the attacker being able

00:29:15,740 --> 00:29:22,429
to spin up new boxes in your AWS

00:29:18,950 --> 00:29:24,650
installation and the final one is to

00:29:22,429 --> 00:29:28,309
have a good feedback loop have incident

00:29:24,650 --> 00:29:30,590
reports etc management visibility to

00:29:28,309 --> 00:29:35,210
provide better and consistently

00:29:30,590 --> 00:29:36,830
improving security we gave this talk

00:29:35,210 --> 00:29:40,280
internally and and one of the questions

00:29:36,830 --> 00:29:43,970
that came up was how do you deal with

00:29:40,280 --> 00:29:46,100
the operator who gets very disgruntled

00:29:43,970 --> 00:29:48,170
and then destructive just wants to take

00:29:46,100 --> 00:29:52,280
your environment down completely as they

00:29:48,170 --> 00:29:54,590
leave so one of the best things that you

00:29:52,280 --> 00:29:59,510
can do is really restrict individual

00:29:54,590 --> 00:30:02,210
ayahs users ability to delete so we want

00:29:59,510 --> 00:30:04,490
to avoid them deleting s3 buckets we

00:30:02,210 --> 00:30:07,700
don't want them to delete subnets or V

00:30:04,490 --> 00:30:10,160
pcs this is because the subnet ID is

00:30:07,700 --> 00:30:13,370
actually used in the manifest so

00:30:10,160 --> 00:30:14,830
depending on how you've backed it up it

00:30:13,370 --> 00:30:18,470
could be problematic bringing it back

00:30:14,830 --> 00:30:20,679
and the final thing of course you don't

00:30:18,470 --> 00:30:23,270
want them deleting your backups

00:30:20,679 --> 00:30:26,809
everything else can be recovered so

00:30:23,270 --> 00:30:29,000
that's the important point to take the

00:30:26,809 --> 00:30:33,530
other thing is AWS provides some support

00:30:29,000 --> 00:30:35,450
for advanced monitoring of users it

00:30:33,530 --> 00:30:38,600
provides through cloud trial things like

00:30:35,450 --> 00:30:40,760
cloud trail alerts so that if a

00:30:38,600 --> 00:30:42,710
particular say security group has had

00:30:40,760 --> 00:30:45,650
things changed in it you can actually

00:30:42,710 --> 00:30:47,390
alert a wider group so this becomes

00:30:45,650 --> 00:30:49,309
really really important really useful

00:30:47,390 --> 00:30:53,059
because you can build audit logs you can

00:30:49,309 --> 00:30:54,500
build safety into it and you can also

00:30:53,059 --> 00:30:57,340
have the ability to roll back changes

00:30:54,500 --> 00:30:57,340
that have been made

00:30:59,400 --> 00:31:04,810
so we're at the top of our session so

00:31:01,990 --> 00:31:06,340
just do a couple of quick takeaways when

00:31:04,810 --> 00:31:08,260
you're looking at deploying an architect

00:31:06,340 --> 00:31:10,450
in Cloud Foundry especially in a high

00:31:08,260 --> 00:31:12,190
available scenario you really need to

00:31:10,450 --> 00:31:14,140
understand the trade-offs and the

00:31:12,190 --> 00:31:16,270
environmental constraints there's no

00:31:14,140 --> 00:31:18,070
one-size-fits-all it comes down to what

00:31:16,270 --> 00:31:19,840
you're actually trying to achieve and

00:31:18,070 --> 00:31:22,420
what level of h.a and what level of D

00:31:19,840 --> 00:31:24,070
are you need specifically at the service

00:31:22,420 --> 00:31:25,840
layer you need to be cognizant of the

00:31:24,070 --> 00:31:27,460
impact of dual data centers and any

00:31:25,840 --> 00:31:29,650
impact you might have what type of data

00:31:27,460 --> 00:31:31,150
you have how you want to persist it how

00:31:29,650 --> 00:31:34,000
you want to replicate it how you keep it

00:31:31,150 --> 00:31:35,740
concurrent all of those concerns you

00:31:34,000 --> 00:31:38,290
have to be aware of any corporate

00:31:35,740 --> 00:31:41,020
security concerns networking constraints

00:31:38,290 --> 00:31:43,260
because they can also shape and affect

00:31:41,020 --> 00:31:45,520
how you deploy and tune Cloud Foundry

00:31:43,260 --> 00:31:47,650
when you look at back up you need to

00:31:45,520 --> 00:31:50,080
backup your configuration you need to

00:31:47,650 --> 00:31:53,320
backup your databases and your blobstore

00:31:50,080 --> 00:31:54,910
this is your Cloud Foundry and finally

00:31:53,320 --> 00:31:56,890
you need to think about the usage of

00:31:54,910 --> 00:31:58,540
Cloud Foundry how things get used the

00:31:56,890 --> 00:32:00,760
egress and ingress of traffic and

00:31:58,540 --> 00:32:03,070
locking Cloud Foundry down to make it as

00:32:00,760 --> 00:32:04,420
secure as possible and without thank you

00:32:03,070 --> 00:32:10,259
very much thank you

00:32:04,420 --> 00:32:10,259

YouTube URL: https://www.youtube.com/watch?v=1hYG4y7vm5w


