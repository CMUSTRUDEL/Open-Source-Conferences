Title: Ford Motor Company’s Transition from Auto-Motive to Auto-Mobility
Publication date: 2017-06-22
Playlist: Cloud Foundry Summit Silicon Valley 2017
Description: 
	Ford Motor Company’s Transition from Auto-Motive to Auto-Mobility [I] - Mohsin Ahmed & Manu Pasari, Ford Motor Company    

Software is driving the transformation at Ford Motor Company by enabling it to transition from an Automotive to both an Auto and a Mobility company. Embracing the agile software development methodologies and micro services architecture, Ford has launched business critical applications by leveraging the Cloud Foundry Platform in its Data Centers and the Public Cloud. Rapid adoption and iteration for the platform and its services presented the challenge to scale it globally and stay current on releases while maintaining its stability along with strong logging and monitoring. Learn how Ford Motor Company incorporated the DevOps model and integrated tools to overcome these challenges to maintain this continuous innovation platform.


Mohsin Ahmed
Ford Motor Company
Senior Systems Engineer
Mohsin Ahmed is working as Senior Systems Engineer at Ford Motor Company. He has been involved in various enterprise infrastructure design, development and implementation projects for the past 15 years. He has engineered resilient and highly available solutions with VMware virtualization platforms, Microsoft server and cloud infrastructure. His recent engagement has been with the design and implementation of Pivotal Cloud Foundry platform in the Microsoft Azure public cloud to support Ford’s transformation into both an automotive and a mobility company.

Manu Pasari
Ford Motor Company
Manu Pasari is a Senior Systems Engineer at Ford Motor Company with over 15 years of experience in IT industry. He has worked on design, implementation and support of Virtualization solutions, Client & Server technologies and hosting Cloud Foundry platform in the enterprise Data Centers. He has recently been involved in designing Cloud Foundry platform solutions and integrating tools for its support and automation to host next generation of applications.
Captions: 
	00:00:01,740 --> 00:00:06,330
so let's get started since we are tight

00:00:03,780 --> 00:00:11,880
on time people would continue to filter

00:00:06,330 --> 00:00:13,860
in so welcome everyone my name is marcin

00:00:11,880 --> 00:00:14,730
Amith I'm a senior systems engineer at

00:00:13,860 --> 00:00:17,310
Ford Motor Company

00:00:14,730 --> 00:00:19,140
I'm manu / sorry senior systems engineer

00:00:17,310 --> 00:00:21,140
working along with more cynic Ford Motor

00:00:19,140 --> 00:00:23,670
Company

00:00:21,140 --> 00:00:26,160
so I delivered the presentation here

00:00:23,670 --> 00:00:28,740
last year as well and this slide and the

00:00:26,160 --> 00:00:32,009
content is a repeat from last time just

00:00:28,740 --> 00:00:34,050
to provide a quick recap for everyone so

00:00:32,009 --> 00:00:36,329
early last year Ford Motor Company

00:00:34,050 --> 00:00:38,670
announced its transformation plan with

00:00:36,329 --> 00:00:40,950
the creation of four smart mobility LLC

00:00:38,670 --> 00:00:43,399
subsidiary to expand from an automotive

00:00:40,950 --> 00:00:45,899
into both an auto and a mobility company

00:00:43,399 --> 00:00:48,360
this strategy has allowed four to stay

00:00:45,899 --> 00:00:50,570
focused on strengthening and investing

00:00:48,360 --> 00:00:53,579
in its core business of manufacturing

00:00:50,570 --> 00:00:56,760
cars trucks and utilities and at the

00:00:53,579 --> 00:00:58,620
same time enable us to pursue

00:00:56,760 --> 00:01:00,300
aggressively pursue emerging

00:00:58,620 --> 00:01:03,210
opportunities through for smart mobility

00:01:00,300 --> 00:01:04,710
to transform the customer experience to

00:01:03,210 --> 00:01:09,900
a new dimension which is core to our

00:01:04,710 --> 00:01:12,840
strategy next this is a high-level Force

00:01:09,900 --> 00:01:15,360
One IT strategy which has a futuristic

00:01:12,840 --> 00:01:17,670
theme I t's main areas of focus are run

00:01:15,360 --> 00:01:19,970
and protect the business improve IIT

00:01:17,670 --> 00:01:22,140
capacity agility and efficiency and

00:01:19,970 --> 00:01:27,180
continuously innovate to improve the

00:01:22,140 --> 00:01:29,729
business so we embarked on the Cloud

00:01:27,180 --> 00:01:31,079
Foundry journey about years ago with the

00:01:29,729 --> 00:01:34,439
development of a mobile application

00:01:31,079 --> 00:01:35,820
title Ford pass the launch of Forth pass

00:01:34,439 --> 00:01:37,770
was part of Ford smart mobility

00:01:35,820 --> 00:01:39,659
initiative and the beginnings of

00:01:37,770 --> 00:01:42,479
beginning of Ford's transformation into

00:01:39,659 --> 00:01:44,610
an auto and a mobility company Ford /

00:01:42,479 --> 00:01:47,700
assets the experience platform deployed

00:01:44,610 --> 00:01:49,799
on PCF to deliver mobility products and

00:01:47,700 --> 00:01:52,290
services to a broader base of mobility

00:01:49,799 --> 00:01:55,140
users that provides both in vehicle and

00:01:52,290 --> 00:01:58,079
remote features and capabilities after

00:01:55,140 --> 00:01:59,969
the launch of Ford pass last year there

00:01:58,079 --> 00:02:01,649
was a significant increase in the

00:01:59,969 --> 00:02:03,960
interest by the application teams to

00:02:01,649 --> 00:02:06,450
test and consume the micro-services

00:02:03,960 --> 00:02:08,520
architecture we continue to onboard

00:02:06,450 --> 00:02:10,610
additional application on the platform

00:02:08,520 --> 00:02:12,470
by expanding its footprint

00:02:10,610 --> 00:02:16,100
in public loud and our enterprise data

00:02:12,470 --> 00:02:17,960
centers besides expanding the platform a

00:02:16,100 --> 00:02:19,570
lot of focus was placed on improving the

00:02:17,960 --> 00:02:22,070
governance for the platform and

00:02:19,570 --> 00:02:23,900
implementing a robust toolset to secure

00:02:22,070 --> 00:02:29,630
automate and find you in the

00:02:23,900 --> 00:02:31,130
infrastructure the main theme of today's

00:02:29,630 --> 00:02:33,110
presentation will be around how we

00:02:31,130 --> 00:02:34,820
expanded the PCF platform and the

00:02:33,110 --> 00:02:37,430
efforts involved during the last year in

00:02:34,820 --> 00:02:39,860
improving its governance the six main

00:02:37,430 --> 00:02:42,170
areas are architecture security

00:02:39,860 --> 00:02:42,830
availability scalability automation and

00:02:42,170 --> 00:02:44,959
maintenance

00:02:42,830 --> 00:02:47,150
I'll dip you touch on each of these

00:02:44,959 --> 00:02:49,790
areas as we will get into their

00:02:47,150 --> 00:02:51,500
respective details in a moment so on the

00:02:49,790 --> 00:02:53,480
architecture side the two years ago we

00:02:51,500 --> 00:02:55,640
started PCF s-- implementation in Azure

00:02:53,480 --> 00:02:58,970
and completely the architecture for

00:02:55,640 --> 00:03:00,739
on-prem last year we expanded the number

00:02:58,970 --> 00:03:02,360
of cloud foundry foundations and

00:03:00,739 --> 00:03:03,680
approved certain areas to extend the

00:03:02,360 --> 00:03:07,160
integration with internal and external

00:03:03,680 --> 00:03:09,650
services on the security side a lot of

00:03:07,160 --> 00:03:12,709
focus was on placed on implementing a

00:03:09,650 --> 00:03:15,019
toolset we deployed wall and on Prem

00:03:12,709 --> 00:03:17,660
github Enterprise and expanded the use

00:03:15,019 --> 00:03:19,100
of certificate authority base root sign

00:03:17,660 --> 00:03:21,650
SSL Certificates

00:03:19,100 --> 00:03:23,780
on the availability and we improved the

00:03:21,650 --> 00:03:26,450
backup and dr capabilities to introduce

00:03:23,780 --> 00:03:28,700
better overlays for security scalability

00:03:26,450 --> 00:03:30,230
we enhanced logging and alerting to

00:03:28,700 --> 00:03:33,500
monitor resource consumption and

00:03:30,230 --> 00:03:36,049
streamline the scaling of resources on

00:03:33,500 --> 00:03:38,450
the automation and we deployed concourse

00:03:36,049 --> 00:03:40,070
for managing the platform and for

00:03:38,450 --> 00:03:41,780
maintenance as you know it's a high

00:03:40,070 --> 00:03:44,480
maintenance platform which requires

00:03:41,780 --> 00:03:46,670
continuous and rapid iteration with

00:03:44,480 --> 00:03:48,709
various updates we enhance the use of

00:03:46,670 --> 00:03:51,620
concourse to create new pipelines for

00:03:48,709 --> 00:03:53,269
various use cases so next we will get

00:03:51,620 --> 00:03:55,250
into each of these governance areas and

00:03:53,269 --> 00:03:57,170
cover them in more detail we'll start

00:03:55,250 --> 00:03:59,690
off with the review of architecture for

00:03:57,170 --> 00:04:04,010
Azure and on followed by rest of

00:03:59,690 --> 00:04:06,080
the Kyrie's of governance this is the

00:04:04,010 --> 00:04:09,079
high-level current snapshot of our

00:04:06,080 --> 00:04:10,820
on-ramp and Azure deployments we

00:04:09,079 --> 00:04:12,950
initially started in error and then

00:04:10,820 --> 00:04:15,980
applied the lessons learned to our

00:04:12,950 --> 00:04:18,979
on-premise implementation and applied

00:04:15,980 --> 00:04:21,410
some improvements along the way we try

00:04:18,979 --> 00:04:23,539
to keep both implementations aligned as

00:04:21,410 --> 00:04:24,020
closely as possible however as you can

00:04:23,539 --> 00:04:25,009
see

00:04:24,020 --> 00:04:27,500
there are some differences between

00:04:25,009 --> 00:04:28,819
on-prem and as your architecture due to

00:04:27,500 --> 00:04:30,979
the availability of feature set and

00:04:28,819 --> 00:04:33,229
components at the time of implementation

00:04:30,979 --> 00:04:36,770
and differences in network and storage

00:04:33,229 --> 00:04:37,639
tiers we implemented the OP stack model

00:04:36,770 --> 00:04:39,500
at on-ramp

00:04:37,639 --> 00:04:42,199
I plan to roll it out in agile in the

00:04:39,500 --> 00:04:43,849
next few months now moving into the

00:04:42,199 --> 00:04:46,909
individual architectures for Azure and

00:04:43,849 --> 00:04:49,400
on pram this is a high-level

00:04:46,909 --> 00:04:51,349
architecture in Azure deployment with

00:04:49,400 --> 00:04:54,020
active active topology across two

00:04:51,349 --> 00:04:56,000
regions as you can see we are leveraging

00:04:54,020 --> 00:04:58,159
services our in our enterprise data

00:04:56,000 --> 00:05:00,169
centers where it made sense and deployed

00:04:58,159 --> 00:05:01,310
components in the public cloud we

00:05:00,169 --> 00:05:04,520
retired the technical and business

00:05:01,310 --> 00:05:06,409
justification DCF foundation is deployed

00:05:04,520 --> 00:05:08,419
across multiple regions utilize global

00:05:06,409 --> 00:05:10,909
traffic manager and regional load

00:05:08,419 --> 00:05:13,430
balancers some applications are

00:05:10,909 --> 00:05:16,159
utilizing API manager as well to be

00:05:13,430 --> 00:05:18,560
their client API front-end as you can

00:05:16,159 --> 00:05:20,300
see on the slide towards the left we are

00:05:18,560 --> 00:05:22,599
consuming on Prem infrastructure

00:05:20,300 --> 00:05:25,090
services which include github Enterprise

00:05:22,599 --> 00:05:27,469
directory services for single sign-on

00:05:25,090 --> 00:05:29,300
logging uses syslog and develop

00:05:27,469 --> 00:05:32,960
dashboard in Splunk for monitoring and

00:05:29,300 --> 00:05:34,550
alerting towards the lower end center of

00:05:32,960 --> 00:05:36,590
the screen the diagram illustrates

00:05:34,550 --> 00:05:38,750
platform being integrated with vault and

00:05:36,590 --> 00:05:40,550
concourse with some applications using

00:05:38,750 --> 00:05:43,849
sequel database for persistent storage

00:05:40,550 --> 00:05:46,969
along with events of handing event based

00:05:43,849 --> 00:05:48,650
messaging next menu will cover the

00:05:46,969 --> 00:05:50,870
details for our long-term architecture

00:05:48,650 --> 00:05:53,659
and we'll delve into the commonalities

00:05:50,870 --> 00:05:56,240
and uniqueness of the PCO platform in

00:05:53,659 --> 00:05:57,830
Azure and on Prem by touching on the

00:05:56,240 --> 00:05:59,360
areas of architecture security

00:05:57,830 --> 00:06:04,340
scalability and availability

00:05:59,360 --> 00:06:05,930
so my new Thank You Mohsen so now that

00:06:04,340 --> 00:06:07,370
we've actually looked at all the

00:06:05,930 --> 00:06:10,250
different implementations that we have

00:06:07,370 --> 00:06:11,930
done across a door and on Prem and how

00:06:10,250 --> 00:06:13,460
we actually implemented so many

00:06:11,930 --> 00:06:15,620
different boundaries I'd like to kind of

00:06:13,460 --> 00:06:17,060
like take us down to a single foundry

00:06:15,620 --> 00:06:19,550
and how we implemented in our

00:06:17,060 --> 00:06:21,560
environment so this is kind of like a

00:06:19,550 --> 00:06:23,630
typical implementation of any cloud

00:06:21,560 --> 00:06:25,699
foundry in most of your data centers so

00:06:23,630 --> 00:06:27,949
maybe most of you guys might be thinking

00:06:25,699 --> 00:06:29,840
why am I saying this all over again so

00:06:27,949 --> 00:06:32,270
the key things that I'm actually I want

00:06:29,840 --> 00:06:34,340
to point out here is you see that the

00:06:32,270 --> 00:06:36,410
top-level edge firewall and

00:06:34,340 --> 00:06:38,240
from there you see the load balancing

00:06:36,410 --> 00:06:41,090
systems combined together in the SL B

00:06:38,240 --> 00:06:43,130
and then the H a proxy layer and then

00:06:41,090 --> 00:06:45,919
the NAD gateway and firewall systems and

00:06:43,130 --> 00:06:47,810
the reason why I touch these points are

00:06:45,919 --> 00:06:50,360
these are the ones that are actually

00:06:47,810 --> 00:06:53,389
that were the most pain points for us

00:06:50,360 --> 00:06:56,270
which we actually solved using different

00:06:53,389 --> 00:07:01,180
mechanisms throughout our implementation

00:06:56,270 --> 00:07:04,100
phases so what this gave us is different

00:07:01,180 --> 00:07:06,320
points of control points for us to make

00:07:04,100 --> 00:07:08,960
sure that the ingress traffic is a known

00:07:06,320 --> 00:07:10,700
traffic and also we had a control point

00:07:08,960 --> 00:07:12,680
on the egress traffic as well with the

00:07:10,700 --> 00:07:14,600
NAD gateway or the firewall device that

00:07:12,680 --> 00:07:16,550
we had another point that I want to

00:07:14,600 --> 00:07:18,500
touch on this slide is the H of proxies

00:07:16,550 --> 00:07:20,270
that we deployed in our foundry seized

00:07:18,500 --> 00:07:21,889
and open source is a proxy which

00:07:20,270 --> 00:07:26,510
actually provides the functionality to

00:07:21,889 --> 00:07:29,150
offload SSL to support custom URLs for

00:07:26,510 --> 00:07:33,110
applications running on this foundry and

00:07:29,150 --> 00:07:35,360
also we have implemented some ACLs at

00:07:33,110 --> 00:07:37,220
the H a proxy level to make sure that we

00:07:35,360 --> 00:07:38,810
are not exposing all the administrative

00:07:37,220 --> 00:07:42,500
endpoints for these foundries to

00:07:38,810 --> 00:07:44,930
external networks that are trying to

00:07:42,500 --> 00:07:47,360
access these founders so now that we

00:07:44,930 --> 00:07:48,710
actually reviewed Oh actually one more

00:07:47,360 --> 00:07:50,990
one more thing that I want to touch on

00:07:48,710 --> 00:07:54,169
this is the dotted line that you see

00:07:50,990 --> 00:07:55,760
there is an RFC 1918 address spaces

00:07:54,169 --> 00:07:57,560
where we actually deployed this

00:07:55,760 --> 00:08:00,050
foundries and that is one of the reasons

00:07:57,560 --> 00:08:02,650
why we see or you see an ad slash

00:08:00,050 --> 00:08:06,380
firewall system at the perimeter of the

00:08:02,650 --> 00:08:08,750
foundry so going on to or taking us down

00:08:06,380 --> 00:08:11,330
to one layer down on how is our

00:08:08,750 --> 00:08:13,340
infrastructure laid out across app stack

00:08:11,330 --> 00:08:16,070
that we've been talking about or and at

00:08:13,340 --> 00:08:18,470
the foundry level abstract is actually

00:08:16,070 --> 00:08:20,750
deployed in a management vSphere cluster

00:08:18,470 --> 00:08:22,910
which is actually leveraging or using

00:08:20,750 --> 00:08:24,830
storage that is actually replicated

00:08:22,910 --> 00:08:26,750
across the data centers so in case of a

00:08:24,830 --> 00:08:28,610
single data center failure I still have

00:08:26,750 --> 00:08:29,690
my management infrastructure up and

00:08:28,610 --> 00:08:32,180
running so that I can continue

00:08:29,690 --> 00:08:34,550
maintaining and managing my other rest

00:08:32,180 --> 00:08:36,529
of the foundries that are in running in

00:08:34,550 --> 00:08:39,380
the other data center so now going down

00:08:36,529 --> 00:08:41,000
to the actual foundries itself as you

00:08:39,380 --> 00:08:42,770
see there they're actually deployed

00:08:41,000 --> 00:08:45,260
across two different vSphere clusters

00:08:42,770 --> 00:08:46,060
that are and each of the vSphere cluster

00:08:45,260 --> 00:08:48,520
is spanning

00:08:46,060 --> 00:08:51,700
across two different racks so that

00:08:48,520 --> 00:08:53,620
actually gives me a resiliency of even

00:08:51,700 --> 00:08:55,600
if a full rag goes down my foundries

00:08:53,620 --> 00:08:58,090
still up and running and even if a full

00:08:55,600 --> 00:09:00,400
cluster goes down my still my founder is

00:08:58,090 --> 00:09:02,650
still up and running so that way we

00:09:00,400 --> 00:09:04,720
introduced multiple layers of resiliency

00:09:02,650 --> 00:09:06,100
both at the cloud foundry level as well

00:09:04,720 --> 00:09:07,450
as at the vSphere or the AI

00:09:06,100 --> 00:09:12,850
infrastructure level in our

00:09:07,450 --> 00:09:14,070
implementations and now going it now

00:09:12,850 --> 00:09:16,750
that we talked about architecture

00:09:14,070 --> 00:09:18,400
designs and how we implemented the

00:09:16,750 --> 00:09:19,930
different foundries and all across Asia

00:09:18,400 --> 00:09:22,060
and on-prem I'd like to kind of like

00:09:19,930 --> 00:09:24,160
take us through what are the differences

00:09:22,060 --> 00:09:26,110
and commonalities across as our and down

00:09:24,160 --> 00:09:28,270
from implementations at the current

00:09:26,110 --> 00:09:30,190
state of our current day

00:09:28,270 --> 00:09:31,779
so the commonality is we've been

00:09:30,190 --> 00:09:34,180
deploying cloud foundries across

00:09:31,779 --> 00:09:36,970
multiple regions in the Azure space

00:09:34,180 --> 00:09:39,940
multiple data centers in our on from

00:09:36,970 --> 00:09:42,010
implementations the the differences are

00:09:39,940 --> 00:09:45,940
the tool sets that we used to actually

00:09:42,010 --> 00:09:47,680
deploy all these foundries so in in in

00:09:45,940 --> 00:09:50,260
our untrim environment we've been using

00:09:47,680 --> 00:09:51,640
a full stack app stack that we've been

00:09:50,260 --> 00:09:53,740
calling which is a combination which

00:09:51,640 --> 00:09:55,210
will combination of different components

00:09:53,740 --> 00:09:58,210
which we'll get to in the next few

00:09:55,210 --> 00:10:01,690
slides so what we try to bring in using

00:09:58,210 --> 00:10:04,240
those abstract components is the the

00:10:01,690 --> 00:10:05,860
fully automated deployment of not just

00:10:04,240 --> 00:10:08,589
the app stack which is the management

00:10:05,860 --> 00:10:10,240
stack it also kind of like helps us to

00:10:08,589 --> 00:10:12,839
automate fully automate the Cloud

00:10:10,240 --> 00:10:15,910
Foundry implementation as well so on the

00:10:12,839 --> 00:10:21,160
azure side we've been leveraging jump

00:10:15,910 --> 00:10:22,450
Server and to deploy our foundries one

00:10:21,160 --> 00:10:26,020
of the things that I would like to touch

00:10:22,450 --> 00:10:28,660
on this is when obstacles deployed using

00:10:26,020 --> 00:10:31,660
protoboys and protoboys gives us the

00:10:28,660 --> 00:10:34,510
capabilities to protobufs manages or

00:10:31,660 --> 00:10:36,880
monitors the abstract components and in

00:10:34,510 --> 00:10:38,920
case if an abstract component fails it

00:10:36,880 --> 00:10:42,040
actually will resurrect that component

00:10:38,920 --> 00:10:46,060
in case of a failure so I actually have

00:10:42,040 --> 00:10:49,150
a resiliency at that layer as well so

00:10:46,060 --> 00:10:50,800
moving on to what is our stack and how

00:10:49,150 --> 00:10:52,959
did we deploy this when I say fully

00:10:50,800 --> 00:10:55,120
automated stack what did we do so the

00:10:52,959 --> 00:10:57,820
only virtual machine or the

00:10:55,120 --> 00:11:00,070
that we deployed manually is using a

00:10:57,820 --> 00:11:01,990
template bar shinik template and from

00:11:00,070 --> 00:11:03,790
there on every other component that we

00:11:01,990 --> 00:11:05,740
deployed in the OP stack as well as

00:11:03,790 --> 00:11:10,120
individual foundries is all automated

00:11:05,740 --> 00:11:13,420
and automated using the Concours

00:11:10,120 --> 00:11:15,339
pipelines and we also leveraged a couple

00:11:13,420 --> 00:11:17,140
of tools that are part of the Bastion

00:11:15,339 --> 00:11:19,300
there that you see genesis and spruce

00:11:17,140 --> 00:11:22,750
which actually gave us the capabilities

00:11:19,300 --> 00:11:25,810
to split our long deployment manifest

00:11:22,750 --> 00:11:28,390
files into like smaller components and

00:11:25,810 --> 00:11:30,850
smaller prop properties that we can use

00:11:28,390 --> 00:11:33,730
and maintain and manage and understand

00:11:30,850 --> 00:11:36,550
them so that we can actually it's much

00:11:33,730 --> 00:11:39,580
easier to deploy it's much easier to

00:11:36,550 --> 00:11:41,380
actually kind of like remove the

00:11:39,580 --> 00:11:43,480
credentials and the certificates out of

00:11:41,380 --> 00:11:45,279
the manifest files and put them into

00:11:43,480 --> 00:11:47,260
something like a secrets management tool

00:11:45,279 --> 00:11:50,770
like Walt that we've been leveraging in

00:11:47,260 --> 00:11:53,110
the in our space so now now that we

00:11:50,770 --> 00:11:55,690
looked at what's app stack let's look at

00:11:53,110 --> 00:11:57,310
how did we actually bring in security

00:11:55,690 --> 00:12:00,610
which is part of one of the governance

00:11:57,310 --> 00:12:03,400
pillars that we were working on all this

00:12:00,610 --> 00:12:05,740
year so the commonalities are we've been

00:12:03,400 --> 00:12:08,770
leveraging evolved for secrets

00:12:05,740 --> 00:12:10,720
management and our credential management

00:12:08,770 --> 00:12:13,330
and certificates management ad

00:12:10,720 --> 00:12:17,230
Federation across all the foundries to

00:12:13,330 --> 00:12:19,450
maintain access controls to Apps Manager

00:12:17,230 --> 00:12:22,600
github for version control as well as

00:12:19,450 --> 00:12:26,529
for authentication for conkers itself

00:12:22,600 --> 00:12:28,690
and SSL Certificates across the foundry

00:12:26,529 --> 00:12:31,089
using getcha proxy sni support and

00:12:28,690 --> 00:12:33,490
extensive logging and monitoring and

00:12:31,089 --> 00:12:35,589
logging into an external system as well

00:12:33,490 --> 00:12:38,080
as we actually push those logs into an

00:12:35,589 --> 00:12:40,930
sas here SIEM system that is sitting

00:12:38,080 --> 00:12:42,880
outside of the foundry so now that we

00:12:40,930 --> 00:12:45,940
talked about the commonalities quickly

00:12:42,880 --> 00:12:48,990
touching on the the differences from a

00:12:45,940 --> 00:12:51,459
non-print a few we implemented a SES

00:12:48,990 --> 00:12:54,250
even though a is these bring in some

00:12:51,459 --> 00:12:55,810
basic security functionality today we

00:12:54,250 --> 00:12:57,550
are actually expecting the future

00:12:55,810 --> 00:12:59,800
releases of a is used to bring in much

00:12:57,550 --> 00:13:01,930
more robust implementation process as

00:12:59,800 --> 00:13:04,640
well as login capabilities within a SDS

00:13:01,930 --> 00:13:06,070
and also as you have seen

00:13:04,640 --> 00:13:08,510
we were actually we implemented

00:13:06,070 --> 00:13:10,070
perimeter firewalls but at the ingress

00:13:08,510 --> 00:13:13,010
and egress point of view in our data

00:13:10,070 --> 00:13:15,290
centers and also in agile space we were

00:13:13,010 --> 00:13:18,200
leveraging network security groups which

00:13:15,290 --> 00:13:20,690
are neural network security groups to

00:13:18,200 --> 00:13:23,120
actually implement or control access

00:13:20,690 --> 00:13:28,160
ingress and outbound access into our

00:13:23,120 --> 00:13:31,550
founders so the next slide actually very

00:13:28,160 --> 00:13:35,210
high-level automated implementation of a

00:13:31,550 --> 00:13:37,640
SES basically the way we implement is

00:13:35,210 --> 00:13:39,860
yes in our environment is once the

00:13:37,640 --> 00:13:41,480
application team gives us the ports and

00:13:39,860 --> 00:13:43,010
protocols or access control lists that

00:13:41,480 --> 00:13:45,560
they need to be implemented at that

00:13:43,010 --> 00:13:48,740
space level we as one of our engineers

00:13:45,560 --> 00:13:51,440
reviews them they were they put them

00:13:48,740 --> 00:13:52,970
into github once that is approved they

00:13:51,440 --> 00:13:55,520
acts they're actually pushed through

00:13:52,970 --> 00:13:57,470
using concourse pipelines and then that

00:13:55,520 --> 00:13:59,210
is actually in place in production we

00:13:57,470 --> 00:14:01,130
were actually working on another

00:13:59,210 --> 00:14:03,440
pipeline which actually is to validate

00:14:01,130 --> 00:14:05,870
what we've implemented versus what's

00:14:03,440 --> 00:14:08,270
approved so that way we can reconcile

00:14:05,870 --> 00:14:10,940
them on a regular basis and this kind of

00:14:08,270 --> 00:14:13,070
like leads that in leads us into our

00:14:10,940 --> 00:14:16,150
next future projects that we've been

00:14:13,070 --> 00:14:18,890
working on which is actually to add Auto

00:14:16,150 --> 00:14:21,680
onboard the users on to the foundry

00:14:18,890 --> 00:14:25,700
which includes creating the orgs and

00:14:21,680 --> 00:14:28,520
then onboarding the user onto the

00:14:25,700 --> 00:14:30,470
foundries so if we automate the ESG

00:14:28,520 --> 00:14:32,180
implementation as well that kind of like

00:14:30,470 --> 00:14:34,070
merges both the worlds together wherein

00:14:32,180 --> 00:14:35,570
they get the access to the foundry as

00:14:34,070 --> 00:14:38,330
well as they get the necessary security

00:14:35,570 --> 00:14:40,460
policies in place so that they can right

00:14:38,330 --> 00:14:42,410
away start developing their code without

00:14:40,460 --> 00:14:45,410
engaging yet another security group to

00:14:42,410 --> 00:14:48,260
open those firewalls so now that we

00:14:45,410 --> 00:14:50,270
talked about is your workflow at a very

00:14:48,260 --> 00:14:52,100
high level let's talk about availability

00:14:50,270 --> 00:14:54,770
how did we bring in availability into

00:14:52,100 --> 00:14:56,870
our foundry implementations we actually

00:14:54,770 --> 00:14:58,790
like we've been talking about we've

00:14:56,870 --> 00:15:00,770
implemented come from foundries across

00:14:58,790 --> 00:15:03,860
multiple data centers and multiple

00:15:00,770 --> 00:15:06,740
regions and we also have gem file

00:15:03,860 --> 00:15:09,080
replication across multiple foundry

00:15:06,740 --> 00:15:11,090
implementations with extensive logging

00:15:09,080 --> 00:15:12,940
and monitoring and also alerting in

00:15:11,090 --> 00:15:15,250
place that actually allow

00:15:12,940 --> 00:15:18,490
our sense alerts of anything that

00:15:15,250 --> 00:15:20,530
happens to our operations team which to

00:15:18,490 --> 00:15:24,010
act for them to act on a regular basis

00:15:20,530 --> 00:15:26,710
and then the differences are the in the

00:15:24,010 --> 00:15:29,050
tooling that we've used to back up the

00:15:26,710 --> 00:15:30,580
data the metadata that is critical for

00:15:29,050 --> 00:15:31,990
the foundry to be resurrected

00:15:30,580 --> 00:15:34,810
so we've leveraged something called

00:15:31,990 --> 00:15:35,890
shield which actually backs up ccdb

00:15:34,810 --> 00:15:38,380
blobstore

00:15:35,890 --> 00:15:40,510
bash blob bash DB and other a few other

00:15:38,380 --> 00:15:42,340
components to for us that gives us the

00:15:40,510 --> 00:15:45,250
capabilities for us to resurrect and

00:15:42,340 --> 00:15:46,870
bring the foundry up to a state where

00:15:45,250 --> 00:15:48,460
the application teams can start

00:15:46,870 --> 00:15:51,160
deploying their applications in a short

00:15:48,460 --> 00:15:53,770
window of time from an azure point of

00:15:51,160 --> 00:15:56,380
view we've actually been running some

00:15:53,770 --> 00:15:57,550
custom scripts and which actually backup

00:15:56,380 --> 00:16:00,460
all those components that I just

00:15:57,550 --> 00:16:02,080
mentioned and that jump arcs where those

00:16:00,460 --> 00:16:05,940
custom scripts are run is actually

00:16:02,080 --> 00:16:08,410
backed up using Azure recovery services

00:16:05,940 --> 00:16:10,690
so now that we talked about availability

00:16:08,410 --> 00:16:15,700
let's touch on how do we scale and how

00:16:10,690 --> 00:16:17,680
do we keep this I mean keep up keep up

00:16:15,700 --> 00:16:20,220
on a regular basis so we actually

00:16:17,680 --> 00:16:22,750
generate regular weekly and monthly

00:16:20,220 --> 00:16:26,530
capacity management or capacity reports

00:16:22,750 --> 00:16:28,720
and we proactively monitor all the Diego

00:16:26,530 --> 00:16:31,390
self capacity so that we make sure that

00:16:28,720 --> 00:16:33,840
we are ahead of our customer consumption

00:16:31,390 --> 00:16:36,400
of Diego cells teams are working

00:16:33,840 --> 00:16:39,220
currently to actually do the same

00:16:36,400 --> 00:16:41,170
capacity reporting for service instances

00:16:39,220 --> 00:16:44,310
also so once that is in place we are

00:16:41,170 --> 00:16:48,100
planning to roll it into production so

00:16:44,310 --> 00:16:49,750
today we actually have Diego cell scale

00:16:48,100 --> 00:16:54,040
up and scale down automated through

00:16:49,750 --> 00:16:56,050
concourse pipelines so I mean to keep us

00:16:54,040 --> 00:16:58,090
ahead of the ahead of the demand that we

00:16:56,050 --> 00:16:59,500
are seeing we actually leverage

00:16:58,090 --> 00:17:01,960
pipelines to scale up and scale down

00:16:59,500 --> 00:17:04,390
counts concourse pipelines so now now

00:17:01,960 --> 00:17:06,160
that I talked about the all the

00:17:04,390 --> 00:17:08,230
different few of the governance items

00:17:06,160 --> 00:17:10,630
and also a little bit of our lecture on

00:17:08,230 --> 00:17:13,089
on from foundry I'd like to hand it over

00:17:10,630 --> 00:17:16,270
to Mosel to talk about automation all

00:17:13,089 --> 00:17:17,740
right thanks man you so on the

00:17:16,270 --> 00:17:21,130
automation and as you mentioned we

00:17:17,740 --> 00:17:24,339
implemented the Concours pipelines along

00:17:21,130 --> 00:17:25,600
with genesis so as you know deploying

00:17:24,339 --> 00:17:27,370
code or updates

00:17:25,600 --> 00:17:29,380
multiple environments can be a

00:17:27,370 --> 00:17:31,660
challenging task especially when it

00:17:29,380 --> 00:17:33,970
comes to ensuring that there are

00:17:31,660 --> 00:17:36,700
commonalities like IRS properties and

00:17:33,970 --> 00:17:39,340
networking are common and when they when

00:17:36,700 --> 00:17:43,120
where they are common and where they

00:17:39,340 --> 00:17:45,669
should be specialized an ideal

00:17:43,120 --> 00:17:47,380
deployment pipeline is where you are

00:17:45,669 --> 00:17:49,150
implementing tested code in sandbox

00:17:47,380 --> 00:17:51,130
performing your integration and

00:17:49,150 --> 00:17:53,410
acceptance testing in pre part and then

00:17:51,130 --> 00:17:55,150
moving the code to production we

00:17:53,410 --> 00:17:56,710
encountered significant challenges when

00:17:55,150 --> 00:17:58,929
we needed to make changes to the common

00:17:56,710 --> 00:18:00,520
elements of these different environment

00:17:58,929 --> 00:18:02,169
environments and observed that without

00:18:00,520 --> 00:18:04,659
proper discipline these different

00:18:02,169 --> 00:18:07,240
environments can easily drift leading to

00:18:04,659 --> 00:18:09,220
negative consequences to address these

00:18:07,240 --> 00:18:12,730
challenges we introduced Genesis proofs

00:18:09,220 --> 00:18:15,100
and Concours in our environment so just

00:18:12,730 --> 00:18:17,380
to give a level set of to everyone what

00:18:15,100 --> 00:18:18,970
Genesis is Genesis basically changes the

00:18:17,380 --> 00:18:21,250
scenario of making changes to the common

00:18:18,970 --> 00:18:22,809
elements of these different environments

00:18:21,250 --> 00:18:25,080
for breaking up the Bosch configuration

00:18:22,809 --> 00:18:29,200
manifests along three logical strata

00:18:25,080 --> 00:18:30,520
global site and environment at the top

00:18:29,200 --> 00:18:32,500
the most generic configuration is

00:18:30,520 --> 00:18:35,350
considered global this is a general

00:18:32,500 --> 00:18:37,120
outline of the deployment about jobs on

00:18:35,350 --> 00:18:38,169
which instances is specified here and

00:18:37,120 --> 00:18:40,539
used everywhere

00:18:38,169 --> 00:18:42,370
beneath that the site stratum defines

00:18:40,539 --> 00:18:44,500
the composition and configuration of the

00:18:42,370 --> 00:18:46,929
infrastructure and at the lowest level

00:18:44,500 --> 00:18:48,549
environment it provides the place to set

00:18:46,929 --> 00:18:50,740
the networking for a single deployment

00:18:48,549 --> 00:18:53,409
and specify the override properties and

00:18:50,740 --> 00:18:54,850
scaling factors Genesis combines these

00:18:53,409 --> 00:18:57,520
different levels of configuration to

00:18:54,850 --> 00:18:59,559
produce a single manifest single Bosch

00:18:57,520 --> 00:19:01,630
manifest for each environment and uses

00:18:59,559 --> 00:19:03,220
another tool called spruce to handle

00:19:01,630 --> 00:19:05,100
overrides and references in a

00:19:03,220 --> 00:19:07,830
straightforward and a predictable manner

00:19:05,100 --> 00:19:10,720
at this point we can leverage Concours

00:19:07,830 --> 00:19:15,220
to consume the manifest produced by

00:19:10,720 --> 00:19:17,350
genesis so this is the architecture how

00:19:15,220 --> 00:19:19,240
implemented Concours a pretty standard

00:19:17,350 --> 00:19:21,850
way of deployment a single

00:19:19,240 --> 00:19:24,100
implementation of web content with a

00:19:21,850 --> 00:19:26,669
back-end database and a cluster of

00:19:24,100 --> 00:19:29,020
workers across all the foundations

00:19:26,669 --> 00:19:31,179
concourse is in created integrated with

00:19:29,020 --> 00:19:35,100
github and vault and we are using github

00:19:31,179 --> 00:19:38,860
as a back and forth for vault and github

00:19:35,100 --> 00:19:39,460
for vault and concourse now let's review

00:19:38,860 --> 00:19:43,990
how

00:19:39,460 --> 00:19:45,700
and concourse there together our

00:19:43,990 --> 00:19:48,279
infrastructure cannot be considered s

00:19:45,700 --> 00:19:50,980
code each PCF deployment uses a Djamel

00:19:48,279 --> 00:19:53,799
file exceeding five thousand lines of

00:19:50,980 --> 00:19:55,539
code to deploy by using Genesis we are

00:19:53,799 --> 00:19:57,580
taking our five thousand-plus lines of

00:19:55,539 --> 00:20:00,399
manifest and turning it into an

00:19:57,580 --> 00:20:02,049
object-oriented general design we split

00:20:00,399 --> 00:20:04,330
up our code into smaller more manageable

00:20:02,049 --> 00:20:06,610
files and adduced duplication by using

00:20:04,330 --> 00:20:09,580
references instead of duplicating lines

00:20:06,610 --> 00:20:11,649
of GML some of these references pull

00:20:09,580 --> 00:20:13,450
multiple certificates from Walt for the

00:20:11,649 --> 00:20:16,270
reducing the size of manifest we need to

00:20:13,450 --> 00:20:18,010
work with we achieve this by taking

00:20:16,270 --> 00:20:19,770
properties that apply to all foundations

00:20:18,010 --> 00:20:21,580
and put them at the global level

00:20:19,770 --> 00:20:23,140
properties that apply to the

00:20:21,580 --> 00:20:25,270
Foundation's in the same region are

00:20:23,140 --> 00:20:27,100
stored in the site level and properties

00:20:25,270 --> 00:20:29,350
that apply to a specific foundation are

00:20:27,100 --> 00:20:31,870
stored in the environment level all

00:20:29,350 --> 00:20:33,520
these are merged with Genesis upon build

00:20:31,870 --> 00:20:38,289
time to create one manifest that we

00:20:33,520 --> 00:20:40,690
deploy with once these manifests are

00:20:38,289 --> 00:20:42,490
organized using Genesis we can use

00:20:40,690 --> 00:20:44,799
Genesis to create course deployment

00:20:42,490 --> 00:20:46,630
pipelines for us PCF and all services

00:20:44,799 --> 00:20:50,649
are then deployed using this pipeline

00:20:46,630 --> 00:20:52,299
the Genesis creates this is just a

00:20:50,649 --> 00:20:56,440
high-level layout of our conquer sea of

00:20:52,299 --> 00:20:59,020
pipeline next this is the end-to-end

00:20:56,440 --> 00:21:01,360
Concours workflow that we already talked

00:20:59,020 --> 00:21:04,240
about starting from an admin checking in

00:21:01,360 --> 00:21:05,890
their manifest to github that triggers

00:21:04,240 --> 00:21:08,409
our alert at the concourse level and

00:21:05,890 --> 00:21:10,750
then the Concours worker doing the heavy

00:21:08,409 --> 00:21:14,320
lifting of using sprues to do the merge

00:21:10,750 --> 00:21:16,720
and pulling the secrets from the vault

00:21:14,320 --> 00:21:20,950
and then deployed using Bosh to either

00:21:16,720 --> 00:21:23,169
elastic downtime or PCF services so next

00:21:20,950 --> 00:21:25,210
mono is going to get into our data

00:21:23,169 --> 00:21:27,190
operations and how our operations team

00:21:25,210 --> 00:21:32,980
is managing the platform on a day to day

00:21:27,190 --> 00:21:35,260
basis alright so now that we talked

00:21:32,980 --> 00:21:37,419
about all the automation that we built

00:21:35,260 --> 00:21:39,640
in all the different architectures how

00:21:37,419 --> 00:21:44,200
is our operations team managing this on

00:21:39,640 --> 00:21:47,279
a regular basis so what we have a daily

00:21:44,200 --> 00:21:49,840
stand up call in our environment so

00:21:47,279 --> 00:21:52,210
every day pretty much the operations

00:21:49,840 --> 00:21:53,030
team looks at efficiencies around how

00:21:52,210 --> 00:21:54,410
what are the

00:21:53,030 --> 00:21:56,450
different things that we can automate

00:21:54,410 --> 00:22:00,770
today that's the mindset that we start

00:21:56,450 --> 00:22:02,750
with so so far they've been able to like

00:22:00,770 --> 00:22:05,780
you have seen they automate the

00:22:02,750 --> 00:22:09,770
deployment upgrade scaling reporting on

00:22:05,780 --> 00:22:12,260
a regular basis to our management we

00:22:09,770 --> 00:22:14,030
also are in the process of bringing

00:22:12,260 --> 00:22:16,460
automation around like I mentioned

00:22:14,030 --> 00:22:18,140
briefly early on on how do we actually

00:22:16,460 --> 00:22:20,810
automate the process of onboarding a

00:22:18,140 --> 00:22:23,570
user without an ops team getting engaged

00:22:20,810 --> 00:22:25,430
in the onboarding process so that's

00:22:23,570 --> 00:22:27,230
being actually coded and tested in the

00:22:25,430 --> 00:22:28,460
lab environments pretty soon we hope we

00:22:27,230 --> 00:22:31,460
can roll that into our production

00:22:28,460 --> 00:22:33,740
environment and also briefly touched on

00:22:31,460 --> 00:22:36,830
this other next line item as well we

00:22:33,740 --> 00:22:40,250
actually have pipelines being built to

00:22:36,830 --> 00:22:42,440
actually reconcile the ASG rules that we

00:22:40,250 --> 00:22:44,930
have documented in github versus what's

00:22:42,440 --> 00:22:47,180
being running so that way we have a good

00:22:44,930 --> 00:22:48,710
understanding and a clear picture of why

00:22:47,180 --> 00:22:52,010
a particular application is working

00:22:48,710 --> 00:22:54,140
versus not working so that's the

00:22:52,010 --> 00:22:57,980
operational activity and coming to the

00:22:54,140 --> 00:23:00,320
maintenance activity of the foundries we

00:22:57,980 --> 00:23:02,120
actually created a true DevOps model in

00:23:00,320 --> 00:23:04,520
our environment where engineering team

00:23:02,120 --> 00:23:06,800
and operations team work hand-in-hand we

00:23:04,520 --> 00:23:10,040
basically sit in the same room working

00:23:06,800 --> 00:23:11,780
hand-in-hand we created a very efficient

00:23:10,040 --> 00:23:14,180
and then nimble environment where we

00:23:11,780 --> 00:23:15,650
iterate through we discuss through what

00:23:14,180 --> 00:23:17,870
are the different versions needed the

00:23:15,650 --> 00:23:19,880
compatibilities and things like that so

00:23:17,870 --> 00:23:23,030
that we can actually keep upgrading and

00:23:19,880 --> 00:23:24,560
fixing the code or fixing systems to

00:23:23,030 --> 00:23:27,140
facilitate our application teams

00:23:24,560 --> 00:23:28,910
onboarding process the application code

00:23:27,140 --> 00:23:30,230
itself there were some some instances

00:23:28,910 --> 00:23:31,880
where we had to work with the

00:23:30,230 --> 00:23:34,250
application teams to actually fix their

00:23:31,880 --> 00:23:37,370
help fix their code so that's the kind

00:23:34,250 --> 00:23:40,550
of environment that we work in we

00:23:37,370 --> 00:23:43,610
actually extensively leveraged compost

00:23:40,550 --> 00:23:46,640
pipelines like we we saw in our earlier

00:23:43,610 --> 00:23:48,710
slides we actually perform platform

00:23:46,640 --> 00:23:50,540
upgrades during normal business hours

00:23:48,710 --> 00:23:54,200
that's the most important thing that I'd

00:23:50,540 --> 00:23:56,000
like to stress on here now moving on to

00:23:54,200 --> 00:23:58,250
now that we talked about operational

00:23:56,000 --> 00:23:59,870
activity the maintenance of it let's

00:23:58,250 --> 00:24:02,750
look at what are the challenges and

00:23:59,870 --> 00:24:03,299
lessons learned by our team over the

00:24:02,750 --> 00:24:05,520
last

00:24:03,299 --> 00:24:08,820
year two to e two years period of time

00:24:05,520 --> 00:24:10,320
so we were actually ran into four

00:24:08,820 --> 00:24:12,539
specifically from an on from point of

00:24:10,320 --> 00:24:15,390
view who ran into the license limit at

00:24:12,539 --> 00:24:18,120
the LTM level by running only what

00:24:15,390 --> 00:24:19,980
performance running of only one

00:24:18,120 --> 00:24:22,679
application with eight micro-services

00:24:19,980 --> 00:24:24,450
running within the fountain so what we

00:24:22,679 --> 00:24:27,029
did is we work with our partners brought

00:24:24,450 --> 00:24:29,370
in this custom or open source H a proxy

00:24:27,029 --> 00:24:32,399
which actually brings in the asan I

00:24:29,370 --> 00:24:34,830
support and the custom URLs support to

00:24:32,399 --> 00:24:36,690
alleviate that problem and we also have

00:24:34,830 --> 00:24:38,580
the capabilities to scale up scale down

00:24:36,690 --> 00:24:41,340
H of practice because it's bosh deployed

00:24:38,580 --> 00:24:45,210
and Bausch managed and when we were

00:24:41,340 --> 00:24:47,549
deploying this the the latest foundries

00:24:45,210 --> 00:24:49,950
in our data centers we also deployed an

00:24:47,549 --> 00:24:52,049
ad device which ended up being a

00:24:49,950 --> 00:24:53,760
firewall device which you never want to

00:24:52,049 --> 00:24:56,220
do that because now you actually have to

00:24:53,760 --> 00:24:57,750
work with your firewall team to document

00:24:56,220 --> 00:24:59,490
the ports and protocols of every

00:24:57,750 --> 00:25:01,350
application team that's being on-boarded

00:24:59,490 --> 00:25:03,390
so that's one of the overhead that we

00:25:01,350 --> 00:25:04,770
introduced into the process so every

00:25:03,390 --> 00:25:06,870
application team has to go through this

00:25:04,770 --> 00:25:09,000
laborious process which we would want to

00:25:06,870 --> 00:25:11,700
eliminate in future if if at all

00:25:09,000 --> 00:25:14,039
possible and then in order for us to

00:25:11,700 --> 00:25:15,690
give a to active active sense from an

00:25:14,039 --> 00:25:17,460
application point of view we actually

00:25:15,690 --> 00:25:19,770
had to introduce something called GSL be

00:25:17,460 --> 00:25:21,809
sticky configuration to continue

00:25:19,770 --> 00:25:25,620
maintaining the user session to a given

00:25:21,809 --> 00:25:28,350
Cloud Foundry for about 20 minutes time

00:25:25,620 --> 00:25:30,750
for to avoid him going through the loop

00:25:28,350 --> 00:25:32,909
of authentication loop if he was bounce

00:25:30,750 --> 00:25:34,260
back and forth across the boundaries so

00:25:32,909 --> 00:25:37,140
that's one of the things that we want to

00:25:34,260 --> 00:25:41,250
avoid and we also implemented shield

00:25:37,140 --> 00:25:41,610
which again it is a temporary short

00:25:41,250 --> 00:25:45,000
fault

00:25:41,610 --> 00:25:47,700
most likely which is it doesn't backup

00:25:45,000 --> 00:25:49,799
wall today which has all the secrets

00:25:47,700 --> 00:25:51,570
that we need and it doesn't encrypt the

00:25:49,799 --> 00:25:53,789
data that his identity is backing up

00:25:51,570 --> 00:25:55,950
today and we were hoping those will be

00:25:53,789 --> 00:25:58,020
fixed in future releases and now I'd

00:25:55,950 --> 00:26:00,899
like to hand it over to motion to talk

00:25:58,020 --> 00:26:03,029
about challenges on Azure space alright

00:26:00,899 --> 00:26:04,919
thanks man ooh so some of the lessons

00:26:03,029 --> 00:26:07,440
learned of the challenges in the edge of

00:26:04,919 --> 00:26:09,390
space we found out that the network and

00:26:07,440 --> 00:26:11,820
network address space has to be unique

00:26:09,390 --> 00:26:13,590
across all your foundations that just

00:26:11,820 --> 00:26:17,549
makes the login and then

00:26:13,590 --> 00:26:19,169
creation of to set much easier there is

00:26:17,549 --> 00:26:21,779
a limitation of mounting external

00:26:19,169 --> 00:26:23,100
storage energy to one terabyte but I

00:26:21,779 --> 00:26:25,919
think with the recent announcement from

00:26:23,100 --> 00:26:28,020
Microsoft for managed disks we will be

00:26:25,919 --> 00:26:31,289
evaluating that feature in each food

00:26:28,020 --> 00:26:33,120
near future another thing we don't have

00:26:31,289 --> 00:26:37,020
the ability to create custom rows to

00:26:33,120 --> 00:26:40,320
delegate granular permission set to our

00:26:37,020 --> 00:26:42,210
developers and on the converse pipeline

00:26:40,320 --> 00:26:44,700
we've seen that they aren't fully

00:26:42,210 --> 00:26:46,980
portable there's still a manual work

00:26:44,700 --> 00:26:48,450
required for deployment Club for

00:26:46,980 --> 00:26:52,049
interior from one environment to another

00:26:48,450 --> 00:26:54,029
and on the logging side if we ran into

00:26:52,049 --> 00:26:56,909
issues where any time a developer turns

00:26:54,029 --> 00:26:59,070
on we're both logging especially since

00:26:56,909 --> 00:27:00,690
we are using Splunk and the backend it

00:26:59,070 --> 00:27:03,510
has exceeded our threshold for the

00:27:00,690 --> 00:27:09,570
licensing so that does create a issue on

00:27:03,510 --> 00:27:12,000
the the logging now what's next so the

00:27:09,570 --> 00:27:14,039
some of the items we plan to focus on is

00:27:12,000 --> 00:27:16,470
the draw mode alignment across our own

00:27:14,039 --> 00:27:19,970
on purim architectures and the toolset

00:27:16,470 --> 00:27:21,929
do frequent credential rotations

00:27:19,970 --> 00:27:25,020
definitely logging and monitoring

00:27:21,929 --> 00:27:28,140
enhancements we like to evaluate some of

00:27:25,020 --> 00:27:32,039
the azure service brokers around event

00:27:28,140 --> 00:27:33,809
hubs and cosmos database we like to

00:27:32,039 --> 00:27:35,640
introduce a self-service developer

00:27:33,809 --> 00:27:39,809
portal that they can developers can go

00:27:35,640 --> 00:27:42,750
in and create their own orbs and there's

00:27:39,809 --> 00:27:44,850
another initiative in work site now for

00:27:42,750 --> 00:27:48,450
the software development develop

00:27:44,850 --> 00:27:51,570
ecosystem we would like to just not

00:27:48,450 --> 00:27:53,730
automate the CF but to set up as well

00:27:51,570 --> 00:27:56,340
including Jenkins and some of the other

00:27:53,730 --> 00:27:58,320
things so before we open up for QA I

00:27:56,340 --> 00:27:59,940
just like to recognize one of our team

00:27:58,320 --> 00:28:02,190
members named Eichelberger he couldn't

00:27:59,940 --> 00:28:04,230
make it to the conference he's part of

00:28:02,190 --> 00:28:06,899
RPC of ops team he helped us putting

00:28:04,230 --> 00:28:08,700
this stack together and I see a Chris

00:28:06,899 --> 00:28:10,860
Cole Ian and steam they've been very

00:28:08,700 --> 00:28:14,760
helpful along the way post

00:28:10,860 --> 00:28:16,340
implementation in automating our Cloud

00:28:14,760 --> 00:28:19,890
Foundry through Concours and

00:28:16,340 --> 00:28:23,419
implementing that toolset so with that I

00:28:19,890 --> 00:28:23,419
open the floor for Q&A

00:28:37,450 --> 00:28:40,559
[Music]

00:28:42,050 --> 00:28:46,160
no we are not turning in a hybrid mode

00:28:44,700 --> 00:28:50,310
so those are two independent

00:28:46,160 --> 00:28:57,690
implementations yes on time manager it's

00:28:50,310 --> 00:28:59,510
not a hybrid cloud yes the way the way

00:28:57,690 --> 00:29:02,160
we've implemented the load balancing is

00:28:59,510 --> 00:29:04,260
active active in both on-prem and in

00:29:02,160 --> 00:29:06,840
Azure so it's active Africa

00:29:04,260 --> 00:29:09,870
implementation in Azure in two regions

00:29:06,840 --> 00:29:13,110
same thing multiple data centers in on

00:29:09,870 --> 00:29:14,580
time and also to add to that the

00:29:13,110 --> 00:29:17,220
applications that we are running in

00:29:14,580 --> 00:29:18,000
Azure and on Fram their focus to those

00:29:17,220 --> 00:29:21,830
environments

00:29:18,000 --> 00:29:25,820
so things that are mobile ready and

00:29:21,830 --> 00:29:28,140
customer facing which actually needs

00:29:25,820 --> 00:29:30,060
proximity to the customer are running in

00:29:28,140 --> 00:29:32,580
Azure space whereas all the corporate

00:29:30,060 --> 00:29:35,100
stuff is running in the in our data

00:29:32,580 --> 00:29:37,440
centers so so most of our connected

00:29:35,100 --> 00:29:40,410
vehicles stuff you know applications

00:29:37,440 --> 00:29:43,650
that need geographical proximity to the

00:29:40,410 --> 00:29:45,420
vehicles is in Azure and most of the

00:29:43,650 --> 00:29:56,940
marketing and sales types of application

00:29:45,420 --> 00:29:59,240
are on Ram part of it is in Asia but we

00:29:56,940 --> 00:30:01,860
are bringing back all the logging data

00:29:59,240 --> 00:30:03,960
especially with our deployment in China

00:30:01,860 --> 00:30:06,390
there are some regulatory constraints

00:30:03,960 --> 00:30:13,020
also to keep the data in those specific

00:30:06,390 --> 00:30:15,470
regions so it's a mixed bag any other

00:30:13,020 --> 00:30:15,470
questions

00:30:16,670 --> 00:30:22,720
all right thank you they're out of time

00:30:18,800 --> 00:30:22,720

YouTube URL: https://www.youtube.com/watch?v=ryQ7s8q8YWw


