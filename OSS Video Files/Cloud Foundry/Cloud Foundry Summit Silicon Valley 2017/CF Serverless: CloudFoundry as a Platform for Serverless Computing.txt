Title: CF Serverless: CloudFoundry as a Platform for Serverless Computing
Publication date: 2017-06-21
Playlist: Cloud Foundry Summit Silicon Valley 2017
Description: 
	CF Serverless: CloudFoundry as a Platform for Serverless Computing [I] - Nima Kaviani & Michael Maximilien, IBM    

As CloudFoundry matures to become the de-facto platform for enterprise cloud computing, it is no surprise that various types of computing services need to be supported. Server-less computing is a new name for various previous concepts or technologies, e.g. functors, functional programming. In this model of computing, applications are written as a series of small functions with clear input and output and can be executed in the cloud on demand. The difference between this approach and eg. micro-services is that the backend part only executes when invoked. In this talk we compare and contrast possible approaches to make CF a first class platform for server-less computing. We explore retrofitting the current CF app push model, leverage open source server less platforms like OpenWhisk, and investigate how the CF runtime (Diego) could be improved to support server less apps as on-off tasks.

Nima Kaviani
IBM
Nima Kaviani is a cloud engineer with IBM and a contributor to CF Diego. Nima holds a PhD in computer science and tweets and blogs about Cloud Foundry, distributed systems, and technology in general.

Michael (aka dr.max)
IBM
Scientist, Architect, and Engineer
My name is Michael Maximilien, better known as max or dr.max, and I am a computer scientist with IBM. At IBM Research Triangle Park, I was a principal engineer for the worldwide industry point-of-sale standard: JavaPOS. At IBM Research, some highlights include pioneering research on semantic Web services, mashups, and cloud computing, and platform-as-a-service. I joined the IBM Cloud Labs in 2014 and work closely with Pivotal Inc., to help make the Cloud Found the best PaaS.
Captions: 
	00:00:00,030 --> 00:00:05,640
okay so the I guess it's cool thing and

00:00:03,899 --> 00:00:08,910
a bad thing so first is that I'm part of

00:00:05,640 --> 00:00:12,349
this talk I get to talk so I want to

00:00:08,910 --> 00:00:14,580
mention a couple things number one is

00:00:12,349 --> 00:00:16,289
since I'm part of this talk you should

00:00:14,580 --> 00:00:17,760
be questioning well why is he running

00:00:16,289 --> 00:00:20,670
the talk and and he's a part of the talk

00:00:17,760 --> 00:00:23,910
it turns out that we submitted a talk

00:00:20,670 --> 00:00:25,560
Nia and I and my colleague and of course

00:00:23,910 --> 00:00:27,449
you know when it came to review it I was

00:00:25,560 --> 00:00:29,660
like well I submitted it so I got a stay

00:00:27,449 --> 00:00:32,489
out of it and then it turned out that

00:00:29,660 --> 00:00:34,440
Matt ranked it high so when I saw that I

00:00:32,489 --> 00:00:36,840
was like well you know it's gonna be bad

00:00:34,440 --> 00:00:39,570
because I'm part of the extension and he

00:00:36,840 --> 00:00:40,050
kept on insisting and I was like okay

00:00:39,570 --> 00:00:42,450
fine

00:00:40,050 --> 00:00:43,350
that's cool because maybe people want to

00:00:42,450 --> 00:00:45,480
hear about serverless

00:00:43,350 --> 00:00:47,370
so that's what we'll talk about but the

00:00:45,480 --> 00:00:50,969
other thing I want to mention is I don't

00:00:47,370 --> 00:00:57,870
know about a lot of you but it's always

00:00:50,969 --> 00:01:00,690
good and bad when you feel old it's bad

00:00:57,870 --> 00:01:06,840
I guess and good so why am i mentioning

00:01:00,690 --> 00:01:09,479
this so first thing is NEMA here I used

00:01:06,840 --> 00:01:12,930
to be at IBM Research and he was a

00:01:09,479 --> 00:01:16,799
student doing his PhD and and work with

00:01:12,930 --> 00:01:19,140
me so that makes me feel old so that's

00:01:16,799 --> 00:01:21,570
one and then the second part is of

00:01:19,140 --> 00:01:23,280
course it used to be a time when when I

00:01:21,570 --> 00:01:24,659
work with somebody I kind of led the

00:01:23,280 --> 00:01:27,060
project and I kind of like did

00:01:24,659 --> 00:01:28,650
everything well I'm feeling very old

00:01:27,060 --> 00:01:32,729
because he pretty much did all the work

00:01:28,650 --> 00:01:35,009
so I'm thinking some credit but he

00:01:32,729 --> 00:01:36,810
really did the work so let me try to

00:01:35,009 --> 00:01:40,200
introduce the talk and then we can

00:01:36,810 --> 00:01:41,400
commiserate later about being old so

00:01:40,200 --> 00:01:44,070
what are we talking about we're talking

00:01:41,400 --> 00:01:45,540
about serverless so why is this cool and

00:01:44,070 --> 00:01:47,970
kind of like why did I participate in

00:01:45,540 --> 00:01:50,310
this it's because part of my job is to

00:01:47,970 --> 00:01:53,100
try to find out what's cool but also how

00:01:50,310 --> 00:01:54,990
can we improve the platform right I mean

00:01:53,100 --> 00:01:56,700
the platform we love cloud foundry which

00:01:54,990 --> 00:01:59,399
we're spending a lot of time working on

00:01:56,700 --> 00:02:02,549
and surveillance was the obvious thing

00:01:59,399 --> 00:02:04,590
so first thing is sort of what is server

00:02:02,549 --> 00:02:06,479
that's computing right that's the first

00:02:04,590 --> 00:02:09,799
question you should ask a lot of people

00:02:06,479 --> 00:02:12,150
look at it as functional programming

00:02:09,799 --> 00:02:13,500
sure but there is like tons of

00:02:12,150 --> 00:02:16,860
functional programming languages that

00:02:13,500 --> 00:02:18,569
you can use so why was server list right

00:02:16,860 --> 00:02:20,549
well so functional programming on the

00:02:18,569 --> 00:02:21,150
web right that's one other way people

00:02:20,549 --> 00:02:24,030
look at it

00:02:21,150 --> 00:02:26,849
so you're invoking remote functions but

00:02:24,030 --> 00:02:28,260
then somebody at pivotal I mentioned his

00:02:26,849 --> 00:02:31,890
name maybe not

00:02:28,260 --> 00:02:33,900
Demetri told me well that's like CGI and

00:02:31,890 --> 00:02:35,609
I'm like no it's not CGI it's like CGI

00:02:33,900 --> 00:02:38,190
it's like you know his Russian accent

00:02:35,609 --> 00:02:40,230
it's CGI and then try to convince us I

00:02:38,190 --> 00:02:42,150
don't know I think it's it's it's

00:02:40,230 --> 00:02:44,400
different but it certainly looks like

00:02:42,150 --> 00:02:47,519
CGI but it's lightweight stateless and

00:02:44,400 --> 00:02:49,859
so obvious question that come up when

00:02:47,519 --> 00:02:51,599
you say ok well should I start using

00:02:49,859 --> 00:02:54,989
server lasso like what what's the

00:02:51,599 --> 00:02:58,049
advantage right so clearly why why is it

00:02:54,989 --> 00:02:59,370
different from pass right like what's so

00:02:58,049 --> 00:03:01,170
different what's so cool about it right

00:02:59,370 --> 00:03:03,030
so first thing that comes up especially

00:03:01,170 --> 00:03:04,769
when I start talking to other people at

00:03:03,030 --> 00:03:06,690
IBM that I've been pushing serverless

00:03:04,769 --> 00:03:09,420
for a couple years is that it's gonna

00:03:06,690 --> 00:03:11,129
save you money right well but then the

00:03:09,420 --> 00:03:12,750
question is is it gonna save you guys

00:03:11,129 --> 00:03:15,419
money that are maybe potentially

00:03:12,750 --> 00:03:17,699
customers of ours or competitors of ours

00:03:15,419 --> 00:03:20,940
like some of our colleagues earphone

00:03:17,699 --> 00:03:24,449
pivotal that are running Cloud Foundry

00:03:20,940 --> 00:03:26,430
like we are so which part what is it

00:03:24,449 --> 00:03:28,440
gonna do for you right so I think those

00:03:26,430 --> 00:03:30,389
are the kind of things the other thing

00:03:28,440 --> 00:03:33,329
is it is their way to sort of compare

00:03:30,389 --> 00:03:35,160
you know like what I'm doing now versus

00:03:33,329 --> 00:03:36,629
what I was doing later and of course the

00:03:35,160 --> 00:03:39,269
obvious question is couldn't you just

00:03:36,629 --> 00:03:40,739
like CF push the app and then just you

00:03:39,269 --> 00:03:43,739
know be done with it why do you have to

00:03:40,739 --> 00:03:45,150
create another complexity or another

00:03:43,739 --> 00:03:49,260
abstraction so what's what's the

00:03:45,150 --> 00:03:50,940
advantage so what we try to do and I

00:03:49,260 --> 00:03:53,010
guess in this talk is to try to help

00:03:50,940 --> 00:03:56,699
insert a question in the context of

00:03:53,010 --> 00:03:58,709
Cloud Foundry and specifically I want to

00:03:56,699 --> 00:04:00,480
limit what it's not and then Nima is

00:03:58,709 --> 00:04:03,569
gonna go into the details of what it is

00:04:00,480 --> 00:04:05,519
including some data and results so first

00:04:03,569 --> 00:04:07,260
thing is we're not proposing CF

00:04:05,519 --> 00:04:09,419
serverless all right so this is not a

00:04:07,260 --> 00:04:11,609
proposal I understand the whole process

00:04:09,419 --> 00:04:13,260
of proposal I run it but we're not

00:04:11,609 --> 00:04:17,010
trying to do this here we're trying to

00:04:13,260 --> 00:04:19,469
experiment and we clearly cannot explore

00:04:17,010 --> 00:04:20,959
all possibilities of service and Cloud

00:04:19,469 --> 00:04:24,169
Foundry just because we don't have

00:04:20,959 --> 00:04:26,240
time Neimark San Diego I kind of move

00:04:24,169 --> 00:04:28,759
around spent some time on Bosch in other

00:04:26,240 --> 00:04:30,889
places so we have day jobs basically so

00:04:28,759 --> 00:04:35,840
we try to do something that was sort of

00:04:30,889 --> 00:04:37,460
like 20% work and also even though we

00:04:35,840 --> 00:04:39,050
work for IBM we're not trying to favor

00:04:37,460 --> 00:04:40,550
one or the other as a matter fact last

00:04:39,050 --> 00:04:42,860
time we try to give this talk

00:04:40,550 --> 00:04:45,889
some people IBM to us nobody can do it I

00:04:42,860 --> 00:04:49,160
was like wow no we were open-source like

00:04:45,889 --> 00:04:51,169
no you can't talk about it right ok but

00:04:49,160 --> 00:04:53,360
I will so we are here to talk about it

00:04:51,169 --> 00:04:55,580
anyway and part of it is because we're

00:04:53,360 --> 00:04:57,080
not trying to say you should go to ours

00:04:55,580 --> 00:05:00,860
we're trying to give you a way for you

00:04:57,080 --> 00:05:02,690
to make your own decisions and the other

00:05:00,860 --> 00:05:04,250
thing also is that because we look at it

00:05:02,690 --> 00:05:06,410
as a set of test Suites it's not a

00:05:04,250 --> 00:05:09,849
complete test suite and the final thing

00:05:06,410 --> 00:05:12,919
I want to do and I'll pass it to Nima is

00:05:09,849 --> 00:05:14,810
to mention briefly what we try to do in

00:05:12,919 --> 00:05:18,069
terms of the experiments and then the

00:05:14,810 --> 00:05:21,199
different platform that we targeted so

00:05:18,069 --> 00:05:22,909
we are defining experiments run the

00:05:21,199 --> 00:05:25,130
experiments and share the results so

00:05:22,909 --> 00:05:27,889
sort of rinse and repeat and those are

00:05:25,130 --> 00:05:31,639
the platform that we targeted so a juror

00:05:27,889 --> 00:05:35,210
now has functions open wisk is IBM

00:05:31,639 --> 00:05:37,610
solution for this you know there is iron

00:05:35,210 --> 00:05:39,380
io that has an open-source version of

00:05:37,610 --> 00:05:41,479
this open which is open source a zero of

00:05:39,380 --> 00:05:43,130
course it's not open source and then of

00:05:41,479 --> 00:05:45,800
course many of you probably know about

00:05:43,130 --> 00:05:48,380
AWS lambda which is sort of one of those

00:05:45,800 --> 00:05:51,289
pioneering work around serverless that

00:05:48,380 --> 00:05:53,539
was released a while back so with that

00:05:51,289 --> 00:05:55,250
let me switch it to NEMA I'm gonna sit

00:05:53,539 --> 00:05:56,630
down and then at the end we'll have some

00:05:55,250 --> 00:06:00,169
time for questions so I can come back

00:05:56,630 --> 00:06:02,509
thank you all right thank you very much

00:06:00,169 --> 00:06:04,880
so I think to disclaimer is I don't

00:06:02,509 --> 00:06:06,409
think he's much older than me and then

00:06:04,880 --> 00:06:08,990
the other thing is that I think he also

00:06:06,409 --> 00:06:13,400
contributed significantly to this work

00:06:08,990 --> 00:06:16,580
and so just gotta credit him for that so

00:06:13,400 --> 00:06:18,289
yes as Max mentioned the main reason we

00:06:16,580 --> 00:06:19,880
actually did this experiment was to

00:06:18,289 --> 00:06:21,710
understand several US platforms

00:06:19,880 --> 00:06:24,530
especially because when we started in

00:06:21,710 --> 00:06:26,330
this work it was fall 2016 and the

00:06:24,530 --> 00:06:28,430
surrealist platform kind of were

00:06:26,330 --> 00:06:30,440
emerging at that point it's more mature

00:06:28,430 --> 00:06:31,020
now but at that point it was like very

00:06:30,440 --> 00:06:32,819
realistic

00:06:31,020 --> 00:06:34,919
for a lot of these platforms so what we

00:06:32,819 --> 00:06:36,330
wanted to do was to kind of have a way

00:06:34,919 --> 00:06:39,000
to understand how these surveillance

00:06:36,330 --> 00:06:40,770
platforms work and you know if you want

00:06:39,000 --> 00:06:42,599
to choose one service platform over the

00:06:40,770 --> 00:06:45,509
other what are the factors that you need

00:06:42,599 --> 00:06:46,979
to consider so in order to do that the

00:06:45,509 --> 00:06:48,780
first step that we took was that we

00:06:46,979 --> 00:06:52,590
decided that okay maybe it's better for

00:06:48,780 --> 00:06:54,389
us to kind of simulate something that

00:06:52,590 --> 00:06:56,370
resembles the behavior of a service

00:06:54,389 --> 00:06:57,810
system and then that gives us enough

00:06:56,370 --> 00:06:59,190
understanding to then go and define

00:06:57,810 --> 00:07:00,990
experiments that we can run on these

00:06:59,190 --> 00:07:02,759
surveillance platforms and see how they

00:07:00,990 --> 00:07:04,289
behaved and because we were all from

00:07:02,759 --> 00:07:05,699
Cloud Foundry we were like okay Cloud

00:07:04,289 --> 00:07:08,460
Foundry is a platform that runs

00:07:05,699 --> 00:07:10,110
applications and several assistants that

00:07:08,460 --> 00:07:12,300
are essentially functions that you load

00:07:10,110 --> 00:07:14,400
into containers and run so it's more or

00:07:12,300 --> 00:07:16,229
less the same so why not build something

00:07:14,400 --> 00:07:17,460
on top of Cloud Foundry that we can

00:07:16,229 --> 00:07:18,930
experiment with and understand the

00:07:17,460 --> 00:07:22,139
system and that's how CF server list

00:07:18,930 --> 00:07:24,930
emerged so we worked with the the man in

00:07:22,139 --> 00:07:26,819
the boss named Dmitri and we basically

00:07:24,930 --> 00:07:28,680
started coding something that was

00:07:26,819 --> 00:07:30,599
emulating the behavior of a service

00:07:28,680 --> 00:07:32,520
system and the way it worked was that it

00:07:30,599 --> 00:07:34,650
was basically a CF application that you

00:07:32,520 --> 00:07:37,080
would push to a Cloud Foundry deployment

00:07:34,650 --> 00:07:39,360
and that CF application would manage

00:07:37,080 --> 00:07:40,889
other applications in the platform so if

00:07:39,360 --> 00:07:42,539
you have functions you would define them

00:07:40,889 --> 00:07:44,789
in the form of typical CF server

00:07:42,539 --> 00:07:47,460
CF applications and then you would make

00:07:44,789 --> 00:07:50,190
a call to this manager application that

00:07:47,460 --> 00:07:52,349
would then turn on or off your other CF

00:07:50,190 --> 00:07:54,240
application as the request would go in

00:07:52,349 --> 00:07:55,979
and that's more or less the same thing

00:07:54,240 --> 00:07:58,500
basically it would turn the application

00:07:55,979 --> 00:08:01,020
on it would respond to the 2-day request

00:07:58,500 --> 00:08:02,699
through the application that it launches

00:08:01,020 --> 00:08:04,289
and then we'd keep the container for

00:08:02,699 --> 00:08:06,090
that application around for certain

00:08:04,289 --> 00:08:07,860
amount of time let's say 30 seconds and

00:08:06,090 --> 00:08:09,479
then if there is no new request to that

00:08:07,860 --> 00:08:11,250
application then it would shut down the

00:08:09,479 --> 00:08:12,779
container the main reasons the

00:08:11,250 --> 00:08:15,300
observable is are interesting is for two

00:08:12,779 --> 00:08:19,169
two reasons first of all it actually

00:08:15,300 --> 00:08:21,630
cuts abstraction significant like you I

00:08:19,169 --> 00:08:23,460
think we we are we pass we went one

00:08:21,630 --> 00:08:24,930
level above is in the sense that we

00:08:23,460 --> 00:08:27,389
didn't have to care about infrastructure

00:08:24,930 --> 00:08:29,669
with containers we went one level above

00:08:27,389 --> 00:08:32,039
it with applications we didn't have to

00:08:29,669 --> 00:08:33,839
care about anything like at the level of

00:08:32,039 --> 00:08:36,570
managing and you know running the

00:08:33,839 --> 00:08:38,579
operations and CF and surveillance is

00:08:36,570 --> 00:08:40,829
interesting because essentially all you

00:08:38,579 --> 00:08:44,190
need to care about is the function that

00:08:40,829 --> 00:08:44,800
you write so it's very low very high at

00:08:44,190 --> 00:08:46,570
the level of

00:08:44,800 --> 00:08:48,040
direction and significantly saves on

00:08:46,570 --> 00:08:49,390
engineering efforts that you need to put

00:08:48,040 --> 00:08:53,019
in place in order to manage a surrealist

00:08:49,390 --> 00:08:54,850
application also it's presumably cheaper

00:08:53,019 --> 00:08:55,959
because you're going to be charged only

00:08:54,850 --> 00:08:59,440
for the amount of time that your

00:08:55,959 --> 00:09:02,320
function runs so if your function runs

00:08:59,440 --> 00:09:04,149
only like occasionally you're going to

00:09:02,320 --> 00:09:05,620
get charged a lot less than having an

00:09:04,149 --> 00:09:07,240
application up and running for the

00:09:05,620 --> 00:09:09,190
entire time and having to pay for all

00:09:07,240 --> 00:09:10,810
the resources that you're using for

00:09:09,190 --> 00:09:13,660
while you know having your application

00:09:10,810 --> 00:09:14,380
sitting idle so with all this

00:09:13,660 --> 00:09:16,329
information

00:09:14,380 --> 00:09:17,560
we built CF several s and we realize

00:09:16,329 --> 00:09:19,029
that there are a couple of things that

00:09:17,560 --> 00:09:21,310
are very important so we started

00:09:19,029 --> 00:09:22,779
defining metrics the first thing that we

00:09:21,310 --> 00:09:24,010
realize is that it's important to

00:09:22,779 --> 00:09:26,500
understand what the throughput of the

00:09:24,010 --> 00:09:29,350
service applications are it's important

00:09:26,500 --> 00:09:31,300
to understand how they behave when there

00:09:29,350 --> 00:09:33,220
are memory intensive functions or CPU

00:09:31,300 --> 00:09:34,779
intensive functions and also it's

00:09:33,220 --> 00:09:37,149
important to understand how these

00:09:34,779 --> 00:09:38,860
several platforms manage containers

00:09:37,149 --> 00:09:41,170
because we realized that container

00:09:38,860 --> 00:09:44,649
management is a significant part of you

00:09:41,170 --> 00:09:47,019
know operating your cellular system so

00:09:44,649 --> 00:09:48,850
we define a set of functions that would

00:09:47,019 --> 00:09:51,970
allow us to address these requirements

00:09:48,850 --> 00:09:53,380
we define an echo function which was

00:09:51,970 --> 00:09:55,959
just basically a hello world function

00:09:53,380 --> 00:09:57,430
and then you would launch it and then it

00:09:55,959 --> 00:09:58,630
would send the hello word back and then

00:09:57,430 --> 00:10:01,390
we would measure at the time that it

00:09:58,630 --> 00:10:04,089
takes for the round-trip we had a memory

00:10:01,390 --> 00:10:07,260
intensive matrix multiplication function

00:10:04,089 --> 00:10:11,110
which would multiply like two 300 by 300

00:10:07,260 --> 00:10:14,079
matrix as matrices and then returned the

00:10:11,110 --> 00:10:16,180
results and that's like a typical memory

00:10:14,079 --> 00:10:18,220
intensive problem for CPU intensive

00:10:16,180 --> 00:10:20,680
functions we had a function that would

00:10:18,220 --> 00:10:23,260
find the prime numbers below 1000 and

00:10:20,680 --> 00:10:25,120
again that's a known problem and also we

00:10:23,260 --> 00:10:28,420
had the caroler function that would

00:10:25,120 --> 00:10:30,130
launch carolling of an endpoint from

00:10:28,420 --> 00:10:31,570
within a container so that we could

00:10:30,130 --> 00:10:33,250
understand whether a container stays

00:10:31,570 --> 00:10:35,680
around or whether containers gets killed

00:10:33,250 --> 00:10:38,170
after we basically send a request to B

00:10:35,680 --> 00:10:39,490
so if a Kerala function you know to stop

00:10:38,170 --> 00:10:41,290
carrolling the endpoint that we were

00:10:39,490 --> 00:10:44,320
expecting you to care they will notice

00:10:41,290 --> 00:10:46,089
that the container was killed so full

00:10:44,320 --> 00:10:48,670
disclosure the data that I'm going to

00:10:46,089 --> 00:10:51,880
talk about is based on the results that

00:10:48,670 --> 00:10:54,520
we collected during fall 2016 when we

00:10:51,880 --> 00:10:56,740
ran these experiments so it might be

00:10:54,520 --> 00:10:59,890
that you know results have changed soon

00:10:56,740 --> 00:11:01,450
but for the experiments we actually set

00:10:59,890 --> 00:11:03,279
it up so that all the functions that we

00:11:01,450 --> 00:11:06,820
deployed to all the platform and that

00:11:03,279 --> 00:11:09,760
include the AWS lambda the ESRI wave

00:11:06,820 --> 00:11:12,130
functions IRA and IO our own surrealist

00:11:09,760 --> 00:11:15,070
instance and open VIX this IBM service

00:11:12,130 --> 00:11:17,740
and they we all gave them 512 megabytes

00:11:15,070 --> 00:11:18,279
of data we set all of them up in u.s.

00:11:17,740 --> 00:11:20,500
East

00:11:18,279 --> 00:11:23,410
except for open risk which i think is

00:11:20,500 --> 00:11:26,050
primarily available in Dallas we

00:11:23,410 --> 00:11:29,260
launched 100 a ramp up request to all

00:11:26,050 --> 00:11:31,870
these endpoints and to warm up the

00:11:29,260 --> 00:11:34,420
platform and then run another 100

00:11:31,870 --> 00:11:37,750
requests to collect data and we did this

00:11:34,420 --> 00:11:41,740
in three rounds of execution and yes the

00:11:37,750 --> 00:11:43,870
environments I mentioned so let's have a

00:11:41,740 --> 00:11:45,850
look at some of the results and we

00:11:43,870 --> 00:11:47,649
intentionally do not put these results

00:11:45,850 --> 00:11:49,930
side-by-side because we don't want to

00:11:47,649 --> 00:11:51,550
draw any comparative conclusions if you

00:11:49,930 --> 00:11:52,480
wanna draw any comparative conclusions

00:11:51,550 --> 00:11:54,279
go do it yourself

00:11:52,480 --> 00:11:55,660
but the purpose here is to just give you

00:11:54,279 --> 00:11:58,270
some ideas about how the platform's

00:11:55,660 --> 00:11:59,470
behave so the type of workload that we

00:11:58,270 --> 00:12:01,270
actually launched towards these

00:11:59,470 --> 00:12:02,740
endpoints were in two modes sequential

00:12:01,270 --> 00:12:04,810
or parallel and that's how we actually

00:12:02,740 --> 00:12:06,760
decided about the throughput the idea

00:12:04,810 --> 00:12:09,310
with sequential workload was that you

00:12:06,760 --> 00:12:10,720
would hit the endpoint once and then

00:12:09,310 --> 00:12:13,000
wait for a certain amount of time and

00:12:10,720 --> 00:12:15,040
then we hit again and I would collect

00:12:13,000 --> 00:12:16,720
information about how much time it takes

00:12:15,040 --> 00:12:19,180
for that just for that request to come

00:12:16,720 --> 00:12:21,010
back with the result so if you can see

00:12:19,180 --> 00:12:23,920
these are the results for CF serverless

00:12:21,010 --> 00:12:26,709
and essentially the first request that

00:12:23,920 --> 00:12:28,899
you send to CF surrealists and the

00:12:26,709 --> 00:12:31,209
managing application that we wrote on

00:12:28,899 --> 00:12:33,760
top of sierra lifts CF serverless would

00:12:31,209 --> 00:12:35,920
go create a container load the code for

00:12:33,760 --> 00:12:37,390
the function into that container create

00:12:35,920 --> 00:12:39,399
the endpoint and then respond to the

00:12:37,390 --> 00:12:40,839
request that's why if you look at the

00:12:39,399 --> 00:12:42,850
graph the first request usually takes

00:12:40,839 --> 00:12:44,380
around like seven seconds but once the

00:12:42,850 --> 00:12:46,180
container is up and running the next

00:12:44,380 --> 00:12:48,550
follow-up requests are going to take a

00:12:46,180 --> 00:12:50,260
lot less because you know the

00:12:48,550 --> 00:12:54,670
application is ready and it can quickly

00:12:50,260 --> 00:12:56,440
respond if you delay for more than 30

00:12:54,670 --> 00:12:58,839
seconds in case of CF serverless

00:12:56,440 --> 00:13:00,670
we would kill the container so your next

00:12:58,839 --> 00:13:02,620
request would take another set of time

00:13:00,670 --> 00:13:05,040
for everything to be up and running

00:13:02,620 --> 00:13:08,560
also if you increase the load

00:13:05,040 --> 00:13:10,040
potentially that in such a way that you

00:13:08,560 --> 00:13:11,779
need another instance of

00:13:10,040 --> 00:13:13,759
the application to respond to your

00:13:11,779 --> 00:13:15,769
request we would see a spike which would

00:13:13,759 --> 00:13:19,970
involve creating a second container to

00:13:15,769 --> 00:13:21,470
respond to the application so here is an

00:13:19,970 --> 00:13:23,600
example of a high throughput function

00:13:21,470 --> 00:13:26,480
where we actually did the same echo and

00:13:23,600 --> 00:13:28,399
this time by sending all the requests at

00:13:26,480 --> 00:13:29,839
the same time and one thing that is very

00:13:28,399 --> 00:13:32,569
interesting and these are the results

00:13:29,839 --> 00:13:34,279
from Microsoft Azure one thing that we

00:13:32,569 --> 00:13:35,959
notice is that as we started sending

00:13:34,279 --> 00:13:37,850
more requests at the endpoint the

00:13:35,959 --> 00:13:40,880
response time actually got slower and

00:13:37,850 --> 00:13:42,560
slower so essentially the requests would

00:13:40,880 --> 00:13:45,949
get queued up and it would take longer

00:13:42,560 --> 00:13:47,540
for the requests to to basically come

00:13:45,949 --> 00:13:50,540
back with the result but there was

00:13:47,540 --> 00:13:52,399
suddenly a significant drop in response

00:13:50,540 --> 00:13:54,319
time and that's why that's when the

00:13:52,399 --> 00:13:56,779
surveillance platform in this case I

00:13:54,319 --> 00:13:58,370
sure would realize that hey you know I'm

00:13:56,779 --> 00:14:00,050
probably hitting the thresholds and it's

00:13:58,370 --> 00:14:01,579
better to launch another container to

00:14:00,050 --> 00:14:02,029
respond to this and that second

00:14:01,579 --> 00:14:04,399
container

00:14:02,029 --> 00:14:05,959
what's the once it's in effect also then

00:14:04,399 --> 00:14:07,639
it kind of drops the workload but then

00:14:05,959 --> 00:14:14,029
it again is such ramping up as you know

00:14:07,639 --> 00:14:16,370
more requests get cuter so another thing

00:14:14,029 --> 00:14:18,560
that we looked at was the container

00:14:16,370 --> 00:14:20,660
management and one thing that is very

00:14:18,560 --> 00:14:22,730
important and one thing that we realized

00:14:20,660 --> 00:14:25,250
with CF server lives was that you know

00:14:22,730 --> 00:14:28,069
creating the container is probably the

00:14:25,250 --> 00:14:30,470
most expensive part of the whole CF the

00:14:28,069 --> 00:14:31,940
whole serverless ecosystem because

00:14:30,470 --> 00:14:33,620
that's the part where you actually need

00:14:31,940 --> 00:14:35,449
to put these bits and pieces together

00:14:33,620 --> 00:14:38,269
and have an application up and running

00:14:35,449 --> 00:14:39,560
so we wanted to see how other platforms

00:14:38,269 --> 00:14:42,350
behave when it comes to creating

00:14:39,560 --> 00:14:45,500
containers one thing that we noticed was

00:14:42,350 --> 00:14:48,560
that for CF serverless we notice that

00:14:45,500 --> 00:14:50,810
you know if you send a request it

00:14:48,560 --> 00:14:53,000
creates a container loads the code into

00:14:50,810 --> 00:14:54,470
the container if your container is idle

00:14:53,000 --> 00:14:57,290
for certain amount of time we would kill

00:14:54,470 --> 00:15:01,370
it and that's 30 seconds for open risk

00:14:57,290 --> 00:15:03,199
for AWS lambda and for ashore we

00:15:01,370 --> 00:15:05,870
realized that they actually keep the

00:15:03,199 --> 00:15:07,880
container around somehow because the

00:15:05,870 --> 00:15:10,310
requests would take a lot less time to

00:15:07,880 --> 00:15:12,199
route to to come back in the order of

00:15:10,310 --> 00:15:13,939
hundreds of milliseconds which was very

00:15:12,199 --> 00:15:16,339
different from you know the seven-second

00:15:13,939 --> 00:15:19,819
that we would notice in case of CF

00:15:16,339 --> 00:15:23,540
servers now some of this is because in

00:15:19,819 --> 00:15:25,220
cloud foundry we do some crazy magic

00:15:23,540 --> 00:15:27,230
comes to launching applications that

00:15:25,220 --> 00:15:29,180
involves you know downloading the bill

00:15:27,230 --> 00:15:31,699
pack downloading the droplet and all of

00:15:29,180 --> 00:15:33,949
that is quite expensive but essentially

00:15:31,699 --> 00:15:35,660
for some of these platforms like leisure

00:15:33,949 --> 00:15:37,279
we also noticed that when they start the

00:15:35,660 --> 00:15:39,320
container even though it's like a plain

00:15:37,279 --> 00:15:41,360
simple container it can take you know

00:15:39,320 --> 00:15:43,910
two to three seconds for the container

00:15:41,360 --> 00:15:45,949
to become available we noticed that for

00:15:43,910 --> 00:15:47,630
lambda for example that was a lot

00:15:45,949 --> 00:15:50,000
shorter sometimes like in the order of

00:15:47,630 --> 00:15:52,459
80 milliseconds one of the assumptions

00:15:50,000 --> 00:15:54,500
we we had was that maybe they are

00:15:52,459 --> 00:15:56,540
creating you know some sort of a cache

00:15:54,500 --> 00:15:58,310
for the containers and keep one instance

00:15:56,540 --> 00:15:59,899
of the container around and you know

00:15:58,310 --> 00:16:02,029
kind of freeze it so that it doesn't

00:15:59,899 --> 00:16:03,980
utilize that much resources but then it

00:16:02,029 --> 00:16:05,839
becomes available very instantly once

00:16:03,980 --> 00:16:07,130
you need it so one thing that is

00:16:05,839 --> 00:16:08,779
important is that we treated all of

00:16:07,130 --> 00:16:10,130
these platforms as black box because

00:16:08,779 --> 00:16:11,959
there's not that much information about

00:16:10,130 --> 00:16:13,819
how exactly they manage containers so

00:16:11,959 --> 00:16:15,680
these are most like some of this is

00:16:13,819 --> 00:16:16,730
based on our observations and the data

00:16:15,680 --> 00:16:20,990
that we collected some of the

00:16:16,730 --> 00:16:23,660
assumptions that we have so here I'm

00:16:20,990 --> 00:16:26,329
showing the results of a memory

00:16:23,660 --> 00:16:29,630
intensive sequential function execution

00:16:26,329 --> 00:16:31,610
for 300 by 300 matrix multiplication and

00:16:29,630 --> 00:16:33,380
you see this is the typical behavior

00:16:31,610 --> 00:16:36,050
that you expect from a surveillance

00:16:33,380 --> 00:16:37,699
platform so all the requests more or

00:16:36,050 --> 00:16:40,940
less come back at around the same time

00:16:37,699 --> 00:16:42,860
it's kind of a flat line and in this

00:16:40,940 --> 00:16:45,139
case it's a sequential one so requests

00:16:42,860 --> 00:16:47,569
go one after another but the amount of

00:16:45,139 --> 00:16:50,480
time that you see the function takes to

00:16:47,569 --> 00:16:53,870
execute is more or less the same one

00:16:50,480 --> 00:16:56,029
thing that is interesting is that once

00:16:53,870 --> 00:16:58,839
you make something like resource

00:16:56,029 --> 00:17:01,069
intensive like matrix multiplication

00:16:58,839 --> 00:17:03,110
parallel then it can significantly

00:17:01,069 --> 00:17:05,720
degrade the performance of the system

00:17:03,110 --> 00:17:07,400
this example is from major the first few

00:17:05,720 --> 00:17:09,500
requests would actually take some time

00:17:07,400 --> 00:17:11,390
and then return back but all of a sudden

00:17:09,500 --> 00:17:13,220
the platform would crash and everything

00:17:11,390 --> 00:17:15,470
would stop and we wouldn't receive any

00:17:13,220 --> 00:17:17,510
results then you know another container

00:17:15,470 --> 00:17:19,069
would probably be launched and then it

00:17:17,510 --> 00:17:21,709
would respond to a few of the requests

00:17:19,069 --> 00:17:23,959
and then it would go down so for

00:17:21,709 --> 00:17:26,600
something heavily memory intensive we

00:17:23,959 --> 00:17:29,059
noticed that Asher was not able to cope

00:17:26,600 --> 00:17:34,640
and scale well when there are a good

00:17:29,059 --> 00:17:36,470
number of requests that go in for CPU

00:17:34,640 --> 00:17:36,810
intensive and other resource intensive

00:17:36,470 --> 00:17:38,520
case

00:17:36,810 --> 00:17:40,350
we actually did the experiments but this

00:17:38,520 --> 00:17:42,930
is the result from CF serverless and we

00:17:40,350 --> 00:17:44,520
did it in a sequential fashion you can

00:17:42,930 --> 00:17:47,190
see that again the same pattern that we

00:17:44,520 --> 00:17:48,660
saw for echo the first request takes a

00:17:47,190 --> 00:17:50,640
lot longer because it involves creating

00:17:48,660 --> 00:17:54,240
the container the follow-up requests are

00:17:50,640 --> 00:17:56,520
kind of flat and in case of parallel we

00:17:54,240 --> 00:17:59,520
did it with open risk and we saw that

00:17:56,520 --> 00:18:02,010
open risk and also lambda I believe did

00:17:59,520 --> 00:18:03,690
a much better job compared to Asher when

00:18:02,010 --> 00:18:06,330
it came to handling parallel requests

00:18:03,690 --> 00:18:08,700
for resource intensive computation so

00:18:06,330 --> 00:18:10,470
you see that there is a slight increase

00:18:08,700 --> 00:18:13,560
in the amount of response time that we

00:18:10,470 --> 00:18:16,770
noticed but basically every single

00:18:13,560 --> 00:18:18,480
request came back successfully and the

00:18:16,770 --> 00:18:19,830
slope is not that steep actually the

00:18:18,480 --> 00:18:22,160
increase in response time wasn't

00:18:19,830 --> 00:18:24,540
significant which was quite interesting

00:18:22,160 --> 00:18:26,340
so it basically means that those

00:18:24,540 --> 00:18:28,530
platforms were able to better identify

00:18:26,340 --> 00:18:30,600
and understand that you know there was

00:18:28,530 --> 00:18:32,730
heavier load on the platform and they

00:18:30,600 --> 00:18:34,890
were able to better you know create new

00:18:32,730 --> 00:18:39,510
containers and provide more resources as

00:18:34,890 --> 00:18:41,040
the number of requests increased so some

00:18:39,510 --> 00:18:44,310
of the observations that we did in this

00:18:41,040 --> 00:18:46,710
experiment and the first of that one of

00:18:44,310 --> 00:18:49,710
the interesting ones was time outs we

00:18:46,710 --> 00:18:51,660
noticed that you know these platforms

00:18:49,710 --> 00:18:53,520
generally claim that you know that the

00:18:51,660 --> 00:18:56,640
timeout the default timeout for a lot of

00:18:53,520 --> 00:18:59,760
the functions is 30 seconds but you can

00:18:56,640 --> 00:19:02,100
change it up to 5 minutes and the

00:18:59,760 --> 00:19:04,560
interesting thing is that these timeouts

00:19:02,100 --> 00:19:06,990
are only applicable if you launch

00:19:04,560 --> 00:19:08,850
request to these functions from bidding

00:19:06,990 --> 00:19:11,040
and the cloud environment that you

00:19:08,850 --> 00:19:13,200
operate so if you're using lambda for

00:19:11,040 --> 00:19:17,730
example you can increase the timeout to

00:19:13,200 --> 00:19:19,200
up to 5 5 minutes but then you have to

00:19:17,730 --> 00:19:21,390
make calls to those functions from

00:19:19,200 --> 00:19:23,310
reading the VPC if you make also the

00:19:21,390 --> 00:19:26,340
functions from outside the VPC from

00:19:23,310 --> 00:19:28,080
outside AWS the timeout is always 30

00:19:26,340 --> 00:19:29,700
seconds and it always kills your

00:19:28,080 --> 00:19:31,410
function if it takes longer than 30

00:19:29,700 --> 00:19:33,390
seconds so that's one of the things that

00:19:31,410 --> 00:19:35,580
you may need to take into consideration

00:19:33,390 --> 00:19:38,490
if you want to use several functions

00:19:35,580 --> 00:19:39,960
from outside another thing that we

00:19:38,490 --> 00:19:41,490
noticed was that it's important to

00:19:39,960 --> 00:19:43,950
understand how your cloud platform

00:19:41,490 --> 00:19:46,050
scales if you launch a lot of parallel

00:19:43,950 --> 00:19:48,720
requests to it like for example in case

00:19:46,050 --> 00:19:49,230
of a juror when the computation was you

00:19:48,720 --> 00:19:51,000
know Reese

00:19:49,230 --> 00:19:53,669
intensive we noticed that you know the

00:19:51,000 --> 00:19:55,350
platform may simply give up so it's

00:19:53,669 --> 00:19:58,440
important to understand how the platform

00:19:55,350 --> 00:20:00,750
responds also for that same matter you

00:19:58,440 --> 00:20:02,520
need to plan for unforeseen load because

00:20:00,750 --> 00:20:06,600
if they load all of sudden increases and

00:20:02,520 --> 00:20:07,710
your your underlying system or basically

00:20:06,600 --> 00:20:10,290
the surveillance platform that you're

00:20:07,710 --> 00:20:13,799
utilizing may actually perform in a

00:20:10,290 --> 00:20:15,780
weird way that you don't expect the

00:20:13,799 --> 00:20:17,549
other thing that we noticed was weather

00:20:15,780 --> 00:20:19,169
like there is this general assumption

00:20:17,549 --> 00:20:21,450
that more money always means better

00:20:19,169 --> 00:20:23,700
performance but we notice that it

00:20:21,450 --> 00:20:25,500
doesn't necessarily it doesn't

00:20:23,700 --> 00:20:27,600
necessarily happen to be the case in

00:20:25,500 --> 00:20:29,070
case of service platforms it very much

00:20:27,600 --> 00:20:30,510
depends on the architecture of the

00:20:29,070 --> 00:20:31,679
platform the way they manage their

00:20:30,510 --> 00:20:34,020
functions the way they manage their

00:20:31,679 --> 00:20:36,240
containers so even though you may end up

00:20:34,020 --> 00:20:38,280
spending a lot a lot more money on a

00:20:36,240 --> 00:20:39,690
given server less platform if the

00:20:38,280 --> 00:20:42,120
platform doesn't do a good job managing

00:20:39,690 --> 00:20:45,000
containers then it doesn't really matter

00:20:42,120 --> 00:20:49,710
like you still are gonna notice you know

00:20:45,000 --> 00:20:51,960
disruptions in your service there is a

00:20:49,710 --> 00:20:53,880
blog post that we have wrote

00:20:51,960 --> 00:20:55,590
highlighting some of these lessons that

00:20:53,880 --> 00:20:57,179
we've learned running these experiments

00:20:55,590 --> 00:20:58,679
and I think the link is here if you're

00:20:57,179 --> 00:21:01,650
interested you can definitely go and

00:20:58,679 --> 00:21:03,299
check that out and so the last thing

00:21:01,650 --> 00:21:04,830
that I want to mention before wrapping

00:21:03,299 --> 00:21:07,950
this table is where we want to go from

00:21:04,830 --> 00:21:10,770
here and the main reason we we did this

00:21:07,950 --> 00:21:13,380
was to understand answerless platforms

00:21:10,770 --> 00:21:14,520
so we started thinking about something

00:21:13,380 --> 00:21:17,280
like spec serverless

00:21:14,520 --> 00:21:19,230
a bunch of tools and that we would make

00:21:17,280 --> 00:21:21,390
available open source so that you can go

00:21:19,230 --> 00:21:23,309
and run these against these different

00:21:21,390 --> 00:21:25,559
service platforms collect similar data

00:21:23,309 --> 00:21:27,510
to they want to what we've reported here

00:21:25,559 --> 00:21:28,919
and then decide for your own whether a

00:21:27,510 --> 00:21:33,419
given service platform is the right

00:21:28,919 --> 00:21:35,730
choice for you so what we've done so far

00:21:33,419 --> 00:21:38,490
is that and you know we've put the

00:21:35,730 --> 00:21:40,410
proposal out there and we've defined the

00:21:38,490 --> 00:21:42,270
objectives the metrics the workloads and

00:21:40,410 --> 00:21:43,799
also the tests and we've drafted

00:21:42,270 --> 00:21:46,710
something that we are communicating

00:21:43,799 --> 00:21:48,419
internally within IBM and also people in

00:21:46,710 --> 00:21:50,429
order to decide you know at one point we

00:21:48,419 --> 00:21:52,980
want to make it publicly available both

00:21:50,429 --> 00:21:54,780
the tool and also the benchmark

00:21:52,980 --> 00:21:57,750
definitions that we've prepared and

00:21:54,780 --> 00:22:00,350
worked on so I think that's it and thank

00:21:57,750 --> 00:22:00,350
you very much

00:22:03,100 --> 00:22:07,780
we have time for questions like five

00:22:05,630 --> 00:22:10,750
minutes all right

00:22:07,780 --> 00:22:14,720
just mention your name and Association

00:22:10,750 --> 00:22:17,540
hi i'm sabab from butyl so you speak or

00:22:14,720 --> 00:22:18,860
task ApS not sufficient enough to run

00:22:17,540 --> 00:22:20,570
something like serverless functions

00:22:18,860 --> 00:22:22,910
because you can define a function and

00:22:20,570 --> 00:22:26,000
Diego basically run CLR piece with some

00:22:22,910 --> 00:22:29,150
tasks so there is a task API why can't

00:22:26,000 --> 00:22:31,190
we use that and basically have a pool of

00:22:29,150 --> 00:22:33,620
container instances so those can be

00:22:31,190 --> 00:22:35,750
immediately run with that specific task

00:22:33,620 --> 00:22:38,120
definition you pass like a Python or go

00:22:35,750 --> 00:22:39,980
or whatever Ruby or bash you mean in

00:22:38,120 --> 00:22:42,350
class foundry right okay so there are a

00:22:39,980 --> 00:22:44,630
couple of interesting things about tasks

00:22:42,350 --> 00:22:47,660
the one the first thing is that tasks

00:22:44,630 --> 00:22:49,850
are do not get routes so they do not

00:22:47,660 --> 00:22:52,370
become available externally so you can

00:22:49,850 --> 00:22:53,960
have you know an endpoint for your task

00:22:52,370 --> 00:22:55,520
so that you hit it and every time you

00:22:53,960 --> 00:22:55,940
hit it it actually does something for

00:22:55,520 --> 00:22:57,799
you

00:22:55,940 --> 00:22:59,179
tasks are kind of like background jobs

00:22:57,799 --> 00:23:01,160
more or less that's one of the main

00:22:59,179 --> 00:23:03,830
reasons you don't do it the other thing

00:23:01,160 --> 00:23:05,900
is that tasks suffer from the same

00:23:03,830 --> 00:23:07,580
problems that I've mentioned here in

00:23:05,900 --> 00:23:09,919
case of CF server lists every single

00:23:07,580 --> 00:23:11,809
time you run a task you need to create a

00:23:09,919 --> 00:23:13,250
container you need to download the build

00:23:11,809 --> 00:23:15,020
pack you need to download the droplet

00:23:13,250 --> 00:23:16,640
and that's a very expensive thing to do

00:23:15,020 --> 00:23:18,620
so oftentimes your task is going to end

00:23:16,640 --> 00:23:22,490
up taking seven seconds before it's

00:23:18,620 --> 00:23:24,169
actually ready to run your thing exactly

00:23:22,490 --> 00:23:25,520
so that's one of the other things that

00:23:24,169 --> 00:23:26,690
we would think that is that is

00:23:25,520 --> 00:23:33,080
definitely something but it's a lot more

00:23:26,690 --> 00:23:36,559
expensive work sure sure you could do

00:23:33,080 --> 00:23:38,540
that do this part so see if that like

00:23:36,559 --> 00:23:40,640
our implementation is basically a lot

00:23:38,540 --> 00:23:41,750
closer into a service behavior because

00:23:40,640 --> 00:23:44,120
it actually brings up the application

00:23:41,750 --> 00:23:47,059
and then it manages the life cycle of

00:23:44,120 --> 00:23:49,070
the application so similar to tasks but

00:23:47,059 --> 00:23:50,480
basically benefiting more from the

00:23:49,070 --> 00:23:56,000
fundamentals of cloud foundry in that

00:23:50,480 --> 00:23:58,460
sense it seems like the largest time

00:23:56,000 --> 00:24:00,470
consuming process as you mentioned is

00:23:58,460 --> 00:24:02,480
starting up that that container and all

00:24:00,470 --> 00:24:04,669
the infrastructure efforts that needs to

00:24:02,480 --> 00:24:08,450
go along with that did you consider the

00:24:04,669 --> 00:24:11,990
idea of simply having a maybe a existing

00:24:08,450 --> 00:24:13,610
generic Python container that was always

00:24:11,990 --> 00:24:15,740
running and then load

00:24:13,610 --> 00:24:17,660
Lord without those functions as modules

00:24:15,740 --> 00:24:19,880
on demand so we weren't spending that

00:24:17,660 --> 00:24:21,470
container startup time every year we go

00:24:19,880 --> 00:24:24,590
that's it that's a very good question

00:24:21,470 --> 00:24:26,180
yes so essentially there are a couple of

00:24:24,590 --> 00:24:27,890
issues with having one container and

00:24:26,180 --> 00:24:29,660
then loading code and that's one of the

00:24:27,890 --> 00:24:31,430
primary ones insecurity because

00:24:29,660 --> 00:24:33,290
containers provides a certain level of

00:24:31,430 --> 00:24:34,820
isolation and you want your processes to

00:24:33,290 --> 00:24:36,200
be isolated from one another you want

00:24:34,820 --> 00:24:38,480
you don't want one process to go and

00:24:36,200 --> 00:24:40,130
ruin the other processes and data for

00:24:38,480 --> 00:24:41,480
example if it's running there or during

00:24:40,130 --> 00:24:43,730
the state of that process for whatever

00:24:41,480 --> 00:24:45,320
reason so you want the isolation that

00:24:43,730 --> 00:24:46,670
you get from containers when it comes to

00:24:45,320 --> 00:24:47,990
service and that's one of the primary

00:24:46,670 --> 00:24:49,130
reasons you need to actually have

00:24:47,990 --> 00:24:51,080
separate containers for separate

00:24:49,130 --> 00:24:52,490
platforms but yes if you want to forget

00:24:51,080 --> 00:24:54,110
about all the security and all the good

00:24:52,490 --> 00:24:57,800
things that containers provide that's

00:24:54,110 --> 00:25:04,640
potentially solution yeah any other

00:24:57,800 --> 00:25:08,819
questions all right thank you very much

00:25:04,640 --> 00:25:08,819

YouTube URL: https://www.youtube.com/watch?v=5JAINnKXI_8


