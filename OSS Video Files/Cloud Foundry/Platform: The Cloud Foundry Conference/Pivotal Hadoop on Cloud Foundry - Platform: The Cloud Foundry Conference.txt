Title: Pivotal Hadoop on Cloud Foundry - Platform: The Cloud Foundry Conference
Publication date: 2013-11-22
Playlist: Platform: The Cloud Foundry Conference
Description: 
	Pivotal Hadoop on Cloud Foundry
Ashwin Kumar, Principal Software Engineer, Pivotal
Platform: The Cloud Foundry Conference (http://www.platformcf.com) September 8-9, 2013
Captions: 
	00:00:00,870 --> 00:00:03,979
[Music]

00:00:07,099 --> 00:00:12,059
hey guys my name is Ashton Kumar I'm a

00:00:09,990 --> 00:00:13,860
software engineer at pivotal and I'm

00:00:12,059 --> 00:00:17,460
going to talk about pivotal Hadoop as a

00:00:13,860 --> 00:00:19,320
cloud foundry service for those of you

00:00:17,460 --> 00:00:21,750
who don't know Hadoop is an open source

00:00:19,320 --> 00:00:24,570
Apache project for distributed computing

00:00:21,750 --> 00:00:27,630
at large scale it's made up of a couple

00:00:24,570 --> 00:00:30,179
of components HDFS which is the Hadoop

00:00:27,630 --> 00:00:33,290
file system it's designed for large io

00:00:30,179 --> 00:00:36,180
meaning block sizes of tens of megabytes

00:00:33,290 --> 00:00:38,730
where the typical access pattern is you

00:00:36,180 --> 00:00:41,190
write data once and read it many times

00:00:38,730 --> 00:00:44,370
over the course of long-running

00:00:41,190 --> 00:00:45,989
analytics there's also yarn which is a

00:00:44,370 --> 00:00:48,390
cluster and resource scheduling

00:00:45,989 --> 00:00:49,860
framework and probably the component

00:00:48,390 --> 00:00:52,590
you're you're most familiar with is

00:00:49,860 --> 00:00:55,350
MapReduce which is the popular paradigm

00:00:52,590 --> 00:00:58,320
for doing batch processing with extreme

00:00:55,350 --> 00:01:00,600
parallelism so pivotal has its own

00:00:58,320 --> 00:01:02,100
Hadoop distribution it adds the

00:01:00,600 --> 00:01:04,830
enterprise features you might expect

00:01:02,100 --> 00:01:06,630
like a GUI for monitoring the system to

00:01:04,830 --> 00:01:09,119
see which nodes of the system have

00:01:06,630 --> 00:01:11,700
failed starting and stopping services

00:01:09,119 --> 00:01:13,650
things like that we also added a bulk

00:01:11,700 --> 00:01:16,290
data loaders that you could get data in

00:01:13,650 --> 00:01:18,020
and out efficiently and also noteworthy

00:01:16,290 --> 00:01:20,070
here is that we've added the

00:01:18,020 --> 00:01:22,560
virtualization extensions that came out

00:01:20,070 --> 00:01:24,659
of VMware so that when HDFS is

00:01:22,560 --> 00:01:25,890
replicating your blocks across different

00:01:24,659 --> 00:01:26,939
physical nodes for the sake of

00:01:25,890 --> 00:01:28,799
reliability

00:01:26,939 --> 00:01:30,689
we're also cognizant of the fact that

00:01:28,799 --> 00:01:33,990
two VMs might actually be on the same

00:01:30,689 --> 00:01:36,290
physical host on top of this we've also

00:01:33,990 --> 00:01:38,820
added the advanced database services

00:01:36,290 --> 00:01:41,400
which the main component of which is

00:01:38,820 --> 00:01:43,500
hawke this is our greenplum database

00:01:41,400 --> 00:01:45,540
technology which we've matured over the

00:01:43,500 --> 00:01:48,540
last 10 years for data warehousing and

00:01:45,540 --> 00:01:51,540
very complex analytics redesigned and

00:01:48,540 --> 00:01:53,909
re-architected to work natively on HDFS

00:01:51,540 --> 00:01:56,040
it is at the moment the world's fastest

00:01:53,909 --> 00:01:58,320
sequel on a doob distribution and what's

00:01:56,040 --> 00:02:00,960
key there is that it's pure sequel it is

00:01:58,320 --> 00:02:02,820
the sequel 92 standard which means you

00:02:00,960 --> 00:02:04,979
get window functions you get any type of

00:02:02,820 --> 00:02:08,069
join that you like and user-defined

00:02:04,979 --> 00:02:10,560
functions are coming shortly in addition

00:02:08,069 --> 00:02:12,750
to that we've also got anti sequel

00:02:10,560 --> 00:02:13,710
transaction support which is something

00:02:12,750 --> 00:02:18,060
that is

00:02:13,710 --> 00:02:20,190
difficult to implement on HDFS so in

00:02:18,060 --> 00:02:23,070
talking about pivotal Hadoop for Cloud

00:02:20,190 --> 00:02:24,810
Foundry we firmly believe that these

00:02:23,070 --> 00:02:26,430
Cloud Foundry applications are going to

00:02:24,810 --> 00:02:28,920
become increasingly data intensive

00:02:26,430 --> 00:02:30,660
rather than needing a single node

00:02:28,920 --> 00:02:33,750
Postgres instance you're going to need

00:02:30,660 --> 00:02:35,970
the the capabilities of a vast Hadoop

00:02:33,750 --> 00:02:37,380
cluster and so the kinds of use cases

00:02:35,970 --> 00:02:39,500
that we're starting to think about are

00:02:37,380 --> 00:02:42,270
being able to Park on structured data on

00:02:39,500 --> 00:02:45,330
HDFS be able to then analyze that data

00:02:42,270 --> 00:02:47,310
with MapReduce and then use Hauk if

00:02:45,330 --> 00:02:49,170
you'd like to do very deep and complex

00:02:47,310 --> 00:02:51,390
analytics using machine learning

00:02:49,170 --> 00:02:54,510
algorithms to do fraud detection or

00:02:51,390 --> 00:02:56,180
regression analysis and so this this is

00:02:54,510 --> 00:02:58,530
really the value proposition for

00:02:56,180 --> 00:03:01,740
application developers of these very

00:02:58,530 --> 00:03:03,600
data-intensive applications but how

00:03:01,740 --> 00:03:05,970
exactly are we going to do this you know

00:03:03,600 --> 00:03:07,290
as users of Cloud Foundry you might be

00:03:05,970 --> 00:03:10,110
familiar with the notion that Cloud

00:03:07,290 --> 00:03:13,500
Foundry is open fundamentally but that

00:03:10,110 --> 00:03:15,060
might be in particular in regards to how

00:03:13,500 --> 00:03:18,570
Cloud Foundry is portable across

00:03:15,060 --> 00:03:20,340
different infrastructures it's I think

00:03:18,570 --> 00:03:22,170
that openness is a bit more fundamental

00:03:20,340 --> 00:03:25,050
than that in that Cloud Foundry is

00:03:22,170 --> 00:03:27,390
fundamentally extensible itself and so

00:03:25,050 --> 00:03:30,780
what we intend to do is to make Hadoop

00:03:27,390 --> 00:03:32,640
run as a service in Cloud Foundry so

00:03:30,780 --> 00:03:35,820
that the application developers building

00:03:32,640 --> 00:03:38,310
on top are able to leverage more value

00:03:35,820 --> 00:03:40,290
from the paths just so you know just the

00:03:38,310 --> 00:03:42,870
way that you get a domain name and get

00:03:40,290 --> 00:03:44,970
your App hosted on the pads and get

00:03:42,870 --> 00:03:47,220
single node services like Redis and

00:03:44,970 --> 00:03:50,970
Postgres you'll now get the distributed

00:03:47,220 --> 00:03:54,330
computing services of Hadoop at the core

00:03:50,970 --> 00:03:55,860
of this extensibility is a communication

00:03:54,330 --> 00:03:59,160
that occurs between the cloud controller

00:03:55,860 --> 00:04:01,200
in Cloud Foundry and the service broker

00:03:59,160 --> 00:04:04,320
that is responsible for negotiating on

00:04:01,200 --> 00:04:05,790
behalf of of the external service this

00:04:04,320 --> 00:04:08,090
communication is responsible for a

00:04:05,790 --> 00:04:11,010
couple of things catalog management

00:04:08,090 --> 00:04:12,570
provisioning and binding catalog

00:04:11,010 --> 00:04:14,550
management is the act of the service

00:04:12,570 --> 00:04:17,400
broker telling the cloud controller what

00:04:14,550 --> 00:04:18,870
service am i providing to the paths to

00:04:17,400 --> 00:04:21,359
the application developers that are

00:04:18,870 --> 00:04:22,830
going to use the paths provisioning is

00:04:21,359 --> 00:04:25,380
the act of actually setting aside

00:04:22,830 --> 00:04:25,980
resources on the service so that apps

00:04:25,380 --> 00:04:28,590
can act

00:04:25,980 --> 00:04:32,520
they consume the service binding is the

00:04:28,590 --> 00:04:34,230
ensuing act where applications are given

00:04:32,520 --> 00:04:36,450
the authorization and the credentials

00:04:34,230 --> 00:04:38,160
they need to actually use that service

00:04:36,450 --> 00:04:40,980
those those resources that we've set

00:04:38,160 --> 00:04:43,620
aside what's what's key to note here is

00:04:40,980 --> 00:04:46,230
that provisioning and binding they're

00:04:43,620 --> 00:04:48,390
all entirely service defined which means

00:04:46,230 --> 00:04:51,360
it's up to the service to declare what

00:04:48,390 --> 00:04:54,420
it means to provision Hadoop or my

00:04:51,360 --> 00:04:56,100
sequel or Postgres or Redis and for a

00:04:54,420 --> 00:04:58,500
service that's quite as complicated as

00:04:56,100 --> 00:04:59,850
Hadoop that's that's critical because

00:04:58,500 --> 00:05:02,970
there's so many different ways of

00:04:59,850 --> 00:05:04,500
thinking of Hadoop as a service I'd like

00:05:02,970 --> 00:05:06,330
to start with what's probably the most

00:05:04,500 --> 00:05:08,430
straightforward most accessible way of

00:05:06,330 --> 00:05:11,880
thinking of Hadoop as a service and that

00:05:08,430 --> 00:05:15,450
is to think about a shared static Hadoop

00:05:11,880 --> 00:05:17,520
cluster that is bosh deployed along with

00:05:15,450 --> 00:05:20,580
the service broker adjacent to whatever

00:05:17,520 --> 00:05:22,590
Cloud Foundry deployment that you have

00:05:20,580 --> 00:05:25,920
on whatever infrastructure you've chosen

00:05:22,590 --> 00:05:27,720
in this model you'll have an HDFS yarn

00:05:25,920 --> 00:05:29,940
hoc cluster along with many of the other

00:05:27,720 --> 00:05:32,070
components and it would be the service

00:05:29,940 --> 00:05:34,700
brokers responsibility to negotiate on

00:05:32,070 --> 00:05:37,290
behalf of that shared static cluster

00:05:34,700 --> 00:05:39,330
when the provision request is received

00:05:37,290 --> 00:05:41,040
by the service broker it will merely

00:05:39,330 --> 00:05:44,250
propagate that on to the various sub

00:05:41,040 --> 00:05:46,290
components of the Hadoop cluster the

00:05:44,250 --> 00:05:49,110
various sub components will then reserve

00:05:46,290 --> 00:05:52,050
space and capacity for that provision

00:05:49,110 --> 00:05:54,900
requests so in the case of HDFS this is

00:05:52,050 --> 00:05:57,090
a matter of setting aside space 10

00:05:54,900 --> 00:05:58,590
terabytes of storage perhaps in the case

00:05:57,090 --> 00:06:00,300
of hawk just as it might be with

00:05:58,590 --> 00:06:03,420
postgrads it's a matter of creating a

00:06:00,300 --> 00:06:05,640
database the act of binding that occurs

00:06:03,420 --> 00:06:08,280
next to actually get apps access to what

00:06:05,640 --> 00:06:09,750
you set aside on HDFS is a matter of

00:06:08,280 --> 00:06:12,180
getting the right permissions to the

00:06:09,750 --> 00:06:14,640
filesystem on Hawk it's a matter of

00:06:12,180 --> 00:06:16,110
creating the users in the hawk database

00:06:14,640 --> 00:06:19,260
so that they can start manipulating

00:06:16,110 --> 00:06:21,840
sequel objects I think that's really the

00:06:19,260 --> 00:06:23,280
the first incarnation of Hadoop as a

00:06:21,840 --> 00:06:25,590
service it's really I think the most

00:06:23,280 --> 00:06:27,480
accessible but there's a very full

00:06:25,590 --> 00:06:28,980
roadmap here and I'd like to walk you

00:06:27,480 --> 00:06:32,550
through some of these in a bit more

00:06:28,980 --> 00:06:35,430
detail so we do sell hadoop

00:06:32,550 --> 00:06:37,140
independently of course and you know for

00:06:35,430 --> 00:06:39,060
those customers who have bought hadoop

00:06:37,140 --> 00:06:39,910
they're going to have to make a decision

00:06:39,060 --> 00:06:42,040
between

00:06:39,910 --> 00:06:44,650
running it on bare metal or running it

00:06:42,040 --> 00:06:46,240
as a Bosch deployed Hadoop cluster on

00:06:44,650 --> 00:06:48,280
potentially virtualized infrastructure

00:06:46,240 --> 00:06:50,710
that's going to be a very subtle

00:06:48,280 --> 00:06:52,930
trade-off between some performance

00:06:50,710 --> 00:06:54,490
versus the manageability and the unified

00:06:52,930 --> 00:06:56,140
experience that you're going to get with

00:06:54,490 --> 00:06:57,940
Bosch with the rest of the stack that

00:06:56,140 --> 00:07:00,670
you get from us including Cloud Foundry

00:06:57,940 --> 00:07:03,190
and so we know we need to cater to both

00:07:00,670 --> 00:07:05,200
of those use cases and so what we're

00:07:03,190 --> 00:07:07,810
going to look at next is being able to

00:07:05,200 --> 00:07:10,390
take a dupe lustre on the hundred nodes

00:07:07,810 --> 00:07:12,190
of bare metal commodity hardware that

00:07:10,390 --> 00:07:14,050
you've bought that you've invested in

00:07:12,190 --> 00:07:16,000
and be able to use that same Hadoop

00:07:14,050 --> 00:07:16,720
cluster to work with your Cloud Foundry

00:07:16,000 --> 00:07:18,730
instance

00:07:16,720 --> 00:07:21,490
and so again this would be a shared

00:07:18,730 --> 00:07:24,270
cluster that simply that the nodes of

00:07:21,490 --> 00:07:27,490
your Hadoop cluster are on bare metal

00:07:24,270 --> 00:07:29,560
beyond this we recognize that once

00:07:27,490 --> 00:07:32,380
you've got bare metal clusters you've

00:07:29,560 --> 00:07:33,910
got some Bosch deployed clusters we're

00:07:32,380 --> 00:07:36,550
going to need to be able to negotiate

00:07:33,910 --> 00:07:38,380
among multiple clusters and what that

00:07:36,550 --> 00:07:40,120
means is some of your clusters might be

00:07:38,380 --> 00:07:42,250
at ninety percent utilization some of

00:07:40,120 --> 00:07:44,260
them might be lower utilization we need

00:07:42,250 --> 00:07:46,810
to have the placement algorithms that

00:07:44,260 --> 00:07:48,790
allow the service broker to decide where

00:07:46,810 --> 00:07:50,800
should I put that next provision request

00:07:48,790 --> 00:07:53,650
where's the best place to allocate that

00:07:50,800 --> 00:07:55,540
ten terabytes of storage which cluster

00:07:53,650 --> 00:07:57,340
isn't utilized with respect to compute

00:07:55,540 --> 00:08:01,000
capacity and where do we put that

00:07:57,340 --> 00:08:02,920
request moving beyond this I think when

00:08:01,000 --> 00:08:05,530
you really start talking about deep

00:08:02,920 --> 00:08:07,930
complex analytics it's commonly the case

00:08:05,530 --> 00:08:09,450
that some of those queries you may not

00:08:07,930 --> 00:08:11,590
know exactly what the resource

00:08:09,450 --> 00:08:13,750
requirements of those computations are

00:08:11,590 --> 00:08:15,790
ahead of time that's very much the case

00:08:13,750 --> 00:08:18,370
when you talk about a query with ten

00:08:15,790 --> 00:08:19,810
joins and user defined functions where

00:08:18,370 --> 00:08:21,570
you don't know quite exactly what those

00:08:19,810 --> 00:08:25,120
functions are going to do ahead of time

00:08:21,570 --> 00:08:26,890
so as to not disrupt other workloads

00:08:25,120 --> 00:08:28,660
that might be on the cluster it becomes

00:08:26,890 --> 00:08:30,790
important for these use cases to have

00:08:28,660 --> 00:08:33,000
exclusive clusters and so our service

00:08:30,790 --> 00:08:36,970
broker is going to need to be able to

00:08:33,000 --> 00:08:39,100
provision and bind a series of apps to

00:08:36,970 --> 00:08:42,550
one cluster that is used fully an

00:08:39,100 --> 00:08:45,070
entirety by that provision request by

00:08:42,550 --> 00:08:46,540
that set of applications and so we know

00:08:45,070 --> 00:08:49,990
that's that's a use case that will come

00:08:46,540 --> 00:08:52,889
up and from here it's really a natural

00:08:49,990 --> 00:08:55,559
logical jump to wash in the cluster of

00:08:52,889 --> 00:08:58,559
provision beforehand why can't we at the

00:08:55,559 --> 00:09:01,439
time you run cf create service why can't

00:08:58,559 --> 00:09:02,999
we find the capacity designate one of

00:09:01,439 --> 00:09:04,949
them as the name though designate one of

00:09:02,999 --> 00:09:06,989
them as the hoc master designate the

00:09:04,949 --> 00:09:08,639
remaining hundred as Hadoop slaves and

00:09:06,989 --> 00:09:11,160
build the cluster right at that moment

00:09:08,639 --> 00:09:14,249
and that's that's what dynamic

00:09:11,160 --> 00:09:17,939
provisioning will look like and and from

00:09:14,249 --> 00:09:19,230
here as application developers you don't

00:09:17,939 --> 00:09:21,239
really want to care about how many

00:09:19,230 --> 00:09:23,489
Hadoop nodes you have you're going to

00:09:21,239 --> 00:09:26,129
run a MapReduce job and you want that

00:09:23,489 --> 00:09:27,660
paradigm to exist you're going to want a

00:09:26,129 --> 00:09:29,309
quality of service that can describe

00:09:27,660 --> 00:09:32,009
when you're going to get your data back

00:09:29,309 --> 00:09:34,589
and so from here it's natural to think

00:09:32,009 --> 00:09:37,109
well why can't we create the cluster run

00:09:34,589 --> 00:09:40,109
the MapReduce job teardown the MapReduce

00:09:37,109 --> 00:09:42,929
cluster and return the result back to

00:09:40,109 --> 00:09:45,119
the user and that sort of flexibility

00:09:42,929 --> 00:09:47,369
that sort of elasticity is something

00:09:45,119 --> 00:09:49,439
that we're going to pursue and something

00:09:47,369 --> 00:09:51,499
that takes us closer to what Amazon

00:09:49,439 --> 00:09:54,119
Elastic MapReduce will have and that's

00:09:51,499 --> 00:09:55,350
really in the the recurring theme of

00:09:54,119 --> 00:09:57,629
what you've heard over the last day or

00:09:55,350 --> 00:09:59,399
so not not wanting to care about what

00:09:57,629 --> 00:10:00,989
the infrastructure is running on not

00:09:59,399 --> 00:10:03,089
wanting to care about what Hadoop

00:10:00,989 --> 00:10:05,309
cluster you're using we're aware that as

00:10:03,089 --> 00:10:06,720
application developers you're going to

00:10:05,309 --> 00:10:08,309
want something higher-level in the stack

00:10:06,720 --> 00:10:10,649
and it's not necessarily Hadoop as a

00:10:08,309 --> 00:10:13,679
service it's going to be analytics as a

00:10:10,649 --> 00:10:15,899
service so to wrap this up what we're

00:10:13,679 --> 00:10:18,029
delivering later this year is a pivotal

00:10:15,899 --> 00:10:20,160
Hadoop cluster that is Bosch deployable

00:10:18,029 --> 00:10:22,980
and will be exposed as a cloud foundry

00:10:20,160 --> 00:10:26,040
service this will be a shared static

00:10:22,980 --> 00:10:29,100
HDFS cluster with Hawk and hive and

00:10:26,040 --> 00:10:30,749
HBase and when you provision you're

00:10:29,100 --> 00:10:33,179
going to get shared capacity on that

00:10:30,749 --> 00:10:35,519
cluster and we firmly believe that this

00:10:33,179 --> 00:10:36,809
in conjunction with Cloud Foundry puts

00:10:35,519 --> 00:10:39,059
us in a position to seize the

00:10:36,809 --> 00:10:42,389
opportunity of this new wave of very

00:10:39,059 --> 00:10:44,399
data-intensive apps and again to

00:10:42,389 --> 00:10:46,289
reiterate all of this coping with all of

00:10:44,399 --> 00:10:48,509
that complexity of what a Hadoop service

00:10:46,289 --> 00:10:50,879
means that's only possible really

00:10:48,509 --> 00:10:55,579
because of the extensibility of Cloud

00:10:50,879 --> 00:10:55,579
Foundry that's it thank you

00:11:02,900 --> 00:11:09,190

YouTube URL: https://www.youtube.com/watch?v=jTyoPHxMpU0


