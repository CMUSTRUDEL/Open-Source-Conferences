Title: IBM Dashboard Operational Metrics - Platform: The Cloud Foundry Conference
Publication date: 2013-11-22
Playlist: Platform: The Cloud Foundry Conference
Description: 
	IBM Dashboard Operational Metrics (Lightning Talk)
Daniel Krook, Senior Certified IT Specialist, Advanced Cloud Solutions, IBM
Platform: The Cloud Foundry Conference (http://www.platformcf.com) September 8-9, 2013


This lightning talk covers the key operational metrics we monitor to prevent problems and keep Cloud Foundry running smoothly. Sharing our notes with the community, we hope to validate our approach, learn about other data points, and spark efforts to include additional monitoring hooks in future releases.
Captions: 
	00:00:00,890 --> 00:00:03,999
[Music]

00:00:06,259 --> 00:00:10,500
so I work for IBM and we have an

00:00:09,030 --> 00:00:12,420
internal dashboard that we use for

00:00:10,500 --> 00:00:14,160
monitoring Cloud Foundry that we find

00:00:12,420 --> 00:00:15,509
very useful and we plan to commit back

00:00:14,160 --> 00:00:19,560
to the community pretty soon so I'm

00:00:15,509 --> 00:00:21,330
gonna take it through that so we run to

00:00:19,560 --> 00:00:23,600
large clusters we've got classic cluster

00:00:21,330 --> 00:00:26,580
we've got an energy cluster fairly large

00:00:23,600 --> 00:00:28,260
several dozen they're not small VMs or

00:00:26,580 --> 00:00:30,689
the jelly on the large side and that

00:00:28,260 --> 00:00:32,669
doesn't include any of the other devops

00:00:30,689 --> 00:00:34,410
clusters we have for development staging

00:00:32,669 --> 00:00:37,559
tests things like that and that's all

00:00:34,410 --> 00:00:40,290
running on about 50 OpenStack nodes it's

00:00:37,559 --> 00:00:41,670
about two iData flex racks so in the

00:00:40,290 --> 00:00:44,180
past year running these these two

00:00:41,670 --> 00:00:46,170
systems we've learned some great lessons

00:00:44,180 --> 00:00:48,120
so we'll be able to keep Cloud Foundry

00:00:46,170 --> 00:00:50,640
running pretty smoothly well fairly few

00:00:48,120 --> 00:00:52,230
outages we've been able to discover

00:00:50,640 --> 00:00:56,280
problems before they happen and fix them

00:00:52,230 --> 00:00:57,750
and resolve them when they do so what

00:00:56,280 --> 00:01:00,030
I'm gonna do is show you those key data

00:00:57,750 --> 00:01:01,500
points that we look for and show how the

00:01:00,030 --> 00:01:04,530
dashboard helps you monitor that data

00:01:01,500 --> 00:01:06,659
and hopefully share some ideas start a

00:01:04,530 --> 00:01:08,250
discussion on where we can go next to

00:01:06,659 --> 00:01:10,530
find better data in the next releases

00:01:08,250 --> 00:01:12,659
and finally throughout this conference

00:01:10,530 --> 00:01:14,159
engage and start some discussions about

00:01:12,659 --> 00:01:16,680
how we can make this better how we can

00:01:14,159 --> 00:01:20,250
make this data very visible to enable

00:01:16,680 --> 00:01:21,689
the the rapid DevOps behind pod foundry

00:01:20,250 --> 00:01:24,090
so we're looking to get better at this

00:01:21,689 --> 00:01:26,939
and we're hoping the community benefits

00:01:24,090 --> 00:01:27,360
from what we've learned as well so the

00:01:26,939 --> 00:01:28,950
key data

00:01:27,360 --> 00:01:30,240
we're gentle looking for the things that

00:01:28,950 --> 00:01:32,310
can help us predict the problems before

00:01:30,240 --> 00:01:34,290
they happen and things that we can track

00:01:32,310 --> 00:01:38,280
over time so we can know when to start

00:01:34,290 --> 00:01:40,409
planning to grow and there's three main

00:01:38,280 --> 00:01:42,119
components behind that at the past layer

00:01:40,409 --> 00:01:43,229
we're looking at so the general health

00:01:42,119 --> 00:01:46,110
of all the components whether they're up

00:01:43,229 --> 00:01:48,000
and down the da's and the apps on top of

00:01:46,110 --> 00:01:49,439
those and of course the database nodes

00:01:48,000 --> 00:01:50,729
and the services and this is all talking

00:01:49,439 --> 00:01:54,149
about the fabric itself not any

00:01:50,729 --> 00:01:56,939
brokerage services so why do we need

00:01:54,149 --> 00:01:59,640
those metrics you know the promise of

00:01:56,939 --> 00:02:02,520
cloud is to have continuously avail

00:01:59,640 --> 00:02:04,979
available services and respond to

00:02:02,520 --> 00:02:06,930
problems not react to them not the fix

00:02:04,979 --> 00:02:09,360
patch as in the earlier speaker talk

00:02:06,930 --> 00:02:11,280
about and help understand the system as

00:02:09,360 --> 00:02:14,720
best we can so we can deliver on the

00:02:11,280 --> 00:02:17,370
problems promises on automation

00:02:14,720 --> 00:02:19,200
so the data we're looking for we can

00:02:17,370 --> 00:02:21,450
generally find it Nats some of its in

00:02:19,200 --> 00:02:23,400
controller database but NASA's the

00:02:21,450 --> 00:02:24,740
message bus that helps us discover

00:02:23,400 --> 00:02:27,300
things that are on the system

00:02:24,740 --> 00:02:31,620
interrogate their endpoints and find out

00:02:27,300 --> 00:02:33,840
about their health so we monitor that

00:02:31,620 --> 00:02:36,030
data with our metrics dashboard our

00:02:33,840 --> 00:02:38,400
admin tool we call it I mean it gives

00:02:36,030 --> 00:02:41,160
five key features so there's the

00:02:38,400 --> 00:02:43,020
component health the DA's the service

00:02:41,160 --> 00:02:44,550
nodes things like that there's

00:02:43,020 --> 00:02:46,920
information on what they're consuming

00:02:44,550 --> 00:02:48,959
what's in the system where it's

00:02:46,920 --> 00:02:50,760
generally growing how it's growing and

00:02:48,959 --> 00:02:53,250
to make sure it work or it's doing we're

00:02:50,760 --> 00:02:54,870
expecting it to do and it gives us

00:02:53,250 --> 00:02:56,340
access to raw logs in there as well as

00:02:54,870 --> 00:02:59,190
email notifications when things do go

00:02:56,340 --> 00:03:00,750
wrong so here's basically our main

00:02:59,190 --> 00:03:03,840
summary you have this dashboard it's

00:03:00,750 --> 00:03:06,240
essentially gives us some running total

00:03:03,840 --> 00:03:08,130
to basically users apps and instances

00:03:06,240 --> 00:03:09,750
within the system and how those are

00:03:08,130 --> 00:03:12,209
growing over time so we had a large

00:03:09,750 --> 00:03:14,340
hackathon back in June since then on

00:03:12,209 --> 00:03:17,670
this particular cluster we've basically

00:03:14,340 --> 00:03:19,980
tripled growth since June we've got a

00:03:17,670 --> 00:03:21,870
list of DEA s in there and that's giving

00:03:19,980 --> 00:03:23,640
us all the information from Varzi most

00:03:21,870 --> 00:03:26,340
importantly on the right side the app

00:03:23,640 --> 00:03:28,530
memory shows us the reserved memory of

00:03:26,340 --> 00:03:31,620
applications on the da's versus the

00:03:28,530 --> 00:03:34,110
Mac's so we can scale we can drill in to

00:03:31,620 --> 00:03:35,430
particular DA's buying their general

00:03:34,110 --> 00:03:36,900
health and we can click through the

00:03:35,430 --> 00:03:38,400
applications that are on there for

00:03:36,900 --> 00:03:40,800
service nodes we can tell what our

00:03:38,400 --> 00:03:42,360
capacity is there how many people are

00:03:40,800 --> 00:03:44,970
using those services we can kind of sort

00:03:42,360 --> 00:03:46,530
and find what's most popular and again

00:03:44,970 --> 00:03:48,510
if we need to scale those we can you can

00:03:46,530 --> 00:03:49,230
see you've got six mongos here five

00:03:48,510 --> 00:03:51,299
MySQL's

00:03:49,230 --> 00:03:53,430
and they're pretty good shape right now

00:03:51,299 --> 00:03:56,190
drilling in you can see the actual

00:03:53,430 --> 00:03:58,769
capacities on those server nodes i've

00:03:56,190 --> 00:04:00,810
got user details you can find out who's

00:03:58,769 --> 00:04:03,030
using the system in this particular

00:04:00,810 --> 00:04:05,459
example about five down got someone

00:04:03,030 --> 00:04:08,730
using a lot of CPU we can drill into

00:04:05,459 --> 00:04:10,739
that user find out what they're doing

00:04:08,730 --> 00:04:12,860
applications same thing we can see what

00:04:10,739 --> 00:04:15,870
sort of consumption patterns there are

00:04:12,860 --> 00:04:17,190
we can drive into the details find out

00:04:15,870 --> 00:04:20,669
where the applications are running and

00:04:17,190 --> 00:04:23,039
URLs and then we have a consolidated

00:04:20,669 --> 00:04:25,320
list the logs on the system so for

00:04:23,039 --> 00:04:26,710
example this is co-located with the

00:04:25,320 --> 00:04:29,440
cloud controller and several other

00:04:26,710 --> 00:04:29,949
components so we can find look at the

00:04:29,440 --> 00:04:33,479
nginx

00:04:29,949 --> 00:04:33,479
error log right in there

00:04:33,550 --> 00:04:38,770
and again part of our post problem

00:04:37,509 --> 00:04:40,960
resolution that we occasionally have to

00:04:38,770 --> 00:04:44,289
do does send email notifications for us

00:04:40,960 --> 00:04:46,660
to go look at downed components okay so

00:04:44,289 --> 00:04:48,340
going forward we've got great stuff in

00:04:46,660 --> 00:04:50,289
there but we want to resolve gaps

00:04:48,340 --> 00:04:53,830
between what we're looking at an NG and

00:04:50,289 --> 00:04:57,310
classic versus what we're in doing an NG

00:04:53,830 --> 00:04:59,349
and we'd like to see if we can get that

00:04:57,310 --> 00:05:02,199
same data from the old system into the

00:04:59,349 --> 00:05:04,990
new my colleague Doug Davis actually put

00:05:02,199 --> 00:05:06,340
in a an issue against the cloud

00:05:04,990 --> 00:05:08,050
controller and ng to see if we can get

00:05:06,340 --> 00:05:09,490
some of that same data or find a

00:05:08,050 --> 00:05:11,620
different way to get it we also start to

00:05:09,490 --> 00:05:13,750
like to link the metrics that were

00:05:11,620 --> 00:05:16,150
statically looking at and link that into

00:05:13,750 --> 00:05:17,889
any sort of automation solving scaling

00:05:16,150 --> 00:05:20,380
doing things like that

00:05:17,889 --> 00:05:22,690
another example came from ball on our

00:05:20,380 --> 00:05:24,789
team when the leaders an argument with

00:05:22,690 --> 00:05:25,960
Cloud Foundry about preemptively helping

00:05:24,789 --> 00:05:27,789
out users that are seeing problems

00:05:25,960 --> 00:05:29,979
because we can look at the logs we can

00:05:27,789 --> 00:05:33,419
look at the health their app and go from

00:05:29,979 --> 00:05:36,280
there and all this in the hope to to

00:05:33,419 --> 00:05:38,229
heal system scale and I'll let it evolve

00:05:36,280 --> 00:05:40,599
as we can see what sort of patterns we

00:05:38,229 --> 00:05:43,330
have going forward we really want to

00:05:40,599 --> 00:05:44,830
expand the usage of this tool which is

00:05:43,330 --> 00:05:47,889
currently limited to admins at the

00:05:44,830 --> 00:05:51,039
moment to help other types of admins do

00:05:47,889 --> 00:05:54,130
more proactive things such as add things

00:05:51,039 --> 00:05:55,750
add das the cluster if need be or invoke

00:05:54,130 --> 00:05:59,080
any Jenkins Bosch depending on how you

00:05:55,750 --> 00:06:01,150
deploy cloud funds Cloud Foundry and

00:05:59,080 --> 00:06:03,210
possibly show different tiers of admin

00:06:01,150 --> 00:06:05,409
access it's all pretty flat right now

00:06:03,210 --> 00:06:06,610
and of course we always have people

00:06:05,409 --> 00:06:09,009
interested and want to know how we're

00:06:06,610 --> 00:06:10,780
doing is assist them up maybe something

00:06:09,009 --> 00:06:12,009
down over the weekend they want to see

00:06:10,780 --> 00:06:15,070
all right I pushed nap is there a

00:06:12,009 --> 00:06:17,949
problem the system a way to expose this

00:06:15,070 --> 00:06:21,020
data and make it available to different

00:06:17,949 --> 00:06:26,180
roles inside the system

00:06:21,020 --> 00:06:30,100
and of course thank you and of course

00:06:26,180 --> 00:06:30,100
the very long will hiring too

00:06:30,270 --> 00:06:33,459
[Applause]

00:06:36,750 --> 00:06:43,040

YouTube URL: https://www.youtube.com/watch?v=uEOyTGGsfu0


