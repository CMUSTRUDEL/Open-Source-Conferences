Title: The Tale of Two Runtimes: Kubernetes in Cloud Foundry - Konstantin Semenov, Pivotal
Publication date: 2018-04-21
Playlist: Cloud Foundry Summit NA 2018
Description: 
	The Tale of Two Runtimes: Kubernetes in Cloud Foundry - Konstantin Semenov, Pivotal

 The Container Runtime, CFCR, has been recently introduced as part of the Cloud Foundry Foundation. It leverages Kubernetes to run all sorts of workloads.

The core CFCR team has been busy making sure that CFCR runs on multiple platforms, provides high availability, follows the best security practises, and integrates with CFAR, previously known as the Elastic Runtime.

As part of this presentation, a demo will be presented showing how 12-factor apps running on Application Runtime can leverage 3-rd party Docker packaged software with persistence, and what will happen during an infrastructure failure.

Attend this talk if you're interested in learning the available features of CFCR, or if you may be wondering when to use it, and how to get the best of both Application Runtime and the Container Runtime by using them alongside each other. 

About Konstantin Semenov
Konstantin is a Principal Software Engineer at Pivotal. He spent most of 2017 with his colleagues from Pivotal, Google and vmWare building CFCR (a.k.a Kubo) â€“ an open source automated reliable, reproducible and highly available Kubernetes deployment. Before that he was leading and developing software products across a wide range of industries and technologies: from desktop database applications through telecom servers and transactional banking systems to mobile applications and 3D model generators. As a speaker, he gave the first Kubo presentation together with his colleagues from Google at the CF Summit 2017 in Santa Clara, spoke about CFCR at DevOps Days in Galway, and has given talks at tech meetups in Dublin and Belfast.
Captions: 
	00:00:00,030 --> 00:00:06,029
and this presentation my name is

00:00:04,170 --> 00:00:08,280
Konstantin Simonov I'm the principal

00:00:06,029 --> 00:00:10,980
software engineer from pivotal based in

00:00:08,280 --> 00:00:13,950
Dublin and today I'll be talking about

00:00:10,980 --> 00:00:17,640
the application runtime in the container

00:00:13,950 --> 00:00:21,210
runtime not our part of the car family

00:00:17,640 --> 00:00:23,250
foundation I'm pretty much sure you've

00:00:21,210 --> 00:00:26,939
already seen this slide today a number

00:00:23,250 --> 00:00:29,609
of times but I'll have to repeat that

00:00:26,939 --> 00:00:32,910
you have to note the nearest exit sign

00:00:29,609 --> 00:00:34,890
and make sure that in an event of a fire

00:00:32,910 --> 00:00:37,649
alarm or other emergency calmly exit to

00:00:34,890 --> 00:00:39,899
the public concourse area emergency

00:00:37,649 --> 00:00:41,520
exits devil's leading to the outside of

00:00:39,899 --> 00:00:43,710
this facility are located along the

00:00:41,520 --> 00:00:45,030
public concourse and for your safety in

00:00:43,710 --> 00:00:50,640
an emergency please follow the

00:00:45,030 --> 00:00:53,570
directions of the public safety staff so

00:00:50,640 --> 00:00:56,129
the tale of two runtimes

00:00:53,570 --> 00:00:58,260
today we're going to briefly touch on

00:00:56,129 --> 00:01:01,260
the software deployment and delivery

00:00:58,260 --> 00:01:03,750
history the inception of cough foundry

00:01:01,260 --> 00:01:06,900
and the power of Bosch the use cases of

00:01:03,750 --> 00:01:09,299
kubernetes and at the end of the talk I

00:01:06,900 --> 00:01:12,600
will give a demo that will show how you

00:01:09,299 --> 00:01:15,240
can use both coronary applications

00:01:12,600 --> 00:01:20,130
runtime together with workloads deploy

00:01:15,240 --> 00:01:25,110
to the container runtime and we'll have

00:01:20,130 --> 00:01:31,259
a small space for questions so brief

00:01:25,110 --> 00:01:33,270
history what is the main goal of those

00:01:31,259 --> 00:01:36,810
platforms of the technologies were

00:01:33,270 --> 00:01:40,520
working on businesses are getting very

00:01:36,810 --> 00:01:44,130
very competitive and they have to

00:01:40,520 --> 00:01:44,990
deliver and deliver fast and deliver

00:01:44,130 --> 00:01:48,270
reliably

00:01:44,990 --> 00:01:52,520
so what was going on in the industry in

00:01:48,270 --> 00:01:56,450
that quest for the speed and reliability

00:01:52,520 --> 00:01:59,490
back in the 90s you had to physically

00:01:56,450 --> 00:02:01,710
provision a server to deploy your

00:01:59,490 --> 00:02:03,119
workloads so you would probably if you

00:02:01,710 --> 00:02:05,399
worked in the corporation you would make

00:02:03,119 --> 00:02:09,119
a request wait for that to get cleared

00:02:05,399 --> 00:02:11,069
signed off delivered then handed off to

00:02:09,119 --> 00:02:13,379
the operations department which would be

00:02:11,069 --> 00:02:13,800
completely separate those people would

00:02:13,379 --> 00:02:16,170
in

00:02:13,800 --> 00:02:18,060
stole the server in the rack configure

00:02:16,170 --> 00:02:20,340
it with the operating system and all the

00:02:18,060 --> 00:02:24,120
whatnots then received the package from

00:02:20,340 --> 00:02:26,330
you and try to run it maybe fail then

00:02:24,120 --> 00:02:30,620
you will provide them with an update and

00:02:26,330 --> 00:02:33,690
whenever the queue reached your update

00:02:30,620 --> 00:02:36,560
they will attempt another one and

00:02:33,690 --> 00:02:39,180
another one and so on and so forth

00:02:36,560 --> 00:02:43,350
needless to say that it doesn't feel

00:02:39,180 --> 00:02:46,740
very fast or reliable become the

00:02:43,350 --> 00:02:49,380
automation was fairly complicated and

00:02:46,740 --> 00:02:51,150
you had had to have physical access if

00:02:49,380 --> 00:02:52,790
you want to change a configuration in

00:02:51,150 --> 00:02:57,710
your machine

00:02:52,790 --> 00:03:00,660
fast forward to mm when VM were

00:02:57,710 --> 00:03:04,010
successfully virtualized commodity

00:03:00,660 --> 00:03:06,330
hardware such as Intel servers

00:03:04,010 --> 00:03:09,180
previously the virtualization technology

00:03:06,330 --> 00:03:11,880
has been used by IBM since the 60s in

00:03:09,180 --> 00:03:16,590
the mainframes but never before was used

00:03:11,880 --> 00:03:23,760
for commodity hard work that's been a

00:03:16,590 --> 00:03:27,930
big boom in of operations industry the

00:03:23,760 --> 00:03:30,239
operators life significantly got

00:03:27,930 --> 00:03:32,700
simplified they still had to know the

00:03:30,239 --> 00:03:34,530
ins and outs of all the hardware but a

00:03:32,700 --> 00:03:37,550
very job was just to plug it into the

00:03:34,530 --> 00:03:42,570
hypervisor and then anybody could

00:03:37,550 --> 00:03:45,360
operate this remotely so your hardware

00:03:42,570 --> 00:03:47,489
maintenance can be seamless because of

00:03:45,360 --> 00:03:49,950
different technologies to migrate VMs so

00:03:47,489 --> 00:03:52,920
you could migrate them before you turned

00:03:49,950 --> 00:03:55,709
off the machines for maintenance and the

00:03:52,920 --> 00:03:58,410
automation has become easier and spun

00:03:55,709 --> 00:04:01,680
off the whole DevOps movement slowly but

00:03:58,410 --> 00:04:03,090
surely but the boot time for every

00:04:01,680 --> 00:04:06,360
machine is still slow because you have

00:04:03,090 --> 00:04:09,140
to put the whole operating system every

00:04:06,360 --> 00:04:12,630
time you turn the Machine off

00:04:09,140 --> 00:04:15,239
in the meantime Google was experimenting

00:04:12,630 --> 00:04:20,310
with different technologies that were

00:04:15,239 --> 00:04:24,690
working on bare metal that was Borg they

00:04:20,310 --> 00:04:27,139
launched borg in around 2005 it worked

00:04:24,690 --> 00:04:29,509
with massively huge

00:04:27,139 --> 00:04:33,169
workloads a single-board cluster could

00:04:29,509 --> 00:04:36,199
easily handle 10,000 machines or even

00:04:33,169 --> 00:04:38,060
more and it acted is a learning ground

00:04:36,199 --> 00:04:39,919
for cluster orchestration and scheduling

00:04:38,060 --> 00:04:42,590
evolution in the future

00:04:39,919 --> 00:04:45,199
and it's really amazing and it still

00:04:42,590 --> 00:04:48,229
evolves and it's still being used by

00:04:45,199 --> 00:04:51,860
Google today well let's get back to

00:04:48,229 --> 00:05:00,460
virtual machines and zoom in to the

00:04:51,860 --> 00:05:05,139
future by 2010 thanks to docker

00:05:00,460 --> 00:05:05,139
containers became more and more popular

00:05:05,889 --> 00:05:11,509
previously containers have also been

00:05:09,229 --> 00:05:14,569
known or similar technologies but they

00:05:11,509 --> 00:05:19,240
were very hard to run and to maintain

00:05:14,569 --> 00:05:22,810
and to create and they were not so easy

00:05:19,240 --> 00:05:26,300
so doc has simplified that greatly and

00:05:22,810 --> 00:05:28,370
in addition to that docker hub that is a

00:05:26,300 --> 00:05:32,120
sharing ground for your target files

00:05:28,370 --> 00:05:36,080
that you can build on top of also give

00:05:32,120 --> 00:05:37,400
the movement a huge boost your booting

00:05:36,080 --> 00:05:39,379
time for a container is just a few

00:05:37,400 --> 00:05:42,199
seconds because you're sharing the

00:05:39,379 --> 00:05:44,659
kernel now you're sharing the operating

00:05:42,199 --> 00:05:47,110
system resources all you have to run is

00:05:44,659 --> 00:05:50,440
the software that you're running on top

00:05:47,110 --> 00:05:53,360
it's one more layer of abstraction

00:05:50,440 --> 00:05:57,919
making it even easier for developers to

00:05:53,360 --> 00:06:01,490
create the containers that they run

00:05:57,919 --> 00:06:03,710
their workloads on and by default the

00:06:01,490 --> 00:06:07,580
virtual machine that is being shared

00:06:03,710 --> 00:06:11,479
between containers is secure but there

00:06:07,580 --> 00:06:14,900
are always flags dangerous ones that you

00:06:11,479 --> 00:06:17,930
can use to override this and the focus

00:06:14,900 --> 00:06:23,560
of every container is typically on a

00:06:17,930 --> 00:06:23,560
single application so what that leads to

00:06:24,159 --> 00:06:32,479
in by the end of 2009 ish VMware started

00:06:31,099 --> 00:06:35,750
working on something called

00:06:32,479 --> 00:06:38,449
a VMware cloud application platform

00:06:35,750 --> 00:06:40,670
braying the application platforms

00:06:38,449 --> 00:06:45,080
available in public I asses on

00:06:40,670 --> 00:06:47,810
Ramis they were it was initially a

00:06:45,080 --> 00:06:49,490
proprietary VMware project but it was

00:06:47,810 --> 00:06:51,110
eventually handed over to pivotal and

00:06:49,490 --> 00:06:55,480
pivotal publish that is open source and

00:06:51,110 --> 00:06:58,340
started with a cloud foundry foundation

00:06:55,480 --> 00:07:01,640
when we talk about cloud foundry or

00:06:58,340 --> 00:07:03,410
platforms in particular a platform is

00:07:01,640 --> 00:07:07,580
not something that is visible to the

00:07:03,410 --> 00:07:11,960
end-user if a platform is perfect the

00:07:07,580 --> 00:07:13,670
end-user will never actually notice so

00:07:11,960 --> 00:07:17,930
the interesting thing that I'm going to

00:07:13,670 --> 00:07:21,020
touch on today is what is running the

00:07:17,930 --> 00:07:25,370
platform it's the second level of

00:07:21,020 --> 00:07:26,750
invisibility a platform especially cloud

00:07:25,370 --> 00:07:29,390
foundry is pretty complicated it

00:07:26,750 --> 00:07:32,960
contains a large number of different

00:07:29,390 --> 00:07:35,290
modules configured to run together to

00:07:32,960 --> 00:07:39,670
provide this experience to the end-user

00:07:35,290 --> 00:07:44,120
and at the beginning it was using chef

00:07:39,670 --> 00:07:47,390
for deploying core foundry and it worked

00:07:44,120 --> 00:07:50,210
at the beginning but when we attempted

00:07:47,390 --> 00:07:54,050
to scale up and using large deployments

00:07:50,210 --> 00:07:58,520
and starting started to run maintenance

00:07:54,050 --> 00:08:02,060
tasks turned out that chef was posing a

00:07:58,520 --> 00:08:04,070
number of problems for that it was hard

00:08:02,060 --> 00:08:07,220
to scale didn't have any health checks

00:08:04,070 --> 00:08:10,390
and updating machines was very difficult

00:08:07,220 --> 00:08:14,860
and unreliable

00:08:10,390 --> 00:08:18,500
that's where Bosch enters the scene

00:08:14,860 --> 00:08:22,670
Bosch is a virtual machine Orchestrator

00:08:18,500 --> 00:08:26,180
you can call it that that does a number

00:08:22,670 --> 00:08:30,380
of things it separates the operating

00:08:26,180 --> 00:08:32,720
system from the deployed services so

00:08:30,380 --> 00:08:35,270
each of those can be updated

00:08:32,720 --> 00:08:37,910
independently every time you get an

00:08:35,270 --> 00:08:41,300
update for the operating system you

00:08:37,910 --> 00:08:43,670
update the operating system and continue

00:08:41,300 --> 00:08:46,520
running all of your software on top and

00:08:43,670 --> 00:08:48,410
likewise when your software updates it

00:08:46,520 --> 00:08:51,320
can be updated independently from the

00:08:48,410 --> 00:08:53,660
operating system it supports multiple AI

00:08:51,320 --> 00:08:56,690
asses so that you can

00:08:53,660 --> 00:08:59,569
deploy the same software over and over

00:08:56,690 --> 00:09:01,839
again using fairly the same

00:08:59,569 --> 00:09:04,250
configuration or a similar one

00:09:01,839 --> 00:09:07,879
regardless of whether you deploy on GCP

00:09:04,250 --> 00:09:12,470
or Amazon or Asia it provides a number

00:09:07,879 --> 00:09:15,430
levels of high availability which means

00:09:12,470 --> 00:09:18,319
that blush is babysitting

00:09:15,430 --> 00:09:20,569
every process that is been entrusted

00:09:18,319 --> 00:09:23,389
with making sure that the process works

00:09:20,569 --> 00:09:28,339
and if it doesn't it will report and try

00:09:23,389 --> 00:09:29,649
to relaunch and same goes to VMs every

00:09:28,339 --> 00:09:32,180
time a vm goes down

00:09:29,649 --> 00:09:35,689
Bausch can be configured to bring it

00:09:32,180 --> 00:09:38,319
back again to life and it supports

00:09:35,689 --> 00:09:42,379
automation for day 2 operations such as

00:09:38,319 --> 00:09:47,769
scaling and updates and it uses an

00:09:42,379 --> 00:09:50,750
internal DNS for service discovery and

00:09:47,769 --> 00:09:55,069
that internal DNS can be combined with

00:09:50,750 --> 00:09:58,689
health checks so that the services that

00:09:55,069 --> 00:10:01,009
are returned are the ones that are alive

00:09:58,689 --> 00:10:05,050
an additional piece of software that

00:10:01,009 --> 00:10:08,779
runs in tandem with Bosch is grant hub

00:10:05,050 --> 00:10:12,430
it's a storage for credentials and

00:10:08,779 --> 00:10:16,069
configuration information and simplifies

00:10:12,430 --> 00:10:17,660
the internal credential management

00:10:16,069 --> 00:10:20,120
external credential management and

00:10:17,660 --> 00:10:22,459
configuration between components more

00:10:20,120 --> 00:10:24,529
often than not the operators don't even

00:10:22,459 --> 00:10:26,060
need to know what the certificates and

00:10:24,529 --> 00:10:28,069
the secrets and the internal passwords

00:10:26,060 --> 00:10:29,810
of the deployments are they just need

00:10:28,069 --> 00:10:32,750
the components to talk to each other and

00:10:29,810 --> 00:10:35,240
if you don't expose those secrets then

00:10:32,750 --> 00:10:37,910
they're less likely to leak and even if

00:10:35,240 --> 00:10:40,699
they do leak it's fairly simple through

00:10:37,910 --> 00:10:43,610
cred hub to rotate them all over your

00:10:40,699 --> 00:10:45,740
deployments and if you're really

00:10:43,610 --> 00:10:49,899
paranoid or have special requirements

00:10:45,740 --> 00:10:52,850
you can have hardware encryption support

00:10:49,899 --> 00:10:54,709
so this is the cloud fire inside of

00:10:52,850 --> 00:10:58,430
things that we're more or less familiar

00:10:54,709 --> 00:11:01,189
with let's jump on to kubernetes what's

00:10:58,430 --> 00:11:04,550
this beast it's an open source container

00:11:01,189 --> 00:11:07,250
Orchestrator that was developed by

00:11:04,550 --> 00:11:10,730
Google as an open source tool

00:11:07,250 --> 00:11:12,710
it is largely influenced by Borg and

00:11:10,730 --> 00:11:15,280
takes a lot of contact concepts from

00:11:12,710 --> 00:11:17,990
there and evolves them

00:11:15,280 --> 00:11:20,530
it supports provisioning scaling and

00:11:17,990 --> 00:11:23,240
high availability for containers and

00:11:20,530 --> 00:11:25,730
integrates with the most used iOS

00:11:23,240 --> 00:11:29,120
platforms the integration though is a

00:11:25,730 --> 00:11:32,060
little bit limited in terms of the

00:11:29,120 --> 00:11:37,040
provided resources the is integration

00:11:32,060 --> 00:11:39,530
can provides storage to to get mounted

00:11:37,040 --> 00:11:41,300
into the containers and optionally if

00:11:39,530 --> 00:11:44,300
the underlying platform supports tip

00:11:41,300 --> 00:11:46,700
load balancing to expose the services

00:11:44,300 --> 00:11:49,490
but it does not provide any health

00:11:46,700 --> 00:11:50,990
checks for either the control plane or

00:11:49,490 --> 00:11:54,670
the machines that are running the

00:11:50,990 --> 00:11:57,320
containers so let me show you an example

00:11:54,670 --> 00:11:59,930
maybe you're running kubernetes

00:11:57,320 --> 00:12:02,720
somewhere on the enterprise and you have

00:11:59,930 --> 00:12:05,330
a bunch of developers and they tell the

00:12:02,720 --> 00:12:10,250
kubernetes control plan to spin up some

00:12:05,330 --> 00:12:12,400
containers nice they all get scheduled

00:12:10,250 --> 00:12:16,850
and provisioned on different VMs

00:12:12,400 --> 00:12:20,290
according to the policy but what happens

00:12:16,850 --> 00:12:23,450
if all of a sudden one of those VMs

00:12:20,290 --> 00:12:26,720
disappears somebody tripped off or over

00:12:23,450 --> 00:12:28,040
a wire or something well the good thing

00:12:26,720 --> 00:12:32,180
is that the kubernetes will reschedule

00:12:28,040 --> 00:12:34,700
those containers on existing VMs but it

00:12:32,180 --> 00:12:36,980
will leave the gaping hole where the VM

00:12:34,700 --> 00:12:42,730
was because it doesn't know how to

00:12:36,980 --> 00:12:46,700
create one and your blush and CFC are

00:12:42,730 --> 00:12:49,760
they Bosh will with help of CFC our Bosh

00:12:46,700 --> 00:12:53,839
will recreate worker vm or the control

00:12:49,760 --> 00:12:57,440
plane VM so that the cluster won't be

00:12:53,839 --> 00:13:02,000
overstressed unnecessarily and we'll

00:12:57,440 --> 00:13:05,330
keep the available volume for the

00:13:02,000 --> 00:13:10,850
workload so quick quiz

00:13:05,330 --> 00:13:14,510
what is CFC R is it contain a focused

00:13:10,850 --> 00:13:19,190
cloud ram or is it a certain factor of

00:13:14,510 --> 00:13:22,270
complicated risk or maybe it's a cute

00:13:19,190 --> 00:13:28,040
fluffy canine runs

00:13:22,270 --> 00:13:30,710
I'd rather say it is but really CFC are

00:13:28,040 --> 00:13:33,610
is quite found to contain a runtime also

00:13:30,710 --> 00:13:36,560
known as kuba the guy on the left

00:13:33,610 --> 00:13:40,580
it's bas-reliefs for plain vanilla

00:13:36,560 --> 00:13:43,060
kubernetes which means that updates to

00:13:40,580 --> 00:13:47,720
kubernetes can be easily integrated and

00:13:43,060 --> 00:13:50,650
that means that the CFC our promises to

00:13:47,720 --> 00:13:53,810
integrate patch releases within a week

00:13:50,650 --> 00:13:56,210
after they are published and minor

00:13:53,810 --> 00:14:01,370
versions within a month after they are

00:13:56,210 --> 00:14:04,370
published and in addition to that of the

00:14:01,370 --> 00:14:12,650
kubernetes that is integrated into CFC

00:14:04,370 --> 00:14:14,180
are it means to be on par with gka it is

00:14:12,650 --> 00:14:16,690
an open source project developed jointly

00:14:14,180 --> 00:14:20,600
by pivotal Google VMware and Swisscom

00:14:16,690 --> 00:14:22,880
and it provides automatic recovery for

00:14:20,600 --> 00:14:25,400
kubernetes control plane or the worker

00:14:22,880 --> 00:14:28,339
nodes whenever possible what does it

00:14:25,400 --> 00:14:31,280
mean whenever possible well basically if

00:14:28,339 --> 00:14:35,770
the data store that is back in your

00:14:31,280 --> 00:14:38,360
cluster has been destroyed beyond repair

00:14:35,770 --> 00:14:41,750
unfortunately CFC R won't help either

00:14:38,360 --> 00:14:43,640
it won't magically recreate lost data

00:14:41,750 --> 00:14:48,200
but it can recreate all the

00:14:43,640 --> 00:14:52,550
infrastructure it provides it cluster

00:14:48,200 --> 00:14:55,550
settings in CF CR are secured by default

00:14:52,550 --> 00:14:59,570
same as the darker settings that I

00:14:55,550 --> 00:15:05,540
showed you previously but we also have

00:14:59,570 --> 00:15:08,930
some knobs to expose dangerous ones so

00:15:05,540 --> 00:15:13,959
the question that I hear as asked time

00:15:08,930 --> 00:15:16,670
and time again a question why why would

00:15:13,959 --> 00:15:17,650
our foundry Foundation have those two

00:15:16,670 --> 00:15:20,510
runtimes

00:15:17,650 --> 00:15:25,940
whether you should stick to one or the

00:15:20,510 --> 00:15:29,720
other and the answer is both it depends

00:15:25,940 --> 00:15:33,900
on your needs from every application run

00:15:29,720 --> 00:15:36,720
time is being evolved further

00:15:33,900 --> 00:15:39,210
further there is a lot of exciting

00:15:36,720 --> 00:15:41,910
changes of the road it has fantastic

00:15:39,210 --> 00:15:48,620
supports first wait stateless 12 fact

00:15:41,910 --> 00:15:50,730
wraps and can micro-service meshes

00:15:48,620 --> 00:15:53,240
especially with the coming support with

00:15:50,730 --> 00:15:57,720
sto and envoy there was a talk

00:15:53,240 --> 00:15:59,670
previously the storage needs for most of

00:15:57,720 --> 00:16:03,300
applications can be provided by external

00:15:59,670 --> 00:16:05,160
services but that still doesn't cover a

00:16:03,300 --> 00:16:08,540
hundred percent of the workloads that

00:16:05,160 --> 00:16:14,279
you're gonna run in an enterprise you

00:16:08,540 --> 00:16:16,890
can use CFC are to run data services you

00:16:14,279 --> 00:16:20,100
can use it to port large legacy

00:16:16,890 --> 00:16:21,990
applications and third-party docker

00:16:20,100 --> 00:16:24,360
packaged or kubernetes packaged

00:16:21,990 --> 00:16:26,700
applications when you're not the

00:16:24,360 --> 00:16:31,140
developer you've been handed a package

00:16:26,700 --> 00:16:34,020
please run it there you go you have a

00:16:31,140 --> 00:16:36,960
way of quickly standing up a cluster and

00:16:34,020 --> 00:16:39,570
running that workload there and also you

00:16:36,960 --> 00:16:42,660
can use it for running workloads that

00:16:39,570 --> 00:16:46,370
require customized infrastructure and

00:16:42,660 --> 00:16:49,800
that's a requirement like GPUs or

00:16:46,370 --> 00:16:54,060
special sorts of networking or exposing

00:16:49,800 --> 00:17:00,060
number of various ports or other ISO

00:16:54,060 --> 00:17:04,050
Taric stuff that being said let's jump

00:17:00,060 --> 00:17:14,300
into a demo which will be fairly quick

00:17:04,050 --> 00:17:14,300
I hope let me switch there we go

00:17:16,980 --> 00:17:24,069
so I have a simple application there

00:17:19,679 --> 00:17:29,260
that is running on foundry application

00:17:24,069 --> 00:17:32,530
run time is everybody read the text it's

00:17:29,260 --> 00:17:35,620
not that hugely important but it's nice

00:17:32,530 --> 00:17:40,650
if you can it's basically a simple

00:17:35,620 --> 00:17:45,820
application that talks to a MongoDB and

00:17:40,650 --> 00:17:49,570
the MongoDB is deployed on the foundry

00:17:45,820 --> 00:17:55,170
container run time so what I'm gonna do

00:17:49,570 --> 00:17:57,940
now just to show you a little bit I will

00:17:55,170 --> 00:18:01,240
query the database for a bunch of

00:17:57,940 --> 00:18:04,390
records so we have four records with

00:18:01,240 --> 00:18:07,570
some strings in them and we can put

00:18:04,390 --> 00:18:12,480
another record with an extra string that

00:18:07,570 --> 00:18:12,480
would be a random pirate message

00:18:16,920 --> 00:18:22,830
that got recorded got assigned an ID and

00:18:19,950 --> 00:18:28,170
if we ask the database for it here we

00:18:22,830 --> 00:18:30,540
have our message neat so what are we

00:18:28,170 --> 00:18:32,730
going to do next next we're going to

00:18:30,540 --> 00:18:39,750
destroy the virtual machine that is

00:18:32,730 --> 00:18:43,410
running so here I have some out

00:18:39,750 --> 00:18:45,690
but I will comment on it's a bit later

00:18:43,410 --> 00:18:50,070
because virtual machine destruction

00:18:45,690 --> 00:18:52,680
takes a little while on GCP but what is

00:18:50,070 --> 00:18:55,200
the necessary thing is this is the

00:18:52,680 --> 00:18:56,940
entity that is running our database and

00:18:55,200 --> 00:18:58,220
this is the name of the virtual machine

00:18:56,940 --> 00:19:04,230
that it runs on

00:18:58,220 --> 00:19:07,530
so it's something ends on 0 5 - 0 4 we

00:19:04,230 --> 00:19:10,440
have here's the machine and we'll go

00:19:07,530 --> 00:19:18,540
ahead and delete it along with the boot

00:19:10,440 --> 00:19:20,340
disk goodbye cruel world so in the

00:19:18,540 --> 00:19:24,080
meantime I can explain can you see the

00:19:20,340 --> 00:19:28,260
text in the terminal or is it too small

00:19:24,080 --> 00:19:32,610
ok great so I have a bunch of things

00:19:28,260 --> 00:19:35,540
displayed here as I already said this is

00:19:32,610 --> 00:19:39,090
the entry that represents the container

00:19:35,540 --> 00:19:42,510
oops this line here that represents the

00:19:39,090 --> 00:19:46,080
container running the MongoDB this is

00:19:42,510 --> 00:19:49,980
output from Bosh that shows you the VMs

00:19:46,080 --> 00:19:52,650
that are involved in the cluster we have

00:19:49,980 --> 00:19:55,110
three master nodes and three worker

00:19:52,650 --> 00:19:57,240
nodes the master nodes are at the

00:19:55,110 --> 00:20:00,350
control plan that contains at CD

00:19:57,240 --> 00:20:03,780
database and a bunch of rest api's and

00:20:00,350 --> 00:20:05,400
some demons and the workers are the

00:20:03,780 --> 00:20:09,330
workhorses that are running the

00:20:05,400 --> 00:20:11,790
containers now here at the bottom we

00:20:09,330 --> 00:20:13,950
have a load balancer and as a service

00:20:11,790 --> 00:20:16,620
that is exposing the MongoDB so this is

00:20:13,950 --> 00:20:19,800
the actual public port public IP address

00:20:16,620 --> 00:20:24,000
and port that the database is accessible

00:20:19,800 --> 00:20:26,190
on or supposed to be and finally the

00:20:24,000 --> 00:20:28,710
last section shows you the view of

00:20:26,190 --> 00:20:30,059
kubernetes of the nodes that it has

00:20:28,710 --> 00:20:31,679
available

00:20:30,059 --> 00:20:34,590
and right now it believes that it has

00:20:31,679 --> 00:20:37,679
three nodes it hasn't picked up that one

00:20:34,590 --> 00:20:41,700
of them is going to die here we'll

00:20:37,679 --> 00:20:47,539
eventually but if we try querying the

00:20:41,700 --> 00:20:47,539
database pretty much we probably will oh

00:20:47,749 --> 00:21:00,840
it's still working let's wait for it to

00:20:54,119 --> 00:21:05,330
completely go away in the meantime does

00:21:00,840 --> 00:21:05,330
anybody have any questions while we wait

00:21:11,400 --> 00:21:16,410
okay good

00:21:14,550 --> 00:21:22,470
everybody everybody got a little bit

00:21:16,410 --> 00:21:25,710
tired I guess let's try again huh now

00:21:22,470 --> 00:21:33,809
we're not getting any data this is

00:21:25,710 --> 00:21:35,640
because the database has gone down Bosh

00:21:33,809 --> 00:21:39,150
has picked up that has gone down but

00:21:35,640 --> 00:21:41,880
kubernetes still believes that it is up

00:21:39,150 --> 00:21:48,780
and running here this line still says

00:21:41,880 --> 00:21:56,010
ready eventually it'll figure out about

00:21:48,780 --> 00:21:58,350
a minute I think it might be depending

00:21:56,010 --> 00:22:02,340
it might be dependent on different

00:21:58,350 --> 00:22:05,400
things so it's not really predict oh

00:22:02,340 --> 00:22:08,550
here you go not ready it figured out

00:22:05,400 --> 00:22:11,309
that the node has gone down so there's a

00:22:08,550 --> 00:22:12,809
lot of different machinery in place so

00:22:11,309 --> 00:22:14,220
for example it still thinks that your

00:22:12,809 --> 00:22:18,900
database is running although it's

00:22:14,220 --> 00:22:28,710
actually dead it's good to know that you

00:22:18,900 --> 00:22:31,440
have this latency when things go down so

00:22:28,710 --> 00:22:33,360
it detected that the database gone down

00:22:31,440 --> 00:22:35,490
finally and now is creating a new

00:22:33,360 --> 00:22:38,520
container for it and within a minute

00:22:35,490 --> 00:22:41,090
we'll have it up and running the only

00:22:38,520 --> 00:22:43,440
reason why I'm this is not really a

00:22:41,090 --> 00:22:45,840
setup recommended for production the

00:22:43,440 --> 00:22:48,360
only reason why I brought it up in this

00:22:45,840 --> 00:22:50,100
manner is to show you that this is the

00:22:48,360 --> 00:22:51,870
actual database that we're talking to

00:22:50,100 --> 00:22:54,240
and that it's actually dead and

00:22:51,870 --> 00:22:57,020
inaccessible and when it comes up we'll

00:22:54,240 --> 00:22:59,220
see that all the data is still intact

00:22:57,020 --> 00:23:01,890
typically in a production environment

00:22:59,220 --> 00:23:04,200
you would have a number of replicas so

00:23:01,890 --> 00:23:05,760
if one of them goes down the rest of

00:23:04,200 --> 00:23:11,300
them are still serving the traffic and

00:23:05,760 --> 00:23:11,300
the data gets saved and maintained

00:23:13,560 --> 00:23:18,990
so now kubernetes sees that there are

00:23:15,330 --> 00:23:20,460
only two worker nodes Bosch also sees

00:23:18,990 --> 00:23:30,620
two worker nodes that will eventually

00:23:20,460 --> 00:23:30,620
create the third one yes

00:23:38,559 --> 00:23:45,679
no I haven't to be honest so we've been

00:23:43,999 --> 00:23:50,179
we've been experimenting with different

00:23:45,679 --> 00:23:57,169
failure modes but we haven't been

00:23:50,179 --> 00:23:58,759
working on a sure yet so now we see that

00:23:57,169 --> 00:24:01,369
the container is running and this let

00:23:58,759 --> 00:24:03,320
number one here says that the probe is

00:24:01,369 --> 00:24:04,820
working so we can go back to our

00:24:03,320 --> 00:24:07,340
application and although you can see

00:24:04,820 --> 00:24:10,489
that the node the worker now didn't come

00:24:07,340 --> 00:24:12,229
up yet and didn't is not ready yet

00:24:10,489 --> 00:24:16,129
registered with kubernetes our database

00:24:12,229 --> 00:24:21,109
is already up and running let's get our

00:24:16,129 --> 00:24:25,249
list of products here we go and that

00:24:21,109 --> 00:24:27,080
concludes the demo so eventually the the

00:24:25,249 --> 00:24:36,830
worker will come back up but it's too

00:24:27,080 --> 00:24:39,859
long to wait ok moving forward what are

00:24:36,830 --> 00:24:42,590
the directions that CF CR can take in

00:24:39,859 --> 00:24:46,729
the future this is not a promise this is

00:24:42,590 --> 00:24:48,259
just vision support for Windows

00:24:46,729 --> 00:24:51,669
workloads is one of the most requested

00:24:48,259 --> 00:24:55,639
features it will take a lot of time as

00:24:51,669 --> 00:24:57,169
there has been a spike and we've tried

00:24:55,639 --> 00:24:59,289
to integrate with Windows but the

00:24:57,169 --> 00:25:01,519
networking seems to be very tricky

00:24:59,289 --> 00:25:04,940
eventually we'll overcome it but it

00:25:01,519 --> 00:25:08,749
won't happen right away integration with

00:25:04,940 --> 00:25:11,239
Bosch backup and restore kubernetes is

00:25:08,749 --> 00:25:13,580
backed up by the sed database so we need

00:25:11,239 --> 00:25:14,690
to figure out a way of backing up and

00:25:13,580 --> 00:25:16,940
restoring it safely

00:25:14,690 --> 00:25:20,419
and regular intervals without disrupting

00:25:16,940 --> 00:25:23,419
the cluster interaction with the

00:25:20,419 --> 00:25:25,940
underlying as via Bosch as opposed to

00:25:23,419 --> 00:25:29,019
directly via the platform why is that

00:25:25,940 --> 00:25:33,080
important well when kubernetes creates

00:25:29,019 --> 00:25:36,109
workloads it provisions disks and load

00:25:33,080 --> 00:25:38,899
balancers on different platforms but if

00:25:36,109 --> 00:25:42,229
you kill the cluster there is no easy

00:25:38,899 --> 00:25:44,739
way of cleaning up those resources if

00:25:42,229 --> 00:25:48,169
all the interaction will go through Bosh

00:25:44,739 --> 00:25:50,330
then you can easily do that you can

00:25:48,169 --> 00:25:51,590
identify those resources and you can

00:25:50,330 --> 00:25:53,900
either throw them away

00:25:51,590 --> 00:25:56,059
restore them or restore them or reuse

00:25:53,900 --> 00:25:59,350
whatever but there is an easy way of

00:25:56,059 --> 00:26:01,940
locating them service broker integration

00:25:59,350 --> 00:26:07,720
and that's an interesting one the OS

00:26:01,940 --> 00:26:10,490
bappy team is working on providing

00:26:07,720 --> 00:26:13,370
unified standardized way of providing

00:26:10,490 --> 00:26:16,760
services so it would be really really

00:26:13,370 --> 00:26:18,679
great if you could do CF create service

00:26:16,760 --> 00:26:20,900
and that would create something on your

00:26:18,679 --> 00:26:24,200
kubernetes cluster if you could feed in

00:26:20,900 --> 00:26:27,970
a llamo manifest of a deployment or

00:26:24,200 --> 00:26:27,970
something like this that'd be fantastic

00:26:28,090 --> 00:26:33,559
we welcome all the contributors so these

00:26:31,070 --> 00:26:37,190
are all the bits and bobs that the CFC

00:26:33,559 --> 00:26:39,770
our Bosch release contains off the CFC

00:26:37,190 --> 00:26:42,380
our Bosch release itself the sed Bosch

00:26:39,770 --> 00:26:46,240
release as a separate release and the

00:26:42,380 --> 00:26:49,580
darker Bosch release we also have

00:26:46,240 --> 00:26:51,470
documentation website that is separate

00:26:49,580 --> 00:26:54,080
and it's also github repo so if you want

00:26:51,470 --> 00:26:57,770
to contribute to documentation you're

00:26:54,080 --> 00:27:00,020
also welcome to do so and with that we

00:26:57,770 --> 00:27:10,360
can jump into questions if there are any

00:27:00,020 --> 00:27:10,360
left no great yes

00:27:17,610 --> 00:27:23,580
at the time we didn't even try to do we

00:27:21,000 --> 00:27:28,380
haven't even tried it doing it yet but

00:27:23,580 --> 00:27:32,430
we're just anticipating locking up at CD

00:27:28,380 --> 00:27:36,270
database in order to backup to make a

00:27:32,430 --> 00:27:38,700
consistent backup would stop vapi from

00:27:36,270 --> 00:27:42,470
registering any changes so we would want

00:27:38,700 --> 00:27:45,240
to somehow avoid downtime by either

00:27:42,470 --> 00:27:50,060
streaming a slave or something like this

00:27:45,240 --> 00:27:50,060
I don't know yes

00:28:10,590 --> 00:28:17,559
okay so the question is there there were

00:28:14,620 --> 00:28:22,890
known issues with etcd before in CFC our

00:28:17,559 --> 00:28:22,890
and whether we have overcome them

00:28:30,450 --> 00:28:36,550
yes so the question is whether we have

00:28:34,630 --> 00:28:41,770
the same problems as the application run

00:28:36,550 --> 00:28:44,170
time at C D and the answer is no the at

00:28:41,770 --> 00:28:46,480
CD release for CFC R is a separate at CD

00:28:44,170 --> 00:28:48,970
release the reason being that the use

00:28:46,480 --> 00:28:50,860
cases are completely different the CF

00:28:48,970 --> 00:28:53,620
application runtime used at CD as a

00:28:50,860 --> 00:28:56,290
cache meaning that if you have a problem

00:28:53,620 --> 00:28:58,720
just nuke the datastore bring up a new

00:28:56,290 --> 00:29:00,190
one and start caching again you can't do

00:28:58,720 --> 00:29:02,020
that with C f/c are you come to that

00:29:00,190 --> 00:29:03,760
with kubernetes because that's your

00:29:02,020 --> 00:29:06,940
source of truth you can't throw it away

00:29:03,760 --> 00:29:13,710
so we had to create a different release

00:29:06,940 --> 00:29:13,710
to reflect that difference yes

00:29:30,140 --> 00:29:43,430
how you can delegate that answer to the

00:29:32,750 --> 00:29:46,220
anchor of the PKS team great upcoming

00:29:43,430 --> 00:29:48,910
kubernetes events there is a panel in

00:29:46,220 --> 00:29:52,070
experiments and extensions next door

00:29:48,910 --> 00:29:54,590
right after this talk tomorrow there'll

00:29:52,070 --> 00:29:59,030
be CFC our office hours with Colin

00:29:54,590 --> 00:30:03,700
Humphreys pivotal and after lunch I will

00:29:59,030 --> 00:30:07,840
have three kubernetes related talks from

00:30:03,700 --> 00:30:10,730
Jonathan birkel and Morgan Barra IBM

00:30:07,840 --> 00:30:12,050
pivotal and sue Sato discussing open

00:30:10,730 --> 00:30:15,020
service brokers that I have just

00:30:12,050 --> 00:30:17,450
mentioned and Huey and to give

00:30:15,020 --> 00:30:21,560
authentication for kubernetes from Al

00:30:17,450 --> 00:30:22,360
Taurus and on that note thank you very

00:30:21,560 --> 00:30:26,700
much

00:30:22,360 --> 00:30:26,700
[Applause]

00:30:28,190 --> 00:30:34,759
please leave any feedback you might have

00:30:31,609 --> 00:30:38,119
for the talk I would be very grateful

00:30:34,759 --> 00:30:38,119

YouTube URL: https://www.youtube.com/watch?v=KvwZIshANsk


