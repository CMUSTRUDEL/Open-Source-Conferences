Title: Troubleshooting .NET Apps on Cloud Foundry - Matyukevich Sergey, Altoros
Publication date: 2018-04-21
Playlist: Cloud Foundry Summit NA 2018
Description: 
	Troubleshooting .NET Apps on Cloud Foundry - Matyukevich Sergey, Altoros

With Java applications pushed to Cloud Foundry, remote debugging is a piece of cake. With .NET applications, it’s a different story. Execution of the troubleshooting operations is barely possible with the Cloud Foundry platform as it is.
In this presentation, I’m going to share my experience of troubleshooting and debugging .NET applications pushed to Cloud Foundry and uncover the details of some generic operations that need to be performed on the Cloud Foundry platform for these ends. 

About Matyukevich Sergey
Sergey Matyukevich is Solutions Architect at Altoros. With 6+ years in software engineering, he is an expert in cloud automation and designing architectures for complex cloud-based systems. An active member of the Go community and a frequent contributor to open-source projects (Ubuntu, Juju Charms, etc.), Sergey has authored a series of popular blog posts on Golang internals. He has also designed and delivered a range of training courses on Cloud Foundry and cloud-native development for DevOps engineers, software developers, and architects.
Captions: 
	00:00:00,170 --> 00:00:05,850
let's start our session about

00:00:02,659 --> 00:00:07,440
troubleshooted NetApp Legation welcome

00:00:05,850 --> 00:00:11,790
everybody my name is Sergey matakovich

00:00:07,440 --> 00:00:14,610
I'm a cloud engineer from Alturas I'm

00:00:11,790 --> 00:00:22,160
sure you have probably already seen this

00:00:14,610 --> 00:00:24,630
slide so but for those who didn't yeah

00:00:22,160 --> 00:00:26,550
so for those who didn't please review it

00:00:24,630 --> 00:00:29,189
because it contains important fire

00:00:26,550 --> 00:00:33,030
announcement but while we're reading let

00:00:29,189 --> 00:00:36,329
me quickly introduce myself so I started

00:00:33,030 --> 00:00:38,160
a dotnet developer seven years ago but

00:00:36,329 --> 00:00:40,800
then I moved to develop stuff completely

00:00:38,160 --> 00:00:42,660
and I can claim that I have seen the

00:00:40,800 --> 00:00:45,530
platform for most points of view as a

00:00:42,660 --> 00:00:49,020
developer and as it evolves and my

00:00:45,530 --> 00:00:52,949
current topic is closely related to both

00:00:49,020 --> 00:00:55,590
of those experiences so today we're

00:00:52,949 --> 00:00:59,129
gonna talk about efficient ways to

00:00:55,590 --> 00:01:02,280
troubleshoot dotnet application deployed

00:00:59,129 --> 00:01:04,409
to code finally I can split the topic of

00:01:02,280 --> 00:01:07,920
troubleshooting dotnet application into

00:01:04,409 --> 00:01:09,689
three three broad areas so first of all

00:01:07,920 --> 00:01:12,900
we are gonna talk about how to utilize

00:01:09,689 --> 00:01:15,030
blocks we will see how we can use

00:01:12,900 --> 00:01:17,040
structured login to efficiently lock

00:01:15,030 --> 00:01:19,500
messages and help platform to aggregate

00:01:17,040 --> 00:01:21,210
those locks and presents them for us we

00:01:19,500 --> 00:01:24,360
will talk a little bit about monitoring

00:01:21,210 --> 00:01:26,310
as well how we can use monitoring to

00:01:24,360 --> 00:01:28,650
notify us if something goes wrong with

00:01:26,310 --> 00:01:31,829
the platform and finally we will talk

00:01:28,650 --> 00:01:34,979
about debugging and remote debugging of

00:01:31,829 --> 00:01:39,540
dotnet application deployed to called

00:01:34,979 --> 00:01:42,829
foundry platform a few important scene

00:01:39,540 --> 00:01:46,710
that you need to know about login in

00:01:42,829 --> 00:01:49,079
cloud foundry and this is related to

00:01:46,710 --> 00:01:52,890
most cloud platforms so first of all

00:01:49,079 --> 00:01:56,310
your application is not supposed to lock

00:01:52,890 --> 00:01:58,710
anything into log files so instead you

00:01:56,310 --> 00:02:02,070
should print output of your logs in the

00:01:58,710 --> 00:02:08,250
standard our standard out of your

00:02:02,070 --> 00:02:09,350
process and also it is important to know

00:02:08,250 --> 00:02:12,070
that

00:02:09,350 --> 00:02:16,130
from is responsible for aggregating and

00:02:12,070 --> 00:02:18,590
ham the log for use so later you can

00:02:16,130 --> 00:02:20,990
consume those log and utilize them and

00:02:18,590 --> 00:02:23,900
use some vocab regression framework that

00:02:20,990 --> 00:02:26,540
I'm gonna show you a little bit later in

00:02:23,900 --> 00:02:28,670
my presentation and other important

00:02:26,540 --> 00:02:32,030
concepts it is related to login in a

00:02:28,670 --> 00:02:32,870
cloud application is called structured

00:02:32,030 --> 00:02:35,540
log

00:02:32,870 --> 00:02:40,910
so what does it mean is that usually you

00:02:35,540 --> 00:02:44,540
don't just print some streamed message

00:02:40,910 --> 00:02:46,520
in as your log message you attach some

00:02:44,540 --> 00:02:49,550
important information to each of your

00:02:46,520 --> 00:02:53,600
message and this allows later to work

00:02:49,550 --> 00:02:57,050
efficiently sort your messages filter

00:02:53,600 --> 00:03:01,940
your messages and drill down into an

00:02:57,050 --> 00:03:04,100
error if you had one and later I will

00:03:01,940 --> 00:03:06,950
show you how we can utilize facilities

00:03:04,100 --> 00:03:11,480
provided for us by dotnet framework to

00:03:06,950 --> 00:03:15,950
implement such kind of login here is the

00:03:11,480 --> 00:03:19,040
course that does this for a sample I

00:03:15,950 --> 00:03:20,990
spit net core application so the top

00:03:19,040 --> 00:03:23,840
code snippet is responsible for

00:03:20,990 --> 00:03:26,540
configuring logger so here I'm using

00:03:23,840 --> 00:03:31,070
Siri log library to implement structured

00:03:26,540 --> 00:03:33,950
log in but that's not as important

00:03:31,070 --> 00:03:36,530
because almost all logging libraries

00:03:33,950 --> 00:03:40,730
like n logs for example a lock phone app

00:03:36,530 --> 00:03:44,000
implements similar kind of features so

00:03:40,730 --> 00:03:45,950
here we configure default lock level 4

00:03:44,000 --> 00:03:50,500
standard libraries because we don't want

00:03:45,950 --> 00:03:52,760
to be overflow with messages from

00:03:50,500 --> 00:03:55,820
Microsoft namespace and system namespace

00:03:52,760 --> 00:03:58,610
but the most important line here is the

00:03:55,820 --> 00:04:01,400
one where we are configure that we are

00:03:58,610 --> 00:04:04,310
going to write our message to console so

00:04:01,400 --> 00:04:06,230
that's essential for cloud application

00:04:04,310 --> 00:04:08,120
because as I said previously we don't

00:04:06,230 --> 00:04:10,490
want to lock into files we just output

00:04:08,120 --> 00:04:13,040
every sin to console and also we're

00:04:10,490 --> 00:04:16,200
using JSON for a matter so if you use

00:04:13,040 --> 00:04:18,980
JSON for matters this allows you to wrap

00:04:16,200 --> 00:04:21,780
or messages in JSON object and attach

00:04:18,980 --> 00:04:26,220
fields or text to each message that

00:04:21,780 --> 00:04:29,720
later can be used to search for logs and

00:04:26,220 --> 00:04:31,890
second code snippet is configuring the

00:04:29,720 --> 00:04:34,920
application itself and the most

00:04:31,890 --> 00:04:38,040
important important line here is the

00:04:34,920 --> 00:04:40,710
ones that says use Siri log this is

00:04:38,040 --> 00:04:43,350
extension methods that actually applies

00:04:40,710 --> 00:04:45,060
previously configured logger to current

00:04:43,350 --> 00:04:50,010
application and allows a speed of

00:04:45,060 --> 00:04:53,520
network up to use this logger next after

00:04:50,010 --> 00:04:55,440
we have our logger configured we can try

00:04:53,520 --> 00:04:57,980
to utilize our logger

00:04:55,440 --> 00:05:01,170
but what we really want to achieve is to

00:04:57,980 --> 00:05:04,740
transparently add those kind of tags

00:05:01,170 --> 00:05:06,930
that I was telling you previously to all

00:05:04,740 --> 00:05:10,020
our messages we don't want to attach

00:05:06,930 --> 00:05:13,320
request ID and user ID each time we log

00:05:10,020 --> 00:05:16,290
in something and that's where middleware

00:05:13,320 --> 00:05:19,710
comes into place it is a very useful

00:05:16,290 --> 00:05:22,170
method to implement some functionality

00:05:19,710 --> 00:05:25,160
before and after your request is

00:05:22,170 --> 00:05:29,670
processed so here you can see that we

00:05:25,160 --> 00:05:31,860
write in some string to log before the

00:05:29,670 --> 00:05:34,590
request after the request and most

00:05:31,860 --> 00:05:37,680
importantly we use begin scope method of

00:05:34,590 --> 00:05:40,080
eyes p.net for lager and this method

00:05:37,680 --> 00:05:42,810
allows us to create custom scope

00:05:40,080 --> 00:05:44,790
what does it mean is that it creates a

00:05:42,810 --> 00:05:48,210
new fields it is gonna be attached to

00:05:44,790 --> 00:05:50,880
each subsequent message in this code by

00:05:48,210 --> 00:05:53,220
default is p.net core create scope for

00:05:50,880 --> 00:05:55,880
each request and it attached such fields

00:05:53,220 --> 00:05:58,800
as a request ID we will talk how to very

00:05:55,880 --> 00:06:00,930
efficiently use request ID later but

00:05:58,800 --> 00:06:03,150
nobody prevents you from create custom

00:06:00,930 --> 00:06:04,890
scopes and it's not necessarily should

00:06:03,150 --> 00:06:08,100
be the case that you should wrap only

00:06:04,890 --> 00:06:11,550
request into custom scopes you can wrap

00:06:08,100 --> 00:06:15,600
any other functionality into custom

00:06:11,550 --> 00:06:18,280
scopes as well so if we are talking

00:06:15,600 --> 00:06:21,070
about custom scopes

00:06:18,280 --> 00:06:25,060
there are a few important scenes that I

00:06:21,070 --> 00:06:29,400
want to highlight so first of all scopes

00:06:25,060 --> 00:06:32,380
allows you to make your login

00:06:29,400 --> 00:06:35,290
transparent for the court so you don't

00:06:32,380 --> 00:06:38,380
attach user ID and scope ID each time

00:06:35,290 --> 00:06:40,930
you login something instead you use such

00:06:38,380 --> 00:06:43,210
facilities as custom middleware or

00:06:40,930 --> 00:06:47,260
custom filters to do this job for you

00:06:43,210 --> 00:06:49,210
and secondly it ensures consistent login

00:06:47,260 --> 00:06:51,700
format so you know that request ID is

00:06:49,210 --> 00:06:54,220
going to be attached to each message and

00:06:51,700 --> 00:06:57,220
then you can utilize this when filtering

00:06:54,220 --> 00:06:59,950
for your messages and as I said

00:06:57,220 --> 00:07:01,960
previously you can wrap any other kind

00:06:59,950 --> 00:07:04,900
of functionality for example request to

00:07:01,960 --> 00:07:07,180
database transaction database

00:07:04,900 --> 00:07:09,430
transactions can be wrapped in a custom

00:07:07,180 --> 00:07:11,620
blogger scope if a lot of work is going

00:07:09,430 --> 00:07:15,150
to be is going to be done during the

00:07:11,620 --> 00:07:20,050
process processing of this transaction

00:07:15,150 --> 00:07:25,390
let me also show you a few fields that I

00:07:20,050 --> 00:07:28,090
usually consider to be important when

00:07:25,390 --> 00:07:30,220
talking about load so first of all if we

00:07:28,090 --> 00:07:33,040
are talking about custom scope so

00:07:30,220 --> 00:07:35,860
usually we attach a DS that later can be

00:07:33,040 --> 00:07:38,260
used to filter our messages as the most

00:07:35,860 --> 00:07:41,350
important one is request ID but also

00:07:38,260 --> 00:07:45,060
it's important to attach user ID maybe

00:07:41,350 --> 00:07:48,010
threat ID if you are doing some

00:07:45,060 --> 00:07:53,640
multi-threaded processing during your

00:07:48,010 --> 00:07:57,910
action message method execution but also

00:07:53,640 --> 00:08:00,070
those fields are used to filter your

00:07:57,910 --> 00:08:02,830
messages but also it is important to

00:08:00,070 --> 00:08:05,650
trace everything that it goes inside

00:08:02,830 --> 00:08:09,669
your application and goes outside it so

00:08:05,650 --> 00:08:13,840
what do I mean by is that so it's a good

00:08:09,669 --> 00:08:15,700
practice to lock the content of your

00:08:13,840 --> 00:08:19,600
request because later you can reproduce

00:08:15,700 --> 00:08:22,030
this request and if you rely on custom

00:08:19,600 --> 00:08:24,660
here for example you can look completely

00:08:22,030 --> 00:08:24,660
the

00:08:25,770 --> 00:08:30,449
the caloric well also it is important to

00:08:28,590 --> 00:08:32,219
lock everything that you send to

00:08:30,449 --> 00:08:34,649
external system and you can consider

00:08:32,219 --> 00:08:37,110
your database as external system so you

00:08:34,649 --> 00:08:40,310
can lock your sequel queries and later

00:08:37,110 --> 00:08:43,260
you can analyze performance and find

00:08:40,310 --> 00:08:45,570
bottlenecks when analyzing those queries

00:08:43,260 --> 00:08:50,310
and if you talk to some external system

00:08:45,570 --> 00:08:53,399
it's also useful to log these and the

00:08:50,310 --> 00:08:55,890
main idea why I pay so much attention to

00:08:53,399 --> 00:08:58,500
this because this logon can be

00:08:55,890 --> 00:09:00,089
implemented in a consistent way using

00:08:58,500 --> 00:09:02,070
such techniques as I showed you

00:09:00,089 --> 00:09:04,890
previously with custom middleware for

00:09:02,070 --> 00:09:07,230
example you can use your RM to attach

00:09:04,890 --> 00:09:11,610
some login information before each

00:09:07,230 --> 00:09:13,980
request and after each request ok at

00:09:11,610 --> 00:09:16,560
this point of time we have our login

00:09:13,980 --> 00:09:18,600
configure for our simple application and

00:09:16,560 --> 00:09:20,610
it brings something to standard output

00:09:18,600 --> 00:09:22,980
and co-founder is responsible for

00:09:20,610 --> 00:09:26,060
collecting those messages now we need to

00:09:22,980 --> 00:09:29,640
talk a little bit how we can utilize

00:09:26,060 --> 00:09:32,399
this and how we can filter our messages

00:09:29,640 --> 00:09:36,209
and search them so in my presentation

00:09:32,399 --> 00:09:38,550
I'm showing you screenshots from

00:09:36,209 --> 00:09:41,640
applications that is called Cubana and

00:09:38,550 --> 00:09:44,070
here I'm using a very popular stack that

00:09:41,640 --> 00:09:47,130
is used very commonly with refinery it

00:09:44,070 --> 00:09:49,770
is called ALK elasticsearch lakhs -

00:09:47,130 --> 00:09:53,120
kibana so this stack is responsible for

00:09:49,770 --> 00:09:55,950
taking messages from Cloud Foundry

00:09:53,120 --> 00:09:57,990
storing them and later cabañas this is

00:09:55,950 --> 00:09:59,910
the screenshot from this application

00:09:57,990 --> 00:10:02,040
provides you a way to see your log

00:09:59,910 --> 00:10:03,209
messages and here you can see that we

00:10:02,040 --> 00:10:05,880
have three messages

00:10:03,209 --> 00:10:08,399
something that are two messages from our

00:10:05,880 --> 00:10:12,300
custom middleware and the important

00:10:08,399 --> 00:10:14,370
thing here is that our custom field is

00:10:12,300 --> 00:10:16,320
attached to all of the messages and we

00:10:14,370 --> 00:10:21,740
also see that request ID is attached

00:10:16,320 --> 00:10:25,050
here very briefly I want to explain how

00:10:21,740 --> 00:10:28,370
lk stack works so called foundry by

00:10:25,050 --> 00:10:33,630
itself doesn't provide you any

00:10:28,370 --> 00:10:35,310
possibility to efficiently search for

00:10:33,630 --> 00:10:36,270
logs instead called funny we just

00:10:35,310 --> 00:10:39,240
provide

00:10:36,270 --> 00:10:42,060
you way to grab all locks from the

00:10:39,240 --> 00:10:44,790
system and it should be a responsibility

00:10:42,060 --> 00:10:49,710
of some other system to collect those

00:10:44,790 --> 00:10:54,900
locks store them and provide search and

00:10:49,710 --> 00:10:56,850
filter disabilities so the applications

00:10:54,900 --> 00:10:59,310
that collects locks from God founder is

00:10:56,850 --> 00:11:01,620
called fire hose nozzle then locks are

00:10:59,310 --> 00:11:05,730
sent to intermediate buffer usually we

00:11:01,620 --> 00:11:08,640
use Redis for this then there is a

00:11:05,730 --> 00:11:13,710
components that is called elastics locks

00:11:08,640 --> 00:11:18,860
- so locks - takes JSON strings from an

00:11:13,710 --> 00:11:23,280
input the ones that we have been locked

00:11:18,860 --> 00:11:26,120
previously and convert them to an object

00:11:23,280 --> 00:11:28,920
and those objects are stored in

00:11:26,120 --> 00:11:32,910
elasticsearch and later Cabana can talk

00:11:28,920 --> 00:11:34,710
directly to elasticsearch and take log

00:11:32,910 --> 00:11:39,000
messages out of there this is very

00:11:34,710 --> 00:11:42,900
briefly how it works so now I want to

00:11:39,000 --> 00:11:45,990
show you how we can use these to search

00:11:42,900 --> 00:11:48,030
for locks in case if something happens

00:11:45,990 --> 00:11:51,600
so for example let's consider very

00:11:48,030 --> 00:11:56,040
common scenario we have an error in our

00:11:51,600 --> 00:11:59,100
application so first of all we find our

00:11:56,040 --> 00:12:01,650
error message with stack trace in Cabana

00:11:59,100 --> 00:12:04,080
and then we can use request ID field

00:12:01,650 --> 00:12:06,560
attached to this message we can use this

00:12:04,080 --> 00:12:08,910
request ID to filter all message and

00:12:06,560 --> 00:12:11,520
messages that belongs to this particular

00:12:08,910 --> 00:12:14,310
request then we can analyze what

00:12:11,520 --> 00:12:19,440
parameters have been sent to us we know

00:12:14,310 --> 00:12:22,370
which user was in context of which user

00:12:19,440 --> 00:12:26,640
our requests have been executed so we

00:12:22,370 --> 00:12:29,150
have basically all necessary data to

00:12:26,640 --> 00:12:32,910
fully reproduce the request if we need

00:12:29,150 --> 00:12:35,310
and usually this this should be enough

00:12:32,910 --> 00:12:39,750
to troubleshoot any kind of error and

00:12:35,310 --> 00:12:41,340
from my point of view a good way to say

00:12:39,750 --> 00:12:44,850
that okay I'm doing

00:12:41,340 --> 00:12:47,040
and right for my platform is that if you

00:12:44,850 --> 00:12:51,720
can troubleshoot any issues just by

00:12:47,040 --> 00:12:54,600
using logs then you are really logging

00:12:51,720 --> 00:12:57,210
enough information and you are really

00:12:54,600 --> 00:12:59,310
logging information that is necessary

00:12:57,210 --> 00:13:03,000
for you so I'm not overflowed with some

00:12:59,310 --> 00:13:06,570
unnecessary information and that doesn't

00:13:03,000 --> 00:13:10,740
help you in during debugging your

00:13:06,570 --> 00:13:13,800
application next I'm gonna talk a little

00:13:10,740 --> 00:13:19,050
bit about metrics and how we can use

00:13:13,800 --> 00:13:22,170
them in cloud foundry and utilize

00:13:19,050 --> 00:13:24,570
metrics from dotnet applications so if

00:13:22,170 --> 00:13:26,760
walks are mostly used to troubleshoot

00:13:24,570 --> 00:13:28,830
some issue like in in the examples that

00:13:26,760 --> 00:13:33,330
I discussed previously we have an error

00:13:28,830 --> 00:13:39,200
and then we use logs to troubleshoot the

00:13:33,330 --> 00:13:41,640
issue metrics usually are used to send

00:13:39,200 --> 00:13:45,360
notification that something is wrong

00:13:41,640 --> 00:13:50,370
with your system like disk utilization

00:13:45,360 --> 00:13:52,380
is too high or memory we are going out

00:13:50,370 --> 00:13:54,480
of memory or we're having too many

00:13:52,380 --> 00:13:57,390
requests and our application cannot keep

00:13:54,480 --> 00:14:01,560
up with them by default all foundry

00:13:57,390 --> 00:14:05,700
provides you providing you a possibility

00:14:01,560 --> 00:14:07,800
to access basic metrics such as for

00:14:05,700 --> 00:14:10,589
example disk utilization memory

00:14:07,800 --> 00:14:12,660
utilization number of requests average

00:14:10,589 --> 00:14:15,150
time of processing the request and those

00:14:12,660 --> 00:14:17,550
metrics are very useful and if we are

00:14:15,150 --> 00:14:20,520
using PCF the recent applications it is

00:14:17,550 --> 00:14:23,730
called PCF metrics and those metrics are

00:14:20,520 --> 00:14:25,830
available by default and you can just

00:14:23,730 --> 00:14:30,750
open dashboard and see how the schedule

00:14:25,830 --> 00:14:33,270
ization changes over time but that's

00:14:30,750 --> 00:14:35,880
actually is it were very easy to use but

00:14:33,270 --> 00:14:38,870
what I'm really want to focus on is how

00:14:35,880 --> 00:14:41,630
you can create your custom metrics

00:14:38,870 --> 00:14:44,279
creating custom metrics can be very

00:14:41,630 --> 00:14:45,990
useful because obviously called foundry

00:14:44,279 --> 00:14:48,060
doesn't know anything about your

00:14:45,990 --> 00:14:50,790
application business logic it doesn't

00:14:48,060 --> 00:14:51,940
know when your customer logs in into

00:14:50,790 --> 00:14:54,220
system or

00:14:51,940 --> 00:14:57,340
when some important event happened when

00:14:54,220 --> 00:15:00,880
an error happened and so on so it's

00:14:57,340 --> 00:15:02,650
usually useful to when designing your

00:15:00,880 --> 00:15:04,660
application to came up with a few

00:15:02,650 --> 00:15:08,050
important metrics and here I'm showing

00:15:04,660 --> 00:15:11,680
you how you can very simply send value

00:15:08,050 --> 00:15:15,970
for some particular some metric and I'm

00:15:11,680 --> 00:15:18,460
using graphite for this so graphite is a

00:15:15,970 --> 00:15:20,980
time series database that can store

00:15:18,460 --> 00:15:23,050
values for some matrix and later we can

00:15:20,980 --> 00:15:25,510
use other software to visualize those

00:15:23,050 --> 00:15:31,180
metrics so let's see how it works on

00:15:25,510 --> 00:15:34,360
example if we this is a diagram taken

00:15:31,180 --> 00:15:39,550
from one of products just got her visit

00:15:34,360 --> 00:15:41,410
is doing exactly this it cost time

00:15:39,550 --> 00:15:44,530
serious database and connects to Cloud

00:15:41,410 --> 00:15:46,600
Foundry and grabs all default metrics

00:15:44,530 --> 00:15:49,210
out of the system by default metrics I

00:15:46,600 --> 00:15:51,370
mean matrix that came from virtual

00:15:49,210 --> 00:15:54,460
machines where our code fundraiser

00:15:51,370 --> 00:15:56,050
hosted I also mean metrics came from the

00:15:54,460 --> 00:16:01,810
platform itself

00:15:56,050 --> 00:16:04,420
and from all components of the platform

00:16:01,810 --> 00:16:06,070
but also there is a way for your

00:16:04,420 --> 00:16:09,100
application like I showed you previously

00:16:06,070 --> 00:16:11,590
to send metrics directly to this

00:16:09,100 --> 00:16:13,750
database this is very useful if you want

00:16:11,590 --> 00:16:17,440
to compare two metrics for example you

00:16:13,750 --> 00:16:21,160
want to see how disk utilization changes

00:16:17,440 --> 00:16:23,800
when logging a number of login attempts

00:16:21,160 --> 00:16:26,290
grows over time for example or you want

00:16:23,800 --> 00:16:28,630
to correlate two metrics one of which is

00:16:26,290 --> 00:16:30,730
application specific and other is

00:16:28,630 --> 00:16:32,850
platform specific and such kind of

00:16:30,730 --> 00:16:35,500
solution provide you a way to do this so

00:16:32,850 --> 00:16:38,530
all of them are stored in time serious

00:16:35,500 --> 00:16:42,460
database and we use an applications that

00:16:38,530 --> 00:16:44,110
is called DRA fauna to visualize those

00:16:42,460 --> 00:16:48,460
metrics so actually on my previous

00:16:44,110 --> 00:16:51,640
slides there's sample of graph how that

00:16:48,460 --> 00:16:56,470
can be built in Agrafena to visualize

00:16:51,640 --> 00:17:00,640
some metric and the last functionality

00:16:56,470 --> 00:17:01,910
that is very important is alerting so if

00:17:00,640 --> 00:17:03,830
you have some

00:17:01,910 --> 00:17:07,070
freak usually you want to be notified

00:17:03,830 --> 00:17:09,560
when something runs when this the value

00:17:07,070 --> 00:17:13,610
of this metric goes beyond some

00:17:09,560 --> 00:17:19,000
threshold for example if the CPU

00:17:13,610 --> 00:17:22,030
utilization goes beyond 80% and as

00:17:19,000 --> 00:17:27,140
usually monitoring tools allows you to

00:17:22,030 --> 00:17:30,530
use logic to say okay when's this event

00:17:27,140 --> 00:17:33,860
happen I want to email to be sent to

00:17:30,530 --> 00:17:36,800
system administrators and this interface

00:17:33,860 --> 00:17:39,890
shows you an example how we can create

00:17:36,800 --> 00:17:43,600
such queries and how we can use value of

00:17:39,890 --> 00:17:47,180
particular metric to identify user so

00:17:43,600 --> 00:17:50,240
same as for logs I want to talk a little

00:17:47,180 --> 00:17:56,480
bit about what kind of metrics I

00:17:50,240 --> 00:17:59,930
consider - what kind of metrics are

00:17:56,480 --> 00:18:02,540
important to be monitored so first of

00:17:59,930 --> 00:18:04,640
all default metrics that we talked about

00:18:02,540 --> 00:18:07,250
previously definitely a very important

00:18:04,640 --> 00:18:09,830
container metrics CPU memory disk number

00:18:07,250 --> 00:18:11,450
of requests requests latency if we are

00:18:09,830 --> 00:18:13,760
talking about application specific

00:18:11,450 --> 00:18:16,670
metrics from my previous experience I

00:18:13,760 --> 00:18:20,120
can say that the most important one are

00:18:16,670 --> 00:18:22,280
is database metrics because I have seen

00:18:20,120 --> 00:18:24,560
a lot of times situation when

00:18:22,280 --> 00:18:26,660
application works well in development

00:18:24,560 --> 00:18:30,440
but as soon as we push it into a

00:18:26,660 --> 00:18:33,380
production and put it under a lot the

00:18:30,440 --> 00:18:35,390
processing time grows exponentially and

00:18:33,380 --> 00:18:37,970
that's where monitoring can help you

00:18:35,390 --> 00:18:42,140
because you can see okay now request

00:18:37,970 --> 00:18:44,210
time for goals up and we see the number

00:18:42,140 --> 00:18:46,670
of database requests we correlate those

00:18:44,210 --> 00:18:48,980
two metrics and if we see that a number

00:18:46,670 --> 00:18:50,900
of database requests also grows

00:18:48,980 --> 00:18:52,790
exponentially this might indicate an

00:18:50,900 --> 00:18:55,460
error in your application for example

00:18:52,790 --> 00:18:58,360
one very popular error is n plus 1

00:18:55,460 --> 00:19:04,160
requests when your application doesn't

00:18:58,360 --> 00:19:06,710
use sequel to get all information it

00:19:04,160 --> 00:19:11,060
needed in one round-trip to database but

00:19:06,710 --> 00:19:13,549
instead talk to database for iterate

00:19:11,060 --> 00:19:15,289
over all items in

00:19:13,549 --> 00:19:19,279
database this is very common and it's

00:19:15,289 --> 00:19:21,110
very easily too easy to see when you

00:19:19,279 --> 00:19:23,379
compare a number of database requests

00:19:21,110 --> 00:19:28,190
with number of items in a particular

00:19:23,379 --> 00:19:30,769
table and finally I want the last topic

00:19:28,190 --> 00:19:35,029
for today is how we can use remote

00:19:30,769 --> 00:19:40,759
debugger in dotnet application so I want

00:19:35,029 --> 00:19:43,820
to tell about this on using ice p.net

00:19:40,759 --> 00:19:45,980
core application as a sample deployed to

00:19:43,820 --> 00:19:48,200
a Linux container and I will talk about

00:19:45,980 --> 00:19:51,470
Windows in the end of my presentation as

00:19:48,200 --> 00:19:53,929
well so this diagram illustrate how this

00:19:51,470 --> 00:19:56,179
process work so inside cone foundry we

00:19:53,929 --> 00:19:59,149
have a dotnet process running and in

00:19:56,179 --> 00:20:02,749
order for dotnet debugger to be able to

00:19:59,149 --> 00:20:05,419
attached to attach to your application

00:20:02,749 --> 00:20:08,720
you need to use special software that

00:20:05,419 --> 00:20:11,840
can help you so in case of Linux

00:20:08,720 --> 00:20:16,759
containers we use the SD BG application

00:20:11,840 --> 00:20:19,340
that should be installed in the

00:20:16,759 --> 00:20:21,830
application container and the LT BG is

00:20:19,340 --> 00:20:24,700
responsible for monitoring your process

00:20:21,830 --> 00:20:30,159
and then we as dbg talks to debugger and

00:20:24,700 --> 00:20:34,039
actually is responsible for executing

00:20:30,159 --> 00:20:36,440
all the bugger comments in order for

00:20:34,039 --> 00:20:39,169
this to work we need a special file that

00:20:36,440 --> 00:20:44,590
is called lunch JSON and this file can

00:20:39,169 --> 00:20:47,809
be used to configure the back in vs quad

00:20:44,590 --> 00:20:50,480
it is possible to use a lunch json in

00:20:47,809 --> 00:20:53,840
via visual studio but in Visual Studio

00:20:50,480 --> 00:20:55,669
there is no native support for lounging

00:20:53,840 --> 00:20:58,009
so so you need to type a few comments

00:20:55,669 --> 00:21:00,499
from comment window if you want to do

00:20:58,009 --> 00:21:03,019
this but if somebody if interested you

00:21:00,499 --> 00:21:05,029
can connect to me after presentation I

00:21:03,019 --> 00:21:07,850
can show you how it works but from via

00:21:05,029 --> 00:21:10,340
Scott you just create these files there

00:21:07,850 --> 00:21:12,799
is a native support for this and what's

00:21:10,340 --> 00:21:15,590
important here important seen here is

00:21:12,799 --> 00:21:18,710
how we connect to remote debugger do you

00:21:15,590 --> 00:21:21,919
see that we use CF CLI there is a

00:21:18,710 --> 00:21:22,249
comment so it's called pipe program it's

00:21:21,919 --> 00:21:26,299
called

00:21:22,249 --> 00:21:27,360
SIA will CF and then pipe arguments so

00:21:26,299 --> 00:21:29,760
we UCF

00:21:27,360 --> 00:21:31,830
saij to go inside container then we

00:21:29,760 --> 00:21:35,580
specify name of our application and

00:21:31,830 --> 00:21:38,520
there C means common that we want to

00:21:35,580 --> 00:21:42,510
execute inside container and vs quad

00:21:38,520 --> 00:21:47,299
attach actual comment we said lunch vs d

00:21:42,510 --> 00:21:50,100
BG and attach to run in dotnet process

00:21:47,299 --> 00:21:51,809
in order for this to work obviously we

00:21:50,100 --> 00:21:54,600
need V and the BG to be installed

00:21:51,809 --> 00:21:58,020
alongside our net up and here is a

00:21:54,600 --> 00:22:00,809
sample of a docker file that can do this

00:21:58,020 --> 00:22:02,970
so what is done here is that we are

00:22:00,809 --> 00:22:05,280
using two containers the first one is

00:22:02,970 --> 00:22:07,350
used to build our app we execute publish

00:22:05,280 --> 00:22:11,220
comment here and the second container

00:22:07,350 --> 00:22:12,960
uses output of the first one and it runs

00:22:11,220 --> 00:22:16,520
our applications this is done in the

00:22:12,960 --> 00:22:20,040
last line here you see that we use we

00:22:16,520 --> 00:22:24,480
dynamically create URL to run our

00:22:20,040 --> 00:22:27,900
application and we also install VSD bgl

00:22:24,480 --> 00:22:32,400
on site with some other packages that

00:22:27,900 --> 00:22:35,460
are required for VSD BG to run inside

00:22:32,400 --> 00:22:37,410
container so after you done something

00:22:35,460 --> 00:22:39,750
like this this is rather simple docker

00:22:37,410 --> 00:22:42,240
file what you can do is to just use

00:22:39,750 --> 00:22:46,520
normal CF push to push your net

00:22:42,240 --> 00:22:50,460
application into a Linux container and

00:22:46,520 --> 00:22:53,370
here you specify name of your container

00:22:50,460 --> 00:22:56,700
you should put it in docker hub or your

00:22:53,370 --> 00:23:00,059
private docker registry and after you

00:22:56,700 --> 00:23:02,970
dance this obvious 2d a quad will show

00:23:00,059 --> 00:23:06,540
you a dialog similar to this here you

00:23:02,970 --> 00:23:08,910
can see all processes running on inside

00:23:06,540 --> 00:23:12,150
remote container so you can just pick

00:23:08,910 --> 00:23:14,220
the dotnet process the ones that you are

00:23:12,150 --> 00:23:17,090
interested in and basically you are done

00:23:14,220 --> 00:23:19,620
you connect it to the

00:23:17,090 --> 00:23:23,549
applications that is running inside

00:23:19,620 --> 00:23:27,899
container so on Windows we are when

00:23:23,549 --> 00:23:33,120
doing application dodges we previously

00:23:27,899 --> 00:23:37,919
most of the times what we did is to use

00:23:33,120 --> 00:23:41,899
some hacks to get inside VMs that it

00:23:37,919 --> 00:23:44,820
runs on Windows and usually we deploy a

00:23:41,899 --> 00:23:47,639
separate VM debugger VM in the same

00:23:44,820 --> 00:23:49,980
network and then I install visual studio

00:23:47,639 --> 00:23:52,619
owns this VM and use RDP to connect to

00:23:49,980 --> 00:23:56,820
this VM and the just use native Visual

00:23:52,619 --> 00:23:58,619
Studio debugging debugger to debug our

00:23:56,820 --> 00:24:00,899
applications so close father was not

00:23:58,619 --> 00:24:06,149
involved and actually this is not a cool

00:24:00,899 --> 00:24:08,129
solution because you need to access you

00:24:06,149 --> 00:24:08,820
need your operation team to help you

00:24:08,129 --> 00:24:11,159
with this

00:24:08,820 --> 00:24:14,159
just as a developer you can adjust

00:24:11,159 --> 00:24:17,669
deploy a VM in the same network as your

00:24:14,159 --> 00:24:23,970
Diego cells to the same network where

00:24:17,669 --> 00:24:27,259
your Diego cells are host but I advise

00:24:23,970 --> 00:24:29,909
you to attend tomorrow's keynote because

00:24:27,259 --> 00:24:34,679
tomorrow pivotal is going to announced a

00:24:29,909 --> 00:24:37,399
support for native debugger on Windows

00:24:34,679 --> 00:24:40,169
and that's very cool feature and you can

00:24:37,399 --> 00:24:43,919
use Visual Studio to attach to any

00:24:40,169 --> 00:24:46,619
remote process that runs inside called

00:24:43,919 --> 00:24:48,299
foundry so I'm very excited about this

00:24:46,619 --> 00:24:50,759
announcement and definitely want to

00:24:48,299 --> 00:24:55,019
check out how it works so I hope they'll

00:24:50,759 --> 00:24:58,279
show us a live demo ZZZ and a lot of

00:24:55,019 --> 00:25:05,279
things that I want to tell you is that

00:24:58,279 --> 00:25:07,110
actually I am so sometimes it's you are

00:25:05,279 --> 00:25:09,419
forced to use the bugger because you

00:25:07,110 --> 00:25:14,369
don't have any other way to troubleshoot

00:25:09,419 --> 00:25:16,590
an issue but if you have to rely heavily

00:25:14,369 --> 00:25:18,059
on the back especially in production

00:25:16,590 --> 00:25:21,230
this might indicate that you have

00:25:18,059 --> 00:25:25,679
problems with your architecture and

00:25:21,230 --> 00:25:27,010
problems with your login infrastructure

00:25:25,679 --> 00:25:28,660
probably you are not

00:25:27,010 --> 00:25:30,910
enough information and you can

00:25:28,660 --> 00:25:33,910
troubleshoot the problem with the back

00:25:30,910 --> 00:25:39,010
is that it's a spent state so actually

00:25:33,910 --> 00:25:41,610
you cannot debug application like life

00:25:39,010 --> 00:25:44,140
applications that works under loaded

00:25:41,610 --> 00:25:46,240
because as soon as you attach to a

00:25:44,140 --> 00:25:48,130
process the process is stopped and

00:25:46,240 --> 00:25:51,070
another problems that we cannot use the

00:25:48,130 --> 00:25:56,290
blogger for post issues if somebody

00:25:51,070 --> 00:25:59,530
complains about your application and you

00:25:56,290 --> 00:26:05,200
cannot reproduce the issue you basically

00:25:59,530 --> 00:26:07,420
don't have a lot of opportunities to

00:26:05,200 --> 00:26:10,270
troubleshoot it using debugger so in

00:26:07,420 --> 00:26:14,530
such situation works are much more

00:26:10,270 --> 00:26:18,190
helpful for troubleshooting and if you

00:26:14,530 --> 00:26:19,830
want to see how all this work in

00:26:18,190 --> 00:26:22,270
practice you can check out my

00:26:19,830 --> 00:26:24,460
repositories that implements the sample

00:26:22,270 --> 00:26:29,440
applications that I have just shown it's

00:26:24,460 --> 00:26:34,270
very simple default dotnet core

00:26:29,440 --> 00:26:38,650
application that I that has all those

00:26:34,270 --> 00:26:42,310
features that I have just shown you so

00:26:38,650 --> 00:26:45,640
you can clone it and play around with it

00:26:42,310 --> 00:26:50,050
if you want so basically that's it that

00:26:45,640 --> 00:26:51,880
I want to share with you and if you have

00:26:50,050 --> 00:26:54,520
any questions you're more than welcome

00:26:51,880 --> 00:26:56,820
to ask them I will try to to answer all

00:26:54,520 --> 00:26:56,820
of them

00:26:57,280 --> 00:27:04,400
[Applause]

00:27:15,929 --> 00:27:24,249
aha you mean debugged you're in

00:27:19,690 --> 00:27:27,969
development so not look as much as

00:27:24,249 --> 00:27:31,299
possible but instead I recommend to when

00:27:27,969 --> 00:27:33,969
login Samson just have clear picture in

00:27:31,299 --> 00:27:36,339
mind why you are login exist in what

00:27:33,969 --> 00:27:44,459
case you want to log this

00:27:36,339 --> 00:27:47,589
so I recommend log in such seals that

00:27:44,459 --> 00:27:49,570
can't be locked transparently for your

00:27:47,589 --> 00:27:52,179
application such as I mentioned

00:27:49,570 --> 00:27:55,839
previously all request data all database

00:27:52,179 --> 00:27:58,779
data it it ensures consistent logon

00:27:55,839 --> 00:28:01,209
format but I cannot say I recommend lock

00:27:58,779 --> 00:28:03,820
as much as possible because I see a lot

00:28:01,209 --> 00:28:06,940
of times code like this like one line of

00:28:03,820 --> 00:28:09,219
code and ok we are calculating parameter

00:28:06,940 --> 00:28:11,940
X then we are talking to database and

00:28:09,219 --> 00:28:17,079
this makes your quote less readable

00:28:11,940 --> 00:28:19,179
instead it's much more useful to half of

00:28:17,079 --> 00:28:21,669
wrapper that will lock every since that

00:28:19,179 --> 00:28:24,669
you lock to database and get rid of this

00:28:21,669 --> 00:28:28,690
login quote from your business logic so

00:28:24,669 --> 00:28:31,209
that's how I see a good debugger

00:28:28,690 --> 00:28:33,899
implement good login implementation in

00:28:31,209 --> 00:28:33,899
the application

00:28:46,040 --> 00:28:49,330
dynamically turn what

00:28:54,080 --> 00:29:02,600
oh yeah that's a perfect question yeah

00:28:59,270 --> 00:29:08,360
so most of the loggers support doing

00:29:02,600 --> 00:29:10,520
this and one common way of doing this is

00:29:08,360 --> 00:29:11,950
provide you a lager configurations that

00:29:10,520 --> 00:29:14,270
externalize from your application

00:29:11,950 --> 00:29:16,730
instead of doing the scenes that I have

00:29:14,270 --> 00:29:21,170
been doing like hardcore the logger

00:29:16,730 --> 00:29:23,090
level you can use JSON file to configure

00:29:21,170 --> 00:29:27,140
your logger levels for different

00:29:23,090 --> 00:29:31,460
namespaces and also some logger

00:29:27,140 --> 00:29:37,280
libraries support configuration based on

00:29:31,460 --> 00:29:39,590
environment variables so you can set

00:29:37,280 --> 00:29:41,120
environment variable restart your

00:29:39,590 --> 00:29:43,070
application if you have a lot of

00:29:41,120 --> 00:29:45,350
instances then they will be restarted

00:29:43,070 --> 00:29:48,380
one by one so there will be no downtime

00:29:45,350 --> 00:29:51,130
and you can dynamically adjust a login

00:29:48,380 --> 00:29:55,220
level but importance in here is that

00:29:51,130 --> 00:29:57,350
okay if we are logging a lot how we can

00:29:55,220 --> 00:30:00,200
avoid impacting the performance of our

00:29:57,350 --> 00:30:04,430
application so first of all in your a

00:30:00,200 --> 00:30:07,760
logger Grenadier in such tools like lk

00:30:04,430 --> 00:30:11,030
you can set up a special process that

00:30:07,760 --> 00:30:16,100
will clean up all logs and you can set

00:30:11,030 --> 00:30:18,860
up policy for example all locks older

00:30:16,100 --> 00:30:22,460
than one month should be deleted for

00:30:18,860 --> 00:30:26,330
example we keep error locks for forever

00:30:22,460 --> 00:30:28,730
and we delete the backlogs as soon as

00:30:26,330 --> 00:30:31,370
one week for example and this can be

00:30:28,730 --> 00:30:35,570
very useful but in terms of slowing down

00:30:31,370 --> 00:30:40,520
application itself well I don't think it

00:30:35,570 --> 00:30:42,200
has worried like extremely large impact

00:30:40,520 --> 00:30:44,360
on application because a lot of logger

00:30:42,200 --> 00:30:47,240
libraries introduce intermediate buffers

00:30:44,360 --> 00:30:50,000
so instead of just writing each time to

00:30:47,240 --> 00:30:52,040
log file or to standard out they lock

00:30:50,000 --> 00:30:56,120
everything in the file and then flash it

00:30:52,040 --> 00:30:59,060
periodically so I never seen problems

00:30:56,120 --> 00:31:03,580
with application performance because we

00:30:59,060 --> 00:31:03,580
are login to too much

00:31:07,750 --> 00:31:11,830
all right anything else

00:31:12,550 --> 00:31:22,230
questions are ya alright thanks

00:31:20,270 --> 00:31:27,590
everybody

00:31:22,230 --> 00:31:27,590

YouTube URL: https://www.youtube.com/watch?v=ZUGNrwxtyaE


