Title: Delivering a Production CF Environment Using BOSH
Publication date: 2016-05-30
Playlist: BOSH Day - Cloud Foundry Summit NA 2016
Description: 
	Julian Fischer, CEO of Anynines presents "Delivering a Production CF Environment Using BOSH."
Captions: 
	00:00:00,030 --> 00:00:04,680
all right here we go so yeah welcome to

00:00:02,790 --> 00:00:06,390
this talk about delivering a production

00:00:04,680 --> 00:00:08,639
clarifying environment with Bosch my

00:00:06,390 --> 00:00:11,370
name is Julian Fisher as mentioned I'm

00:00:08,639 --> 00:00:13,710
CEO of a platform company called any

00:00:11,370 --> 00:00:17,130
nines we've we are consultancy around

00:00:13,710 --> 00:00:19,590
cloud foundry so we started running

00:00:17,130 --> 00:00:23,670
cloud foundry back in 2013 offering a

00:00:19,590 --> 00:00:26,849
public cloud foundry in Europe so we've

00:00:23,670 --> 00:00:28,619
been through a learning curve in the

00:00:26,849 --> 00:00:31,320
last years and I'd like to share some of

00:00:28,619 --> 00:00:35,130
the experience with you today so sadly

00:00:31,320 --> 00:00:37,800
it's only a few 20 minutes and you know

00:00:35,130 --> 00:00:39,059
you can't just hit my play button and I

00:00:37,800 --> 00:00:41,160
will keep on talking about this for

00:00:39,059 --> 00:00:43,200
hours so the most challenging part

00:00:41,160 --> 00:00:46,710
actually was to cut out the right pieces

00:00:43,200 --> 00:00:50,489
so that you get a coherent coherent

00:00:46,710 --> 00:00:52,440
picture or out about what production

00:00:50,489 --> 00:00:55,739
readiness actually means so let's give

00:00:52,440 --> 00:00:59,760
it a try so the agenda for today is

00:00:55,739 --> 00:01:02,609
basically when I once activated my

00:00:59,760 --> 00:01:04,619
trigger so the agenda is going to be a

00:01:02,609 --> 00:01:06,750
few words having a few words about Bosch

00:01:04,619 --> 00:01:08,700
I think Bosch has been introduced very

00:01:06,750 --> 00:01:12,320
well in several Talk's today so we just

00:01:08,700 --> 00:01:15,240
have a few emotional words about wash

00:01:12,320 --> 00:01:18,240
then we are looking at how to actually

00:01:15,240 --> 00:01:21,540
deploy a production Trade Cloud Foundry

00:01:18,240 --> 00:01:24,150
runtime and closing with some words

00:01:21,540 --> 00:01:26,759
about the data services part as it is

00:01:24,150 --> 00:01:32,040
part of a cloud foundry in an essential

00:01:26,759 --> 00:01:34,409
part so looking at Bosch you know any

00:01:32,040 --> 00:01:36,990
nines is a brand so the company behind

00:01:34,409 --> 00:01:40,350
that other tech it exists since 2008 and

00:01:36,990 --> 00:01:43,079
we followed a business model comparable

00:01:40,350 --> 00:01:46,829
to engine yacht where we take physical

00:01:43,079 --> 00:01:50,250
and virtual servers and automate the

00:01:46,829 --> 00:01:53,549
hell out of them using ops code chef so

00:01:50,250 --> 00:01:55,979
since 2010 we then we did a lot of chef

00:01:53,549 --> 00:01:58,710
automation so when we actually started

00:01:55,979 --> 00:02:01,320
using cloud foundry our guys were like

00:01:58,710 --> 00:02:02,219
oh yeah a new automation technology do

00:02:01,320 --> 00:02:04,770
we really need it

00:02:02,219 --> 00:02:08,640
why the hell Bosch and so there were

00:02:04,770 --> 00:02:10,229
some resistance and that resistance was

00:02:08,640 --> 00:02:11,520
mostly about you know changing a tool

00:02:10,229 --> 00:02:13,640
set and because it's a different

00:02:11,520 --> 00:02:16,370
approach

00:02:13,640 --> 00:02:18,530
so after using Bosch to deploy cloud

00:02:16,370 --> 00:02:21,200
foundry and getting used to it a little

00:02:18,530 --> 00:02:24,950
more people slowly changed their mind

00:02:21,200 --> 00:02:27,020
and after recognizing that you can do so

00:02:24,950 --> 00:02:29,360
much more with Bosch then just deploy

00:02:27,020 --> 00:02:31,580
cloud foundry they actually fell in love

00:02:29,360 --> 00:02:34,520
with it so today is the preferred

00:02:31,580 --> 00:02:37,810
automation technology over at our

00:02:34,520 --> 00:02:41,150
company so why is that

00:02:37,810 --> 00:02:43,730
obviously cloud foundry has as a

00:02:41,150 --> 00:02:47,060
distributed system large challenges

00:02:43,730 --> 00:02:49,790
around maintaining state deploying loads

00:02:47,060 --> 00:02:53,570
of virtual machines but also come

00:02:49,790 --> 00:02:56,570
configuring a distributed system so

00:02:53,570 --> 00:02:58,420
making up the making those components

00:02:56,570 --> 00:03:01,270
actually collaborate and work together

00:02:58,420 --> 00:03:03,860
so the challenge of the automation tool

00:03:01,270 --> 00:03:05,870
toolchain here is not only about you

00:03:03,860 --> 00:03:08,720
know spinning up a virtual machine and

00:03:05,870 --> 00:03:11,540
configuring a database but also taking

00:03:08,720 --> 00:03:14,270
care of the lifecycle of such a system

00:03:11,540 --> 00:03:17,120
because you want to update you wanna you

00:03:14,270 --> 00:03:20,750
want to scale you wanna do loads of

00:03:17,120 --> 00:03:23,570
things lots of things during the

00:03:20,750 --> 00:03:25,850
operation of such an operating of such a

00:03:23,570 --> 00:03:28,250
distributed system and I've not seen

00:03:25,850 --> 00:03:32,740
something that really comes close to

00:03:28,250 --> 00:03:35,990
boss capabilities to deal with those

00:03:32,740 --> 00:03:37,430
challenges with a single toolset I mean

00:03:35,990 --> 00:03:39,680
you can use several and put them

00:03:37,430 --> 00:03:41,840
together and have a similar user

00:03:39,680 --> 00:03:43,820
experience but what we really found

00:03:41,840 --> 00:03:46,100
appealing is that it every it has

00:03:43,820 --> 00:03:48,140
everything we actually need together

00:03:46,100 --> 00:03:50,990
fair enough there's a learning curve

00:03:48,140 --> 00:03:53,510
that's a little more let's say it takes

00:03:50,990 --> 00:03:57,320
more effort to really get going once but

00:03:53,510 --> 00:04:02,320
after that I think the values really pay

00:03:57,320 --> 00:04:05,150
off alright so one of the very

00:04:02,320 --> 00:04:06,800
interesting things is the infrastructure

00:04:05,150 --> 00:04:10,190
independence that's built into bosh and

00:04:06,800 --> 00:04:13,670
in any nines we've felt the impact of

00:04:10,190 --> 00:04:16,850
that firsthand we started the past

00:04:13,670 --> 00:04:20,090
offering as a proof of concept because

00:04:16,850 --> 00:04:23,270
we've done OpenStack before that so with

00:04:20,090 --> 00:04:26,040
OpenStack Diablo we thought we could

00:04:23,270 --> 00:04:27,900
become an infrastructure company and

00:04:26,040 --> 00:04:30,750
went through the really belly of

00:04:27,900 --> 00:04:32,639
darkness with OpenStack learning that it

00:04:30,750 --> 00:04:35,130
is a nightmare to operate and everything

00:04:32,639 --> 00:04:36,960
so we want to ensure that we are not

00:04:35,130 --> 00:04:39,930
repeating that story with Cloud Foundry

00:04:36,960 --> 00:04:42,030
and started on a rented VMware is at

00:04:39,930 --> 00:04:44,910
that time coming from a BMS background

00:04:42,030 --> 00:04:47,970
Cloud Foundry seemed to be most stable

00:04:44,910 --> 00:04:51,960
on on VMware so we bootstrapped Cloud

00:04:47,970 --> 00:04:54,060
Foundry in just a few weeks and after a

00:04:51,960 --> 00:04:57,840
while we recognized that our open stack

00:04:54,060 --> 00:05:00,050
could be good enough to run a cloth on

00:04:57,840 --> 00:05:03,360
his top of it so we took the entire

00:05:00,050 --> 00:05:06,060
platform with more than 200 FS 150 apps

00:05:03,360 --> 00:05:08,700
at that time and my credited to

00:05:06,060 --> 00:05:11,760
OpenStack with less than 30 minute down

00:05:08,700 --> 00:05:13,470
times so downtime it was two weeks of

00:05:11,760 --> 00:05:16,860
preparation but the downtime of the apps

00:05:13,470 --> 00:05:18,419
were like kept to a minimum and the

00:05:16,860 --> 00:05:21,120
reason why that was possible is because

00:05:18,419 --> 00:05:24,150
Bosch abstraction from from the

00:05:21,120 --> 00:05:27,050
infrastructure we kept our tool chain in

00:05:24,150 --> 00:05:30,690
a way that will keep those promises and

00:05:27,050 --> 00:05:32,700
recently where our OpenStack finally

00:05:30,690 --> 00:05:34,350
melted down and we recognized that we

00:05:32,700 --> 00:05:36,510
shouldn't run OpenStack at all

00:05:34,350 --> 00:05:39,539
we migrate it to Amazon and that

00:05:36,510 --> 00:05:41,660
transition also was just possible

00:05:39,539 --> 00:05:44,669
because of the infrastructure

00:05:41,660 --> 00:05:46,320
independence coming with portion so we

00:05:44,669 --> 00:05:48,210
know how to run Cloud Foundry we know

00:05:46,320 --> 00:05:51,150
how to break OpenStack we know how to

00:05:48,210 --> 00:05:54,660
reassemble everything and most of that's

00:05:51,150 --> 00:05:56,880
only possible because of quash so we

00:05:54,660 --> 00:05:59,729
have a data service we'd built on top of

00:05:56,880 --> 00:06:01,680
of Bosch and you know customers were

00:05:59,729 --> 00:06:03,720
asking whether they could you know use

00:06:01,680 --> 00:06:06,360
that on-premise so they have a different

00:06:03,720 --> 00:06:09,479
Linux operating system they have to use

00:06:06,360 --> 00:06:11,400
because of internal policies so with

00:06:09,479 --> 00:06:13,410
chef I you know that would trigger a

00:06:11,400 --> 00:06:14,729
nightmare because you have to go through

00:06:13,410 --> 00:06:17,430
all the cookbooks and ensure the

00:06:14,729 --> 00:06:20,250
consistency between those different

00:06:17,430 --> 00:06:21,630
operating systems because you know every

00:06:20,250 --> 00:06:23,340
operating system when their package

00:06:21,630 --> 00:06:25,530
manager ends up having a different

00:06:23,340 --> 00:06:27,630
directory structure and with Bosch those

00:06:25,530 --> 00:06:30,810
problems are not really present because

00:06:27,630 --> 00:06:34,289
it has a proper task on abstracting from

00:06:30,810 --> 00:06:37,110
all these things we're looking at the

00:06:34,289 --> 00:06:38,440
data service part one thing that's also

00:06:37,110 --> 00:06:40,330
very important

00:06:38,440 --> 00:06:41,890
comes also in handy with the Cloud

00:06:40,330 --> 00:06:43,450
Foundry runtime is that you have a

00:06:41,890 --> 00:06:48,160
separation between the abstract

00:06:43,450 --> 00:06:51,760
blueprint of describing the the

00:06:48,160 --> 00:06:53,380
distributed system but also this having

00:06:51,760 --> 00:06:56,380
it separate from the actual deployment

00:06:53,380 --> 00:06:57,970
because you may know that sandbox

00:06:56,380 --> 00:06:59,230
environment is most likely very

00:06:57,970 --> 00:07:02,260
different from a production environment

00:06:59,230 --> 00:07:04,360
but you have you want to have one set of

00:07:02,260 --> 00:07:06,430
Bosch releases and and then actually

00:07:04,360 --> 00:07:09,280
keep them separate from the deployments

00:07:06,430 --> 00:07:10,690
which also comes very handy when dealing

00:07:09,280 --> 00:07:13,510
with data services where you want to

00:07:10,690 --> 00:07:16,840
have several data service plans and

00:07:13,510 --> 00:07:18,460
resulting you know deployments should be

00:07:16,840 --> 00:07:23,190
should be differently but more to that a

00:07:18,460 --> 00:07:25,930
little later when whenever you have a

00:07:23,190 --> 00:07:28,510
clustered component such as the da and

00:07:25,930 --> 00:07:30,730
you are performing upgrades you also

00:07:28,510 --> 00:07:33,940
want to be assure that you don't take

00:07:30,730 --> 00:07:35,530
down all das at the same time so with a

00:07:33,940 --> 00:07:37,270
lot of automation technologies you can

00:07:35,530 --> 00:07:40,450
do that within your cookbooks and health

00:07:37,270 --> 00:07:42,250
you know some smartness in there but

00:07:40,450 --> 00:07:46,300
with boss you just don't have to do that

00:07:42,250 --> 00:07:48,280
it's it's done for you and also it's not

00:07:46,300 --> 00:07:50,260
a fire-and-forget automation where you

00:07:48,280 --> 00:07:52,990
spin a virtual machine up and maybe

00:07:50,260 --> 00:07:55,419
perform some changes around that but it

00:07:52,990 --> 00:07:57,580
really takes the responsibility to take

00:07:55,419 --> 00:08:00,250
care of the babies it actually spawned

00:07:57,580 --> 00:08:03,970
by taking care of the jobs is actually

00:08:00,250 --> 00:08:06,430
spawned and will recreate jobs and and

00:08:03,970 --> 00:08:08,530
virtual machines for you so obviously

00:08:06,430 --> 00:08:10,960
you've been through that learning but

00:08:08,530 --> 00:08:12,490
take all that together like the vm

00:08:10,960 --> 00:08:16,270
provisioning the management of

00:08:12,490 --> 00:08:18,730
persistent disks and and you will see

00:08:16,270 --> 00:08:20,530
that there's actually the whole

00:08:18,730 --> 00:08:22,870
lifecycle of the off of such a

00:08:20,530 --> 00:08:26,260
distributed system already covered and

00:08:22,870 --> 00:08:28,570
all you have to do is use bosch it just

00:08:26,260 --> 00:08:30,700
takes away a lot of functionality you

00:08:28,570 --> 00:08:36,550
have otherwise to rebuild and put into

00:08:30,700 --> 00:08:39,640
your code so as I said Bosch we fell in

00:08:36,550 --> 00:08:42,310
love with it and it has proven to be the

00:08:39,640 --> 00:08:43,810
right choice many many times so let's

00:08:42,310 --> 00:08:45,730
get back to the cloud foundry topic

00:08:43,810 --> 00:08:47,410
because we are here about you know

00:08:45,730 --> 00:08:50,110
bringing production and cloud foundry

00:08:47,410 --> 00:08:51,430
together with Bosch so what does a

00:08:50,110 --> 00:08:56,170
production cloud found

00:08:51,430 --> 00:08:57,850
mean yeah thank you very much so there

00:08:56,170 --> 00:09:02,440
should be there should be a mobile off

00:08:57,850 --> 00:09:04,149
sign somewhere however production what

00:09:02,440 --> 00:09:06,190
does it mean I mean there's so many

00:09:04,149 --> 00:09:08,080
great companies here they they may

00:09:06,190 --> 00:09:09,850
operate Plata Cloud Foundry had even a

00:09:08,080 --> 00:09:12,040
larger scale than we do so I'm pretty

00:09:09,850 --> 00:09:13,779
sure that this will actually mean

00:09:12,040 --> 00:09:18,190
something very different to you than it

00:09:13,779 --> 00:09:20,620
means for us so what I can say as a

00:09:18,190 --> 00:09:22,779
common denominator is that everything

00:09:20,620 --> 00:09:25,660
fails I mean you've seen so many

00:09:22,779 --> 00:09:30,520
distributed systems promising you highly

00:09:25,660 --> 00:09:33,520
high availability and but haven't you

00:09:30,520 --> 00:09:36,730
all seen these systems fail as well I

00:09:33,520 --> 00:09:39,130
mean I've seen every system fail in in

00:09:36,730 --> 00:09:41,950
my career so I think it's just natural

00:09:39,130 --> 00:09:43,870
that things break so and from our

00:09:41,950 --> 00:09:46,570
experience our infrastructure was the

00:09:43,870 --> 00:09:49,570
major source of all failure whether it

00:09:46,570 --> 00:09:51,880
was because a physical server failed or

00:09:49,570 --> 00:09:53,770
the infrastructure layer itself had some

00:09:51,880 --> 00:09:58,230
issues it's just these technologies are

00:09:53,770 --> 00:10:01,540
not perfect so failures occur so when

00:09:58,230 --> 00:10:04,839
asking or defining production readiness

00:10:01,540 --> 00:10:07,600
in our team we came to a definition that

00:10:04,839 --> 00:10:10,000
is as easy as that that is a system is

00:10:07,600 --> 00:10:12,730
production ready if nobody has to get up

00:10:10,000 --> 00:10:14,830
when ordinary failures occur so of

00:10:12,730 --> 00:10:17,890
course the magic word here is the word

00:10:14,830 --> 00:10:21,070
ordinary so what is ordinary it is when

00:10:17,890 --> 00:10:24,220
failures actually happen within a veil

00:10:21,070 --> 00:10:29,560
ability zone so we come to that concept

00:10:24,220 --> 00:10:32,589
a little a little later the idea is to

00:10:29,560 --> 00:10:34,450
change the expectation of we can get rid

00:10:32,589 --> 00:10:37,810
of failures at all even when using

00:10:34,450 --> 00:10:39,820
clustering we just scope a certain set

00:10:37,810 --> 00:10:42,730
of failures and we expect them to happen

00:10:39,820 --> 00:10:45,160
but it will always it will always the

00:10:42,730 --> 00:10:47,800
case that failures will happen that we

00:10:45,160 --> 00:10:52,390
don't expect so how can we actually deal

00:10:47,800 --> 00:10:54,910
with that so one approach to to deal

00:10:52,390 --> 00:10:57,250
with this is to design your system to

00:10:54,910 --> 00:10:59,650
fail again you'll have to scope it in

00:10:57,250 --> 00:11:02,260
order to make things work one of the

00:10:59,650 --> 00:11:04,670
most essential things and I actually

00:11:02,260 --> 00:11:07,220
thought that nobody really needs

00:11:04,670 --> 00:11:10,070
people that but I've seen customer doing

00:11:07,220 --> 00:11:12,730
that horribly horribly wrong is that you

00:11:10,070 --> 00:11:16,430
need infrastructure availability zones

00:11:12,730 --> 00:11:18,650
so whenever you build the infrastructure

00:11:16,430 --> 00:11:21,620
you should be aware that they are that

00:11:18,650 --> 00:11:24,920
there's um should ensure that there's at

00:11:21,620 --> 00:11:27,200
least three different availability zones

00:11:24,920 --> 00:11:29,270
they should be separated as much as they

00:11:27,200 --> 00:11:31,570
can so that they are protected from

00:11:29,270 --> 00:11:36,740
physical physical events such as fire

00:11:31,570 --> 00:11:40,610
you know dedicated dedicated energy

00:11:36,740 --> 00:11:43,460
sources networking different ideally

00:11:40,610 --> 00:11:45,380
also service from different batches it's

00:11:43,460 --> 00:11:48,350
one of the things we just recently

00:11:45,380 --> 00:11:50,090
experienced as we had all the OpenStack

00:11:48,350 --> 00:11:53,300
service we have they were from one batch

00:11:50,090 --> 00:11:56,180
so we had a issue with the internal

00:11:53,300 --> 00:11:58,280
network driver and it took down 20 out

00:11:56,180 --> 00:12:00,980
of 24 servers at the same time with the

00:11:58,280 --> 00:12:02,960
kernel panic so at that moment your

00:12:00,980 --> 00:12:03,770
availability zones won't Werth won't be

00:12:02,960 --> 00:12:06,230
worth anything

00:12:03,770 --> 00:12:11,690
just because the problem you have is at

00:12:06,230 --> 00:12:13,490
a entire infrastructure scale so ensure

00:12:11,690 --> 00:12:16,100
that you have three availability zones

00:12:13,490 --> 00:12:18,500
they should be separate separate

00:12:16,100 --> 00:12:20,300
switches separate networks and at the

00:12:18,500 --> 00:12:22,490
same time you have to ensure that there

00:12:20,300 --> 00:12:25,040
are Lowell Network latencies between

00:12:22,490 --> 00:12:28,640
them so let's have a look at those two

00:12:25,040 --> 00:12:30,560
attributes three why the number three is

00:12:28,640 --> 00:12:32,690
just because whenever you use a quorum

00:12:30,560 --> 00:12:34,700
based service that one available is

00:12:32,690 --> 00:12:36,890
availability zone goes down you still

00:12:34,700 --> 00:12:40,430
have a majority and you know you'll be

00:12:36,890 --> 00:12:42,380
ready to create a quorum and the low

00:12:40,430 --> 00:12:44,990
latency is important because whenever

00:12:42,380 --> 00:12:46,970
you use let's say a synchronous

00:12:44,990 --> 00:12:49,100
replicator database cluster for example

00:12:46,970 --> 00:12:52,430
that you don't run into a split brain

00:12:49,100 --> 00:12:56,270
situation just because there's you know

00:12:52,430 --> 00:12:58,910
let work network latency so how do you

00:12:56,270 --> 00:13:01,040
actually do that I mean building that's

00:12:58,910 --> 00:13:02,690
surely infrastructure specific but all

00:13:01,040 --> 00:13:06,220
the infrastructure software's out there

00:13:02,690 --> 00:13:10,400
they are somehow managed to do that so

00:13:06,220 --> 00:13:12,890
how do you do that with Bosh before

00:13:10,400 --> 00:13:16,010
before they actually the recent versions

00:13:12,890 --> 00:13:18,290
you had to create resource pools

00:13:16,010 --> 00:13:21,320
and put the availability zones in the

00:13:18,290 --> 00:13:24,080
resource pool and then you assigned the

00:13:21,320 --> 00:13:28,120
resource pool to a job which was kind of

00:13:24,080 --> 00:13:30,260
messy because you you mix you know

00:13:28,120 --> 00:13:32,600
availability zone and resource pools

00:13:30,260 --> 00:13:34,370
everything in your deployment manifest

00:13:32,600 --> 00:13:36,140
which makes them a little verbose so

00:13:34,370 --> 00:13:39,890
with recent versions they luckily

00:13:36,140 --> 00:13:42,080
redesigned it to use a cloud config file

00:13:39,890 --> 00:13:44,450
where you can separately handle all

00:13:42,080 --> 00:13:46,370
those you know infrastructure specific

00:13:44,450 --> 00:13:48,890
configuration so you have your

00:13:46,370 --> 00:13:53,420
availability zones being described in a

00:13:48,890 --> 00:13:55,760
separate in a separate file so you can

00:13:53,420 --> 00:13:58,610
then refer just to the availability

00:13:55,760 --> 00:14:01,610
zones which it's kind of neat thing and

00:13:58,610 --> 00:14:04,270
makes things more readable so with that

00:14:01,610 --> 00:14:08,090
in mind your infrastructure being

00:14:04,270 --> 00:14:09,800
structured well and your have the

00:14:08,090 --> 00:14:11,930
possibility on the level of posh to

00:14:09,800 --> 00:14:14,480
distribute virtual machines across

00:14:11,930 --> 00:14:17,570
availability zones and therefore

00:14:14,480 --> 00:14:19,900
influence their placement we can look at

00:14:17,570 --> 00:14:24,430
how to make a production Cloud Foundry

00:14:19,900 --> 00:14:27,740
again this is more an example as it is

00:14:24,430 --> 00:14:29,900
you know if a precise science at the

00:14:27,740 --> 00:14:33,800
moment you might be different for you

00:14:29,900 --> 00:14:36,560
but the runtime looking at the runtime

00:14:33,800 --> 00:14:39,590
there's a generic strategy you can

00:14:36,560 --> 00:14:41,960
actually apply to all distributed

00:14:39,590 --> 00:14:43,940
systems you want to you want to operate

00:14:41,960 --> 00:14:45,890
most important thing is that you

00:14:43,940 --> 00:14:48,260
eliminate all seeing a point of failures

00:14:45,890 --> 00:14:50,480
because whenever one availability zone

00:14:48,260 --> 00:14:53,150
goes down you can be sure that every

00:14:50,480 --> 00:14:56,420
single team could be affected by chance

00:14:53,150 --> 00:14:58,340
could be that it's luckily not exactly

00:14:56,420 --> 00:15:01,250
that virtual machine but it could be so

00:14:58,340 --> 00:15:03,650
how do you do defend yourself is you

00:15:01,250 --> 00:15:05,960
plus everything at least three replicas

00:15:03,650 --> 00:15:10,430
and spread across three availability

00:15:05,960 --> 00:15:12,080
zones so the generic strategy is that

00:15:10,430 --> 00:15:14,660
you create a list of system components

00:15:12,080 --> 00:15:16,160
and their dependencies and you go you go

00:15:14,660 --> 00:15:17,440
through every component and check

00:15:16,160 --> 00:15:20,720
whether it is a single point of failure

00:15:17,440 --> 00:15:23,390
you identify this component you dig into

00:15:20,720 --> 00:15:26,750
it and you'll just find out whether you

00:15:23,390 --> 00:15:28,400
can whether it can be clustered and if

00:15:26,750 --> 00:15:28,860
it can be clustered your cluster it may

00:15:28,400 --> 00:15:30,210
be what

00:15:28,860 --> 00:15:32,340
take some effort maybe you have to

00:15:30,210 --> 00:15:35,130
change your boss release or whatever but

00:15:32,340 --> 00:15:35,880
it would definitely definitely save you

00:15:35,130 --> 00:15:38,580
some time

00:15:35,880 --> 00:15:40,170
so if you find yourself identifying a

00:15:38,580 --> 00:15:42,090
component you cannot cluster I

00:15:40,170 --> 00:15:44,640
definitely prepare myself for night

00:15:42,090 --> 00:15:46,380
shifts because you'll have to get up and

00:15:44,640 --> 00:15:48,810
you know fix things in the middle of the

00:15:46,380 --> 00:15:53,310
night when exactly those components went

00:15:48,810 --> 00:15:55,650
down so looking at a cloud foundry in

00:15:53,310 --> 00:15:58,950
our run time a few months ago we found

00:15:55,650 --> 00:16:01,980
ourselves because we bootstrap from

00:15:58,950 --> 00:16:04,080
early versions that we have a plop store

00:16:01,980 --> 00:16:06,830
based on NFS which is a bad idea because

00:16:04,080 --> 00:16:10,980
NFS doesn't scale it's not redundant and

00:16:06,830 --> 00:16:13,500
we have we had a post press behind the

00:16:10,980 --> 00:16:16,980
cloud controller database as well as the

00:16:13,500 --> 00:16:21,330
UAE as a single point of failure so what

00:16:16,980 --> 00:16:23,400
what do we do to get rid of that the

00:16:21,330 --> 00:16:24,840
first step we did were we made we've

00:16:23,400 --> 00:16:26,910
made some pull requests against the

00:16:24,840 --> 00:16:29,100
cloud control a few years back so that

00:16:26,910 --> 00:16:31,440
we can use OpenStack Swift as a blob

00:16:29,100 --> 00:16:35,420
store or you know as an alternative

00:16:31,440 --> 00:16:38,640
Amazon s3 and also we took some time to

00:16:35,420 --> 00:16:43,890
cluster a Postgres and use use it as a

00:16:38,640 --> 00:16:45,690
database for Cloud Controller and UAE so

00:16:43,890 --> 00:16:48,840
it's worth looking at that component in

00:16:45,690 --> 00:16:51,260
in more detail to be mentioned that in

00:16:48,840 --> 00:16:54,510
the meantime there is a very nice

00:16:51,260 --> 00:16:56,340
Postgres sorry a very nice boat release

00:16:54,510 --> 00:16:59,340
of my sequel Galera out there I think

00:16:56,340 --> 00:17:01,110
it's made meet by pivot all right so

00:16:59,340 --> 00:17:04,920
this can be an alternative to the

00:17:01,110 --> 00:17:06,540
Postgres solution but in our case we

00:17:04,920 --> 00:17:08,970
already had Postgres on the map and

00:17:06,540 --> 00:17:10,860
spend some time and it's a good example

00:17:08,970 --> 00:17:14,010
to see how you actually use Cloud

00:17:10,860 --> 00:17:17,820
Foundry Cloud Foundry spot in in the

00:17:14,010 --> 00:17:19,740
context of you know making something a

00:17:17,820 --> 00:17:21,750
single point of failure and turn it into

00:17:19,740 --> 00:17:24,930
a cluster so we'll walk through that

00:17:21,750 --> 00:17:27,810
example to learn a little on what are

00:17:24,930 --> 00:17:30,150
what kind of challenges you might be

00:17:27,810 --> 00:17:34,290
facing during a journey like that

00:17:30,150 --> 00:17:36,270
so the desired goal was to have a three

00:17:34,290 --> 00:17:37,890
node database cluster it's a little

00:17:36,270 --> 00:17:41,070
small I'm sorry

00:17:37,890 --> 00:17:42,630
with cloud control and UA databases

00:17:41,070 --> 00:17:47,370
being spread across three databases

00:17:42,630 --> 00:17:49,800
across three availability zones so I

00:17:47,370 --> 00:17:50,670
gave a talk about the Postgres cluster

00:17:49,800 --> 00:17:52,470
in more detail

00:17:50,670 --> 00:17:54,990
but a few things should be mentioned for

00:17:52,470 --> 00:17:57,750
those who haven't been able to attend

00:17:54,990 --> 00:17:59,910
that of course we deployed and

00:17:57,750 --> 00:18:03,060
monitoring that cluster by by using

00:17:59,910 --> 00:18:05,250
Bosch so you've got the sports director

00:18:03,060 --> 00:18:07,950
and it's going to deploy three virtual

00:18:05,250 --> 00:18:12,330
machines for you and you know turn that

00:18:07,950 --> 00:18:13,710
into a cluster so as you can see there

00:18:12,330 --> 00:18:15,120
are several components running on each

00:18:13,710 --> 00:18:18,060
virtual machine one of which of course

00:18:15,120 --> 00:18:20,370
is the boss agent and there are two

00:18:18,060 --> 00:18:22,380
other they're two other components the

00:18:20,370 --> 00:18:25,380
replication manager as well as the

00:18:22,380 --> 00:18:27,840
console agent so why do we need that the

00:18:25,380 --> 00:18:30,650
reason is because post press has

00:18:27,840 --> 00:18:33,000
built-in capabilities to perform

00:18:30,650 --> 00:18:36,200
replication it's an S in Chronos

00:18:33,000 --> 00:18:39,390
replication called streaming application

00:18:36,200 --> 00:18:42,110
but it does not come with with failover

00:18:39,390 --> 00:18:44,640
detection or automatic failover

00:18:42,110 --> 00:18:46,140
facilities so you actually have to find

00:18:44,640 --> 00:18:49,410
a cluster manager that would help you

00:18:46,140 --> 00:18:51,630
doing that earlier days we've used

00:18:49,410 --> 00:18:54,240
pacemaker to do that but it's very very

00:18:51,630 --> 00:18:57,210
poor choice on to put that on a on a

00:18:54,240 --> 00:19:00,360
cloud so we found cluster managers doing

00:18:57,210 --> 00:19:03,150
a great job and cluster manager will

00:19:00,360 --> 00:19:05,430
will actually recognize a failing

00:19:03,150 --> 00:19:07,740
failing databases will trigger

00:19:05,430 --> 00:19:11,070
appropriate promote scripts that will

00:19:07,740 --> 00:19:14,610
help us to then perform the actual

00:19:11,070 --> 00:19:16,500
failover so let's have a look at some of

00:19:14,610 --> 00:19:20,430
the challenges during that failover

00:19:16,500 --> 00:19:22,410
scenario and again those are the

00:19:20,430 --> 00:19:25,860
complexities you have to deal with while

00:19:22,410 --> 00:19:27,300
building a Bosch release right so we

00:19:25,860 --> 00:19:32,360
have to wrap these components and we

00:19:27,300 --> 00:19:35,220
have to find a way that we do failover

00:19:32,360 --> 00:19:37,860
specific to Postgres but still being

00:19:35,220 --> 00:19:41,610
operation system and infrastructure

00:19:37,860 --> 00:19:43,890
kostik so the challenges we've seen is

00:19:41,610 --> 00:19:46,050
that you have to provide credentials to

00:19:43,890 --> 00:19:48,360
access your database server so with

00:19:46,050 --> 00:19:49,810
asynchronous replication what you can do

00:19:48,360 --> 00:19:52,540
is just use

00:19:49,810 --> 00:19:54,460
you know every note any of the notes or

00:19:52,540 --> 00:19:57,610
load-balanced them because you have to

00:19:54,460 --> 00:20:02,410
ensure that you're always right against

00:19:57,610 --> 00:20:04,240
the database master so with that with

00:20:02,410 --> 00:20:05,740
that requirement you can use IP

00:20:04,240 --> 00:20:08,590
addresses because they will change

00:20:05,740 --> 00:20:10,060
during failover so if you if your master

00:20:08,590 --> 00:20:13,780
server goes down you want to write to

00:20:10,060 --> 00:20:18,670
the to the slave so you can't use the IP

00:20:13,780 --> 00:20:21,250
addresses and lastly at least at the

00:20:18,670 --> 00:20:23,260
point we were looking at this boss

00:20:21,250 --> 00:20:25,630
internal power D&S was a single point of

00:20:23,260 --> 00:20:30,160
failure so you don't want to use bosch

00:20:25,630 --> 00:20:32,530
internal DNS name to do that either so

00:20:30,160 --> 00:20:36,040
we've decided to go with console and use

00:20:32,530 --> 00:20:38,260
use its internal DNS you know with the

00:20:36,040 --> 00:20:40,870
five note console cluster spread across

00:20:38,260 --> 00:20:44,650
three availability zones that's actually

00:20:40,870 --> 00:20:46,420
a redundant solution we can live with so

00:20:44,650 --> 00:20:49,120
the resulting architecture then looks

00:20:46,420 --> 00:20:51,760
somehow like this so you have a console

00:20:49,120 --> 00:20:55,420
cluster and the console cluster manages

00:20:51,760 --> 00:20:58,000
a DNS name so this DNS name this DNS has

00:20:55,420 --> 00:21:02,170
been has to be registered in your in

00:20:58,000 --> 00:21:03,900
your runners ET series off obviously so

00:21:02,170 --> 00:21:07,090
and with that you can actually use this

00:21:03,900 --> 00:21:10,120
DNS name to always point to the master

00:21:07,090 --> 00:21:14,380
so in in case fancy animation isn't it

00:21:10,120 --> 00:21:16,630
so in in case this master goes down like

00:21:14,380 --> 00:21:18,760
what happens then because all right you

00:21:16,630 --> 00:21:21,190
have two replicas to other servers but

00:21:18,760 --> 00:21:23,050
you have to get up and you know fix it

00:21:21,190 --> 00:21:25,240
no we don't want to do that we want to

00:21:23,050 --> 00:21:26,830
have this happen automatically so this

00:21:25,240 --> 00:21:29,140
is where actually the rep manager comes

00:21:26,830 --> 00:21:32,020
into game so it recognized the master

00:21:29,140 --> 00:21:34,960
server is gone and at that case it will

00:21:32,020 --> 00:21:39,340
trigger a promote script which will also

00:21:34,960 --> 00:21:44,440
tell the remote console to to change the

00:21:39,340 --> 00:21:45,940
DNS entry to the new master so one of

00:21:44,440 --> 00:21:47,680
the challenges with that approach is

00:21:45,940 --> 00:21:49,840
that you shouldn't have a DNS caching

00:21:47,680 --> 00:21:51,820
activated in your runners or your

00:21:49,840 --> 00:21:55,750
applications which might be troubles

00:21:51,820 --> 00:21:58,810
cause trouble with some default Jama JVM

00:21:55,750 --> 00:22:00,730
configurations but it's definitely a

00:21:58,810 --> 00:22:02,060
infrastructure economic way of dealing

00:22:00,730 --> 00:22:04,040
with that situation

00:22:02,060 --> 00:22:05,990
however so far three o'clock in the

00:22:04,040 --> 00:22:09,230
morning one of our servers went down

00:22:05,990 --> 00:22:11,930
this is obviously and we are in the

00:22:09,230 --> 00:22:15,020
creative mode so what actually happens

00:22:11,930 --> 00:22:17,900
now is that the Bosch self feelings come

00:22:15,020 --> 00:22:19,490
self feeding come in to get the game the

00:22:17,900 --> 00:22:22,400
health manager of Bosch will recognize

00:22:19,490 --> 00:22:24,140
that there's a virtual machine missing

00:22:22,400 --> 00:22:26,570
and the resurrector will instruct the

00:22:24,140 --> 00:22:28,940
director to boots reboot from that

00:22:26,570 --> 00:22:32,030
virtual machine and because of the way

00:22:28,940 --> 00:22:33,800
this solution is built at that time when

00:22:32,030 --> 00:22:36,490
the rep manager comes up it will already

00:22:33,800 --> 00:22:40,190
recognize that there's a new master and

00:22:36,490 --> 00:22:43,340
in turn the new note into a slave so

00:22:40,190 --> 00:22:46,700
with boss built-in functionality not

00:22:43,340 --> 00:22:50,170
only that you have to be able to create

00:22:46,700 --> 00:22:53,060
a automatic failover using using console

00:22:50,170 --> 00:22:55,100
wrap that in a Bosch release but boss

00:22:53,060 --> 00:22:57,740
will also help you to automatically

00:22:55,100 --> 00:23:00,980
recover from degraded mode so now you

00:22:57,740 --> 00:23:03,650
are you're actually fine and if you fix

00:23:00,980 --> 00:23:05,120
the physical host maybe which failed

00:23:03,650 --> 00:23:06,700
behind that you don't have to do

00:23:05,120 --> 00:23:09,380
anything

00:23:06,700 --> 00:23:11,960
all right now we reach the check point

00:23:09,380 --> 00:23:14,480
where every every component of our run

00:23:11,960 --> 00:23:16,250
time is clustered so the run time will

00:23:14,480 --> 00:23:20,750
actually survive failures from a single

00:23:16,250 --> 00:23:22,940
availability zone which leads us to the

00:23:20,750 --> 00:23:25,010
insight that a Cloud Foundry is only

00:23:22,940 --> 00:23:27,590
production ready when your data services

00:23:25,010 --> 00:23:29,240
are because the problem will the

00:23:27,590 --> 00:23:32,120
challenge with data services is that

00:23:29,240 --> 00:23:34,850
apps often strongly depend on the data

00:23:32,120 --> 00:23:37,310
services so now you have this wonderful

00:23:34,850 --> 00:23:39,470
runtime that gives you a scalability for

00:23:37,310 --> 00:23:42,440
you can scale you app from one instance

00:23:39,470 --> 00:23:44,450
to a hundred during a peak and then you

00:23:42,440 --> 00:23:46,790
just have a single database cluster that

00:23:44,450 --> 00:23:48,410
doesn't really make sense because there

00:23:46,790 --> 00:23:51,250
is an unhealthy ratio between

00:23:48,410 --> 00:23:55,400
application instances and a physical

00:23:51,250 --> 00:23:57,830
database cluster so at some point this

00:23:55,400 --> 00:24:00,860
cluster might fail either because it's

00:23:57,830 --> 00:24:03,290
overloaded or because it you know it

00:24:00,860 --> 00:24:05,870
fails even clusters may fail so that

00:24:03,290 --> 00:24:07,850
case what happens is that a lot of

00:24:05,870 --> 00:24:10,460
application instances are affected and

00:24:07,850 --> 00:24:13,130
from experience and as I said we've

00:24:10,460 --> 00:24:13,730
broken everything at some point that the

00:24:13,130 --> 00:24:16,970
customer

00:24:13,730 --> 00:24:18,920
we'll give you a hard time so what we

00:24:16,970 --> 00:24:21,020
derive from that is that shared data

00:24:18,920 --> 00:24:22,790
services are not an option I don't want

00:24:21,020 --> 00:24:24,440
to I don't want to have a Postgres or

00:24:22,790 --> 00:24:27,290
any data services where a service

00:24:24,440 --> 00:24:30,110
instance is managed by the data service

00:24:27,290 --> 00:24:33,050
itself so Postgres databases are bad

00:24:30,110 --> 00:24:34,910
service instances it's I'd rather go

00:24:33,050 --> 00:24:38,600
with dedicated Postgres servers or

00:24:34,910 --> 00:24:41,330
clusters at service instances so our

00:24:38,600 --> 00:24:43,550
conclusion was that it's more healthy to

00:24:41,330 --> 00:24:46,070
use on-demand provision dedicated

00:24:43,550 --> 00:24:48,050
services instead so with that being said

00:24:46,070 --> 00:24:49,610
the situation would look like this

00:24:48,050 --> 00:24:52,040
you have a hundreds of application

00:24:49,610 --> 00:24:54,770
instances and whenever one of these

00:24:52,040 --> 00:24:57,830
serves you know service instances goes

00:24:54,770 --> 00:25:00,740
down maybe because they are a single

00:24:57,830 --> 00:25:03,650
server Postgres and the appropriate

00:25:00,740 --> 00:25:07,310
availability zone went down the problem

00:25:03,650 --> 00:25:09,440
is just way more contained which is not

00:25:07,310 --> 00:25:11,930
solving the problem because still things

00:25:09,440 --> 00:25:14,050
fail but you know you have less

00:25:11,930 --> 00:25:16,730
customers yelling at you

00:25:14,050 --> 00:25:19,670
so we actually want to contain problems

00:25:16,730 --> 00:25:22,880
and of course how do you on demand

00:25:19,670 --> 00:25:26,780
provision data service instances so

00:25:22,880 --> 00:25:30,290
let's let's do let's have Bosh do the

00:25:26,780 --> 00:25:32,510
dirty work so a reciting architecture

00:25:30,290 --> 00:25:34,700
might look like this where a cloud

00:25:32,510 --> 00:25:38,210
controller talks to a generic service

00:25:34,700 --> 00:25:42,170
broker who will then trigger the issuing

00:25:38,210 --> 00:25:44,060
of deployment attributes using a data

00:25:42,170 --> 00:25:47,480
service specific component called the

00:25:44,060 --> 00:25:52,330
SPI which will then talk to a deployment

00:25:47,480 --> 00:25:55,010
component slightly abstracting from Bosh

00:25:52,330 --> 00:25:56,660
generating a deployment manifest so

00:25:55,010 --> 00:25:59,570
let's say you have a bas-reliefs for

00:25:56,660 --> 00:26:01,790
post press and you want to create a

00:25:59,570 --> 00:26:04,070
service instance for a single post core

00:26:01,790 --> 00:26:06,470
server this will do this deployment

00:26:04,070 --> 00:26:08,780
manifest will defer from a let's say

00:26:06,470 --> 00:26:10,190
large Postgres cluster so and that

00:26:08,780 --> 00:26:13,130
abstraction is actually made on the

00:26:10,190 --> 00:26:15,110
level of the deployer using different

00:26:13,130 --> 00:26:17,170
templates will which will then be

00:26:15,110 --> 00:26:19,520
generated into deployment manifest and

00:26:17,170 --> 00:26:21,080
subsequently deployment tasks will be

00:26:19,520 --> 00:26:23,690
started and virtual machines will be

00:26:21,080 --> 00:26:27,330
provisioned for you so there are two

00:26:23,690 --> 00:26:29,250
major advantages using that strategy

00:26:27,330 --> 00:26:31,140
first of which you have a stronger

00:26:29,250 --> 00:26:32,730
isolation between service instances

00:26:31,140 --> 00:26:34,950
because you are using means of

00:26:32,730 --> 00:26:36,990
infrastructure so there's a clear

00:26:34,950 --> 00:26:43,650
contract between you and your customer

00:26:36,990 --> 00:26:46,020
because a 8gig Postgres server with 8

00:26:43,650 --> 00:26:48,300
gig of ram of memory I mean that's

00:26:46,020 --> 00:26:50,370
that's that's up to you how you use it

00:26:48,300 --> 00:26:52,050
and nobody else will influence that

00:26:50,370 --> 00:26:54,530
instance unless you do heavy /

00:26:52,050 --> 00:26:56,100
commitment on the infrastructure level

00:26:54,530 --> 00:26:58,140
all right

00:26:56,100 --> 00:26:59,730
and good thing about what we found out

00:26:58,140 --> 00:27:01,560
is that you will actually only have to

00:26:59,730 --> 00:27:04,800
do two things when adding a new data

00:27:01,560 --> 00:27:06,930
service which is creating this SPI whose

00:27:04,800 --> 00:27:08,550
main responsibility is issuing the

00:27:06,930 --> 00:27:10,950
credentials and manage two potentials

00:27:08,550 --> 00:27:14,270
when creating service bindings as well

00:27:10,950 --> 00:27:17,250
as to provide a appropriate bas-reliefs

00:27:14,270 --> 00:27:19,020
so the communication flow looks a little

00:27:17,250 --> 00:27:20,070
like this so whenever you create a

00:27:19,020 --> 00:27:22,590
service instance

00:27:20,070 --> 00:27:24,900
this will be delegated to the service

00:27:22,590 --> 00:27:27,240
broker who will then prepare the

00:27:24,900 --> 00:27:31,680
deployment against a service data

00:27:27,240 --> 00:27:34,050
service specific component this is about

00:27:31,680 --> 00:27:36,090
the the placeholders you will later

00:27:34,050 --> 00:27:38,880
actually fill in the deployment

00:27:36,090 --> 00:27:41,460
templates which will be generated to

00:27:38,880 --> 00:27:43,440
deployment manifests so with that you

00:27:41,460 --> 00:27:45,630
actually talk to the deployer select a

00:27:43,440 --> 00:27:47,190
template let's say a single serve up

00:27:45,630 --> 00:27:49,800
template fill in the appropriate

00:27:47,190 --> 00:27:51,990
attributes and generate a BAS deployment

00:27:49,800 --> 00:27:53,940
from it the cloud controller then keeps

00:27:51,990 --> 00:27:55,860
on following asking whether deployment

00:27:53,940 --> 00:27:57,690
has finished and once the deployment has

00:27:55,860 --> 00:28:01,200
finished the service broker will store

00:27:57,690 --> 00:28:03,780
some service instance specific values

00:28:01,200 --> 00:28:05,880
such as you know credentials to access

00:28:03,780 --> 00:28:08,340
it when because when you later want to

00:28:05,880 --> 00:28:10,260
create a service binding you have to

00:28:08,340 --> 00:28:13,500
know which database cluster to connect

00:28:10,260 --> 00:28:16,710
to so as I said this solution can be

00:28:13,500 --> 00:28:20,340
used for arbitrary data services we in

00:28:16,710 --> 00:28:23,400
particular use it for rabbitmq for Redis

00:28:20,340 --> 00:28:25,920
for MongoDB and Postgres as the more at

00:28:23,400 --> 00:28:28,800
the moment and estimate the effort of

00:28:25,920 --> 00:28:31,160
adding a new data service in production

00:28:28,800 --> 00:28:33,300
for to around four to eight weeks

00:28:31,160 --> 00:28:35,980
because of you have to do a lot of

00:28:33,300 --> 00:28:39,250
testing and learn about the data service

00:28:35,980 --> 00:28:42,789
alright so wrap it up how does the

00:28:39,250 --> 00:28:45,250
system then in the end look like so you

00:28:42,789 --> 00:28:47,380
have three availability zones you have

00:28:45,250 --> 00:28:50,139
your run time on the top and a service

00:28:47,380 --> 00:28:52,000
down below of course you can add several

00:28:50,139 --> 00:28:55,380
services but you know for the sake of

00:28:52,000 --> 00:28:58,019
having it on the slide it's just one

00:28:55,380 --> 00:29:00,639
most importantly is that you'll have a

00:28:58,019 --> 00:29:03,010
you know the the availability zones

00:29:00,639 --> 00:29:04,750
zones configured into your deployment

00:29:03,010 --> 00:29:06,730
manifest so that you influence the

00:29:04,750 --> 00:29:09,010
placement strategy that mission-critical

00:29:06,730 --> 00:29:12,639
virtual machines will be distributed

00:29:09,010 --> 00:29:15,130
across the availability zones and by

00:29:12,639 --> 00:29:17,260
using bosh to deploy the service

00:29:15,130 --> 00:29:19,299
instances you just apply the very same

00:29:17,260 --> 00:29:21,789
strategy and can ensure that when

00:29:19,299 --> 00:29:25,289
deploying a Postgres cluster which then

00:29:21,789 --> 00:29:28,440
you can deploy an arbitrary number of it

00:29:25,289 --> 00:29:30,610
they they are virtual machines will be

00:29:28,440 --> 00:29:33,610
distributed across the availability

00:29:30,610 --> 00:29:36,610
zones as well so with that design you

00:29:33,610 --> 00:29:38,590
can actually survive the outage of a

00:29:36,610 --> 00:29:41,049
availability zone with any impact on

00:29:38,590 --> 00:29:44,519
your only apps beside of little hiccup

00:29:41,049 --> 00:29:49,480
when while reconnecting to the database

00:29:44,519 --> 00:29:51,130
so how can we sum this up well and thewe

00:29:49,480 --> 00:29:52,659
theistic Lea I can say borscht is a

00:29:51,130 --> 00:29:55,120
great companion for all Cloud Foundry

00:29:52,659 --> 00:29:57,789
related automation challenges and this

00:29:55,120 --> 00:30:00,789
does not only include the runtime but

00:29:57,789 --> 00:30:03,399
also includes data services in while

00:30:00,789 --> 00:30:05,980
containers are fancy and hip style of

00:30:03,399 --> 00:30:07,809
solving everything you know I think when

00:30:05,980 --> 00:30:08,320
you got a Hema everything you see is a

00:30:07,809 --> 00:30:11,350
nail

00:30:08,320 --> 00:30:16,480
I think Bosh does and did a really great

00:30:11,350 --> 00:30:18,669
job on on deploying applications to

00:30:16,480 --> 00:30:20,440
Cloud Foundry but also I mean deploying

00:30:18,669 --> 00:30:24,070
cloud phonies as well as solving the

00:30:20,440 --> 00:30:27,159
data source problem so yeah that was it

00:30:24,070 --> 00:30:28,890
and if you have questions feel free to

00:30:27,159 --> 00:30:32,039
ask

00:30:28,890 --> 00:30:32,039
[Applause]

00:30:34,390 --> 00:30:36,849

YouTube URL: https://www.youtube.com/watch?v=GsTcfofSVIA


