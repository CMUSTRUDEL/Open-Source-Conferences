Title: Chaos Heidi vs. Orchard: Self-Disruption and Healing in a Cloud Foundry-Based Service Environment
Publication date: 2016-05-29
Playlist: Cloud Foundry Summit Santa Clara 2016
Description: 
	Chaos Heidi vs. Orchard: Self-Disruption and Healing in a Cloud Foundry-Based Service Environment - Diego Zamboni, Swisscom & Bill Chapman, Stark & Wayne

Cloud Foundry has built-in health management and self-healing capabilities. But when delivering a Cloud Foundry-based service, it is not sufficient to monitor and heal the Cloud Foundry components, since the end-to-end functionality of the service also depends on the state of the entire infrastructure surrounding and supporting Cloud Foundry. This includes the hardware, virtualization and networking platforms on which it runs, the CF Services available, the user interface, and even the billing systems. 

To this effect, at Swisscom we have built an overarching Health Management platform called Orchard, which not only monitors and notifies about failures in our Application Cloud platform, but has the ability to trigger automated responses to heal those failures automatically. To challenge Orchard’s abilities, we have also developed Chaos Heidi, a self-disruption platform that allows us to cause failure scenarios of different types. 

In this talk we will describe Orchard and Chaos Heidi, including their motivation and architecture, their current implementations, and our challenges and ideas for the future. 

Bill Chapman
Cloud Architect, Stark and Wayne
Bill is a Cloud Architect at Stark & Wayne and for the last two years he has been helping Swisscom in their efforts to build management and monitoring tools that support their Cloud Foundry deployments. Before coming to Stark & Wayne Bill spent several years as a consultant, and development manager helping businesses scale both their software stacks and their engineering teams.

Diego Zamboni
Cloud Architect, Swisscom
Diego Zamboni is a computer scientist, consultant, author, programmer and sysadmin who works as a Cloud Architect at Swisscom, where he is building monitoring and self-healing capabilities into Swisscom’s Cloud Infrastructure. He has more than 20 years of experience in system administration and security, and has worked in both the applied and theoretical sides of the computer science field. He holds a Ph.D. from Purdue University, has worked a sysadmin at a supercomputer center, as a researcher at the IBM Zurich Research Lab, as a consultant at HP Enterprise Services and as a Product Manager at CFEngine. He is the author of the book "Learning CFEngine 3", published by O'Reilly Media. He lives in Switzerland with his wife and two daughters.
Captions: 
	00:00:00,030 --> 00:00:09,480
okay given that we are from Switzerland

00:00:06,540 --> 00:00:12,410
we are going to try and start on time so

00:00:09,480 --> 00:00:14,580
let's get going

00:00:12,410 --> 00:00:17,430
welcome everyone my name is Diego

00:00:14,580 --> 00:00:19,439
Zamboni I'm from Swiss CIM and my friend

00:00:17,430 --> 00:00:23,369
and colleague bill Chapman we're going

00:00:19,439 --> 00:00:24,779
to give this talk on self disruption and

00:00:23,369 --> 00:00:27,029
healing on a Cloud Foundry based

00:00:24,779 --> 00:00:28,800
environment you want to introduce I'm

00:00:27,029 --> 00:00:30,630
bill Chapman I'm a cloud architect ik

00:00:28,800 --> 00:00:33,680
Stark and Wayne I've been attached to

00:00:30,630 --> 00:00:35,850
Swisscom for a couple of years now and

00:00:33,680 --> 00:00:37,500
these two projects we're going to talk

00:00:35,850 --> 00:00:39,239
about our culmination of a lot of the

00:00:37,500 --> 00:00:43,230
effort we've had with Diego and I have

00:00:39,239 --> 00:00:46,730
had together so all right first of all

00:00:43,230 --> 00:00:49,079
I'd like to comment briefly on how

00:00:46,730 --> 00:00:50,730
interesting it is to have my name at a

00:00:49,079 --> 00:00:52,890
cloud foundry conference I mean we all

00:00:50,730 --> 00:00:55,559
have instinctive reactions to our names

00:00:52,890 --> 00:00:59,760
right so it's is weird to be hearing it

00:00:55,559 --> 00:01:02,129
everywhere anyway so just a brief

00:00:59,760 --> 00:01:04,350
overview of the agenda not only when

00:01:02,129 --> 00:01:06,150
when we talk about this in in

00:01:04,350 --> 00:01:08,369
Switzerland we have to explain what

00:01:06,150 --> 00:01:09,510
cloud foundry is whereas here we

00:01:08,369 --> 00:01:12,150
probably have to explain a bit what

00:01:09,510 --> 00:01:14,159
Swisscom is so we're going to do that

00:01:12,150 --> 00:01:17,009
very briefly and then we're going to

00:01:14,159 --> 00:01:18,630
talk about the two counterparts of this

00:01:17,009 --> 00:01:21,000
of this effort one is the health

00:01:18,630 --> 00:01:22,680
management monitoring and self-healing

00:01:21,000 --> 00:01:25,380
platform that we've been building and

00:01:22,680 --> 00:01:27,180
the other is a self disruption platform

00:01:25,380 --> 00:01:31,770
that we've been building which we've

00:01:27,180 --> 00:01:35,189
appropriately named chaos Heidi as a tip

00:01:31,770 --> 00:01:37,560
of the hat of to chaos monkey so very

00:01:35,189 --> 00:01:40,909
briefly this is my one marketing slide

00:01:37,560 --> 00:01:43,979
from Swisscom basically Swisscom is the

00:01:40,909 --> 00:01:46,369
biggest telecom provider in Switzerland

00:01:43,979 --> 00:01:50,159
providing everything from fixed line

00:01:46,369 --> 00:01:52,470
internet cellular service outsourcing

00:01:50,159 --> 00:01:55,829
services for big companies connectivity

00:01:52,470 --> 00:02:00,719
all sorts of things and now very

00:01:55,829 --> 00:02:03,060
recently a pass platform built on top of

00:02:00,719 --> 00:02:05,790
cloud foundry which we have named the

00:02:03,060 --> 00:02:08,280
swisco application cloud there's a URL

00:02:05,790 --> 00:02:11,039
over there if you want to try it out we

00:02:08,280 --> 00:02:13,560
have a publicly available version of the

00:02:11,039 --> 00:02:15,780
service which is available

00:02:13,560 --> 00:02:17,819
to anyone in the world there's her tutor

00:02:15,780 --> 00:02:21,120
handles so you can follow up what gets

00:02:17,819 --> 00:02:23,480
announced and basically what this is

00:02:21,120 --> 00:02:26,520
just a very brief architectural overview

00:02:23,480 --> 00:02:28,590
it's built using Cloud Foundry on top of

00:02:26,520 --> 00:02:30,300
OpenStack using a few different

00:02:28,590 --> 00:02:34,260
components for the infrastructure for

00:02:30,300 --> 00:02:36,569
Sdn we use plum bread we use scaleo for

00:02:34,260 --> 00:02:39,870
a storage we have a few other things

00:02:36,569 --> 00:02:41,849
here and there we have built a number of

00:02:39,870 --> 00:02:44,519
services that that can be used together

00:02:41,849 --> 00:02:48,150
with a platform so we have readies

00:02:44,519 --> 00:02:50,130
MongoDB elke RabbitMQ and a few other

00:02:48,150 --> 00:02:51,720
things that that our users can can

00:02:50,130 --> 00:02:59,880
connect to their to their applications

00:02:51,720 --> 00:03:01,860
so one of the the basic ideas of the of

00:02:59,880 --> 00:03:03,870
the application cloud is that we have

00:03:01,860 --> 00:03:06,180
all of these components but we don't

00:03:03,870 --> 00:03:09,120
have just one instance of this instead

00:03:06,180 --> 00:03:11,069
our intention and what we are actually

00:03:09,120 --> 00:03:14,340
doing already in production is to have

00:03:11,069 --> 00:03:16,830
multiple instances of this construct

00:03:14,340 --> 00:03:18,359
called the application Cloud for

00:03:16,830 --> 00:03:20,880
different uses so we have a public

00:03:18,359 --> 00:03:22,200
instance we have an internal Swisscom

00:03:20,880 --> 00:03:24,359
internal instance for internal

00:03:22,200 --> 00:03:27,359
applications and we have virtual private

00:03:24,359 --> 00:03:29,250
instances for different customers and of

00:03:27,359 --> 00:03:31,530
course all of these need to be monitored

00:03:29,250 --> 00:03:35,310
need to be managed need to be operated

00:03:31,530 --> 00:03:38,370
at scale and one of the one of the

00:03:35,310 --> 00:03:43,049
realizations is that I mean Cloud

00:03:38,370 --> 00:03:45,720
Foundry itself has self-healing has also

00:03:43,049 --> 00:03:47,549
all these nice features well but we're

00:03:45,720 --> 00:03:49,769
not just using Cloud Foundry we're using

00:03:47,549 --> 00:03:52,620
all of these other components that

00:03:49,769 --> 00:03:55,829
together create this service and this

00:03:52,620 --> 00:03:57,989
application so this is this is where all

00:03:55,829 --> 00:04:01,549
of this comes from so how do we monitor

00:03:57,989 --> 00:04:04,200
a Cloud Foundry production environment

00:04:01,549 --> 00:04:06,630
we are using this from the beginning

00:04:04,200 --> 00:04:10,410
this thought concept called the OODA

00:04:06,630 --> 00:04:13,950
loop oh sorry wrong circle this is the

00:04:10,410 --> 00:04:16,410
would a loop basically it's it's a

00:04:13,950 --> 00:04:19,560
well-known idea that originated original

00:04:16,410 --> 00:04:23,580
in the in the US military in the US Air

00:04:19,560 --> 00:04:25,740
Force and this it's one of these typical

00:04:23,580 --> 00:04:27,060
feedback loops and it stands for observe

00:04:25,740 --> 00:04:28,800
orient decide

00:04:27,060 --> 00:04:30,870
and act basically the idea is that you

00:04:28,800 --> 00:04:34,020
collect information about what's going

00:04:30,870 --> 00:04:36,690
on you analyze this information you make

00:04:34,020 --> 00:04:39,270
decisions based on what you discover and

00:04:36,690 --> 00:04:41,160
then you act in reaction to the to the

00:04:39,270 --> 00:04:42,900
current conditions and then this repeats

00:04:41,160 --> 00:04:45,720
over and over and this allows you to

00:04:42,900 --> 00:04:49,620
adjust the course of action in reaction

00:04:45,720 --> 00:04:51,360
to to what's happening around you so one

00:04:49,620 --> 00:04:53,760
of our earlier realizations is that this

00:04:51,360 --> 00:04:55,950
coup de loops are already all over the

00:04:53,760 --> 00:04:58,889
place in our infrastructure but they are

00:04:55,950 --> 00:05:01,530
mostly incomplete and they are mostly

00:04:58,889 --> 00:05:04,260
disjoint so each component has its own

00:05:01,530 --> 00:05:07,139
little version of a self-monitoring

00:05:04,260 --> 00:05:08,400
self-healing component some of them only

00:05:07,139 --> 00:05:10,590
fix certain things

00:05:08,400 --> 00:05:12,000
some of them monitor certain parts and

00:05:10,590 --> 00:05:14,220
tell you about it but they don't fix

00:05:12,000 --> 00:05:16,290
anything others just fix things but they

00:05:14,220 --> 00:05:18,750
don't tell you what's going on inside so

00:05:16,290 --> 00:05:21,240
it's a bit of mix and match and we don't

00:05:18,750 --> 00:05:23,520
want to reinvent all of these components

00:05:21,240 --> 00:05:26,240
rather we want to use what's already

00:05:23,520 --> 00:05:30,720
there in different forms I mean we have

00:05:26,240 --> 00:05:34,590
different self-healing health checking

00:05:30,720 --> 00:05:37,410
self management scripts and bits and

00:05:34,590 --> 00:05:39,570
pieces all over the place and we want to

00:05:37,410 --> 00:05:41,669
aggregate the data that they are

00:05:39,570 --> 00:05:43,919
providing and we want to aggregate their

00:05:41,669 --> 00:05:48,690
functionality and build a layer on top

00:05:43,919 --> 00:05:50,940
now ideally at the end of the tunnel we

00:05:48,690 --> 00:05:52,979
would like to automate humans out of the

00:05:50,940 --> 00:05:54,840
equation right and have all of the

00:05:52,979 --> 00:05:57,810
decisions made automatically this is

00:05:54,840 --> 00:05:59,850
clearly not possible realistically with

00:05:57,810 --> 00:06:02,970
or with our technology so we want to

00:05:59,850 --> 00:06:05,520
keep humans in the loop but delegate to

00:06:02,970 --> 00:06:07,710
humans only the really hard tasks and a

00:06:05,520 --> 00:06:11,370
really hard decision so we want to over

00:06:07,710 --> 00:06:14,130
time learn to automate more and more of

00:06:11,370 --> 00:06:15,840
these solutions to problems or at least

00:06:14,130 --> 00:06:18,300
not detection and notification of

00:06:15,840 --> 00:06:20,430
problems so that our operations teams

00:06:18,300 --> 00:06:22,139
and our engineering teams can focus on

00:06:20,430 --> 00:06:24,300
solving the really hard problems that we

00:06:22,139 --> 00:06:27,500
don't know how to solve automatically I

00:06:24,300 --> 00:06:29,669
should clarify we're not talking about

00:06:27,500 --> 00:06:33,270
artificial intelligence or machine

00:06:29,669 --> 00:06:36,539
learning this is really very simple rule

00:06:33,270 --> 00:06:39,949
building and iteratively increasing our

00:06:36,539 --> 00:06:42,780
knowledge base to automate these things

00:06:39,949 --> 00:06:44,340
also in in parallel we're building this

00:06:42,780 --> 00:06:47,580
self disruption platform that was

00:06:44,340 --> 00:06:50,160
inspired by Netflix chaos monkey project

00:06:47,580 --> 00:06:53,910
which we call chaos Heidi and the idea

00:06:50,160 --> 00:06:55,830
is to introduce automatically

00:06:53,910 --> 00:06:57,930
disruptions and destruction in the

00:06:55,830 --> 00:06:59,580
infrastructure so that we can test both

00:06:57,930 --> 00:07:03,570
or detection and our reaction

00:06:59,580 --> 00:07:05,720
capabilities so overall we want to use

00:07:03,570 --> 00:07:08,280
and this is all called order of course

00:07:05,720 --> 00:07:10,350
we want to improve visibility into our

00:07:08,280 --> 00:07:11,729
systems and the ultimate goal is to make

00:07:10,350 --> 00:07:13,590
everyone's lives easier

00:07:11,729 --> 00:07:15,210
not only the operations teams but also

00:07:13,590 --> 00:07:18,510
the engineering teams the development

00:07:15,210 --> 00:07:20,940
teams and even the customers by giving

00:07:18,510 --> 00:07:22,699
them access to some of these data so

00:07:20,940 --> 00:07:25,500
just to give you a very brief overview

00:07:22,699 --> 00:07:28,199
orchard is really a back-end system but

00:07:25,500 --> 00:07:30,630
we discover very early on that you

00:07:28,199 --> 00:07:34,020
cannot properly demo something without

00:07:30,630 --> 00:07:36,120
having a visual component so we have a

00:07:34,020 --> 00:07:38,639
very simple dashboard that basically

00:07:36,120 --> 00:07:40,590
gives us different views into the the

00:07:38,639 --> 00:07:43,830
model the data that is stored into the

00:07:40,590 --> 00:07:45,150
system so we can have use that give us

00:07:43,830 --> 00:07:46,320
information about the state of the

00:07:45,150 --> 00:07:49,110
infrastructure components such as

00:07:46,320 --> 00:07:51,300
OpenStack or Skelly or plum grid we can

00:07:49,110 --> 00:07:53,280
also have views into our business

00:07:51,300 --> 00:07:56,070
integration and end-to-end service

00:07:53,280 --> 00:07:59,400
processes such as end-to-end checks on

00:07:56,070 --> 00:08:03,539
our Atma sir or ELQ service or MongoDB

00:07:59,400 --> 00:08:06,150
or whatnot our building runs our login

00:08:03,539 --> 00:08:08,400
subsystems and so on we can also have

00:08:06,150 --> 00:08:09,930
views on the checks that are being run

00:08:08,400 --> 00:08:12,120
on the Cloud Foundry services

00:08:09,930 --> 00:08:14,909
infrastructure so on the containers and

00:08:12,120 --> 00:08:17,970
the VMS on which the instances of the

00:08:14,909 --> 00:08:19,199
services are being executed so this and

00:08:17,970 --> 00:08:22,080
this is just a matter of reconfiguring

00:08:19,199 --> 00:08:25,560
the view according to what is it that we

00:08:22,080 --> 00:08:28,470
want to look at right in terms of an

00:08:25,560 --> 00:08:30,030
implementation I'm not going to go

00:08:28,470 --> 00:08:31,530
through this in detail because we don't

00:08:30,030 --> 00:08:34,110
have enough time but I just want to

00:08:31,530 --> 00:08:36,900
point out that we are basically

00:08:34,110 --> 00:08:40,860
collecting data from the from the

00:08:36,900 --> 00:08:41,279
infrastructure we have different types

00:08:40,860 --> 00:08:44,459
of data

00:08:41,279 --> 00:08:46,290
our main state checking data comes from

00:08:44,459 --> 00:08:48,089
console we are using console which is an

00:08:46,290 --> 00:08:50,279
open source health management and

00:08:48,089 --> 00:08:52,020
services covered platform so console

00:08:50,279 --> 00:08:53,670
allows us to distribute checks

00:08:52,020 --> 00:08:55,590
throughout all of the node

00:08:53,670 --> 00:08:57,390
in the infrastructure and then we can

00:08:55,590 --> 00:08:59,460
customize the checks depending on each

00:08:57,390 --> 00:09:01,200
component and all of these results are

00:08:59,460 --> 00:09:04,740
being centralized in a console cluster

00:09:01,200 --> 00:09:08,370
we're also collecting logging messages

00:09:04,740 --> 00:09:11,100
and metrics from Cloud Foundry we are

00:09:08,370 --> 00:09:12,480
using both the the stock logging and and

00:09:11,100 --> 00:09:15,360
mirroring mechanisms built into plat

00:09:12,480 --> 00:09:18,920
foundry we're now building in logger

00:09:15,360 --> 00:09:21,720
Gator integration we also developed

00:09:18,920 --> 00:09:23,520
actually build developed a custom Bosch

00:09:21,720 --> 00:09:25,950
monitoring plug-in that allows us to

00:09:23,520 --> 00:09:28,290
check the states of the bosch components

00:09:25,950 --> 00:09:32,370
and post them directly to console all of

00:09:28,290 --> 00:09:34,380
this is stored is fed into a message bus

00:09:32,370 --> 00:09:36,510
and then we're using Riemann which is an

00:09:34,380 --> 00:09:39,990
open source event processing engine to

00:09:36,510 --> 00:09:42,720
fetch these events and then apply custom

00:09:39,990 --> 00:09:45,210
rules on them so we are building a set

00:09:42,720 --> 00:09:47,790
of rules that ranged from very generic

00:09:45,210 --> 00:09:49,890
aggregation and summarization rules of

00:09:47,790 --> 00:09:52,800
the overall state of a certain component

00:09:49,890 --> 00:09:55,500
or a set of components and also we are

00:09:52,800 --> 00:09:57,750
over time starting to add more specific

00:09:55,500 --> 00:10:00,300
rules for specific services specific

00:09:57,750 --> 00:10:04,200
customers or specific purposes also for

00:10:00,300 --> 00:10:08,520
example for computation of level service

00:10:04,200 --> 00:10:11,520
levels and so on we from riemann we can

00:10:08,520 --> 00:10:14,490
feed data into some of our other systems

00:10:11,520 --> 00:10:16,110
we are using Splunk in in Swisscom

00:10:14,490 --> 00:10:18,450
already since a long time so we're

00:10:16,110 --> 00:10:20,700
feeding some data to Splunk for archival

00:10:18,450 --> 00:10:22,890
and notification we are feeding data

00:10:20,700 --> 00:10:25,290
into an influx DB database for for

00:10:22,890 --> 00:10:27,450
longer term storage and we're feeding

00:10:25,290 --> 00:10:29,730
the dashboard from them and then we are

00:10:27,450 --> 00:10:31,260
closing the loop and you can roughly map

00:10:29,730 --> 00:10:34,980
this to the other loop that I showed you

00:10:31,260 --> 00:10:37,440
earlier by trying to trigger some

00:10:34,980 --> 00:10:39,060
automatic reactions to the to the

00:10:37,440 --> 00:10:42,000
failures this is still very early phase

00:10:39,060 --> 00:10:43,980
we have some automatic reactions built

00:10:42,000 --> 00:10:47,880
in but this is this is still very much

00:10:43,980 --> 00:10:51,840
work in progress so I'm going to hand

00:10:47,880 --> 00:10:56,910
the The Voice now to Bill so he can tell

00:10:51,840 --> 00:10:59,160
us about Kaos ID hello so as my team can

00:10:56,910 --> 00:11:00,750
attest to standing there silent for 9

00:10:59,160 --> 00:11:04,250
minutes 10 minutes with all of you

00:11:00,750 --> 00:11:04,250
people in front of me was very difficult

00:11:05,270 --> 00:11:09,980
so what is chaos Heidi Cassidy is a

00:11:07,370 --> 00:11:13,029
resiliency testing framework kind of

00:11:09,980 --> 00:11:17,170
like gas monkey chaos loris castle Emer

00:11:13,029 --> 00:11:19,910
Kass gorilla these are all products

00:11:17,170 --> 00:11:21,709
Castley most chaos lemur and Kass loris

00:11:19,910 --> 00:11:25,070
are actually bashed and Cloud Foundry

00:11:21,709 --> 00:11:29,390
based but this is a little different

00:11:25,070 --> 00:11:31,700
because whereas those solutions tend to

00:11:29,390 --> 00:11:35,680
be targeted at a specific type of attack

00:11:31,700 --> 00:11:38,110
like kill VMs or a specific platform

00:11:35,680 --> 00:11:42,880
mess with Cloud Foundry this is a

00:11:38,110 --> 00:11:45,320
generic attack framework written and go

00:11:42,880 --> 00:11:46,670
so as we said before I'm from Stark and

00:11:45,320 --> 00:11:48,770
Wayne and normally at Stark and Wayne we

00:11:46,670 --> 00:11:50,959
create stuff you know projects like

00:11:48,770 --> 00:11:55,730
shield that some of my colleagues spoke

00:11:50,959 --> 00:11:57,560
about earlier they're used for disaster

00:11:55,730 --> 00:11:59,029
recovery and protecting the data and

00:11:57,560 --> 00:12:02,870
making sure that we can come in and help

00:11:59,029 --> 00:12:04,130
you fix things but Swisscom I've had a

00:12:02,870 --> 00:12:05,390
relationship we've had a relationship

00:12:04,130 --> 00:12:07,580
with them for quite a while and they

00:12:05,390 --> 00:12:10,880
said you guys are pretty good at making

00:12:07,580 --> 00:12:19,459
stuff that keeps things running can you

00:12:10,880 --> 00:12:22,490
break stuff for us we can do that too so

00:12:19,459 --> 00:12:23,990
why do we want to break stuff well you

00:12:22,490 --> 00:12:27,800
know people will say well Cloud Foundry

00:12:23,990 --> 00:12:30,829
is self-healing or boss you can run a

00:12:27,800 --> 00:12:33,380
resurrector or this technology does this

00:12:30,829 --> 00:12:35,779
this technology does that well it turns

00:12:33,380 --> 00:12:39,020
out that the modern cloud stack is a

00:12:35,779 --> 00:12:41,630
really complex ecosystem both open

00:12:39,020 --> 00:12:45,529
source and vendor driven components that

00:12:41,630 --> 00:12:49,940
are glued together by locally developed

00:12:45,529 --> 00:12:53,149
bespoke solutions and I use this term

00:12:49,940 --> 00:12:55,370
separate Sempra vigilante and I always

00:12:53,149 --> 00:12:56,779
thought it meant always vigilant it

00:12:55,370 --> 00:12:59,300
turns out when I was putting the slide

00:12:56,779 --> 00:13:04,279
together it means always watching but I

00:12:59,300 --> 00:13:06,529
left to here anyways so this is you can

00:13:04,279 --> 00:13:08,600
think about as really aggressively

00:13:06,529 --> 00:13:09,980
watching your stack right because you're

00:13:08,600 --> 00:13:13,640
you're not just watching it you're

00:13:09,980 --> 00:13:15,050
poking at it like a little kid who can't

00:13:13,640 --> 00:13:16,670
look at anything without touching it and

00:13:15,050 --> 00:13:18,630
that's what cast ID is and that's where

00:13:16,670 --> 00:13:21,360
the name comes from

00:13:18,630 --> 00:13:24,540
so what might we want to do with a

00:13:21,360 --> 00:13:27,480
resiliency testing framework well things

00:13:24,540 --> 00:13:29,670
like stack validation running random

00:13:27,480 --> 00:13:33,959
daily attacks scenarios like you would

00:13:29,670 --> 00:13:35,190
do with a Netflix chaos monkey what we

00:13:33,959 --> 00:13:36,569
use it for now

00:13:35,190 --> 00:13:39,630
because we're still in the active

00:13:36,569 --> 00:13:41,850
development phase is for testing of

00:13:39,630 --> 00:13:45,120
orchard components so orchard has

00:13:41,850 --> 00:13:46,680
healing components and it's a lot easier

00:13:45,120 --> 00:13:49,620
to test those healing components if we

00:13:46,680 --> 00:13:50,730
can keep taking things down it can see

00:13:49,620 --> 00:13:53,759
that something's down and then try to

00:13:50,730 --> 00:13:54,180
fix it so how does it work that's pretty

00:13:53,759 --> 00:13:57,000
simple

00:13:54,180 --> 00:13:59,490
you've got a controller with handle

00:13:57,000 --> 00:13:59,880
scheduling orchestration monitoring

00:13:59,490 --> 00:14:03,769
things

00:13:59,880 --> 00:14:07,259
it holds all of the attack scripts that

00:14:03,769 --> 00:14:09,360
which are just attacks that can happen

00:14:07,259 --> 00:14:12,209
on an environment and then you have an

00:14:09,360 --> 00:14:14,279
agent now an agent is a small process

00:14:12,209 --> 00:14:16,410
that sits anywhere that has both the

00:14:14,279 --> 00:14:18,899
technologies and the access to the

00:14:16,410 --> 00:14:22,470
victim so you can think of maybe Bosch

00:14:18,899 --> 00:14:24,930
as a victim and the your boss your

00:14:22,470 --> 00:14:27,720
Bastion your boss jump host will be

00:14:24,930 --> 00:14:30,689
where you might put your heidi agent in

00:14:27,720 --> 00:14:32,970
practice the way our architecture is set

00:14:30,689 --> 00:14:36,209
up we would put the heidi agent anywhere

00:14:32,970 --> 00:14:38,189
we put a console agent because it's

00:14:36,209 --> 00:14:40,199
convenient that way and then with the

00:14:38,189 --> 00:14:45,810
puppet automation can just set that all

00:14:40,199 --> 00:14:47,399
up for us and an attack runs on an agent

00:14:45,810 --> 00:14:51,329
and it is a combination of a set of

00:14:47,399 --> 00:14:56,430
configuration and a script of some sort

00:14:51,329 --> 00:14:57,959
and that way we can have multiple

00:14:56,430 --> 00:15:00,060
attacks that do the same thing but with

00:14:57,959 --> 00:15:02,189
different parameters different targets

00:15:00,060 --> 00:15:03,899
let's say we want to kill a Bosch job we

00:15:02,189 --> 00:15:06,750
might have one that kills a runner and

00:15:03,899 --> 00:15:08,939
one that kills a J proxy and we can run

00:15:06,750 --> 00:15:11,250
them randomly it tracks holidays so that

00:15:08,939 --> 00:15:16,350
you don't kill stuff in production while

00:15:11,250 --> 00:15:18,329
your ops team is off it's interesting as

00:15:16,350 --> 00:15:22,529
I said some example attacks kill an open

00:15:18,329 --> 00:15:26,389
stack VM kill a boss job you can really

00:15:22,529 --> 00:15:30,860
do anything like I said it's not

00:15:26,389 --> 00:15:32,779
specifically related to a platform or

00:15:30,860 --> 00:15:35,730
target technology

00:15:32,779 --> 00:15:39,180
so how do we create useful attacks and

00:15:35,730 --> 00:15:40,709
number one here is probably where we get

00:15:39,180 --> 00:15:43,020
most of our attacks from what is broken

00:15:40,709 --> 00:15:46,050
right now we talked to the ops team and

00:15:43,020 --> 00:15:47,580
we say you know what's going on here we

00:15:46,050 --> 00:15:50,580
talk to the Cloud Foundry services team

00:15:47,580 --> 00:15:52,380
and they say this is what is now broken

00:15:50,580 --> 00:15:55,589
so what we can do is we can create an

00:15:52,380 --> 00:15:57,180
attack for them and then when they fix

00:15:55,589 --> 00:15:59,490
the problem we can then implement that

00:15:57,180 --> 00:16:02,670
fix as a healing action in orchard and

00:15:59,490 --> 00:16:05,279
now we have a regression for what that

00:16:02,670 --> 00:16:07,110
problem might have been what is broken

00:16:05,279 --> 00:16:09,630
in the past well if we ever run out of

00:16:07,110 --> 00:16:11,940
things that are currently on fire we can

00:16:09,630 --> 00:16:13,980
start looking into the past and say okay

00:16:11,940 --> 00:16:15,899
well what used to be broken and that

00:16:13,980 --> 00:16:17,850
again another layer of regression and

00:16:15,899 --> 00:16:20,430
then what will break in the future this

00:16:17,850 --> 00:16:23,459
isn't really an academic project this is

00:16:20,430 --> 00:16:24,600
a practical project it might get to the

00:16:23,459 --> 00:16:27,450
point where we have to come up with

00:16:24,600 --> 00:16:29,700
clever recipes that based on patterns

00:16:27,450 --> 00:16:31,560
that we've seen but I don't see what

00:16:29,700 --> 00:16:33,720
will break in the future to be a part of

00:16:31,560 --> 00:16:37,980
development for a while if any time soon

00:16:33,720 --> 00:16:40,709
I don't know we'll see so something that

00:16:37,980 --> 00:16:42,959
broke recently oh wait a minute sorry I

00:16:40,709 --> 00:16:47,480
like getting paid so we're gonna blame

00:16:42,959 --> 00:16:49,380
this one on the virtual network so so

00:16:47,480 --> 00:16:52,829
there's a problem we're dealing with

00:16:49,380 --> 00:16:58,829
right now where the network goes down

00:16:52,829 --> 00:17:02,579
and Bosh will lose connectivity with one

00:16:58,829 --> 00:17:07,199
of its jobs the resurrector will kick

00:17:02,579 --> 00:17:10,740
off but it'll fail and then we end up

00:17:07,199 --> 00:17:12,750
with this ghosted job and then somebody

00:17:10,740 --> 00:17:15,540
down the line might decide that they

00:17:12,750 --> 00:17:17,939
want to bring that back up again because

00:17:15,540 --> 00:17:19,890
it's they need it and now you end up

00:17:17,939 --> 00:17:21,179
with a new version of it and a ghosted

00:17:19,890 --> 00:17:22,620
version of it and this is a problem

00:17:21,179 --> 00:17:25,829
we're experiencing a production right

00:17:22,620 --> 00:17:29,130
now so so we've been trying to emulate

00:17:25,829 --> 00:17:31,410
it and it turns out it's a really

00:17:29,130 --> 00:17:32,910
fascinating process of trying to figure

00:17:31,410 --> 00:17:34,740
out how to break something

00:17:32,910 --> 00:17:36,980
you know you'd feel like oh I can do

00:17:34,740 --> 00:17:39,330
this you know breaking things is easy

00:17:36,980 --> 00:17:42,240
but we run into this problem of

00:17:39,330 --> 00:17:44,520
simulation versus emulation we can imply

00:17:42,240 --> 00:17:45,870
a failure or we can actually cause that

00:17:44,520 --> 00:17:48,330
failure but sometime call

00:17:45,870 --> 00:17:50,370
that failure isn't very practical right

00:17:48,330 --> 00:17:52,200
actually taking a whole segment of the

00:17:50,370 --> 00:17:55,200
network down isn't necessarily going to

00:17:52,200 --> 00:17:58,230
be as practical as monkeying with your

00:17:55,200 --> 00:18:00,420
hosts deny or monkeying with iptables

00:17:58,230 --> 00:18:02,340
just to make a single node think that

00:18:00,420 --> 00:18:04,440
there's a problem this turns out to be a

00:18:02,340 --> 00:18:07,770
really interesting and difficult problem

00:18:04,440 --> 00:18:10,110
to solve so here's an example from what

00:18:07,770 --> 00:18:11,970
we were just talking about we can kill a

00:18:10,110 --> 00:18:15,740
bosch VM so this is an attack that we

00:18:11,970 --> 00:18:19,770
have we say Heidi kill job in this bosch

00:18:15,740 --> 00:18:21,720
and so we take out this H a proxy well

00:18:19,770 --> 00:18:24,929
of course great we just tested the

00:18:21,720 --> 00:18:26,550
resurrector works excellent job that's

00:18:24,929 --> 00:18:29,790
not very helpful so what do we have to

00:18:26,550 --> 00:18:31,500
do we have to kill the resurrector so

00:18:29,790 --> 00:18:33,030
when out we kill the boss job and the

00:18:31,500 --> 00:18:34,620
resurrector but this isn't really the

00:18:33,030 --> 00:18:37,140
problem the problem started because of a

00:18:34,620 --> 00:18:41,520
network outage and if you run a c ck

00:18:37,140 --> 00:18:43,590
it's going to fix the problem anyway so

00:18:41,520 --> 00:18:45,990
now we simulate an outage between the

00:18:43,590 --> 00:18:47,880
boss job vm and the director and now we

00:18:45,990 --> 00:18:51,330
have an inch now we've recreated the

00:18:47,880 --> 00:18:54,630
problem and when someone tries to

00:18:51,330 --> 00:18:57,059
resurrect that vm we end up with a ghost

00:18:54,630 --> 00:19:02,429
@ vm great so we figured out how to

00:18:57,059 --> 00:19:06,300
simulate the error state but but what

00:19:02,429 --> 00:19:09,450
did we do well put a deny rule in

00:19:06,300 --> 00:19:11,760
iptables right and we end up with a

00:19:09,450 --> 00:19:14,130
situation where how are we training

00:19:11,760 --> 00:19:15,450
orchard to heal that because that's not

00:19:14,130 --> 00:19:17,130
what or that's not so going to happen in

00:19:15,450 --> 00:19:18,960
real time in the real world right unless

00:19:17,130 --> 00:19:21,030
someone maybe pushed a configuration

00:19:18,960 --> 00:19:23,429
change that was that they shouldn't have

00:19:21,030 --> 00:19:26,960
but more likely it's going to be related

00:19:23,429 --> 00:19:29,820
to a problem scaling or a problem with

00:19:26,960 --> 00:19:32,730
network traffic somewhere so how does

00:19:29,820 --> 00:19:34,500
orchard fix this and now I'm confused

00:19:32,730 --> 00:19:37,620
because they told me I just had to break

00:19:34,500 --> 00:19:38,880
stuff right but now we come around to

00:19:37,620 --> 00:19:41,580
the problem of how are we training

00:19:38,880 --> 00:19:44,429
orchard to properly fix things and this

00:19:41,580 --> 00:19:46,620
is where we're at right now we've got a

00:19:44,429 --> 00:19:49,679
suite of attacks that can do all kinds

00:19:46,620 --> 00:19:51,360
of really interesting things we've got a

00:19:49,679 --> 00:19:53,610
way to orchestrate it we've got a way to

00:19:51,360 --> 00:19:55,740
keep track of it and now we've got to

00:19:53,610 --> 00:19:58,050
figure out how to make those attacks

00:19:55,740 --> 00:19:59,670
useful so that we can train orchard to

00:19:58,050 --> 00:20:01,860
do what it needs to do

00:19:59,670 --> 00:20:05,190
it's currently an internal project at

00:20:01,860 --> 00:20:09,990
Swisscom I've been asked to gauge

00:20:05,190 --> 00:20:12,390
interest and see if there was interest

00:20:09,990 --> 00:20:14,040
in other people seeing this and open

00:20:12,390 --> 00:20:15,540
sourcing it I don't have the go-ahead to

00:20:14,040 --> 00:20:17,910
do that at this point but if there is

00:20:15,540 --> 00:20:20,040
enough interest in it then we can

00:20:17,910 --> 00:20:21,510
probably make the case and I think it's

00:20:20,040 --> 00:20:23,580
a really cool project that people would

00:20:21,510 --> 00:20:25,620
like to play with so if you do have any

00:20:23,580 --> 00:20:29,040
questions about that come talk to us so

00:20:25,620 --> 00:20:32,100
that we know and the egg you have

00:20:29,040 --> 00:20:34,770
anything else to say just to repeat what

00:20:32,100 --> 00:20:36,870
bill said come talk to us if you're

00:20:34,770 --> 00:20:39,210
interested this is still very much work

00:20:36,870 --> 00:20:40,620
in progress this is where we are right

00:20:39,210 --> 00:20:43,440
now but there's there's still a lot to

00:20:40,620 --> 00:20:45,540
do and if nothing else we have

00:20:43,440 --> 00:20:48,870
application cloud stickers so if you

00:20:45,540 --> 00:20:58,830
want to come and get one come talk to us

00:20:48,870 --> 00:20:59,830
thank you Thanks and we got you out of

00:20:58,830 --> 00:21:03,839
here quickly

00:20:59,830 --> 00:21:03,839
[Laughter]

00:21:15,120 --> 00:21:21,100
yeah I was wondering for the agents what

00:21:19,150 --> 00:21:23,290
what is that like is that did you write

00:21:21,100 --> 00:21:27,549
that yourself or your okay so the agents

00:21:23,290 --> 00:21:30,100
are there's a small go process that can

00:21:27,549 --> 00:21:32,980
run as either a controller or an agent

00:21:30,100 --> 00:21:34,510
node they communicate via RabbitMQ and I

00:21:32,980 --> 00:21:39,580
actually apologize I went way too fast

00:21:34,510 --> 00:21:42,669
through the architecture slide but the

00:21:39,580 --> 00:21:44,260
agent node has all of the configuration

00:21:42,669 --> 00:21:47,590
for any individual attack because we

00:21:44,260 --> 00:21:50,049
don't want to pass any credentials or

00:21:47,590 --> 00:21:51,549
anything over the wire so an aid agent

00:21:50,049 --> 00:21:53,980
knows everything about what it needs to

00:21:51,549 --> 00:21:55,570
do and all it does is run that script

00:21:53,980 --> 00:21:58,390
and then tell the controller what

00:21:55,570 --> 00:22:03,040
happened it has no healing component it

00:21:58,390 --> 00:22:06,580
it it's job is to fire off that kill

00:22:03,040 --> 00:22:08,049
command and and that's it it find out

00:22:06,580 --> 00:22:11,530
what happened tell the controller what

00:22:08,049 --> 00:22:18,520
happened and then wait for orchard to

00:22:11,530 --> 00:22:21,010
try to fix the problem no no this is uh

00:22:18,520 --> 00:22:25,090
this is an internally developed project

00:22:21,010 --> 00:22:28,360
it Swisscom yes do you have any

00:22:25,090 --> 00:22:29,890
suggestions have you seen this paradigm

00:22:28,360 --> 00:22:39,220
before that there's a different way you

00:22:29,890 --> 00:22:41,500
would do it well in our case that's not

00:22:39,220 --> 00:22:46,600
allowed that the access I mean we could

00:22:41,500 --> 00:22:47,980
probably use the puppet em collective we

00:22:46,600 --> 00:22:50,919
could probably use a puppet but we can't

00:22:47,980 --> 00:22:52,480
SSH into you know if you can't in this

00:22:50,919 --> 00:22:55,390
environment if we can't communicate via

00:22:52,480 --> 00:22:56,700
an API or some other way we Wow we have

00:22:55,390 --> 00:22:58,840
to have an agent and the agent

00:22:56,700 --> 00:23:03,330
communicates via the cue but there's no

00:22:58,840 --> 00:23:03,330
direct access yeah

00:23:11,300 --> 00:23:20,100
well no not right now we're not but it

00:23:15,960 --> 00:23:21,510
would be neat if we could be in the lab

00:23:20,100 --> 00:23:23,550
environments it's good for stack

00:23:21,510 --> 00:23:25,530
validation and testing new features and

00:23:23,550 --> 00:23:27,120
things like that but the end goal would

00:23:25,530 --> 00:23:32,780
be to maybe run it in production but

00:23:27,120 --> 00:23:32,780
that takes a much larger buy and I think

00:23:35,330 --> 00:23:41,670
well well Netflix chaos monkey and kala

00:23:38,520 --> 00:23:43,710
and chaos gorilla and the rest of the

00:23:41,670 --> 00:23:46,040
simian army project they do similar

00:23:43,710 --> 00:23:49,260
things to what Orchard as a whole does

00:23:46,040 --> 00:23:52,500
but gas monkey is really just dealing

00:23:49,260 --> 00:23:55,980
with killing VMs and chaos gorilla deals

00:23:52,500 --> 00:23:59,390
with network latency I think but that's

00:23:55,980 --> 00:24:00,990
such a small part of the the attack

00:23:59,390 --> 00:24:04,380
ecosystem that we're trying to

00:24:00,990 --> 00:24:06,420
accomplish that it was either piggyback

00:24:04,380 --> 00:24:07,560
on top of those or see how far we could

00:24:06,420 --> 00:24:10,760
get on our own because there were so

00:24:07,560 --> 00:24:10,760
many more attacks we needed to write

00:24:14,880 --> 00:24:18,359

YouTube URL: https://www.youtube.com/watch?v=Wr4E--kr_KE


