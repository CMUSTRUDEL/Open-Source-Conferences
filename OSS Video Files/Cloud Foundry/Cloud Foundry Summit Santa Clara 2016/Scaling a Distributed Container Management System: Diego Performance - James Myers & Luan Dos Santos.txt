Title: Scaling a Distributed Container Management System: Diego Performance - James Myers & Luan Dos Santos
Publication date: 2016-05-29
Playlist: Cloud Foundry Summit Santa Clara 2016
Description: 
	As a distributed container management system, it is important that Diego not only runs containers, but it does so in a quick and timely manner. In small deployments with manageable work loads, it is easy for us to guarantee that Diego adheres to these performance requirements. However, as both the scale of the deployment and the amount of containers increases, it has become increasingly challenging for us to validate that Diego maintains its performance characteristics. 

This talk will dive into the steps that we took to design, perform, evaluate, and improve performance testing that has given us confidence in Diego as a backend to Cloud Foundry. Topics will include: how the Diego team designed performance experiments and testing suites given product requirements, how we evaluate a performant Diego environment, results that led to various performance changes in the system, and lastly how the Diego team is continuing to build out infrastructure to validate Diegoâ€™s performance at a larger scale on a more regular basis. 

James Myers
Software Engineer, Pivotal
James Myers is a software engineer for Pivotal Software and a core contributor to the Cloud Foundry project. James is currently a member of the Diego team and has worked on Cloud Foundry for two years. He has presented previously at the 2015 North American CF Summit on the CF API.

Luan dos Santos
Software Engineer, Pivotal
Luan Santos is a Pivotal developer and core contributor to the Cloud Foundry platform. He is a member of the Diego team, located in San Francisco, CA. He also spoke at the CF Summit 2015 in Santa Clara, CA about the Cloud Foundry API.
Captions: 
	00:00:00,030 --> 00:00:07,980
so hi my name is LUN Santos I am a

00:00:04,620 --> 00:00:10,110
software engineer at pivotal and I am

00:00:07,980 --> 00:00:12,570
working on the dagger team and this is

00:00:10,110 --> 00:00:18,000
my friend Jim Myers who's also a

00:00:12,570 --> 00:00:19,949
software engineer at pivotal and then

00:00:18,000 --> 00:00:22,619
he's currently the anchor on the Diego

00:00:19,949 --> 00:00:25,769
team and we're gonna talk to you about

00:00:22,619 --> 00:00:29,460
our work on scaling and performance of

00:00:25,769 --> 00:00:31,410
the Diego the cos foundry distributed

00:00:29,460 --> 00:00:35,579
container management system

00:00:31,410 --> 00:00:39,739
more specifically what Diego is a quick

00:00:35,579 --> 00:00:42,809
overview of the system how we defined

00:00:39,739 --> 00:00:46,200
performance and scaling goals as a

00:00:42,809 --> 00:00:48,210
product definition how we design our

00:00:46,200 --> 00:00:51,149
performance experiments and how we

00:00:48,210 --> 00:00:54,180
evaluate success and then a little bit

00:00:51,149 --> 00:00:58,649
about the future of the work and scaling

00:00:54,180 --> 00:01:01,620
that we're doing on Diego right now so

00:00:58,649 --> 00:01:03,809
what is Diego on a high level Diego is a

00:01:01,620 --> 00:01:06,210
distributed container management system

00:01:03,809 --> 00:01:10,439
what it means is that it schedules and

00:01:06,210 --> 00:01:12,240
runs continued eyes workloads and Diego

00:01:10,439 --> 00:01:14,430
is built from the ground up for cloud

00:01:12,240 --> 00:01:18,420
foundry do you replace the old Ruby

00:01:14,430 --> 00:01:21,990
runtime the DA's to give the platform

00:01:18,420 --> 00:01:25,740
more portability stability and

00:01:21,990 --> 00:01:28,350
scalability and even though it's a

00:01:25,740 --> 00:01:31,290
general-purpose container scheduler all

00:01:28,350 --> 00:01:34,049
of its use cases are driven by the cloud

00:01:31,290 --> 00:01:36,479
from cloud foundry platform such as

00:01:34,049 --> 00:01:38,610
building and running built pack based

00:01:36,479 --> 00:01:40,920
applications building and running docker

00:01:38,610 --> 00:01:45,720
based applications and also running

00:01:40,920 --> 00:01:47,040
Windows dot add applications so as we as

00:01:45,720 --> 00:01:49,079
we go through this talk we're going to

00:01:47,040 --> 00:01:52,560
touch on a few components of Diego that

00:01:49,079 --> 00:01:53,670
you can see in this picture and and some

00:01:52,560 --> 00:01:57,329
of the ones we're going to talk about

00:01:53,670 --> 00:01:59,670
our the BBS which is the central Diego

00:01:57,329 --> 00:02:03,060
API everything that happens in the Diego

00:01:59,670 --> 00:02:06,420
cluster has to go through that that API

00:02:03,060 --> 00:02:10,379
and this is where the truth is saved

00:02:06,420 --> 00:02:13,890
basically and that that storage is

00:02:10,379 --> 00:02:15,630
backed by a key value store called HED

00:02:13,890 --> 00:02:18,870
which is a distributed key-value store

00:02:15,630 --> 00:02:23,520
that provides us reliable access to that

00:02:18,870 --> 00:02:26,520
data another component is the cell the

00:02:23,520 --> 00:02:29,610
cell is ultimately where all the

00:02:26,520 --> 00:02:32,400
containers are run it is the capacity of

00:02:29,610 --> 00:02:35,190
a Diego cluster to put simply the more

00:02:32,400 --> 00:02:39,180
cells we can manage the more containers

00:02:35,190 --> 00:02:42,120
we're going to be able to run and it is

00:02:39,180 --> 00:02:44,580
also the cell itself is composed of a

00:02:42,120 --> 00:02:46,650
few components one of the ones we're

00:02:44,580 --> 00:02:50,730
going to talk about today is as garden

00:02:46,650 --> 00:02:53,010
and garden is the successor to warden

00:02:50,730 --> 00:02:56,480
the container of the Cloud Foundry

00:02:53,010 --> 00:03:00,930
container technology and garden is a

00:02:56,480 --> 00:03:04,580
platform agnostic API to expose a

00:03:00,930 --> 00:03:07,170
container and process management without

00:03:04,580 --> 00:03:09,180
having without the system having to

00:03:07,170 --> 00:03:12,470
worry about the details of platform

00:03:09,180 --> 00:03:15,150
specific things and a few

00:03:12,470 --> 00:03:17,180
implementations that garden has today

00:03:15,150 --> 00:03:19,890
that are supported our garden Linux

00:03:17,180 --> 00:03:22,140
garden windows and coming soon garden

00:03:19,890 --> 00:03:23,610
Runcie and the one we're using in

00:03:22,140 --> 00:03:26,400
production right now and the one we used

00:03:23,610 --> 00:03:29,310
for these performance tests is garden

00:03:26,400 --> 00:03:30,690
Linux and garden Linux is the Linux

00:03:29,310 --> 00:03:34,170
implementation of garden as the name

00:03:30,690 --> 00:03:36,570
suggests another term we're gonna use a

00:03:34,170 --> 00:03:40,049
little bit as batch processes or bulk

00:03:36,570 --> 00:03:43,470
processes they are routines that run

00:03:40,049 --> 00:03:46,500
periodically on Diego and they're

00:03:43,470 --> 00:03:49,980
responsible for keeping the consistency

00:03:46,500 --> 00:03:53,880
of the system so for example if in a

00:03:49,980 --> 00:03:57,690
catastrophic situation we lose all of

00:03:53,880 --> 00:04:00,480
our data or part of our database one of

00:03:57,690 --> 00:04:04,440
these batch processes will will run on

00:04:00,480 --> 00:04:07,320
its period and grab all the desired

00:04:04,440 --> 00:04:09,209
state from the Cloud Foundry API using

00:04:07,320 --> 00:04:11,959
the Cloud Controller API and then

00:04:09,209 --> 00:04:17,100
repopulate Diego using the Diego API

00:04:11,959 --> 00:04:20,789
does like read restoring the health of

00:04:17,100 --> 00:04:23,789
the cluster and then the the auctioneer

00:04:20,789 --> 00:04:25,919
is the central logic server for Diego

00:04:23,789 --> 00:04:27,630
it's the component responsible for

00:04:25,919 --> 00:04:29,700
making decisions and scheduling

00:04:27,630 --> 00:04:31,410
so it's responsible for deciding which

00:04:29,700 --> 00:04:33,900
cell is gonna run which workload or

00:04:31,410 --> 00:04:35,520
which batch of workload so it needs to

00:04:33,900 --> 00:04:39,210
have an efficient way of getting an

00:04:35,520 --> 00:04:41,640
accurate picture of what the cluster

00:04:39,210 --> 00:04:43,860
looks like at any given point in time

00:04:41,640 --> 00:04:46,830
and lastly tasks and long-running

00:04:43,860 --> 00:04:49,920
processes they are the units of work on

00:04:46,830 --> 00:04:51,390
Diego so when you when you ask Diego to

00:04:49,920 --> 00:04:54,330
do something you're asking Diego to run

00:04:51,390 --> 00:04:57,690
a task or a long-running process a task

00:04:54,330 --> 00:05:00,330
is a one-off action that runs at most

00:04:57,690 --> 00:05:02,730
once and has no uptime guarantees

00:05:00,330 --> 00:05:06,360
because it's expected to end and a

00:05:02,730 --> 00:05:08,190
long-running processes is something that

00:05:06,360 --> 00:05:13,680
Diego will monitor and guarantee that it

00:05:08,190 --> 00:05:17,340
stays up um so how do we define a

00:05:13,680 --> 00:05:20,220
scaling goal for Diego so it's largely a

00:05:17,340 --> 00:05:22,260
product decision we need to know how

00:05:20,220 --> 00:05:24,240
many cells we want to support and how

00:05:22,260 --> 00:05:28,800
many application instances we want to

00:05:24,240 --> 00:05:31,710
run in those cells so the first part of

00:05:28,800 --> 00:05:33,750
the work is defining where we stand so

00:05:31,710 --> 00:05:35,250
what what are the numbers we can

00:05:33,750 --> 00:05:36,930
currently support or what were the

00:05:35,250 --> 00:05:39,360
numbers we could currently support when

00:05:36,930 --> 00:05:41,720
we started this and then decide where we

00:05:39,360 --> 00:05:44,990
want to be short term and long term

00:05:41,720 --> 00:05:48,500
so for Diego we initially targeted a

00:05:44,990 --> 00:05:52,790
larger than average deployment size

00:05:48,500 --> 00:05:56,460
which for us meant a hundred cells

00:05:52,790 --> 00:06:00,150
running around ten thousand application

00:05:56,460 --> 00:06:03,570
instances that is a modest goal it's not

00:06:00,150 --> 00:06:05,400
those are not huge numbers but it's a

00:06:03,570 --> 00:06:07,500
stepping stone and shoe more aggressive

00:06:05,400 --> 00:06:11,160
numbers that we want to hit in the

00:06:07,500 --> 00:06:15,560
future and it allowed us to flesh out a

00:06:11,160 --> 00:06:15,560
few bottlenecks of the system early on

00:06:15,980 --> 00:06:20,820
as Caitlin performance is also an

00:06:18,360 --> 00:06:23,190
engineering concern because as engineers

00:06:20,820 --> 00:06:25,470
we want to be proud of the software we

00:06:23,190 --> 00:06:26,970
are developing and we want to have a

00:06:25,470 --> 00:06:30,170
good story to tell when we're asked

00:06:26,970 --> 00:06:32,760
questions about how do we compare versus

00:06:30,170 --> 00:06:37,610
some other container schedulers out

00:06:32,760 --> 00:06:40,530
there in the cloud ecosystem and well

00:06:37,610 --> 00:06:41,190
that comparison is important is not the

00:06:40,530 --> 00:06:44,730
prime

00:06:41,190 --> 00:06:46,320
the objective of this experiment but

00:06:44,730 --> 00:06:49,890
it's important for us to know where we

00:06:46,320 --> 00:06:50,880
stand amongst these members of the

00:06:49,890 --> 00:06:53,510
ecosystem

00:06:50,880 --> 00:06:56,220
so now performance is part of the car

00:06:53,510 --> 00:06:58,260
process of development of the Diego team

00:06:56,220 --> 00:07:00,780
and we as engineers have to keep an eye

00:06:58,260 --> 00:07:02,190
open for opportunities to make the

00:07:00,780 --> 00:07:09,990
system scale and make the system better

00:07:02,190 --> 00:07:11,850
in that way cool so yeah now that we've

00:07:09,990 --> 00:07:13,740
defined a set of performance goals that

00:07:11,850 --> 00:07:15,780
address both the product and the

00:07:13,740 --> 00:07:17,400
engineering teams concerns we actually

00:07:15,780 --> 00:07:19,260
now need to design observable and

00:07:17,400 --> 00:07:21,090
reproducible experiments that we can use

00:07:19,260 --> 00:07:23,550
to judge the performance characteristics

00:07:21,090 --> 00:07:24,750
of a Diego deployment the first thing we

00:07:23,550 --> 00:07:27,090
need to be able to do is to measure

00:07:24,750 --> 00:07:29,040
performance this means we need to define

00:07:27,090 --> 00:07:30,360
what it means to be successful when

00:07:29,040 --> 00:07:32,520
you're running at Diego deployment at

00:07:30,360 --> 00:07:34,320
scale from product we know this means

00:07:32,520 --> 00:07:36,020
that we want to run a number of

00:07:34,320 --> 00:07:38,340
instances against some number of cells

00:07:36,020 --> 00:07:39,810
however there's more than that we also

00:07:38,340 --> 00:07:42,120
know that we want to have all of our

00:07:39,810 --> 00:07:43,080
containers be routable at all times we

00:07:42,120 --> 00:07:46,290
want to make sure that the performance

00:07:43,080 --> 00:07:48,000
of our BBS api is acceptable and that

00:07:46,290 --> 00:07:51,360
the bulk process loops are operating

00:07:48,000 --> 00:07:52,770
under reasonable time limits through

00:07:51,360 --> 00:07:54,330
both metrics and loglines

00:07:52,770 --> 00:07:56,280
emitted to logger gator we're actually

00:07:54,330 --> 00:07:58,200
able to monitor the state of this system

00:07:56,280 --> 00:08:01,200
and like get a complete image of

00:07:58,200 --> 00:08:02,790
performance at any given time as well as

00:08:01,200 --> 00:08:04,560
being able to measure and monitor the

00:08:02,790 --> 00:08:06,290
health of the system we also need to

00:08:04,560 --> 00:08:09,720
design experiments that will produce

00:08:06,290 --> 00:08:11,220
meaningful data the questions we have to

00:08:09,720 --> 00:08:13,860
answer are like how do we generally

00:08:11,220 --> 00:08:16,740
generate a realistic load to run on

00:08:13,860 --> 00:08:19,470
Diego also how does Diego perform under

00:08:16,740 --> 00:08:22,770
unusual situations such as bursty loads

00:08:19,470 --> 00:08:24,780
or failures when designing the

00:08:22,770 --> 00:08:26,010
performance test Suites we needed to

00:08:24,780 --> 00:08:27,870
make sure that we generate the correct

00:08:26,010 --> 00:08:29,520
artificial load so that we can have

00:08:27,870 --> 00:08:33,090
confidence in the system and the metrics

00:08:29,520 --> 00:08:34,620
that we obtain so one of the ways we

00:08:33,090 --> 00:08:36,510
measure performance is through CFC own

00:08:34,620 --> 00:08:38,820
internal logging and metric aggregation

00:08:36,510 --> 00:08:40,770
logger gator we're able to emit various

00:08:38,820 --> 00:08:43,140
metrics and then use tools such as data

00:08:40,770 --> 00:08:44,700
dog to generate charts and graphs that

00:08:43,140 --> 00:08:47,070
allow us to visualize metrics in the

00:08:44,700 --> 00:08:48,390
system in these graphs were able to

00:08:47,070 --> 00:08:50,190
track things like total number of

00:08:48,390 --> 00:08:52,770
application instances and the state in

00:08:50,190 --> 00:08:54,150
the deployment as well as the total

00:08:52,770 --> 00:08:55,000
number of routes the throughput and

00:08:54,150 --> 00:08:58,650
latency of the

00:08:55,000 --> 00:09:01,810
yes and very other many other concerns

00:08:58,650 --> 00:09:04,120
another thing along with the metrics the

00:09:01,810 --> 00:09:07,330
remaining is the internal system logging

00:09:04,120 --> 00:09:09,310
of components itself they allow us to

00:09:07,330 --> 00:09:10,720
take a more detailed look at what is

00:09:09,310 --> 00:09:13,750
actually happening on Diego at any given

00:09:10,720 --> 00:09:15,820
time by post-processing these logs we're

00:09:13,750 --> 00:09:17,860
able to create these beautiful graphs

00:09:15,820 --> 00:09:20,050
like the one seen here and so what we do

00:09:17,860 --> 00:09:22,540
for that is we tagged log lines with

00:09:20,050 --> 00:09:25,840
tasks and our P Goods and then we can

00:09:22,540 --> 00:09:27,910
actually map where the individual

00:09:25,840 --> 00:09:30,940
workload is in the system at any given

00:09:27,910 --> 00:09:33,070
time so this chart right here is a chart

00:09:30,940 --> 00:09:34,750
of 4,000 tasks that are running on Diego

00:09:33,070 --> 00:09:38,590
and each color represents a different

00:09:34,750 --> 00:09:41,200
set of its life time and then the x-axis

00:09:38,590 --> 00:09:42,640
is the time that it took this is a great

00:09:41,200 --> 00:09:44,680
way for us to visualize performance

00:09:42,640 --> 00:09:46,090
bottlenecks and help us immediately find

00:09:44,680 --> 00:09:52,210
these bottlenecks so that we can solve

00:09:46,090 --> 00:09:54,100
them quickly so our first experiment was

00:09:52,210 --> 00:09:56,710
designed as a smoke test she simply

00:09:54,100 --> 00:10:00,850
validate basic aspects of performance of

00:09:56,710 --> 00:10:05,470
in Diego they consists of running larger

00:10:00,850 --> 00:10:07,450
and larger number of workloads in in a

00:10:05,470 --> 00:10:09,940
short period of time and these workloads

00:10:07,450 --> 00:10:14,290
are just one-off tasks like the ones

00:10:09,940 --> 00:10:17,110
that Jim showed and also some long

00:10:14,290 --> 00:10:19,660
running processes more specifically we

00:10:17,110 --> 00:10:22,839
spun up 4,000 instances of a lightweight

00:10:19,660 --> 00:10:25,780
web server and - and measured how long

00:10:22,839 --> 00:10:28,750
that web server took to become healthy

00:10:25,780 --> 00:10:30,580
on all those 4,000 instances that is

00:10:28,750 --> 00:10:32,410
comparable to what some of the other

00:10:30,580 --> 00:10:35,500
container schedulers are publishing as

00:10:32,410 --> 00:10:38,440
their performance benchmarks and while

00:10:35,500 --> 00:10:41,290
this isn't a necessarily a realistic

00:10:38,440 --> 00:10:44,410
workload for a production grade Diego

00:10:41,290 --> 00:10:46,510
deployment it did allow us to find some

00:10:44,410 --> 00:10:48,220
obvious button acts really quickly

00:10:46,510 --> 00:10:50,230
one other reasons is does not represent

00:10:48,220 --> 00:10:52,900
a realistic workload is that it only

00:10:50,230 --> 00:10:57,820
fills up up to 40 percent of our

00:10:52,900 --> 00:11:02,680
capacity roughly so right from the start

00:10:57,820 --> 00:11:07,450
of these smoke tests our tools allowed

00:11:02,680 --> 00:11:08,950
us to quickly observe a few obvious

00:11:07,450 --> 00:11:12,280
problems with the system so if

00:11:08,950 --> 00:11:16,500
exemple we were spending a lot of time

00:11:12,280 --> 00:11:19,120
in none marshaling and marshaling Jason

00:11:16,500 --> 00:11:22,590
partially on Diego's own codebase and

00:11:19,120 --> 00:11:26,470
partially on etsy DS codebase when they

00:11:22,590 --> 00:11:28,980
read their data from datastore and the

00:11:26,470 --> 00:11:32,590
other thing we found is that we had

00:11:28,980 --> 00:11:34,630
hard-coded concurrency variables such as

00:11:32,590 --> 00:11:37,840
like amount of treads are gonna run for

00:11:34,630 --> 00:11:40,330
a certain batch processing so those were

00:11:37,840 --> 00:11:42,520
the two first obvious things we found

00:11:40,330 --> 00:11:46,030
with the smoke tests and to fix that

00:11:42,520 --> 00:11:48,930
firstly we replaced Jason with protocol

00:11:46,030 --> 00:11:54,090
buffers which is a mechanism to

00:11:48,930 --> 00:11:58,390
efficiently transmit structured data

00:11:54,090 --> 00:12:00,850
which is both smaller and faster than

00:11:58,390 --> 00:12:03,310
Jason but also was really good for us

00:12:00,850 --> 00:12:05,550
because of the tooling that the

00:12:03,310 --> 00:12:09,520
community provides for cogeneration and

00:12:05,550 --> 00:12:11,710
going for for these protocol buffers the

00:12:09,520 --> 00:12:15,700
other thing we did was we replaced a

00:12:11,710 --> 00:12:18,370
rest with RPC and that allows us allowed

00:12:15,700 --> 00:12:21,790
us to express our API in a more concise

00:12:18,370 --> 00:12:23,530
way and then reducing the amount of

00:12:21,790 --> 00:12:28,590
network calls we do for simple

00:12:23,530 --> 00:12:31,600
operations and finally to to fix the the

00:12:28,590 --> 00:12:34,390
concurrency problem we made Diego more

00:12:31,600 --> 00:12:36,700
configurable so as you grow your Diego

00:12:34,390 --> 00:12:39,000
cluster as you give those gems more

00:12:36,700 --> 00:12:41,830
resources you can also grow those

00:12:39,000 --> 00:12:43,060
concurrency variables it seems like an

00:12:41,830 --> 00:12:48,970
obvious thing but it's something that we

00:12:43,060 --> 00:12:50,590
missed initially and once we fixed those

00:12:48,970 --> 00:12:53,320
obvious problems we were able to get

00:12:50,590 --> 00:12:55,480
some meaningful results such as we were

00:12:53,320 --> 00:12:57,880
able to run those 4,000 instances in

00:12:55,480 --> 00:13:00,460
less than 30 seconds and while 30

00:12:57,880 --> 00:13:02,950
seconds is not a fantastic time it is

00:13:00,460 --> 00:13:04,690
within the boundaries that we consider a

00:13:02,950 --> 00:13:08,680
reasonable amount of time to start all

00:13:04,690 --> 00:13:10,690
this all of this work and we decided to

00:13:08,680 --> 00:13:12,430
focus on growing the number of

00:13:10,690 --> 00:13:17,290
containers we can support on the

00:13:12,430 --> 00:13:22,030
horizontal scale rather than reducing

00:13:17,290 --> 00:13:22,840
this this amount of time one other thing

00:13:22,030 --> 00:13:27,190
we noticed after

00:13:22,840 --> 00:13:30,090
fixing these initial problems was the

00:13:27,190 --> 00:13:33,010
time spent on Diego components was

00:13:30,090 --> 00:13:34,390
significantly lower and if you put in

00:13:33,010 --> 00:13:36,400
comparison with how long it actually

00:13:34,390 --> 00:13:38,950
takes just spin up a container image

00:13:36,400 --> 00:13:40,870
it's almost irrelevant if you look at

00:13:38,950 --> 00:13:43,390
those charts there in real numbers but

00:13:40,870 --> 00:13:46,180
they're based on the numbers from the

00:13:43,390 --> 00:13:48,130
Union the experiment the green bar there

00:13:46,180 --> 00:13:50,020
is the time it takes you spin up a

00:13:48,130 --> 00:13:53,020
container image and the other colors are

00:13:50,020 --> 00:13:57,850
time spent in an actual Diego code so

00:13:53,020 --> 00:14:00,070
that was really encouraging and after it

00:13:57,850 --> 00:14:03,360
is we we felt confident enough to start

00:14:00,070 --> 00:14:07,090
the real full-scale performance test

00:14:03,360 --> 00:14:11,340
because we we didn't find any other

00:14:07,090 --> 00:14:11,340
obvious bottlenecks on the smoke test so

00:14:11,910 --> 00:14:17,680
cool so now that we've proven that do

00:14:15,160 --> 00:14:20,470
can run bursty workloads with no obvious

00:14:17,680 --> 00:14:22,060
performance issues we began to design an

00:14:20,470 --> 00:14:23,830
experiment that's gonna more accurately

00:14:22,060 --> 00:14:25,180
reflect the production style workloads

00:14:23,830 --> 00:14:27,760
that we want to run on this environment

00:14:25,180 --> 00:14:30,700
so the tests that we designed operates

00:14:27,760 --> 00:14:33,310
at like a CF level it CF pushes of

00:14:30,700 --> 00:14:35,710
various set of various applications that

00:14:33,310 --> 00:14:38,140
will load up the environment until it's

00:14:35,710 --> 00:14:40,060
completely saturated full capacity so in

00:14:38,140 --> 00:14:43,060
this case for a hundred cells we push

00:14:40,060 --> 00:14:44,830
10,000 application instances each of the

00:14:43,060 --> 00:14:46,600
different applications stresses the

00:14:44,830 --> 00:14:48,220
environment in a different way we had

00:14:46,600 --> 00:14:50,500
some applications that were very CPU

00:14:48,220 --> 00:14:52,660
intensive and generated a lot of logs we

00:14:50,500 --> 00:14:54,360
also had other applications that would

00:14:52,660 --> 00:14:56,440
crash after a reasonable amount of time

00:14:54,360 --> 00:14:58,150
by using this diverse set of

00:14:56,440 --> 00:15:00,090
applications we're really looking to

00:14:58,150 --> 00:15:01,960
show it like to show a reasonable

00:15:00,090 --> 00:15:05,320
user-provided workload on the

00:15:01,960 --> 00:15:07,090
environment after we successfully

00:15:05,320 --> 00:15:10,240
saturated the environment we actually

00:15:07,090 --> 00:15:11,860
just let it sit for about a week the

00:15:10,240 --> 00:15:13,180
idea here is that we can monitor it

00:15:11,860 --> 00:15:14,530
throughout the sitting time period and

00:15:13,180 --> 00:15:16,540
we shouldn't see any performance

00:15:14,530 --> 00:15:19,980
degradation over this time period

00:15:16,540 --> 00:15:22,210
and then after that we can actually

00:15:19,980 --> 00:15:24,310
since we have a saturated environment we

00:15:22,210 --> 00:15:25,960
can push generalized workloads against

00:15:24,310 --> 00:15:27,820
this fully saturated environment to give

00:15:25,960 --> 00:15:30,220
us additional insight into how Diego

00:15:27,820 --> 00:15:33,820
might react to common workloads at this

00:15:30,220 --> 00:15:35,410
scale this test suite closely reflects

00:15:33,820 --> 00:15:36,190
the initial product goal of being able

00:15:35,410 --> 00:15:38,470
to support

00:15:36,190 --> 00:15:40,050
the given number of application

00:15:38,470 --> 00:15:42,339
instances on a given number of cells

00:15:40,050 --> 00:15:44,529
it's important to note that while we

00:15:42,339 --> 00:15:46,269
strive to compete with our competitors

00:15:44,529 --> 00:15:48,040
and with competitors numbers in the

00:15:46,269 --> 00:15:49,300
previous smoke tests this test suite is

00:15:48,040 --> 00:15:51,129
aimed to accurately reflect the

00:15:49,300 --> 00:15:53,379
performance of Diego to the end-users

00:15:51,129 --> 00:15:55,509
and the operators we want to show that

00:15:53,379 --> 00:15:56,740
dia can run and a large workload for an

00:15:55,509 --> 00:15:58,720
extended period of time with no

00:15:56,740 --> 00:16:00,189
performance degradation rather than

00:15:58,720 --> 00:16:04,810
proving that Diego can run a generally

00:16:00,189 --> 00:16:07,240
unrealistic workload very fast so in our

00:16:04,810 --> 00:16:09,129
real experiment on 100 cells we push

00:16:07,240 --> 00:16:10,600
10000 application instances and we were

00:16:09,129 --> 00:16:12,430
happy to see that we didn't see any

00:16:10,600 --> 00:16:15,129
performance degradation over time the

00:16:12,430 --> 00:16:16,959
BBS API request latency increase from

00:16:15,129 --> 00:16:19,720
almost nothing on a non loaded

00:16:16,959 --> 00:16:20,800
environment to less than a second which

00:16:19,720 --> 00:16:23,470
we felt like it was a reasonable number

00:16:20,800 --> 00:16:25,449
we also saw a very similar pattern for

00:16:23,470 --> 00:16:26,980
all the bulk processing loops they were

00:16:25,449 --> 00:16:28,990
almost non-existent and then they

00:16:26,980 --> 00:16:31,540
increased two times that were much less

00:16:28,990 --> 00:16:33,100
than their operating periods we were

00:16:31,540 --> 00:16:34,689
also able to push new workloads onto

00:16:33,100 --> 00:16:36,639
this environment quite successfully and

00:16:34,689 --> 00:16:38,350
ultimately this experiment gave us a

00:16:36,639 --> 00:16:42,730
huge amount of confidence and running

00:16:38,350 --> 00:16:44,800
Diego in a production scale system the

00:16:42,730 --> 00:16:47,199
other thing though is since we had Diego

00:16:44,800 --> 00:16:50,680
running in the the happy path we also

00:16:47,199 --> 00:16:52,420
wanted to test failure scenarios so now

00:16:50,680 --> 00:16:54,550
that we had a saturated environment we

00:16:52,420 --> 00:16:56,050
actually tried out separate like failure

00:16:54,550 --> 00:16:57,509
scenarios on this environment to see how

00:16:56,050 --> 00:16:59,620
Diego will react

00:16:57,509 --> 00:17:02,170
the first failure mode that we

00:16:59,620 --> 00:17:05,230
investigated was the partial failure of

00:17:02,170 --> 00:17:07,390
the database we interpreted this as a

00:17:05,230 --> 00:17:09,250
loss of connectivity to the database so

00:17:07,390 --> 00:17:10,990
the way that we simulated this was by

00:17:09,250 --> 00:17:12,939
stopping the console agents on all of

00:17:10,990 --> 00:17:14,199
the database VMs this essentially made

00:17:12,939 --> 00:17:18,010
them unreachable from all of the

00:17:14,199 --> 00:17:19,750
internal services in Diego after doing

00:17:18,010 --> 00:17:21,370
so the one thing that we see is that all

00:17:19,750 --> 00:17:23,530
of the applications that are currently

00:17:21,370 --> 00:17:25,839
running continue to stay running and our

00:17:23,530 --> 00:17:27,179
routable this was entirely intentional

00:17:25,839 --> 00:17:29,740
and it's one of Diego's design

00:17:27,179 --> 00:17:32,620
principles is that running workloads are

00:17:29,740 --> 00:17:34,330
the top priority we weren't able to

00:17:32,620 --> 00:17:36,220
desire new work as expected because you

00:17:34,330 --> 00:17:37,900
can't talk to the database but once the

00:17:36,220 --> 00:17:39,820
connectivity was restored by restarting

00:17:37,900 --> 00:17:43,030
the console agent the system returned to

00:17:39,820 --> 00:17:44,409
a stable state the next failure that we

00:17:43,030 --> 00:17:45,429
wanted to investigate was a total

00:17:44,409 --> 00:17:47,950
failure of the database known

00:17:45,429 --> 00:17:49,779
essentially losing all of the data so

00:17:47,950 --> 00:17:51,489
the way we simulated this was

00:17:49,779 --> 00:17:53,289
by removing the persistent store on the

00:17:51,489 --> 00:17:55,659
database node so we just blew away all

00:17:53,289 --> 00:17:57,700
of the data and once again we see

00:17:55,659 --> 00:18:00,639
similar results to what we saw with the

00:17:57,700 --> 00:18:01,989
partial outage the cells and running

00:18:00,639 --> 00:18:04,149
applications continue to operate

00:18:01,989 --> 00:18:06,219
successfully and were routable and then

00:18:04,149 --> 00:18:08,320
once the BBS functionality was restored

00:18:06,219 --> 00:18:09,700
the cells immediately populated all of

00:18:08,320 --> 00:18:12,879
the running data back into the database

00:18:09,700 --> 00:18:14,499
and the bulk batch processes eventually

00:18:12,879 --> 00:18:15,940
took all of the definitions from Cloud

00:18:14,499 --> 00:18:18,639
Controller and put in the desired data

00:18:15,940 --> 00:18:20,679
as well in about 10 to 15 minutes we had

00:18:18,639 --> 00:18:24,129
restored all of the database that we had

00:18:20,679 --> 00:18:25,749
just deleted the last failure scenario

00:18:24,129 --> 00:18:27,219
that we wanted to test was the failure

00:18:25,749 --> 00:18:29,289
of a cell itself with running

00:18:27,219 --> 00:18:31,179
applications in order to simulate this

00:18:29,289 --> 00:18:33,460
we just destroyed the VM in the

00:18:31,179 --> 00:18:35,469
infrastructure we observed that within

00:18:33,460 --> 00:18:37,089
30 seconds one of our batch processes

00:18:35,469 --> 00:18:39,099
noticed that all this workload was

00:18:37,089 --> 00:18:42,700
missing and it actually rebalanced it

00:18:39,099 --> 00:18:44,080
back on to the remaining cells we did

00:18:42,700 --> 00:18:45,489
lose routes for the short period of time

00:18:44,080 --> 00:18:47,080
while this happened because the

00:18:45,489 --> 00:18:48,519
applications were no longer running but

00:18:47,080 --> 00:18:50,559
as soon as they were started up on the

00:18:48,519 --> 00:18:53,289
other remaining cells the route ability

00:18:50,559 --> 00:18:55,210
was restored and everything was great

00:18:53,289 --> 00:18:57,070
once we killed enough cells so that we

00:18:55,210 --> 00:18:59,679
had too much desired work for the

00:18:57,070 --> 00:19:00,759
capacity that was available one thing

00:18:59,679 --> 00:19:02,589
that we noticed was that running

00:19:00,759 --> 00:19:04,629
applications were actually favored over

00:19:02,589 --> 00:19:06,339
crashing applications this is largely

00:19:04,629 --> 00:19:07,929
due to the back off that we have when we

00:19:06,339 --> 00:19:09,940
schedule applications that have crashed

00:19:07,929 --> 00:19:11,469
this is pretty important because it

00:19:09,940 --> 00:19:13,359
shows that a Diego deployment will

00:19:11,469 --> 00:19:15,539
actually run workloads that work rather

00:19:13,359 --> 00:19:17,889
than workloads that are misconfigured

00:19:15,539 --> 00:19:20,259
once we restored all the cells as well

00:19:17,889 --> 00:19:24,190
all of the workload was once again like

00:19:20,259 --> 00:19:25,419
able to run on the system so with these

00:19:24,190 --> 00:19:26,979
performance experience we're pretty

00:19:25,419 --> 00:19:28,629
confident right now that Diego could run

00:19:26,979 --> 00:19:31,149
a reasonable number of instances on a

00:19:28,629 --> 00:19:32,739
hundred cells but as Cloud Foundry keeps

00:19:31,149 --> 00:19:34,210
growing we know that we're gonna have to

00:19:32,739 --> 00:19:35,859
run environments that are in order of

00:19:34,210 --> 00:19:39,099
magnitude larger than what we did in our

00:19:35,859 --> 00:19:41,679
tests so for example our new performance

00:19:39,099 --> 00:19:43,210
target is a thousand nodes and 200,000

00:19:41,679 --> 00:19:45,759
application instances spread across the

00:19:43,210 --> 00:19:47,259
cluster we would love to run this

00:19:45,759 --> 00:19:49,450
performance experiment on a regular

00:19:47,259 --> 00:19:51,099
basis however this is pretty hard to do

00:19:49,450 --> 00:19:52,899
because it's very expensive to run a

00:19:51,099 --> 00:19:54,969
thousand nodes and it's also really hard

00:19:52,899 --> 00:19:57,639
to manage over a period of time with

00:19:54,969 --> 00:19:59,529
just the devs on the team so what we did

00:19:57,639 --> 00:20:01,809
is we created an experiment that we like

00:19:59,529 --> 00:20:03,460
to call the benchmark PBS and the main

00:20:01,809 --> 00:20:05,440
idea behind this experiment

00:20:03,460 --> 00:20:07,210
is that we can simulate the total load

00:20:05,440 --> 00:20:09,100
on the API server of a thousand cell

00:20:07,210 --> 00:20:11,950
deployment without actually running a

00:20:09,100 --> 00:20:13,779
thousand notes so we are pretty

00:20:11,950 --> 00:20:16,899
confident in the fact that a single cell

00:20:13,779 --> 00:20:18,940
when saturated is easy to performance

00:20:16,899 --> 00:20:20,679
test but this is more testing the

00:20:18,940 --> 00:20:23,409
distributed nature of Diego over time

00:20:20,679 --> 00:20:25,149
and as we increase more more nodes how

00:20:23,409 --> 00:20:27,130
does the central database respond to

00:20:25,149 --> 00:20:28,990
this activity so one of the ways we

00:20:27,130 --> 00:20:31,480
created this test suite was by analyzing

00:20:28,990 --> 00:20:33,250
the activity that our various components

00:20:31,480 --> 00:20:35,200
make against the API and then

00:20:33,250 --> 00:20:37,929
replicating it from a smaller number of

00:20:35,200 --> 00:20:41,380
nodes basically within this this

00:20:37,929 --> 00:20:43,409
experiment is mainly set to analyze the

00:20:41,380 --> 00:20:46,390
performance of the read and write

00:20:43,409 --> 00:20:47,620
characteristics of the BBS specifically

00:20:46,390 --> 00:20:49,720
we want to know how long our bulk

00:20:47,620 --> 00:20:51,640
endpoints take and we want to know how

00:20:49,720 --> 00:20:55,179
often do we fail to write or read from

00:20:51,640 --> 00:20:57,100
the database the results from this test

00:20:55,179 --> 00:20:59,049
we're kind of unexpected we found that

00:20:57,100 --> 00:21:01,750
HCD had become our biggest bottleneck in

00:20:59,049 --> 00:21:05,200
the system this is largely due the fact

00:21:01,750 --> 00:21:07,210
that at cvv2 doesn't support large data

00:21:05,200 --> 00:21:09,820
sets specifically in this example we are

00:21:07,210 --> 00:21:11,169
almost at about a gigabyte and it also

00:21:09,820 --> 00:21:15,070
doesn't natively support traditional

00:21:11,169 --> 00:21:16,630
niceties such as indexes as we added

00:21:15,070 --> 00:21:18,940
more and more data we also saw the sed

00:21:16,630 --> 00:21:20,590
continue to degrade cells were taking

00:21:18,940 --> 00:21:22,779
longer than 30 seconds to fetch their

00:21:20,590 --> 00:21:24,100
required records this is essentially

00:21:22,779 --> 00:21:26,590
because you have to do a table scan on

00:21:24,100 --> 00:21:28,570
each fetch we also notice that we were

00:21:26,590 --> 00:21:30,490
failing to see the data for our tests

00:21:28,570 --> 00:21:31,750
and also bulk loops were beginning to

00:21:30,490 --> 00:21:34,299
take longer than their expected

00:21:31,750 --> 00:21:36,669
operating intervals we know that there

00:21:34,299 --> 00:21:37,870
are talks of sed v3 which will improve

00:21:36,669 --> 00:21:41,230
on a lot of these issues by adding

00:21:37,870 --> 00:21:42,970
indices but however v3 is still in beta

00:21:41,230 --> 00:21:44,890
and we need to hit these performance

00:21:42,970 --> 00:21:46,029
targets now so we know that we're gonna

00:21:44,890 --> 00:21:51,909
have to make some changes to our

00:21:46,029 --> 00:21:54,820
database back packing technology so how

00:21:51,909 --> 00:21:58,600
are you gonna get you this scale with

00:21:54,820 --> 00:22:00,279
with these blockers so one thing we're

00:21:58,600 --> 00:22:02,380
doing right now is we we actually

00:22:00,279 --> 00:22:05,610
currently have experimental support for

00:22:02,380 --> 00:22:09,850
a relational back-end on Diego you can

00:22:05,610 --> 00:22:11,620
you can configure a pluggable relational

00:22:09,850 --> 00:22:14,889
database that has to be deployed

00:22:11,620 --> 00:22:17,740
alongside Diego and then Diego will just

00:22:14,889 --> 00:22:21,520
use that and reasons for the switch go

00:22:17,740 --> 00:22:25,150
beyond just Caelian performance our our

00:22:21,520 --> 00:22:26,920
data model is inherently relational like

00:22:25,150 --> 00:22:28,990
for example applications have any

00:22:26,920 --> 00:22:32,380
applications have many instances and

00:22:28,990 --> 00:22:33,940
cells have many containers it's a common

00:22:32,380 --> 00:22:40,059
thing you say when you have a relational

00:22:33,940 --> 00:22:42,340
model we also we also have been wanting

00:22:40,059 --> 00:22:44,950
rich support for indices in our data

00:22:42,340 --> 00:22:48,850
layer for a while having to do a full

00:22:44,950 --> 00:22:50,440
scan like like Jim mention is not really

00:22:48,850 --> 00:22:54,070
the most efficient way to get that data

00:22:50,440 --> 00:22:56,200
and one way we could get around that is

00:22:54,070 --> 00:22:58,150
by completely the normalizing or schema

00:22:56,200 --> 00:23:02,710
but that that's not really sustainable

00:22:58,150 --> 00:23:05,350
in engineering wise one of the concerns

00:23:02,710 --> 00:23:07,540
we had when we started this this effort

00:23:05,350 --> 00:23:10,809
you switch to your relational database

00:23:07,540 --> 00:23:12,730
was high availability because sed is

00:23:10,809 --> 00:23:15,880
highly available by definition it's it's

00:23:12,730 --> 00:23:18,820
a cluster and and you get that high

00:23:15,880 --> 00:23:21,370
availability kind of for free while

00:23:18,820 --> 00:23:27,400
these traditional relational databases

00:23:21,370 --> 00:23:30,100
are generally singer node by default but

00:23:27,400 --> 00:23:32,110
we're confident that efforts like Galera

00:23:30,100 --> 00:23:36,309
for my sequel will bridge that gap

00:23:32,110 --> 00:23:38,260
efficiently and even even in the face of

00:23:36,309 --> 00:23:40,809
failure we saw that Diego is pretty good

00:23:38,260 --> 00:23:43,059
at recovering from that and being robust

00:23:40,809 --> 00:23:46,780
with minimal to no impacts you do should

00:23:43,059 --> 00:23:51,520
the user finally we made the switch

00:23:46,780 --> 00:23:54,010
after exploring this option using that

00:23:51,520 --> 00:23:58,510
benchmark PBS suite that that Jim

00:23:54,010 --> 00:24:02,679
mentioned with a minimal implementation

00:23:58,510 --> 00:24:04,420
of the relational back-end and we got

00:24:02,679 --> 00:24:06,940
some really encouraging results from

00:24:04,420 --> 00:24:09,070
that we were able to store upwards of

00:24:06,940 --> 00:24:11,140
600 thousand instances in the database

00:24:09,070 --> 00:24:14,380
even before we started paginating or

00:24:11,140 --> 00:24:16,690
fragmenting data so that it's a lot more

00:24:14,380 --> 00:24:21,250
than we could possibly have with with

00:24:16,690 --> 00:24:25,169
the old model so we decided to allow

00:24:21,250 --> 00:24:27,220
both my Seco and Postgres being the Chu

00:24:25,169 --> 00:24:29,160
Seco implementations we currently

00:24:27,220 --> 00:24:31,270
support and Cloud Controller in UA

00:24:29,160 --> 00:24:32,530
because that would help

00:24:31,270 --> 00:24:35,200
support the broader Cloud Foundry

00:24:32,530 --> 00:24:37,540
community and they're also widely used

00:24:35,200 --> 00:24:40,740
in the world and proven to be effective

00:24:37,540 --> 00:24:43,300
when used correctly at large scales

00:24:40,740 --> 00:24:45,640
before you ask me when is it going to be

00:24:43,300 --> 00:24:47,350
out of experimental mode we can follow

00:24:45,640 --> 00:24:49,720
the progress in tracker we're working

00:24:47,350 --> 00:24:53,470
out some last implementation and

00:24:49,720 --> 00:24:56,260
migration details and are going to start

00:24:53,470 --> 00:25:00,720
a full-scale test of this whole effort

00:24:56,260 --> 00:25:04,780
soon and hopefully it would be out

00:25:00,720 --> 00:25:06,700
another option we explored that you

00:25:04,780 --> 00:25:08,860
should get to the scale we wanted it was

00:25:06,700 --> 00:25:12,690
adding in memory caching to the API

00:25:08,860 --> 00:25:15,880
server and we had a pair of engineers

00:25:12,690 --> 00:25:17,640
explore that for a few days and our

00:25:15,880 --> 00:25:19,960
conclusions were that wow it would

00:25:17,640 --> 00:25:21,760
definitely get us closer to the numbers

00:25:19,960 --> 00:25:24,670
we wanted and even hit the numbers we

00:25:21,760 --> 00:25:29,110
wanted it wouldn't fix the underlying

00:25:24,670 --> 00:25:31,630
problem that the data model was not like

00:25:29,110 --> 00:25:34,150
we were modeling the data forcefully to

00:25:31,630 --> 00:25:38,170
match our data store rather than having

00:25:34,150 --> 00:25:39,309
it flow naturally so that's why we

00:25:38,170 --> 00:25:40,660
decided not to go with it but it's

00:25:39,309 --> 00:25:41,920
definitely something we still have in

00:25:40,660 --> 00:25:43,470
our pocket and in case we need to

00:25:41,920 --> 00:25:46,540
optimize something else in the future

00:25:43,470 --> 00:25:52,630
and the other thing we can also explore

00:25:46,540 --> 00:25:54,850
in the future is adding a read a read

00:25:52,630 --> 00:25:56,410
replica of the API server because

00:25:54,850 --> 00:26:00,480
currently because currently the API

00:25:56,410 --> 00:26:03,130
server is a leader elected single node

00:26:00,480 --> 00:26:04,990
for reads and writes and it makes sense

00:26:03,130 --> 00:26:07,210
for us to have rights be going through

00:26:04,990 --> 00:26:08,850
one server because we want it to be

00:26:07,210 --> 00:26:12,340
consistent but we could definitely

00:26:08,850 --> 00:26:14,140
expand the read aspect of it to just

00:26:12,340 --> 00:26:16,809
scale horizontally to that's another

00:26:14,140 --> 00:26:22,360
thing we have in our in our pockets for

00:26:16,809 --> 00:26:24,600
the future that's all we had here are

00:26:22,360 --> 00:26:30,600
some links and resources from the stock

00:26:24,600 --> 00:26:30,600
and questions

00:26:41,500 --> 00:26:49,130
yes yes so we actually have we have a

00:26:47,630 --> 00:26:50,900
Bosch release that's like the Diego

00:26:49,130 --> 00:26:53,120
prefer release out right now and it has

00:26:50,900 --> 00:26:55,220
all of the tests and what we ran and

00:26:53,120 --> 00:26:56,960
it's pretty it's not easy to replicate

00:26:55,220 --> 00:27:00,200
but if you've talked to us we could help

00:26:56,960 --> 00:27:02,390
you spin it up and figure it out as we

00:27:00,200 --> 00:27:05,120
start running the full-scale test for

00:27:02,390 --> 00:27:07,970
the now relational back-end we intend to

00:27:05,120 --> 00:27:10,130
recreate that and possibly a more

00:27:07,970 --> 00:27:14,120
reputable way so people can consume it a

00:27:10,130 --> 00:27:27,650
little easier we also have plans to

00:27:14,120 --> 00:27:31,970
publish these metrics publicly - yeah

00:27:27,650 --> 00:27:33,170
these tests are on Amazon ec2 yes we

00:27:31,970 --> 00:27:34,790
currently have plans to maybe

00:27:33,170 --> 00:27:37,450
investigate doing a larger scale one on

00:27:34,790 --> 00:27:37,450
SoftLayer as well

00:28:16,360 --> 00:28:23,630
yes so the question is about application

00:28:19,610 --> 00:28:28,370
caching when you when you push so Diego

00:28:23,630 --> 00:28:30,799
is not part of that system so all that

00:28:28,370 --> 00:28:33,559
happens outside of the Diego boundaries

00:28:30,799 --> 00:28:35,690
I don't know if any plans to integrate

00:28:33,559 --> 00:28:36,919
that I don't think we're going to do

00:28:35,690 --> 00:28:40,130
that right now

00:28:36,919 --> 00:28:49,610
but as far as I know that's completely

00:28:40,130 --> 00:28:51,320
outside of Diego no we just wrote all

00:28:49,610 --> 00:28:53,720
the queries by hand because we felt an

00:28:51,320 --> 00:28:56,480
ORM was too heavy weight for the queries

00:28:53,720 --> 00:28:59,919
that we were making we are leaving that

00:28:56,480 --> 00:28:59,919
option for the future

00:29:09,220 --> 00:29:16,910
so the question is about if we explored

00:29:12,830 --> 00:29:18,410
big tables implementations no we decided

00:29:16,910 --> 00:29:22,060
to go with the traditional solution

00:29:18,410 --> 00:29:24,530
because we were already coming from none

00:29:22,060 --> 00:29:28,130
standard solution for the problem and

00:29:24,530 --> 00:29:48,200
our our data model is actually fairly

00:29:28,130 --> 00:29:50,930
simple so so now that we have this

00:29:48,200 --> 00:29:52,490
single right BBS server we can actually

00:29:50,930 --> 00:29:55,850
generate these events manually by hand

00:29:52,490 --> 00:29:56,990
in like a higher layer and it's actually

00:29:55,850 --> 00:29:58,370
a design goal that we're trying to move

00:29:56,990 --> 00:30:00,350
forward with is moving away from these

00:29:58,370 --> 00:30:02,150
Etsy D watches because it we tied

00:30:00,350 --> 00:30:04,160
ourselves very closely to SED with these

00:30:02,150 --> 00:30:05,900
watches and now that we've kind of

00:30:04,160 --> 00:30:07,520
started to break away from that we can

00:30:05,900 --> 00:30:10,210
now have more freedom in the backing

00:30:07,520 --> 00:30:10,210
layer essentially

00:30:24,110 --> 00:30:28,970
that's not a problem we've solved quite

00:30:25,910 --> 00:30:32,270
yet but currently we have a master

00:30:28,970 --> 00:30:34,130
elected database right server I think we

00:30:32,270 --> 00:30:36,260
were going to keep it that way for the

00:30:34,130 --> 00:30:38,120
time being we may investigate having

00:30:36,260 --> 00:30:40,460
read replicas that will read from the

00:30:38,120 --> 00:30:42,680
database but I don't think we're gonna

00:30:40,460 --> 00:30:47,900
solve the multi rate node problem at

00:30:42,680 --> 00:30:51,230
this point thanks guys this is really

00:30:47,900 --> 00:30:53,270
cool stuff question did you ever play

00:30:51,230 --> 00:30:55,760
with an in-memory database like gem fire

00:30:53,270 --> 00:31:02,120
as the backing store for the BBS since

00:30:55,760 --> 00:31:05,260
durability really isn't a concern we so

00:31:02,120 --> 00:31:08,690
we haven't actually explored that but we

00:31:05,260 --> 00:31:10,720
considered it that's that's the short

00:31:08,690 --> 00:31:10,720
answer

00:31:14,800 --> 00:31:19,310
cool we'll be around if you guys won't

00:31:17,420 --> 00:31:19,830
ask those questions thank you for

00:31:19,310 --> 00:31:24,760
listening

00:31:19,830 --> 00:31:24,760

YouTube URL: https://www.youtube.com/watch?v=VRLgOUGOo-c


