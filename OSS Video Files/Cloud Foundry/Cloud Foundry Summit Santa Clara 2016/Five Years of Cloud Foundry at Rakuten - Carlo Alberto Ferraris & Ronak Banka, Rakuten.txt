Title: Five Years of Cloud Foundry at Rakuten - Carlo Alberto Ferraris & Ronak Banka, Rakuten
Publication date: 2016-05-29
Playlist: Cloud Foundry Summit Santa Clara 2016
Description: 
	Five Years of Cloud Foundry at Rakuten - Carlo Alberto Ferraris & Ronak Banka, Rakuten, Inc.

A team of seven engineers at Rakuten has been running Cloud Foundry in production for five years on over 5,000 virtual servers. In this talk we'll look back at the experience acquired in the past five years and the goals for the next five. We'll share some of the solutions we're building to empower our users - like how we implemented placement pools to run applications in different networks/environments, how we deploy a single Cloud Foundry environment on multiple virtual infrastructures and how we collect and process logs and metrics - as well as how we're applying what we learned from running one of the biggest Cloud Foundry deployments worldwide to make operations and maintenance efforts scale efficiently. 

Ronak Banka
DevOps Engineer, Rakuten, Inc.
DevOps engineer on the Rakuten PaaS team. Speaker at Cloud Foundry meetup Delhi, India (June 2015), Openstack Summit 2015 Tokyo, Japan.

Carlo Alberto Ferraris
Senior DevOps Engineer, Rakuten, Inc.
Senior DevOps engineer, team leader of the Rakuten Platform-as-a-Service team. Speaker at Cloud Foundry Tokyo Meetup.
Captions: 
	00:00:00,030 --> 00:00:09,450
okay hello everybody thank you for

00:00:03,090 --> 00:00:14,610
coming today we are Roenick Carlo from

00:00:09,450 --> 00:00:20,760
Rakatan Braga is this big Japanese

00:00:14,610 --> 00:00:22,710
company they started out in 1999 it's it

00:00:20,760 --> 00:00:26,810
has been expanding globally over the

00:00:22,710 --> 00:00:29,400
last 15 years it has a lot of services

00:00:26,810 --> 00:00:31,679
but it mostly focuses on e-commerce and

00:00:29,400 --> 00:00:34,380
it has acquired the many companies

00:00:31,679 --> 00:00:35,700
around the world during these years you

00:00:34,380 --> 00:00:39,719
might know Ebates in viber

00:00:35,700 --> 00:00:41,940
probably we are here to talk today about

00:00:39,719 --> 00:00:44,309
our experience with foundry over the

00:00:41,940 --> 00:00:46,710
last five years so we will start with

00:00:44,309 --> 00:00:49,230
that and then we will focus on what we

00:00:46,710 --> 00:00:52,710
are doing right now to update our

00:00:49,230 --> 00:00:55,530
deployment and and what we plan to do in

00:00:52,710 --> 00:00:58,109
the future in the next few years at

00:00:55,530 --> 00:01:00,780
least so let's start with actually

00:00:58,109 --> 00:01:03,439
describing what we do with our cloud

00:01:00,780 --> 00:01:07,340
foundry deployment it's an internal

00:01:03,439 --> 00:01:10,290
platform as a service for our developers

00:01:07,340 --> 00:01:12,540
we actually presented this platform

00:01:10,290 --> 00:01:17,490
already a few years back here this year

00:01:12,540 --> 00:01:20,070
summit in 2014 because at the time we

00:01:17,490 --> 00:01:22,110
had just formed the platform to

00:01:20,070 --> 00:01:26,460
implement some of the features that were

00:01:22,110 --> 00:01:28,890
required in Rakatan some of them

00:01:26,460 --> 00:01:31,409
actually in the years were implemented

00:01:28,890 --> 00:01:34,020
in v2 as well what would become v2

00:01:31,409 --> 00:01:38,189
eventually some of them were never

00:01:34,020 --> 00:01:41,159
accepted and the to support these use

00:01:38,189 --> 00:01:47,810
cases we actually had to keep diverging

00:01:41,159 --> 00:01:50,399
from the v2 development branch so we

00:01:47,810 --> 00:01:57,110
this allows that allowed us to actually

00:01:50,399 --> 00:01:59,280
reach quite a sizable deployment we at

00:01:57,110 --> 00:02:02,009
certain time we were probably like the

00:01:59,280 --> 00:02:05,490
second biggest v1 deployment in the

00:02:02,009 --> 00:02:07,829
world all of these it has been running

00:02:05,490 --> 00:02:10,050
for five years with a team of seven

00:02:07,829 --> 00:02:11,060
people doing pretty much everything

00:02:10,050 --> 00:02:14,090
handling

00:02:11,060 --> 00:02:18,050
from user support to operations to

00:02:14,090 --> 00:02:21,470
development architecture everything in

00:02:18,050 --> 00:02:24,290
these years we learn a quite sizable

00:02:21,470 --> 00:02:26,030
amount of stuff about what is good to do

00:02:24,290 --> 00:02:30,200
with this platform what is not good to

00:02:26,030 --> 00:02:33,800
do the first thing is don't try to make

00:02:30,200 --> 00:02:36,290
everything fit in sometimes application

00:02:33,800 --> 00:02:39,860
will have unique requirements sometimes

00:02:36,290 --> 00:02:43,340
for good reason sometimes not don't

00:02:39,860 --> 00:02:45,760
yield to the or my snowflake is so

00:02:43,340 --> 00:02:49,100
unique you need to support it narrative

00:02:45,760 --> 00:02:50,600
in most cases it's the application that

00:02:49,100 --> 00:02:53,209
needs to be adapted to the platform not

00:02:50,600 --> 00:02:55,549
the other way around getting a good

00:02:53,209 --> 00:02:57,830
corporate champion to back you up is

00:02:55,549 --> 00:02:59,480
actually very very important to make

00:02:57,830 --> 00:03:02,660
sure that these ghosts over at the right

00:02:59,480 --> 00:03:06,890
way otherwise what might have might

00:03:02,660 --> 00:03:08,420
happen is that you end up working and we

00:03:06,890 --> 00:03:10,400
learned the hard way that it's not a

00:03:08,420 --> 00:03:13,190
very good idea because there is too much

00:03:10,400 --> 00:03:15,650
value in what the community is doing too

00:03:13,190 --> 00:03:20,030
much momentum don't ever think about

00:03:15,650 --> 00:03:22,130
doing that try to build everything that

00:03:20,030 --> 00:03:24,140
you need to build either on top or on

00:03:22,130 --> 00:03:27,319
the side if you're building on top try

00:03:24,140 --> 00:03:30,709
to keep things as neat and lean as

00:03:27,319 --> 00:03:32,930
possible try to stick to public API so

00:03:30,709 --> 00:03:38,180
that things will not break even long

00:03:32,930 --> 00:03:41,500
term another thing obvious somewhat is

00:03:38,180 --> 00:03:43,489
that engineering time does not scale

00:03:41,500 --> 00:03:45,650
every single thing that you leave behind

00:03:43,489 --> 00:03:48,769
every single manual step that you have

00:03:45,650 --> 00:03:51,530
to do in the long term is going to come

00:03:48,769 --> 00:03:55,519
and actually bite you

00:03:51,530 --> 00:03:59,620
so don't yield to the temptation of

00:03:55,519 --> 00:04:02,120
turning your cattle into pets because

00:03:59,620 --> 00:04:04,519
this really really really doesn't scale

00:04:02,120 --> 00:04:09,680
and you end up with snowflakes

00:04:04,519 --> 00:04:12,590
everywhere that are not good this

00:04:09,680 --> 00:04:15,950
doesn't just apply to provisioning this

00:04:12,590 --> 00:04:18,060
applies even more importantly to the way

00:04:15,950 --> 00:04:21,630
you collect

00:04:18,060 --> 00:04:23,910
aggregate store logs and metrics knowing

00:04:21,630 --> 00:04:26,580
what your platform is doing is vital you

00:04:23,910 --> 00:04:28,440
need to have this information in us in

00:04:26,580 --> 00:04:30,990
the in a very well-defined

00:04:28,440 --> 00:04:32,910
place where you can immediately access

00:04:30,990 --> 00:04:35,040
it and you can go through it and

00:04:32,910 --> 00:04:38,070
correlate events coming from different

00:04:35,040 --> 00:04:39,930
components if you don't do that you are

00:04:38,070 --> 00:04:41,520
going to spend so much time going around

00:04:39,930 --> 00:04:43,950
and trying to find the information

00:04:41,520 --> 00:04:47,070
you're looking for

00:04:43,950 --> 00:04:50,460
keep in mind what when you design your

00:04:47,070 --> 00:04:53,280
local election system what might work

00:04:50,460 --> 00:04:57,690
for 100 VMs will probably not work for

00:04:53,280 --> 00:04:59,610
5,000 so make sure that you build it in

00:04:57,690 --> 00:05:03,240
such a way that you can swap in and swap

00:04:59,610 --> 00:05:05,610
out components quickly and eventually

00:05:03,240 --> 00:05:08,310
also run them in parallel because that's

00:05:05,610 --> 00:05:10,770
allowed that allows you actually to try

00:05:08,310 --> 00:05:16,770
out stuff without breaking your current

00:05:10,770 --> 00:05:18,419
capabilities also one funny bit don't

00:05:16,770 --> 00:05:22,110
share your monitoring system with your

00:05:18,419 --> 00:05:23,940
users because otherwise you you kind of

00:05:22,110 --> 00:05:26,580
risk losing visibility when you probably

00:05:23,940 --> 00:05:32,940
need it the most this actually happened

00:05:26,580 --> 00:05:35,580
to us once so don't try that another

00:05:32,940 --> 00:05:38,190
important thing to notice is that when

00:05:35,580 --> 00:05:41,539
you reach these sizes when you reach

00:05:38,190 --> 00:05:44,490
this scale things fail all the time

00:05:41,539 --> 00:05:46,229
assuming in your design or in your

00:05:44,490 --> 00:05:50,340
components that things are going to work

00:05:46,229 --> 00:05:52,979
is a mistake it's a mistake at any level

00:05:50,340 --> 00:05:55,860
because it's going to come and and bite

00:05:52,979 --> 00:06:00,120
you when again at the worst possible

00:05:55,860 --> 00:06:04,860
time normal as an example we had this

00:06:00,120 --> 00:06:06,810
log pipeline that was built with the

00:06:04,860 --> 00:06:11,400
assumption that logs had not to be lost

00:06:06,810 --> 00:06:13,169
under any circumstances but then what

00:06:11,400 --> 00:06:14,669
happens if some of the components at the

00:06:13,169 --> 00:06:17,010
end of the pipeline actually start

00:06:14,669 --> 00:06:18,120
misbehaving or slowing down then

00:06:17,010 --> 00:06:20,610
actually these ripples through

00:06:18,120 --> 00:06:23,340
everywhere and eventually you're going

00:06:20,610 --> 00:06:28,589
to have problems even on your

00:06:23,340 --> 00:06:31,889
application so we have

00:06:28,589 --> 00:06:34,859
had this deployment running for five

00:06:31,889 --> 00:06:36,629
years it has served us very well now it

00:06:34,859 --> 00:06:38,669
sounded kind of we have many progressive

00:06:36,629 --> 00:06:40,589
there have been problems but there have

00:06:38,669 --> 00:06:44,489
been many success stories and actually

00:06:40,589 --> 00:06:46,409
it provided tremendous value but we

00:06:44,489 --> 00:06:48,599
realized last year that we had to catch

00:06:46,409 --> 00:06:51,899
up with upstream it was not a question

00:06:48,599 --> 00:06:53,999
of whether we should do it or just that

00:06:51,899 --> 00:06:58,379
we had to do it sooner or later

00:06:53,999 --> 00:07:00,989
and now Ronnie is going to spend a

00:06:58,379 --> 00:07:05,699
little bit of how we actually do this

00:07:00,989 --> 00:07:08,489
immature so once we started our journey

00:07:05,699 --> 00:07:11,759
for the second version of the deployment

00:07:08,489 --> 00:07:14,099
we had to decide like the first thing

00:07:11,759 --> 00:07:15,989
for the infrastructure part a

00:07:14,099 --> 00:07:18,659
provisioning part I will be delivering

00:07:15,989 --> 00:07:22,409
the internal software tools and other

00:07:18,659 --> 00:07:25,169
things obviously with the upstream we

00:07:22,409 --> 00:07:27,299
chose Bosch for the provisioning because

00:07:25,169 --> 00:07:29,939
we want to stick with the upstream for

00:07:27,299 --> 00:07:33,419
that we use Concours for delivering our

00:07:29,939 --> 00:07:37,739
internal tools on the infrastructure

00:07:33,419 --> 00:07:39,539
level like as most of the Cloud Foundry

00:07:37,739 --> 00:07:41,579
deployments they're using a single

00:07:39,539 --> 00:07:44,879
deployment for deploying Cloud Foundry

00:07:41,579 --> 00:07:48,449
we are actually we have divided our

00:07:44,879 --> 00:07:51,539
deployment like partially on vSphere and

00:07:48,449 --> 00:07:55,949
on the OpenStack at the same time the

00:07:51,539 --> 00:07:58,919
reason for this is back then we had some

00:07:55,949 --> 00:08:02,669
limitations on our OpenStack deployment

00:07:58,919 --> 00:08:07,319
where our cinder was not giving a proper

00:08:02,669 --> 00:08:09,239
support with the our bosh kpi's until

00:08:07,319 --> 00:08:10,919
few days back when we have this

00:08:09,239 --> 00:08:14,009
functionality working properly on

00:08:10,919 --> 00:08:16,139
OpenStack so we are probably thinking of

00:08:14,009 --> 00:08:20,549
moving all the components on the

00:08:16,139 --> 00:08:22,979
OpenStack so on when I say living on the

00:08:20,549 --> 00:08:25,129
multiple clouds it's actually deploying

00:08:22,979 --> 00:08:29,609
a single CF on two different

00:08:25,129 --> 00:08:32,879
infrastructure we have a bot of the boss

00:08:29,609 --> 00:08:34,649
director which has the VMware CPI which

00:08:32,879 --> 00:08:36,539
is responsible for provisioning the

00:08:34,649 --> 00:08:40,679
persistent components of the Cloud

00:08:36,539 --> 00:08:41,700
Foundry like NFS post creates a CDN our

00:08:40,679 --> 00:08:44,370
other

00:08:41,700 --> 00:08:47,399
internal tools whereas it is also

00:08:44,370 --> 00:08:50,370
responsible for deploying the OpenStack

00:08:47,399 --> 00:08:53,070
that Bosch director which then is

00:08:50,370 --> 00:08:54,600
responsible for provisioning the

00:08:53,070 --> 00:08:59,490
stateless components of the Cloud

00:08:54,600 --> 00:09:02,070
Foundry on top of OpenStack so it's it's

00:08:59,490 --> 00:09:05,070
easy to provision the Cloud Foundry with

00:09:02,070 --> 00:09:08,040
a single manifest but in our case we

00:09:05,070 --> 00:09:10,529
have to make some changes to actually

00:09:08,040 --> 00:09:14,670
create two intermediate manifests for

00:09:10,529 --> 00:09:17,880
each of the infrastructure so we target

00:09:14,670 --> 00:09:20,279
them like individually while sharing the

00:09:17,880 --> 00:09:22,470
properties between them and deploying

00:09:20,279 --> 00:09:28,140
the persistent components followed by

00:09:22,470 --> 00:09:30,690
deploying the stateless components okay

00:09:28,140 --> 00:09:34,200
so with the pro with the previous

00:09:30,690 --> 00:09:36,120
version of our deployment we had three

00:09:34,200 --> 00:09:38,670
different deployments like most of the

00:09:36,120 --> 00:09:41,850
people do they have def they have stage

00:09:38,670 --> 00:09:44,519
they have prod but for a team of seven

00:09:41,850 --> 00:09:47,850
people it was too much the overhead was

00:09:44,519 --> 00:09:51,300
high to actually manage three different

00:09:47,850 --> 00:09:54,420
deployments so we came up with this

00:09:51,300 --> 00:09:57,300
thing and we actually are actually

00:09:54,420 --> 00:10:00,360
giving our internal users three

00:09:57,300 --> 00:10:03,300
different logical environments within a

00:10:00,360 --> 00:10:06,540
same within a single Cloud Foundry

00:10:03,300 --> 00:10:10,050
deployment how we achieve this we are

00:10:06,540 --> 00:10:12,870
actually using three different DEA

00:10:10,050 --> 00:10:16,730
groups and three different router groups

00:10:12,870 --> 00:10:21,000
which are responsible for catering the

00:10:16,730 --> 00:10:25,380
traffic for each of the applications the

00:10:21,000 --> 00:10:26,940
elastic pools are still on road maps

00:10:25,380 --> 00:10:30,899
there they will be coming up very soon

00:10:26,940 --> 00:10:33,570
but then we had to come up with

00:10:30,899 --> 00:10:37,410
something so what we did was we call it

00:10:33,570 --> 00:10:40,279
a stack hack there's a property for the

00:10:37,410 --> 00:10:43,620
route FS itself where on the DEA you can

00:10:40,279 --> 00:10:45,329
change the name of the stack itself and

00:10:43,620 --> 00:10:48,600
when you have to push application you

00:10:45,329 --> 00:10:50,990
can mention this stack to put your

00:10:48,600 --> 00:10:54,329
application on a particular DEA

00:10:50,990 --> 00:10:55,340
supporting that stack so we created

00:10:54,329 --> 00:10:59,210
these

00:10:55,340 --> 00:11:02,540
da pulls on three different networks

00:10:59,210 --> 00:11:06,050
having the stack as development staging

00:11:02,540 --> 00:11:09,710
and production so when a user has to put

00:11:06,050 --> 00:11:12,530
a application on dev he has to put just

00:11:09,710 --> 00:11:16,100
extra property in his application

00:11:12,530 --> 00:11:18,710
manifest the stack as dev stage or prod

00:11:16,100 --> 00:11:22,580
and accordingly the application will end

00:11:18,710 --> 00:11:26,810
up on that particular da this but this

00:11:22,580 --> 00:11:29,480
provides the provisioning da's on three

00:11:26,810 --> 00:11:32,750
different networks provides isolation

00:11:29,480 --> 00:11:39,350
and both like on the security and the

00:11:32,750 --> 00:11:41,840
network isolation after this thing this

00:11:39,350 --> 00:11:44,780
is like just overview how we actually

00:11:41,840 --> 00:11:48,230
deploy our whole platform what all

00:11:44,780 --> 00:11:50,030
components are in there so with the

00:11:48,230 --> 00:11:53,480
previous version like Carlo mentioned we

00:11:50,030 --> 00:11:55,820
fog the upstream version which was a bad

00:11:53,480 --> 00:11:59,300
idea and we won't be doing that again

00:11:55,820 --> 00:12:01,850
so we sync up with the upstream for the

00:11:59,300 --> 00:12:04,310
upstream boss releases whereas for the

00:12:01,850 --> 00:12:07,580
internal boss releases like for some of

00:12:04,310 --> 00:12:10,190
our logging and matrix pipeline we have

00:12:07,580 --> 00:12:12,530
our internal Bosch releases and some of

00:12:10,190 --> 00:12:15,200
the user phases user-facing Cloud

00:12:12,530 --> 00:12:20,840
Foundry plugins we use Concours for

00:12:15,200 --> 00:12:24,620
shipping these internal releases when we

00:12:20,840 --> 00:12:26,330
start deploying on not on the production

00:12:24,620 --> 00:12:31,100
but on the pre-production environment

00:12:26,330 --> 00:12:34,030
it's always good to like analyze the

00:12:31,100 --> 00:12:37,670
behavior of all your components so we

00:12:34,030 --> 00:12:40,780
like collect all of the metrics as much

00:12:37,670 --> 00:12:43,760
as possible and during the deployment we

00:12:40,780 --> 00:12:45,740
like check the behavior the pattern of

00:12:43,760 --> 00:12:47,950
the graphs for pretty much all the

00:12:45,740 --> 00:12:52,520
components to check their behavior

00:12:47,950 --> 00:12:54,440
followed by server spec which is run for

00:12:52,520 --> 00:12:57,140
run using the Boche errands for

00:12:54,440 --> 00:13:00,380
individual components individual

00:12:57,140 --> 00:13:03,230
components can work like very fine as

00:13:00,380 --> 00:13:05,000
they are expected but few times the

00:13:03,230 --> 00:13:06,700
functional integration between two

00:13:05,000 --> 00:13:08,650
different components can break

00:13:06,700 --> 00:13:11,650
even when both the components are

00:13:08,650 --> 00:13:15,220
working fine

00:13:11,650 --> 00:13:18,400
so there here we brings the infra tester

00:13:15,220 --> 00:13:21,250
for checking our subsystems which is

00:13:18,400 --> 00:13:24,610
again the bosses and jobs for checking

00:13:21,250 --> 00:13:27,670
the communication between like api's or

00:13:24,610 --> 00:13:30,490
internal tools followed by the

00:13:27,670 --> 00:13:33,030
acceptance and smoke test on our pre

00:13:30,490 --> 00:13:36,250
prod environment to check the uptime for

00:13:33,030 --> 00:13:39,340
the user facing functionalities and how

00:13:36,250 --> 00:13:43,150
a platform will perform in case of

00:13:39,340 --> 00:13:45,340
failures and disaster over here color

00:13:43,150 --> 00:13:47,590
will brief you more about the other

00:13:45,340 --> 00:13:51,880
features of the next one shoe platform

00:13:47,590 --> 00:13:54,100
so we put a lot of care into designing a

00:13:51,880 --> 00:13:58,000
new log aggregation and collection

00:13:54,100 --> 00:14:00,850
pipelines that whose primary goal was to

00:13:58,000 --> 00:14:05,640
complete the couple the producers from

00:14:00,850 --> 00:14:09,370
the consumers so we have every single

00:14:05,640 --> 00:14:11,140
event log metric that is generated by

00:14:09,370 --> 00:14:15,400
any of the VMS that we deploy and we

00:14:11,140 --> 00:14:19,170
deploy all of them with Bosh we sent to

00:14:15,400 --> 00:14:21,970
Kafka using well for syslog we use the

00:14:19,170 --> 00:14:25,690
uncuffed a plugin for collective right

00:14:21,970 --> 00:14:27,730
Kafka etc for the application logs and

00:14:25,690 --> 00:14:30,550
application matrix we wrote a component

00:14:27,730 --> 00:14:34,000
that pulls from the fire hose and sends

00:14:30,550 --> 00:14:38,200
to a per application topic on Kafka and

00:14:34,000 --> 00:14:40,690
on this side on the right side of the

00:14:38,200 --> 00:14:43,510
slide we have all the consumers we

00:14:40,690 --> 00:14:48,700
archive logs on blob storage using

00:14:43,510 --> 00:14:51,040
seeker we have a LK stack and in fact CB

00:14:48,700 --> 00:14:53,440
graph on stack for logging and

00:14:51,040 --> 00:14:57,660
monitoring internally - for operation

00:14:53,440 --> 00:15:00,790
purposes just for our team we use women

00:14:57,660 --> 00:15:04,870
for alerting and the complex event

00:15:00,790 --> 00:15:08,170
processing we all of these components

00:15:04,870 --> 00:15:11,680
had to be able to scale this was one

00:15:08,170 --> 00:15:13,510
like requirement that we set and even

00:15:11,680 --> 00:15:15,790
some components like women that are not

00:15:13,510 --> 00:15:17,680
naturally able to scale because they are

00:15:15,790 --> 00:15:20,540
ok they're fully stateless in this case

00:15:17,680 --> 00:15:23,810
but you need to have some time

00:15:20,540 --> 00:15:26,120
stateful matric stateful events to

00:15:23,810 --> 00:15:29,870
monitor so it's important that the event

00:15:26,120 --> 00:15:31,490
for a single specific component that

00:15:29,870 --> 00:15:34,550
you're monitoring always end up in the

00:15:31,490 --> 00:15:37,250
right Riemann that is tasked with

00:15:34,550 --> 00:15:38,990
monitoring that specific component so

00:15:37,250 --> 00:15:43,120
what we came up with is actually a

00:15:38,990 --> 00:15:46,940
pretty clever solution to like redirect

00:15:43,120 --> 00:15:51,230
the matrix to the proper Riemann by

00:15:46,940 --> 00:15:52,550
using kafka message IDs and the part and

00:15:51,230 --> 00:15:57,080
the natural partitioning that is

00:15:52,550 --> 00:15:59,080
available in Kafka another thing that we

00:15:57,080 --> 00:16:03,220
set out to do is make sure that we

00:15:59,080 --> 00:16:07,310
really monitor everything that can move

00:16:03,220 --> 00:16:10,520
we collect matrix at all levels starting

00:16:07,310 --> 00:16:13,040
from the system all this stuff every

00:16:10,520 --> 00:16:14,690
component both CF components as well as

00:16:13,040 --> 00:16:16,160
any other of the other components if you

00:16:14,690 --> 00:16:19,310
have Java components just pull

00:16:16,160 --> 00:16:20,900
everything from J max if you have an g

00:16:19,310 --> 00:16:24,950
NX just pull everything from and the

00:16:20,900 --> 00:16:27,640
monitoring endpoint of G of nginx and so

00:16:24,950 --> 00:16:31,580
on and so forth we monitor all the

00:16:27,640 --> 00:16:34,520
systems we depend on as well to be able

00:16:31,580 --> 00:16:37,070
to quickly isolate where a problem can

00:16:34,520 --> 00:16:40,450
be originating from for example we had

00:16:37,070 --> 00:16:43,040
issues in the last years with DNS the

00:16:40,450 --> 00:16:47,750
certain point our DNS system started

00:16:43,040 --> 00:16:49,430
responding erratically and the only way

00:16:47,750 --> 00:16:52,400
we could have been we would have been

00:16:49,430 --> 00:16:55,190
able to catch that fast is if we were

00:16:52,400 --> 00:16:58,250
motoring that DNS system for wrong

00:16:55,190 --> 00:17:00,650
answers so we actually set out to make

00:16:58,250 --> 00:17:02,870
sure that we know this in advance so we

00:17:00,650 --> 00:17:06,230
can quickly pinpoint the source of a

00:17:02,870 --> 00:17:08,960
problem and then we also run we capture

00:17:06,230 --> 00:17:12,050
end to end matrix that capture the

00:17:08,960 --> 00:17:14,240
behavior of more than one system both

00:17:12,050 --> 00:17:15,920
from a passive point of view so for

00:17:14,240 --> 00:17:17,810
example just capture for example the

00:17:15,920 --> 00:17:19,700
latency of requests to a particular

00:17:17,810 --> 00:17:23,600
application that's easy but then also

00:17:19,700 --> 00:17:25,520
capture events that we trigger and that

00:17:23,600 --> 00:17:28,250
we know should take a certain amount of

00:17:25,520 --> 00:17:30,260
time and make sure that we keep that

00:17:28,250 --> 00:17:32,750
amount of time constant so for example

00:17:30,260 --> 00:17:34,220
we have this job that runs every five

00:17:32,750 --> 00:17:36,950
minutes that pushes up

00:17:34,220 --> 00:17:39,590
vacation to all the environment and we

00:17:36,950 --> 00:17:41,809
know for sure that that number should be

00:17:39,590 --> 00:17:43,640
constant within ten seconds

00:17:41,809 --> 00:17:47,150
mostly because the application is always

00:17:43,640 --> 00:17:51,950
the same so this number shouldn't change

00:17:47,150 --> 00:17:58,220
either like transiently or on the long

00:17:51,950 --> 00:18:01,429
term we build in the too old

00:17:58,220 --> 00:18:04,789
all of our rugged has specific features

00:18:01,429 --> 00:18:09,590
on top of foundry we basically have

00:18:04,789 --> 00:18:11,809
nothing on the side some of these will

00:18:09,590 --> 00:18:13,820
be made open-source soon for example to

00:18:11,809 --> 00:18:15,860
log access because we think there is

00:18:13,820 --> 00:18:17,600
actually value in that for the community

00:18:15,860 --> 00:18:19,190
some others are specific so specific use

00:18:17,600 --> 00:18:22,429
specific to rocket and that basically

00:18:19,190 --> 00:18:25,820
makes no sense but we can talk if if

00:18:22,429 --> 00:18:29,330
you're interested moving forward what we

00:18:25,820 --> 00:18:31,130
are planning to do next well after as

00:18:29,330 --> 00:18:34,370
soon as we finish the migration of our

00:18:31,130 --> 00:18:35,750
users from our current deployment to the

00:18:34,370 --> 00:18:38,510
new deployment we are going to target

00:18:35,750 --> 00:18:40,280
Asia for because we need to enable this

00:18:38,510 --> 00:18:45,320
actually one of our requirements burst

00:18:40,280 --> 00:18:46,730
of cloud scenarios we well this is the

00:18:45,320 --> 00:18:50,630
first scenario actually we also have

00:18:46,730 --> 00:18:52,370
others better reliability so that we

00:18:50,630 --> 00:18:54,400
have like another data center to fall

00:18:52,370 --> 00:18:58,340
back into and then eventually better

00:18:54,400 --> 00:18:59,750
latency and performance for our users we

00:18:58,340 --> 00:19:03,830
need to integrate all the service

00:18:59,750 --> 00:19:05,690
provider that asier first and OpenStack

00:19:03,830 --> 00:19:07,970
we have an internal office a team that

00:19:05,690 --> 00:19:10,159
is working on trove we need to integrate

00:19:07,970 --> 00:19:13,840
that into provide these services to our

00:19:10,159 --> 00:19:17,210
users we want to provide HTTP to

00:19:13,840 --> 00:19:19,159
termination in our initial phase and TLS

00:19:17,210 --> 00:19:22,309
should be to is actually fully supported

00:19:19,159 --> 00:19:25,100
everywhere so that people who get

00:19:22,309 --> 00:19:27,860
application gets immediately the

00:19:25,100 --> 00:19:30,590
benefits of HTTP 2 without what part of

00:19:27,860 --> 00:19:32,510
the benefits something is required

00:19:30,590 --> 00:19:33,710
support from the application but part of

00:19:32,510 --> 00:19:36,230
the benefits should be available

00:19:33,710 --> 00:19:40,960
immediately by just enabling that on the

00:19:36,230 --> 00:19:43,630
reverse proxy we are looking to

00:19:40,960 --> 00:19:47,170
certificate auto provisioning meaning

00:19:43,630 --> 00:19:48,920
users can actually push their

00:19:47,170 --> 00:19:50,840
certificate and have it installed

00:19:48,920 --> 00:19:52,130
everywhere on the load balancer for SSL

00:19:50,840 --> 00:19:54,520
termination to be done automatically

00:19:52,130 --> 00:19:57,470
eventually we want to do let's encrypt

00:19:54,520 --> 00:19:59,420
integration so that even if you don't

00:19:57,470 --> 00:20:01,820
have a proper certificate we actually

00:19:59,420 --> 00:20:05,110
create one real certificate for you and

00:20:01,820 --> 00:20:06,260
even testing it but it works seamlessly

00:20:05,110 --> 00:20:08,809
auto-scaling

00:20:06,260 --> 00:20:11,030
is not just the application of the

00:20:08,809 --> 00:20:13,280
scaling side is actually the VM auto

00:20:11,030 --> 00:20:16,520
scaling side that's what we are mostly

00:20:13,280 --> 00:20:19,429
interested in because it actually allows

00:20:16,520 --> 00:20:22,340
us to lower our workload and that's one

00:20:19,429 --> 00:20:23,780
of our long-term goals we want to be

00:20:22,340 --> 00:20:25,970
able to scale these up as much as

00:20:23,780 --> 00:20:28,610
possible without increasing the workload

00:20:25,970 --> 00:20:30,260
on the team because the team cannot

00:20:28,610 --> 00:20:31,880
scale that much it's not that easy to

00:20:30,260 --> 00:20:33,890
find people that can work on this thing

00:20:31,880 --> 00:20:35,990
and then eventually when elastic

00:20:33,890 --> 00:20:37,970
clusters aren't going to be available we

00:20:35,990 --> 00:20:40,340
are going to look into how to make this

00:20:37,970 --> 00:20:42,110
thing work across multiple data centers

00:20:40,340 --> 00:20:44,809
so that users can just push their

00:20:42,110 --> 00:20:47,150
application once have it deployed on

00:20:44,809 --> 00:20:50,090
multiple data centers and the

00:20:47,150 --> 00:20:51,800
integration with the global load

00:20:50,090 --> 00:20:54,230
balancer that we have in Rakatan and

00:20:51,800 --> 00:20:56,750
eventually also in Asia or whatever

00:20:54,230 --> 00:20:59,710
we're going to use have the traffic

00:20:56,750 --> 00:21:03,590
steer to the right data center just

00:20:59,710 --> 00:21:07,000
automatically that's a little bit of

00:21:03,590 --> 00:21:11,300
what we are planning to do there is

00:21:07,000 --> 00:21:14,360
something that we are experiencing in

00:21:11,300 --> 00:21:17,660
our contributions to with to the

00:21:14,360 --> 00:21:20,000
platform that would be very nice to have

00:21:17,660 --> 00:21:26,390
some feedback from like all of you guys

00:21:20,000 --> 00:21:29,630
if you want and from the foundation it

00:21:26,390 --> 00:21:32,330
is well mostly some of the things rely

00:21:29,630 --> 00:21:34,550
on missing documentation like there are

00:21:32,330 --> 00:21:36,980
some conventions that are not really

00:21:34,550 --> 00:21:38,480
well documented and and this causes

00:21:36,980 --> 00:21:43,070
problem like when you open up you are

00:21:38,480 --> 00:21:44,420
because they will complain many of the

00:21:43,070 --> 00:21:46,970
jobs are not really designed for

00:21:44,420 --> 00:21:51,230
colocation actually they explicitly warn

00:21:46,970 --> 00:21:53,980
you that they don't care so it's knowing

00:21:51,230 --> 00:21:56,960
at least how to make things on your side

00:21:53,980 --> 00:21:58,880
work nicely with the other components we

00:21:56,960 --> 00:22:03,559
be on the long term it would be a very

00:21:58,880 --> 00:22:05,600
good what we often find lacking on the

00:22:03,559 --> 00:22:06,950
Bosch side is the inability to know

00:22:05,600 --> 00:22:09,020
exactly which PM's you are going to

00:22:06,950 --> 00:22:13,760
redeploy before you actually try to

00:22:09,020 --> 00:22:15,529
deploy and also the ability to have like

00:22:13,760 --> 00:22:17,330
templates the preview of the templates

00:22:15,529 --> 00:22:20,360
rendered that would simplify the job of

00:22:17,330 --> 00:22:23,240
some of the the guys on the team then

00:22:20,360 --> 00:22:26,350
there is one big complaint that we get

00:22:23,240 --> 00:22:29,779
from our users and that's basically that

00:22:26,350 --> 00:22:35,210
logs are not just normally single lines

00:22:29,779 --> 00:22:37,760
and and knowing that you are losing logs

00:22:35,210 --> 00:22:40,490
it's okay it should should be the

00:22:37,760 --> 00:22:43,100
minimum it's okay to lose logs but you

00:22:40,490 --> 00:22:46,250
should at least know that you are losing

00:22:43,100 --> 00:22:48,740
logs and then we have internal use cases

00:22:46,250 --> 00:22:51,950
that would benefit greatly from having a

00:22:48,740 --> 00:22:54,380
way to actually hook into the API of the

00:22:51,950 --> 00:22:56,029
cloud controller and to perform complex

00:22:54,380 --> 00:22:59,919
validation and complex

00:22:56,029 --> 00:23:03,350
authorization of certain operations so

00:22:59,919 --> 00:23:06,370
that's it we kind of overrun by a little

00:23:03,350 --> 00:23:09,830
bit our time if you have any questions

00:23:06,370 --> 00:23:12,260
you can either ask us now or you can

00:23:09,830 --> 00:23:16,559
catch up later

00:23:12,260 --> 00:23:16,559

YouTube URL: https://www.youtube.com/watch?v=CwJJyQQUsV4


