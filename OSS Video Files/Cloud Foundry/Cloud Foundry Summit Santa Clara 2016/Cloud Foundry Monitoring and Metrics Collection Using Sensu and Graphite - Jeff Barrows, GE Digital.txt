Title: Cloud Foundry Monitoring and Metrics Collection Using Sensu and Graphite - Jeff Barrows, GE Digital
Publication date: 2016-05-29
Playlist: Cloud Foundry Summit Santa Clara 2016
Description: 
	The Cloud Platform Engineering team at GE Digital are big fans of Cloud Foundry, it's the foundation of their Industrial Cloud Platform called Predix. Their global Cloud Foundry install base serves a developer community of thousands, and run tens of thousands of production application instances. 

One of the key benefits of Cloud Foundry is it's ability to dynamically scale all of it's core components using BOSH. As Cloud Foundry subsystems like runners and routers are scaled out to keep up with demand, ensuring that the new nodes automatically come under monitoring and metrics collection coverage, and keeping health dashboards up to date becomes challenging! 

Jeff Barrows will show how the Cloud Platform Monitoring team is using Sensu, an open-source monitoring framework, and Graphite, an open-source realtime metrics collection solution, to provide health and availability data on all Cloud Foundry components and Marketplace Services. He will also show how they build and maintain summary KPI, health, and utilization dashboards in Grafana - an open source graphing and dashboard visualization tool, and cover some simple strategies they use to help keep everything up to date in a dynamic environment. 

Jeff Barrows
GE Digital
Manager / Technical Lead - Cloud Platform
Oakland, CA
Jeff Barrows has been working at GE Digital for the past two years as a technical lead, and manager of the Cloud Platform Engineering team, for GE's Industrial Cloud Platform called Predix. He has been working in technology in the Bay Area since 1994 in industries ranging from Social Media companies like Yelp to Investment Banks, Telcos, and Web Startups. Jeff lives in Oakland, California, with his wife and daughter, and likes to hike in the East Bay hills with his Rhodesian Ridgeback.
Captions: 
	00:00:00,030 --> 00:00:04,140
good morning everybody thanks for coming

00:00:01,890 --> 00:00:05,940
and listening to the talk today I'm

00:00:04,140 --> 00:00:08,700
really grateful to be able to share with

00:00:05,940 --> 00:00:10,050
you today some of the things my team and

00:00:08,700 --> 00:00:12,450
the open-source community have been

00:00:10,050 --> 00:00:13,049
doing and working on around cloudy

00:00:12,450 --> 00:00:16,230
boundary

00:00:13,049 --> 00:00:18,090
muttering cloud boundary I'm Geoff

00:00:16,230 --> 00:00:20,670
barrows and I'd like to answer a

00:00:18,090 --> 00:00:22,140
question that you may have you may have

00:00:20,670 --> 00:00:25,199
right on the top of your mind right now

00:00:22,140 --> 00:00:26,730
as you look at this image no that is

00:00:25,199 --> 00:00:29,039
definitely not me up on top of that

00:00:26,730 --> 00:00:31,560
windmill this is somebody who's crazy

00:00:29,039 --> 00:00:33,450
and does some pretty extreme stuff on

00:00:31,560 --> 00:00:37,200
maintenance on these machines

00:00:33,450 --> 00:00:39,899
I am a engineering manager and technical

00:00:37,200 --> 00:00:44,700
lead for the cloud services team at GE

00:00:39,899 --> 00:00:46,289
digital and I work much more comfortably

00:00:44,700 --> 00:00:50,910
on distributed computer systems like

00:00:46,289 --> 00:00:52,530
cloud foundry my team the cloud services

00:00:50,910 --> 00:00:55,079
team is responsible for building and

00:00:52,530 --> 00:00:58,410
running the cloud foundry ecosystem for

00:00:55,079 --> 00:01:01,190
a GE service called predicts and

00:00:58,410 --> 00:01:03,870
predicts is a cloud platform it allows

00:01:01,190 --> 00:01:05,309
developers to build and run applications

00:01:03,870 --> 00:01:09,600
that can interface with some of the

00:01:05,309 --> 00:01:13,799
things that GE produces like jet engines

00:01:09,600 --> 00:01:16,140
again that guys not me he's standing in

00:01:13,799 --> 00:01:19,650
a test facility with one of G's latest

00:01:16,140 --> 00:01:21,060
jet engines our aviation division makes

00:01:19,650 --> 00:01:22,830
state-of-the-art jet engines that are

00:01:21,060 --> 00:01:25,920
used on many of the commercial jets

00:01:22,830 --> 00:01:26,490
flown today and in fact every two

00:01:25,920 --> 00:01:29,250
seconds

00:01:26,490 --> 00:01:30,659
an aircraft powered by GE jet engines is

00:01:29,250 --> 00:01:33,659
taking off somewhere in the world today

00:01:30,659 --> 00:01:36,030
and at any given moment like right now

00:01:33,659 --> 00:01:39,750
there's 2,200 aircrafts in the air

00:01:36,030 --> 00:01:40,829
carrying over 300,000 people so it's

00:01:39,750 --> 00:01:43,229
kind of crazy to think that there's

00:01:40,829 --> 00:01:46,560
actually 300,000 people above the earth

00:01:43,229 --> 00:01:49,500
at any given moment in time but GE is

00:01:46,560 --> 00:01:53,369
powering some of those those those jet

00:01:49,500 --> 00:01:55,799
planes our trench transportation

00:01:53,369 --> 00:01:58,020
division makes locomotives brand new

00:01:55,799 --> 00:02:00,240
state-of-the-art G locomotive features

00:01:58,020 --> 00:02:01,770
hundreds of sensors and generates

00:02:00,240 --> 00:02:04,560
hundreds of thousands of data points a

00:02:01,770 --> 00:02:08,729
minute and by connecting these rolling

00:02:04,560 --> 00:02:10,679
sensors to predicts it can help us

00:02:08,729 --> 00:02:12,720
develop apps that help unlock new

00:02:10,679 --> 00:02:16,290
efficiencies and rail transportation

00:02:12,720 --> 00:02:20,820
systems so it's estimated that every 1%

00:02:16,290 --> 00:02:24,110
increase in rail efficiency in the u.s.

00:02:20,820 --> 00:02:26,340
today is worth about 1.8 billion dollars

00:02:24,110 --> 00:02:27,690
we also make many of the machines that

00:02:26,340 --> 00:02:29,640
make up oil and gas production

00:02:27,690 --> 00:02:33,330
facilities things like industrial

00:02:29,640 --> 00:02:34,590
compressors generators pumps all the

00:02:33,330 --> 00:02:36,600
things that are critical to the safe

00:02:34,590 --> 00:02:39,090
extraction and production of oil and gas

00:02:36,600 --> 00:02:40,890
around the globe efficiencies in these

00:02:39,090 --> 00:02:43,800
systems gathered through insights

00:02:40,890 --> 00:02:45,959
created by software help extend running

00:02:43,800 --> 00:02:47,850
periods of these machines minimize

00:02:45,959 --> 00:02:51,990
production disruption and help reduce

00:02:47,850 --> 00:02:54,540
costs our power and water division makes

00:02:51,990 --> 00:02:57,930
these massive gas turbines that generate

00:02:54,540 --> 00:03:00,150
electricity and this one alone generates

00:02:57,930 --> 00:03:03,390
enough electricity to supply half a

00:03:00,150 --> 00:03:05,190
million homes with with power GE

00:03:03,390 --> 00:03:08,730
equipment generates half of the world's

00:03:05,190 --> 00:03:10,770
installed power base and 80 percent of

00:03:08,730 --> 00:03:15,300
the electricity flowing in North America

00:03:10,770 --> 00:03:17,280
is controlled by GE systems and you

00:03:15,300 --> 00:03:19,050
guessed it those four guys working on

00:03:17,280 --> 00:03:23,640
that turbine are definitely not me

00:03:19,050 --> 00:03:25,739
this is not my brain but GE Healthcare

00:03:23,640 --> 00:03:27,540
makes some of the most advanced imaging

00:03:25,739 --> 00:03:29,910
systems in the world and by connecting

00:03:27,540 --> 00:03:32,100
these MRI machines to Jie health cloud

00:03:29,910 --> 00:03:34,850
which runs on top of products we can

00:03:32,100 --> 00:03:37,530
provide better faster diagnostics and

00:03:34,850 --> 00:03:39,209
provide actionable information to

00:03:37,530 --> 00:03:40,950
patients more quickly which directly

00:03:39,209 --> 00:03:45,840
impacts their lives and their quality of

00:03:40,950 --> 00:03:47,670
care so by connecting as you can kind of

00:03:45,840 --> 00:03:48,959
get the point by connecting a lot of

00:03:47,670 --> 00:03:50,640
these industrial machines to the pre

00:03:48,959 --> 00:03:52,739
DEQ's platform we're basically enabling

00:03:50,640 --> 00:03:54,690
brand new efficiencies and opportunities

00:03:52,739 --> 00:03:57,600
that only a connected software platform

00:03:54,690 --> 00:04:00,150
can unlock it's really truly an exciting

00:03:57,600 --> 00:04:02,220
time to be working here at GE digital so

00:04:00,150 --> 00:04:04,260
we have any questions about it please

00:04:02,220 --> 00:04:05,880
feel free to ask me about it

00:04:04,260 --> 00:04:07,799
the reason I'm really talking about pre

00:04:05,880 --> 00:04:10,230
dix today is because it's mainly built

00:04:07,799 --> 00:04:12,360
on a number of components the G digital

00:04:10,230 --> 00:04:14,610
software engineers build a whole bunch

00:04:12,360 --> 00:04:16,709
of industrial micro services that can be

00:04:14,610 --> 00:04:19,290
composed and used to build these pretty

00:04:16,709 --> 00:04:21,270
awesome industrial applications

00:04:19,290 --> 00:04:24,150
but pre Dix is also built on top of

00:04:21,270 --> 00:04:26,790
cloud foundry

00:04:24,150 --> 00:04:29,040
as you probably aware cloth boundary

00:04:26,790 --> 00:04:31,080
makes it really easy for app developers

00:04:29,040 --> 00:04:35,130
to quickly deliver production grade

00:04:31,080 --> 00:04:37,889
applications to market over the past 16

00:04:35,130 --> 00:04:40,320
months or so our team the cloud services

00:04:37,889 --> 00:04:42,360
team has built a global cloud boundary

00:04:40,320 --> 00:04:44,220
deployment footprint we've enabled

00:04:42,360 --> 00:04:46,650
thousands of developers to start writing

00:04:44,220 --> 00:04:48,090
industrial applications and we're

00:04:46,650 --> 00:04:50,340
currently running tens of thousands of

00:04:48,090 --> 00:04:53,729
application instances across the globe

00:04:50,340 --> 00:04:54,960
today so it's been quite a journey but

00:04:53,729 --> 00:04:57,630
we didn't just get here magically

00:04:54,960 --> 00:04:59,910
overnight so how did we get here it all

00:04:57,630 --> 00:05:02,880
started pretty simply and it probably

00:04:59,910 --> 00:05:04,260
started in a way that it started for

00:05:02,880 --> 00:05:07,500
many of you starting to run Cloud

00:05:04,260 --> 00:05:11,520
Foundry today started with a POC

00:05:07,500 --> 00:05:14,340
we began with the grain Greenfield empty

00:05:11,520 --> 00:05:16,620
AWS VPC account and some credentials and

00:05:14,340 --> 00:05:19,349
we were really fortunate enough to be

00:05:16,620 --> 00:05:21,090
able to work with dr. Nick and the Stark

00:05:19,349 --> 00:05:22,650
and Wayne folks and in a few months had

00:05:21,090 --> 00:05:25,889
a full-blown dev environment up and

00:05:22,650 --> 00:05:28,199
running in our Amazon environment with

00:05:25,889 --> 00:05:30,539
the help of a handful of app does that

00:05:28,199 --> 00:05:33,389
digital we showed that we could get a

00:05:30,539 --> 00:05:37,260
brilliant factory's application an MVP

00:05:33,389 --> 00:05:39,599
up and running really quickly so after

00:05:37,260 --> 00:05:43,130
proving the point that Cloud Foundry was

00:05:39,599 --> 00:05:45,690
indeed helping developers go really fast

00:05:43,130 --> 00:05:47,539
we got a challenge from leadership to

00:05:45,690 --> 00:05:50,820
say hey listen can you actually deliver

00:05:47,539 --> 00:05:52,620
for production applications to a paying

00:05:50,820 --> 00:05:55,949
customer in less than three months and

00:05:52,620 --> 00:05:57,150
we were like oh man it's that's it's

00:05:55,949 --> 00:05:58,860
getting serious

00:05:57,150 --> 00:06:04,620
so it's game on it's time to production

00:05:58,860 --> 00:06:06,210
lies Cloud Foundry so being the

00:06:04,620 --> 00:06:08,070
operationally inclined kind of guy that

00:06:06,210 --> 00:06:10,710
I am I thought well first things first

00:06:08,070 --> 00:06:12,330
let's get some monitoring and telemetry

00:06:10,710 --> 00:06:13,860
on the system let's see if we can get

00:06:12,330 --> 00:06:17,669
some kind of metrics out of this black

00:06:13,860 --> 00:06:19,440
box that's called Cloud Foundry so being

00:06:17,669 --> 00:06:21,090
new to Cloud Foundry it was time to

00:06:19,440 --> 00:06:22,979
start digging into that black box to

00:06:21,090 --> 00:06:26,340
really start figuring out how things

00:06:22,979 --> 00:06:28,110
worked but have you really have you

00:06:26,340 --> 00:06:31,349
actually looked at all the things that

00:06:28,110 --> 00:06:33,870
comprise Cloud Foundry you're like holy

00:06:31,349 --> 00:06:34,980
crap that's not a moon it's cloud

00:06:33,870 --> 00:06:38,180
foundry

00:06:34,980 --> 00:06:41,430
and you realize like it's composed of

00:06:38,180 --> 00:06:43,980
twelve different subsystems that really

00:06:41,430 --> 00:06:46,770
make up the underpinnings of Cloud

00:06:43,980 --> 00:06:50,850
Foundry cog controllers Nats servers go

00:06:46,770 --> 00:06:53,550
routers runners lager Gators databases

00:06:50,850 --> 00:06:57,120
and Bosch so you're like such wow this

00:06:53,550 --> 00:06:59,880
is going to be a crazy adventure this is

00:06:57,120 --> 00:07:02,040
going to be a really challenging thing

00:06:59,880 --> 00:07:04,350
to kind of get our hands on so where do

00:07:02,040 --> 00:07:07,440
we start

00:07:04,350 --> 00:07:09,180
we started pretty simply with cod watch

00:07:07,440 --> 00:07:11,700
so we were deploying on AWS

00:07:09,180 --> 00:07:13,920
we had cloud watch that's for free we

00:07:11,700 --> 00:07:15,810
kind of took a peek at cloud watch and

00:07:13,920 --> 00:07:17,280
we're like alright great we can kind of

00:07:15,810 --> 00:07:18,870
see all the different components that

00:07:17,280 --> 00:07:22,620
make up Cloud Foundry now we have the

00:07:18,870 --> 00:07:25,170
general feel for you know the Linux

00:07:22,620 --> 00:07:28,050
stats that are coming off of it but we

00:07:25,170 --> 00:07:31,080
quickly realized that cloud watch wasn't

00:07:28,050 --> 00:07:32,580
going to be our long-term solution about

00:07:31,080 --> 00:07:34,590
a number of reasons we wanted to have a

00:07:32,580 --> 00:07:38,040
unified monitoring approach across both

00:07:34,590 --> 00:07:39,180
AWS and data center deployments we

00:07:38,040 --> 00:07:40,340
wanted something that had a little bit

00:07:39,180 --> 00:07:43,650
better

00:07:40,340 --> 00:07:46,080
flexible metrics collection and

00:07:43,650 --> 00:07:48,930
rendering service so we pretty quickly

00:07:46,080 --> 00:07:53,040
moved moved away from cloud watch so

00:07:48,930 --> 00:07:54,690
time to build so engineer inside me says

00:07:53,040 --> 00:07:56,130
awesome this is going to be so much fun

00:07:54,690 --> 00:07:57,990
it's going to be a great project I get

00:07:56,130 --> 00:07:59,700
to learn all that cool new monitoring

00:07:57,990 --> 00:08:01,980
solutions out that are out there today

00:07:59,700 --> 00:08:03,900
and then the engineering manager inside

00:08:01,980 --> 00:08:06,690
of me says no it's going to be this has

00:08:03,900 --> 00:08:07,980
like potential to be a black hole we may

00:08:06,690 --> 00:08:10,200
not be able to make something that's

00:08:07,980 --> 00:08:12,810
actually going to work I'm really

00:08:10,200 --> 00:08:18,210
nervous about kind of approaching this

00:08:12,810 --> 00:08:21,930
adventure so what do we do so first we

00:08:18,210 --> 00:08:23,730
started detailing some high-level goals

00:08:21,930 --> 00:08:26,700
for our metrics and monitoring

00:08:23,730 --> 00:08:28,680
collection system the system we believed

00:08:26,700 --> 00:08:31,350
it should be at like a utility service

00:08:28,680 --> 00:08:33,419
so it should be highly available it

00:08:31,350 --> 00:08:35,580
should be you BIC witness it should be

00:08:33,419 --> 00:08:37,919
everywhere and it should be as easy for

00:08:35,580 --> 00:08:42,270
anyone to use as turning on a light

00:08:37,919 --> 00:08:44,219
switch coverage should be automatic the

00:08:42,270 --> 00:08:46,530
system should be born with monitoring

00:08:44,219 --> 00:08:48,860
coverage it should be able to be driven

00:08:46,530 --> 00:08:50,930
by configuration management

00:08:48,860 --> 00:08:52,100
and then when the system retires or dies

00:08:50,930 --> 00:08:54,970
for whatever reason it should

00:08:52,100 --> 00:08:58,070
automatically be removed from coverage

00:08:54,970 --> 00:09:00,440
it should be an extensible system so as

00:08:58,070 --> 00:09:02,450
we get good at the basics we should be

00:09:00,440 --> 00:09:04,280
really should easily be able to add

00:09:02,450 --> 00:09:07,520
increasingly sophisticated capabilities

00:09:04,280 --> 00:09:09,350
to the system we knew you know how to

00:09:07,520 --> 00:09:11,360
monitor Linux systems really well and

00:09:09,350 --> 00:09:12,980
get the the base statistics out of that

00:09:11,360 --> 00:09:15,080
but Cloud Foundry was a little bit more

00:09:12,980 --> 00:09:16,370
abstract we didn't know exactly how

00:09:15,080 --> 00:09:18,470
we're going to interface with the system

00:09:16,370 --> 00:09:20,030
and how we're going to pull data that

00:09:18,470 --> 00:09:23,090
could help show the health and

00:09:20,030 --> 00:09:24,290
well-being of that system lastly it

00:09:23,090 --> 00:09:26,060
should integrate really well with our

00:09:24,290 --> 00:09:29,480
existing configuration management tools

00:09:26,060 --> 00:09:31,910
so we use Bosch naturally for all of our

00:09:29,480 --> 00:09:33,710
cloud foundry deployments we use it for

00:09:31,910 --> 00:09:35,480
some services deployments and then we

00:09:33,710 --> 00:09:37,370
use chef quite a bit for service

00:09:35,480 --> 00:09:41,210
deployments and some supporting systems

00:09:37,370 --> 00:09:44,230
as well so we took a look at the current

00:09:41,210 --> 00:09:47,180
state of monitoring back in 2014 and

00:09:44,230 --> 00:09:50,360
after a quick bake-off we decided to

00:09:47,180 --> 00:09:52,100
build an MVP solution using sensu as the

00:09:50,360 --> 00:09:54,800
monitoring framework graphite as a

00:09:52,100 --> 00:10:00,650
metrics collection system and grow fond

00:09:54,800 --> 00:10:03,170
ofor data visualization so let's talk a

00:10:00,650 --> 00:10:05,900
little bit about sensu what is sensu so

00:10:03,170 --> 00:10:07,520
aside from being the Japanese word for a

00:10:05,900 --> 00:10:08,570
folding fan you can kind of see where

00:10:07,520 --> 00:10:11,990
the logo is inspired

00:10:08,570 --> 00:10:14,510
it's a composable framework and with it

00:10:11,990 --> 00:10:16,760
you can do things like execute service

00:10:14,510 --> 00:10:19,250
checks you can send notifications and

00:10:16,760 --> 00:10:21,380
alerts you can collect metrics and then

00:10:19,250 --> 00:10:23,060
you can drive all this setup and

00:10:21,380 --> 00:10:24,770
configuration using configuration

00:10:23,060 --> 00:10:26,300
management tools so it checks all the

00:10:24,770 --> 00:10:30,050
boxes for the things that we wanted to

00:10:26,300 --> 00:10:33,290
do give a quick overview of the sensory

00:10:30,050 --> 00:10:34,940
architecture it's comprised of a couple

00:10:33,290 --> 00:10:37,120
different layers the first one is the

00:10:34,940 --> 00:10:40,010
sensitive server layer it's an NT ER

00:10:37,120 --> 00:10:41,860
stateless system you can deploy as many

00:10:40,010 --> 00:10:44,270
sensitive servers as you want

00:10:41,860 --> 00:10:46,670
the sensitive server is responsible for

00:10:44,270 --> 00:10:49,300
publishing the check requests and then

00:10:46,670 --> 00:10:53,450
processing events as they come back

00:10:49,300 --> 00:10:56,540
there's a rabid MQ cluster we have a

00:10:53,450 --> 00:10:58,190
multi node rabbitmq cluster so it can be

00:10:56,540 --> 00:11:00,500
fault tolerant we distribute and

00:10:58,190 --> 00:11:01,180
replicate queues so it's fault tolerant

00:11:00,500 --> 00:11:04,540
and node failure

00:11:01,180 --> 00:11:05,980
availability zone failure and then the

00:11:04,540 --> 00:11:09,010
sensitive clients there are a ruby

00:11:05,980 --> 00:11:11,320
client that gets distributed out to all

00:11:09,010 --> 00:11:16,500
of the the machines that we want to have

00:11:11,320 --> 00:11:20,380
coverage on they execute checks and

00:11:16,500 --> 00:11:22,570
posts back data back to sent to you for

00:11:20,380 --> 00:11:24,670
processing and then there's a Redis

00:11:22,570 --> 00:11:26,200
cluster the Redis cluster basically

00:11:24,670 --> 00:11:28,780
keeps track of a couple of things but

00:11:26,200 --> 00:11:30,850
mainly health check state so then you

00:11:28,780 --> 00:11:34,290
can do things like occurrence based

00:11:30,850 --> 00:11:36,910
checks so you can say my CPUs been over

00:11:34,290 --> 00:11:42,400
threshold for the last three checks then

00:11:36,910 --> 00:11:44,380
go wake somebody up and then lastly the

00:11:42,400 --> 00:11:47,500
sense you API servers so this is

00:11:44,380 --> 00:11:49,300
basically the thing REST API is that can

00:11:47,500 --> 00:11:52,240
interface with the sensory subsystems

00:11:49,300 --> 00:11:55,030
it's what all of the different since you

00:11:52,240 --> 00:11:57,310
admin dashboards are based on so those

00:11:55,030 --> 00:11:59,350
guys can talk to sensitive API is to

00:11:57,310 --> 00:12:03,700
pull a list of clients full lists of

00:11:59,350 --> 00:12:04,900
checks get check data it can also be

00:12:03,700 --> 00:12:08,880
used to integrate with third-party

00:12:04,900 --> 00:12:08,880
systems as well which is pretty cool

00:12:15,010 --> 00:12:21,710
so let's walk through how we actually

00:12:18,170 --> 00:12:23,690
execute a service check using sensing so

00:12:21,710 --> 00:12:26,600
first the sensitive server publishes a

00:12:23,690 --> 00:12:30,050
check request to subscriber cues on

00:12:26,600 --> 00:12:32,540
rabbitmq the sense of clients that are

00:12:30,050 --> 00:12:34,399
configured to subscribe to that

00:12:32,540 --> 00:12:36,290
particular queue see the message that

00:12:34,399 --> 00:12:39,560
gets published to the the queue

00:12:36,290 --> 00:12:43,480
takes it off execute the the check and

00:12:39,560 --> 00:12:47,149
then publishes a response back to

00:12:43,480 --> 00:12:49,040
RabbitMQ essentially servers process

00:12:47,149 --> 00:12:51,050
that check response and as I mentioned

00:12:49,040 --> 00:12:53,720
before we can scale that tier out for

00:12:51,050 --> 00:12:57,430
scalability and resiliency sensory

00:12:53,720 --> 00:12:59,570
server processes that event and triggers

00:12:57,430 --> 00:13:02,180
actions if they're so configured and

00:12:59,570 --> 00:13:08,750
then updates Redis with the TEL check

00:13:02,180 --> 00:13:11,240
state we'll take a quick look at what

00:13:08,750 --> 00:13:14,270
service check actually looks like so

00:13:11,240 --> 00:13:17,390
this is a Natta me of a service check is

00:13:14,270 --> 00:13:20,990
basically it's really simple it's a

00:13:17,390 --> 00:13:22,370
command or script which runs and outputs

00:13:20,990 --> 00:13:25,279
data to standard out or standard error

00:13:22,370 --> 00:13:27,410
so if you're familiar with Nagios health

00:13:25,279 --> 00:13:29,230
checks or Nagios plugins follows the

00:13:27,410 --> 00:13:31,430
same standard if you have a fleet of

00:13:29,230 --> 00:13:34,220
Nagios scripts that you've collected and

00:13:31,430 --> 00:13:36,020
built over time you can drop those in to

00:13:34,220 --> 00:13:38,350
sensu and run those right away so it's

00:13:36,020 --> 00:13:40,910
all supported right out of the box

00:13:38,350 --> 00:13:43,580
basically the the commander script will

00:13:40,910 --> 00:13:47,000
run and then produce an exit code 0 is

00:13:43,580 --> 00:13:50,330
OK 1 is warning 2 is critical and 3 is

00:13:47,000 --> 00:13:52,160
custom so we use an exit code of 3/4 to

00:13:50,330 --> 00:13:54,980
indicate that we have a metrics type

00:13:52,160 --> 00:13:58,010
response for the for the check and then

00:13:54,980 --> 00:14:00,320
it can also put an optional response

00:13:58,010 --> 00:14:02,360
payload usually in JSON on to the

00:14:00,320 --> 00:14:05,570
message bus attached with that

00:14:02,360 --> 00:14:07,640
particular response you define a list of

00:14:05,570 --> 00:14:09,260
subscribers so this is a list of the

00:14:07,640 --> 00:14:12,220
nodes that should be interested in

00:14:09,260 --> 00:14:14,690
running that particular health check and

00:14:12,220 --> 00:14:17,060
then there's handlers handlers are

00:14:14,690 --> 00:14:19,310
things that take action on events if any

00:14:17,060 --> 00:14:20,660
are configured and lastly there's a

00:14:19,310 --> 00:14:23,180
check interval so you can specify how

00:14:20,660 --> 00:14:26,120
often you want the check to run so this

00:14:23,180 --> 00:14:28,250
is a quick overview of what an actual

00:14:26,120 --> 00:14:31,340
check definition looks like simple

00:14:28,250 --> 00:14:33,710
Ehsan you can see it's got the check

00:14:31,340 --> 00:14:36,230
name which is check disk usage it's got

00:14:33,710 --> 00:14:38,510
a couple flags in there for warning and

00:14:36,230 --> 00:14:41,240
critical thresholds it's got the

00:14:38,510 --> 00:14:42,800
subscriber defined as production DEA so

00:14:41,240 --> 00:14:46,190
this would run on all of our production

00:14:42,800 --> 00:14:47,900
runner nodes handler is configured to

00:14:46,190 --> 00:14:48,920
pay tree duty so if this goes bump in

00:14:47,900 --> 00:14:51,080
the night it's gonna go and wake

00:14:48,920 --> 00:14:55,670
somebody up and that runs every 60

00:14:51,080 --> 00:14:57,170
seconds so handlers I mentioned handlers

00:14:55,670 --> 00:14:59,150
a couple times this is it's really hard

00:14:57,170 --> 00:15:01,070
to overstate the power and flexibility

00:14:59,150 --> 00:15:03,920
of the the handler construct within

00:15:01,070 --> 00:15:05,810
sensu handlers are basically actions

00:15:03,920 --> 00:15:09,650
executed by a sensitive server when

00:15:05,810 --> 00:15:12,320
events are received things like send a

00:15:09,650 --> 00:15:14,450
note to pager Duty send a metric to Graf

00:15:12,320 --> 00:15:17,960
I integrate with flow doc or maybe send

00:15:14,450 --> 00:15:19,400
an email there for primary handler

00:15:17,960 --> 00:15:21,950
primitives

00:15:19,400 --> 00:15:25,340
there's a pipe handler type which is

00:15:21,950 --> 00:15:27,800
external command that gets run it can

00:15:25,340 --> 00:15:29,900
consume that JSON payload that gets put

00:15:27,800 --> 00:15:32,570
on the on the event response onto the

00:15:29,900 --> 00:15:34,310
message bus you can parse it it's any

00:15:32,570 --> 00:15:36,020
language that you want can be bash it

00:15:34,310 --> 00:15:37,180
can be Ruby can be whatever that you're

00:15:36,020 --> 00:15:39,980
most comfortable with

00:15:37,180 --> 00:15:42,140
processes that data transforms it and

00:15:39,980 --> 00:15:44,420
then does something with it so send an

00:15:42,140 --> 00:15:45,850
email integrate with flow doc or what

00:15:44,420 --> 00:15:49,220
have you

00:15:45,850 --> 00:15:52,010
second type is the tcp UDP handler type

00:15:49,220 --> 00:15:54,890
so it knows how to make a network socket

00:15:52,010 --> 00:15:57,170
connection to an external system this is

00:15:54,890 --> 00:16:00,170
how we get stats shoved over to our

00:15:57,170 --> 00:16:05,120
graphite system today so it's a pretty

00:16:00,170 --> 00:16:07,670
powerful construct the last main one is

00:16:05,120 --> 00:16:09,860
that the transport handler type so if

00:16:07,670 --> 00:16:12,800
you wanted to you could have a second

00:16:09,860 --> 00:16:15,260
named another named Q on rabbitmq you

00:16:12,800 --> 00:16:17,270
could publish a message to that q and

00:16:15,260 --> 00:16:19,130
then have third party or external

00:16:17,270 --> 00:16:21,230
resources actually watch that Q and that

00:16:19,130 --> 00:16:23,950
pub/sub model and then pull events off

00:16:21,230 --> 00:16:27,080
for like extra third party integrations

00:16:23,950 --> 00:16:28,940
the last one is really a concatenation

00:16:27,080 --> 00:16:30,740
of all of these so you can if you wanted

00:16:28,940 --> 00:16:33,470
to take multiple actions on an event at

00:16:30,740 --> 00:16:35,089
any time you could say I want to send

00:16:33,470 --> 00:16:36,560
the stat over to graphite and then I

00:16:35,089 --> 00:16:37,980
want to page somebody and wake them up

00:16:36,560 --> 00:16:40,290
and

00:16:37,980 --> 00:16:47,660
maybe put a message on a message bus for

00:16:40,290 --> 00:16:50,040
a third party system so metrics and and

00:16:47,660 --> 00:16:52,499
metrics collection using graphite and

00:16:50,040 --> 00:16:55,350
graphite graphite is an open-source

00:16:52,499 --> 00:16:58,309
system that allows us to collect store

00:16:55,350 --> 00:17:01,769
and render time series data it's got a

00:16:58,309 --> 00:17:04,139
simple line protocol that basically

00:17:01,769 --> 00:17:06,089
consists of the metric name the

00:17:04,139 --> 00:17:08,039
timestamp and epoch format and the

00:17:06,089 --> 00:17:10,100
metric value you just can actually

00:17:08,039 --> 00:17:14,429
netcat that out to a socket and get a

00:17:10,100 --> 00:17:16,019
metric for assisted in graphite and then

00:17:14,429 --> 00:17:18,510
it's got flexible storage backends it

00:17:16,019 --> 00:17:21,120
supports whisper DB flat files that's

00:17:18,510 --> 00:17:22,799
what we use today to scale out were

00:17:21,120 --> 00:17:25,709
handling hundreds of thousands of

00:17:22,799 --> 00:17:27,689
metrics a second using whisper DB we

00:17:25,709 --> 00:17:28,649
know that there's a probably a runway

00:17:27,689 --> 00:17:31,649
that we're not going to be able to

00:17:28,649 --> 00:17:33,630
support much a lot more unless we really

00:17:31,649 --> 00:17:37,340
scale that out so it also supports

00:17:33,630 --> 00:17:39,659
influx DBE open TS DB sinai and others

00:17:37,340 --> 00:17:44,250
and then it's got a really super robust

00:17:39,659 --> 00:17:46,230
API for metrics retrieval and analytics

00:17:44,250 --> 00:17:48,659
function so you can basically execute a

00:17:46,230 --> 00:17:50,610
crow command get data back out of

00:17:48,659 --> 00:17:52,950
graphite and you can get it to do some

00:17:50,610 --> 00:17:54,570
things like averaging or percentiles or

00:17:52,950 --> 00:17:57,510
sums and things like that which is

00:17:54,570 --> 00:18:00,750
pretty cool so we'll walk through how

00:17:57,510 --> 00:18:05,429
sensu gets metrics into graphite real

00:18:00,750 --> 00:18:08,760
quick as a remember before the metrics

00:18:05,429 --> 00:18:10,350
check is scheduled the clients run that

00:18:08,760 --> 00:18:12,419
check and then published the event back

00:18:10,350 --> 00:18:15,210
to the message bus the census servers

00:18:12,419 --> 00:18:17,549
are configured with a graphite handler

00:18:15,210 --> 00:18:20,510
which knows how to make that excuse me

00:18:17,549 --> 00:18:24,799
the TCP connection to to graphite

00:18:20,510 --> 00:18:27,750
processes the metrics event request and

00:18:24,799 --> 00:18:30,929
then connects directly to what's called

00:18:27,750 --> 00:18:33,090
carbon relay it's a Python process it's

00:18:30,929 --> 00:18:36,690
basically responsible for metrics

00:18:33,090 --> 00:18:38,669
ingestion and routing it knows how to

00:18:36,690 --> 00:18:42,179
get that metric off the wire and then

00:18:38,669 --> 00:18:43,649
into a persistence layer so there's a

00:18:42,179 --> 00:18:45,690
number of different cool things that can

00:18:43,649 --> 00:18:48,240
do to help support distribution of

00:18:45,690 --> 00:18:51,270
storage using consistent hashing and

00:18:48,240 --> 00:18:53,430
replicas in this case it

00:18:51,270 --> 00:18:55,470
uses consistent hashing to send that

00:18:53,430 --> 00:18:58,020
metric out to the carbon cache and

00:18:55,470 --> 00:18:59,790
whisper DB layer so it sends that metric

00:18:58,020 --> 00:19:02,460
out to three nodes that are split across

00:18:59,790 --> 00:19:07,320
multiple availability zones to support

00:19:02,460 --> 00:19:09,180
that level of fault tolerance and then

00:19:07,320 --> 00:19:11,370
it's written to the whisper DB flat file

00:19:09,180 --> 00:19:14,280
so now your metric is off the wire and

00:19:11,370 --> 00:19:15,600
on the disk we want to be able to look

00:19:14,280 --> 00:19:17,310
at those things because what good is it

00:19:15,600 --> 00:19:20,360
if it's just stored on a disk so we use

00:19:17,310 --> 00:19:23,220
go fauna your fauna is a really awesome

00:19:20,360 --> 00:19:28,770
web interface that lets you build these

00:19:23,220 --> 00:19:31,440
really cool dashboards and KPIs it knows

00:19:28,770 --> 00:19:34,020
how to talk to the graphite API and

00:19:31,440 --> 00:19:37,770
graphite API knows how to pull metrics

00:19:34,020 --> 00:19:43,820
from the disk and then push it back into

00:19:37,770 --> 00:19:47,850
the dashboard for visualization lastly

00:19:43,820 --> 00:19:49,560
we also have a sensor client graphite

00:19:47,850 --> 00:19:51,600
metrics health check that is able to

00:19:49,560 --> 00:19:54,450
execute similar requests against the

00:19:51,600 --> 00:19:56,100
graphite API to be able to pull metrics

00:19:54,450 --> 00:19:59,840
out and then to be able to do some

00:19:56,100 --> 00:20:03,420
threshold and an alerting off of those

00:19:59,840 --> 00:20:05,070
so great but what we have now is a

00:20:03,420 --> 00:20:06,870
monitoring system and a metrics

00:20:05,070 --> 00:20:08,910
collection system we can execute health

00:20:06,870 --> 00:20:10,620
checks we can execute metrics retrieval

00:20:08,910 --> 00:20:14,760
checks we can get that data shoved into

00:20:10,620 --> 00:20:19,020
a data time series database how does

00:20:14,760 --> 00:20:20,700
that help us monitor Cloud Foundry so we

00:20:19,020 --> 00:20:22,830
one of our original goals was that we'd

00:20:20,700 --> 00:20:25,460
have automatic coverage of all the

00:20:22,830 --> 00:20:29,040
things so what we did was create

00:20:25,460 --> 00:20:31,440
bas-reliefs of the sensu client so we

00:20:29,040 --> 00:20:33,780
bundled up all the sensitive client bits

00:20:31,440 --> 00:20:35,540
the ruby parts of it and then all of the

00:20:33,780 --> 00:20:41,700
health checks that have to get executed

00:20:35,540 --> 00:20:43,860
across across the fleet and we included

00:20:41,700 --> 00:20:45,930
that sensu client job and all of our

00:20:43,860 --> 00:20:48,450
boss deployments so anything that bashed

00:20:45,930 --> 00:20:50,610
deploys whether it's cloud boundary or

00:20:48,450 --> 00:20:54,120
any of the the tiles or things that we

00:20:50,610 --> 00:20:59,690
use bacha to deploy we also include the

00:20:54,120 --> 00:21:02,490
sensitive client job so now every node

00:20:59,690 --> 00:21:04,740
that gets deployed and pushed out using

00:21:02,490 --> 00:21:07,380
Bosh has coverage we configure

00:21:04,740 --> 00:21:08,880
to belong to the all group it's just a

00:21:07,380 --> 00:21:10,800
default it's just a word we could have

00:21:08,880 --> 00:21:13,230
called it digital we could have call it

00:21:10,800 --> 00:21:16,170
whatever but now basically it allows us

00:21:13,230 --> 00:21:18,870
to capture all base Linux statistics for

00:21:16,170 --> 00:21:23,400
all the nodes that that Bosch is pushing

00:21:18,870 --> 00:21:25,800
out which is pretty cool now we capture

00:21:23,400 --> 00:21:29,429
things like CPU utilization Network

00:21:25,800 --> 00:21:36,990
utilization memory and disk and and the

00:21:29,429 --> 00:21:39,750
such a quick note on metric James this

00:21:36,990 --> 00:21:42,120
probably can't be understated as well

00:21:39,750 --> 00:21:43,320
setting naming standard for metrics is

00:21:42,120 --> 00:21:46,370
one of the most important things you can

00:21:43,320 --> 00:21:50,309
do as you plan your graphite deployment

00:21:46,370 --> 00:21:53,640
if you do it in a rationalized way it

00:21:50,309 --> 00:21:55,650
will enable you to do things like wild

00:21:53,640 --> 00:21:58,140
card aggregation of Statistics which is

00:21:55,650 --> 00:22:02,190
super nice to be able to do and a Cloud

00:21:58,140 --> 00:22:04,830
Foundry distributed systems world

00:22:02,190 --> 00:22:06,809
it minimizes maintenance of our

00:22:04,830 --> 00:22:09,210
dashboards and KPIs so things are kind

00:22:06,809 --> 00:22:11,640
of self maintaining as we scale out sub

00:22:09,210 --> 00:22:13,950
systems of cloud boundary so this is

00:22:11,640 --> 00:22:17,940
kind of an anatomy of a name for our

00:22:13,950 --> 00:22:20,490
metrics we base our names on the Bosch

00:22:17,940 --> 00:22:23,190
deployment so you can see here the first

00:22:20,490 --> 00:22:24,870
part is the this is a Bosch deployment

00:22:23,190 --> 00:22:28,110
for our u.s. West production Cloud

00:22:24,870 --> 00:22:30,660
Foundry deployment the second component

00:22:28,110 --> 00:22:32,160
of that is the actual Bosch job name so

00:22:30,660 --> 00:22:34,920
if you're familiar with looking at the

00:22:32,160 --> 00:22:36,420
Bosch manifests I'm sorry but you'll

00:22:34,920 --> 00:22:38,130
probably be familiar with seeing the

00:22:36,420 --> 00:22:40,170
different sub components that are

00:22:38,130 --> 00:22:44,010
comprising that that deployment

00:22:40,170 --> 00:22:46,320
this is runners e1 so this is a runner

00:22:44,010 --> 00:22:49,200
job that is for availability zone one

00:22:46,320 --> 00:22:51,660
and then this is the instance number or

00:22:49,200 --> 00:22:54,450
the index of that job so if you want it

00:22:51,660 --> 00:22:56,520
if you're your deployment has 10 runners

00:22:54,450 --> 00:22:59,010
within availability zone one this will

00:22:56,520 --> 00:23:01,530
go through zero through nine and then

00:22:59,010 --> 00:23:03,120
the last part of it is the actual metric

00:23:01,530 --> 00:23:05,490
name and this is generated more by the

00:23:03,120 --> 00:23:07,950
metrics check that you scheduled to run

00:23:05,490 --> 00:23:13,460
so this one in particular will grab a

00:23:07,950 --> 00:23:13,460
zero transmit bytes statistics

00:23:15,580 --> 00:23:21,380
so this is a dashboard that I was going

00:23:19,730 --> 00:23:22,970
to walk through real quick showing like

00:23:21,380 --> 00:23:26,180
how we actually construct a dashboard

00:23:22,970 --> 00:23:29,900
but this is a basic example of what you

00:23:26,180 --> 00:23:33,860
can capture just by getting the base

00:23:29,900 --> 00:23:35,510
Linux statistics and some of the cool

00:23:33,860 --> 00:23:38,330
things this I don't know if it's really

00:23:35,510 --> 00:23:41,750
easy to see here but what we've done is

00:23:38,330 --> 00:23:43,640
basically for each of the Cloud Foundry

00:23:41,750 --> 00:23:46,670
sub components we've created a high

00:23:43,640 --> 00:23:49,970
level KPI that captures these Linux

00:23:46,670 --> 00:23:52,430
statistics and using wild card

00:23:49,970 --> 00:23:55,670
aggregation with our metrics definitions

00:23:52,430 --> 00:23:58,190
we can actually get like CPU utilization

00:23:55,670 --> 00:24:01,550
or memory utilization across the entire

00:23:58,190 --> 00:24:04,100
fleet of say the runner pool and then we

00:24:01,550 --> 00:24:05,510
can do things like do some percentiles

00:24:04,100 --> 00:24:07,310
and things like that so we can kind of

00:24:05,510 --> 00:24:10,910
see where the outliers are and where the

00:24:07,310 --> 00:24:13,400
common where the common things are so we

00:24:10,910 --> 00:24:16,340
can kind of drill into those if we need

00:24:13,400 --> 00:24:18,110
to it's really easy to basically rinse

00:24:16,340 --> 00:24:20,600
and repeat so you kind of define your

00:24:18,110 --> 00:24:21,860
standard ones for for one set and then

00:24:20,600 --> 00:24:24,260
you can just go through and change the

00:24:21,860 --> 00:24:29,060
names generate quick dashboards for all

00:24:24,260 --> 00:24:31,190
of the others which is great but it

00:24:29,060 --> 00:24:34,340
still doesn't really give us a lot of

00:24:31,190 --> 00:24:36,830
depth into the Cloud Foundry subsystems

00:24:34,340 --> 00:24:40,670
right so now we can monitor all this

00:24:36,830 --> 00:24:42,860
stuff like Linux is spitting out CPU

00:24:40,670 --> 00:24:44,330
memory Network and disk for all that

00:24:42,860 --> 00:24:46,160
stuff it's just a little bit further

00:24:44,330 --> 00:24:49,160
than what we had with cloud watch

00:24:46,160 --> 00:24:51,800
because we have a nice environment to be

00:24:49,160 --> 00:24:54,190
able to generate dashboards and explore

00:24:51,800 --> 00:24:56,800
that data a little bit more flexibly but

00:24:54,190 --> 00:24:59,000
now I really need to start peering into

00:24:56,800 --> 00:25:02,450
the health and well-being of the Cloud

00:24:59,000 --> 00:25:04,910
Foundry subsystems so after a bit of

00:25:02,450 --> 00:25:07,700
research we came across a few open

00:25:04,910 --> 00:25:11,810
source tools that helped us crack that

00:25:07,700 --> 00:25:14,510
case and the first one we use is the

00:25:11,810 --> 00:25:16,280
collector so it's a ruby program it's an

00:25:14,510 --> 00:25:18,740
open source project

00:25:16,280 --> 00:25:21,640
it basically listens on the Nats bus 4cf

00:25:18,740 --> 00:25:24,260
subsystem announcements it pulls their

00:25:21,640 --> 00:25:27,570
their subsystems Varzi and hel Z

00:25:24,260 --> 00:25:29,930
endpoints and then it knows how to get

00:25:27,570 --> 00:25:32,940
data and parse it out and then push it

00:25:29,930 --> 00:25:35,870
directly to the carbon demons for the

00:25:32,940 --> 00:25:38,010
graphite system to persist a disk

00:25:35,870 --> 00:25:39,810
unfortunately that's being phased out

00:25:38,010 --> 00:25:44,130
and it's being phased out pretty rapidly

00:25:39,810 --> 00:25:46,740
I think I just heard as of 236 cloud

00:25:44,130 --> 00:25:48,450
foundry release the not a lot of the

00:25:46,740 --> 00:25:51,410
subsystems are no longer supporting that

00:25:48,450 --> 00:25:54,660
of our zlz endpoint which kind of sucks

00:25:51,410 --> 00:25:55,860
but there's hope so they're changing the

00:25:54,660 --> 00:25:58,650
model and they're starting to publish

00:25:55,860 --> 00:26:00,090
statistics tool aggregator and then you

00:25:58,650 --> 00:26:02,100
can actually make some nozzles that

00:26:00,090 --> 00:26:04,290
attach to the logging system so you can

00:26:02,100 --> 00:26:06,750
start parsing that out so we're actively

00:26:04,290 --> 00:26:08,370
working on I was in moving to that

00:26:06,750 --> 00:26:09,540
direction so we can continue to get all

00:26:08,370 --> 00:26:13,170
the stats that we need from a cloud

00:26:09,540 --> 00:26:16,050
foundry but a collector gives you a ton

00:26:13,170 --> 00:26:18,150
of different data so this is a sample

00:26:16,050 --> 00:26:20,280
dashboard that we created it's based on

00:26:18,150 --> 00:26:22,710
a lot of the good work that the PWS guys

00:26:20,280 --> 00:26:25,610
have done a lot of documentation up on

00:26:22,710 --> 00:26:28,620
the the cloud foundry website about

00:26:25,610 --> 00:26:31,230
monitoring Cloud Foundry I followed

00:26:28,620 --> 00:26:33,780
those outlines pretty detailed and then

00:26:31,230 --> 00:26:35,040
built this kind of stoplight type

00:26:33,780 --> 00:26:37,650
dashboard gives a really high-level

00:26:35,040 --> 00:26:41,820
overview of all the different subsystems

00:26:37,650 --> 00:26:43,920
and Cloud Foundry things like total

00:26:41,820 --> 00:26:47,790
number of runners the expected number of

00:26:43,920 --> 00:26:52,410
cloud controllers UA a servers total da

00:26:47,790 --> 00:26:54,390
memory used routes this ones intra the

00:26:52,410 --> 00:26:57,300
yellow one is pretty interesting and

00:26:54,390 --> 00:26:58,800
it's the available memory ratio helps us

00:26:57,300 --> 00:27:01,830
know when we need to scale out and add

00:26:58,800 --> 00:27:03,090
more capacity to the runner layer we can

00:27:01,830 --> 00:27:07,140
spend tons of time just talking about

00:27:03,090 --> 00:27:09,540
this one this is kind of like a high

00:27:07,140 --> 00:27:11,820
level one it's a this dashboards up in

00:27:09,540 --> 00:27:13,140
our like in our NOC area so people can

00:27:11,820 --> 00:27:15,000
kind of look at it and get a general

00:27:13,140 --> 00:27:16,560
sense of like hey his things are is

00:27:15,000 --> 00:27:19,680
everything pretty green yep everything's

00:27:16,560 --> 00:27:22,530
pretty green but if you need to you can

00:27:19,680 --> 00:27:24,780
continue to drill in to make specialized

00:27:22,530 --> 00:27:27,120
dashboards for even for each cloud

00:27:24,780 --> 00:27:30,330
foundry sub component so this one shows

00:27:27,120 --> 00:27:32,490
some detailed router stats so it shows

00:27:30,330 --> 00:27:35,340
things like total routes

00:27:32,490 --> 00:27:38,100
it shows HTTP B response codes from both

00:27:35,340 --> 00:27:40,169
your your DEA s the applications that

00:27:38,100 --> 00:27:42,029
are running as well as respond

00:27:40,169 --> 00:27:44,129
codes that come off of like kind of the

00:27:42,029 --> 00:27:45,749
core Cloud Foundry components like your

00:27:44,129 --> 00:27:47,549
cloud controllers if you see a lot of

00:27:45,749 --> 00:27:48,659
500s coming off cloud controller maybe

00:27:47,549 --> 00:27:51,480
there's something wrong with the cloud

00:27:48,659 --> 00:27:53,539
controller it also shows like CPU

00:27:51,480 --> 00:27:56,039
aggregation and and some other things

00:27:53,539 --> 00:27:58,109
that might be of interest total network

00:27:56,039 --> 00:28:00,149
that's like throughput that's coming

00:27:58,109 --> 00:28:01,679
across your whole go router fleet you

00:28:00,149 --> 00:28:05,340
can do aggregations like that which is

00:28:01,679 --> 00:28:09,090
pretty cool and you have that sweet

00:28:05,340 --> 00:28:12,090
dashboard but I'm not watching that

00:28:09,090 --> 00:28:14,429
dashboard all the time 24 by 7 so how do

00:28:12,090 --> 00:28:17,129
we actually get actions and take actions

00:28:14,429 --> 00:28:20,460
out of that thing and that's where we

00:28:17,129 --> 00:28:22,739
developed the since you HTTP health

00:28:20,460 --> 00:28:26,970
check that can query the graph I a P I

00:28:22,739 --> 00:28:29,279
so we schedule the HTTP help check on a

00:28:26,970 --> 00:28:31,889
couple different sensitive clients it

00:28:29,279 --> 00:28:33,809
knows how to construct basically a curl

00:28:31,889 --> 00:28:35,549
statement so you can actually go in and

00:28:33,809 --> 00:28:38,309
those dashboards that you've created you

00:28:35,549 --> 00:28:41,850
can pull the the definition out of that

00:28:38,309 --> 00:28:44,220
and put it into a curl statement fire it

00:28:41,850 --> 00:28:47,309
off against the graphite API graph I can

00:28:44,220 --> 00:28:48,450
go and retrieve the metrics do the type

00:28:47,309 --> 00:28:50,279
of aggregation that you're thinking

00:28:48,450 --> 00:28:52,139
about doing you can get an individual

00:28:50,279 --> 00:28:55,340
data point or a set of data points or

00:28:52,139 --> 00:28:57,659
you can average or some and then respond

00:28:55,340 --> 00:29:01,499
give a give a response back to the

00:28:57,659 --> 00:29:02,580
sensitive client with a JSON payload

00:29:01,499 --> 00:29:06,210
that has the details

00:29:02,580 --> 00:29:08,009
and then since you can actually take

00:29:06,210 --> 00:29:10,289
action on that so you can define

00:29:08,009 --> 00:29:11,730
thresholds and do all the good things

00:29:10,289 --> 00:29:14,970
that you can do with handlers tend to

00:29:11,730 --> 00:29:16,739
wake somebody up with Pedro duty and and

00:29:14,970 --> 00:29:18,450
that's about it so I'm sorry I only had

00:29:16,739 --> 00:29:20,940
a few minutes to give you a high-level

00:29:18,450 --> 00:29:23,340
overview about this but hopefully it

00:29:20,940 --> 00:29:25,649
gives you a better idea about how we're

00:29:23,340 --> 00:29:29,340
using sensu and graphite as the backbone

00:29:25,649 --> 00:29:31,129
of our monitoring solution we're working

00:29:29,340 --> 00:29:34,200
on getting the Boche release of

00:29:31,129 --> 00:29:37,289
sensitive client up on our github so it

00:29:34,200 --> 00:29:38,730
should be publicly available hopefully

00:29:37,289 --> 00:29:40,970
that'll help give you a head start and

00:29:38,730 --> 00:29:43,590
you can get get working with that I know

00:29:40,970 --> 00:29:45,960
Stark and Wayne guys have helped put out

00:29:43,590 --> 00:29:48,499
a Bosch release of sensitive server in

00:29:45,960 --> 00:29:50,759
the ecosystem so that's out there and

00:29:48,499 --> 00:29:53,270
yeah we look forward to putting up maybe

00:29:50,759 --> 00:29:54,950
dashboards and and templates and

00:29:53,270 --> 00:29:57,470
and other components on github as soon

00:29:54,950 --> 00:30:00,770
as we get those all cleared from from

00:29:57,470 --> 00:30:03,440
our lovely legal department so we have

00:30:00,770 --> 00:30:05,470
like I think no time for quite maybe you

00:30:03,440 --> 00:30:07,900
have a minute or two for questions but

00:30:05,470 --> 00:30:11,249
otherwise thank you very much

00:30:07,900 --> 00:30:11,249

YouTube URL: https://www.youtube.com/watch?v=xgi5kbIsp1Y


