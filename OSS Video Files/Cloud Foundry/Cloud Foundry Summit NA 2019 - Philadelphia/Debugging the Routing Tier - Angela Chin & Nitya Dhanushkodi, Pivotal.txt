Title: Debugging the Routing Tier - Angela Chin & Nitya Dhanushkodi, Pivotal
Publication date: 2019-04-11
Playlist: Cloud Foundry Summit NA 2019 - Philadelphia
Description: 
	Debugging the Routing Tier - Angela Chin & Nitya Dhanushkodi, Pivotal

As the entrypoint to Cloud Foundry, routing-tier related errors come from a variety of places. While some of these errors are indications of actual bugs in Cloud Foundry source code, there can be a number of other root causes-- misbehaving applications, misconfigured load balancers, and infrastructure issues, just to name a few. In order to isolate and debug these problems, it becomes important to know which data to collect, how to collect it, and what in the data can indicate the root cause. 

In this talk, Angela and Nitya will cover what information is useful to collect and what to look for in the data to systematically eliminate possible causes of the problem. Drawing on their experiences as members of the CF Routing team, they will go through issues that operators have seen in the past, and how they use tools such as routing logs, pprof, flamegraphs, and wireshark to debug, isolate, and work around these issues.

About Angela Chin
Angela is a software engineer at Pivotal, currently working on the Pivotal Container Service (PKS) team. Prior to this, she was a major contributor to Cloud Foundry, working on the CF Networking and CF Infrastructure teams. Angela has previously given talks at KubeCon, Open Source Summit, Open Networking Summit, and CF Summit (North America and Europe).

About Nitya Dhanushkodi
Nitya Dhanushkodi is a software engineer at Pivotal, working on the Cloud Foundry Routing team. Nitya enjoys learning about and working on projects related to Linux. She has also worked on the CF MySQL team and Credhub teams prior to Routing.

https://www.cloudfoundry.org/
Captions: 
	00:00:00,000 --> 00:00:05,310
hi everybody it's a minute early but

00:00:02,520 --> 00:00:09,179
again we're so excited so we're gonna

00:00:05,310 --> 00:00:13,530
get started thank you all for coming to

00:00:09,179 --> 00:00:16,020
our session today on debugging the

00:00:13,530 --> 00:00:17,940
routing tier my name is Angela I'm a

00:00:16,020 --> 00:00:19,470
software engineer a pivotal working in

00:00:17,940 --> 00:00:21,570
the CF networking program

00:00:19,470 --> 00:00:24,769
I'm nithya I'm also a software engineer

00:00:21,570 --> 00:00:28,560
at pivotal in the networking program and

00:00:24,769 --> 00:00:30,420
so obviously we're talking about

00:00:28,560 --> 00:00:32,130
debugging the routing tier but let's

00:00:30,420 --> 00:00:34,920
first start before we dive into the meat

00:00:32,130 --> 00:00:36,930
of the talk with a brief overview of

00:00:34,920 --> 00:00:38,250
what exactly we're planning on

00:00:36,930 --> 00:00:40,320
discussing for the next thirty minutes

00:00:38,250 --> 00:00:42,510
we're gonna start with a brief overview

00:00:40,320 --> 00:00:44,129
of what is the routing chair what do we

00:00:42,510 --> 00:00:46,620
mean when we say the routing tier what

00:00:44,129 --> 00:00:48,989
components make it up before then

00:00:46,620 --> 00:00:52,530
looking and taking a deep dive into two

00:00:48,989 --> 00:00:55,500
case studies in which we as members of

00:00:52,530 --> 00:00:57,360
the networking program actually worked

00:00:55,500 --> 00:01:00,629
on in terms of debugging so these are

00:00:57,360 --> 00:01:04,080
actual case studies from actual users

00:01:00,629 --> 00:01:05,820
where we really took a deep dive in to

00:01:04,080 --> 00:01:07,890
figure out what was going wrong when the

00:01:05,820 --> 00:01:09,960
problem manifests in the routing tier

00:01:07,890 --> 00:01:12,659
and then we're going to finish it off

00:01:09,960 --> 00:01:14,790
with additional debugging tips our hope

00:01:12,659 --> 00:01:17,340
is through looking at these case studies

00:01:14,790 --> 00:01:19,350
and talking about debugging tips that

00:01:17,340 --> 00:01:23,430
everyone in this room leaves feeling

00:01:19,350 --> 00:01:25,380
empowered and able to when encountering

00:01:23,430 --> 00:01:28,409
an issue that manifests in the routing

00:01:25,380 --> 00:01:30,720
tier have the tools necessary to at

00:01:28,409 --> 00:01:33,990
least start the debugging process and

00:01:30,720 --> 00:01:37,560
sort of utilize it to hopefully figure

00:01:33,990 --> 00:01:40,140
out and fix the issue at hand and with

00:01:37,560 --> 00:01:42,240
that I'm going to hand it over to ya to

00:01:40,140 --> 00:01:45,780
talk about what exactly is the routing

00:01:42,240 --> 00:01:47,729
tier cool so what is the routing tier

00:01:45,780 --> 00:01:49,829
and how does it fit into the rest of

00:01:47,729 --> 00:01:52,380
your COG foundry we want to give you a

00:01:49,829 --> 00:01:54,119
brief overview of what it is so that we

00:01:52,380 --> 00:01:57,979
can look at the case studies and defy

00:01:54,119 --> 00:02:01,110
you tips if we look at this diagram here

00:01:57,979 --> 00:02:04,619
the gray box represents the system

00:02:01,110 --> 00:02:07,890
components ocf like the api diego UAA

00:02:04,619 --> 00:02:09,959
and the routers the components in that

00:02:07,890 --> 00:02:12,510
the gray box represents make it possible

00:02:09,959 --> 00:02:15,629
for you to see f.push and run your

00:02:12,510 --> 00:02:17,610
and then all of the colorful components

00:02:15,629 --> 00:02:21,299
of our the components of the routing

00:02:17,610 --> 00:02:23,430
tier in a typical CF deployment your

00:02:21,299 --> 00:02:26,310
routing tier may consist of a load

00:02:23,430 --> 00:02:30,030
balancer H a proxies the routers and

00:02:26,310 --> 00:02:31,830
your applications so if we look at just

00:02:30,030 --> 00:02:34,080
the left side of this diagram we can see

00:02:31,830 --> 00:02:36,690
all the components that an HTTP request

00:02:34,080 --> 00:02:42,120
might hit before finally being routed to

00:02:36,690 --> 00:02:45,209
an HTTP application the load balancer is

00:02:42,120 --> 00:02:47,640
your front facing part of your CF it'll

00:02:45,209 --> 00:02:50,519
be the component that receives the

00:02:47,640 --> 00:02:54,090
request and it'll forward it on to H a

00:02:50,519 --> 00:02:56,040
proxy and the H a proxies would be

00:02:54,090 --> 00:02:58,440
configured to point to the go routers as

00:02:56,040 --> 00:03:00,299
its back ends you'd tend to use H a

00:02:58,440 --> 00:03:02,579
proxy and a deployment if you need

00:03:00,299 --> 00:03:04,859
features offered by H a proxy that are

00:03:02,579 --> 00:03:08,459
not offered by your load balancer or

00:03:04,859 --> 00:03:10,620
your CF routers for example H a proxy

00:03:08,459 --> 00:03:12,480
allows you to define access control

00:03:10,620 --> 00:03:16,220
lists to block different kinds of

00:03:12,480 --> 00:03:19,169
requests for an extra layer of security

00:03:16,220 --> 00:03:22,079
and then you have go router go router is

00:03:19,169 --> 00:03:24,180
a layer 7 HTTP router that handles

00:03:22,079 --> 00:03:26,639
connections to the application backends

00:03:24,180 --> 00:03:29,280
it has a series of handlers that will

00:03:26,639 --> 00:03:32,299
run and process your request before it

00:03:29,280 --> 00:03:35,519
then makes a new request to the backend

00:03:32,299 --> 00:03:39,269
it can also route you to other CF

00:03:35,519 --> 00:03:40,769
components like the API so now that you

00:03:39,269 --> 00:03:42,480
have an overview of what are the

00:03:40,769 --> 00:03:44,340
components that make up the writing

00:03:42,480 --> 00:03:46,680
tyrian club foundry we'll talk about

00:03:44,340 --> 00:03:49,290
issues that people have encountered when

00:03:46,680 --> 00:03:51,150
running CF in production we'll go

00:03:49,290 --> 00:03:53,480
through some case studies and show how

00:03:51,150 --> 00:03:56,549
to debug and also give some general

00:03:53,480 --> 00:03:58,230
debugging tips for common issues so I'll

00:03:56,549 --> 00:04:01,440
go back to Angela and we'll kick off the

00:03:58,230 --> 00:04:03,750
case studies great thanks nithya so

00:04:01,440 --> 00:04:07,500
we're gonna start with case study number

00:04:03,750 --> 00:04:10,889
one again these are real problems that

00:04:07,500 --> 00:04:12,150
we had to slog through debugging and so

00:04:10,889 --> 00:04:13,680
by going through them again we really

00:04:12,150 --> 00:04:17,160
want to give sort of a framework and

00:04:13,680 --> 00:04:20,519
tools in order to hopefully help you

00:04:17,160 --> 00:04:24,160
debug in the future so oftentimes as

00:04:20,519 --> 00:04:27,540
members of the networking program we get

00:04:24,160 --> 00:04:30,820
she's from users from customers

00:04:27,540 --> 00:04:33,520
oftentimes the initial statement isn't

00:04:30,820 --> 00:04:35,410
necessarily the most helpful being told

00:04:33,520 --> 00:04:38,250
something like the co router it's broken

00:04:35,410 --> 00:04:40,540
or panicking right now it's been prod

00:04:38,250 --> 00:04:42,550
it's definitely scary but it doesn't

00:04:40,540 --> 00:04:44,620
give us information on what exactly we

00:04:42,550 --> 00:04:47,380
should be debugging so the first thing

00:04:44,620 --> 00:04:50,280
that you probably want to do when

00:04:47,380 --> 00:04:52,300
debugging the routing chair is to first

00:04:50,280 --> 00:04:54,700
state the problem in your own words

00:04:52,300 --> 00:04:57,520
maybe you're the one who encountered the

00:04:54,700 --> 00:04:59,680
issue but formulating the problem in an

00:04:57,520 --> 00:05:02,710
actual problem statement makes it clear

00:04:59,680 --> 00:05:05,950
to you that what exactly you're trying

00:05:02,710 --> 00:05:08,410
to debug and if you're somebody who's

00:05:05,950 --> 00:05:10,090
getting an issue like we were as members

00:05:08,410 --> 00:05:12,070
of the networking program stating the

00:05:10,090 --> 00:05:14,650
problem make sure that we have the same

00:05:12,070 --> 00:05:17,620
understanding of what's occurring as the

00:05:14,650 --> 00:05:21,040
user who's reporting the issue right so

00:05:17,620 --> 00:05:22,570
in this case we formulated a problem

00:05:21,040 --> 00:05:24,730
statement and the problem that was

00:05:22,570 --> 00:05:26,740
occurring was that the customer had to

00:05:24,730 --> 00:05:29,290
micro services deployed in their Cloud

00:05:26,740 --> 00:05:31,870
Foundry application one micro service

00:05:29,290 --> 00:05:33,850
was externally facing and sending crud

00:05:31,870 --> 00:05:35,610
requests to another certain micro

00:05:33,850 --> 00:05:38,380
service which was acting as a back-end

00:05:35,610 --> 00:05:40,270
the micro service that was the front end

00:05:38,380 --> 00:05:41,700
would send a get request and then

00:05:40,270 --> 00:05:44,500
immediately follow up with a put request

00:05:41,700 --> 00:05:48,090
the get request would always succeed and

00:05:44,500 --> 00:05:51,460
the put would consistently error out so

00:05:48,090 --> 00:05:54,180
this is a wall of text and I know

00:05:51,460 --> 00:05:56,530
sometimes it's hard for me to actually

00:05:54,180 --> 00:05:58,210
internalize what it really means so

00:05:56,530 --> 00:06:01,510
let's visualize what the statement is

00:05:58,210 --> 00:06:03,700
actually saying so in this case we're

00:06:01,510 --> 00:06:05,830
talking about the go router and to micro

00:06:03,700 --> 00:06:07,900
services a front end back end the get

00:06:05,830 --> 00:06:09,790
request comes in hits the go router is

00:06:07,900 --> 00:06:11,410
forwarded to your front end application

00:06:09,790 --> 00:06:13,840
and the front end wants to communicate

00:06:11,410 --> 00:06:15,730
to the back end in this case it was hair

00:06:13,840 --> 00:06:17,530
pinning back out and through the go

00:06:15,730 --> 00:06:19,930
router and the get was being forwarded

00:06:17,530 --> 00:06:21,669
to the back end the get would succeed

00:06:19,930 --> 00:06:24,370
the information would be sent back and

00:06:21,669 --> 00:06:27,460
the put requests would be

00:06:24,370 --> 00:06:31,150
following and then what happened

00:06:27,460 --> 00:06:34,210
we know that the put arrow da did the go

00:06:31,150 --> 00:06:37,600
router fail to respond was the error

00:06:34,210 --> 00:06:39,760
happening at that level or was the go

00:06:37,600 --> 00:06:42,430
router successfully connecting to the

00:06:39,760 --> 00:06:47,020
back end only for the back end to fail

00:06:42,430 --> 00:06:50,290
out who really knows right this is

00:06:47,020 --> 00:06:52,720
compounded by the fact that as an if he

00:06:50,290 --> 00:06:55,870
alluded to earlier the go out of front

00:06:52,720 --> 00:06:59,170
end and back end are only some of the

00:06:55,870 --> 00:07:01,690
components in your routing tier when we

00:06:59,170 --> 00:07:02,860
zoom out from this when we look at the

00:07:01,690 --> 00:07:05,860
entirety of your cloud foundry

00:07:02,860 --> 00:07:07,630
installation you're probably going to

00:07:05,860 --> 00:07:11,110
also have an H a proxy and a load

00:07:07,630 --> 00:07:15,070
balancer so instead of just having four

00:07:11,110 --> 00:07:17,890
connections to worry about you now have

00:07:15,070 --> 00:07:19,950
a multitude more a lot more places where

00:07:17,890 --> 00:07:22,660
the failure could actually be occurring

00:07:19,950 --> 00:07:24,280
so the second thing that you want to do

00:07:22,660 --> 00:07:25,900
after stating the problem in your own

00:07:24,280 --> 00:07:28,420
words and understanding what issue

00:07:25,900 --> 00:07:29,920
you're currently trying to debug is that

00:07:28,420 --> 00:07:33,010
you want to level set on the

00:07:29,920 --> 00:07:36,430
architecture how exactly have you set up

00:07:33,010 --> 00:07:38,380
the routing tier in this case the

00:07:36,430 --> 00:07:41,140
architecture was actually a pretty

00:07:38,380 --> 00:07:43,840
straightforward setup with the load

00:07:41,140 --> 00:07:44,950
balancer forwarding to an H a proxy so

00:07:43,840 --> 00:07:47,350
the go router is and then your

00:07:44,950 --> 00:07:50,810
applications

00:07:47,350 --> 00:07:52,940
it's easy at this point to say okay I

00:07:50,810 --> 00:07:56,020
know what the topology looks like I know

00:07:52,940 --> 00:07:59,389
what the problem is let's figure out

00:07:56,020 --> 00:08:02,419
exactly which connection is failing but

00:07:59,389 --> 00:08:04,940
we're still in the information gathering

00:08:02,419 --> 00:08:07,190
phase and so in addition to level

00:08:04,940 --> 00:08:08,780
setting on the architecture I would

00:08:07,190 --> 00:08:10,370
strongly encourage at this point to also

00:08:08,780 --> 00:08:13,280
level sub the configuration and that's

00:08:10,370 --> 00:08:15,340
what we did when we say level sound the

00:08:13,280 --> 00:08:17,599
configuration when you're talking about

00:08:15,340 --> 00:08:19,970
gathering information about the load

00:08:17,599 --> 00:08:23,150
balancer at the H a proxy the go router

00:08:19,970 --> 00:08:25,550
things like the multitude of timeouts

00:08:23,150 --> 00:08:29,120
that are set on each of these levels

00:08:25,550 --> 00:08:30,919
because oftentimes those in conjunction

00:08:29,120 --> 00:08:35,419
with one another could be what's

00:08:30,919 --> 00:08:38,690
actually causing the error so after

00:08:35,419 --> 00:08:42,110
doing all of this the last step is to

00:08:38,690 --> 00:08:44,149
gather additional information you know

00:08:42,110 --> 00:08:46,399
the architecture you've been able to put

00:08:44,149 --> 00:08:49,370
all the configuration information next

00:08:46,399 --> 00:08:51,680
to each of those boxes but there's

00:08:49,370 --> 00:08:54,410
probably more that you want to gather in

00:08:51,680 --> 00:08:57,560
this case the initial problem statement

00:08:54,410 --> 00:09:00,500
we came up with was determined based on

00:08:57,560 --> 00:09:02,660
the application logs where they saw in

00:09:00,500 --> 00:09:04,940
the app logs that there is an i/o error

00:09:02,660 --> 00:09:07,850
on the put request for the front end

00:09:04,940 --> 00:09:11,390
application so from there and makes

00:09:07,850 --> 00:09:14,450
sense for us as people who are debugging

00:09:11,390 --> 00:09:15,380
this problem to then look at additional

00:09:14,450 --> 00:09:18,260
application logs

00:09:15,380 --> 00:09:20,270
see if we can map this put request to

00:09:18,260 --> 00:09:23,270
any errors in the go router logs or any

00:09:20,270 --> 00:09:25,850
errors and the H a proxy logs so you

00:09:23,270 --> 00:09:28,490
want to gather logs on basically every

00:09:25,850 --> 00:09:31,180
single component you can load balancer H

00:09:28,490 --> 00:09:35,959
a proxy go router and applications

00:09:31,180 --> 00:09:37,880
sometimes this will lead you to a root

00:09:35,959 --> 00:09:40,880
cause in our case

00:09:37,880 --> 00:09:43,420
there weren't any additional information

00:09:40,880 --> 00:09:46,120
to be found in the logs unfortunately

00:09:43,420 --> 00:09:49,270
and so this led us to the use of our

00:09:46,120 --> 00:09:52,630
first debugging tool which is TCP jumpa

00:09:49,270 --> 00:09:54,700
all right so what is TCP dump TCP dump

00:09:52,630 --> 00:09:55,540
is a packet analyzer that you can use on

00:09:54,700 --> 00:09:58,900
the command line

00:09:55,540 --> 00:10:02,710
there is no GUI so it's sometimes hard

00:09:58,900 --> 00:10:05,590
to actually parse as a human but it's

00:10:02,710 --> 00:10:08,710
really useful to run on remote servers

00:10:05,590 --> 00:10:10,720
to gather information put it into a file

00:10:08,710 --> 00:10:12,880
and then you're able to load that file

00:10:10,720 --> 00:10:14,740
into some other packet analyzer that

00:10:12,880 --> 00:10:18,750
does have a GUI it makes it easy and

00:10:14,740 --> 00:10:22,750
intuitive to read through packet traces

00:10:18,750 --> 00:10:25,270
so we are able to run just a very

00:10:22,750 --> 00:10:27,760
straightforward TCP dump on all of the

00:10:25,270 --> 00:10:29,890
VMs that we had access to so that Diego

00:10:27,760 --> 00:10:33,130
sells where your micro services were

00:10:29,890 --> 00:10:35,380
running your H a proxy and your go

00:10:33,130 --> 00:10:37,510
router and we just ran a TCP dump

00:10:35,380 --> 00:10:40,270
writing it out to a pcap file and

00:10:37,510 --> 00:10:44,050
listening on the EZ row interface for

00:10:40,270 --> 00:10:45,820
network traffic doing so we then took

00:10:44,050 --> 00:10:47,800
all of those files and we piped them

00:10:45,820 --> 00:10:49,270
into our next tool that we use for

00:10:47,800 --> 00:10:53,020
debugging in this case which is

00:10:49,270 --> 00:10:55,680
Wireshark so Wireshark is an open source

00:10:53,020 --> 00:10:58,450
packet analyzer in this case with a GUI

00:10:55,680 --> 00:11:00,880
you can use it to analyze local traffic

00:10:58,450 --> 00:11:03,970
in real time but you can also load files

00:11:00,880 --> 00:11:05,650
into it and then debug in that GUI and

00:11:03,970 --> 00:11:07,930
so that's what we did in this case and

00:11:05,650 --> 00:11:10,900
it allows filtering and inspection of

00:11:07,930 --> 00:11:12,850
packets to make it easy to follow along

00:11:10,900 --> 00:11:15,610
and hopefully debug what's actually

00:11:12,850 --> 00:11:18,760
occurring with your network traffic so

00:11:15,610 --> 00:11:22,930
what exactly does this look like so

00:11:18,760 --> 00:11:25,360
here's an example of looking at a packet

00:11:22,930 --> 00:11:27,910
trace on Wireshark if we're reading from

00:11:25,360 --> 00:11:30,070
left to right we see the packet number a

00:11:27,910 --> 00:11:33,520
timestamp and then a source and

00:11:30,070 --> 00:11:36,160
destination address we then see the type

00:11:33,520 --> 00:11:40,000
of traffic the length of the packet and

00:11:36,160 --> 00:11:43,120
then any additional information this was

00:11:40,000 --> 00:11:43,710
a trace that we filtered of a connection

00:11:43,120 --> 00:11:45,450
between

00:11:43,710 --> 00:11:47,880
the Diego cell where the front end

00:11:45,450 --> 00:11:52,170
application was running and the H a

00:11:47,880 --> 00:11:55,950
proxy if we actually zoom in on this

00:11:52,170 --> 00:11:58,860
packet trace we see that partway through

00:11:55,950 --> 00:12:02,400
this connection there was a fin a packet

00:11:58,860 --> 00:12:04,800
sent from the H a proxy the Diego cell

00:12:02,400 --> 00:12:07,260
basically the H a proxy was saying I

00:12:04,800 --> 00:12:11,250
want to close this network connection to

00:12:07,260 --> 00:12:14,820
the Diego cell and that's interesting

00:12:11,250 --> 00:12:18,590
because immediately after what we see is

00:12:14,820 --> 00:12:21,540
the Diego cell sending a mired of data

00:12:18,590 --> 00:12:24,420
to the H a proxy so it looks like the

00:12:21,540 --> 00:12:26,280
Diego cell isn't respecting the close

00:12:24,420 --> 00:12:29,190
packet what you would normally expect to

00:12:26,280 --> 00:12:30,510
happen is after a Finnish Packer a

00:12:29,190 --> 00:12:33,960
closed packet being sent in one

00:12:30,510 --> 00:12:35,970
direction the other the server on the

00:12:33,960 --> 00:12:37,560
other end should be sending a fin a

00:12:35,970 --> 00:12:39,720
it back to say oh I see you want to

00:12:37,560 --> 00:12:41,910
close the connection I'm going to close

00:12:39,720 --> 00:12:43,850
this connection too but in this case we

00:12:41,910 --> 00:12:47,250
didn't see that instead we see data

00:12:43,850 --> 00:12:50,730
likely the put request being forwarded

00:12:47,250 --> 00:12:52,640
and then a bunch of red for the reset so

00:12:50,730 --> 00:12:55,230
we see that the issue at hand is

00:12:52,640 --> 00:12:57,930
connection v between the front end and

00:12:55,230 --> 00:13:01,920
AC proxy this is where our problem is

00:12:57,930 --> 00:13:03,480
occurring and so when you're debugging

00:13:01,920 --> 00:13:04,260
you're usually not debugging in

00:13:03,480 --> 00:13:06,420
isolation

00:13:04,260 --> 00:13:08,670
you're usually not the only one

00:13:06,420 --> 00:13:10,530
debugging as well and so you're going to

00:13:08,670 --> 00:13:12,600
have information coming in from multiple

00:13:10,530 --> 00:13:15,210
sources you're not living in a bubble

00:13:12,600 --> 00:13:17,490
and so at the time that we had this

00:13:15,210 --> 00:13:19,680
realization we got an interesting tidbit

00:13:17,490 --> 00:13:21,960
from the user who reported in this issue

00:13:19,680 --> 00:13:23,880
and they noted that disabling keep

00:13:21,960 --> 00:13:28,710
allies from the go router to the back

00:13:23,880 --> 00:13:30,210
end fix the problem and we were like

00:13:28,710 --> 00:13:33,210
what like what does that even mean

00:13:30,210 --> 00:13:35,340
so disabling keep allies from the go

00:13:33,210 --> 00:13:39,150
router to back-end was a change made to

00:13:35,340 --> 00:13:42,870
the red arrows here and somehow this was

00:13:39,150 --> 00:13:45,600
fixing the problem like what how is

00:13:42,870 --> 00:13:46,970
something on connection four and seven

00:13:45,600 --> 00:13:50,900
these keep alive

00:13:46,970 --> 00:13:53,690
fixing the error on connection five

00:13:50,900 --> 00:13:55,970
so based on this we decided to come up

00:13:53,690 --> 00:13:59,089
with a hypothesis and plan of attack and

00:13:55,970 --> 00:14:02,000
our hypothesis was that some value was

00:13:59,089 --> 00:14:03,710
being propagated from the go router to

00:14:02,000 --> 00:14:05,570
the backends that would make the

00:14:03,710 --> 00:14:09,160
connection between the H a proxy and the

00:14:05,570 --> 00:14:13,400
front-end application fail now

00:14:09,160 --> 00:14:17,990
that is to say again that these right

00:14:13,400 --> 00:14:21,589
here these run arrows here or somehow

00:14:17,990 --> 00:14:23,900
propagating information so that the

00:14:21,589 --> 00:14:28,250
arrow here oh my god

00:14:23,900 --> 00:14:31,010
I'm just dying so that the connection

00:14:28,250 --> 00:14:35,570
would be failing there and so disabling

00:14:31,010 --> 00:14:38,140
it made it better but hypotheses don't

00:14:35,570 --> 00:14:40,220
just live a load they also live with

00:14:38,140 --> 00:14:43,130
coming into a way to either prove or

00:14:40,220 --> 00:14:45,050
disprove them and going back to all the

00:14:43,130 --> 00:14:47,240
information we had a hand going back to

00:14:45,050 --> 00:14:50,810
this Wireshark output we couldn't come

00:14:47,240 --> 00:14:53,240
up with a reason on why a value being

00:14:50,810 --> 00:14:56,029
propagated would either cause this error

00:14:53,240 --> 00:14:58,670
or fix this error and so we decided

00:14:56,029 --> 00:15:01,310
instead to really try to view this as a

00:14:58,670 --> 00:15:03,740
red herring and again go back to the

00:15:01,310 --> 00:15:06,589
initial problem statement we had let's

00:15:03,740 --> 00:15:08,000
not look at what was a solution but

00:15:06,589 --> 00:15:12,470
let's instead really focus on

00:15:08,000 --> 00:15:13,940
identifying the root cause so doing so

00:15:12,470 --> 00:15:16,250
we decided to come up with a new

00:15:13,940 --> 00:15:18,350
hypothesis we were repeating the step

00:15:16,250 --> 00:15:20,839
four and come up with one where

00:15:18,350 --> 00:15:24,080
hopefully we could valet or invalidate

00:15:20,839 --> 00:15:25,910
it and so our hypothesis number two was

00:15:24,080 --> 00:15:27,980
that something about the HTTP client

00:15:25,910 --> 00:15:30,860
what's causing the closed packet to be

00:15:27,980 --> 00:15:35,260
ignored this really narrowed in on the

00:15:30,860 --> 00:15:36,890
connection that we were seeing fail and

00:15:35,260 --> 00:15:38,900
didn't really take into consideration

00:15:36,890 --> 00:15:42,890
any other components so we really

00:15:38,900 --> 00:15:44,779
narrowed narrow down it and actually

00:15:42,890 --> 00:15:46,250
when we came up with a hypothesis we got

00:15:44,779 --> 00:15:49,670
another interesting tidbit of

00:15:46,250 --> 00:15:53,000
information that was sort of supporting

00:15:49,670 --> 00:15:55,010
our new hypothesis that if we waited

00:15:53,000 --> 00:15:57,890
five seconds between sending the get

00:15:55,010 --> 00:15:59,140
request and the put request the put

00:15:57,890 --> 00:16:01,180
would always succeed

00:15:59,140 --> 00:16:03,089
but when the put request was happening

00:16:01,180 --> 00:16:08,649
immediately after it would always fail

00:16:03,089 --> 00:16:10,000
and so it seemed pretty close in line to

00:16:08,649 --> 00:16:12,970
our hypothesis that there was something

00:16:10,000 --> 00:16:14,740
about the httpclient so we actually

00:16:12,970 --> 00:16:18,610
looked at the application source code

00:16:14,740 --> 00:16:22,810
for the front edge it was a spring boot

00:16:18,610 --> 00:16:24,760
up it used HTTP you get the default HTTP

00:16:22,810 --> 00:16:28,720
client that you would assume does the

00:16:24,760 --> 00:16:32,829
right thing but we took the slack to ask

00:16:28,720 --> 00:16:35,200
our local spring experts and we took to

00:16:32,829 --> 00:16:39,250
Stack Overflow and what we found is that

00:16:35,200 --> 00:16:41,470
the default HTTP client actually doesn't

00:16:39,250 --> 00:16:45,790
respect closed packets from the server

00:16:41,470 --> 00:16:47,920
it will just ignore it and so the in

00:16:45,790 --> 00:16:52,470
conjunction with that the default

00:16:47,920 --> 00:16:55,209
keepalive timeout is five seconds and

00:16:52,470 --> 00:16:57,970
again going back to the configuration

00:16:55,209 --> 00:17:00,550
information we gathered we saw the HTTP

00:16:57,970 --> 00:17:03,430
teeth a lifetime out on the HJ proxy was

00:17:00,550 --> 00:17:05,880
half a second so the H a proxy after

00:17:03,430 --> 00:17:09,579
half a second would close the connection

00:17:05,880 --> 00:17:11,799
but the default HTTP client wouldn't

00:17:09,579 --> 00:17:14,559
respect it and for the next five seconds

00:17:11,799 --> 00:17:18,299
would try to reuse that connection which

00:17:14,559 --> 00:17:18,299
was causing the put to err RL

00:17:18,510 --> 00:17:25,189
salt so what lessons can we actually

00:17:22,650 --> 00:17:28,679
learn from this debugging exploration

00:17:25,189 --> 00:17:30,809
one of the main things we took away was

00:17:28,679 --> 00:17:32,820
that clients can have configuration -

00:17:30,809 --> 00:17:35,520
it's so easy to think about the routing

00:17:32,820 --> 00:17:37,410
tier as just the system components but

00:17:35,520 --> 00:17:39,059
your application is an important part of

00:17:37,410 --> 00:17:42,240
that routing tear with configuration

00:17:39,059 --> 00:17:44,280
that is set and so when you're talking

00:17:42,240 --> 00:17:46,290
about configuration together should

00:17:44,280 --> 00:17:50,340
always gather information about the HTTP

00:17:46,290 --> 00:17:51,870
client as well additionally we also

00:17:50,340 --> 00:17:54,090
learned that we should probably be

00:17:51,870 --> 00:17:57,720
ignoring the red herrings and short-term

00:17:54,090 --> 00:18:00,929
fixes the HTTP keeper lives from go

00:17:57,720 --> 00:18:03,150
routers the backends wasn't helping us

00:18:00,929 --> 00:18:05,970
with our root cause analysis it sort of

00:18:03,150 --> 00:18:07,950
led us astray and only after we figured

00:18:05,970 --> 00:18:09,780
out what the root cause actually was did

00:18:07,950 --> 00:18:13,590
we then go back and try and figure out

00:18:09,780 --> 00:18:15,390
how studying this value could fix the

00:18:13,590 --> 00:18:17,400
problem in the short term which was

00:18:15,390 --> 00:18:20,760
another interesting exploration in and

00:18:17,400 --> 00:18:23,429
of itself but with that I'm gonna hand

00:18:20,760 --> 00:18:30,169
it over to Nana to talk about the second

00:18:23,429 --> 00:18:30,169
very interesting use case cool

00:18:30,280 --> 00:18:35,809
cool so let's read the problem statement

00:18:33,020 --> 00:18:38,240
for this one the API becomes

00:18:35,809 --> 00:18:41,420
unresponsive when trying to see if login

00:18:38,240 --> 00:18:43,490
other requests succeed and some of the

00:18:41,420 --> 00:18:44,440
go router instances have high memory

00:18:43,490 --> 00:18:48,140
usage

00:18:44,440 --> 00:18:49,610
so first let's visualize this even

00:18:48,140 --> 00:18:52,130
though we were told that the go router

00:18:49,610 --> 00:18:54,170
has high memory usage let's remember

00:18:52,130 --> 00:18:56,150
that there are several components that

00:18:54,170 --> 00:18:58,850
the CF login request needs to travel

00:18:56,150 --> 00:19:00,470
through in this case here's a diagram of

00:18:58,850 --> 00:19:03,470
what the deployment architecture looks

00:19:00,470 --> 00:19:05,390
like and request travel into the load

00:19:03,470 --> 00:19:07,400
balancer through one of the H a proxy

00:19:05,390 --> 00:19:09,890
instances into one of the go routers

00:19:07,400 --> 00:19:11,600
before finally making it to the API and

00:19:09,890 --> 00:19:13,549
then the response needs to go back

00:19:11,600 --> 00:19:16,850
through all these components to get a

00:19:13,549 --> 00:19:19,730
response to the client so now we can

00:19:16,850 --> 00:19:21,890
start to gather information we'll try to

00:19:19,730 --> 00:19:27,049
start broad and gather clues and then

00:19:21,890 --> 00:19:29,000
zoom in as we eliminate possibilities so

00:19:27,049 --> 00:19:31,730
here's a couple of places that you can

00:19:29,000 --> 00:19:34,940
start to look for information roughly

00:19:31,730 --> 00:19:37,210
ordered from broad to more focused as

00:19:34,940 --> 00:19:39,169
you move from top to bottom

00:19:37,210 --> 00:19:42,049
investigating various sources of

00:19:39,169 --> 00:19:44,570
information you may find some indicators

00:19:42,049 --> 00:19:47,330
like specific error messages that allow

00:19:44,570 --> 00:19:49,580
you to skip relevant components right

00:19:47,330 --> 00:19:52,010
now the only information we have relates

00:19:49,580 --> 00:19:54,290
to high memory usage and CF log and

00:19:52,010 --> 00:19:58,130
requests hanging so let's start broad

00:19:54,290 --> 00:20:01,549
and look at the VM vitals so here's a

00:19:58,130 --> 00:20:04,100
snippet of some VM vitals this is all

00:20:01,549 --> 00:20:05,630
stuff that you can get from bosch that

00:20:04,100 --> 00:20:10,100
can tell you about the health of the VMS

00:20:05,630 --> 00:20:12,679
and this snippet happens to be only from

00:20:10,100 --> 00:20:14,600
router instance zero and this is one of

00:20:12,679 --> 00:20:17,690
the VMS that was experiencing the

00:20:14,600 --> 00:20:19,730
problem and the problem statement so you

00:20:17,690 --> 00:20:23,120
can observe here that the memory usage

00:20:19,730 --> 00:20:27,799
percentage on router 0 was pretty high

00:20:23,120 --> 00:20:30,020
but the CPU usage is fairly low another

00:20:27,799 --> 00:20:32,450
interesting thing on this router is that

00:20:30,020 --> 00:20:35,299
the memory swap percentage is actually

00:20:32,450 --> 00:20:37,799
increasing over time

00:20:35,299 --> 00:20:39,240
so based on what we've gathered so far

00:20:37,799 --> 00:20:41,250
we see high memory on this router

00:20:39,240 --> 00:20:44,039
instance and that the memory swap

00:20:41,250 --> 00:20:46,590
percentage increases over time so one

00:20:44,039 --> 00:20:48,059
hypothesis could be the go router isn't

00:20:46,590 --> 00:20:49,890
skilled enough and there aren't enough

00:20:48,059 --> 00:20:53,340
instances of go router to handle the

00:20:49,890 --> 00:20:55,559
load but we also learned from poking

00:20:53,340 --> 00:20:57,600
around on these VMS that even though we

00:20:55,559 --> 00:21:00,179
see these symptoms occur on router

00:20:57,600 --> 00:21:02,400
instance 0 this wasn't the case for all

00:21:00,179 --> 00:21:04,710
the router instances it's just happening

00:21:02,400 --> 00:21:06,480
on a few of them in fact after

00:21:04,710 --> 00:21:08,640
restarting the routers with the high

00:21:06,480 --> 00:21:12,029
memory usage the problem would return

00:21:08,640 --> 00:21:13,529
often on another router vm and the

00:21:12,029 --> 00:21:16,049
routers that did experience these

00:21:13,529 --> 00:21:18,570
symptoms also still reported that they

00:21:16,049 --> 00:21:20,610
were healthy so there's no indication to

00:21:18,570 --> 00:21:24,149
the load bouncer that it should redirect

00:21:20,610 --> 00:21:26,970
traffic elsewhere so there's a couple of

00:21:24,149 --> 00:21:28,770
things that we've seen so far that tell

00:21:26,970 --> 00:21:30,210
us that horizontally scaling the gore

00:21:28,770 --> 00:21:33,480
adder isn't really going to help us with

00:21:30,210 --> 00:21:35,760
this issue one the CPU usage is fairly

00:21:33,480 --> 00:21:38,399
low so the router isn't overwhelmed with

00:21:35,760 --> 00:21:41,640
large amounts of work in fact a majority

00:21:38,399 --> 00:21:43,740
of the cpu time was spent waiting and

00:21:41,640 --> 00:21:45,179
the load balancer held check requests

00:21:43,740 --> 00:21:48,210
themselves are just fine

00:21:45,179 --> 00:21:50,370
and that indicates that only some of the

00:21:48,210 --> 00:21:52,350
requests are bottlenecking so adding

00:21:50,370 --> 00:21:54,210
more go router instances won't help

00:21:52,350 --> 00:21:58,559
since traffic would still be going to

00:21:54,210 --> 00:22:00,570
the problematic VMs so now when we

00:21:58,559 --> 00:22:02,370
should try to zoom in further look at

00:22:00,570 --> 00:22:05,970
some lugs see if we can find anything

00:22:02,370 --> 00:22:09,870
useful looking at the go router error

00:22:05,970 --> 00:22:12,179
logs one error sticks out the gerado is

00:22:09,870 --> 00:22:13,740
having some trouble making connections

00:22:12,179 --> 00:22:17,010
to other components because it's hitting

00:22:13,740 --> 00:22:18,720
the maximum number of open files so now

00:22:17,010 --> 00:22:22,309
we have something to start digging into

00:22:18,720 --> 00:22:25,919
what is causing us to hit this limit is

00:22:22,309 --> 00:22:29,159
limit itself to low or which connections

00:22:25,919 --> 00:22:32,070
both incoming and outgoing are causing

00:22:29,159 --> 00:22:34,230
us to hit this limit let's look at the

00:22:32,070 --> 00:22:37,480
diagram again and try to help pinpoint

00:22:34,230 --> 00:22:40,070
where we think the problem is happening

00:22:37,480 --> 00:22:42,289
so this is an architecture diagram of

00:22:40,070 --> 00:22:44,659
possible request paths in this

00:22:42,289 --> 00:22:46,610
deployment open files on the gutter

00:22:44,659 --> 00:22:48,710
could be being caused by any one of

00:22:46,610 --> 00:22:51,110
these three red arrows H a proxy to go

00:22:48,710 --> 00:22:54,200
router go router to up backends or gore

00:22:51,110 --> 00:22:56,120
adder to other system components so

00:22:54,200 --> 00:22:58,190
turns out there could actually be one

00:22:56,120 --> 00:22:59,960
more source of open files on the go

00:22:58,190 --> 00:23:01,820
router which is if you have a route

00:22:59,960 --> 00:23:04,370
service and your request is going to a

00:23:01,820 --> 00:23:08,059
route service the go router as part of

00:23:04,370 --> 00:23:10,610
handling that request will send a

00:23:08,059 --> 00:23:13,070
request to the route service and wait

00:23:10,610 --> 00:23:15,950
for that request to be processed before

00:23:13,070 --> 00:23:18,649
sending a response back to H a proxy so

00:23:15,950 --> 00:23:21,950
H a proxy to go router that arrow will

00:23:18,649 --> 00:23:24,980
say open while processing is happening

00:23:21,950 --> 00:23:27,049
in a row service so we want to try to

00:23:24,980 --> 00:23:30,679
zoom into these red arrows where we can

00:23:27,049 --> 00:23:32,059
and gather some more information the

00:23:30,679 --> 00:23:34,309
information that we were able to get

00:23:32,059 --> 00:23:36,559
about these connections is that the

00:23:34,309 --> 00:23:38,899
number of connections between H a proxy

00:23:36,559 --> 00:23:42,700
and the go router are growing but not H

00:23:38,899 --> 00:23:44,889
a proxy not go router to the back ends

00:23:42,700 --> 00:23:47,240
that's pretty helpful

00:23:44,889 --> 00:23:49,190
it helps us narrow down to this

00:23:47,240 --> 00:23:52,369
highlighted arrow as the side that's

00:23:49,190 --> 00:23:54,049
growing so now we can start to reason

00:23:52,369 --> 00:23:56,330
about hypotheses for wide that

00:23:54,049 --> 00:23:59,869
highlighted arrow might grow so we just

00:23:56,330 --> 00:24:02,419
talked about connections from the H a

00:23:59,869 --> 00:24:04,669
proxy to grow router stay open while go

00:24:02,419 --> 00:24:06,740
router is processing that request and in

00:24:04,669 --> 00:24:08,480
the case of a route service that

00:24:06,740 --> 00:24:10,369
connection will stay open while go

00:24:08,480 --> 00:24:13,190
router sends and has the route service

00:24:10,369 --> 00:24:15,080
processing the request so a slow or

00:24:13,190 --> 00:24:17,269
misbehaving route service could cause

00:24:15,080 --> 00:24:19,850
connections from H a proxy to go router

00:24:17,269 --> 00:24:23,600
to stay open and that could be a reason

00:24:19,850 --> 00:24:27,260
why this arrow is growing so misbehaving

00:24:23,600 --> 00:24:29,490
route service is our next hypothesis

00:24:27,260 --> 00:24:31,260
we ruled this out by killing route

00:24:29,490 --> 00:24:32,880
services that were slow to respond and

00:24:31,260 --> 00:24:36,900
seeing the problem of unclosed

00:24:32,880 --> 00:24:39,780
connections persist on the go router so

00:24:36,900 --> 00:24:41,730
if you recall from before it did seem

00:24:39,780 --> 00:24:42,390
like only some of the requests were

00:24:41,730 --> 00:24:44,430
bottlenecking

00:24:42,390 --> 00:24:46,320
and we've ruled out route services at

00:24:44,430 --> 00:24:48,240
this point we need more information

00:24:46,320 --> 00:24:49,860
about what requests are actually going

00:24:48,240 --> 00:24:52,890
into the go router and the Gardiners

00:24:49,860 --> 00:24:54,930
access logs can give us that so from the

00:24:52,890 --> 00:24:58,470
access logs we actually saw many 404s

00:24:54,930 --> 00:25:00,390
but also that the 404 request time was

00:24:58,470 --> 00:25:04,170
much higher than that of other response

00:25:00,390 --> 00:25:05,820
codes so our new hypothesis is that 404s

00:25:04,170 --> 00:25:08,520
are causing the connections to stay

00:25:05,820 --> 00:25:12,690
alive and we have a lot of outside

00:25:08,520 --> 00:25:15,090
information at this point but all of our

00:25:12,690 --> 00:25:17,790
observations so far seem to be a symptom

00:25:15,090 --> 00:25:19,620
of like a deeper problem so what we want

00:25:17,790 --> 00:25:21,930
to do is really dig into the 404s and

00:25:19,620 --> 00:25:25,320
see how go router itself is dealing with

00:25:21,930 --> 00:25:27,210
them so to help us dig into what the go

00:25:25,320 --> 00:25:31,010
router is doing we can profile the code

00:25:27,210 --> 00:25:33,570
a good tool for doing that is pea broth

00:25:31,010 --> 00:25:36,300
beef broth allows you to collect and

00:25:33,570 --> 00:25:38,400
visualize profiles like CPU profiles

00:25:36,300 --> 00:25:41,520
heat profiles goroutine profiles and

00:25:38,400 --> 00:25:43,560
traces go router has a peep rough

00:25:41,520 --> 00:25:46,560
endpoint for collecting profile and

00:25:43,560 --> 00:25:48,660
trace data and flame graphs are a way

00:25:46,560 --> 00:25:52,020
that you can visualize this profile data

00:25:48,660 --> 00:25:54,630
that contains stack traces that's

00:25:52,020 --> 00:25:57,630
collected by tools like peep rough so

00:25:54,630 --> 00:26:00,150
we'll look at a flame graph of the

00:25:57,630 --> 00:26:03,960
profile data for go router while the

00:26:00,150 --> 00:26:06,240
issue was occurring so this is that I

00:26:03,960 --> 00:26:08,490
know you can't read the text but we'll

00:26:06,240 --> 00:26:11,720
talk about it and first let's talk about

00:26:08,490 --> 00:26:11,720
how to read a flame graph

00:26:11,790 --> 00:26:18,120
so the text and all of the little boxes

00:26:14,910 --> 00:26:20,790
are function names when using CPU

00:26:18,120 --> 00:26:23,430
profiling the profiler interrupts the

00:26:20,790 --> 00:26:25,950
program execution at specified intervals

00:26:23,430 --> 00:26:29,220
and logs the state of the programs call

00:26:25,950 --> 00:26:32,850
stack the length of the boxes tells you

00:26:29,220 --> 00:26:35,610
how many samples during the profile were

00:26:32,850 --> 00:26:38,010
spent in this function while this

00:26:35,610 --> 00:26:41,490
function is executing so you can very

00:26:38,010 --> 00:26:43,770
very roughly think of it as time spent

00:26:41,490 --> 00:26:45,690
in this function and generally you want

00:26:43,770 --> 00:26:47,610
to look at the wide boxes and the last

00:26:45,690 --> 00:26:49,380
couple of calls in the stack in these

00:26:47,610 --> 00:26:52,680
wide boxes which are towards the bottom

00:26:49,380 --> 00:26:55,080
so there's a lot of samples spent in

00:26:52,680 --> 00:26:56,880
this stack an interesting part about

00:26:55,080 --> 00:26:59,220
this stack is that this is a stack that

00:26:56,880 --> 00:27:00,680
calls to the logger so there's a

00:26:59,220 --> 00:27:03,330
bottleneck in logging

00:27:00,680 --> 00:27:05,430
we know that when profiling this code

00:27:03,330 --> 00:27:08,570
there were a lot of 404 x' and that the

00:27:05,430 --> 00:27:11,670
404 is likely resulted in logging and

00:27:08,570 --> 00:27:14,550
further down the stack we see that

00:27:11,670 --> 00:27:17,610
during the write for the log it's also

00:27:14,550 --> 00:27:19,650
calling to a locking function we finally

00:27:17,610 --> 00:27:23,610
found our bottleneck which is lock

00:27:19,650 --> 00:27:25,170
contention when writing to the disk so

00:27:23,610 --> 00:27:27,390
it turns out writing to the disk was a

00:27:25,170 --> 00:27:30,300
bottleneck and lock contention when

00:27:27,390 --> 00:27:33,330
writing to the disk could be caused by

00:27:30,300 --> 00:27:36,210
any combo of many writes to the disk or

00:27:33,330 --> 00:27:39,120
an actual slow disk and the router

00:27:36,210 --> 00:27:41,460
attempts to log route unknown for all

00:27:39,120 --> 00:27:43,530
404s which is happening very frequently

00:27:41,460 --> 00:27:45,930
during this case when the problem been

00:27:43,530 --> 00:27:47,640
infested so this could be what's causing

00:27:45,930 --> 00:27:50,850
the connections to persist because

00:27:47,640 --> 00:27:53,580
they're stuck waiting to write and this

00:27:50,850 --> 00:27:56,580
was verified by isolating the gerado VM

00:27:53,580 --> 00:27:58,410
a go router BM to an empty hypervisor

00:27:56,580 --> 00:28:00,270
with no other bands and seeing the

00:27:58,410 --> 00:28:04,430
problem go away because there is no more

00:28:00,270 --> 00:28:07,440
storage contention yay

00:28:04,430 --> 00:28:10,500
so here's what we can take away from

00:28:07,440 --> 00:28:12,810
this case there were a lot of VM metrics

00:28:10,500 --> 00:28:14,250
so we looked at at the beginning but

00:28:12,810 --> 00:28:16,740
none of them really had to do with disk

00:28:14,250 --> 00:28:19,260
so maybe looking at i/o stat or

00:28:16,740 --> 00:28:20,850
something on the problem VM earlier on

00:28:19,260 --> 00:28:21,880
could have tipped us off in the right

00:28:20,850 --> 00:28:25,720
direction

00:28:21,880 --> 00:28:27,940
earlier but also it's useful to start

00:28:25,720 --> 00:28:30,070
broad and gather as much information as

00:28:27,940 --> 00:28:32,200
you can at each step because then when

00:28:30,070 --> 00:28:35,710
you look at more granular data from like

00:28:32,200 --> 00:28:37,780
tools like P prof you're able to have

00:28:35,710 --> 00:28:40,240
some context when looking at that we

00:28:37,780 --> 00:28:41,800
knew a lot about what was going on in

00:28:40,240 --> 00:28:43,960
the system by the time we looked at the

00:28:41,800 --> 00:28:47,500
flame graph so it's like easy to tell

00:28:43,960 --> 00:28:49,210
what was going on cool so now we'll go

00:28:47,500 --> 00:28:53,950
back to Angela for some more general

00:28:49,210 --> 00:28:57,250
debugging tips thanks ed yeah that's so

00:28:53,950 --> 00:28:59,260
fascinating so now we're just going to

00:28:57,250 --> 00:29:02,380
go into some brief general debugging

00:28:59,260 --> 00:29:04,690
tips one of the first things we like to

00:29:02,380 --> 00:29:06,220
look at when we were debugging issues in

00:29:04,690 --> 00:29:08,530
the writing tier is time out

00:29:06,220 --> 00:29:10,690
configurations in general you want to

00:29:08,530 --> 00:29:13,510
make sure that your timeouts are highest

00:29:10,690 --> 00:29:15,550
on your load balancer and then lowest as

00:29:13,510 --> 00:29:17,350
you go further down so you know load

00:29:15,550 --> 00:29:20,350
balancer each a proxy go about our

00:29:17,350 --> 00:29:22,840
application high to low and you want to

00:29:20,350 --> 00:29:25,060
do this to make sure that you don't have

00:29:22,840 --> 00:29:27,040
any component further down in your

00:29:25,060 --> 00:29:29,890
network stack closing connections

00:29:27,040 --> 00:29:33,850
prematurely and causing problems to

00:29:29,890 --> 00:29:37,750
manifest the next thing we want to talk

00:29:33,850 --> 00:29:40,120
about in is logging so while this may

00:29:37,750 --> 00:29:41,620
seem sort of silly one thing we always

00:29:40,120 --> 00:29:44,260
like to highlight is your application

00:29:41,620 --> 00:29:46,330
logs have timestamps too so if you see

00:29:44,260 --> 00:29:48,550
an error that manifests in your

00:29:46,330 --> 00:29:50,380
application log you can take that

00:29:48,550 --> 00:29:52,690
timestamp and look at the corresponding

00:29:50,380 --> 00:29:54,130
logs and your go router or H a proxy and

00:29:52,690 --> 00:29:57,520
see what was happening at around the

00:29:54,130 --> 00:29:58,900
same time we also want to note that we

00:29:57,520 --> 00:30:01,120
recently added a support that for

00:29:58,900 --> 00:30:03,970
endpoint failure requests that occur in

00:30:01,120 --> 00:30:06,700
the access log that you can actually map

00:30:03,970 --> 00:30:09,130
them to a log message in your go router

00:30:06,700 --> 00:30:11,560
logs as well by using the beef cap

00:30:09,130 --> 00:30:14,680
request header ID so you can gather more

00:30:11,560 --> 00:30:16,660
information about what exactly is

00:30:14,680 --> 00:30:18,660
happening with the endpoint failure by

00:30:16,660 --> 00:30:22,500
mapping the two logs

00:30:18,660 --> 00:30:25,890
and lastly some additional other tips

00:30:22,500 --> 00:30:27,450
are that when in doubt you can always

00:30:25,890 --> 00:30:29,610
pull your load balancer out of the

00:30:27,450 --> 00:30:33,600
equation you can go on to your network

00:30:29,610 --> 00:30:35,640
and ping the go routers directly this

00:30:33,600 --> 00:30:38,340
also helps you if you're worried about

00:30:35,640 --> 00:30:40,830
individual routers by being by allowing

00:30:38,340 --> 00:30:43,410
you to test routers individually and

00:30:40,830 --> 00:30:46,310
last but not least we also recommend

00:30:43,410 --> 00:30:48,720
checking your certificates on each level

00:30:46,310 --> 00:30:51,120
certificates can be pretty hard to

00:30:48,720 --> 00:30:52,680
manage and so one in doubt you might

00:30:51,120 --> 00:30:56,880
want to make sure that everyone's

00:30:52,680 --> 00:30:58,890
expecting the right certificates and

00:30:56,880 --> 00:31:00,330
with that we have some references if you

00:30:58,890 --> 00:31:01,710
want to look further into any of the

00:31:00,330 --> 00:31:03,750
tools we talked about in both case

00:31:01,710 --> 00:31:05,430
studies our slides aren't up yet but we

00:31:03,750 --> 00:31:08,100
will definitely be posting them and you

00:31:05,430 --> 00:31:10,620
can click on the links there where you

00:31:08,100 --> 00:31:12,840
definitely ran over time as we expected

00:31:10,620 --> 00:31:14,100
so thank you so much we'll be outside

00:31:12,840 --> 00:31:16,410
the room if you want to talk to us

00:31:14,100 --> 00:31:18,680
further but otherwise have a great rest

00:31:16,410 --> 00:31:21,160
of summit

00:31:18,680 --> 00:31:21,160

YouTube URL: https://www.youtube.com/watch?v=cKqXYFIA3Is


