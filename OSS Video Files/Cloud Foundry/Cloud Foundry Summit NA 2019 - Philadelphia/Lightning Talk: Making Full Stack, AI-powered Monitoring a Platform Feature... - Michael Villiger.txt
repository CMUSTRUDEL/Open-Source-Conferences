Title: Lightning Talk: Making Full Stack, AI-powered Monitoring a Platform Feature... - Michael Villiger
Publication date: 2019-04-11
Playlist: Cloud Foundry Summit NA 2019 - Philadelphia
Description: 
	Lightning Talk: Making Full Stack, AI-powered Monitoring a Platform Feature with BOSH - Michael Villiger, Dynatrace

Cloud-native applications create a unique set of challenges when it comes to monitoring. In an environment with thousands of processes supporting hundreds of applications, monitoring a limited number of them (5 to 10 percent according to analysts) is insufficient for ensuring optimal performance. Learn how the Dynatrace OneAgent deployed by BOSH makes AI powered monitoring with automated root cause analysis a platform feature with three easy steps.

About Michael Villiger
Mike has spent the last 20 years occupying various engineering positions surrounding web-scale performance, architecture, and operations before joining Dynatrace in 2014. At Dynatrace Mike has specialized in assisting customers implementing Application Performance Management technologies and processes in the worlds of Public/Private Cloud, DevOps, SRE, and Platform-as-a-Service (PaaS). Mike has constructed many of the technical integrations customers and partners utilize to deploy and utilize Dynatrace on their platforms.

https://www.cloudfoundry.org/
Captions: 
	00:00:00,000 --> 00:00:06,180
all right so I'm Mike villager I'm with

00:00:03,210 --> 00:00:08,010
Donna trace so I'm giving a brief little

00:00:06,180 --> 00:00:09,450
lightning talk here I've done a couple

00:00:08,010 --> 00:00:12,240
of these before so they're always a good

00:00:09,450 --> 00:00:14,570
time and of course the entire audience

00:00:12,240 --> 00:00:20,160
is my teammates so that's always good

00:00:14,570 --> 00:00:21,990
all right so all right so monitoring is

00:00:20,160 --> 00:00:23,490
something that's required to drive your

00:00:21,990 --> 00:00:25,350
business forward with speed right so

00:00:23,490 --> 00:00:28,619
it's a key crucial part of your digital

00:00:25,350 --> 00:00:30,390
transformation right but in the industry

00:00:28,619 --> 00:00:32,759
we feel that traditional monitoring is a

00:00:30,390 --> 00:00:35,250
dead end right so let's go to a couple

00:00:32,759 --> 00:00:38,129
of reasons why why that might be right

00:00:35,250 --> 00:00:40,260
so your typical organization is really

00:00:38,129 --> 00:00:43,140
monitoring no more than about 5% of the

00:00:40,260 --> 00:00:45,629
apps in their environment this is real

00:00:43,140 --> 00:00:48,780
actual information that we've gotten

00:00:45,629 --> 00:00:51,690
from our analysts partners and one of

00:00:48,780 --> 00:00:53,250
the reasons why that you know they they

00:00:51,690 --> 00:00:55,289
haven't rolled out for more than that is

00:00:53,250 --> 00:00:58,170
because usually the effort is too high

00:00:55,289 --> 00:01:01,620
to implement the monitoring or the

00:00:58,170 --> 00:01:03,180
tooling itself doesn't scale right and

00:01:01,620 --> 00:01:05,970
then as we start to expand those same

00:01:03,180 --> 00:01:07,590
concepts into you know micro services

00:01:05,970 --> 00:01:10,439
and in cloud native app development

00:01:07,590 --> 00:01:12,810
right that the same you know existing

00:01:10,439 --> 00:01:14,509
problems are happening here as well our

00:01:12,810 --> 00:01:16,799
traditional techniques don't scale

00:01:14,509 --> 00:01:18,720
installing agents into every container

00:01:16,799 --> 00:01:20,220
by you know manipulating file systems

00:01:18,720 --> 00:01:23,850
and so on it's not a very scalable

00:01:20,220 --> 00:01:25,950
process right and you know things like

00:01:23,850 --> 00:01:27,450
just container metrics don't actually

00:01:25,950 --> 00:01:31,250
represent what's happening to the

00:01:27,450 --> 00:01:33,420
application right so you know enter

00:01:31,250 --> 00:01:35,579
software intelligence for pivotal Cloud

00:01:33,420 --> 00:01:39,119
Foundry right so we're we're gonna have

00:01:35,579 --> 00:01:43,579
full stack visibility into every

00:01:39,119 --> 00:01:47,220
component of the of the cloud platform

00:01:43,579 --> 00:01:50,310
right and then I've got this fancy

00:01:47,220 --> 00:01:52,079
snazzy animation here that I won't let

00:01:50,310 --> 00:01:55,799
die even though it's not necessarily

00:01:52,079 --> 00:01:57,810
brand compliant so you know what's

00:01:55,799 --> 00:02:01,170
actually happening here is is you know

00:01:57,810 --> 00:02:02,610
we've chosen a different implementation

00:02:01,170 --> 00:02:04,770
methodology right so we're actually

00:02:02,610 --> 00:02:06,540
utilizing a Bosch add-on to roll out our

00:02:04,770 --> 00:02:10,289
agents which actually gives us bottom up

00:02:06,540 --> 00:02:13,800
visibility into the environment and that

00:02:10,289 --> 00:02:16,590
is implemented via

00:02:13,800 --> 00:02:18,630
Bausch right now right so we actually

00:02:16,590 --> 00:02:20,670
start by you know uploading our release

00:02:18,630 --> 00:02:23,610
you're gonna give that release a runtime

00:02:20,670 --> 00:02:25,440
config and then you're going to execute

00:02:23,610 --> 00:02:27,150
a deployment and then the agent is gonna

00:02:25,440 --> 00:02:30,030
magically appear on all of your boss

00:02:27,150 --> 00:02:35,460
managed VMs whether they be cloud

00:02:30,030 --> 00:02:37,020
foundry or kubernetes via Kubo so you

00:02:35,460 --> 00:02:38,310
know while this is going on right so

00:02:37,020 --> 00:02:39,840
anybody who's actually done a boss

00:02:38,310 --> 00:02:41,460
deployment understands that boss

00:02:39,840 --> 00:02:44,520
deployments take a little bit of time

00:02:41,460 --> 00:02:47,730
usually enough time to maybe have two if

00:02:44,520 --> 00:02:49,440
not three entire pots of coffee so what

00:02:47,730 --> 00:02:51,630
can you do while your boss deployment is

00:02:49,440 --> 00:02:53,400
is rolling out you can take a look at a

00:02:51,630 --> 00:02:55,410
couple of blogs obviously dynaTrace blog

00:02:53,400 --> 00:02:56,910
is there I actually really like the

00:02:55,410 --> 00:02:59,190
Starck and Wayne blog there's some

00:02:56,910 --> 00:03:01,650
really great articles there take a look

00:02:59,190 --> 00:03:04,050
at some some new tech news right

00:03:01,650 --> 00:03:05,760
we can also take some time to start

00:03:04,050 --> 00:03:07,370
creating some dashboards for these hosts

00:03:05,760 --> 00:03:09,660
as they start to connect to dynaTrace

00:03:07,370 --> 00:03:11,100
you know taking a look at some of the

00:03:09,660 --> 00:03:12,450
system metrics that are coming out of

00:03:11,100 --> 00:03:14,670
there taking a look at some of the

00:03:12,450 --> 00:03:16,170
greates platform metrics that we're

00:03:14,670 --> 00:03:18,780
delivering around things like the go

00:03:16,170 --> 00:03:20,850
router and in auctioneer you know start

00:03:18,780 --> 00:03:22,770
keeping track of HTTP requests coming in

00:03:20,850 --> 00:03:24,840
to your go router take a look at failed

00:03:22,770 --> 00:03:28,650
application placements and and things

00:03:24,840 --> 00:03:31,140
like that but dashboarding is a

00:03:28,650 --> 00:03:32,700
technique that came out of the 70s right

00:03:31,140 --> 00:03:33,510
maybe we should start to think about

00:03:32,700 --> 00:03:36,600
doing something else

00:03:33,510 --> 00:03:40,410
right let's start to look at automated

00:03:36,600 --> 00:03:42,959
root cause analysis right so let's take

00:03:40,410 --> 00:03:44,790
a look at a particular problem pattern

00:03:42,959 --> 00:03:46,650
that I'm actually having in my cloud

00:03:44,790 --> 00:03:48,780
foundry environment like literally like

00:03:46,650 --> 00:03:50,880
today I actually just updated this deck

00:03:48,780 --> 00:03:53,070
about 15 minutes ago to include this

00:03:50,880 --> 00:03:55,530
particular problem right so we have a

00:03:53,070 --> 00:03:57,690
response time degradation in an order

00:03:55,530 --> 00:04:00,600
service we can see that it's impacted

00:03:57,690 --> 00:04:04,890
five people in 196 service calls right

00:04:00,600 --> 00:04:07,680
and then we see a root cause that it

00:04:04,890 --> 00:04:09,510
these things are not advancing right we

00:04:07,680 --> 00:04:12,480
see that there's a root cause associated

00:04:09,510 --> 00:04:13,650
with a deployment event right so it's it

00:04:12,480 --> 00:04:14,820
kind of eliminates a lot of the

00:04:13,650 --> 00:04:16,650
guesswork that that you might have

00:04:14,820 --> 00:04:18,810
traditionally and these are techniques

00:04:16,650 --> 00:04:21,380
that are available to you because you've

00:04:18,810 --> 00:04:23,970
basically woven you're monitoring fabric

00:04:21,380 --> 00:04:25,470
into the platform itself right so it's

00:04:23,970 --> 00:04:27,330
something that that comes along for the

00:04:25,470 --> 00:04:28,740
ride with everything

00:04:27,330 --> 00:04:30,300
it's deployed to the platform all right

00:04:28,740 --> 00:04:32,400
let's take a look at another example

00:04:30,300 --> 00:04:33,810
here right so this is something that

00:04:32,400 --> 00:04:35,069
happened to us awhile back when we were

00:04:33,810 --> 00:04:37,560
first rolling out one of our first

00:04:35,069 --> 00:04:40,620
coffee foundry clusters we had a cloud

00:04:37,560 --> 00:04:43,379
controller problem CPU was pegged right

00:04:40,620 --> 00:04:44,969
and it was a technology that we didn't

00:04:43,379 --> 00:04:46,500
even support with by code injection but

00:04:44,969 --> 00:04:48,840
we were able to use log analytics to

00:04:46,500 --> 00:04:50,699
find out that our sequel server for the

00:04:48,840 --> 00:04:52,949
call controller had gone away which was

00:04:50,699 --> 00:04:54,379
causing the CPU to be picked another

00:04:52,949 --> 00:04:57,349
situation that I keep on seeing

00:04:54,379 --> 00:04:59,310
surprisingly it seems like

00:04:57,349 --> 00:05:02,490
network-attached storage is still a

00:04:59,310 --> 00:05:04,349
problem storage is not a solved problem

00:05:02,490 --> 00:05:06,389
right now so slow disk is something that

00:05:04,349 --> 00:05:09,030
will frequently alarm and and generate

00:05:06,389 --> 00:05:10,590
alerts buts really important here is by

00:05:09,030 --> 00:05:12,319
having that full stack visibility you

00:05:10,590 --> 00:05:14,759
have the ability to prove a negative

00:05:12,319 --> 00:05:18,330
right so we know that this particular

00:05:14,759 --> 00:05:20,400
problem had no user impact right so we

00:05:18,330 --> 00:05:21,779
know that no users were impacted so if

00:05:20,400 --> 00:05:23,789
we're sitting there eating dinner with

00:05:21,779 --> 00:05:25,639
our families and we get this alert from

00:05:23,789 --> 00:05:28,800
Donna trace we can see immediately that

00:05:25,639 --> 00:05:30,779
that there was no user impact right

00:05:28,800 --> 00:05:33,389
which kind of leads me to one of my

00:05:30,779 --> 00:05:35,310
final slides here this is kind of like a

00:05:33,389 --> 00:05:36,900
couple of closing thoughts here right

00:05:35,310 --> 00:05:39,120
environments that are as complex as

00:05:36,900 --> 00:05:41,069
something like Cloud Foundry you really

00:05:39,120 --> 00:05:42,839
need to take a different approach it's

00:05:41,069 --> 00:05:43,949
really great to embrace the tooling

00:05:42,839 --> 00:05:46,469
that's available to us in these

00:05:43,949 --> 00:05:48,240
platforms so Bosch has made delivery of

00:05:46,469 --> 00:05:51,330
agents into every VM of a large

00:05:48,240 --> 00:05:54,599
environment like this almost trivial and

00:05:51,330 --> 00:05:56,129
then probably the punchiest thing is if

00:05:54,599 --> 00:06:00,180
a tree falls in the forest and doesn't

00:05:56,129 --> 00:06:02,580
hit a user does anyone care right my

00:06:00,180 --> 00:06:07,529
hypothesis is that the answer to that is

00:06:02,580 --> 00:06:08,909
no a couple of little links here this

00:06:07,529 --> 00:06:11,370
doesn't really matter unless somebody's

00:06:08,909 --> 00:06:14,430
going to take a picture of this and then

00:06:11,370 --> 00:06:16,500
we have another fancy pretty marketing

00:06:14,430 --> 00:06:18,089
slide to close things off so hopefully

00:06:16,500 --> 00:06:20,580
everybody likes it and finds it

00:06:18,089 --> 00:06:21,530
beautiful because I do so all right

00:06:20,580 --> 00:06:26,150
thanks everybody

00:06:21,530 --> 00:06:26,150

YouTube URL: https://www.youtube.com/watch?v=gow8cE94rkA


