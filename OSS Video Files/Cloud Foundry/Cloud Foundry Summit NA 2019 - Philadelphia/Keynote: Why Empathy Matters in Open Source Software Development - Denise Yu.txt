Title: Keynote: Why Empathy Matters in Open Source Software Development - Denise Yu
Publication date: 2019-04-11
Playlist: Cloud Foundry Summit NA 2019 - Philadelphia
Description: 
	Keynote: Why Empathy Matters in Open Source Software Development - Denise Yu, Software Engineer, Pivotal

A chat between Abby Kearns and Denise Yu about the importance of thinking about empathy early in the software development lifecycle by centering the end users of open source products.

About Abby Kearns
With nearly twenty years in the tech world, Abby Kearns is a true veteran of the industry. Her lengthy career has spanned product marketing, product management and consulting across Fortune 500 companies and startups alike. As Executive Director of Cloud Foundry Foundation, Abby helms the ecosystem of developers, users and applications running on Cloud Foundry, and works closely with the Board to drive the Foundationâ€™s vision and grow the open source project. Prior to Cloud Foundry Foundation, Abby focused on Pivotal Cloud Foundry as part of the Product Management team at Pivotal. She spent eight years at Verizon where she led Product Management and Product Marketing teams dedicated to the early days of cloud services. In her free time, Abby enjoys indulging in food and wine, and spending time with her husband and son.

About Denise Yu
Denise is a Senior Software Engineer at Pivotal Cloud Foundry (PCF) in Toronto. In her time at Pivotal she has worked on a variety of open source and enterprise products, served briefly as the Product Manager of Pivotal's On-Demand Service Broker SDK, then moved across an ocean to the Toronto office where she now works on enabling high-confidence release engineering for the PCF program, as part of the Master Pipeline team. Denise has previously delivered talks at conferences and meetups throughout Europe, North America and Asia on topics ranging from continuous delivery to functional programming to cultural linguistics. She enjoys learning about distributed systems, release engineering, and low-level Linux kernel programming. In her spare time she creates illustrations that break down complex technical topics at deniseyu.io/art, and yes you may use these images with attribution in your decks!

https://www.cloudfoundry.org/
Captions: 
	00:00:00,030 --> 00:00:06,029
so how many of you attended the

00:00:01,319 --> 00:00:08,370
unconference last night yes thank you

00:00:06,029 --> 00:00:12,740
for putting it on I unfortunately wasn't

00:00:08,370 --> 00:00:15,809
able to go but I heard it was amazing

00:00:12,740 --> 00:00:17,789
often times a lot of talks make our way

00:00:15,809 --> 00:00:21,890
to the unconference first and then make

00:00:17,789 --> 00:00:24,029
our way to the mainstage and I wanted to

00:00:21,890 --> 00:00:25,470
one give a special shout out to the

00:00:24,029 --> 00:00:28,050
people that put that on because it is a

00:00:25,470 --> 00:00:31,830
hard and thankless task but it means a

00:00:28,050 --> 00:00:35,460
lot to the community so thank you but

00:00:31,830 --> 00:00:41,670
also it's a it's a great segue into to

00:00:35,460 --> 00:00:43,170
my next guest because Denise originally

00:00:41,670 --> 00:00:45,899
suggested this for the open space

00:00:43,170 --> 00:00:47,910
conversation but one of the things we

00:00:45,899 --> 00:00:49,590
start experimenting with our summit in

00:00:47,910 --> 00:00:52,680
Basel was bringing a lot more of those

00:00:49,590 --> 00:00:55,170
conversations front and center there's

00:00:52,680 --> 00:00:56,699
opportunity to have them in sessions and

00:00:55,170 --> 00:00:58,500
in the unconferences but we'd like to

00:00:56,699 --> 00:01:03,120
bring a lot more of them front stage as

00:00:58,500 --> 00:01:06,810
well and so I wanted to really make

00:01:03,120 --> 00:01:09,979
space on the main stage on the first day

00:01:06,810 --> 00:01:12,869
to spend some time talking about empathy

00:01:09,979 --> 00:01:14,939
and so I'm going to ask Denise you

00:01:12,869 --> 00:01:17,520
senior software engineer at pivotal and

00:01:14,939 --> 00:01:20,189
also the designer of that wonderful

00:01:17,520 --> 00:01:22,860
video that you saw she wrote that all by

00:01:20,189 --> 00:01:24,540
hand to come on stage and join me to

00:01:22,860 --> 00:01:28,019
talk about empathy come on up Denise

00:01:24,540 --> 00:01:28,019
[Applause]

00:01:29,560 --> 00:01:32,700
[Music]

00:01:34,040 --> 00:01:39,590
yeah and also one more like applause for

00:01:38,090 --> 00:01:43,270
Denise because she actually drew that

00:01:39,590 --> 00:01:43,270
whole video you saw my hand

00:01:45,710 --> 00:01:52,940
I continue to be amazed by that whole

00:01:48,800 --> 00:01:54,350
video so I know a lot of people are

00:01:52,940 --> 00:01:55,940
gonna why are they talking about empathy

00:01:54,350 --> 00:01:59,750
on mainstage like this is a cloud

00:01:55,940 --> 00:02:01,340
conference come on but you know I

00:01:59,750 --> 00:02:02,720
couldn't think of anyone better than to

00:02:01,340 --> 00:02:04,040
have Denise joining me to talk a little

00:02:02,720 --> 00:02:05,660
bit about empathy it's something that he

00:02:04,040 --> 00:02:09,020
spends a lot of time talking about and

00:02:05,660 --> 00:02:12,560
writing about has your piece posted yet

00:02:09,020 --> 00:02:14,540
on this yes the new stack published an

00:02:12,560 --> 00:02:16,700
article that Caitlyn helped me write

00:02:14,540 --> 00:02:17,989
called why empathy matters more than you

00:02:16,700 --> 00:02:19,730
think it does for open source software

00:02:17,989 --> 00:02:21,590
so if you're like I'm really curious

00:02:19,730 --> 00:02:24,980
about it but they look that up after

00:02:21,590 --> 00:02:26,260
this as well but so Denise we spend a

00:02:24,980 --> 00:02:28,370
lot of time talking about empathy

00:02:26,260 --> 00:02:31,160
particularly in agile it comes up a lot

00:02:28,370 --> 00:02:33,200
right but I feel like it's a broad vague

00:02:31,160 --> 00:02:36,290
term that means a lot of things to a lot

00:02:33,200 --> 00:02:39,020
of people and oftentimes I feel like it

00:02:36,290 --> 00:02:40,910
might be misused so maybe you could walk

00:02:39,020 --> 00:02:42,830
us through what do you mean by empathy

00:02:40,910 --> 00:02:43,520
and how does that actually relate to why

00:02:42,830 --> 00:02:47,030
we're all here

00:02:43,520 --> 00:02:49,820
sure so I think the classic definition

00:02:47,030 --> 00:02:53,060
of empathy is trying to see the world in

00:02:49,820 --> 00:02:55,580
someone else's shoes or from someone

00:02:53,060 --> 00:02:58,670
else's eyes but I think that when it

00:02:55,580 --> 00:03:01,370
comes to practicing empathy as product

00:02:58,670 --> 00:03:03,739
developers as contributors on product

00:03:01,370 --> 00:03:06,380
teams I think it has a lot more to do

00:03:03,739 --> 00:03:09,320
with how you apply those insights rather

00:03:06,380 --> 00:03:11,510
than simply having a certain mindset so

00:03:09,320 --> 00:03:13,820
in other words I think that empathy is

00:03:11,510 --> 00:03:17,060
actually a set of verbs rather than a

00:03:13,820 --> 00:03:18,799
set of nouns like lab right on that like

00:03:17,060 --> 00:03:22,030
I let's talk through the verb side of

00:03:18,799 --> 00:03:23,959
this sure so I think empathy is

00:03:22,030 --> 00:03:26,209
acknowledging that we all have a set of

00:03:23,959 --> 00:03:28,340
biases and beliefs about who our end

00:03:26,209 --> 00:03:31,220
users are about who our collaborators

00:03:28,340 --> 00:03:33,200
are and being empathetic means leaving

00:03:31,220 --> 00:03:35,450
your office and maybe leaving your

00:03:33,200 --> 00:03:39,230
comfort zone and trying to validate or

00:03:35,450 --> 00:03:41,660
invalidate those beliefs it means also

00:03:39,230 --> 00:03:43,820
may be thinking about interviewing

00:03:41,660 --> 00:03:45,560
people speaking to people who are not

00:03:43,820 --> 00:03:47,390
only in current users of the things that

00:03:45,560 --> 00:03:48,680
you're doing but also maybe people who

00:03:47,390 --> 00:03:50,600
have stopped using the product that

00:03:48,680 --> 00:03:54,320
you're building I think a lot of the

00:03:50,600 --> 00:03:57,799
time we fall into a fallacy called

00:03:54,320 --> 00:03:59,329
survivorship bias why don't you like

00:03:57,799 --> 00:03:59,660
define that I feel like it's something

00:03:59,329 --> 00:04:02,150
we

00:03:59,660 --> 00:04:03,920
that in confirmation bias a lot maybe if

00:04:02,150 --> 00:04:06,680
you can help elaborate on what that

00:04:03,920 --> 00:04:09,410
means absolutely survivorship bias and

00:04:06,680 --> 00:04:12,200
confirmation bias are really similar I

00:04:09,410 --> 00:04:14,060
think it's easiest to explain what it is

00:04:12,200 --> 00:04:18,440
through a quick story

00:04:14,060 --> 00:04:21,380
so in World War two there were a lot of

00:04:18,440 --> 00:04:23,120
planes were being shot down and the

00:04:21,380 --> 00:04:24,560
Allied army started thinking what are

00:04:23,120 --> 00:04:26,510
some ways that we can reinforce our

00:04:24,560 --> 00:04:28,430
planes and make sure that more of our

00:04:26,510 --> 00:04:30,260
pilots come home so they collect the

00:04:28,430 --> 00:04:31,880
data they looked at where planes are

00:04:30,260 --> 00:04:33,320
being shot it was generally in the

00:04:31,880 --> 00:04:35,720
middle of the plane the edges of the

00:04:33,320 --> 00:04:37,220
wings and on the tail and at first

00:04:35,720 --> 00:04:38,720
people thought alright that's where

00:04:37,220 --> 00:04:41,420
planes are being shot let's reinforce

00:04:38,720 --> 00:04:43,850
those parts but then a statistician

00:04:41,420 --> 00:04:45,830
pointed out hang on those are the planes

00:04:43,850 --> 00:04:47,870
that are coming home we don't need to

00:04:45,830 --> 00:04:49,910
reinforce those parts of the plane we

00:04:47,870 --> 00:04:52,280
need to reinforce the parts on the

00:04:49,910 --> 00:04:54,710
planes that are getting shot down so

00:04:52,280 --> 00:04:55,910
survivorship bias means only looking at

00:04:54,710 --> 00:04:58,190
the people who are still in your

00:04:55,910 --> 00:04:59,960
community who are still actively using

00:04:58,190 --> 00:05:02,030
the things that you're building rather

00:04:59,960 --> 00:05:03,620
than getting further into the field and

00:05:02,030 --> 00:05:06,200
trying to understand why it is that

00:05:03,620 --> 00:05:08,210
people might leave that's actually

00:05:06,200 --> 00:05:09,950
really insightful and I think that can

00:05:08,210 --> 00:05:11,630
be applied to both product and

00:05:09,950 --> 00:05:13,880
open-source and maybe we we kind of

00:05:11,630 --> 00:05:15,800
explore them separately because I don't

00:05:13,880 --> 00:05:17,600
want to be overly broad brush with this

00:05:15,800 --> 00:05:19,820
but you know as we you know we're did

00:05:17,600 --> 00:05:22,669
open source conference how do you think

00:05:19,820 --> 00:05:23,990
that relates to open source I think when

00:05:22,669 --> 00:05:28,310
it comes to open source software

00:05:23,990 --> 00:05:30,350
development being empathetic can involve

00:05:28,310 --> 00:05:32,600
many different groups of people so being

00:05:30,350 --> 00:05:33,800
empathetic can mean contributors being

00:05:32,600 --> 00:05:36,919
empathetic towards maintainer x'

00:05:33,800 --> 00:05:38,720
maintainer x' trying to see the eyes see

00:05:36,919 --> 00:05:41,060
the perspective of contributors and

00:05:38,720 --> 00:05:42,470
maybe everyone trying to work a little

00:05:41,060 --> 00:05:45,260
bit harder to see the world through the

00:05:42,470 --> 00:05:47,840
eyes of new users and new and

00:05:45,260 --> 00:05:51,290
experienced end users I think in

00:05:47,840 --> 00:05:52,730
practice this means trying to understand

00:05:51,290 --> 00:05:54,860
why it is that someone is making a

00:05:52,730 --> 00:05:56,660
feature request trying to make space to

00:05:54,860 --> 00:05:58,930
listen to everyone rather than those who

00:05:56,660 --> 00:06:02,150
just happen to be shouting the loudest

00:05:58,930 --> 00:06:04,550
so not listening to the loudest voices

00:06:02,150 --> 00:06:07,550
in their room exactly I know it's a

00:06:04,550 --> 00:06:08,030
thing we have to all practice it's been

00:06:07,550 --> 00:06:10,070
hours

00:06:08,030 --> 00:06:11,539
yes an open source in particular because

00:06:10,070 --> 00:06:12,800
that's into who we focus on are the

00:06:11,539 --> 00:06:15,590
people that are the most vocal

00:06:12,800 --> 00:06:18,560
or the most active on slack or Twitter

00:06:15,590 --> 00:06:20,240
and I think we need to figure out ways

00:06:18,560 --> 00:06:21,620
to make space for people that are a

00:06:20,240 --> 00:06:24,860
little quieter and maybe not as

00:06:21,620 --> 00:06:27,169
comfortable now translating that to an

00:06:24,860 --> 00:06:28,190
actual product standpoint and this I

00:06:27,169 --> 00:06:31,340
think she plays a role in your

00:06:28,190 --> 00:06:33,800
day-to-day current job as a developer

00:06:31,340 --> 00:06:37,940
how do you apply that to product

00:06:33,800 --> 00:06:41,449
development I think we apply it to

00:06:37,940 --> 00:06:43,569
product development by trying to make it

00:06:41,449 --> 00:06:45,949
clear that there are no stupid questions

00:06:43,569 --> 00:06:49,000
trying to make the new contributor

00:06:45,949 --> 00:06:51,860
experience as welcoming as possible and

00:06:49,000 --> 00:06:53,719
treating almost treating our

00:06:51,860 --> 00:06:55,370
relationship with the community like a

00:06:53,719 --> 00:06:59,449
product I know that sounds kind of weird

00:06:55,370 --> 00:07:01,400
but it means your you need to have some

00:06:59,449 --> 00:07:03,469
idea of what a productive working

00:07:01,400 --> 00:07:07,550
relationship looks like and identify

00:07:03,469 --> 00:07:09,319
steps towards iterating towards that and

00:07:07,550 --> 00:07:10,909
when do you often visualize what that

00:07:09,319 --> 00:07:12,590
looks like ahead of time or do you think

00:07:10,909 --> 00:07:15,020
it's a way that you kind of navigate

00:07:12,590 --> 00:07:18,110
there through conversation I think it

00:07:15,020 --> 00:07:19,490
definitely is emergent I think it would

00:07:18,110 --> 00:07:21,979
be very hard to define what that looks

00:07:19,490 --> 00:07:23,569
like ahead of time but that could be one

00:07:21,979 --> 00:07:24,979
of the biases you apply if you're like

00:07:23,569 --> 00:07:25,460
okay I think I know how this is going to

00:07:24,979 --> 00:07:28,340
end

00:07:25,460 --> 00:07:32,150
yep absolutely and not following along

00:07:28,340 --> 00:07:34,430
the the path one of the things that you

00:07:32,150 --> 00:07:35,750
know I really thought was powerful that

00:07:34,430 --> 00:07:38,389
you mentioned to me actually before we

00:07:35,750 --> 00:07:40,669
came on stage was you know how you

00:07:38,389 --> 00:07:41,690
design with empathy if you can elaborate

00:07:40,669 --> 00:07:43,039
a little bit mechs I thought that was a

00:07:41,690 --> 00:07:44,900
really powerful way to think about

00:07:43,039 --> 00:07:46,940
design and the way we approach any of

00:07:44,900 --> 00:07:51,349
the the products or the projects we work

00:07:46,940 --> 00:07:53,690
on yeah so designing with empathy is

00:07:51,349 --> 00:07:55,219
actually something that the product

00:07:53,690 --> 00:07:56,509
community and the design community have

00:07:55,219 --> 00:07:59,270
been thinking about for a really really

00:07:56,509 --> 00:08:01,099
long time I recently spent some time

00:07:59,270 --> 00:08:03,349
learning about what human centered

00:08:01,099 --> 00:08:04,819
design is shout out to all the designers

00:08:03,349 --> 00:08:07,520
at pivotal that have helped me learn

00:08:04,819 --> 00:08:10,340
about this and I think essentially it's

00:08:07,520 --> 00:08:12,349
about centering the user and trying to

00:08:10,340 --> 00:08:15,319
figure out how the end user is likely to

00:08:12,349 --> 00:08:18,199
use your product not how you know the

00:08:15,319 --> 00:08:19,490
user is meant to use your product so

00:08:18,199 --> 00:08:20,990
I'll give it I'll give an example of

00:08:19,490 --> 00:08:24,229
actually applying human centered design

00:08:20,990 --> 00:08:25,169
towards a real-world scenario so Ellie

00:08:24,229 --> 00:08:28,319
our era

00:08:25,169 --> 00:08:29,909
aly Blanken r2r a product manager and

00:08:28,319 --> 00:08:31,770
designer at pivotal who have recently

00:08:29,909 --> 00:08:36,389
started applying human centered design

00:08:31,770 --> 00:08:38,669
towards non technology related scenarios

00:08:36,389 --> 00:08:42,719
so they've been working on an initiative

00:08:38,669 --> 00:08:45,089
where they try to improve public health

00:08:42,719 --> 00:08:48,420
outcomes in refugee camps

00:08:45,089 --> 00:08:50,040
oh and interestingly when they conducted

00:08:48,420 --> 00:08:52,589
interviews with the women and children

00:08:50,040 --> 00:08:55,920
in these camps around why you know what

00:08:52,589 --> 00:08:57,779
would it take for you to use the toilets

00:08:55,920 --> 00:09:00,209
of the Red Cross's building they learned

00:08:57,779 --> 00:09:02,730
that when there is a big light near the

00:09:00,209 --> 00:09:04,350
toilets men congregated around them and

00:09:02,730 --> 00:09:06,300
it made it unsafe for women and children

00:09:04,350 --> 00:09:08,490
to use them at night so that's an

00:09:06,300 --> 00:09:10,920
example of an insight that you can't get

00:09:08,490 --> 00:09:13,170
without getting out there into the field

00:09:10,920 --> 00:09:15,180
and conducting one-to-one interviews and

00:09:13,170 --> 00:09:16,199
of course putting in the work to make

00:09:15,180 --> 00:09:18,959
sure that people feel psychologically

00:09:16,199 --> 00:09:21,930
safe enough to tell you these things

00:09:18,959 --> 00:09:23,459
yeah because on paper that sounds super

00:09:21,930 --> 00:09:26,639
logical of course we would put a light

00:09:23,459 --> 00:09:28,470
in front of it but it actually acts in

00:09:26,639 --> 00:09:31,670
the opposite way so that's really that's

00:09:28,470 --> 00:09:34,649
a fascinating way of thinking about it I

00:09:31,670 --> 00:09:37,199
do want to end on the other note you

00:09:34,649 --> 00:09:39,240
said don't let me forget to say this but

00:09:37,199 --> 00:09:40,050
I do think it's important we've been

00:09:39,240 --> 00:09:41,430
talking a lot about digital

00:09:40,050 --> 00:09:43,290
transformation we've been talking about

00:09:41,430 --> 00:09:45,779
how the culture change and the power

00:09:43,290 --> 00:09:48,300
around that but I really wanted to end

00:09:45,779 --> 00:09:50,579
on your points you really wanted to make

00:09:48,300 --> 00:09:50,970
about empathy and how it's no longer an

00:09:50,579 --> 00:09:54,630
option

00:09:50,970 --> 00:09:56,220
yeah so there are lots of different ways

00:09:54,630 --> 00:09:59,130
to get to the same end goal today

00:09:56,220 --> 00:10:00,959
whether users adopt your product and

00:09:59,130 --> 00:10:03,600
continue to use your product is going to

00:10:00,959 --> 00:10:05,459
be down to whether they can actually use

00:10:03,600 --> 00:10:08,399
it effectively whether your product

00:10:05,459 --> 00:10:11,699
meets usability criteria so the only way

00:10:08,399 --> 00:10:13,380
to build towards that is to design with

00:10:11,699 --> 00:10:16,560
empathy and to get out there and talk to

00:10:13,380 --> 00:10:19,019
your end user constantly so empathy is

00:10:16,560 --> 00:10:22,709
not empathy is no longer a nice-to-have

00:10:19,019 --> 00:10:26,430
in business it's something that you that

00:10:22,709 --> 00:10:28,170
is critical for transforming the way

00:10:26,430 --> 00:10:31,050
that people interact with technology and

00:10:28,170 --> 00:10:33,360
your product and your product which I

00:10:31,050 --> 00:10:35,310
think that's important piece well thank

00:10:33,360 --> 00:10:36,710
you so much for joining us here Denise I

00:10:35,310 --> 00:10:38,090
for one

00:10:36,710 --> 00:10:41,060
was excited to be able to talk about

00:10:38,090 --> 00:10:44,240
this on the main stage again I'm gonna

00:10:41,060 --> 00:10:45,380
do a plug for your PC published but the

00:10:44,240 --> 00:10:48,440
new stack I thought it was a really

00:10:45,380 --> 00:10:50,270
really powerful analogy around how we

00:10:48,440 --> 00:10:51,830
can actually in real world examples of

00:10:50,270 --> 00:10:53,750
how we can pull empathy into our

00:10:51,830 --> 00:10:57,610
everyday so thank you so much for

00:10:53,750 --> 00:10:57,610

YouTube URL: https://www.youtube.com/watch?v=s9c3lgqbip8


