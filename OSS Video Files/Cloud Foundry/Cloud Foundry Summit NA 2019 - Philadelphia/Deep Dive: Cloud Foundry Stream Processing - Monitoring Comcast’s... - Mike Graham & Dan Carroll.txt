Title: Deep Dive: Cloud Foundry Stream Processing - Monitoring Comcast’s... - Mike Graham & Dan Carroll
Publication date: 2019-04-11
Playlist: Cloud Foundry Summit NA 2019 - Philadelphia
Description: 
	Deep Dive: Cloud Foundry Stream Processing - Monitoring Comcast’s Outside Plant - Charles (Mike) Graham & Dan Carroll, Comcast Cable Communications

Comcast manages over 2 million miles of fiber and coax, and over 40 million in home devices. This “outside plant” is subject to adverse conditions from severe weather to power grid outages to construction-related disruptions. Maintaining the health of this large and important infrastructure requires a distributed, scalable, reliable and fast information system capable of real-time processing and rapid analysis and response. Using Cloud Foundry, Comcast is able to deploy a Java Spring Kafka Streams application for monitoring, problem analysis, metrics reporting and action response for the outside plant.

In this talk, you’ll learn how to create and deploy a Cloud Foundry Kafka Streams application. You will see how to scale the application using partitioning and how to maintain high speed, durable instance-based state. You'll see how easy it is to connect several of these applications (or micro services) together using asynchronous messaging. You will see how to build a REST API using Kafka Streams “interactive queries” that can be used to retrieve the data in the state stores. We will explore the deployment and monitoring mechanisms used to deliver this system as a set of independently deployed components.

About Dan Carroll
Dan Carroll is a senior software engineer at Comcast. Dan is currently developing a Cloud Foundry Java Spring streaming-based application to monitor and analyze Comcast’s outside plant. He has many years of experience in the telecommunications field with different technologies and stacks. Kafka and stream processing have proved to be a very powerful tool for Comcast and Dan is looking forward to delivering more applications using the platform.

About Charles (Mike) Graham
Mike Graham has been building spatial information systems since 1987 on a variety of technology stacks.

He has architected dynamic, extensible platforms for ExxonMobil, Chevron, IHS Markit, and other large enterprises.

Recently, in work for Comcast, Mike has been architecting a Cloud Foundry deployed Java Spring streaming platform based on Kafka and Kafka Streams for monitoring and analyzing Comcast's entire outside plant including over 40 million in home devices and 2 million miles of coax and fiber.

Mike has seen the introduction of significant technologies in his career and believes that container-deployed streaming apps are a game changer.

https://www.cloudfoundry.org/
Captions: 
	00:00:00,030 --> 00:00:06,270
good afternoon my name is Mike Graham

00:00:03,300 --> 00:00:09,269
this is Dan Carroll we work for Comcast

00:00:06,270 --> 00:00:11,550
in the Denver offices dan and I have

00:00:09,269 --> 00:00:14,330
been working to modernize the monitoring

00:00:11,550 --> 00:00:17,039
system for Comcast's outside plant

00:00:14,330 --> 00:00:19,439
leveraging cloud foundry and kafka

00:00:17,039 --> 00:00:23,010
streams and we're excited to share those

00:00:19,439 --> 00:00:24,449
experiences with you today we'll provide

00:00:23,010 --> 00:00:27,660
you with a roadmap showing you how to

00:00:24,449 --> 00:00:30,510
create a spring java app how to add the

00:00:27,660 --> 00:00:32,820
cloud found kafka streams libraries to

00:00:30,510 --> 00:00:35,399
it and how to deploy the application to

00:00:32,820 --> 00:00:38,610
Cloud Foundry application and container

00:00:35,399 --> 00:00:42,770
runtimes with the goal of building a

00:00:38,610 --> 00:00:47,250
system that is distributed scalable

00:00:42,770 --> 00:00:49,460
reliable performant and reactive to

00:00:47,250 --> 00:00:52,649
events in real time

00:00:49,460 --> 00:00:56,760
we'll discuss recipes for creating and

00:00:52,649 --> 00:01:01,190
deploying apps for scaling apps for

00:00:56,760 --> 00:01:04,019
managing app state for modularizing apps

00:01:01,190 --> 00:01:07,500
using streams to build micro services

00:01:04,019 --> 00:01:11,729
and for implementing rest services that

00:01:07,500 --> 00:01:13,950
expose a streaming app state by adopting

00:01:11,729 --> 00:01:15,810
these technologies you'll see the same

00:01:13,950 --> 00:01:19,140
benefits that we've seen including

00:01:15,810 --> 00:01:21,420
reduced developer friction resulting in

00:01:19,140 --> 00:01:24,420
increased innovation and significantly

00:01:21,420 --> 00:01:26,900
reduce time to market immediate

00:01:24,420 --> 00:01:29,729
successes from initial laptop based

00:01:26,900 --> 00:01:33,590
development spikes to full-scale

00:01:29,729 --> 00:01:36,000
deployments application modularity

00:01:33,590 --> 00:01:39,630
resulting in smaller team sizes and

00:01:36,000 --> 00:01:43,829
deployments the ability to easily scale

00:01:39,630 --> 00:01:46,110
applications horizontally an application

00:01:43,829 --> 00:01:48,270
deployment portability using the Cloud

00:01:46,110 --> 00:01:52,020
Foundry runtimes which helps prevent

00:01:48,270 --> 00:01:54,479
cloud vendor lock-in we'll talk about

00:01:52,020 --> 00:01:56,430
these things in more detail but first

00:01:54,479 --> 00:02:00,119
let's take a quick look at the high

00:01:56,430 --> 00:02:03,409
level application domain we monitor

00:02:00,119 --> 00:02:06,329
Comcast's outside plant but what is it

00:02:03,409 --> 00:02:08,640
outside plant is the broadband

00:02:06,329 --> 00:02:11,370
infrastructure that connects Comcast's

00:02:08,640 --> 00:02:13,140
business and residential customers to

00:02:11,370 --> 00:02:14,090
manage data centers and onto the

00:02:13,140 --> 00:02:17,880
Internet

00:02:14,090 --> 00:02:21,270
this includes customer devices cabling

00:02:17,880 --> 00:02:23,280
at the customer location drop lines from

00:02:21,270 --> 00:02:25,500
the customer location to shared fibre

00:02:23,280 --> 00:02:28,500
and coax cabling that's either buried or

00:02:25,500 --> 00:02:30,510
elevated overhead that terminates at a

00:02:28,500 --> 00:02:34,320
nearby computing facility called a head

00:02:30,510 --> 00:02:36,660
end parts of this infrastructure are

00:02:34,320 --> 00:02:39,630
outside and are subject to extreme

00:02:36,660 --> 00:02:42,720
weather events and construction activity

00:02:39,630 --> 00:02:44,910
events in addition customers can be

00:02:42,720 --> 00:02:49,200
impacted by power grid events like power

00:02:44,910 --> 00:02:51,420
outages as you might imagine comcast

00:02:49,200 --> 00:02:54,050
customer satisfaction is directly tied

00:02:51,420 --> 00:02:56,220
to the reliability of the outside plant

00:02:54,050 --> 00:02:57,810
we're constantly looking for ways to

00:02:56,220 --> 00:02:59,820
improve the reliability of this

00:02:57,810 --> 00:03:03,780
infrastructure and to improve our

00:02:59,820 --> 00:03:07,020
ability to monitor it here we have a

00:03:03,780 --> 00:03:08,640
schematic view of the outside plant you

00:03:07,020 --> 00:03:11,190
can see the drop lines that connect the

00:03:08,640 --> 00:03:13,110
customers to the shared cabling that

00:03:11,190 --> 00:03:13,800
terminates at a computer system called a

00:03:13,110 --> 00:03:17,520
cmts

00:03:13,800 --> 00:03:19,620
or cable modem termination system you

00:03:17,520 --> 00:03:22,250
can see that we organized the customers

00:03:19,620 --> 00:03:25,290
that share common cabling into groups

00:03:22,250 --> 00:03:27,630
these groups are located in geographic

00:03:25,290 --> 00:03:32,250
proximity to each other and are often

00:03:27,630 --> 00:03:34,320
impacted by the same events we call

00:03:32,250 --> 00:03:37,380
these groups nodes and each node has a

00:03:34,320 --> 00:03:39,540
unique identifier the goal of our

00:03:37,380 --> 00:03:43,380
monitoring system is to analyze these

00:03:39,540 --> 00:03:45,950
nodes to do that we need an inventory of

00:03:43,380 --> 00:03:48,570
the devices and their corresponding cmts

00:03:45,950 --> 00:03:50,880
as well as reference data like

00:03:48,570 --> 00:03:53,780
thresholds for computing good warning

00:03:50,880 --> 00:03:56,910
and error levels of device properties

00:03:53,780 --> 00:03:59,580
once we have the inventory we can pull

00:03:56,910 --> 00:04:01,760
the devices in the cmts to collect

00:03:59,580 --> 00:04:05,100
attributes like signal-to-noise ratio

00:04:01,760 --> 00:04:09,630
transmit and receive power device online

00:04:05,100 --> 00:04:11,940
offline status and more we analyze this

00:04:09,630 --> 00:04:15,180
data to detect various event conditions

00:04:11,940 --> 00:04:18,150
on the node once we have the event

00:04:15,180 --> 00:04:21,269
conditions we can compare it with the

00:04:18,150 --> 00:04:24,060
previous event conditions this allows us

00:04:21,269 --> 00:04:26,460
to generate note event status for the

00:04:24,060 --> 00:04:27,690
node in three lists new events that have

00:04:26,460 --> 00:04:30,330
just started

00:04:27,690 --> 00:04:31,890
continuing events and events which are

00:04:30,330 --> 00:04:36,420
no longer present on the node and have

00:04:31,890 --> 00:04:39,870
just completed from the event lists we

00:04:36,420 --> 00:04:43,260
can calculate a node score which is a

00:04:39,870 --> 00:04:45,870
measure of the health of the node from

00:04:43,260 --> 00:04:47,700
the event list in the node score we can

00:04:45,870 --> 00:04:52,080
prioritize the work for the text in the

00:04:47,700 --> 00:04:54,390
field we decided to use Kafka streams

00:04:52,080 --> 00:04:58,800
and Cloud Foundry to implement this new

00:04:54,390 --> 00:05:00,630
system wise stream wise stream

00:04:58,800 --> 00:05:04,920
processing a good fit for building this

00:05:00,630 --> 00:05:06,810
type of system stream processing closely

00:05:04,920 --> 00:05:08,430
models our world of streams of

00:05:06,810 --> 00:05:11,790
asynchronous events that we want to

00:05:08,430 --> 00:05:13,980
process in real time stream processing

00:05:11,790 --> 00:05:16,020
is modular you can see that we've

00:05:13,980 --> 00:05:17,670
identified several high level processing

00:05:16,020 --> 00:05:21,150
functions and the input and output

00:05:17,670 --> 00:05:22,680
messages for each you can imagine that

00:05:21,150 --> 00:05:25,290
each of these can be further broken down

00:05:22,680 --> 00:05:27,980
into a network of smaller processing

00:05:25,290 --> 00:05:31,440
functions with input and output messages

00:05:27,980 --> 00:05:33,420
in addition we have a very nice high

00:05:31,440 --> 00:05:35,370
level graphical representation of our

00:05:33,420 --> 00:05:39,060
system that directly maps to the

00:05:35,370 --> 00:05:41,490
underlying implementation stream

00:05:39,060 --> 00:05:43,650
processing is built using distributed

00:05:41,490 --> 00:05:46,860
systems architecture so it can be scaled

00:05:43,650 --> 00:05:49,130
horizontally stream processing is

00:05:46,860 --> 00:05:52,290
resilient with built-in data redundancy

00:05:49,130 --> 00:05:54,870
failure detection an automatic failover

00:05:52,290 --> 00:05:58,230
in both the kafka server and the Kafka

00:05:54,870 --> 00:05:59,669
streams applications we'll talk more

00:05:58,230 --> 00:06:02,210
about streaming in a minute but first

00:05:59,669 --> 00:06:05,100
let's take a look at how we deploy an

00:06:02,210 --> 00:06:08,600
angular and spring java application to

00:06:05,100 --> 00:06:11,970
the cloud foundry application runtime

00:06:08,600 --> 00:06:13,980
ultimately the cloud is just a

00:06:11,970 --> 00:06:17,160
collection of physical computers with IP

00:06:13,980 --> 00:06:19,380
addresses what makes it powerful are the

00:06:17,160 --> 00:06:21,240
abstractions that have been created that

00:06:19,380 --> 00:06:24,510
allow developers to deploy and manage

00:06:21,240 --> 00:06:27,150
applications in this environment we want

00:06:24,510 --> 00:06:29,850
to deploy our applications and scale

00:06:27,150 --> 00:06:32,160
them with multiple instances and have

00:06:29,850 --> 00:06:35,040
the instances be restarted automatically

00:06:32,160 --> 00:06:37,050
if they fail let's take a look at how we

00:06:35,040 --> 00:06:40,690
use the Cloud Foundry application

00:06:37,050 --> 00:06:41,800
runtime to accomplish this goal

00:06:40,690 --> 00:06:46,300
we start with the Cloud Foundry

00:06:41,800 --> 00:06:48,790
command-line interface or CLI the CLI

00:06:46,300 --> 00:06:52,720
can be run on a developer's laptop or as

00:06:48,790 --> 00:06:56,080
part of a CI CD pipeline using the CLI

00:06:52,720 --> 00:06:59,500
we login to the target instance of the

00:06:56,080 --> 00:07:01,540
Cloud Foundry application runtime these

00:06:59,500 --> 00:07:04,210
instances could be running in multiple

00:07:01,540 --> 00:07:05,710
data centers around the world some of

00:07:04,210 --> 00:07:08,710
them may be running in private data

00:07:05,710 --> 00:07:10,840
centers within the company and others

00:07:08,710 --> 00:07:14,650
may be running in one or more of the

00:07:10,840 --> 00:07:16,600
public cloud providers after we log in

00:07:14,650 --> 00:07:19,530
we can use the CLI to push the

00:07:16,600 --> 00:07:21,910
applications to the application run time

00:07:19,530 --> 00:07:24,550
the push command inspects the

00:07:21,910 --> 00:07:26,700
application folder to determine the type

00:07:24,550 --> 00:07:30,010
of application that is being pushed and

00:07:26,700 --> 00:07:31,780
selects the appropriate build pack which

00:07:30,010 --> 00:07:33,880
creates a container image of the

00:07:31,780 --> 00:07:37,780
application and pushes it to the

00:07:33,880 --> 00:07:40,990
application runtime in the case of the

00:07:37,780 --> 00:07:43,750
angular web app we have to help the push

00:07:40,990 --> 00:07:45,490
command identify the appropriate build

00:07:43,750 --> 00:07:49,030
pack by creating a file in the app

00:07:45,490 --> 00:07:51,610
folder called static file the contents

00:07:49,030 --> 00:07:53,020
of this file push state enabled will

00:07:51,610 --> 00:07:57,040
allow deep linking into the angular

00:07:53,020 --> 00:07:59,230
application in recent years and has been

00:07:57,040 --> 00:08:04,060
come much easier to get started creating

00:07:59,230 --> 00:08:06,610
these types of applications for example

00:08:04,060 --> 00:08:08,950
to create the angular application we can

00:08:06,610 --> 00:08:13,840
follow the simple instructions at CLI

00:08:08,950 --> 00:08:16,030
angular Daioh to create a Java sprint a

00:08:13,840 --> 00:08:19,780
spring java application we can visit the

00:08:16,030 --> 00:08:21,790
start spring Daioh website we can

00:08:19,780 --> 00:08:25,300
identify the project's dependencies like

00:08:21,790 --> 00:08:28,690
web and actuator and then generate and

00:08:25,300 --> 00:08:31,860
download the project we really like

00:08:28,690 --> 00:08:34,870
using swagger with our rest services

00:08:31,860 --> 00:08:37,539
swagger generates metadata about our

00:08:34,870 --> 00:08:41,410
controllers and pojos that we use in our

00:08:37,539 --> 00:08:44,770
REST API this metadata can be used to

00:08:41,410 --> 00:08:47,260
automatically generate typescript proxy

00:08:44,770 --> 00:08:51,520
classes that the angular client can use

00:08:47,260 --> 00:08:53,920
to call the REST API as our REST API

00:08:51,520 --> 00:08:54,490
evolves we can regenerate the proxy

00:08:53,920 --> 00:08:56,590
classes

00:08:54,490 --> 00:09:00,160
and the typescript compiler will tell us

00:08:56,590 --> 00:09:02,470
if we have breaking changes to build our

00:09:00,160 --> 00:09:06,610
angular application we use the angular

00:09:02,470 --> 00:09:08,740
CLI s ng build command to build the

00:09:06,610 --> 00:09:12,460
spring java application we use the

00:09:08,740 --> 00:09:14,710
Gradle build command now let's focus in

00:09:12,460 --> 00:09:17,800
on just the spring java application and

00:09:14,710 --> 00:09:23,380
look at how we add kafka streams

00:09:17,800 --> 00:09:26,200
processing to it first we add the Kafka

00:09:23,380 --> 00:09:28,570
client jar and the Kafka streams jar to

00:09:26,200 --> 00:09:32,740
the application by referencing them in

00:09:28,570 --> 00:09:35,830
the build Gradle file using the API is

00:09:32,740 --> 00:09:38,140
provided by these jars our spring java

00:09:35,830 --> 00:09:43,330
application can interact with the Kafka

00:09:38,140 --> 00:09:46,060
server the client API provides the

00:09:43,330 --> 00:09:47,950
lowest level building blocks for sending

00:09:46,060 --> 00:09:51,670
and receiving messages to and from the

00:09:47,950 --> 00:09:55,120
Kafka server a producer is used to send

00:09:51,670 --> 00:09:57,690
messages to the server and a consumer is

00:09:55,120 --> 00:10:01,020
used to receive messages from the server

00:09:57,690 --> 00:10:03,940
the higher level processor API

00:10:01,020 --> 00:10:07,510
introduces a container class called a

00:10:03,940 --> 00:10:10,150
topology with the topology we can define

00:10:07,510 --> 00:10:13,450
a network of processors and define how

00:10:10,150 --> 00:10:15,280
the message is flow between them we can

00:10:13,450 --> 00:10:18,820
define a source and add it to the

00:10:15,280 --> 00:10:21,190
topology that the source will receive

00:10:18,820 --> 00:10:23,830
messages from the Kafka server and bring

00:10:21,190 --> 00:10:26,500
them into the topology another special

00:10:23,830 --> 00:10:29,230
processor called a sync can be added to

00:10:26,500 --> 00:10:33,280
the topology to send messages out to the

00:10:29,230 --> 00:10:37,030
Kafka server in Kafka we organize our

00:10:33,280 --> 00:10:40,000
message streams into topics let's look

00:10:37,030 --> 00:10:43,210
at an example of how we could receive

00:10:40,000 --> 00:10:45,460
messages from topic a and filter them

00:10:43,210 --> 00:10:49,120
and send the filtered messages to topic

00:10:45,460 --> 00:10:52,480
B to do this we first create an instance

00:10:49,120 --> 00:10:55,720
of the topology class then we use the

00:10:52,480 --> 00:10:57,700
topologies add source method to create a

00:10:55,720 --> 00:11:01,600
source in the topology that's connected

00:10:57,700 --> 00:11:03,940
to topic a next we create a processor a

00:11:01,600 --> 00:11:06,720
filter processor to filter the messages

00:11:03,940 --> 00:11:08,420
and then we use the topologies ad

00:11:06,720 --> 00:11:10,399
processor method

00:11:08,420 --> 00:11:14,149
to add the filter to the topology

00:11:10,399 --> 00:11:17,230
connecting it to the source and finally

00:11:14,149 --> 00:11:19,579
we call the topologies ad sync method

00:11:17,230 --> 00:11:23,690
connecting the sync to the processor

00:11:19,579 --> 00:11:26,149
into topic B at this point we can call

00:11:23,690 --> 00:11:30,139
the topologies run method to begin

00:11:26,149 --> 00:11:32,930
processing messages the spring Java app

00:11:30,139 --> 00:11:34,970
will connect to the Kafka server begin

00:11:32,930 --> 00:11:38,709
receiving messages from topic a and

00:11:34,970 --> 00:11:42,019
sending the filtered messages to topic B

00:11:38,709 --> 00:11:44,209
this is a stateless example the

00:11:42,019 --> 00:11:46,190
processor can perform its operation

00:11:44,209 --> 00:11:50,630
based solely on the contents of the

00:11:46,190 --> 00:11:53,360
messages we can also define stateful

00:11:50,630 --> 00:11:57,380
processors that interact with a state

00:11:53,360 --> 00:12:01,639
store using get and put methods on key

00:11:57,380 --> 00:12:05,389
value pairs to create our to add a

00:12:01,639 --> 00:12:07,370
stateful processor we create a state

00:12:05,389 --> 00:12:12,320
store in the topology by calling the

00:12:07,370 --> 00:12:14,980
topologies add state store method this

00:12:12,320 --> 00:12:19,459
Kafka streams jar contains another API

00:12:14,980 --> 00:12:23,079
called the streams API this API is built

00:12:19,459 --> 00:12:25,339
using the processor API the streams API

00:12:23,079 --> 00:12:28,160
introduces a container class called a

00:12:25,339 --> 00:12:31,250
streams builder using the streams

00:12:28,160 --> 00:12:34,070
builder we can define K streams and K

00:12:31,250 --> 00:12:38,920
tables which are abstractions of the

00:12:34,070 --> 00:12:38,920
processor api's source and state store

00:12:39,490 --> 00:12:44,420
the case stream in K table classes

00:12:42,350 --> 00:12:46,519
provide fluent methods that can be

00:12:44,420 --> 00:12:50,029
chained together to define the desired

00:12:46,519 --> 00:12:52,760
processing one of these methods the two

00:12:50,029 --> 00:12:56,660
method can be used to send messages out

00:12:52,760 --> 00:12:59,209
to a Kafka topic now let's look at how

00:12:56,660 --> 00:13:02,779
we would implement our filtering example

00:12:59,209 --> 00:13:04,459
using the streams API first we create

00:13:02,779 --> 00:13:07,790
create an instance of the streams

00:13:04,459 --> 00:13:11,329
builder class then we use that to define

00:13:07,790 --> 00:13:15,260
a case stream object that connects to

00:13:11,329 --> 00:13:19,519
topic a then we chain together the

00:13:15,260 --> 00:13:21,680
filter and two methods the filter method

00:13:19,519 --> 00:13:24,920
accepts a lambda function

00:13:21,680 --> 00:13:27,499
that is evaluated to determine if a

00:13:24,920 --> 00:13:29,389
message should be output or not the to

00:13:27,499 --> 00:13:34,040
method writes the messages out to the

00:13:29,389 --> 00:13:36,110
topic B we use the stream builders build

00:13:34,040 --> 00:13:38,929
command which actually generates a

00:13:36,110 --> 00:13:46,129
processor API topology that we can then

00:13:38,929 --> 00:13:48,920
run to begin processing messages when

00:13:46,129 --> 00:13:51,050
we're building our stream processing

00:13:48,920 --> 00:13:53,360
application we can mix and match the

00:13:51,050 --> 00:13:58,369
capabilities from all three of these

00:13:53,360 --> 00:14:02,059
api's so now let's dig into the details

00:13:58,369 --> 00:14:06,139
of the stream processing starting with

00:14:02,059 --> 00:14:08,929
the client API here we have a topic an

00:14:06,139 --> 00:14:12,339
unbounded continuous real-time flow of

00:14:08,929 --> 00:14:15,309
messages each with a key and a value

00:14:12,339 --> 00:14:18,079
topics are broken into partitions

00:14:15,309 --> 00:14:21,589
partitions are a unit of scalability and

00:14:18,079 --> 00:14:24,559
replication producers are used to send

00:14:21,589 --> 00:14:27,829
messages to a Kafka topic the producer

00:14:24,559 --> 00:14:32,149
Maps a message is key to its associated

00:14:27,829 --> 00:14:34,459
partition the same key will always map

00:14:32,149 --> 00:14:38,839
to the same partition so in our

00:14:34,459 --> 00:14:41,179
implementation we use the unique outside

00:14:38,839 --> 00:14:43,730
plant node identifier as the key in our

00:14:41,179 --> 00:14:45,139
messages and we make sure that all of

00:14:43,730 --> 00:14:48,139
our topics have the same number of

00:14:45,139 --> 00:14:50,839
partitions so what ends up happening is

00:14:48,139 --> 00:14:52,790
a particular partition will contain the

00:14:50,839 --> 00:14:57,439
same set of keys across all of our

00:14:52,790 --> 00:15:02,059
topics a consumer is used to receive

00:14:57,439 --> 00:15:03,920
messages from a topic let's say we

00:15:02,059 --> 00:15:07,699
create a consumer application and deploy

00:15:03,920 --> 00:15:10,100
four instances of it you can see that

00:15:07,699 --> 00:15:13,160
the consumer negotiates with the Kafka

00:15:10,100 --> 00:15:16,179
server to determine which consumers will

00:15:13,160 --> 00:15:18,740
receive messages from which partitions

00:15:16,179 --> 00:15:21,949
this distributes the load across our

00:15:18,740 --> 00:15:24,079
consumers if there are more consumers

00:15:21,949 --> 00:15:26,740
than there are partitions some of them

00:15:24,079 --> 00:15:30,170
will be idle and won't receive messages

00:15:26,740 --> 00:15:32,870
if a consumer fails its failure will be

00:15:30,170 --> 00:15:34,620
detected automatically and rebalancing

00:15:32,870 --> 00:15:37,930
will occur

00:15:34,620 --> 00:15:40,200
one of the other healthy consumers will

00:15:37,930 --> 00:15:43,270
pick up its processing responsibilities

00:15:40,200 --> 00:15:46,030
now let's say we deploy another consumer

00:15:43,270 --> 00:15:47,950
with just a single instance notice that

00:15:46,030 --> 00:15:50,650
all of the partitions are mapped to that

00:15:47,950 --> 00:15:55,660
consumer and all of the messages feed to

00:15:50,650 --> 00:15:58,140
that consumer for the topic our Kafka

00:15:55,660 --> 00:16:01,210
cluster is made up of multiple brokers

00:15:58,140 --> 00:16:04,840
the brokers share the work of managing

00:16:01,210 --> 00:16:06,700
the partitions the brokers create copies

00:16:04,840 --> 00:16:10,480
of the partitions called replicas and

00:16:06,700 --> 00:16:12,730
distribute them across the cluster the

00:16:10,480 --> 00:16:15,100
cluster will automatically detect if one

00:16:12,730 --> 00:16:16,720
of the brokers fails and its work will

00:16:15,100 --> 00:16:22,710
be picked up by some of the other

00:16:16,720 --> 00:16:25,560
brokers in the cluster because the other

00:16:22,710 --> 00:16:28,660
api's are built using the client API

00:16:25,560 --> 00:16:31,750
they follow these same patterns for

00:16:28,660 --> 00:16:34,500
achieving scaling and fault tolerance so

00:16:31,750 --> 00:16:40,120
next let's look at the processor API and

00:16:34,500 --> 00:16:42,910
a topology so here we have a topology

00:16:40,120 --> 00:16:45,970
with sources processors sinks and state

00:16:42,910 --> 00:16:48,040
stores when we add a processor to the

00:16:45,970 --> 00:16:51,250
topology we indicate the one or more

00:16:48,040 --> 00:16:53,220
processors that feed it messages the

00:16:51,250 --> 00:16:56,080
sources and sinks connect our topology

00:16:53,220 --> 00:16:59,170
to the Associated Kafka topics in the

00:16:56,080 --> 00:17:02,800
Kafka server now let's say that we

00:16:59,170 --> 00:17:05,350
deploy or we create topics with three

00:17:02,800 --> 00:17:07,810
partitions each and then let's say that

00:17:05,350 --> 00:17:11,620
we deploy our application with three

00:17:07,810 --> 00:17:13,960
instances we've effectively subdivided

00:17:11,620 --> 00:17:20,110
our processing into three independent

00:17:13,960 --> 00:17:22,090
systems each with its own data if we

00:17:20,110 --> 00:17:26,440
need more scale we can just create more

00:17:22,090 --> 00:17:29,470
partitions and more instances so in our

00:17:26,440 --> 00:17:32,350
application we have close to sixty

00:17:29,470 --> 00:17:34,930
instances and sixty partitions in each

00:17:32,350 --> 00:17:38,340
of the topics but you can scale that up

00:17:34,930 --> 00:17:41,170
to 100 or 200 or however many you need

00:17:38,340 --> 00:17:42,760
so next let's actually go inside one of

00:17:41,170 --> 00:17:45,250
these instances and look at how the

00:17:42,760 --> 00:17:46,919
messages flow and how the state store

00:17:45,250 --> 00:17:49,570
works

00:17:46,919 --> 00:17:51,820
so here we are inside a running instance

00:17:49,570 --> 00:17:55,500
of the topology that's been assigned the

00:17:51,820 --> 00:17:59,139
responsibility to process partition zero

00:17:55,500 --> 00:18:01,809
the source on the Left ties to an

00:17:59,139 --> 00:18:03,789
underlying Kafka topic next to that we

00:18:01,809 --> 00:18:05,889
have a processor and next to that we

00:18:03,789 --> 00:18:09,130
have a state store that the processor

00:18:05,889 --> 00:18:12,159
will be using below the state store we

00:18:09,130 --> 00:18:13,990
have a changelog topic when a message

00:18:12,159 --> 00:18:18,100
arrives in the underlying Kafka server

00:18:13,990 --> 00:18:21,850
it is consumed by the source the source

00:18:18,100 --> 00:18:23,730
emits the message the topology is

00:18:21,850 --> 00:18:25,990
responsible for orchestrating

00:18:23,730 --> 00:18:29,409
transmission of that emitted message to

00:18:25,990 --> 00:18:32,620
the processor the processor can perform

00:18:29,409 --> 00:18:34,899
a get method on the state store to read

00:18:32,620 --> 00:18:37,529
a value in this case the state store is

00:18:34,899 --> 00:18:40,179
empty and no value but will be retrieved

00:18:37,529 --> 00:18:43,149
the processor can then do a put on the

00:18:40,179 --> 00:18:46,149
state store and put that message into

00:18:43,149 --> 00:18:48,309
the state store the state store stores

00:18:46,149 --> 00:18:51,789
this message in a local data structure

00:18:48,309 --> 00:18:54,149
so it's very fast it also sends the

00:18:51,789 --> 00:19:04,629
message to a backing changelog topic

00:18:54,149 --> 00:19:05,950
let's repeat this for keys 2 & 3 now

00:19:04,629 --> 00:19:07,779
let's look at a message whose key

00:19:05,950 --> 00:19:10,659
already exists in the state store and

00:19:07,779 --> 00:19:12,580
has a different value when this message

00:19:10,659 --> 00:19:14,529
is put into the state store notice that

00:19:12,580 --> 00:19:16,720
it overwrites the existing value and

00:19:14,529 --> 00:19:21,070
again is written to the changelog topic

00:19:16,720 --> 00:19:26,080
let's repeat this four key two and again

00:19:21,070 --> 00:19:28,029
four key one notice that the state store

00:19:26,080 --> 00:19:30,879
contains the most recent value for each

00:19:28,029 --> 00:19:32,799
key and the changelog contains all of

00:19:30,879 --> 00:19:37,360
the messages including the historical

00:19:32,799 --> 00:19:39,639
ones if we want to query for the most

00:19:37,360 --> 00:19:43,179
recent values we can query the state

00:19:39,639 --> 00:19:45,309
store if we're interested in changes to

00:19:43,179 --> 00:19:49,720
the state store we can subscribe to the

00:19:45,309 --> 00:19:52,000
changelog topic in addition we can

00:19:49,720 --> 00:19:55,929
rebuild the state store by replaying the

00:19:52,000 --> 00:19:58,389
messages in the changelog topic over

00:19:55,929 --> 00:20:00,550
time the changelog will grow larger and

00:19:58,389 --> 00:20:02,380
larger this means that it will

00:20:00,550 --> 00:20:04,990
take longer and longer to rebuild our

00:20:02,380 --> 00:20:09,340
state store there are three ways in

00:20:04,990 --> 00:20:11,740
Kafka to limit a topic size the first is

00:20:09,340 --> 00:20:14,560
based in default is based on time frame

00:20:11,740 --> 00:20:17,230
message is older than the specified time

00:20:14,560 --> 00:20:21,310
frame will be deleted the second is

00:20:17,230 --> 00:20:23,770
based on topic size older messages will

00:20:21,310 --> 00:20:27,070
be deleted when the topic exceeds the

00:20:23,770 --> 00:20:29,830
configured limit both of these methods

00:20:27,070 --> 00:20:33,850
could result in removing the most recent

00:20:29,830 --> 00:20:35,980
key value pair for one of our keys the

00:20:33,850 --> 00:20:39,790
third way to limit topic size is by

00:20:35,980 --> 00:20:43,120
indicating that a topic is a compacted

00:20:39,790 --> 00:20:48,580
topic periodically Kafka will run a

00:20:43,120 --> 00:20:51,910
compaction process which will remove the

00:20:48,580 --> 00:20:55,620
key value pairs whose key is present in

00:20:51,910 --> 00:20:59,230
a more recent message so this is how we

00:20:55,620 --> 00:21:04,510
rebuild our state stores from a backing

00:20:59,230 --> 00:21:07,660
changelog topic so just to review state

00:21:04,510 --> 00:21:09,820
stores are fast because the get and put

00:21:07,660 --> 00:21:12,580
operations are acting on local data

00:21:09,820 --> 00:21:14,320
structures they're durable because they

00:21:12,580 --> 00:21:17,200
can be rebuilt from the backing change

00:21:14,320 --> 00:21:19,060
lock topic and they're scalable because

00:21:17,200 --> 00:21:23,830
they're based on the same distributed

00:21:19,060 --> 00:21:25,150
partitioning scheme as other topics at

00:21:23,830 --> 00:21:27,160
this point I'd like to turn it over to

00:21:25,150 --> 00:21:30,630
Dan who is responsible for driving the

00:21:27,160 --> 00:21:32,890
implementation of our system thanks Mike

00:21:30,630 --> 00:21:34,660
that's a real good look at the processor

00:21:32,890 --> 00:21:37,810
API once again my name is Dan Carroll

00:21:34,660 --> 00:21:40,390
I'm a DevOps engineer at Comcast I'd

00:21:37,810 --> 00:21:45,520
like to present some of our processor

00:21:40,390 --> 00:21:48,760
API patterns all right so we use state

00:21:45,520 --> 00:21:50,650
stores for deduplication every node

00:21:48,760 --> 00:21:55,330
requests materialized view of table data

00:21:50,650 --> 00:21:57,070
and rolling aggregates we have a use

00:21:55,330 --> 00:22:00,220
case to not have duplicate node poles

00:21:57,070 --> 00:22:02,710
running concurrently so when Mike made

00:22:00,220 --> 00:22:05,020
mention before we use SNMP to go out and

00:22:02,710 --> 00:22:06,910
crawl our network and we generally do

00:22:05,020 --> 00:22:08,440
our poles at the node level kind of the

00:22:06,910 --> 00:22:11,550
neighborhood level to find out the

00:22:08,440 --> 00:22:13,840
status of all the devices on that node

00:22:11,550 --> 00:22:16,049
so we don't want more

00:22:13,840 --> 00:22:21,460
one of those running at a time on a note

00:22:16,049 --> 00:22:24,309
course uh we could we like to reduce

00:22:21,460 --> 00:22:26,230
resource stress and also database

00:22:24,309 --> 00:22:28,390
locking we had a little issue last year

00:22:26,230 --> 00:22:30,400
with database locking when we're hitting

00:22:28,390 --> 00:22:34,630
the same node with two different weak

00:22:30,400 --> 00:22:36,820
requests so we for our UI we work in

00:22:34,630 --> 00:22:39,220
conjunction with sockets to return the

00:22:36,820 --> 00:22:41,850
requests the response there would be

00:22:39,220 --> 00:22:46,240
multiple requests only one will run with

00:22:41,850 --> 00:22:51,039
multiple responses to the UI will also

00:22:46,240 --> 00:22:53,289
publish the results to a state store ok

00:22:51,039 --> 00:22:56,260
the flow for this pattern is we have the

00:22:53,289 --> 00:22:57,610
UI to kick off a node request there's a

00:22:56,260 --> 00:22:59,850
couple other methods to we'll look at

00:22:57,610 --> 00:23:02,679
those in a few minutes

00:22:59,850 --> 00:23:05,250
so we have a node analysis request

00:23:02,679 --> 00:23:10,470
processor that takes in the request

00:23:05,250 --> 00:23:13,090
it'll read the state store to see if

00:23:10,470 --> 00:23:15,250
that node is running if it's running

00:23:13,090 --> 00:23:18,130
it'll ignore this request if it's at

00:23:15,250 --> 00:23:19,419
rest it'll set it to running and then

00:23:18,130 --> 00:23:22,000
forward that request to the node

00:23:19,419 --> 00:23:23,529
analysis here is represented as a cloud

00:23:22,000 --> 00:23:25,929
because there's a whole lot of steps to

00:23:23,529 --> 00:23:29,409
a node analysis the last step in the

00:23:25,929 --> 00:23:34,390
node analysis is the publish processor

00:23:29,409 --> 00:23:35,890
so that processor will first set the

00:23:34,390 --> 00:23:39,190
value in the state store for that node

00:23:35,890 --> 00:23:41,289
to at rest and then we'll publish the

00:23:39,190 --> 00:23:43,360
results generally to a state store we

00:23:41,289 --> 00:23:46,539
can also forward the results to other

00:23:43,360 --> 00:23:52,090
topics for other apps use or for any

00:23:46,539 --> 00:23:53,950
other need we also have a use case that

00:23:52,090 --> 00:23:58,570
we need to have the current list of the

00:23:53,950 --> 00:24:02,289
Weisse data location device type much

00:23:58,570 --> 00:24:04,840
other things about each node and device

00:24:02,289 --> 00:24:10,260
and also ref data pull data there's a

00:24:04,840 --> 00:24:15,039
lot of other needs for current data so

00:24:10,260 --> 00:24:17,710
we reshape the raw data for the intended

00:24:15,039 --> 00:24:19,179
use and and put it in a state store and

00:24:17,710 --> 00:24:23,080
we're finding that is faster than

00:24:19,179 --> 00:24:25,809
continually querying for the data that

00:24:23,080 --> 00:24:27,250
we need at that point in time so we use

00:24:25,809 --> 00:24:31,890
the UI

00:24:27,250 --> 00:24:35,830
or an external system push as we call it

00:24:31,890 --> 00:24:37,900
to load the data to a topic and then a

00:24:35,830 --> 00:24:42,850
state store so that's where we inject

00:24:37,900 --> 00:24:45,370
into the processors in the future we

00:24:42,850 --> 00:24:47,500
would like to get any change to data

00:24:45,370 --> 00:24:49,690
pushed to us so we don't have to do a

00:24:47,500 --> 00:24:51,280
load you know we only want to get the

00:24:49,690 --> 00:24:54,010
deltas but we do have to read a database

00:24:51,280 --> 00:24:56,230
at some point and get the data with this

00:24:54,010 --> 00:24:58,000
method like Mike mentioned earlier we

00:24:56,230 --> 00:25:02,710
can query the app and not the database

00:24:58,000 --> 00:25:05,380
when we're running okay so we start at

00:25:02,710 --> 00:25:08,260
the top left we have currently most of

00:25:05,380 --> 00:25:11,080
our data still is in a database in our

00:25:08,260 --> 00:25:16,740
existing system so we have a timer that

00:25:11,080 --> 00:25:19,870
can kick off a load we have the UI app I

00:25:16,740 --> 00:25:21,220
can kick off a load and we also have a

00:25:19,870 --> 00:25:22,990
rest endpoint where you can kick off

00:25:21,220 --> 00:25:25,840
loads so other systems can actually kick

00:25:22,990 --> 00:25:28,390
off a load of data in our system a

00:25:25,840 --> 00:25:31,210
general it's the same pattern for each

00:25:28,390 --> 00:25:33,010
one though you query the database you

00:25:31,210 --> 00:25:35,410
get the raw data as we call it so that's

00:25:33,010 --> 00:25:38,050
all the data about the node you stream

00:25:35,410 --> 00:25:43,510
that to a topic off of that topic we'll

00:25:38,050 --> 00:25:46,060
have a processor that'll change the data

00:25:43,510 --> 00:25:49,150
to the needed use so in this case for

00:25:46,060 --> 00:25:51,010
some node data we have a state store for

00:25:49,150 --> 00:25:53,170
device and location we have another

00:25:51,010 --> 00:25:54,610
state store to keep the stats about the

00:25:53,170 --> 00:25:55,780
node and then we have another state

00:25:54,610 --> 00:25:59,560
store for some performance

00:25:55,780 --> 00:26:00,850
characteristics about the node so that

00:25:59,560 --> 00:26:04,720
processor will write all those records

00:26:00,850 --> 00:26:06,640
there and then that targeted data is

00:26:04,720 --> 00:26:08,890
there and available to the rest of the

00:26:06,640 --> 00:26:11,320
application inside of the application so

00:26:08,890 --> 00:26:12,820
every time we need to find that the

00:26:11,320 --> 00:26:14,860
devices for a node we don't have to

00:26:12,820 --> 00:26:19,420
query the database they're right there

00:26:14,860 --> 00:26:21,850
in the app we also have a use case to

00:26:19,420 --> 00:26:27,240
keep track of the steps of a node

00:26:21,850 --> 00:26:29,680
request with this we can pull stats from

00:26:27,240 --> 00:26:31,870
all the no requests we do you know a

00:26:29,680 --> 00:26:34,120
couple million a day it really gives us

00:26:31,870 --> 00:26:37,870
a good look at into checking in to the

00:26:34,120 --> 00:26:40,210
health of our application and it gives

00:26:37,870 --> 00:26:41,080
us a window into the node analysis step

00:26:40,210 --> 00:26:44,379
so we can see what's

00:26:41,080 --> 00:26:48,100
with any individual node at any time so

00:26:44,379 --> 00:26:49,539
here's a partial look at our UI the

00:26:48,100 --> 00:26:52,389
names of the steps have been changed for

00:26:49,539 --> 00:26:55,690
the demo but just from the elapsed time

00:26:52,389 --> 00:26:58,119
I can tell that step 3a is the actual

00:26:55,690 --> 00:26:59,409
poll step so this is where we have the

00:26:58,119 --> 00:27:03,369
list of devices we're going to go out

00:26:59,409 --> 00:27:09,639
and use SNMP ping each device on that

00:27:03,369 --> 00:27:11,110
node and return the results so what's

00:27:09,639 --> 00:27:13,899
helpful for this for especially for

00:27:11,110 --> 00:27:16,179
DevOps and various debugging is if we

00:27:13,899 --> 00:27:17,440
see a spike in that elapsed time we know

00:27:16,179 --> 00:27:21,340
something's going out with a node pull

00:27:17,440 --> 00:27:23,700
so whether it's through network latency

00:27:21,340 --> 00:27:26,379
or something going on with our system

00:27:23,700 --> 00:27:27,909
it's also helpful to see every other

00:27:26,379 --> 00:27:32,470
step runs much quicker

00:27:27,909 --> 00:27:35,049
so if we see elapsed times going longer

00:27:32,470 --> 00:27:36,340
in other steps we can target operations

00:27:35,049 --> 00:27:40,539
right to that step and what it's doing

00:27:36,340 --> 00:27:44,259
so we can find out what's going on when

00:27:40,539 --> 00:27:45,970
we when we see events also the lack of a

00:27:44,259 --> 00:27:49,090
step tells us something too if we don't

00:27:45,970 --> 00:27:50,950
see step 6 running here it runs multiple

00:27:49,090 --> 00:27:53,259
times that's one of our analyzers we

00:27:50,950 --> 00:27:54,789
call them then we know something is

00:27:53,259 --> 00:27:56,440
happening before that step or that step

00:27:54,789 --> 00:28:02,139
can't finish and then we could target

00:27:56,440 --> 00:28:04,809
operations right to that step 2 okay now

00:28:02,139 --> 00:28:07,600
we'd like to present timers and

00:28:04,809 --> 00:28:10,450
streaming logic for our data pump and

00:28:07,600 --> 00:28:14,049
our plant validator we have a use case

00:28:10,450 --> 00:28:16,989
to load data from other systems on a

00:28:14,049 --> 00:28:19,809
schedule generally we do a lot of things

00:28:16,989 --> 00:28:21,220
twice a day it seems like and that this

00:28:19,809 --> 00:28:23,859
is because like I said earlier most

00:28:21,220 --> 00:28:25,570
other systems don't push the data to us

00:28:23,859 --> 00:28:28,059
they make it available through rest URIs

00:28:25,570 --> 00:28:30,190
and we have to get you our REST API is

00:28:28,059 --> 00:28:34,450
excuse me and we have to go get the data

00:28:30,190 --> 00:28:36,580
twice a day so it's a general pattern of

00:28:34,450 --> 00:28:38,980
streaming the data to a topic and a

00:28:36,580 --> 00:28:41,320
processor takes that data shapes it to

00:28:38,980 --> 00:28:46,509
the needed needed use and populates a

00:28:41,320 --> 00:28:48,789
state store or topic all right so we are

00:28:46,509 --> 00:28:51,100
a spring application so we can use the

00:28:48,789 --> 00:28:53,639
enabled scheduling annotation and then

00:28:51,100 --> 00:28:55,010
we use the scheduled annotation on the

00:28:53,639 --> 00:28:58,070
method

00:28:55,010 --> 00:29:00,860
we want to run on a timer this one runs

00:28:58,070 --> 00:29:04,940
every 60 seconds and it looks like we're

00:29:00,860 --> 00:29:06,769
getting definitely doing a web service

00:29:04,940 --> 00:29:10,970
call and we're getting I believe the

00:29:06,769 --> 00:29:13,100
inventory so after we're gonna look at

00:29:10,970 --> 00:29:15,950
response describe in a minute but after

00:29:13,100 --> 00:29:17,539
we get the answer we walk through so I'm

00:29:15,950 --> 00:29:19,789
gonna know data and we use a producer

00:29:17,539 --> 00:29:21,500
record to get all that data back into a

00:29:19,789 --> 00:29:27,830
topic and back into our streams

00:29:21,500 --> 00:29:30,320
processing okay now when we find events

00:29:27,830 --> 00:29:32,059
in our plant some events have a soaking

00:29:30,320 --> 00:29:33,769
period so we wait I think it's generally

00:29:32,059 --> 00:29:35,870
about five minutes to see if that event

00:29:33,769 --> 00:29:37,789
is still happening some events go right

00:29:35,870 --> 00:29:41,570
to confirmed and we take actions on them

00:29:37,789 --> 00:29:42,799
other events we wait a few minutes to

00:29:41,570 --> 00:29:44,690
see if it's still happening so we don't

00:29:42,799 --> 00:29:45,669
roll a truck if something is just

00:29:44,690 --> 00:29:49,850
temporary

00:29:45,669 --> 00:29:51,440
okay yeah so in this case we use a one

00:29:49,850 --> 00:29:53,779
minute timer we check the status of

00:29:51,440 --> 00:29:55,370
event and and state stores have proven

00:29:53,779 --> 00:29:57,350
to be very useful for this because we

00:29:55,370 --> 00:30:01,130
don't have to query our database we have

00:29:57,350 --> 00:30:02,690
the data so here's a look at that

00:30:01,130 --> 00:30:05,480
pattern and it looks a lot like the

00:30:02,690 --> 00:30:06,950
pattern from earlier except instead of a

00:30:05,480 --> 00:30:10,100
database we have the soaking events

00:30:06,950 --> 00:30:13,580
store so every I guess a minute I think

00:30:10,100 --> 00:30:15,169
in this case the timer runs it gets the

00:30:13,580 --> 00:30:17,240
soaking events that are expired and need

00:30:15,169 --> 00:30:20,570
to be checked again it sends a request

00:30:17,240 --> 00:30:23,059
to a validation processor the validation

00:30:20,570 --> 00:30:24,980
processor then runs a node analysis it's

00:30:23,059 --> 00:30:28,610
actually a partial node analysis we only

00:30:24,980 --> 00:30:32,870
run the steps and analyzers that look

00:30:28,610 --> 00:30:35,269
for the same problem and then we have

00:30:32,870 --> 00:30:38,960
the validation result processor which

00:30:35,269 --> 00:30:41,240
will then take the result figure out

00:30:38,960 --> 00:30:43,850
what status that event should go to

00:30:41,240 --> 00:30:45,950
whether it's confirmed or an outbound or

00:30:43,850 --> 00:30:48,289
closed they'll update the state store

00:30:45,950 --> 00:30:49,970
and forward some messages to other

00:30:48,289 --> 00:30:54,529
topics so we could take the appropriate

00:30:49,970 --> 00:30:56,779
actions based on that status all right

00:30:54,529 --> 00:30:58,820
and we also had a use case I have use

00:30:56,779 --> 00:31:01,220
case to remove latency when we're using

00:30:58,820 --> 00:31:05,210
web services there's one stream

00:31:01,220 --> 00:31:08,299
available an excuse me one thread

00:31:05,210 --> 00:31:08,630
available per stream topology so if if

00:31:08,299 --> 00:31:10,640
we

00:31:08,630 --> 00:31:13,220
have a web service call it will block

00:31:10,640 --> 00:31:15,950
the rest of the stream from running so

00:31:13,220 --> 00:31:17,330
we switched to asynchronous calls and

00:31:15,950 --> 00:31:18,920
this was most evident when we were

00:31:17,330 --> 00:31:21,230
pulling a node and it takes 10 plus

00:31:18,920 --> 00:31:23,210
seconds we noticed that nothing else was

00:31:21,230 --> 00:31:25,700
going to get processed in that topology

00:31:23,210 --> 00:31:27,530
at that time but then the second thing

00:31:25,700 --> 00:31:30,710
we found is when the async call returns

00:31:27,530 --> 00:31:33,490
were no longer in that stream processing

00:31:30,710 --> 00:31:38,480
so we have to switch back to a producer

00:31:33,490 --> 00:31:40,220
at that point to stream the response or

00:31:38,480 --> 00:31:44,360
you know the answer back into the

00:31:40,220 --> 00:31:48,370
topology to get it running again so from

00:31:44,360 --> 00:31:51,590
you saw Mike's diagram earlier the two

00:31:48,370 --> 00:31:53,540
red boxes are where we do the poll so of

00:31:51,590 --> 00:31:55,730
that like said when we go out and crawl

00:31:53,540 --> 00:31:58,220
using a send an SNMP to get all the

00:31:55,730 --> 00:32:00,170
device data if those two boxes are

00:31:58,220 --> 00:32:03,200
blocking you can just see that the rest

00:32:00,170 --> 00:32:05,660
of the topology would not be freed up so

00:32:03,200 --> 00:32:08,480
that was our first clue into that we

00:32:05,660 --> 00:32:11,480
need to do all our web services via an

00:32:08,480 --> 00:32:14,360
asynchronous call so I like to show the

00:32:11,480 --> 00:32:16,070
code pattern for that now this is just a

00:32:14,360 --> 00:32:20,270
straight on web service call this is a

00:32:16,070 --> 00:32:22,070
post method we're using a web client we

00:32:20,270 --> 00:32:25,580
return a mono which basically is one

00:32:22,070 --> 00:32:27,740
answer we do a post in this one and it's

00:32:25,580 --> 00:32:29,840
just your other web service stuff media

00:32:27,740 --> 00:32:34,130
type and or context type and what we're

00:32:29,840 --> 00:32:36,920
accepting so that code is the second

00:32:34,130 --> 00:32:39,710
line in this code example and then the

00:32:36,920 --> 00:32:41,420
next the response subscribe is is the

00:32:39,710 --> 00:32:44,420
job of future that's waiting to get the

00:32:41,420 --> 00:32:46,220
answer back so anything inside there the

00:32:44,420 --> 00:32:48,410
function inside of there will be run

00:32:46,220 --> 00:32:53,120
when we get the answer from the web

00:32:48,410 --> 00:32:56,480
service call and then that code is the

00:32:53,120 --> 00:32:58,550
business logic on this line so after the

00:32:56,480 --> 00:33:00,530
business logic runs that's where we pull

00:32:58,550 --> 00:33:04,340
the data apart and get the record ready

00:33:00,530 --> 00:33:07,340
for the intended use down the line we

00:33:04,340 --> 00:33:09,710
then have to switch to a producer to

00:33:07,340 --> 00:33:14,780
inject back into the topology because

00:33:09,710 --> 00:33:16,850
this function here the the thread you

00:33:14,780 --> 00:33:18,740
know didn't wait and moved on so when

00:33:16,850 --> 00:33:20,480
this runs it's not in that topology

00:33:18,740 --> 00:33:22,220
anymore because you're really outside of

00:33:20,480 --> 00:33:27,919
the producer you're inside of this

00:33:22,220 --> 00:33:31,250
function when it runs okay so when we're

00:33:27,919 --> 00:33:34,760
looking for solutions to redo our

00:33:31,250 --> 00:33:39,320
current application we found for these

00:33:34,760 --> 00:33:41,870
use cases that streaming is faster than

00:33:39,320 --> 00:33:44,120
a traditional database for these uses

00:33:41,870 --> 00:33:47,720
we're finding it's not monolithic like

00:33:44,120 --> 00:33:49,789
Mike mentioned before we we can add new

00:33:47,720 --> 00:33:51,260
things to this apology without breaking

00:33:49,789 --> 00:33:54,470
the other things because we just kind of

00:33:51,260 --> 00:33:56,510
stream over to that topic see if it

00:33:54,470 --> 00:34:00,230
works then we can bring that topic into

00:33:56,510 --> 00:34:02,059
the rest of the application you can see

00:34:00,230 --> 00:34:05,450
from the programming model we only have

00:34:02,059 --> 00:34:07,899
a couple of patterns and we were able to

00:34:05,450 --> 00:34:10,580
solve most of our issues so that was a

00:34:07,899 --> 00:34:13,340
it's a very straightforward programming

00:34:10,580 --> 00:34:14,720
model and we're finding it's

00:34:13,340 --> 00:34:17,929
fundamentally different but it does

00:34:14,720 --> 00:34:22,070
mirror the real world of the events that

00:34:17,929 --> 00:34:23,389
were we're out looking for all right I

00:34:22,070 --> 00:34:30,230
like to turn it back over to Mike for

00:34:23,389 --> 00:34:32,060
some technical notes thanks Dan so let's

00:34:30,230 --> 00:34:34,659
take a quick look at some of the

00:34:32,060 --> 00:34:36,649
technical notes we've made along the way

00:34:34,659 --> 00:34:39,129
it's easy to stand up a

00:34:36,649 --> 00:34:41,510
fully-functioning system on a laptop

00:34:39,129 --> 00:34:43,839
we've talked about how we can get the

00:34:41,510 --> 00:34:46,790
angular and spring Java applications

00:34:43,839 --> 00:34:49,940
deployed to the local environment we can

00:34:46,790 --> 00:34:52,330
also set up a local kafka server we can

00:34:49,940 --> 00:34:55,609
visit the Kafka Apache org website

00:34:52,330 --> 00:34:58,220
download Kafka and start the zookeeper

00:34:55,609 --> 00:35:00,230
and Kafka broker services or we can

00:34:58,220 --> 00:35:02,240
visit the confluent do website and

00:35:00,230 --> 00:35:06,440
download the confluent distribution

00:35:02,240 --> 00:35:08,780
which includes the confluent CLI we can

00:35:06,440 --> 00:35:11,390
use the confluent start command to start

00:35:08,780 --> 00:35:14,060
up all of the Kafka services this is

00:35:11,390 --> 00:35:17,390
also nice because we can run confluent

00:35:14,060 --> 00:35:19,760
destroy to create a clean empty Kafka

00:35:17,390 --> 00:35:21,560
instance and then run confluence start

00:35:19,760 --> 00:35:25,520
again so in your development loop that's

00:35:21,560 --> 00:35:27,740
really nice when you can experiment by

00:35:25,520 --> 00:35:29,839
creating Kafka topics and experiment

00:35:27,740 --> 00:35:31,070
with the streaming applications when

00:35:29,839 --> 00:35:32,330
you're ready you can push these

00:35:31,070 --> 00:35:35,600
applications to Cloud Foundry

00:35:32,330 --> 00:35:38,120
application runtime like we saw

00:35:35,600 --> 00:35:40,480
we may want to create stateful services

00:35:38,120 --> 00:35:44,080
for use with these applications the

00:35:40,480 --> 00:35:46,400
foundry website at cloud foundry org

00:35:44,080 --> 00:35:52,550
provides a gateway to the cloud foundry

00:35:46,400 --> 00:35:55,330
ecosystem this website lists services

00:35:52,550 --> 00:35:58,580
that support the open service broker API

00:35:55,330 --> 00:36:01,970
you can create these services with a CF

00:35:58,580 --> 00:36:07,550
create service command from the Cloud

00:36:01,970 --> 00:36:10,100
Foundry CLI in this note it's all about

00:36:07,550 --> 00:36:12,050
the naming suppose that Dan and I are

00:36:10,100 --> 00:36:14,810
sharing a Kafka server as we do our

00:36:12,050 --> 00:36:16,370
development one day dan and I are both

00:36:14,810 --> 00:36:18,560
running our development environments and

00:36:16,370 --> 00:36:21,560
Dan notices that he's not receiving the

00:36:18,560 --> 00:36:25,100
messages he's expecting to see do you

00:36:21,560 --> 00:36:27,170
know why this might be happening we're

00:36:25,100 --> 00:36:28,610
using the same topic names the same

00:36:27,170 --> 00:36:31,400
consumer group names in the same

00:36:28,610 --> 00:36:33,290
topology names so Kafka happily load

00:36:31,400 --> 00:36:34,700
balances the partitions between our two

00:36:33,290 --> 00:36:37,460
running development environment

00:36:34,700 --> 00:36:39,620
instances this is obviously not what we

00:36:37,460 --> 00:36:42,740
want to happen and we fix this by

00:36:39,620 --> 00:36:45,550
creating a prefix for our topic names

00:36:42,740 --> 00:36:48,460
consumer group names and topology names

00:36:45,550 --> 00:36:51,020
we create these topics using a script

00:36:48,460 --> 00:36:52,730
that has the environment topic prefix

00:36:51,020 --> 00:36:56,270
the number of partitions we want to

00:36:52,730 --> 00:36:58,850
create the target Kafka cluster on which

00:36:56,270 --> 00:37:00,380
to create the topics whether we should

00:36:58,850 --> 00:37:02,690
delete the existing topics or just

00:37:00,380 --> 00:37:04,690
create the new ones so this script will

00:37:02,690 --> 00:37:10,490
create a collection of topics with the

00:37:04,690 --> 00:37:12,560
specified prefix this is an example of a

00:37:10,490 --> 00:37:15,710
monitoring system for a Kafka server

00:37:12,560 --> 00:37:18,740
environment this system revolves around

00:37:15,710 --> 00:37:21,950
creating JMX properties for the Kafka

00:37:18,740 --> 00:37:23,720
Brokers and exporting them configuring

00:37:21,950 --> 00:37:26,930
Prometheus to scrape the properties at a

00:37:23,720 --> 00:37:29,300
specified interval and accessing the

00:37:26,930 --> 00:37:34,130
properties from a graph on a dashboard

00:37:29,300 --> 00:37:35,630
to display an alert on the data now

00:37:34,130 --> 00:37:38,120
let's look at a streaming application

00:37:35,630 --> 00:37:40,100
that we have here which is has a

00:37:38,120 --> 00:37:42,860
topology with a graph of connected

00:37:40,100 --> 00:37:44,810
processors and state stores let's say

00:37:42,860 --> 00:37:46,910
that we want to break this application

00:37:44,810 --> 00:37:48,830
into two independently deployed

00:37:46,910 --> 00:37:51,380
applications

00:37:48,830 --> 00:37:54,530
we can sever our topology into two

00:37:51,380 --> 00:37:56,750
topologies notice that the two

00:37:54,530 --> 00:38:00,380
processors are now separated across our

00:37:56,750 --> 00:38:03,290
application boundary we can reconnect

00:38:00,380 --> 00:38:05,960
them by creating a sync in the first

00:38:03,290 --> 00:38:09,290
topology and sending its messages to a

00:38:05,960 --> 00:38:11,870
Kafka topic and then creating a source

00:38:09,290 --> 00:38:13,700
in the second topology that receives

00:38:11,870 --> 00:38:17,750
those messages and plug them into the

00:38:13,700 --> 00:38:20,660
second processor the topic that they

00:38:17,750 --> 00:38:22,430
share represents a contract between

00:38:20,660 --> 00:38:26,690
these two independently deployed

00:38:22,430 --> 00:38:28,670
applications we can use a technology

00:38:26,690 --> 00:38:31,160
like Avro to help us define this

00:38:28,670 --> 00:38:34,730
contract and we can even disallow

00:38:31,160 --> 00:38:36,200
breaking changes to the contract we can

00:38:34,730 --> 00:38:38,090
repeat this pattern for another

00:38:36,200 --> 00:38:42,320
independently deployed application and

00:38:38,090 --> 00:38:43,910
again as many times as we like notice

00:38:42,320 --> 00:38:46,550
that the dependent applications are

00:38:43,910 --> 00:38:48,890
receiving the messages and the producer

00:38:46,550 --> 00:38:52,460
does not need to be aware of how those

00:38:48,890 --> 00:38:55,390
messages are being used this pattern

00:38:52,460 --> 00:38:57,940
called receiver driven control-flow

00:38:55,390 --> 00:39:01,340
facilitates a pluggable architecture

00:38:57,940 --> 00:39:03,320
where we can add features to our system

00:39:01,340 --> 00:39:06,170
with very little risk of breaking

00:39:03,320 --> 00:39:08,210
existing functionality let's look at

00:39:06,170 --> 00:39:11,240
receiver driven control flow in a little

00:39:08,210 --> 00:39:13,370
bit more detail here we have another

00:39:11,240 --> 00:39:16,790
example with a topology that has an

00:39:13,370 --> 00:39:18,890
analyzer its internal implementation is

00:39:16,790 --> 00:39:20,480
shown as ellipses here and it's

00:39:18,890 --> 00:39:24,650
connected its inputs and outputs are

00:39:20,480 --> 00:39:26,870
connected to Kafka topics let's say that

00:39:24,650 --> 00:39:28,910
the business has ideas about how they

00:39:26,870 --> 00:39:31,430
would like to modify this analyzer and

00:39:28,910 --> 00:39:33,470
we want to test those ideas out we can

00:39:31,430 --> 00:39:36,170
deploy a second version of the analyzer

00:39:33,470 --> 00:39:39,380
and tap into the same message stream

00:39:36,170 --> 00:39:40,970
that feeds the first analyzer this has

00:39:39,380 --> 00:39:44,060
very little risk of breaking or

00:39:40,970 --> 00:39:46,580
modifying the first analyzer we can take

00:39:44,060 --> 00:39:48,620
the output of v2 send it to a state

00:39:46,580 --> 00:39:52,550
store and create a rest service so that

00:39:48,620 --> 00:39:54,800
we can look at what v2 is producing we

00:39:52,550 --> 00:39:56,840
can extend this further to compare the

00:39:54,800 --> 00:39:59,390
v1 and v2 results in store the

00:39:56,840 --> 00:40:01,100
comparison in the state store and we can

00:39:59,390 --> 00:40:02,099
build an angular application to display

00:40:01,100 --> 00:40:04,950
the

00:40:02,099 --> 00:40:06,900
in comparison results we could even

00:40:04,950 --> 00:40:10,890
email these results out on a periodic

00:40:06,900 --> 00:40:12,630
basis if we like let's say that we're

00:40:10,890 --> 00:40:16,049
happy with the results and we want to

00:40:12,630 --> 00:40:18,599
slowly phase v2 into production first

00:40:16,049 --> 00:40:21,119
step is to take the output of v1 and

00:40:18,599 --> 00:40:23,759
send it to a processor that we will use

00:40:21,119 --> 00:40:27,299
to suppress the v1 messages that we want

00:40:23,759 --> 00:40:30,859
to flow through the v2 analyzer the next

00:40:27,299 --> 00:40:33,479
step is to create a mechanism for

00:40:30,859 --> 00:40:34,920
declaring which nodes we want to flow

00:40:33,479 --> 00:40:37,769
through v1 and which ones we want to

00:40:34,920 --> 00:40:39,749
flow through v2 we do this by creating a

00:40:37,769 --> 00:40:43,170
rest service that we can call with the

00:40:39,749 --> 00:40:45,479
node and version that rest service will

00:40:43,170 --> 00:40:48,269
produce a message to a kafka topic with

00:40:45,479 --> 00:40:51,479
the node inversion we can then create a

00:40:48,269 --> 00:40:53,700
source a processor in a state store to

00:40:51,479 --> 00:40:56,670
collect and store the node version

00:40:53,700 --> 00:41:00,089
information and update the rest service

00:40:56,670 --> 00:41:02,549
to read from the state store now we can

00:41:00,089 --> 00:41:04,380
build an angular application that we can

00:41:02,549 --> 00:41:08,249
view the node and version information

00:41:04,380 --> 00:41:12,920
and we can also update it the final step

00:41:08,249 --> 00:41:15,989
in the process we suppress the v1

00:41:12,920 --> 00:41:19,349
results for the nodes we want to run

00:41:15,989 --> 00:41:22,529
through v2 and we create a processor to

00:41:19,349 --> 00:41:25,739
suppress the v2 results for the nodes we

00:41:22,529 --> 00:41:28,229
want to run through v1 so notice that

00:41:25,739 --> 00:41:30,630
the sync that's sending the output to

00:41:28,229 --> 00:41:32,999
the Kafka topic is now receiving

00:41:30,630 --> 00:41:35,450
messages from both the v1 and v2

00:41:32,999 --> 00:41:38,369
analyzer and the state store is

00:41:35,450 --> 00:41:42,089
determining which version gets output to

00:41:38,369 --> 00:41:44,460
that topic so we can just set one node

00:41:42,089 --> 00:41:46,289
to be v2 and see how it works

00:41:44,460 --> 00:41:48,390
if we like it we can start adding more

00:41:46,289 --> 00:41:51,630
nodes and then eventually we can push

00:41:48,390 --> 00:41:54,660
them all across so we've found that we

00:41:51,630 --> 00:41:57,569
can actually keep an updated diagram of

00:41:54,660 --> 00:41:59,579
these topologies the other thing that's

00:41:57,569 --> 00:42:01,289
nice about this is the diagram actually

00:41:59,579 --> 00:42:04,200
Maps directly to the underlying

00:42:01,289 --> 00:42:07,469
implementation so you can see a dashed

00:42:04,200 --> 00:42:09,630
boundary around part of the topology and

00:42:07,469 --> 00:42:12,569
call that a subsystem and it maps to a

00:42:09,630 --> 00:42:14,339
Java package in our project the items

00:42:12,569 --> 00:42:16,250
within that - boundary are actually

00:42:14,339 --> 00:42:18,780
classes in the java package

00:42:16,250 --> 00:42:20,580
so this actually solves one of the very

00:42:18,780 --> 00:42:22,800
difficult problems that we have when we

00:42:20,580 --> 00:42:24,450
bring new developers on or they can look

00:42:22,800 --> 00:42:28,130
at the diagram understand what it's

00:42:24,450 --> 00:42:28,130
doing and then find the Associated code

00:42:29,570 --> 00:42:35,520
finally we can access the state stores

00:42:33,780 --> 00:42:37,250
through a REST API we've talked about

00:42:35,520 --> 00:42:39,930
doing this let's look at how it works

00:42:37,250 --> 00:42:42,780
this is called interactive query and the

00:42:39,930 --> 00:42:44,490
Kafka streams documentation so here we

00:42:42,780 --> 00:42:46,710
have three instances of our application

00:42:44,490 --> 00:42:49,860
it's been deployed to the application

00:42:46,710 --> 00:42:53,220
runtime it contains a REST API and it's

00:42:49,860 --> 00:42:54,900
apology with the state store the

00:42:53,220 --> 00:42:56,850
application runtime will automatically

00:42:54,900 --> 00:42:59,400
create a load balancer in front of our

00:42:56,850 --> 00:43:02,060
rest services so when we call the REST

00:42:59,400 --> 00:43:04,620
API to ask for a particular key in value

00:43:02,060 --> 00:43:08,010
it will randomly select one of the

00:43:04,620 --> 00:43:09,750
instances if you look at the diagram the

00:43:08,010 --> 00:43:12,210
instance that's selected may or may not

00:43:09,750 --> 00:43:15,960
contain the state store that has the key

00:43:12,210 --> 00:43:18,180
of interest if the state store contains

00:43:15,960 --> 00:43:21,390
the key we can just return the key in a

00:43:18,180 --> 00:43:23,850
value from the state store if it doesn't

00:43:21,390 --> 00:43:25,500
we can issue a second rest call directly

00:43:23,850 --> 00:43:28,830
to the instance that has the key in

00:43:25,500 --> 00:43:31,320
value and return that the way we're able

00:43:28,830 --> 00:43:34,620
to do this is through metadata that's

00:43:31,320 --> 00:43:36,210
made available to us in the topology we

00:43:34,620 --> 00:43:38,160
get the topology to capture this

00:43:36,210 --> 00:43:39,990
metadata by setting a property called

00:43:38,160 --> 00:43:43,440
the application nut server property in

00:43:39,990 --> 00:43:45,930
the topology the rest of the mechanism

00:43:43,440 --> 00:43:50,040
has to do with dependency injection so

00:43:45,930 --> 00:43:51,840
we create a spring component class we

00:43:50,040 --> 00:43:55,260
save the running topology as a property

00:43:51,840 --> 00:43:57,290
in that class we inject that class into

00:43:55,260 --> 00:44:00,180
the constructor and our rest controller

00:43:57,290 --> 00:44:02,880
we pick up the metadata from the running

00:44:00,180 --> 00:44:04,530
topology and then we make the

00:44:02,880 --> 00:44:06,360
determination about whether we need to

00:44:04,530 --> 00:44:08,010
pick it up from the local state store or

00:44:06,360 --> 00:44:11,790
go to another instance to get the key in

00:44:08,010 --> 00:44:13,860
the value if it is in the state store

00:44:11,790 --> 00:44:15,810
from the running topology we can

00:44:13,860 --> 00:44:20,460
actually ask for the state store and do

00:44:15,810 --> 00:44:21,450
a get to get the value for the key all

00:44:20,460 --> 00:44:23,550
right now I'd like to turn it back over

00:44:21,450 --> 00:44:26,370
to Dan let's talk about running at scale

00:44:23,550 --> 00:44:28,550
and various logging techniques thanks

00:44:26,370 --> 00:44:28,550
Mike

00:44:30,350 --> 00:44:34,890
alright so we use Kafka in our Cloud

00:44:33,120 --> 00:44:37,680
Foundry environment for application

00:44:34,890 --> 00:44:42,990
logging and an Oracle feed for event

00:44:37,680 --> 00:44:44,580
comparison so we have a use case where

00:44:42,990 --> 00:44:46,320
we need a logging solution that we can

00:44:44,580 --> 00:44:49,170
monitor so when we were first developing

00:44:46,320 --> 00:44:52,170
and we had sometimes 3 app instances

00:44:49,170 --> 00:44:53,640
with three partitions we can just kind

00:44:52,170 --> 00:44:55,530
of watch the logging and see what was

00:44:53,640 --> 00:45:00,120
going on once we went up to scale with

00:44:55,530 --> 00:45:03,540
57 Athens's instances and thousands of

00:45:00,120 --> 00:45:06,240
nodes became impossible to follow the

00:45:03,540 --> 00:45:08,130
logging there is lots of solutions out

00:45:06,240 --> 00:45:10,500
there but we just used the built-in

00:45:08,130 --> 00:45:13,020
capabilities of Kafka for our logging

00:45:10,500 --> 00:45:15,980
framework it also works in our local

00:45:13,020 --> 00:45:20,940
instance instances because we use log4j

00:45:15,980 --> 00:45:23,130
there we then use Kafka connect to send

00:45:20,940 --> 00:45:24,870
the logging data to elasticsearch and

00:45:23,130 --> 00:45:30,090
from there we can have visualized using

00:45:24,870 --> 00:45:32,580
cabana we also have used case to follow

00:45:30,090 --> 00:45:35,040
a node through all the steps of a node

00:45:32,580 --> 00:45:37,530
analysis so we use the same logging

00:45:35,040 --> 00:45:40,020
framework we developed using Kafka and

00:45:37,530 --> 00:45:43,260
then we have a swagger endpoint where we

00:45:40,020 --> 00:45:47,490
can turn on and off the logging for that

00:45:43,260 --> 00:45:51,440
node so here's still has that stuff on

00:45:47,490 --> 00:45:53,220
it so here's a look at the Stratus UI

00:45:51,440 --> 00:45:56,040
provide by Cloud Foundry

00:45:53,220 --> 00:45:57,660
it's an aggregate of all 57 logs so if

00:45:56,040 --> 00:45:59,100
we're doing a couple thousand node

00:45:57,660 --> 00:46:01,320
analysis a minute you can just picture

00:45:59,100 --> 00:46:04,260
you can't read this it goes by too quick

00:46:01,320 --> 00:46:08,970
you can actually get to the logs for

00:46:04,260 --> 00:46:12,420
each of the instances of the app but you

00:46:08,970 --> 00:46:13,620
don't know what instance you're if

00:46:12,420 --> 00:46:15,180
you're a trying look at an individual

00:46:13,620 --> 00:46:19,700
node you won't know what instance that's

00:46:15,180 --> 00:46:21,660
gonna be hosted in alright so here's a

00:46:19,700 --> 00:46:23,430
you know it like Mike said we are a

00:46:21,660 --> 00:46:26,310
spring application and swagger enabled

00:46:23,430 --> 00:46:28,500
so we have this end point where we can

00:46:26,310 --> 00:46:30,000
turn on the logging for an individual

00:46:28,500 --> 00:46:33,420
node so usually we turn it on

00:46:30,000 --> 00:46:35,310
temporarily look at the node see what's

00:46:33,420 --> 00:46:37,110
going on and compared we can also at

00:46:35,310 --> 00:46:40,140
that point compared to existing app and

00:46:37,110 --> 00:46:41,850
then we turn it back off

00:46:40,140 --> 00:46:44,330
it's kind of the same pattern we saw

00:46:41,850 --> 00:46:48,540
before when you hit try it out on this

00:46:44,330 --> 00:46:53,280
screen it'll inject a message to a topic

00:46:48,540 --> 00:46:55,200
and then the processor on that topic

00:46:53,280 --> 00:46:56,640
will update the state store value for

00:46:55,200 --> 00:46:58,440
this particular node and say yep start

00:46:56,640 --> 00:47:02,570
logging this one so during the node

00:46:58,440 --> 00:47:05,070
analysis we'll write all the statements

00:47:02,570 --> 00:47:08,280
for that node all the logging statements

00:47:05,070 --> 00:47:10,500
here's a look at our partial look at our

00:47:08,280 --> 00:47:12,600
UI this is our application home page

00:47:10,500 --> 00:47:15,180
this is the UI where we can hit a node

00:47:12,600 --> 00:47:17,130
analysis so the green button therefore

00:47:15,180 --> 00:47:19,140
run once again there's other ways we

00:47:17,130 --> 00:47:23,400
kick off node analysis it's just one of

00:47:19,140 --> 00:47:27,120
them and then here's a a console

00:47:23,400 --> 00:47:31,410
consumer of the log the kafka topic we

00:47:27,120 --> 00:47:33,510
use for logging a single node so this is

00:47:31,410 --> 00:47:35,820
good enough sometimes you can follow the

00:47:33,510 --> 00:47:37,410
node there being that we usually turn on

00:47:35,820 --> 00:47:39,990
one or two nodes at a time you won't get

00:47:37,410 --> 00:47:44,000
overwhelmed with how many log messages

00:47:39,990 --> 00:47:47,490
are going by but to make it even more

00:47:44,000 --> 00:47:50,400
useful is we send all those log messages

00:47:47,490 --> 00:47:52,800
up to elastic and then from elastic we

00:47:50,400 --> 00:47:55,530
can use a visualization in Cabana this

00:47:52,800 --> 00:47:58,040
is just Cabana showing the messages as

00:47:55,530 --> 00:48:00,840
they came in there's other

00:47:58,040 --> 00:48:02,820
visualizations we can use in this

00:48:00,840 --> 00:48:05,610
display there's not actually the message

00:48:02,820 --> 00:48:08,250
either we had a blank out to cmts a node

00:48:05,610 --> 00:48:12,600
but usually it get to the real message

00:48:08,250 --> 00:48:15,390
that's in the log so with this we can

00:48:12,600 --> 00:48:17,220
query over time for the logs and we can

00:48:15,390 --> 00:48:20,280
do aggregates like the average response

00:48:17,220 --> 00:48:23,610
time of no request how many times

00:48:20,280 --> 00:48:25,260
various nodes have been had a node

00:48:23,610 --> 00:48:28,080
analysis done a whole lot of good

00:48:25,260 --> 00:48:34,380
information can come out of elastic and

00:48:28,080 --> 00:48:36,300
kibana so we do have a use case also as

00:48:34,380 --> 00:48:38,880
we run parallel between the existing app

00:48:36,300 --> 00:48:41,730
and our Kafka app to compare the results

00:48:38,880 --> 00:48:44,280
of the to the existing app stores its

00:48:41,730 --> 00:48:47,040
data and Oracle so we need to compare

00:48:44,280 --> 00:48:49,410
the Oracle data to the Kafka data so one

00:48:47,040 --> 00:48:51,990
solution we have is to trigger have a

00:48:49,410 --> 00:48:53,730
trigger on the Oracle tables that write

00:48:51,990 --> 00:48:54,000
to a Delta table so we'll have all the

00:48:53,730 --> 00:48:58,590
chain

00:48:54,000 --> 00:49:00,030
in that Delta Oracle table what's good

00:48:58,590 --> 00:49:02,040
about that solution is it doesn't affect

00:49:00,030 --> 00:49:05,550
the existing app either so we have our

00:49:02,040 --> 00:49:07,710
own set of tables for comparison and we

00:49:05,550 --> 00:49:10,170
can't get in any trouble with the

00:49:07,710 --> 00:49:13,590
current app writing to the main tables

00:49:10,170 --> 00:49:17,640
and we use Kafka Connect for the JBC

00:49:13,590 --> 00:49:21,620
JDBC publish of those deltas to a Kafka

00:49:17,640 --> 00:49:24,030
topic and then we have a streaming

00:49:21,620 --> 00:49:28,350
application that does the comparison for

00:49:24,030 --> 00:49:31,220
us so a quick look at that is say the

00:49:28,350 --> 00:49:34,860
a16 existing application is on the Left

00:49:31,220 --> 00:49:36,750
it finds an event and our needs to

00:49:34,860 --> 00:49:38,730
update an event so it updates the event

00:49:36,750 --> 00:49:41,340
tables we'll have trick a trigger on

00:49:38,730 --> 00:49:43,980
those tables which will push that Delta

00:49:41,340 --> 00:49:44,610
to a another table as we call them the

00:49:43,980 --> 00:49:47,280
Delta table

00:49:44,610 --> 00:49:51,450
Kafka Connect will see that record and

00:49:47,280 --> 00:49:54,720
push it to a Kafka topic so over in our

00:49:51,450 --> 00:49:56,160
Kafka application we do have our Kafka

00:49:54,720 --> 00:50:00,150
topic in our state stores with other

00:49:56,160 --> 00:50:04,700
events they those messages will also be

00:50:00,150 --> 00:50:07,440
pushed or read by the comparison app and

00:50:04,700 --> 00:50:09,480
we have a right now like a running total

00:50:07,440 --> 00:50:12,000
for instance of events so any event that

00:50:09,480 --> 00:50:15,840
happens in the existing app at any event

00:50:12,000 --> 00:50:17,220
that happens in the Kafka app we just

00:50:15,840 --> 00:50:18,870
keep a running total of them and if

00:50:17,220 --> 00:50:22,350
we're about in sync then we know

00:50:18,870 --> 00:50:24,300
everything's running okay there's other

00:50:22,350 --> 00:50:30,570
comparisons too that's just kind of a a

00:50:24,300 --> 00:50:35,040
main one that's going well that's it no

00:50:30,570 --> 00:50:36,840
mics up again okay in this final section

00:50:35,040 --> 00:50:38,640
of the talk we're gonna look at

00:50:36,840 --> 00:50:41,310
containers in the Cloud Foundry

00:50:38,640 --> 00:50:44,310
container runtime let's return to the

00:50:41,310 --> 00:50:47,250
running instance of our application look

00:50:44,310 --> 00:50:48,900
closely at its state stores state stores

00:50:47,250 --> 00:50:51,090
are fast because they use local data

00:50:48,900 --> 00:50:53,160
structures and they're durable because

00:50:51,090 --> 00:50:56,130
they can rebe rebuilt from backing

00:50:53,160 --> 00:50:57,930
changelog topics we've talked about how

00:50:56,130 --> 00:51:01,350
we can push these applications to the

00:50:57,930 --> 00:51:03,750
cloud foundry application run time when

00:51:01,350 --> 00:51:06,600
these instances restart and they come

00:51:03,750 --> 00:51:08,210
back up their disk storage will be blank

00:51:06,600 --> 00:51:10,920
that's called ephemeral store

00:51:08,210 --> 00:51:12,570
in this case we will have to reload the

00:51:10,920 --> 00:51:15,180
state stores from the backing changelog

00:51:12,570 --> 00:51:16,620
topic if instead we deploy our

00:51:15,180 --> 00:51:19,860
application to the cloud foundry

00:51:16,620 --> 00:51:23,280
container runtime we can use kubernetes

00:51:19,860 --> 00:51:25,490
stateful sets these debt sample sets

00:51:23,280 --> 00:51:27,840
have a number of advantages our

00:51:25,490 --> 00:51:29,550
instances will have stable persistent

00:51:27,840 --> 00:51:31,650
storage which means that when our

00:51:29,550 --> 00:51:34,590
instances are restarted and they come

00:51:31,650 --> 00:51:36,540
back up the disk storage from before the

00:51:34,590 --> 00:51:39,570
restart will be reattached to the new

00:51:36,540 --> 00:51:41,220
instance that came back up in this case

00:51:39,570 --> 00:51:43,160
we will not have to rebuild our state

00:51:41,220 --> 00:51:45,450
stores from the backing changelog topic

00:51:43,160 --> 00:51:48,570
we get a number of other benefits from

00:51:45,450 --> 00:51:52,380
stateful sets including stable unique

00:51:48,570 --> 00:51:54,480
Network identifiers order graceful

00:51:52,380 --> 00:51:57,810
deployment and scaling of our streaming

00:51:54,480 --> 00:51:59,250
application and ordered automated

00:51:57,810 --> 00:52:02,460
rolling updates of our streaming

00:51:59,250 --> 00:52:03,720
application so how do we deploy an

00:52:02,460 --> 00:52:06,360
application to the cloud foundry

00:52:03,720 --> 00:52:08,460
container runtime let's return to our

00:52:06,360 --> 00:52:11,100
deployment diagram for the cloud foundry

00:52:08,460 --> 00:52:12,990
application runtime and look at how we

00:52:11,100 --> 00:52:17,820
modify it to support the container

00:52:12,990 --> 00:52:19,620
runtime the Cloud Foundry container

00:52:17,820 --> 00:52:21,270
runtime provides us with a kubernetes

00:52:19,620 --> 00:52:24,720
cluster that we can use to run our

00:52:21,270 --> 00:52:27,210
applications to deploy into a kubernetes

00:52:24,720 --> 00:52:30,330
cluster we are responsible for building

00:52:27,210 --> 00:52:32,400
our own containers we use docker to

00:52:30,330 --> 00:52:36,030
create container images of our

00:52:32,400 --> 00:52:40,110
application we push these images to a

00:52:36,030 --> 00:52:43,020
harbour image registry then we define a

00:52:40,110 --> 00:52:45,090
helm cart that identifies the number of

00:52:43,020 --> 00:52:47,970
instances that we want for our stateful

00:52:45,090 --> 00:52:51,390
set and references our container image

00:52:47,970 --> 00:52:53,640
in the harbour image registry then we

00:52:51,390 --> 00:52:57,240
can run helm install to deploy our

00:52:53,640 --> 00:53:00,120
application the Cloud Foundry

00:52:57,240 --> 00:53:03,240
application in container runtimes can be

00:53:00,120 --> 00:53:05,160
run separately or together when they are

00:53:03,240 --> 00:53:06,600
used together they can share

00:53:05,160 --> 00:53:10,230
cross-cutting concerns like

00:53:06,600 --> 00:53:12,240
authentication authorization and service

00:53:10,230 --> 00:53:15,650
access to the services that we created

00:53:12,240 --> 00:53:15,650
with the service broker API

00:53:17,869 --> 00:53:22,759
so as a development team at Comcast we

00:53:20,719 --> 00:53:25,579
just requests and are given a kubernetes

00:53:22,759 --> 00:53:27,380
cluster but now next let's look behind

00:53:25,579 --> 00:53:29,089
the scenes and how these kubernetes

00:53:27,380 --> 00:53:33,589
clusters are created by the Cloud

00:53:29,089 --> 00:53:36,529
Foundry container runtime here we have a

00:53:33,589 --> 00:53:38,029
kubernetes cluster kubernetes is an open

00:53:36,529 --> 00:53:40,700
source best-of-breed

00:53:38,029 --> 00:53:42,380
container orchestration engine that's

00:53:40,700 --> 00:53:46,599
provided by the cloud native computing

00:53:42,380 --> 00:53:48,739
foundation or CN CF our development team

00:53:46,599 --> 00:53:51,559
accesses the cluster through the

00:53:48,739 --> 00:53:54,349
kubernetes api we can deploy our

00:53:51,559 --> 00:53:56,690
applications through this API using helm

00:53:54,349 --> 00:54:01,430
charts or directly using the coop

00:53:56,690 --> 00:54:04,549
control CLI kubernetes exposes a number

00:54:01,430 --> 00:54:06,140
of resources tractions shown here the

00:54:04,549 --> 00:54:10,160
most important for our purposes is the

00:54:06,140 --> 00:54:13,910
stateful set inside the kubernetes

00:54:10,160 --> 00:54:17,229
cluster we have master at CD and worker

00:54:13,910 --> 00:54:19,969
services running on multiple computers

00:54:17,229 --> 00:54:23,599
consider the operational effort required

00:54:19,969 --> 00:54:25,670
to provision these computers to install

00:54:23,599 --> 00:54:28,999
and configure the operating systems on

00:54:25,670 --> 00:54:32,509
each and to install the master at CD and

00:54:28,999 --> 00:54:35,089
worker services consider the additional

00:54:32,509 --> 00:54:37,369
operational effort required to scale the

00:54:35,089 --> 00:54:40,249
cluster up or down to patch the

00:54:37,369 --> 00:54:41,930
operating systems or to invade the

00:54:40,249 --> 00:54:45,349
cluster as new versions of kubernetes

00:54:41,930 --> 00:54:47,900
are released Google also faced this

00:54:45,349 --> 00:54:52,029
problem and developed an internal system

00:54:47,900 --> 00:54:55,099
called Borg to address these needs later

00:54:52,029 --> 00:54:57,200
VMware's Palmer its recruited several of

00:54:55,099 --> 00:55:01,849
these engineers to create a refactored

00:54:57,200 --> 00:55:03,890
version of board called bosh which is

00:55:01,849 --> 00:55:06,890
the Borg name with the R and the G

00:55:03,890 --> 00:55:13,759
incremented by one letter so it's board

00:55:06,890 --> 00:55:16,789
plus plus Bosch is an open-source tool

00:55:13,759 --> 00:55:19,099
chain for deployment and lifecycle

00:55:16,789 --> 00:55:23,599
management of large scale distributed

00:55:19,099 --> 00:55:27,950
systems like kubernetes we control Bosch

00:55:23,599 --> 00:55:30,259
by issuing commands to the Bosch CLI one

00:55:27,950 --> 00:55:31,450
of the key benefits of Bosch is the

00:55:30,259 --> 00:55:34,390
ability

00:55:31,450 --> 00:55:36,910
for deployment portability through

00:55:34,390 --> 00:55:39,160
infrastructure abstractions of servers

00:55:36,910 --> 00:55:42,250
storage and network provisioning and

00:55:39,160 --> 00:55:44,320
management Bosch groups these AI as

00:55:42,250 --> 00:55:47,490
abstractions into an interface called

00:55:44,320 --> 00:55:47,490
the cloud provider interface

00:55:49,770 --> 00:55:56,860
implementations of this CPI for private

00:55:53,650 --> 00:55:59,440
and public cloud are what allow Bosch to

00:55:56,860 --> 00:56:02,920
deploy automate deployments to a large

00:55:59,440 --> 00:56:09,250
number of private and public cloud

00:56:02,920 --> 00:56:12,310
backends using the CPI Vash provisions

00:56:09,250 --> 00:56:13,930
the virtual machines and installs the

00:56:12,310 --> 00:56:15,790
operating system from an operating

00:56:13,930 --> 00:56:19,090
system image called a stem cell

00:56:15,790 --> 00:56:22,620
in addition Bosch installs an agent

00:56:19,090 --> 00:56:25,420
service on each of these VMs that Bosch

00:56:22,620 --> 00:56:29,470
director can use to monitor the health

00:56:25,420 --> 00:56:31,570
of the VMS Bosch can automatically

00:56:29,470 --> 00:56:34,240
recover from a VM failure by either

00:56:31,570 --> 00:56:38,950
repairing the VM or provisioning a

00:56:34,240 --> 00:56:41,850
replacement the cloud foundry container

00:56:38,950 --> 00:56:44,890
runtime is what's called a Bosch release

00:56:41,850 --> 00:56:49,210
this is a set of jobs for installing the

00:56:44,890 --> 00:56:52,390
master at CD and work our services the

00:56:49,210 --> 00:56:56,410
CFC our Bosch release is guided by a

00:56:52,390 --> 00:56:58,870
deployment manifest which specifies the

00:56:56,410 --> 00:57:02,830
number of instances of each job that we

00:56:58,870 --> 00:57:04,830
would like to create using the CFC our

00:57:02,830 --> 00:57:08,110
release and the deployment manifest

00:57:04,830 --> 00:57:12,640
Bosch can install the master at CD and

00:57:08,110 --> 00:57:15,910
worker nodes in the cluster the worker

00:57:12,640 --> 00:57:22,180
nodes actually contain the containers in

00:57:15,910 --> 00:57:24,370
our staple set if a new version of

00:57:22,180 --> 00:57:26,620
kubernetes is released Bosch can perform

00:57:24,370 --> 00:57:30,370
a rolling update at the cluster without

00:57:26,620 --> 00:57:32,500
bringing down the applications Bosch can

00:57:30,370 --> 00:57:34,720
also deploy a rolling update of the stem

00:57:32,500 --> 00:57:37,510
cell OS as new versions of the operating

00:57:34,720 --> 00:57:39,820
system are released if we want to scale

00:57:37,510 --> 00:57:43,800
the cluster up or down we can just edit

00:57:39,820 --> 00:57:43,800
the deployment manifest and redeploy

00:57:44,350 --> 00:57:48,610
here we're showing the use of Bosch for

00:57:47,020 --> 00:57:52,030
deploying that Cloud Foundry container

00:57:48,610 --> 00:57:54,130
runtime really bas-reliefs many other

00:57:52,030 --> 00:57:56,620
BOTS release of releases have been

00:57:54,130 --> 00:58:01,630
created for deploying systems like my

00:57:56,620 --> 00:58:04,060
sequel Redis Kafka and more now let's

00:58:01,630 --> 00:58:07,180
return to our kubernetes cluster with

00:58:04,060 --> 00:58:09,760
our deployed application streaming

00:58:07,180 --> 00:58:11,920
application notice that it's accessing

00:58:09,760 --> 00:58:14,260
the Kafka server outside of the

00:58:11,920 --> 00:58:16,300
kubernetes cluster what if we wanted to

00:58:14,260 --> 00:58:19,500
deploy our Kafka server inside the

00:58:16,300 --> 00:58:21,760
kubernetes cluster at this point in time

00:58:19,500 --> 00:58:23,290
for production deployments I would

00:58:21,760 --> 00:58:25,510
actually recommend deploying Kafka

00:58:23,290 --> 00:58:28,750
server onto its own physical servers or

00:58:25,510 --> 00:58:31,710
VMs if physicals aren't available but in

00:58:28,750 --> 00:58:34,420
the not-too-distant future it may be

00:58:31,710 --> 00:58:36,420
recommended production practice to

00:58:34,420 --> 00:58:38,740
deploy Kafka into the kubernetes cluster

00:58:36,420 --> 00:58:41,920
let's look at a couple of ways to deploy

00:58:38,740 --> 00:58:43,660
Kafka server into kubernetes the first

00:58:41,920 --> 00:58:46,180
is very similar to how we deploy our own

00:58:43,660 --> 00:58:49,030
applications we start with docker images

00:58:46,180 --> 00:58:52,860
of the Kafka server services zookeeper

00:58:49,030 --> 00:58:55,510
the Kafka broker and the Kafka connect

00:58:52,860 --> 00:58:58,450
we use corresponding helm charts for

00:58:55,510 --> 00:59:00,490
zookeeper Kafka and Kafka connect and a

00:58:58,450 --> 00:59:03,820
Helmut art of charts to install all of

00:59:00,490 --> 00:59:06,070
the services with the single command the

00:59:03,820 --> 00:59:08,470
result is three stateful sets one for

00:59:06,070 --> 00:59:12,550
zookeeper one for the Kafka brokers and

00:59:08,470 --> 00:59:14,560
one for Kafka connect the second way to

00:59:12,550 --> 00:59:17,610
deploy Kafka server into a kubernetes

00:59:14,560 --> 00:59:21,760
cluster is even more exciting than that

00:59:17,610 --> 00:59:24,790
Kafka has kubernetes has a concept of a

00:59:21,760 --> 00:59:27,670
kubernetes operator an operator is a

00:59:24,790 --> 00:59:29,920
method of packaging deploying and

00:59:27,670 --> 00:59:33,250
managing an application a kubernetes

00:59:29,920 --> 00:59:35,170
application a kubernetes application is

00:59:33,250 --> 00:59:36,970
an application that's both deployed on

00:59:35,170 --> 00:59:41,400
kubernetes and managed through the

00:59:36,970 --> 00:59:43,810
kubernetes api or coop control tooling

00:59:41,400 --> 00:59:46,390
confluent is developing an Kafka

00:59:43,810 --> 00:59:50,200
operator for kubernetes which has a

00:59:46,390 --> 00:59:52,810
number of benefits confluent has used

00:59:50,200 --> 00:59:55,810
their operational knowledge of deploying

00:59:52,810 --> 00:59:57,849
Kafka at scale in cloud environments to

00:59:55,810 --> 01:00:01,420
automate the operation

00:59:57,849 --> 01:00:03,789
tasks in the kafka operator they can

01:00:01,420 --> 01:00:06,039
perform rolling updates on the Kafka

01:00:03,789 --> 01:00:08,979
server as new versions of Kafka are

01:00:06,039 --> 01:00:11,410
released they can automate adding and

01:00:08,979 --> 01:00:14,319
removing brokers and rebalancing the

01:00:11,410 --> 01:00:16,449
partitions across the brokers and can

01:00:14,319 --> 01:00:20,289
also perform a number of internal and

01:00:16,449 --> 01:00:22,209
external configuration tasks so we can

01:00:20,289 --> 01:00:24,400
deploy Kafka server into our kubernetes

01:00:22,209 --> 01:00:27,729
cluster what else could we deploy in

01:00:24,400 --> 01:00:30,479
there how about our entire application

01:00:27,729 --> 01:00:33,130
and all of the supporting subsystems

01:00:30,479 --> 01:00:34,959
this is a future state for sure but

01:00:33,130 --> 01:00:39,219
something we will be keeping a close eye

01:00:34,959 --> 01:00:41,819
on before we conclude I just like to say

01:00:39,219 --> 01:00:43,900
a few words about comcast open source

01:00:41,819 --> 01:00:47,079
I'm very proud of the open source

01:00:43,900 --> 01:00:49,479
efforts of Comcast Comcast encourages

01:00:47,079 --> 01:00:52,019
engineers like myself to open source the

01:00:49,479 --> 01:00:57,269
projects that we develop at Comcast

01:00:52,019 --> 01:01:00,279
Comcast introduces and open standards

01:00:57,269 --> 01:01:03,400
groups including the Linux Foundation

01:01:00,279 --> 01:01:06,430
the OpenStack foundation the Apache

01:01:03,400 --> 01:01:08,319
foundation Cloud Foundry Foundation and

01:01:06,430 --> 01:01:13,719
the internet Engineering Task Force just

01:01:08,319 --> 01:01:15,309
to name a few comcast is committed to

01:01:13,719 --> 01:01:17,940
being part of the global open-source

01:01:15,309 --> 01:01:20,319
community now and into the future

01:01:17,940 --> 01:01:21,999
if being a part of this vision appeals

01:01:20,319 --> 01:01:23,109
to you feel free to stop by the Comcast

01:01:21,999 --> 01:01:26,079
booth and we can talk about

01:01:23,109 --> 01:01:27,579
opportunities dan and I would like to

01:01:26,079 --> 01:01:29,559
thank you for your attention and for

01:01:27,579 --> 01:01:32,190
being part of the Cloud Foundry open

01:01:29,559 --> 01:01:32,190
source community

01:01:37,869 --> 01:01:41,810
questions

01:01:39,140 --> 01:01:56,599
Oh mind I guess we can take questions if

01:01:41,810 --> 01:01:59,359
anybody has any yes okay so yeah so we

01:01:56,599 --> 01:02:02,150
have physical collectors repeat the

01:01:59,359 --> 01:02:04,670
question oh yeah sorry so the question

01:02:02,150 --> 01:02:06,890
is how big are the kafka clusters that

01:02:04,670 --> 01:02:10,040
we use in our in our implementation that

01:02:06,890 --> 01:02:13,609
Comcast so we have six physical servers

01:02:10,040 --> 01:02:15,980
for the kafka brokers and this actually

01:02:13,609 --> 01:02:20,530
performs extremely well we haven't hit

01:02:15,980 --> 01:02:26,450
any capacity limits on on those servers

01:02:20,530 --> 01:02:28,369
you know we have a process called

01:02:26,450 --> 01:02:30,730
whopper that runs through a complete

01:02:28,369 --> 01:02:34,970
node analysis of all 40 million devices

01:02:30,730 --> 01:02:37,760
every four hours that's running on top

01:02:34,970 --> 01:02:40,520
of that infrastructure and like Dan said

01:02:37,760 --> 01:02:47,150
we have 57 instances at this point of

01:02:40,520 --> 01:02:50,030
our application yes the the Kafka

01:02:47,150 --> 01:02:55,910
streams spring java applications run in

01:02:50,030 --> 01:02:58,160
Cloud Foundry application runtime and

01:02:55,910 --> 01:03:00,020
just kind of as a technical note where

01:02:58,160 --> 01:03:03,140
we originally thought it would be good

01:03:00,020 --> 01:03:04,760
to have sort of odd number or you know

01:03:03,140 --> 01:03:07,460
prime number for our number of

01:03:04,760 --> 01:03:10,069
partitions and instances but in

01:03:07,460 --> 01:03:12,680
hindsight I think power of two is

01:03:10,069 --> 01:03:15,940
probably a better choice right because

01:03:12,680 --> 01:03:19,220
then you can start out like with 64

01:03:15,940 --> 01:03:20,750
partitions and four instances and then

01:03:19,220 --> 01:03:21,680
you can double it double it and double

01:03:20,750 --> 01:03:23,119
it and you have the same number of

01:03:21,680 --> 01:03:29,720
partitions being allocated to each

01:03:23,119 --> 01:03:31,910
instance yeah I think we actually run in

01:03:29,720 --> 01:03:34,630
57 just because we thought maybe not

01:03:31,910 --> 01:03:34,630
number would be good

01:03:37,700 --> 01:03:40,700
yeah

01:03:41,510 --> 01:03:46,710
right so when you configure a state I'm

01:03:45,269 --> 01:03:49,470
sorry the question is what are we using

01:03:46,710 --> 01:03:52,109
is our state store mechanism because

01:03:49,470 --> 01:03:53,940
kafka state stores can be in memory

01:03:52,109 --> 01:03:55,950
state stores or they can be on disk

01:03:53,940 --> 01:03:58,200
state stores and the on disk state

01:03:55,950 --> 01:03:59,579
stores use rocks DB as the mechanism and

01:03:58,200 --> 01:04:01,680
it's really a simple data structure it's

01:03:59,579 --> 01:04:05,880
a key and a value right so it's just a

01:04:01,680 --> 01:04:10,529
key value lookup so my understanding is

01:04:05,880 --> 01:04:12,390
that when the JVM memory reaches a

01:04:10,529 --> 01:04:14,339
certain limit and will actually default

01:04:12,390 --> 01:04:18,029
automatically over to on disk storage

01:04:14,339 --> 01:04:20,069
using rocks DB so that you don't need to

01:04:18,029 --> 01:04:21,930
specify I'm using one or the other it'll

01:04:20,069 --> 01:04:26,940
just overflow onto the disk when it

01:04:21,930 --> 01:04:29,010
needs to we don't specify anything in

01:04:26,940 --> 01:04:30,720
configuration but I think it overflows

01:04:29,010 --> 01:04:45,150
to the disk and when it does it uses

01:04:30,720 --> 01:04:49,680
rocks TP we've been working on this for

01:04:45,150 --> 01:04:52,230
a while most of the concepts that that

01:04:49,680 --> 01:04:53,730
you see in here I would say we had you

01:04:52,230 --> 01:04:55,650
know functional prototypes working

01:04:53,730 --> 01:04:57,599
pretty rapidly on a laptop like that's

01:04:55,650 --> 01:05:00,210
actually one of the really good benefits

01:04:57,599 --> 01:05:03,240
of this architecture is you can take a

01:05:00,210 --> 01:05:05,250
laptop and you know in a day have it set

01:05:03,240 --> 01:05:08,069
up and running kafka streams app against

01:05:05,250 --> 01:05:10,200
a local Kafka's server create topics and

01:05:08,069 --> 01:05:12,109
you know start experimenting with it and

01:05:10,200 --> 01:05:14,400
that actually is one of the key

01:05:12,109 --> 01:05:16,289
decision-making factors for me when I'm

01:05:14,400 --> 01:05:18,390
evaluating something is can I learn it

01:05:16,289 --> 01:05:20,670
can I experiment with it you know can I

01:05:18,390 --> 01:05:22,230
test it out validate that it's actually

01:05:20,670 --> 01:05:26,250
as good as it looks in the PowerPoint

01:05:22,230 --> 01:05:29,759
slides but I'd say we've been working on

01:05:26,250 --> 01:05:30,960
this for a year and we're now running in

01:05:29,759 --> 01:05:33,569
production in parallel with our

01:05:30,960 --> 01:05:36,390
production system and getting really

01:05:33,569 --> 01:05:38,400
good results at a base app with the

01:05:36,390 --> 01:05:41,430
concepts was probably three months and

01:05:38,400 --> 01:05:42,930
that was learning Kafka you know learn

01:05:41,430 --> 01:05:45,210
how to enter how it interacts with our

01:05:42,930 --> 01:05:46,769
app so just to run on our laps house was

01:05:45,210 --> 01:05:48,800
pretty quick and to scale it up took a

01:05:46,769 --> 01:05:50,300
little bit longer but all the

01:05:48,800 --> 01:05:51,980
concepts for the same like I said

01:05:50,300 --> 01:05:54,020
there's only a few of them to get all

01:05:51,980 --> 01:05:56,060
our use cases done and you know as we

01:05:54,020 --> 01:05:57,800
look at the next use case we're like my

01:05:56,060 --> 01:05:59,570
way we can solve it with a very similar

01:05:57,800 --> 01:06:03,520
pattern we already had so it was very

01:05:59,570 --> 01:06:05,900
very quick development yeah I'd say in

01:06:03,520 --> 01:06:07,310
three to six months if you're actually

01:06:05,900 --> 01:06:08,900
starting up and you're able to deploy

01:06:07,310 --> 01:06:10,880
your Kafka server into a kubernetes

01:06:08,900 --> 01:06:12,800
cluster and you have a kubernetes

01:06:10,880 --> 01:06:30,260
cluster I think your time frame could be

01:06:12,800 --> 01:06:33,860
compressed significantly so the question

01:06:30,260 --> 01:06:36,440
is we're recommending that you use

01:06:33,860 --> 01:06:39,380
physical servers for Kafka or the Kafka

01:06:36,440 --> 01:06:41,410
servers and the question is why are we

01:06:39,380 --> 01:06:44,660
recommending that over deploying the

01:06:41,410 --> 01:06:47,330
kubernetes cluster and and really it's

01:06:44,660 --> 01:06:49,250
just based on what we see out there in

01:06:47,330 --> 01:06:50,810
the marketplace like confluent is

01:06:49,250 --> 01:06:53,030
developing these things but they're all

01:06:50,810 --> 01:06:55,040
like pre-production like you know there

01:06:53,030 --> 01:06:57,500
are docker container images out there of

01:06:55,040 --> 01:06:59,540
these services but or specifically

01:06:57,500 --> 01:07:01,730
marked as being not for production use

01:06:59,540 --> 01:07:04,460
at the moment like you know the

01:07:01,730 --> 01:07:06,380
confluent Kafka operator is under

01:07:04,460 --> 01:07:08,450
development and you can get early access

01:07:06,380 --> 01:07:09,830
to it but it's not you know it hasn't

01:07:08,450 --> 01:07:13,040
been out there and had a chance to bake

01:07:09,830 --> 01:07:17,480
yet I really think ultimately that will

01:07:13,040 --> 01:07:18,800
be the you know the good path is to you

01:07:17,480 --> 01:07:22,430
know to be able to deploy all these

01:07:18,800 --> 01:07:24,350
things into a single environment it has

01:07:22,430 --> 01:07:27,170
some nice security benefits like Sergey

01:07:24,350 --> 01:07:28,550
was talking about you kind of create

01:07:27,170 --> 01:07:31,280
this perimeter around all of your

01:07:28,550 --> 01:07:34,010
sensitive information and then you only

01:07:31,280 --> 01:07:38,060
have to defend the ingress and egress

01:07:34,010 --> 01:07:40,160
from the the outside world right so that

01:07:38,060 --> 01:07:45,020
there's a much smaller attack surface

01:07:40,160 --> 01:07:47,380
and things like that yeah all right

01:07:45,020 --> 01:07:47,380
thanks a lot

01:07:49,010 --> 01:07:51,429

YouTube URL: https://www.youtube.com/watch?v=oVvoqg-NiKU


