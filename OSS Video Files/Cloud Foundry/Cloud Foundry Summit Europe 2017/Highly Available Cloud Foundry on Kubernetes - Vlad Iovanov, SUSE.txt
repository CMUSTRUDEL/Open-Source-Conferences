Title: Highly Available Cloud Foundry on Kubernetes - Vlad Iovanov, SUSE
Publication date: 2017-10-18
Playlist: Cloud Foundry Summit Europe 2017
Description: 
	Highly Available Cloud Foundry on Kubernetes - Vlad Iovanov, SUSE

In this talk the presenters will present how SUSE Cloud Foundry is deployed and configured to run in a highly available fashion. We will show how we avoid SPOFs using Kubernetes features like stateful sets, readiness and liveness probes, etc. Furthermore, we will show how the end user of SCF can deploy applications without worrying about downtime. The presentation will include a demo of a disruptive agent simulating failures across the Kubernetes nodes and containers, while user applications are still live and healthy.

About Vlad Iovanov
Vlad Iovanov is currently working as a Technical Lead on the SUSE Cloud Foundry project at SUSE. He has given various talks in industry for topics ranging from Container technologies, Windows framework and Best practices for Application Development.
Captions: 
	00:00:00,030 --> 00:00:06,029
hello everyone welcome to the session

00:00:03,149 --> 00:00:09,960
about highly available cloud foundry on

00:00:06,029 --> 00:00:13,710
cube in vladimirov I work as a technical

00:00:09,960 --> 00:00:17,460
lead for cloud foundry at Sousa and I'm

00:00:13,710 --> 00:00:20,130
gonna talk to you today about Susy cloud

00:00:17,460 --> 00:00:22,800
application platform how we've

00:00:20,130 --> 00:00:26,760
containerized it and how we're trying to

00:00:22,800 --> 00:00:28,650
make it highly available so what is it

00:00:26,760 --> 00:00:31,859
it's it's a platform as a service that's

00:00:28,650 --> 00:00:34,290
based on cloud foundry runs applications

00:00:31,859 --> 00:00:37,590
has a large number of components just

00:00:34,290 --> 00:00:40,219
like cloud foundry does and it's

00:00:37,590 --> 00:00:49,829
deployed on cube using help

00:00:40,219 --> 00:00:55,110
so probably everyone here knows a few of

00:00:49,829 --> 00:00:57,930
these components its cap is made of the

00:00:55,110 --> 00:00:59,879
exact same stuff as cloud foundry yes

00:00:57,930 --> 00:01:03,000
it's actually built from the same

00:00:59,879 --> 00:01:05,909
sources so we have here the exact same

00:01:03,000 --> 00:01:08,479
pieces as Club foundry and this is a

00:01:05,909 --> 00:01:14,220
view that shows them kind of grouped by

00:01:08,479 --> 00:01:16,650
by their function and every container in

00:01:14,220 --> 00:01:21,420
the system is actually represented by a

00:01:16,650 --> 00:01:23,280
green box so again same cloud foundry

00:01:21,420 --> 00:01:27,330
that you know and love just

00:01:23,280 --> 00:01:30,600
containerized and we can see that there

00:01:27,330 --> 00:01:32,930
are quite a few components so container

00:01:30,600 --> 00:01:36,659
izing them is only part of the story

00:01:32,930 --> 00:01:38,430
that can happen automatically and once

00:01:36,659 --> 00:01:41,520
you do that it becomes easy but actually

00:01:38,430 --> 00:01:43,680
running it and orchestrating it can be

00:01:41,520 --> 00:01:46,590
difficult and making that happen

00:01:43,680 --> 00:01:52,409
in highly available fashion can be even

00:01:46,590 --> 00:01:57,259
more difficult so next we hope you

00:01:52,409 --> 00:01:59,880
realize that some boxes have turned red

00:01:57,259 --> 00:02:03,329
so it's useful to think about the

00:01:59,880 --> 00:02:05,640
critical pieces of the system that you

00:02:03,329 --> 00:02:09,270
want to be highly available more than

00:02:05,640 --> 00:02:10,460
anything else so in this case you can

00:02:09,270 --> 00:02:13,580
see that the less

00:02:10,460 --> 00:02:17,750
room time is read and routing is read as

00:02:13,580 --> 00:02:20,180
well so basically these are the things

00:02:17,750 --> 00:02:22,370
that run your applications and also

00:02:20,180 --> 00:02:26,020
route traffic to them and this is very

00:02:22,370 --> 00:02:30,410
important you you want the system to

00:02:26,020 --> 00:02:32,330
gracefully degrade but the application

00:02:30,410 --> 00:02:34,760
and of course traffic flowing through

00:02:32,330 --> 00:02:38,870
them should still be online so this is

00:02:34,760 --> 00:02:40,760
the most important part of of thought

00:02:38,870 --> 00:02:42,440
foundry let's say you want the

00:02:40,760 --> 00:02:45,290
applications to stay online it's okay if

00:02:42,440 --> 00:02:46,970
you can't deploy applications for a few

00:02:45,290 --> 00:02:49,700
minutes or for an hour while while you

00:02:46,970 --> 00:02:51,980
deal with with the problem that that

00:02:49,700 --> 00:02:55,220
occurred but you want applications to

00:02:51,980 --> 00:02:58,010
stay online and being aware of these

00:02:55,220 --> 00:03:01,190
critical pieces is important because it

00:02:58,010 --> 00:03:05,840
drives us to make specific decisions on

00:03:01,190 --> 00:03:08,540
how we locate components for example we

00:03:05,840 --> 00:03:11,120
have special affinity and anti affinity

00:03:08,540 --> 00:03:15,610
rules for the router and for the Diego

00:03:11,120 --> 00:03:22,790
cells that make make them run better

00:03:15,610 --> 00:03:25,490
separately ok so how are we

00:03:22,790 --> 00:03:26,690
containerized in cloud foundry we all

00:03:25,490 --> 00:03:31,550
know that cloud foundry is usually

00:03:26,690 --> 00:03:33,710
deployed using Bosh and barges the tool

00:03:31,550 --> 00:03:37,340
chain that allows deployment of highly

00:03:33,710 --> 00:03:38,990
complicated systems on top of VMs but we

00:03:37,340 --> 00:03:41,870
wanted to do containers on top of

00:03:38,990 --> 00:03:44,840
kubernetes so we we had to develop this

00:03:41,870 --> 00:03:48,650
tool called Fissel to essentially

00:03:44,840 --> 00:03:51,080
convert Bosh releases into docker images

00:03:48,650 --> 00:03:53,620
and we're still using Bosh

00:03:51,080 --> 00:03:58,310
so Bosh takes you from your source code

00:03:53,620 --> 00:04:03,290
to your end and today deployment in your

00:03:58,310 --> 00:04:06,110
environment but a lot of Bosh is how you

00:04:03,290 --> 00:04:09,860
package your sources so packages jobs

00:04:06,110 --> 00:04:13,070
stem cells and so on so we use all that

00:04:09,860 --> 00:04:15,200
release information respect for the

00:04:13,070 --> 00:04:18,519
packages for the jobs how you compile

00:04:15,200 --> 00:04:21,129
things we use all that information that

00:04:18,519 --> 00:04:24,340
create the container images and we

00:04:21,129 --> 00:04:27,849
actually built the stem cell just like

00:04:24,340 --> 00:04:30,550
you build a normal bar stem cell so at

00:04:27,849 --> 00:04:34,150
some point during the Bosch stem cell VM

00:04:30,550 --> 00:04:37,240
creation process there's a split

00:04:34,150 --> 00:04:40,569
happening on the one side you you

00:04:37,240 --> 00:04:43,240
actually convert the OS image into a bar

00:04:40,569 --> 00:04:45,639
stem cell that's specific to the CPI

00:04:43,240 --> 00:04:48,789
that you're going to use or as your AWS

00:04:45,639 --> 00:04:51,220
and so on and for us on the other hand

00:04:48,789 --> 00:04:54,940
we turned that OS image into a docker

00:04:51,220 --> 00:04:58,840
image and that's the basis for all of

00:04:54,940 --> 00:05:02,680
the containers that make up our cloud

00:04:58,840 --> 00:05:05,849
application platform so essentially the

00:05:02,680 --> 00:05:08,800
same we just skip the CPI parts that

00:05:05,849 --> 00:05:16,240
that kind of turn each bar stem cell

00:05:08,800 --> 00:05:19,210
into a bio specific VM image and since

00:05:16,240 --> 00:05:23,530
we actually used the same exact sources

00:05:19,210 --> 00:05:26,349
so we use the capi release Diego release

00:05:23,530 --> 00:05:28,870
and so on we actually believe that we

00:05:26,349 --> 00:05:31,710
can be a certified distro by the

00:05:28,870 --> 00:05:31,710
foundation

00:05:34,370 --> 00:05:40,820
okay so going back a bit to

00:05:36,889 --> 00:05:43,780
high-availability we want to think about

00:05:40,820 --> 00:05:45,889
the mechanisms that we need to make each

00:05:43,780 --> 00:05:48,979
component of Cloud Foundry highly

00:05:45,889 --> 00:05:53,449
available and we kind of have two

00:05:48,979 --> 00:05:55,430
flavors that that we work with things

00:05:53,449 --> 00:05:57,710
that can be load balanced things like

00:05:55,430 --> 00:06:00,979
the cloud controller or the routers or

00:05:57,710 --> 00:06:04,430
the Diego cells and then you have things

00:06:00,979 --> 00:06:07,580
that cluster so for example my Maria DB

00:06:04,430 --> 00:06:10,520
or at CD or console these are a bit more

00:06:07,580 --> 00:06:13,010
special the actual replicas that you

00:06:10,520 --> 00:06:22,040
instantiate need to be aware of each

00:06:13,010 --> 00:06:24,380
other so for example replica number one

00:06:22,040 --> 00:06:26,900
of my sequel needs to have a specific

00:06:24,380 --> 00:06:30,020
address that you can reach it where you

00:06:26,900 --> 00:06:32,270
can reach it and if it goes down and

00:06:30,020 --> 00:06:35,000
comes back up it needs to come back up

00:06:32,270 --> 00:06:36,740
as my sequel one so other replicas in

00:06:35,000 --> 00:06:39,260
the network need to be able to identify

00:06:36,740 --> 00:06:43,580
it and it needs to be able to end the

00:06:39,260 --> 00:06:47,060
identify itself for the low bounce

00:06:43,580 --> 00:06:49,340
pieces you know you can just add more

00:06:47,060 --> 00:06:51,229
routers for the load balancer in front

00:06:49,340 --> 00:06:53,840
of them and it's it's all okay you don't

00:06:51,229 --> 00:06:56,599
need to specifically be able to identify

00:06:53,840 --> 00:07:00,139
a router and you can also start them up

00:06:56,599 --> 00:07:02,740
all at the same time so it's a bit

00:07:00,139 --> 00:07:08,770
easier to run these load balanced

00:07:02,740 --> 00:07:12,590
components rather than the clustered one

00:07:08,770 --> 00:07:14,960
next to these two flavors we also have

00:07:12,590 --> 00:07:18,080
components that follow an active/passive

00:07:14,960 --> 00:07:21,970
model so for example I think the Diego

00:07:18,080 --> 00:07:25,070
database does this you can run multiple

00:07:21,970 --> 00:07:27,710
replicas of it but only one will be

00:07:25,070 --> 00:07:29,810
active at at at the same time so they

00:07:27,710 --> 00:07:31,940
will connect to console they'll grab a

00:07:29,810 --> 00:07:33,349
lock from there one of them will be

00:07:31,940 --> 00:07:36,500
elected as the master and the other

00:07:33,349 --> 00:07:39,710
other ones will be passive meaning that

00:07:36,500 --> 00:07:41,720
whenever the master goes down one of the

00:07:39,710 --> 00:07:44,330
passives will will be promoted and

00:07:41,720 --> 00:07:46,430
they'll become the active component so

00:07:44,330 --> 00:07:48,080
we need to be able to support all these

00:07:46,430 --> 00:07:50,949
flavors all of

00:07:48,080 --> 00:07:55,400
configurations and this is where

00:07:50,949 --> 00:07:59,479
kubernetes comes in and we have various

00:07:55,400 --> 00:08:04,129
cube primitives that allow us to to

00:07:59,479 --> 00:08:11,569
configure a deployment a deployment of

00:08:04,129 --> 00:08:12,919
of cloud foundry so we have the fissile

00:08:11,569 --> 00:08:16,490
tool the light that I talked about

00:08:12,919 --> 00:08:19,870
earlier that actually turns Bosh

00:08:16,490 --> 00:08:23,719
releases into container images it also

00:08:19,870 --> 00:08:26,469
creates cube configs and Hum charts and

00:08:23,719 --> 00:08:28,599
home charts are basically templatized

00:08:26,469 --> 00:08:31,550
cube configs

00:08:28,599 --> 00:08:35,510
that describe everything you need in

00:08:31,550 --> 00:08:37,690
order to stand Cloud Foundry up so we

00:08:35,510 --> 00:08:42,890
have services storage classes

00:08:37,690 --> 00:08:46,519
deployments stateful sets probes of

00:08:42,890 --> 00:08:48,740
various kinds pods and I'm going to talk

00:08:46,519 --> 00:08:50,959
about a few of these in detail so you

00:08:48,740 --> 00:08:58,750
can understand how how we use them and

00:08:50,959 --> 00:09:02,870
what what they offer so first we have

00:08:58,750 --> 00:09:07,730
cube services and cube services describe

00:09:02,870 --> 00:09:09,560
how we want to talk to a component so

00:09:07,730 --> 00:09:13,040
for each of those components that you

00:09:09,560 --> 00:09:15,260
saw at the beginning we have services

00:09:13,040 --> 00:09:17,660
that describe how you talk to them and

00:09:15,260 --> 00:09:20,959
that includes the port the protocol that

00:09:17,660 --> 00:09:23,000
you use to talk to them and after you

00:09:20,959 --> 00:09:25,640
describe a service and you you can turn

00:09:23,000 --> 00:09:28,490
it on for for a pod

00:09:25,640 --> 00:09:32,839
you also get an address for that service

00:09:28,490 --> 00:09:36,410
for example you get console dot CF as we

00:09:32,839 --> 00:09:39,260
see dock cluster so that's a well known

00:09:36,410 --> 00:09:44,540
address for that service in our case

00:09:39,260 --> 00:09:48,170
console and for that port 8500 and this

00:09:44,540 --> 00:09:50,750
will actually also load-balanced so you

00:09:48,170 --> 00:09:54,140
create a service for the console

00:09:50,750 --> 00:09:57,260
component what you get is an address you

00:09:54,140 --> 00:09:59,480
get load balancing for it so we define

00:09:57,260 --> 00:10:00,960
multiple of these for each component and

00:09:59,480 --> 00:10:03,540
for each port and protocol

00:10:00,960 --> 00:10:05,250
that we need in the system so if you

00:10:03,540 --> 00:10:06,990
were to look at our home charts you'll

00:10:05,250 --> 00:10:13,590
you'll see a bunch of these services pop

00:10:06,990 --> 00:10:17,090
up and you can imagine that the load

00:10:13,590 --> 00:10:23,370
balancing is very useful for our h.a

00:10:17,090 --> 00:10:24,900
requirements so next stateful sets these

00:10:23,370 --> 00:10:26,910
are very important because they allow us

00:10:24,900 --> 00:10:31,350
to support clustered components like

00:10:26,910 --> 00:10:34,140
MariaDB in a cube deployment pod

00:10:31,350 --> 00:10:36,360
replicas have no real distinction you

00:10:34,140 --> 00:10:39,860
can't tell one from the other they get a

00:10:36,360 --> 00:10:41,640
random hostname when they start up and

00:10:39,860 --> 00:10:44,790
that's basically it

00:10:41,640 --> 00:10:47,640
and you have no control over when they

00:10:44,790 --> 00:10:49,800
start so if I want say 20 routers

00:10:47,640 --> 00:10:52,110
they'll all start up they'll be the pro

00:10:49,800 --> 00:10:55,560
deployed across kubernetes nodes and

00:10:52,110 --> 00:11:00,240
they'll eventually come up in whatever

00:10:55,560 --> 00:11:04,410
order in with a stateful set you

00:11:00,240 --> 00:11:06,720
actually get an index so all those

00:11:04,410 --> 00:11:09,150
troopers there are different they're the

00:11:06,720 --> 00:11:14,130
same but they're different you can

00:11:09,150 --> 00:11:17,040
identify them by an index and cube will

00:11:14,130 --> 00:11:21,690
also make sure that each index it's

00:11:17,040 --> 00:11:27,660
started in a particular order so say you

00:11:21,690 --> 00:11:30,120
have again 3 3 Nats replicas that's

00:11:27,660 --> 00:11:34,260
number one will not come up until next

00:11:30,120 --> 00:11:37,290
number zero is up and ready and we're

00:11:34,260 --> 00:11:40,230
gonna gonna talk about what it means to

00:11:37,290 --> 00:11:43,350
be alive and ready in a second that's

00:11:40,230 --> 00:11:46,170
very important so they don't all start

00:11:43,350 --> 00:11:48,750
up at once kubernetes will make sure

00:11:46,170 --> 00:11:51,180
that they start in an ordered fashion so

00:11:48,750 --> 00:11:53,850
that you know you can do database

00:11:51,180 --> 00:11:56,670
migrations in index 0 and then when

00:11:53,850 --> 00:11:58,800
index 1 comes up it can talk to index 0

00:11:56,670 --> 00:12:02,430
it knows that it's there it can share

00:11:58,800 --> 00:12:05,580
information with it it can talk to it so

00:12:02,430 --> 00:12:10,940
it enables us to deploy cluster Abell

00:12:05,580 --> 00:12:13,710
things like like mariadb furthermore

00:12:10,940 --> 00:12:14,490
with with deployments when when you

00:12:13,710 --> 00:12:17,580
create a service

00:12:14,490 --> 00:12:19,770
four four four pods you get that little

00:12:17,580 --> 00:12:22,050
balancing that's basically round-robin

00:12:19,770 --> 00:12:24,980
across all the parts that you that you

00:12:22,050 --> 00:12:27,630
deployed and you can't really target an

00:12:24,980 --> 00:12:30,240
individual part by a well known address

00:12:27,630 --> 00:12:33,930
but stateful sets give you this

00:12:30,240 --> 00:12:37,350
capability so here if I have a stateful

00:12:33,930 --> 00:12:42,690
set for Nats I actually have Nats - one

00:12:37,350 --> 00:12:45,480
so the index of the of the replicas and

00:12:42,690 --> 00:12:48,750
I can be sure that it will always target

00:12:45,480 --> 00:12:51,720
that's number one so individual

00:12:48,750 --> 00:12:54,420
components of the of that cluster will

00:12:51,720 --> 00:13:03,000
component can talk to each other by a

00:12:54,420 --> 00:13:04,680
well known address now we have probes so

00:13:03,000 --> 00:13:08,790
there are two types of probes that we

00:13:04,680 --> 00:13:11,550
use there's a lightness probe that

00:13:08,790 --> 00:13:15,180
basically detects when a container is

00:13:11,550 --> 00:13:18,209
running or not running and based on this

00:13:15,180 --> 00:13:19,620
we restart so for example if if

00:13:18,209 --> 00:13:22,589
something bad has happened in the

00:13:19,620 --> 00:13:24,450
container the liveness probe should be

00:13:22,589 --> 00:13:26,730
able to detect that and then cube will

00:13:24,450 --> 00:13:30,000
actually take action and restart the pot

00:13:26,730 --> 00:13:36,329
and since we automatically generate

00:13:30,000 --> 00:13:39,149
these configurations we we used I think

00:13:36,329 --> 00:13:40,950
in almost all cases we use monitor to

00:13:39,149 --> 00:13:44,220
figure out if something is wrong inside

00:13:40,950 --> 00:13:46,079
the container so bonnet has an API like

00:13:44,220 --> 00:13:48,209
I said we so if you look inside a

00:13:46,079 --> 00:13:50,820
container it's basically the same as a

00:13:48,209 --> 00:13:54,450
Bosch TM so you'll find monitor you know

00:13:50,820 --> 00:13:58,200
you'll find VAR v cap directories and so

00:13:54,450 --> 00:14:01,470
on and we use monitor to tell if if the

00:13:58,200 --> 00:14:03,540
containers is okay and alive or not

00:14:01,470 --> 00:14:08,070
and then you have their readiness probes

00:14:03,540 --> 00:14:11,190
and these are very cool basically the

00:14:08,070 --> 00:14:14,940
readiness probe will tell cube if that

00:14:11,190 --> 00:14:19,380
container is ready to accept traffic so

00:14:14,940 --> 00:14:24,449
this is exactly what we want for the

00:14:19,380 --> 00:14:26,860
active passive model so if if one of the

00:14:24,449 --> 00:14:29,320
containers grabs the lock from console

00:14:26,860 --> 00:14:31,810
becomes the master it'll actually tell

00:14:29,320 --> 00:14:34,300
kubernetes via the readiness probes that

00:14:31,810 --> 00:14:36,310
it's ready to accept traffic all the

00:14:34,300 --> 00:14:38,290
other ones won't be able to do that and

00:14:36,310 --> 00:14:40,750
they won't show up as ready and that's

00:14:38,290 --> 00:14:43,750
okay I mean if you do you know cube

00:14:40,750 --> 00:14:46,390
cattell get pods for your namespace you

00:14:43,750 --> 00:14:48,400
will see a bunch of pause that are not

00:14:46,390 --> 00:14:50,140
ready but that's okay that doesn't mean

00:14:48,400 --> 00:14:52,510
they're not healthy they're just not

00:14:50,140 --> 00:14:55,510
ready to accept traffic and if you kill

00:14:52,510 --> 00:14:58,660
the one that it actually is ready you'll

00:14:55,510 --> 00:15:00,880
see that one of the other ones will grab

00:14:58,660 --> 00:15:04,300
the lock become ready and then cube will

00:15:00,880 --> 00:15:06,000
start routing traffic to to it and of

00:15:04,300 --> 00:15:09,070
course this is also important for

00:15:06,000 --> 00:15:11,260
cluster Abell services cluster Abell

00:15:09,070 --> 00:15:13,450
components you don't want them to accept

00:15:11,260 --> 00:15:20,040
traffic well--there's while data is

00:15:13,450 --> 00:15:25,480
still being migrated so very useful

00:15:20,040 --> 00:15:27,100
capability there okay so now that we

00:15:25,480 --> 00:15:30,490
kind of know the primitives that we used

00:15:27,100 --> 00:15:32,890
in cube to be able to to achieve high

00:15:30,490 --> 00:15:37,120
availability I want to talk a bit about

00:15:32,890 --> 00:15:39,040
how we're exposing it with help so helm

00:15:37,120 --> 00:15:39,580
is where everything kind of comes

00:15:39,040 --> 00:15:43,860
together

00:15:39,580 --> 00:15:46,600
in the end even though this is capable

00:15:43,860 --> 00:15:48,190
we can do this even though this is

00:15:46,600 --> 00:15:52,870
possible we also want to make it easy

00:15:48,190 --> 00:15:54,790
for for the for the operator so when

00:15:52,870 --> 00:15:57,250
someone is managing their CF deployment

00:15:54,790 --> 00:16:01,300
it should be easy for them to scale

00:15:57,250 --> 00:16:04,450
their cluster up and down move from a

00:16:01,300 --> 00:16:08,580
basic deployment to an ha1 and so on so

00:16:04,450 --> 00:16:08,580
I'm not sure if you can read this but

00:16:08,970 --> 00:16:16,600
you can see there the account for Nats

00:16:13,690 --> 00:16:18,640
for example for the operator it's as

00:16:16,600 --> 00:16:21,640
easy as changing that value from one to

00:16:18,640 --> 00:16:23,950
three and what will happen in the

00:16:21,640 --> 00:16:26,320
background is that the helm templates

00:16:23,950 --> 00:16:29,230
will pick up that change in value and

00:16:26,320 --> 00:16:32,410
will actually change the replica count

00:16:29,230 --> 00:16:35,290
of the Nats stateful set it will change

00:16:32,410 --> 00:16:36,910
a bunch of environment variables and the

00:16:35,290 --> 00:16:38,980
operator is not aware of all this

00:16:36,910 --> 00:16:39,939
complexity they just change from one to

00:16:38,980 --> 00:16:41,999
three and then

00:16:39,939 --> 00:16:45,609
bunch of stuff happens in the background

00:16:41,999 --> 00:16:48,489
parts get restarted components that need

00:16:45,609 --> 00:16:52,559
to be aware that Nats has gone from non

00:16:48,489 --> 00:16:54,639
H a to H a will be restarted and

00:16:52,559 --> 00:16:57,759
essentially the cluster will transition

00:16:54,639 --> 00:17:02,459
from from a basic deployment to to an H

00:16:57,759 --> 00:17:07,799
a1 seamlessly and this is all possible

00:17:02,459 --> 00:17:07,799
using these helm templates

00:17:12,250 --> 00:17:20,269
so so what have we achieved so far we

00:17:18,110 --> 00:17:23,000
can horizontally scale the critical

00:17:20,269 --> 00:17:25,760
pieces to make sure that user apps stay

00:17:23,000 --> 00:17:27,380
online and they suffer no downtime we

00:17:25,760 --> 00:17:30,049
can actually make all of the cap

00:17:27,380 --> 00:17:32,740
components a chain so when when you do

00:17:30,049 --> 00:17:35,690
an upgrade and you do rolling upgrades

00:17:32,740 --> 00:17:40,429
the all the components stay up and

00:17:35,690 --> 00:17:42,409
running and the service doesn't doesn't

00:17:40,429 --> 00:17:44,659
degrade essentially what we talked about

00:17:42,409 --> 00:17:47,539
the fact that you know it would be okay

00:17:44,659 --> 00:17:49,460
kind of if it degraded and your

00:17:47,539 --> 00:17:51,019
application still stayed online but we

00:17:49,460 --> 00:17:52,669
actually want to make sure that when you

00:17:51,019 --> 00:17:58,639
do an upgrade

00:17:52,669 --> 00:18:02,659
there's no degradation in there's no

00:17:58,639 --> 00:18:05,090
degradation in the service and then we

00:18:02,659 --> 00:18:06,559
want to survive the chaos monkey little

00:18:05,090 --> 00:18:09,830
disclaimer here is not the actual

00:18:06,559 --> 00:18:12,730
Netflix chaos monkey we just wrote a

00:18:09,830 --> 00:18:16,370
simple script that gets a random pod

00:18:12,730 --> 00:18:21,559
from the deployment and basically just

00:18:16,370 --> 00:18:25,570
kills it so I'm going to show a video

00:18:21,559 --> 00:18:29,860
here because this has actually happened

00:18:25,570 --> 00:18:32,990
this was recorded across three hours

00:18:29,860 --> 00:18:36,010
this is a full deployment where you

00:18:32,990 --> 00:18:36,010
can't actually see this

00:18:38,440 --> 00:18:45,009
there we go so this is a full deployment

00:18:42,399 --> 00:18:46,990
of the cloud application platform runs

00:18:45,009 --> 00:18:51,129
on kubernetes it has an application

00:18:46,990 --> 00:18:52,929
deployed on it with four instances and

00:18:51,129 --> 00:18:55,679
we have this chaos monkey script that

00:18:52,929 --> 00:19:00,250
killed something every three minutes and

00:18:55,679 --> 00:19:02,529
we will see that we have some scripts

00:19:00,250 --> 00:19:04,029
that constantly monitor the API of Cloud

00:19:02,529 --> 00:19:06,759
Foundry and the application that's

00:19:04,029 --> 00:19:08,769
deployed there and counts how many times

00:19:06,759 --> 00:19:11,350
it succeeded and how many times it

00:19:08,769 --> 00:19:16,350
failed and this was done over three

00:19:11,350 --> 00:19:27,159
hours and compressed it to a few minutes

00:19:16,350 --> 00:19:29,559
so hopefully you can read this so on the

00:19:27,159 --> 00:19:31,960
left here you see the actual list of all

00:19:29,559 --> 00:19:33,879
the pods running in the system you can

00:19:31,960 --> 00:19:36,820
see a few of the components that are

00:19:33,879 --> 00:19:40,090
active passive so you have the diego

00:19:36,820 --> 00:19:43,269
database there one of them is the master

00:19:40,090 --> 00:19:47,669
one of them is its passive there so it's

00:19:43,269 --> 00:19:50,940
not ready and as we move move forward

00:19:47,669 --> 00:19:55,750
we're gonna start seeing things being

00:19:50,940 --> 00:19:57,700
killed on the right here and on the

00:19:55,750 --> 00:19:59,980
right you see application requests that

00:19:57,700 --> 00:20:03,009
have happened how many of them were ok

00:19:59,980 --> 00:20:08,320
and how many of them failed same for the

00:20:03,009 --> 00:20:14,320
API and I'm gonna fast-forward through

00:20:08,320 --> 00:20:20,620
this a bit so the movie does accelerate

00:20:14,320 --> 00:20:22,809
you'll see here and essentially you'll

00:20:20,620 --> 00:20:26,830
see things popping in and out of

00:20:22,809 --> 00:20:28,690
existence as things get killed so what

00:20:26,830 --> 00:20:30,789
we want to happen here is that we don't

00:20:28,690 --> 00:20:35,019
want to see application requests failing

00:20:30,789 --> 00:20:38,639
or API requests failing and we just run

00:20:35,019 --> 00:20:38,639
this for for about three hours

00:20:54,450 --> 00:21:00,779
and you'll see that we kill essentially

00:20:58,230 --> 00:21:05,489
everything there are no special pieces

00:21:00,779 --> 00:21:09,480
here we we kill my sequel we kill gnats

00:21:05,489 --> 00:21:12,359
we kill the cells the API all of these

00:21:09,480 --> 00:21:14,519
things and if you look at the pod list

00:21:12,359 --> 00:21:16,409
you can actually see which ones are

00:21:14,519 --> 00:21:20,009
deployed as stateful sets so they

00:21:16,409 --> 00:21:22,379
actually have an index 0 1 2 versus the

00:21:20,009 --> 00:21:25,529
ones that are deployed as a as a cube

00:21:22,379 --> 00:21:28,369
deployment that just have random strings

00:21:25,529 --> 00:21:28,369
in their name

00:21:33,480 --> 00:21:41,970
so in total over these three hours we

00:21:39,240 --> 00:21:43,649
had 54 killings of various components in

00:21:41,970 --> 00:21:47,700
the system

00:21:43,649 --> 00:21:50,639
there was a 99.5 percent application

00:21:47,700 --> 00:21:54,149
availability and then 9.8 for the API

00:21:50,639 --> 00:21:56,610
this is actually the other way around we

00:21:54,149 --> 00:21:59,399
actually want more availability than API

00:21:56,610 --> 00:22:02,100
availability like I said we don't really

00:21:59,399 --> 00:22:03,929
care if you lose the ability to deploy a

00:22:02,100 --> 00:22:06,360
new application across a few minutes but

00:22:03,929 --> 00:22:09,899
you care if the application is down so

00:22:06,360 --> 00:22:14,299
we still have some work to do but I

00:22:09,899 --> 00:22:14,299
think we'll get there because

00:22:19,730 --> 00:22:24,850
okay

00:22:21,770 --> 00:22:24,850
we sit there

00:22:25,419 --> 00:22:31,929
so in conclusion I'd like to say that

00:22:29,529 --> 00:22:36,519
cube is an awesome for host for Cloud

00:22:31,929 --> 00:22:38,379
Foundry it can make it run in highly

00:22:36,519 --> 00:22:40,059
available fashion it can do this

00:22:38,379 --> 00:22:47,829
seamlessly it has all the primitives

00:22:40,059 --> 00:22:51,839
required to to do this and we've

00:22:47,829 --> 00:22:55,450
actually deployed the same exact bit so

00:22:51,839 --> 00:23:00,549
our product is open source you can see

00:22:55,450 --> 00:23:02,429
that github.com Susi / SEF we have a

00:23:00,549 --> 00:23:06,519
better release there that you can deploy

00:23:02,429 --> 00:23:09,279
by yourself and the reason we think is

00:23:06,519 --> 00:23:12,789
it's an awesome host is that we deployed

00:23:09,279 --> 00:23:16,059
the beta bits on Google container engine

00:23:12,789 --> 00:23:18,909
and on Azure container surfaces with

00:23:16,059 --> 00:23:21,309
very few changes just enabling some

00:23:18,909 --> 00:23:24,909
kernel parameters on the VMS that run

00:23:21,309 --> 00:23:29,649
there so the exact same helm release

00:23:24,909 --> 00:23:34,479
that we deployed on top of our own

00:23:29,649 --> 00:23:36,940
container management stuff on ran fine

00:23:34,479 --> 00:23:41,349
on Google container engine and as your

00:23:36,940 --> 00:23:48,369
container surfaces so Cloud Foundry

00:23:41,349 --> 00:23:50,739
loves cube I think so I'm gonna open it

00:23:48,369 --> 00:23:52,989
up for questions if anyone has any

00:23:50,739 --> 00:23:59,190
questions I'd be happy to answer yes

00:23:52,989 --> 00:23:59,190
please go ahead yeah

00:24:02,620 --> 00:24:08,809
so question is if we tested loss of an

00:24:06,380 --> 00:24:10,400
entire host we haven't done that yet

00:24:08,809 --> 00:24:13,309
we're getting to it

00:24:10,400 --> 00:24:15,890
this was a small deployment it was not

00:24:13,309 --> 00:24:17,750
across multiple AZ's or anything like

00:24:15,890 --> 00:24:20,090
that it's actually just sitting on one

00:24:17,750 --> 00:24:22,429
VMs we just wanted to test the actual

00:24:20,090 --> 00:24:24,710
concept that all of the cue pieces work

00:24:22,429 --> 00:24:27,320
as expected so that the services are

00:24:24,710 --> 00:24:30,080
correctly routing that stateful sets

00:24:27,320 --> 00:24:37,309
come back online without any loss of

00:24:30,080 --> 00:24:39,760
data and so on yeah how does the

00:24:37,309 --> 00:24:41,630
deployment compare to a posh deployment

00:24:39,760 --> 00:24:45,679
speed of the deployment

00:24:41,630 --> 00:24:48,800
so once the depending on how fast your

00:24:45,679 --> 00:24:51,290
hardware is after the images get

00:24:48,800 --> 00:24:53,809
downloaded from the registry and you can

00:24:51,290 --> 00:24:55,820
think about this like like a Bosch

00:24:53,809 --> 00:24:58,820
compiled release because the container

00:24:55,820 --> 00:25:02,900
images have all the compiled binaries

00:24:58,820 --> 00:25:04,580
inside them about five minutes you'll

00:25:02,900 --> 00:25:08,270
get stuff running so if you were to

00:25:04,580 --> 00:25:11,330
deploy on on Azure on medium VMs for the

00:25:08,270 --> 00:25:17,470
cube nodes everything will stand up in

00:25:11,330 --> 00:25:17,470
about five minutes yep

00:25:41,060 --> 00:25:44,060
right

00:25:48,620 --> 00:25:53,210
so the question is about a parallel

00:25:50,809 --> 00:25:56,659
between how kubernetes will act as a

00:25:53,210 --> 00:25:58,309
host verse compared to OpenStack and

00:25:56,659 --> 00:26:01,220
Bosch being a host for Cloud Foundry

00:25:58,309 --> 00:26:03,799
deployments and how you know the host

00:26:01,220 --> 00:26:06,289
can the host system that you deploy on

00:26:03,799 --> 00:26:07,929
has various configuration settings like

00:26:06,289 --> 00:26:12,740
timeouts that can affect the health of

00:26:07,929 --> 00:26:17,120
the system so cube gives you a lot of

00:26:12,740 --> 00:26:19,130
control on how these types of things are

00:26:17,120 --> 00:26:22,610
set up for the deployment that you're

00:26:19,130 --> 00:26:26,149
doing so timeouts for readiness probes

00:26:22,610 --> 00:26:29,029
for loudness probes and so on we haven't

00:26:26,149 --> 00:26:33,679
seen an issue moving from Asia to Google

00:26:29,029 --> 00:26:35,899
to our own cube deployment so far we do

00:26:33,679 --> 00:26:39,529
have some specific requirements for the

00:26:35,899 --> 00:26:43,399
cube that's that's supposed to host us

00:26:39,529 --> 00:26:48,490
things like enabling privileged mode of

00:26:43,399 --> 00:26:53,510
course having a like an overlay network

00:26:48,490 --> 00:26:58,090
available having some specific kernel

00:26:53,510 --> 00:27:02,419
parameters enabled but overall I think

00:26:58,090 --> 00:27:06,289
it should be easier because kubernetes

00:27:02,419 --> 00:27:08,480
exposes timeouts and you know the

00:27:06,289 --> 00:27:10,370
network topology to us

00:27:08,480 --> 00:27:13,960
so that we can make the decisions

00:27:10,370 --> 00:27:18,260
instead of the actual infrastructure so

00:27:13,960 --> 00:27:20,679
it should be okay we think or at least

00:27:18,260 --> 00:27:20,679
better

00:27:31,270 --> 00:27:37,159
so the question is if we were able to do

00:27:34,279 --> 00:27:40,159
an upgrade like a helm upgrade while the

00:27:37,159 --> 00:27:44,059
system was running so we're working on

00:27:40,159 --> 00:27:46,130
that right now the one reason we can't

00:27:44,059 --> 00:27:48,820
do it with the beta release that's

00:27:46,130 --> 00:27:51,440
available there is how we treat secrets

00:27:48,820 --> 00:27:54,289
basically every time that we do an

00:27:51,440 --> 00:27:57,200
upgrade all the secrets get rotated so

00:27:54,289 --> 00:28:00,440
of course most things will come up but

00:27:57,200 --> 00:28:03,340
then they don't because all the secrets

00:28:00,440 --> 00:28:03,340
get rotated so

00:28:10,490 --> 00:28:15,350
question is if if you create a secret

00:28:13,310 --> 00:28:19,760
inside cube is it available in Cloud

00:28:15,350 --> 00:28:21,650
Foundry no it's not so you don't from

00:28:19,760 --> 00:28:24,740
your application you don't see the

00:28:21,650 --> 00:28:28,190
environment of the hosting of the

00:28:24,740 --> 00:28:30,020
hosting container just like in a VM if

00:28:28,190 --> 00:28:32,330
you have an environment variable in VM

00:28:30,020 --> 00:28:34,360
the the app is shielded from from all of

00:28:32,330 --> 00:28:34,360
that

00:28:52,250 --> 00:28:59,059
so why did we choose kubernetes so first

00:28:55,940 --> 00:29:01,669
I think you mentioned the CPI we don't

00:28:59,059 --> 00:29:04,309
actually use a CPI for this so this is

00:29:01,669 --> 00:29:06,559
the not deployed using Bosch it's

00:29:04,309 --> 00:29:11,480
deployed using helm there is a CPI I

00:29:06,559 --> 00:29:14,330
think available for cube from s AP but I

00:29:11,480 --> 00:29:17,330
think that's still in in early in an

00:29:14,330 --> 00:29:19,159
early phase why did we choose cube it's

00:29:17,330 --> 00:29:22,070
because it has all the features that we

00:29:19,159 --> 00:29:25,039
require and I think that the cube

00:29:22,070 --> 00:29:30,100
community has a tremendous momentum

00:29:25,039 --> 00:29:33,590
behind it and it seems to work great so

00:29:30,100 --> 00:29:36,289
we looked at at all its capabilities and

00:29:33,590 --> 00:29:40,250
specifically around these staple sets

00:29:36,289 --> 00:29:44,179
and and probes we realized that we could

00:29:40,250 --> 00:29:48,010
deploy we can deploy on par with what

00:29:44,179 --> 00:29:48,010
you can do with bosch and VMs

00:29:51,760 --> 00:30:01,870
can you please repeat so yep so the

00:30:00,020 --> 00:30:05,510
question is what's running inside of the

00:30:01,870 --> 00:30:07,309
Diego cells the same exact thing so like

00:30:05,510 --> 00:30:10,490
I said we built from the exact same

00:30:07,309 --> 00:30:13,340
sources the Diego cell is one of those

00:30:10,490 --> 00:30:15,559
components that has to be privileged in

00:30:13,340 --> 00:30:18,620
order to be able to run containers in

00:30:15,559 --> 00:30:20,600
containers well we realized that that

00:30:18,620 --> 00:30:22,400
sometimes sounds scary that you're

00:30:20,600 --> 00:30:25,370
running containers in containers and it

00:30:22,400 --> 00:30:26,990
kinda sounds like VMs in VMs that's not

00:30:25,370 --> 00:30:28,940
actually the case because you're sharing

00:30:26,990 --> 00:30:31,669
the kernel you're not running a kernel

00:30:28,940 --> 00:30:34,429
inside another kernel so it's just see

00:30:31,669 --> 00:30:38,780
groups namespaces at the end of the day

00:30:34,429 --> 00:30:41,360
so we haven't seen a performance

00:30:38,780 --> 00:30:43,910
degradation or anything like that so

00:30:41,360 --> 00:30:47,450
it's it's garden with Runcie back-end

00:30:43,910 --> 00:30:51,020
and we actually used route FS if you

00:30:47,450 --> 00:30:53,210
know that that's a new project in in

00:30:51,020 --> 00:30:57,470
upstream Cloud Foundry that allows you

00:30:53,210 --> 00:31:01,640
to use a butter FS or overlay instead of

00:30:57,470 --> 00:31:03,710
the old a OFS which helps us greatly

00:31:01,640 --> 00:31:05,890
when contain containerize in cloud cloud

00:31:03,710 --> 00:31:05,890
foundry

00:31:10,679 --> 00:31:18,050
we haven't done those tests yet you're

00:31:13,590 --> 00:31:20,400
looking at application performance or

00:31:18,050 --> 00:31:23,460
infrastructure performance okay yeah

00:31:20,400 --> 00:31:27,980
well take a note and and and look at

00:31:23,460 --> 00:31:27,980
that yeah

00:31:36,630 --> 00:31:43,170
yep so how do we keep our helm chart

00:31:40,980 --> 00:31:47,520
essentially on par with with a Bosch

00:31:43,170 --> 00:31:50,190
manifests so fissile will take up Bosch

00:31:47,520 --> 00:31:53,730
releases it uses the same exact

00:31:50,190 --> 00:31:56,820
descriptors as as the packages and jobs

00:31:53,730 --> 00:31:59,580
specify so those spec files consumes all

00:31:56,820 --> 00:32:02,340
of those compiles everything and then we

00:31:59,580 --> 00:32:06,420
we wanted to expose configuration to the

00:32:02,340 --> 00:32:08,990
user in an opinionated way so we we

00:32:06,420 --> 00:32:12,960
don't expose everything that's available

00:32:08,990 --> 00:32:15,600
from the Bosch releases we only expose

00:32:12,960 --> 00:32:17,430
things via environment variables so

00:32:15,600 --> 00:32:19,710
technically if you wanted to you could

00:32:17,430 --> 00:32:21,420
grab all of the darker images run them

00:32:19,710 --> 00:32:23,610
manually with specific environment

00:32:21,420 --> 00:32:24,300
variables and you'd get a Cloud Foundry

00:32:23,610 --> 00:32:28,410
out of it

00:32:24,300 --> 00:32:30,360
so the helm templates only expose the

00:32:28,410 --> 00:32:33,990
environment variables that we choose as

00:32:30,360 --> 00:32:35,820
a distro of Cloud Foundry and the other

00:32:33,990 --> 00:32:41,940
ones are basically automatically

00:32:35,820 --> 00:32:44,430
generated so you'll see you actually see

00:32:41,940 --> 00:32:46,620
something like this this doesn't contain

00:32:44,430 --> 00:32:50,330
the actual environment variables that I

00:32:46,620 --> 00:32:54,990
was discussing but imagine you have an

00:32:50,330 --> 00:32:58,380
dot domain there and domain would be the

00:32:54,990 --> 00:33:01,050
domain of the of the of the system or

00:32:58,380 --> 00:33:03,030
cloud admin password and then we use

00:33:01,050 --> 00:33:05,220
those environment variables and feed

00:33:03,030 --> 00:33:08,330
them into the actual bash properties and

00:33:05,220 --> 00:33:11,490
we feel that this makes it much easier

00:33:08,330 --> 00:33:14,820
you don't have to repeat yourself as

00:33:11,490 --> 00:33:18,510
much and you have one one way to

00:33:14,820 --> 00:33:23,100
configure the entire system one values

00:33:18,510 --> 00:33:24,900
file it's essentially in help so we

00:33:23,100 --> 00:33:28,520
don't expose it one-to-one hopefully

00:33:24,900 --> 00:33:28,520
that answers the question

00:33:31,290 --> 00:33:41,359
thank you

00:33:33,630 --> 00:33:41,359

YouTube URL: https://www.youtube.com/watch?v=_-H7B31_Y6A


