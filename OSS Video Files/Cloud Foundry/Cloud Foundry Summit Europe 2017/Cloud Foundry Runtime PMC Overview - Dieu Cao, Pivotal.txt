Title: Cloud Foundry Runtime PMC Overview - Dieu Cao, Pivotal
Publication date: 2017-10-18
Playlist: Cloud Foundry Summit Europe 2017
Description: 
	Cloud Foundry Runtime PMC Overview - Dieu Cao, Pivotal

Do you want to learn more about the Cloud Foundry Runtime PMC? Dieu will give an overview of the 17 projects in the PMC, and briefly describe how they coordinate to ship compatible combinations of releases underlying the core of Cloud Foundry. Additionally, Dieu will give an overview of recent accomplishments and upcoming areas of investment in Developer Happiness, Security, Performance, and Stability.

About Dieu Cao
Director of Product Management, CFF Runtime PMC Lead, Pivotal
Dieu Cao is responsible for the strategic direction of Pivotal's Elastic Runtime product. Dieu is also the PMC Lead for the Cloud Foundry Foundation's Runtime Project Management Committee. Dieu previously spoke at CF Summit EU 2016.
Captions: 
	00:00:00,079 --> 00:00:06,210
so I am Yi Kao I'll be giving an

00:00:04,319 --> 00:00:10,410
overview of the Cloud Foundry runtime

00:00:06,210 --> 00:00:12,750
PMC I'm the Cloud Foundry runtime PMC

00:00:10,410 --> 00:00:17,880
lead and I'm also the director of

00:00:12,750 --> 00:00:20,789
product management at pivotal so so what

00:00:17,880 --> 00:00:25,080
is the application runtime PMC newly

00:00:20,789 --> 00:00:28,500
named today the application runtime PMC

00:00:25,080 --> 00:00:30,599
direct strategy development and quality

00:00:28,500 --> 00:00:33,899
control of the core components of cloud

00:00:30,599 --> 00:00:37,760
foundry all of the required components

00:00:33,899 --> 00:00:41,010
for cloud foundry certified paths are

00:00:37,760 --> 00:00:43,520
produced by projects in this PMC so of

00:00:41,010 --> 00:00:45,890
the seven certified passes that were

00:00:43,520 --> 00:00:48,450
mentioned in this morning's keynote

00:00:45,890 --> 00:00:51,420
they're using the components produced by

00:00:48,450 --> 00:00:53,160
this runtime PMC for more information

00:00:51,420 --> 00:00:56,340
about any of the projects you can go to

00:00:53,160 --> 00:00:58,829
cloud foundry org slash projects there's

00:00:56,340 --> 00:01:01,440
links to the slack Channel

00:00:58,829 --> 00:01:03,260
the pivotal tracker projects that show

00:01:01,440 --> 00:01:05,970
the work that they're they're working on

00:01:03,260 --> 00:01:07,770
links to github some of the projects

00:01:05,970 --> 00:01:10,500
also have included links to their

00:01:07,770 --> 00:01:14,340
documentation as well you can also

00:01:10,500 --> 00:01:17,759
subscribe to CF dev list Cloud Foundry

00:01:14,340 --> 00:01:21,479
org and there you'll also see the

00:01:17,759 --> 00:01:24,060
bi-weekly PMC notes to keep up to date

00:01:21,479 --> 00:01:28,409
with what the individual project teams

00:01:24,060 --> 00:01:31,079
are working on so here's a terrible

00:01:28,409 --> 00:01:34,650
market texture diagram of the runtime

00:01:31,079 --> 00:01:36,390
PMC there's loose relations in the

00:01:34,650 --> 00:01:38,180
placement but but really it's quite

00:01:36,390 --> 00:01:41,939
difficult because there's 19 projects

00:01:38,180 --> 00:01:43,759
and they work very closely together the

00:01:41,939 --> 00:01:46,680
ones highlighted in blue here

00:01:43,759 --> 00:01:49,770
permissions but service and AJ proxy are

00:01:46,680 --> 00:01:52,890
incubating projects new projects this

00:01:49,770 --> 00:01:57,140
year where the services API that CF

00:01:52,890 --> 00:01:57,140
permissions and H a proxy bas-reliefs

00:01:59,390 --> 00:02:04,950
runtime themes of investment broadly and

00:02:02,880 --> 00:02:10,360
I'm still shopping this around so would

00:02:04,950 --> 00:02:13,010
love feedback on this but I think

00:02:10,360 --> 00:02:15,650
security and stability are kind of table

00:02:13,010 --> 00:02:19,010
stakes in terms of themes of investment

00:02:15,650 --> 00:02:22,670
you need to be able to have a stable

00:02:19,010 --> 00:02:24,590
platform scale out and guarantee

00:02:22,670 --> 00:02:26,900
application workloads can continue

00:02:24,590 --> 00:02:31,970
running you need to do that in a secure

00:02:26,900 --> 00:02:34,720
way so that people are are able to run

00:02:31,970 --> 00:02:38,000
their workloads confidently in in

00:02:34,720 --> 00:02:40,700
complicated environments and developer

00:02:38,000 --> 00:02:44,600
happiness is my personal favorite theme

00:02:40,700 --> 00:02:47,390
of investment or in encouraging

00:02:44,600 --> 00:02:52,130
developer productivity and allowing

00:02:47,390 --> 00:02:54,620
developers to focus on business value so

00:02:52,130 --> 00:02:56,600
in these following slides we'll be

00:02:54,620 --> 00:02:59,000
talking about a lot of things that I

00:02:56,600 --> 00:03:05,209
hope fit mostly into these themes of

00:02:59,000 --> 00:03:09,070
investment application lifecycle I think

00:03:05,209 --> 00:03:12,590
broadly fits into developer happiness

00:03:09,070 --> 00:03:18,610
things around improving first push that

00:03:12,590 --> 00:03:23,900
includes adding more support for

00:03:18,610 --> 00:03:25,790
commands into the CF CLI manifest making

00:03:23,900 --> 00:03:31,280
that a first-class citizen and making

00:03:25,790 --> 00:03:33,920
that first push better improving second

00:03:31,280 --> 00:03:37,010
push that that includes investments in

00:03:33,920 --> 00:03:39,950
rolling app updates zero downtime

00:03:37,010 --> 00:03:42,110
updates it people are often confused

00:03:39,950 --> 00:03:43,610
when you go from first push to second

00:03:42,110 --> 00:03:45,769
push and there is a significant amount

00:03:43,610 --> 00:03:48,019
of down time in between because of the

00:03:45,769 --> 00:03:51,140
amount of time it takes to stage an app

00:03:48,019 --> 00:03:53,630
and perhaps maybe that staging didn't go

00:03:51,140 --> 00:03:56,150
so well and you might have to stage

00:03:53,630 --> 00:03:57,470
again with some new bits of code or you

00:03:56,150 --> 00:03:59,930
might have to maintain your own

00:03:57,470 --> 00:04:01,850
blue/green scripts and so if the

00:03:59,930 --> 00:04:06,650
platform can own some of that complexity

00:04:01,850 --> 00:04:11,269
you can again focus on on on providing

00:04:06,650 --> 00:04:16,310
application provide providing business

00:04:11,269 --> 00:04:19,580
value we've heard a lot about people

00:04:16,310 --> 00:04:22,220
caring about native a be testing so they

00:04:19,580 --> 00:04:23,630
can roll out a little bit of their next

00:04:22,220 --> 00:04:27,720
version

00:04:23,630 --> 00:04:31,650
and and see how that goes with the while

00:04:27,720 --> 00:04:36,199
still running a majority of the previous

00:04:31,650 --> 00:04:40,139
version Canary deploys being able to

00:04:36,199 --> 00:04:41,940
roll one out and then of your new

00:04:40,139 --> 00:04:45,060
version and make sure that's healthy

00:04:41,940 --> 00:04:47,340
before going on with the rest smoke test

00:04:45,060 --> 00:04:50,490
being able to run smoke tests in a

00:04:47,340 --> 00:04:52,800
coordinated fashion with with while

00:04:50,490 --> 00:04:54,540
you're rolling out your updates and

00:04:52,800 --> 00:04:58,950
these are all areas that I think we can

00:04:54,540 --> 00:05:00,570
improve in in push app promotion is an

00:04:58,950 --> 00:05:04,200
also an area that I think we're

00:05:00,570 --> 00:05:07,050
interested in so that say you're

00:05:04,200 --> 00:05:08,669
iterating very quickly in your dev space

00:05:07,050 --> 00:05:14,280
but then you want to promote it to

00:05:08,669 --> 00:05:16,979
another space where your product manager

00:05:14,280 --> 00:05:20,570
wants to do acceptance and so how can

00:05:16,979 --> 00:05:25,560
you more or less promote that same

00:05:20,570 --> 00:05:28,650
artifact to the next space in a nice way

00:05:25,560 --> 00:05:31,139
cf local is being proposed into the

00:05:28,650 --> 00:05:34,050
extensions PMC but it's also in this

00:05:31,139 --> 00:05:40,680
broad theme of how can you get faster

00:05:34,050 --> 00:05:42,630
feedback locally all right

00:05:40,680 --> 00:05:48,080
improved operator experience we have

00:05:42,630 --> 00:05:54,750
Bosh boot loader that's a CLI tool for

00:05:48,080 --> 00:06:00,840
as starting up for configuring and

00:05:54,750 --> 00:06:03,169
paving and I as quickly starting with a

00:06:00,840 --> 00:06:07,800
Bosh director and getting all of that

00:06:03,169 --> 00:06:11,840
configuration very simply and we

00:06:07,800 --> 00:06:16,710
currently have support for AWS and GCP

00:06:11,840 --> 00:06:21,450
measures in progress vSphere I think is

00:06:16,710 --> 00:06:26,310
on the horizon as terraform support for

00:06:21,450 --> 00:06:30,409
vSphere matures CF deployment it's very

00:06:26,310 --> 00:06:33,450
close to replacing CF release and this

00:06:30,409 --> 00:06:35,610
allows for much simpler manifest

00:06:33,450 --> 00:06:37,740
generation taking advantage of the Bosh

00:06:35,610 --> 00:06:41,400
2.0 features

00:06:37,740 --> 00:06:44,730
and also allowing you to have a much

00:06:41,400 --> 00:06:48,000
more composable experience with with

00:06:44,730 --> 00:06:50,810
what you deploy Bosch backup and restore

00:06:48,000 --> 00:06:54,900
support so the Bosch backup and restore

00:06:50,810 --> 00:06:58,050
exists in the extensions PMC but each of

00:06:54,900 --> 00:07:02,160
the components that have state within

00:06:58,050 --> 00:07:05,690
Cloud Foundry now have support to hook

00:07:02,160 --> 00:07:09,230
into the Bosch backup and restore and

00:07:05,690 --> 00:07:11,730
perhaps go into read-only mode or

00:07:09,230 --> 00:07:16,890
gracefully stop itself so it can have a

00:07:11,730 --> 00:07:20,690
consistent state across with within that

00:07:16,890 --> 00:07:23,730
single backup for when you restore that

00:07:20,690 --> 00:07:26,760
we're also investing in improving route

00:07:23,730 --> 00:07:33,300
consistency and availability so that in

00:07:26,760 --> 00:07:38,610
the face of instability in in the

00:07:33,300 --> 00:07:40,590
management plane Nats for example we may

00:07:38,610 --> 00:07:44,490
not have to completely approve the

00:07:40,590 --> 00:07:47,460
routes after two minutes and still be

00:07:44,490 --> 00:07:53,030
able to guarantee that your apps will

00:07:47,460 --> 00:07:53,030
not be routed to an incorrect container

00:07:53,990 --> 00:07:59,970
connecting services we have a container

00:07:56,910 --> 00:08:02,700
to container networking it was g8

00:07:59,970 --> 00:08:06,330
earlier this year you can now securely

00:08:02,700 --> 00:08:10,890
connect and an app from one space to

00:08:06,330 --> 00:08:12,120
another space and not have to go out and

00:08:10,890 --> 00:08:16,230
around through this through the go

00:08:12,120 --> 00:08:18,000
router with with this in place we have

00:08:16,230 --> 00:08:20,850
application incidence identity

00:08:18,000 --> 00:08:23,790
credentials which i think is an OLE a

00:08:20,850 --> 00:08:26,850
new tool for us and we're still kind of

00:08:23,790 --> 00:08:29,820
seeing how we can make use of that but I

00:08:26,850 --> 00:08:33,450
think in Eric's demo earlier today he he

00:08:29,820 --> 00:08:35,340
showed how you could how applications

00:08:33,450 --> 00:08:38,190
can use those credentials to

00:08:35,340 --> 00:08:42,100
authenticate with each other and be very

00:08:38,190 --> 00:08:44,630
sure of who

00:08:42,100 --> 00:08:47,030
who's communicating with what and not

00:08:44,630 --> 00:08:48,590
have having applications actually have

00:08:47,030 --> 00:08:53,510
to deal with provisioning of those

00:08:48,590 --> 00:08:58,750
credentials MPLS support through the go

00:08:53,510 --> 00:09:01,580
router so so now if you want to have a

00:08:58,750 --> 00:09:04,030
mutual TLS you can have that client

00:09:01,580 --> 00:09:06,290
certificate forwarded through the header

00:09:04,030 --> 00:09:08,570
and if your applications are able to

00:09:06,290 --> 00:09:11,930
consume that which is now supported in

00:09:08,570 --> 00:09:15,980
the Java build pack you don't have to

00:09:11,930 --> 00:09:19,580
spin up a TCP router and figure out how

00:09:15,980 --> 00:09:22,400
to manage that port and whatnot you can

00:09:19,580 --> 00:09:27,860
just trust in that header that's

00:09:22,400 --> 00:09:29,990
forwarded through the go router operator

00:09:27,860 --> 00:09:32,660
and manage multiple certificate support

00:09:29,990 --> 00:09:35,480
that's also now supported in the go

00:09:32,660 --> 00:09:38,420
router and H a proxy bas-reliefs so now

00:09:35,480 --> 00:09:42,070
you can have an operator configured

00:09:38,420 --> 00:09:46,730
support for for custom domains using s

00:09:42,070 --> 00:09:49,310
S&I things that are still in progress

00:09:46,730 --> 00:09:53,600
include securing service inserts

00:09:49,310 --> 00:09:57,110
credentials with cred hub so that if you

00:09:53,600 --> 00:10:00,200
wanted to you could opt into service

00:09:57,110 --> 00:10:04,880
brokers that can store credentials into

00:10:00,200 --> 00:10:07,460
credit hub and and and applications are

00:10:04,880 --> 00:10:09,920
then able to retrieve those credentials

00:10:07,460 --> 00:10:11,660
and not have those stored in cloud

00:10:09,920 --> 00:10:14,020
controller where a cloud controller

00:10:11,660 --> 00:10:17,330
admin who may only care about operating

00:10:14,020 --> 00:10:20,150
Cloud Foundry doesn't actually need

00:10:17,330 --> 00:10:24,350
access to all of those credentials kind

00:10:20,150 --> 00:10:26,540
of reducing the blast radius also the

00:10:24,350 --> 00:10:30,040
services API team is working on service

00:10:26,540 --> 00:10:33,590
incident sharing across orbs and spaces

00:10:30,040 --> 00:10:36,620
it's been long requested the ability to

00:10:33,590 --> 00:10:38,240
share a particular service instance from

00:10:36,620 --> 00:10:40,430
one space to another space and I think

00:10:38,240 --> 00:10:44,390
this is common with with micro-services

00:10:40,430 --> 00:10:47,410
patterns platform provided service

00:10:44,390 --> 00:10:53,360
discovery is another area of investment

00:10:47,410 --> 00:10:55,190
so that you can

00:10:53,360 --> 00:10:56,839
and and I think that fits really well

00:10:55,190 --> 00:10:58,970
with the Container tunic container

00:10:56,839 --> 00:11:03,860
networking now that there's policy how

00:10:58,970 --> 00:11:07,790
do you provide internal routes and

00:11:03,860 --> 00:11:11,450
internally to provide discovery for

00:11:07,790 --> 00:11:16,220
those those routes and communication

00:11:11,450 --> 00:11:19,370
paths envoy and sto are also a fairly

00:11:16,220 --> 00:11:22,430
hot topic nowadays and how can we take

00:11:19,370 --> 00:11:25,360
advantage of envoy and sto in the Cloud

00:11:22,430 --> 00:11:29,899
Foundry community there's a lot of

00:11:25,360 --> 00:11:43,519
capabilities that envoy provides and in

00:11:29,899 --> 00:11:47,899
terms of weighted routing sorry and sto

00:11:43,519 --> 00:11:51,290
we think will help control and configure

00:11:47,899 --> 00:11:59,510
on voice and perhaps be that bridging

00:11:51,290 --> 00:12:01,459
mechanism between o be that bridging

00:11:59,510 --> 00:12:04,760
mechanism between apps running on Cloud

00:12:01,459 --> 00:12:08,260
Foundry and apps running somewhere else

00:12:04,760 --> 00:12:08,260
perhaps on the container service

00:12:09,790 --> 00:12:15,920
investments to support legacy or non 12

00:12:12,890 --> 00:12:21,410
factor apps they're there a number of

00:12:15,920 --> 00:12:23,750
legacy apps that can't run on Cloud

00:12:21,410 --> 00:12:27,140
Foundry currently but with support for

00:12:23,750 --> 00:12:32,270
multiple ports a lot of Java EE apps for

00:12:27,140 --> 00:12:34,279
example need multiple ports but with

00:12:32,270 --> 00:12:37,940
just that example extension they could

00:12:34,279 --> 00:12:39,680
run on Cloud Foundry shared volumes we

00:12:37,940 --> 00:12:42,290
also have support for and there's

00:12:39,680 --> 00:12:46,130
additional drivers and brokers that are

00:12:42,290 --> 00:12:50,959
being developed we started with an NFS

00:12:46,130 --> 00:12:52,519
v3 driver broker the EFS driver broker

00:12:50,959 --> 00:12:57,290
is being developed and I think there's

00:12:52,519 --> 00:13:01,060
interest as well as in NFS and samba and

00:12:57,290 --> 00:13:01,060
Sif's drop driver broker

00:13:01,670 --> 00:13:10,759
build packs there's investment in multi

00:13:06,350 --> 00:13:13,519
build pack support which allows for

00:13:10,759 --> 00:13:16,300
polyglot apps or potentially API

00:13:13,519 --> 00:13:18,980
gateways to be composed with your app

00:13:16,300 --> 00:13:22,360
and less working of build packs in

00:13:18,980 --> 00:13:27,100
general so that if you need to provide

00:13:22,360 --> 00:13:29,990
certificates or a particular agent or

00:13:27,100 --> 00:13:32,569
whatnot you can actually include that in

00:13:29,990 --> 00:13:35,750
your own little specialty build pack and

00:13:32,569 --> 00:13:38,569
then compose that with a Java build pack

00:13:35,750 --> 00:13:43,220
or Python or whatever that may be

00:13:38,569 --> 00:13:44,779
and and have these be coordinated so

00:13:43,220 --> 00:13:48,560
that you don't have to fork the build

00:13:44,779 --> 00:13:49,850
packs and then merge back upstream all

00:13:48,560 --> 00:13:53,680
the time

00:13:49,850 --> 00:13:57,439
Oh see I build paksas is an area of

00:13:53,680 --> 00:14:02,029
experimentation where we're looking at

00:13:57,439 --> 00:14:04,430
could we have droplets and rude FS that

00:14:02,029 --> 00:14:08,180
are actually image layers would those be

00:14:04,430 --> 00:14:11,630
more portable what what benefits could

00:14:08,180 --> 00:14:13,639
we get from that one one idea is if

00:14:11,630 --> 00:14:17,709
we're able to do that

00:14:13,639 --> 00:14:23,990
perhaps the rather large windows root FS

00:14:17,709 --> 00:14:27,380
could be an image layer or refer to a

00:14:23,990 --> 00:14:31,279
Jers layer out out there because they

00:14:27,380 --> 00:14:37,610
they provide the canonical layer for for

00:14:31,279 --> 00:14:39,800
Windows CF lunes at Linux FS 3 will come

00:14:37,610 --> 00:14:41,990
at some point in time I think that

00:14:39,800 --> 00:14:45,980
there's a current question of should we

00:14:41,990 --> 00:14:48,939
should we wait until 1804 comes out

00:14:45,980 --> 00:14:53,569
should we build one for 1604 are there

00:14:48,939 --> 00:14:56,029
compelling reasons to to do this sooner

00:14:53,569 --> 00:14:59,300
with 1604 and that's something you can

00:14:56,029 --> 00:15:04,550
talk with Steven Levine about openSUSE

00:14:59,300 --> 00:15:06,290
they're also developing a route FS for

00:15:04,550 --> 00:15:09,410
Tuesday and of course build packs would

00:15:06,290 --> 00:15:10,050
then need to support compilation on top

00:15:09,410 --> 00:15:19,339
of

00:15:10,050 --> 00:15:22,740
Susu all right windows they we now have

00:15:19,339 --> 00:15:24,750
HWC buildpack in.net cora build pack and

00:15:22,740 --> 00:15:28,230
I'm really excited about the Windows

00:15:24,750 --> 00:15:31,230
2016 containerization support that

00:15:28,230 --> 00:15:35,040
brings to the windows world support for

00:15:31,230 --> 00:15:38,010
CFS SH and volume services and that

00:15:35,040 --> 00:15:41,519
sifts Samba driver is actually very

00:15:38,010 --> 00:15:44,130
popular for for users who are interested

00:15:41,519 --> 00:15:48,620
in Windows Network loads being able to

00:15:44,130 --> 00:15:54,390
connect to their SIF Samba file share

00:15:48,620 --> 00:16:00,300
natively on that they have within their

00:15:54,390 --> 00:16:03,540
environment a user management this past

00:16:00,300 --> 00:16:06,120
year or so we introduced two new cloud

00:16:03,540 --> 00:16:08,790
controller scopes to again help reduce

00:16:06,120 --> 00:16:11,160
blast radius cloud controller admin

00:16:08,790 --> 00:16:14,880
read-only cloud controller global

00:16:11,160 --> 00:16:18,240
auditor there they act very much like

00:16:14,880 --> 00:16:20,520
cloud controller admin both of them do

00:16:18,240 --> 00:16:24,870
not have write ability the global

00:16:20,520 --> 00:16:26,610
auditor one acts just like the auditor

00:16:24,870 --> 00:16:29,310
role but without having to add yourself

00:16:26,610 --> 00:16:32,279
to every single space which is the

00:16:29,310 --> 00:16:35,040
experience that that was before if you

00:16:32,279 --> 00:16:38,459
wanted to give permissions to someone

00:16:35,040 --> 00:16:41,279
and not to be an auditor but not have

00:16:38,459 --> 00:16:46,820
them see all the credentials that are in

00:16:41,279 --> 00:16:50,130
the system the CF permissions team is

00:16:46,820 --> 00:16:54,600
working on user role to group mappings

00:16:50,130 --> 00:16:58,860
so such that it simplifies the user

00:16:54,600 --> 00:17:01,800
management process so that you can map a

00:16:58,860 --> 00:17:05,130
particular group to particular

00:17:01,800 --> 00:17:07,290
permissions perhaps based developer in a

00:17:05,130 --> 00:17:09,600
particular space in their particular org

00:17:07,290 --> 00:17:12,600
and by someone joining that group

00:17:09,600 --> 00:17:14,939
they'll natively have permission that

00:17:12,600 --> 00:17:18,079
particular permission and when they

00:17:14,939 --> 00:17:20,750
leave that group that permission will be

00:17:18,079 --> 00:17:23,490
removed

00:17:20,750 --> 00:17:26,010
that I think that's their first phase of

00:17:23,490 --> 00:17:28,200
work the the next thing they're hoping

00:17:26,010 --> 00:17:30,840
to tackle after that includes a

00:17:28,200 --> 00:17:36,210
finer-grained authorization the ability

00:17:30,840 --> 00:17:38,400
to allow a cloud controller for example

00:17:36,210 --> 00:17:44,490
or potentially other components in the

00:17:38,400 --> 00:17:47,790
system to to to define finer grained

00:17:44,490 --> 00:17:50,280
permissions in this context like while

00:17:47,790 --> 00:17:53,820
still being able to the use user role to

00:17:50,280 --> 00:17:59,010
group mappings but say we often hear

00:17:53,820 --> 00:18:04,200
about separation of duties where someone

00:17:59,010 --> 00:18:07,350
has to have a only once the ability to

00:18:04,200 --> 00:18:09,120
stop and start and scale an app but for

00:18:07,350 --> 00:18:13,230
whatever reason it's not allowed to

00:18:09,120 --> 00:18:19,410
modify the app or delete the app and so

00:18:13,230 --> 00:18:22,740
to satisfy compliance how a custom role

00:18:19,410 --> 00:18:24,630
I find fine-grained authorization

00:18:22,740 --> 00:18:28,260
wouldn't support that and then custom

00:18:24,630 --> 00:18:33,480
roles allows you to build up meaningful

00:18:28,260 --> 00:18:35,400
names for that vulnerability fixing I

00:18:33,480 --> 00:18:39,420
think Cloud Foundry is really

00:18:35,400 --> 00:18:41,610
best-in-class here with how quickly we

00:18:39,420 --> 00:18:43,350
are able to patch through too fast and

00:18:41,610 --> 00:18:48,690
stem cells and really our components

00:18:43,350 --> 00:18:50,430
even as as things are reported were I'm

00:18:48,690 --> 00:18:53,550
sure you'll you see those updates all

00:18:50,430 --> 00:18:56,430
the time on the security mailing list we

00:18:53,550 --> 00:18:59,220
have an amazing number of penetration

00:18:56,430 --> 00:19:01,770
tests from all of the the wide ecosystem

00:18:59,220 --> 00:19:06,840
and users if you can imagine all the

00:19:01,770 --> 00:19:10,260
enterprise come companies and providers

00:19:06,840 --> 00:19:17,420
they are throwing all all of their

00:19:10,260 --> 00:19:20,460
resources at uaa and and and and the the

00:19:17,420 --> 00:19:24,420
projects in general and providing that

00:19:20,460 --> 00:19:27,480
back in and almost on a weekly basis

00:19:24,420 --> 00:19:30,240
where triaging those and addressing

00:19:27,480 --> 00:19:31,420
those as quick as we can and individual

00:19:30,240 --> 00:19:32,500
teams are

00:19:31,420 --> 00:19:35,170
keeping up-to-date with third-party

00:19:32,500 --> 00:19:37,270
dependencies and we're working on

00:19:35,170 --> 00:19:41,350
additional scanning tools on how to

00:19:37,270 --> 00:19:44,250
identify that better in in languages

00:19:41,350 --> 00:19:49,930
that maybe don't have great support for

00:19:44,250 --> 00:19:54,760
dependencies another area of investment

00:19:49,930 --> 00:19:56,740
was securing communication paths and one

00:19:54,760 --> 00:19:58,330
of the bigger ones was securing the

00:19:56,740 --> 00:20:01,540
communication path between Cloud

00:19:58,330 --> 00:20:03,940
Controller and Diego while Diego was

00:20:01,540 --> 00:20:06,010
being developed there were a number of

00:20:03,940 --> 00:20:08,710
bridge components that were developed

00:20:06,010 --> 00:20:09,370
and between the bridge and Diego was

00:20:08,710 --> 00:20:10,990
secure

00:20:09,370 --> 00:20:14,320
but between Cloud Controller and the

00:20:10,990 --> 00:20:17,110
bridge it was completely insecure and so

00:20:14,320 --> 00:20:19,360
we did quite a bit of refactoring to get

00:20:17,110 --> 00:20:24,190
to the state as shown on the bottom

00:20:19,360 --> 00:20:25,990
there where we actually eliminated some

00:20:24,190 --> 00:20:29,220
of those components and absorbed the

00:20:25,990 --> 00:20:35,050
functionality into cloud controller and

00:20:29,220 --> 00:20:39,150
and otherwise introduced a mutuality LS

00:20:35,050 --> 00:20:41,590
between cloud controller and Diego I

00:20:39,150 --> 00:20:45,280
think you can find in the documentation

00:20:41,590 --> 00:20:48,010
now much better documentation about all

00:20:45,280 --> 00:20:51,880
of the communication paths the port's

00:20:48,010 --> 00:20:54,130
the protocols and they're there we've

00:20:51,880 --> 00:20:57,520
been steps slowly stamping them out

00:20:54,130 --> 00:21:01,110
there's still a couple but we've been

00:20:57,520 --> 00:21:01,110
making good progress against this

00:21:01,470 --> 00:21:07,530
rootless garden we have currently

00:21:05,830 --> 00:21:11,130
experimental support we're hoping to

00:21:07,530 --> 00:21:14,280
roll that out to the PWS environment

00:21:11,130 --> 00:21:17,200
soon to see how this goes

00:21:14,280 --> 00:21:20,590
assuming you have all unprivileged

00:21:17,200 --> 00:21:24,970
containers well you'll be able to easily

00:21:20,590 --> 00:21:27,430
opt into this mode such that your

00:21:24,970 --> 00:21:30,070
surface area of attack should there be a

00:21:27,430 --> 00:21:35,010
container breakout is it's much lower

00:21:30,070 --> 00:21:37,210
and Cloud Foundry on the garden team

00:21:35,010 --> 00:21:39,820
contributed greatly to this work

00:21:37,210 --> 00:21:41,850
providing PRS to make it happen and

00:21:39,820 --> 00:21:47,180
we're the first

00:21:41,850 --> 00:21:47,180
in the wider OCI community to adopt it

00:21:48,960 --> 00:21:54,450
isolation segments we've made up we've

00:21:52,320 --> 00:21:57,600
made great progress with this I'm seeing

00:21:54,450 --> 00:21:59,910
a lot of adoption in the community the

00:21:57,600 --> 00:22:06,060
number of use cases are wide and varying

00:21:59,910 --> 00:22:09,300
I we're seeing people put an isolation

00:22:06,060 --> 00:22:11,760
segment into their public dmz so that

00:22:09,300 --> 00:22:13,830
the routers and the cells are in their

00:22:11,760 --> 00:22:16,320
public dmz and those are publicly

00:22:13,830 --> 00:22:19,890
accessible but everything else they

00:22:16,320 --> 00:22:21,500
leave in their internal networks or

00:22:19,890 --> 00:22:25,110
otherwise we're just seeing

00:22:21,500 --> 00:22:27,870
consolidation we have one customer who

00:22:25,110 --> 00:22:31,350
is going from sixteen separate

00:22:27,870 --> 00:22:38,300
foundations down to four because of

00:22:31,350 --> 00:22:41,370
isolation segments in the spirit of

00:22:38,300 --> 00:22:45,030
refactoring we're removing console

00:22:41,370 --> 00:22:47,310
dependencies distributed service locks

00:22:45,030 --> 00:22:49,260
we are looking to move or eliminate them

00:22:47,310 --> 00:22:54,380
the ones that we were moving we're

00:22:49,260 --> 00:22:57,590
moving to the database the ones that or

00:22:54,380 --> 00:22:59,910
through the Lockett service here and

00:22:57,590 --> 00:23:03,560
otherwise eliminating some of the ones

00:22:59,910 --> 00:23:06,660
where for whatever reason we reached for

00:23:03,560 --> 00:23:10,880
console for that even when the component

00:23:06,660 --> 00:23:10,880
didn't actually need distributed locking

00:23:11,810 --> 00:23:16,770
the the other aspect of console the

00:23:14,670 --> 00:23:18,660
other use case for it was for service

00:23:16,770 --> 00:23:21,950
discovery in health checks and this is

00:23:18,660 --> 00:23:25,290
very much still in progress as Bosch DNS

00:23:21,950 --> 00:23:27,810
develops but the idea is to leverage

00:23:25,290 --> 00:23:30,480
Bosch links for discovery for zone

00:23:27,810 --> 00:23:34,770
affinity and for that health and and

00:23:30,480 --> 00:23:37,530
bachianas for for healthiness and that

00:23:34,770 --> 00:23:43,380
hopefully will remove our dependence on

00:23:37,530 --> 00:23:43,940
console in general we've also in cloud

00:23:43,380 --> 00:23:47,280
foundry

00:23:43,940 --> 00:23:51,270
anyway the application run time we've

00:23:47,280 --> 00:23:53,790
removed our sed dependency entirely and

00:23:51,270 --> 00:23:55,270
replace that with Postgres in my sequel

00:23:53,790 --> 00:23:57,640
support I think that

00:23:55,270 --> 00:23:59,560
is a great deal of operator burden as

00:23:57,640 --> 00:24:02,050
well because many operators are very

00:23:59,560 --> 00:24:04,630
familiar with operating Postgres or my

00:24:02,050 --> 00:24:09,180
sequel but their familiarity with

00:24:04,630 --> 00:24:12,460
troubleshooting at CD much lower and I

00:24:09,180 --> 00:24:16,540
think we're seeing that the relational

00:24:12,460 --> 00:24:21,270
databases is supporting us in the

00:24:16,540 --> 00:24:24,370
fashion that we need and Diego scaling

00:24:21,270 --> 00:24:26,950
benefited was one of the the ones that

00:24:24,370 --> 00:24:29,170
proved out that EDD wasn't able at the

00:24:26,950 --> 00:24:33,220
time to scale it to the workloads that

00:24:29,170 --> 00:24:35,710
we needed and I still this was over a

00:24:33,220 --> 00:24:39,130
year ago but I still want to highlight

00:24:35,710 --> 00:24:44,410
that Diego and Cloud Foundry in general

00:24:39,130 --> 00:24:47,410
as a whole integrated component was able

00:24:44,410 --> 00:24:49,090
to scale it's not we didn't just you

00:24:47,410 --> 00:24:54,460
know start up a hundred thousand

00:24:49,090 --> 00:24:56,560
containers and and and with what whatnot

00:24:54,460 --> 00:25:01,210
we we actually run as an integrated

00:24:56,560 --> 00:25:02,680
platform at this large scale and from

00:25:01,210 --> 00:25:04,480
this graph you can see like things

00:25:02,680 --> 00:25:14,770
crashing intentionally and things

00:25:04,480 --> 00:25:17,200
recovering and I think the the degree of

00:25:14,770 --> 00:25:19,660
scale that we were able to achieve with

00:25:17,200 --> 00:25:21,820
the components that we have I think

00:25:19,660 --> 00:25:23,860
that's great benefit to the community

00:25:21,820 --> 00:25:28,210
and I don't I still think that's

00:25:23,860 --> 00:25:32,950
unmatched by any of the other things

00:25:28,210 --> 00:25:37,510
that are out there currently routing

00:25:32,950 --> 00:25:39,670
performance the routing team invested in

00:25:37,510 --> 00:25:43,480
improving performance and they had were

00:25:39,670 --> 00:25:46,600
able to achieve 3x throughput of the go

00:25:43,480 --> 00:25:48,370
router by doing a few things here my

00:25:46,600 --> 00:25:53,980
favorite there was update the

00:25:48,370 --> 00:25:57,910
dependencies and here's the the Headroom

00:25:53,980 --> 00:26:00,760
pot of before which is that red line

00:25:57,910 --> 00:26:03,940
there and the that throughput and the

00:26:00,760 --> 00:26:06,700
much nicer one the blue one there in the

00:26:03,940 --> 00:26:09,960
in the after much better throughput

00:26:06,700 --> 00:26:09,960
requests per second

00:26:10,169 --> 00:26:17,789
longer Gator performance the logger

00:26:14,850 --> 00:26:25,350
Gator invested greatly in becoming less

00:26:17,789 --> 00:26:28,679
lossy so you can see here these are our

00:26:25,350 --> 00:26:34,619
graphs from from kind of before and

00:26:28,679 --> 00:26:37,559
after certain deploys and you can see I

00:26:34,619 --> 00:26:40,619
think PCF 1:10 I forget what what the

00:26:37,559 --> 00:26:45,330
open source versions were for that but

00:26:40,619 --> 00:26:49,350
it goes from 50% kind of really spiky

00:26:45,330 --> 00:26:51,090
terrible which is bad it gets a little

00:26:49,350 --> 00:26:53,100
bit better after that certain release

00:26:51,090 --> 00:26:54,779
after a certain number of investments

00:26:53,100 --> 00:26:58,409
and improvements were made in the logger

00:26:54,779 --> 00:27:01,289
Gator system and after CF 260 was

00:26:58,409 --> 00:27:03,929
deployed it's much better much closer to

00:27:01,289 --> 00:27:05,399
100% a few spikes there and I think even

00:27:03,929 --> 00:27:08,779
after this we've continued to make

00:27:05,399 --> 00:27:14,700
improvements in in logger Gators

00:27:08,779 --> 00:27:16,860
availability there's now logger Gator SL

00:27:14,700 --> 00:27:19,919
O's and scaling guidelines so that

00:27:16,860 --> 00:27:21,629
you'll have a better idea of when and

00:27:19,919 --> 00:27:28,009
how to scale the different components of

00:27:21,629 --> 00:27:33,139
logger Gator I think that's it for me

00:27:28,009 --> 00:27:33,139
questions and join us

00:27:37,530 --> 00:27:40,190
yes

00:27:54,930 --> 00:28:01,470
do you want to repeat yeah this is one

00:27:58,450 --> 00:28:04,210
question around isolation segment and

00:28:01,470 --> 00:28:07,450
preventing one compromised isolation

00:28:04,210 --> 00:28:12,640
segment from compromising the shared

00:28:07,450 --> 00:28:17,770
control plane so that we can have true

00:28:12,640 --> 00:28:21,600
multi-tenancy even though one insulation

00:28:17,770 --> 00:28:26,320
gentleman might be compromised I think

00:28:21,600 --> 00:28:29,170
this is something that we thought about

00:28:26,320 --> 00:28:30,310
it's not currently prioritized but it's

00:28:29,170 --> 00:28:32,260
something we're thinking about

00:28:30,310 --> 00:28:35,140
so I would appreciate additional

00:28:32,260 --> 00:28:39,400
feedback but I believe we've we've

00:28:35,140 --> 00:28:41,380
thought about could the things in the

00:28:39,400 --> 00:28:45,150
isolation segment there go router and

00:28:41,380 --> 00:28:49,750
the Diego cells for example provide

00:28:45,150 --> 00:28:53,890
clear when when they communicate back to

00:28:49,750 --> 00:28:57,330
the control plane have in their identity

00:28:53,890 --> 00:28:59,410
in their certificates more clear

00:28:57,330 --> 00:29:01,060
identifiers about what sorts of

00:28:59,410 --> 00:29:07,210
workloads they're allowed to ask about

00:29:01,060 --> 00:29:09,940
or get right so that when say a cell was

00:29:07,210 --> 00:29:13,750
compromised and it goes to talk to BBS

00:29:09,940 --> 00:29:17,380
it was only able to get the workloads in

00:29:13,750 --> 00:29:20,170
the blue segment for example so I think

00:29:17,380 --> 00:29:24,220
there are some ideas around here but

00:29:20,170 --> 00:29:26,890
it's it's not currently prioritized but

00:29:24,220 --> 00:29:28,900
I think if you're interested perhaps

00:29:26,890 --> 00:29:31,650
reach out reaching out to you to Eric

00:29:28,900 --> 00:29:31,650
over there

00:29:40,080 --> 00:29:46,800
um I think we can all agree that the the

00:29:44,890 --> 00:29:50,320
security work that has been done is

00:29:46,800 --> 00:29:54,550
pretty good absolutely there is only one

00:29:50,320 --> 00:29:56,590
thing that is a little bit left out

00:29:54,550 --> 00:29:59,770
maybe in this picture and that is

00:29:56,590 --> 00:30:04,450
actually vulnerability management for

00:29:59,770 --> 00:30:06,670
the applications themselves right we

00:30:04,450 --> 00:30:08,710
already have a better story with build

00:30:06,670 --> 00:30:11,800
paths and what that means for

00:30:08,710 --> 00:30:14,440
applications but I think there could be

00:30:11,800 --> 00:30:16,570
something to be done there that would

00:30:14,440 --> 00:30:18,310
improve the story for uptick for our

00:30:16,570 --> 00:30:21,120
users right

00:30:18,310 --> 00:30:25,750
is there anything planned for that I

00:30:21,120 --> 00:30:27,670
think there's nothing currently in plan

00:30:25,750 --> 00:30:29,980
at least not in the near term there

00:30:27,670 --> 00:30:32,500
there are things being investigated I

00:30:29,980 --> 00:30:35,860
think there was an idea that for example

00:30:32,500 --> 00:30:40,060
you could consider in that multi built

00:30:35,860 --> 00:30:43,390
pack thing perhaps you have a black

00:30:40,060 --> 00:30:45,700
black duct build pack that inserts

00:30:43,390 --> 00:30:49,120
itself at a certain point during the

00:30:45,700 --> 00:30:52,060
staging process to scan that app that's

00:30:49,120 --> 00:30:53,620
kind of in in the source and is that the

00:30:52,060 --> 00:30:55,960
right place to insert it should it have

00:30:53,620 --> 00:30:59,650
been done in an earlier pipeline at the

00:30:55,960 --> 00:31:01,510
code the there's a few different

00:30:59,650 --> 00:31:05,950
considerations there but but that was

00:31:01,510 --> 00:31:08,410
one idea sure that's obviously possible

00:31:05,950 --> 00:31:10,180
as it is possible to do it like during

00:31:08,410 --> 00:31:12,670
the deployment pipeline right but the

00:31:10,180 --> 00:31:16,000
thing is that often like application

00:31:12,670 --> 00:31:18,420
teams deploy something and is free of

00:31:16,000 --> 00:31:20,980
vulnerabilities right at the ploy time

00:31:18,420 --> 00:31:24,670
that might not be the situation in one

00:31:20,980 --> 00:31:28,300
week later right how to deal with that

00:31:24,670 --> 00:31:32,380
is was was before the main focus of my

00:31:28,300 --> 00:31:34,780
question actually how to deal like with

00:31:32,380 --> 00:31:36,090
like incoming vulnerabilities meaning

00:31:34,780 --> 00:31:39,090
when vulnerabilities are discovered

00:31:36,090 --> 00:31:41,200
after the application has been deployed

00:31:39,090 --> 00:31:43,240
something that is part of the build pack

00:31:41,200 --> 00:31:49,180
can warn you maybe during staging right

00:31:43,240 --> 00:31:50,860
yes oh I see um so we have and and this

00:31:49,180 --> 00:31:51,559
is something you you might want to

00:31:50,860 --> 00:31:54,769
follow up

00:31:51,559 --> 00:31:57,320
Stephen Levine perhaps at least on the

00:31:54,769 --> 00:31:58,970
built pack side of the house we have

00:31:57,320 --> 00:32:04,039
thought about like could we provide

00:31:58,970 --> 00:32:06,409
additional metadata on on the app such

00:32:04,039 --> 00:32:09,019
that we could log what dependencies did

00:32:06,409 --> 00:32:11,840
actually pull down which build pack

00:32:09,019 --> 00:32:14,269
exactly did it did it staged with and

00:32:11,840 --> 00:32:16,039
with that information some about what

00:32:14,269 --> 00:32:18,409
specific dependencies did it pull down

00:32:16,039 --> 00:32:20,840
and what build pacted actually staged

00:32:18,409 --> 00:32:24,139
with and some of that information you do

00:32:20,840 --> 00:32:26,539
get with the new v3 API is when you go

00:32:24,139 --> 00:32:30,889
through the new staging process at least

00:32:26,539 --> 00:32:32,539
which build pack was used with it but

00:32:30,889 --> 00:32:33,860
but I think there could be more metadata

00:32:32,539 --> 00:32:35,960
and that's something that he's been

00:32:33,860 --> 00:32:37,850
looking at and once you have more

00:32:35,960 --> 00:32:41,690
metadata how to query that's that

00:32:37,850 --> 00:32:44,960
information out based on knowing oh this

00:32:41,690 --> 00:32:48,730
dependency has a vulnerability let me

00:32:44,960 --> 00:32:48,730
scan through my systems metadata

00:32:54,690 --> 00:32:59,890
all right I think that's it thank you

00:32:57,669 --> 00:33:03,029
very much

00:32:59,890 --> 00:33:03,029

YouTube URL: https://www.youtube.com/watch?v=NRAGp8z6wzA


