Title: Robot Tracers - Extraction and Classification at scale using & CNTK
Publication date: 2018-08-24
Playlist: SotM 2018, Day 3, S.0.2
Description: 
	Nikola Trifunovic (Microsoft), State of the Map 2018
https://2018.stateofthemap.org/2018/T133-Robot_Tracers_-_Extraction_and_Classification_at_scale_using___CNTK/

During State of the Map US Microsoft outlined it's target to leverage computer vision to create a building footprint indistinguishable from a human traced edit. We believe we have hit that milestone for at least 75% of the cases for the United States.

In this technical talk Microsoft will describe using Azure COSMOS DB in conjunction with the Open Source CNTK framework to develop feature extraction models and polygonization algorithms for footprint extraction at scale. We will present the relevance of building footprints as well as other potential targets for computer vision including coastlines, water features, surface=* tags and land use.
Captions: 
	00:00:00,469 --> 00:00:08,730
hello okay so hi everybody and welcome

00:00:06,259 --> 00:00:11,639
thank you for your interest and joining

00:00:08,730 --> 00:00:13,920
with us here so today I'm going to talk

00:00:11,639 --> 00:00:15,990
about one project which put our

00:00:13,920 --> 00:00:18,960
infrastructure in Microsoft to a great

00:00:15,990 --> 00:00:22,020
test and that is building extraction

00:00:18,960 --> 00:00:24,660
from satellite imagery so here you can

00:00:22,020 --> 00:00:27,150
see one snapshot of our output and we

00:00:24,660 --> 00:00:29,730
did its extraction for the whole United

00:00:27,150 --> 00:00:33,360
States and so far this was very fun and

00:00:29,730 --> 00:00:35,910
interesting project for us so basically

00:00:33,360 --> 00:00:38,850
today I will try to dive deep into

00:00:35,910 --> 00:00:42,360
technical details how we did it share

00:00:38,850 --> 00:00:44,820
some of our learnings showcase some

00:00:42,360 --> 00:00:48,000
problems that we face some of the

00:00:44,820 --> 00:00:50,370
problems that remained and basically

00:00:48,000 --> 00:00:54,989
trying to improve your perspective and

00:00:50,370 --> 00:01:00,329
insights what actually is needed for

00:00:54,989 --> 00:01:03,390
this to be done so this is some basic

00:01:00,329 --> 00:01:05,430
overview of our solution so we have our

00:01:03,390 --> 00:01:08,460
component defects floods buildings

00:01:05,430 --> 00:01:11,460
add input our satellite imagery and the

00:01:08,460 --> 00:01:14,100
output the output are produced building

00:01:11,460 --> 00:01:16,259
vectors inside the solution actually

00:01:14,100 --> 00:01:18,600
there are two stages first stage is

00:01:16,259 --> 00:01:23,189
semantic segmentation a deep neural net

00:01:18,600 --> 00:01:24,750
that the classified image pixels into

00:01:23,189 --> 00:01:27,689
whether there are buildings or not and

00:01:24,750 --> 00:01:30,960
once we have this building pixels views

00:01:27,689 --> 00:01:34,770
in polygon is a stage we convert them

00:01:30,960 --> 00:01:38,369
into the polygons as you can see this is

00:01:34,770 --> 00:01:40,770
a two-stage process ideally what we

00:01:38,369 --> 00:01:43,710
engineers like is to make it like one

00:01:40,770 --> 00:01:45,869
stage the event when trainable because

00:01:43,710 --> 00:01:49,110
whenever you have a like cascade the

00:01:45,869 --> 00:01:50,670
cascade is the design when you make

00:01:49,110 --> 00:01:52,140
improvements on the first stage you

00:01:50,670 --> 00:01:55,680
actually hope that these improvements

00:01:52,140 --> 00:01:58,920
will propagate for the next stages so so

00:01:55,680 --> 00:02:01,079
far since we are still developing this

00:01:58,920 --> 00:02:03,930
matter be every kind of improvement we

00:02:01,079 --> 00:02:06,450
did actually was showcased at the end

00:02:03,930 --> 00:02:09,179
point but sometimes in the future when

00:02:06,450 --> 00:02:11,640
the project becomes more mature and get

00:02:09,179 --> 00:02:13,799
saturated actually some kind of solution

00:02:11,640 --> 00:02:16,739
that is end-to-end trainable would be

00:02:13,799 --> 00:02:18,750
preferred so what is actually needed of

00:02:16,739 --> 00:02:21,870
resources in order for this to be done

00:02:18,750 --> 00:02:23,609
so you need excellent building labels so

00:02:21,870 --> 00:02:25,920
your algorithm can learn what actually

00:02:23,609 --> 00:02:28,049
is the building on the image you need

00:02:25,920 --> 00:02:30,680
the great and big infrastructure to run

00:02:28,049 --> 00:02:34,439
and produce all of this Delta and

00:02:30,680 --> 00:02:36,780
regarding human resources in our case it

00:02:34,439 --> 00:02:40,889
was three software developers working on

00:02:36,780 --> 00:02:42,780
this for one year we had some previously

00:02:40,889 --> 00:02:44,790
we had some experience with machine

00:02:42,780 --> 00:02:47,189
learning but fairly limited with deep

00:02:44,790 --> 00:02:53,129
learning learning but in this one year

00:02:47,189 --> 00:02:54,810
we pick fell out so regarding imagery in

00:02:53,129 --> 00:02:56,639
Microsoft with open imagery by

00:02:54,810 --> 00:02:59,099
referencing them through their quad kids

00:02:56,639 --> 00:03:02,489
I think it's also very similar in other

00:02:59,099 --> 00:03:05,609
systems there are this different level

00:03:02,489 --> 00:03:08,669
of details you can like choose they go

00:03:05,609 --> 00:03:11,280
18 19 20 depending what kind of

00:03:08,669 --> 00:03:13,199
resolution you want we select igniting

00:03:11,280 --> 00:03:16,620
because for the human eyes that is

00:03:13,199 --> 00:03:17,940
something that is desirable it's our

00:03:16,620 --> 00:03:19,590
human hunch

00:03:17,940 --> 00:03:21,180
basically what you would like to do we

00:03:19,590 --> 00:03:23,819
would like to test with different levels

00:03:21,180 --> 00:03:27,120
but this is very time consuming and very

00:03:23,819 --> 00:03:29,939
too much resources are needed with 11:18

00:03:27,120 --> 00:03:32,010
you might also recognize buildings

00:03:29,939 --> 00:03:34,709
mostly but for them for example other

00:03:32,010 --> 00:03:36,870
objects are cars like us lose their

00:03:34,709 --> 00:03:39,209
properties they are too much pixelized

00:03:36,870 --> 00:03:41,120
and even though we are not recognizing

00:03:39,209 --> 00:03:43,109
cars we want our deep learning

00:03:41,120 --> 00:03:45,629
inherently to recognize this object

00:03:43,109 --> 00:03:47,370
because the final decision is based of

00:03:45,629 --> 00:03:49,590
all of these objects and how they

00:03:47,370 --> 00:03:52,019
interact because like recognizing

00:03:49,590 --> 00:03:54,629
acharyas is a strong picture that there

00:03:52,019 --> 00:03:57,479
is not a bad video but yeah that's just

00:03:54,629 --> 00:04:00,030
one part in big big equation regarding

00:03:57,479 --> 00:04:02,819
the scale we have in US four billion

00:04:00,030 --> 00:04:05,340
like of these styles and one important

00:04:02,819 --> 00:04:07,859
thing is that not all of the tiles the

00:04:05,340 --> 00:04:09,989
stars are shot by a single camera we

00:04:07,859 --> 00:04:12,299
have like 14 different cameras with

00:04:09,989 --> 00:04:14,220
different camera properties ideally you

00:04:12,299 --> 00:04:16,440
would like to train a one single neural

00:04:14,220 --> 00:04:21,510
net with all of them but that that is

00:04:16,440 --> 00:04:23,010
not optimal or it a yuge time of poor

00:04:21,510 --> 00:04:25,470
investigating out this in

00:04:23,010 --> 00:04:27,450
properly done so basically what we did

00:04:25,470 --> 00:04:28,320
we cluster these photographs into

00:04:27,450 --> 00:04:30,630
hypothesis

00:04:28,320 --> 00:04:33,170
pato graphs and lo focus photographs and

00:04:30,630 --> 00:04:41,370
create Vienna Network for each of these

00:04:33,170 --> 00:04:43,410
problems so regarding our labels we use

00:04:41,370 --> 00:04:45,270
bolder buildings as our labels both the

00:04:43,410 --> 00:04:47,310
buildings that are buildings that we

00:04:45,270 --> 00:04:49,230
came into possession by acquiring a

00:04:47,310 --> 00:04:52,890
boulder company which did the mapping

00:04:49,230 --> 00:04:54,690
there of excellent quality edges follow

00:04:52,890 --> 00:04:58,260
our building edges on images well

00:04:54,690 --> 00:05:00,230
develop they also very granular indeed a

00:04:58,260 --> 00:05:03,990
lot of building faster each building is

00:05:00,230 --> 00:05:06,000
separately identifiable they are very

00:05:03,990 --> 00:05:07,860
good aligned with being imagery and one

00:05:06,000 --> 00:05:10,590
of the most important properties for

00:05:07,860 --> 00:05:13,110
deep learning is they are clustered into

00:05:10,590 --> 00:05:15,330
fully labeled they put the label

00:05:13,110 --> 00:05:17,700
clusters so basically as they select it

00:05:15,330 --> 00:05:20,130
like 100 clusters of interest and

00:05:17,700 --> 00:05:23,310
started labeling systematically in these

00:05:20,130 --> 00:05:26,220
kind of areas so this is kind of data we

00:05:23,310 --> 00:05:28,950
want in the pranic we don't want sparse

00:05:26,220 --> 00:05:30,930
data because the images we feed into the

00:05:28,950 --> 00:05:33,360
deep neural Nets all buildings on these

00:05:30,930 --> 00:05:38,250
images must be recognized but must be

00:05:33,360 --> 00:05:40,590
present so we were lucky with that so

00:05:38,250 --> 00:05:42,570
creating the training set was not too

00:05:40,590 --> 00:05:45,960
hard for training the set you need the

00:05:42,570 --> 00:05:48,270
bare the pair of image and the pixel

00:05:45,960 --> 00:05:51,060
masks which show to the deep neural net

00:05:48,270 --> 00:05:54,510
how to learn to recognize buildings so

00:05:51,060 --> 00:05:56,970
once we get these clusters we just take

00:05:54,510 --> 00:05:59,550
drops of the images for there and take

00:05:56,970 --> 00:06:01,560
these labels from our spatial database

00:05:59,550 --> 00:06:03,840
rendered image where the building mask

00:06:01,560 --> 00:06:06,480
is even we added some of this

00:06:03,840 --> 00:06:08,520
uncertainty areas you pixels around the

00:06:06,480 --> 00:06:11,910
edges because this rendering because of

00:06:08,520 --> 00:06:15,450
pixelization rounding you know it can be

00:06:11,910 --> 00:06:17,100
offset by few pixels but and and because

00:06:15,450 --> 00:06:19,650
of some alignment problems and then we

00:06:17,100 --> 00:06:22,230
don't want that that have a huge cost in

00:06:19,650 --> 00:06:23,970
our training also first time can be

00:06:22,230 --> 00:06:26,700
trained with these images we have very

00:06:23,970 --> 00:06:29,670
good results in residential areas but in

00:06:26,700 --> 00:06:32,250
some rural areas of build earnest

00:06:29,670 --> 00:06:34,500
mountains glaciers buzzards and so on

00:06:32,250 --> 00:06:36,300
some of the images we didn't have in the

00:06:34,500 --> 00:06:38,490
training set we perform

00:06:36,300 --> 00:06:42,180
we perform bad so actually what we did

00:06:38,490 --> 00:06:45,930
from being for being knowledge we

00:06:42,180 --> 00:06:48,780
extracted areas different various areas

00:06:45,930 --> 00:06:53,280
and sampled from those and add it to the

00:06:48,780 --> 00:06:55,710
trends so what we end up with is a

00:06:53,280 --> 00:06:58,170
training side set which has like 5

00:06:55,710 --> 00:07:01,410
million hypothesis imagery and around 1

00:06:58,170 --> 00:07:05,450
million locusts imagery so in storage

00:07:01,410 --> 00:07:09,390
sense it is like 150 p gigabytes of data

00:07:05,450 --> 00:07:11,310
so it's a very big training set and for

00:07:09,390 --> 00:07:14,970
to be able to train on this big train

00:07:11,310 --> 00:07:16,890
then he said you need high you need a

00:07:14,970 --> 00:07:18,780
very good tool in the Microsoft we

00:07:16,890 --> 00:07:21,750
prefer C and B K as our deep learning

00:07:18,780 --> 00:07:24,660
platform because it makes possible to

00:07:21,750 --> 00:07:28,880
train on multiple GPUs for this case we

00:07:24,660 --> 00:07:32,790
use 64 GPUs and 64 GPUs have huge

00:07:28,880 --> 00:07:34,800
bandwidth and when you kept this huge

00:07:32,790 --> 00:07:37,230
banding one important thing is to think

00:07:34,800 --> 00:07:41,780
is can you read the fast the data as

00:07:37,230 --> 00:07:44,190
fast as these GPUs can process it but

00:07:41,780 --> 00:07:46,590
the antique infrastructure and in

00:07:44,190 --> 00:07:49,230
Microsoft taught already about this

00:07:46,590 --> 00:07:51,360
restore of data in HDFS it can be

00:07:49,230 --> 00:07:53,550
parallel streamed into these digi-fuse

00:07:51,360 --> 00:07:55,410
so we have a pretty good infrastructure

00:07:53,550 --> 00:07:58,430
for deliveries regarding react

00:07:55,410 --> 00:08:02,340
architecture we use resinate models with

00:07:58,430 --> 00:08:05,430
34 layers and perhaps an absent link we

00:08:02,340 --> 00:08:07,230
use refine it so if you're interested in

00:08:05,430 --> 00:08:12,000
more details about this you can contact

00:08:07,230 --> 00:08:16,050
me later so this is how our predictions

00:08:12,000 --> 00:08:18,500
look like we pretty much perform very

00:08:16,050 --> 00:08:21,150
good they are like some of

00:08:18,500 --> 00:08:23,700
misclassifications for example in the

00:08:21,150 --> 00:08:26,220
first case you see like the problems

00:08:23,700 --> 00:08:28,290
with the shadows but actually BNN was

00:08:26,220 --> 00:08:30,930
not confused with it in the second

00:08:28,290 --> 00:08:32,130
example again here the shadowing

00:08:30,930 --> 00:08:35,160
actually caused some

00:08:32,130 --> 00:08:39,540
miss classification also here you can

00:08:35,160 --> 00:08:43,080
see this is like some ground corridor

00:08:39,540 --> 00:08:45,150
miss also our natural screens used and

00:08:43,080 --> 00:08:47,850
you can see like a little holes and you

00:08:45,150 --> 00:08:49,050
can see how it was uncertain better like

00:08:47,850 --> 00:08:50,790
this area here

00:08:49,050 --> 00:08:52,320
is the building or not because you know

00:08:50,790 --> 00:08:54,420
this building ready to problematic

00:08:52,320 --> 00:08:59,010
because their roofs are similar to the

00:08:54,420 --> 00:09:00,810
neighboring roads and since we are still

00:08:59,010 --> 00:09:06,950
working in a pixel domain you can see

00:09:00,810 --> 00:09:10,589
our precision and recall in pixels so

00:09:06,950 --> 00:09:12,630
the next page is polygon ization you

00:09:10,589 --> 00:09:15,120
would think that the hardest part is

00:09:12,630 --> 00:09:17,180
like completed and the polygon ization

00:09:15,120 --> 00:09:20,250
is something that can be easily done

00:09:17,180 --> 00:09:21,600
that's what we thought as well so when

00:09:20,250 --> 00:09:24,240
you want to solve this problem first

00:09:21,600 --> 00:09:26,519
when you look to some existing solutions

00:09:24,240 --> 00:09:29,550
you come up with bagless package I have

00:09:26,519 --> 00:09:31,589
written which polygon eyes is the

00:09:29,550 --> 00:09:33,839
predictions the problem is that this

00:09:31,589 --> 00:09:38,390
algorithm in its nature is very greedy

00:09:33,839 --> 00:09:41,670
so it makes some decisions based on some

00:09:38,390 --> 00:09:43,680
on some local space so basically those

00:09:41,670 --> 00:09:47,760
pixel based pixels and then some pixel

00:09:43,680 --> 00:09:50,010
is offset it on a line for some some

00:09:47,760 --> 00:09:53,279
value it makes a decision that that's a

00:09:50,010 --> 00:09:55,560
new edge and you get some images like

00:09:53,279 --> 00:09:57,660
this you know which really polygons

00:09:55,560 --> 00:09:59,670
which looked really funny you know

00:09:57,660 --> 00:10:01,279
they ought to actually don't look like

00:09:59,670 --> 00:10:03,990
buildings no human can say you know

00:10:01,279 --> 00:10:04,800
these are not buildings they don't look

00:10:03,990 --> 00:10:07,410
like that

00:10:04,800 --> 00:10:11,130
so actually what we want is the polygons

00:10:07,410 --> 00:10:13,500
ization algorithm which can look at the

00:10:11,130 --> 00:10:17,399
prediction as a whole and make these

00:10:13,500 --> 00:10:20,100
polygons ization decisions at once so

00:10:17,399 --> 00:10:23,540
they look for this solution we were not

00:10:20,100 --> 00:10:26,520
able to find it so we invented our so

00:10:23,540 --> 00:10:28,730
what we do here is some brief example

00:10:26,520 --> 00:10:31,980
how we do it so we extract the outline

00:10:28,730 --> 00:10:34,230
wrinkles and any kind of mathematical

00:10:31,980 --> 00:10:37,560
operation on to the in 2d space is very

00:10:34,230 --> 00:10:41,430
complex so we transform it to the 1d

00:10:37,560 --> 00:10:43,740
space do some fitting and produce the

00:10:41,430 --> 00:10:47,519
final product so actually here what we

00:10:43,740 --> 00:10:50,220
do we create a turning function of the

00:10:47,519 --> 00:10:53,130
input input curve so basically this

00:10:50,220 --> 00:10:56,130
function contain the angle between each

00:10:53,130 --> 00:10:59,180
consecutive points so if you have a line

00:10:56,130 --> 00:11:00,720
in a line each consecutive point form

00:10:59,180 --> 00:11:03,180
since

00:11:00,720 --> 00:11:05,280
same angle so lines of

00:11:03,180 --> 00:11:07,650
be identified light horizontal lines

00:11:05,280 --> 00:11:09,780
here and since polygon is a set of line

00:11:07,650 --> 00:11:12,300
what you actually want to do once you

00:11:09,780 --> 00:11:15,360
have this 1d representation is to fit a

00:11:12,300 --> 00:11:23,910
stepwise function in order to obtain the

00:11:15,360 --> 00:11:25,680
polygon lines so this produced a little

00:11:23,910 --> 00:11:28,710
bit better results then Douglas pecker

00:11:25,680 --> 00:11:31,770
but still not as good because again

00:11:28,710 --> 00:11:34,830
these predictions don't actually look

00:11:31,770 --> 00:11:36,810
like buildings and now the question is

00:11:34,830 --> 00:11:39,000
you know what exactly is building you

00:11:36,810 --> 00:11:40,740
know what is some of our a priori

00:11:39,000 --> 00:11:43,290
knowledge how building actually looks

00:11:40,740 --> 00:11:44,700
and when you think about it usually

00:11:43,290 --> 00:11:48,660
buildings you know to each consecutive

00:11:44,700 --> 00:11:52,020
edge form right 90 degrees angle usually

00:11:48,660 --> 00:11:54,600
the edges are of length more than 3 or 5

00:11:52,020 --> 00:11:56,700
meters you cannot have a consecutive

00:11:54,600 --> 00:11:58,620
edges which form light less than 15

00:11:56,700 --> 00:12:02,430
degrees these are like numbers I'm just

00:11:58,620 --> 00:12:05,880
drawing out of my head but still that so

00:12:02,430 --> 00:12:10,560
we imposed some rules in this illegal is

00:12:05,880 --> 00:12:12,570
a ssin in order to when we fit this

00:12:10,560 --> 00:12:15,570
function we want to avoid this kind of

00:12:12,570 --> 00:12:17,820
situation so when we did this line if

00:12:15,570 --> 00:12:20,100
it's like in the next pitch forms like

00:12:17,820 --> 00:12:22,530
80 degree angle we asked okay can we

00:12:20,100 --> 00:12:25,020
just slide it over to be 90 degree and

00:12:22,530 --> 00:12:28,740
the way if the cost error cost is like

00:12:25,020 --> 00:12:30,660
remains similarly just there so actually

00:12:28,740 --> 00:12:34,350
there are a lot of manually controlled

00:12:30,660 --> 00:12:35,160
parameters which makes this process a

00:12:34,350 --> 00:12:37,850
little bit hard

00:12:35,160 --> 00:12:40,410
so we basically choose what are these

00:12:37,850 --> 00:12:43,440
constraints and then then we do some

00:12:40,410 --> 00:12:46,020
random search in on cross-validation set

00:12:43,440 --> 00:12:48,720
to find the optimal thresholds and so on

00:12:46,020 --> 00:12:51,090
actually what we would like and we will

00:12:48,720 --> 00:12:53,730
definitely export explore next is how to

00:12:51,090 --> 00:12:55,440
do this in automatically since we

00:12:53,730 --> 00:12:57,990
already here but huge set of existing

00:12:55,440 --> 00:13:01,350
buildings maybe we can create a neural

00:12:57,990 --> 00:13:03,480
net which is which will incur in itself

00:13:01,350 --> 00:13:05,040
encode some building properties and

00:13:03,480 --> 00:13:06,960
which can provide feedback to the

00:13:05,040 --> 00:13:09,290
polygons ization algorithm to produce

00:13:06,960 --> 00:13:11,850
the polygons that actually look like

00:13:09,290 --> 00:13:14,030
buildings that exist currently in the

00:13:11,850 --> 00:13:14,030
world

00:13:14,370 --> 00:13:20,249
so regarding the matrix so more

00:13:17,579 --> 00:13:22,860
completeness is like the most use matrix

00:13:20,249 --> 00:13:26,029
today it's like intersection over Union

00:13:22,860 --> 00:13:30,240
but actually this is not good enough

00:13:26,029 --> 00:13:32,430
metric to use for evaluation this metric

00:13:30,240 --> 00:13:34,680
will tell you typically that your

00:13:32,430 --> 00:13:39,329
prediction is those or is of good size

00:13:34,680 --> 00:13:42,209
is on right location but when you see

00:13:39,329 --> 00:13:44,699
the prediction they have high map value

00:13:42,209 --> 00:13:46,170
of this metric you just see your human

00:13:44,699 --> 00:13:49,050
eye can see you know this is not that

00:13:46,170 --> 00:13:51,569
building that looks like the label

00:13:49,050 --> 00:13:54,180
so this first metric tells you

00:13:51,569 --> 00:13:58,170
something's so that's why we introduced

00:13:54,180 --> 00:14:00,329
additional metric which tells you so

00:13:58,170 --> 00:14:02,180
which tells you what what is the shape

00:14:00,329 --> 00:14:05,249
of the building so if the label is

00:14:02,180 --> 00:14:07,709
square and your prediction is triangle

00:14:05,249 --> 00:14:10,379
it would produce a bad score so

00:14:07,709 --> 00:14:13,139
basically this other metric shows how

00:14:10,379 --> 00:14:15,300
the shapes are similar but even in this

00:14:13,139 --> 00:14:17,759
case you have similar shapes you have

00:14:15,300 --> 00:14:20,579
very strong intersection over Union but

00:14:17,759 --> 00:14:23,999
your rotation is quite not right and for

00:14:20,579 --> 00:14:27,240
human eye even one degree of rotation is

00:14:23,999 --> 00:14:29,639
you simply notice it I don't like when I

00:14:27,240 --> 00:14:31,620
see my predictions form rotation angle

00:14:29,639 --> 00:14:33,870
more than one degree so this is also a

00:14:31,620 --> 00:14:36,809
third metric which you must also

00:14:33,870 --> 00:14:39,809
consider and and this now becomes very

00:14:36,809 --> 00:14:42,809
big problem for for the next steps of

00:14:39,809 --> 00:14:45,089
how to improve the the cold process

00:14:42,809 --> 00:14:47,579
because now we can ask you know should

00:14:45,089 --> 00:14:49,980
we improved our DNN should improve our

00:14:47,579 --> 00:14:51,870
polymerization or maybe create

00:14:49,980 --> 00:14:54,149
everything to be end-to-end trainable

00:14:51,870 --> 00:14:56,370
but what is the purpose if we still not

00:14:54,149 --> 00:14:58,980
sure you know what is the matrix how to

00:14:56,370 --> 00:15:04,379
measure what is better and what is not

00:14:58,980 --> 00:15:06,420
so here are some iteration results of

00:15:04,379 --> 00:15:09,800
our extraction on our village on our

00:15:06,420 --> 00:15:13,139
revelation set which is 15,000 buildings

00:15:09,800 --> 00:15:16,529
so our matching matrix is our precision

00:15:13,139 --> 00:15:18,929
is about 99% so less than 1% of the

00:15:16,529 --> 00:15:22,110
buildings we make our predictions so

00:15:18,929 --> 00:15:25,920
that there is actually no building but

00:15:22,110 --> 00:15:27,870
let's look at the quality matrix so we

00:15:25,920 --> 00:15:30,150
have these three matrix complete

00:15:27,870 --> 00:15:32,490
shaped instance rotation distance and

00:15:30,150 --> 00:15:35,400
one interesting thing here is that our

00:15:32,490 --> 00:15:40,500
extracted buildings generally better

00:15:35,400 --> 00:15:43,710
metrics than OSM so yeah Matt metrics

00:15:40,500 --> 00:15:46,950
doesn't lie but what you need to always

00:15:43,710 --> 00:15:49,260
to remember yourself in looking into the

00:15:46,950 --> 00:15:51,420
matrix that matrix are not perfect they

00:15:49,260 --> 00:15:53,430
don't lie but they tell you something

00:15:51,420 --> 00:15:56,130
completely different you can remember

00:15:53,430 --> 00:15:59,790
that actually our labels of Boulder

00:15:56,130 --> 00:16:03,150
buildings label by humans and the SM is

00:15:59,790 --> 00:16:07,410
also labels label by humans so our

00:16:03,150 --> 00:16:09,570
labels are not like created by God so if

00:16:07,410 --> 00:16:12,029
you see like some what would be like

00:16:09,570 --> 00:16:12,960
perfect label in some space it can be

00:16:12,029 --> 00:16:16,320
somewhere here

00:16:12,960 --> 00:16:18,420
Boulder buildings error can go in this

00:16:16,320 --> 00:16:22,529
direction in this space or SM buildings

00:16:18,420 --> 00:16:27,150
can go in other direction so it becomes

00:16:22,529 --> 00:16:32,640
very we need to rethink actually how the

00:16:27,150 --> 00:16:34,200
matrix should be done what we can say

00:16:32,640 --> 00:16:36,779
from this result that we are pretty

00:16:34,200 --> 00:16:40,770
close at the quality of our buildings is

00:16:36,779 --> 00:16:44,360
pretty close to both OS m and both and

00:16:40,770 --> 00:16:47,339
bowler buildings here you can see charts

00:16:44,360 --> 00:16:49,320
histograms the majority of our buildings

00:16:47,339 --> 00:16:52,200
on completeness are better on turning

00:16:49,320 --> 00:16:54,630
vistas are better but these charts of

00:16:52,200 --> 00:16:57,600
rotation angle gives gives us some

00:16:54,630 --> 00:17:02,220
interesting insight this job because you

00:16:57,600 --> 00:17:03,959
can see up to top 90% when these metric

00:17:02,220 --> 00:17:08,040
or ecstatic buildings are better than

00:17:03,959 --> 00:17:10,370
osm but Donald suddenly on last 5% this

00:17:08,040 --> 00:17:14,339
average becomes to rise very quickly

00:17:10,370 --> 00:17:16,530
which means that our buildings on

00:17:14,339 --> 00:17:19,920
average are very good but we still have

00:17:16,530 --> 00:17:21,929
problems on some outliers on some small

00:17:19,920 --> 00:17:24,630
subset of buildings there we are pretty

00:17:21,929 --> 00:17:26,100
bad and usually this pretty bad

00:17:24,630 --> 00:17:28,980
buildings are large buildings

00:17:26,100 --> 00:17:32,130
skyscrapers because we kept them like

00:17:28,980 --> 00:17:35,880
very small amount in our data set and

00:17:32,130 --> 00:17:38,130
the other thing in order to deep neural

00:17:35,880 --> 00:17:40,710
net to see the whole view of this large

00:17:38,130 --> 00:17:43,650
building you need to be much deeper

00:17:40,710 --> 00:17:45,810
this is some compromises that need to be

00:17:43,650 --> 00:17:47,370
made but in our case this is not

00:17:45,810 --> 00:17:49,710
interesting because actually

00:17:47,370 --> 00:17:51,870
osm already have this building the whole

00:17:49,710 --> 00:17:55,940
point of what we do is actually to fill

00:17:51,870 --> 00:18:00,060
the gaps so stole 100 terabytes of data

00:17:55,940 --> 00:18:02,760
it took like two weeks on 4464 GPUs to

00:18:00,060 --> 00:18:04,790
extract all the buildings since our

00:18:02,760 --> 00:18:07,470
model is fully convolutional rerun on

00:18:04,790 --> 00:18:09,750
larger buildings when running when large

00:18:07,470 --> 00:18:12,990
buildings you connect higher resolution

00:18:09,750 --> 00:18:15,450
better resolution images there are

00:18:12,990 --> 00:18:18,210
artificial lines formed our training

00:18:15,450 --> 00:18:20,970
model didn't learn how to differentiate

00:18:18,210 --> 00:18:23,370
this so in these lines it produces like

00:18:20,970 --> 00:18:26,190
some problematic predictions so in our

00:18:23,370 --> 00:18:30,740
could be filter area where these high

00:18:26,190 --> 00:18:30,740
and low resolution images are connected

00:18:30,800 --> 00:18:37,010
we publish the buildings on being so

00:18:34,800 --> 00:18:40,950
what we did be to cause and buildings

00:18:37,010 --> 00:18:43,830
and edit all CV buildings that are not

00:18:40,950 --> 00:18:45,690
intersecting with EOS and also our

00:18:43,830 --> 00:18:49,020
buildings you can find on our github

00:18:45,690 --> 00:18:52,230
repo what we additionally think if you

00:18:49,020 --> 00:18:54,540
were here like two hours ago in

00:18:52,230 --> 00:18:57,270
Australia we had problems with paved

00:18:54,540 --> 00:18:58,950
property of the road so basically once

00:18:57,270 --> 00:19:01,050
you have all those distribution from

00:18:58,950 --> 00:19:03,240
blank paper to some design and

00:19:01,050 --> 00:19:08,250
processing for the whole Australia took

00:19:03,240 --> 00:19:08,790
us basically two weeks and one more

00:19:08,250 --> 00:19:10,650
thing

00:19:08,790 --> 00:19:12,780
we are getting more and more different

00:19:10,650 --> 00:19:15,510
kind of images read side imagery is

00:19:12,780 --> 00:19:18,720
oblique images we can do very keen to

00:19:15,510 --> 00:19:20,880
extract various things we can even think

00:19:18,720 --> 00:19:24,300
about how to combine all of these images

00:19:20,880 --> 00:19:26,350
and process them in power so glad

00:19:24,300 --> 00:19:33,040
surrogacy thank you

00:19:26,350 --> 00:19:33,040
[Applause]

00:19:36,429 --> 00:19:43,669
hi how did your algorithm perform on

00:19:40,190 --> 00:19:46,250
round buildings like oil tanks thank you

00:19:43,669 --> 00:19:49,519
variation so as you can see our polygon

00:19:46,250 --> 00:19:52,640
ization algorithm pit lines so actually

00:19:49,519 --> 00:19:55,370
we don't do that one curve degrees so

00:19:52,640 --> 00:20:03,529
that's also another out outlier in our

00:19:55,370 --> 00:20:05,059
metrics okay I would imagine that in

00:20:03,529 --> 00:20:07,340
areas in which buildings are really

00:20:05,059 --> 00:20:09,980
close to one another your predictions

00:20:07,340 --> 00:20:11,690
might actually overlap so how do you

00:20:09,980 --> 00:20:13,220
split them after do you have you

00:20:11,690 --> 00:20:15,320
experimented with that I may be instance

00:20:13,220 --> 00:20:17,149
second tation or you just do some like

00:20:15,320 --> 00:20:19,399
morphological transformations like

00:20:17,149 --> 00:20:20,510
opening closing in order to separate

00:20:19,399 --> 00:20:22,760
different buildings where your

00:20:20,510 --> 00:20:26,389
prediction is just like one block you

00:20:22,760 --> 00:20:28,370
know so so far we produced just a single

00:20:26,389 --> 00:20:31,130
building but that but we thought about

00:20:28,370 --> 00:20:33,049
these ideas so one idea that we tested

00:20:31,130 --> 00:20:35,960
since we have a parcels data for

00:20:33,049 --> 00:20:38,210
yourself us a to use parcels to split

00:20:35,960 --> 00:20:40,190
these buildings but yeah that approach

00:20:38,210 --> 00:20:42,409
when we do this and do additional

00:20:40,190 --> 00:20:43,909
instance segmentation which we can use

00:20:42,409 --> 00:20:50,570
to split the buildings that slightly the

00:20:43,909 --> 00:20:54,980
next step right okay so in your previous

00:20:50,570 --> 00:20:56,779
slide I saw like a 45 angle image I

00:20:54,980 --> 00:21:00,649
think if I'm correct the the the one

00:20:56,779 --> 00:21:03,200
below so do you think like a future step

00:21:00,649 --> 00:21:05,570
would be to also capture or detect the

00:21:03,200 --> 00:21:08,179
height of the building and maybe I don't

00:21:05,570 --> 00:21:10,340
know like the the 3d shape at some point

00:21:08,179 --> 00:21:14,269
yes so basically with this oblique

00:21:10,340 --> 00:21:16,340
imagery like our some initial tests show

00:21:14,269 --> 00:21:19,370
that each of this location is shot with

00:21:16,340 --> 00:21:23,269
like almost 40 cameras from different

00:21:19,370 --> 00:21:24,919
azimuth and elevation angles so creating

00:21:23,269 --> 00:21:27,860
some 3d perspective sounds very

00:21:24,919 --> 00:21:31,810
interesting and possible so yeah maybe

00:21:27,860 --> 00:21:34,880
we are contemplating on these things

00:21:31,810 --> 00:21:34,880
[Music]

00:21:36,799 --> 00:21:42,600
hello I'm interested in how do you

00:21:40,020 --> 00:21:46,049
handle the parallettes row so if you

00:21:42,600 --> 00:21:48,630
have your aerial imagery it's not always

00:21:46,049 --> 00:21:50,940
taken center down so the footprint on

00:21:48,630 --> 00:21:53,370
the ground is not the outline of the

00:21:50,940 --> 00:21:55,410
roof of the building so how do you shift

00:21:53,370 --> 00:21:59,250
the footprint later to the correct

00:21:55,410 --> 00:22:01,289
position on the ground so we don't do

00:21:59,250 --> 00:22:04,500
any kind of post processing and

00:22:01,289 --> 00:22:06,390
pre-processing so basically if we have

00:22:04,500 --> 00:22:09,140
shots from the angle it might be that

00:22:06,390 --> 00:22:13,020
our footprint is not quite well

00:22:09,140 --> 00:22:15,870
corrected but what we also showed it

00:22:13,020 --> 00:22:19,080
somehow you know these deep neural nets

00:22:15,870 --> 00:22:21,270
are fascinating based on the shadow you

00:22:19,080 --> 00:22:23,640
know sometimes it produced prediction

00:22:21,270 --> 00:22:26,400
like that is offsetting not like the

00:22:23,640 --> 00:22:28,230
roof like follow the footprint just

00:22:26,400 --> 00:22:30,270
because the label was like that you know

00:22:28,230 --> 00:22:32,159
then it gets label which is exactly the

00:22:30,270 --> 00:22:35,640
footprint and there is some shadow

00:22:32,159 --> 00:22:37,830
nearby these DNS are amazing in some

00:22:35,640 --> 00:22:40,409
cases even they figure it out on their

00:22:37,830 --> 00:22:44,520
own so but yeah we didn't do any kind of

00:22:40,409 --> 00:22:47,630
additional processing product hi have

00:22:44,520 --> 00:22:50,429
you experienced the tiling issues I

00:22:47,630 --> 00:22:53,990
didn't care have you experienced the

00:22:50,429 --> 00:22:57,690
tiling issues when you perform detection

00:22:53,990 --> 00:23:00,120
so yeah they are tiling issues so that's

00:22:57,690 --> 00:23:02,190
why when you run your model so we

00:23:00,120 --> 00:23:04,110
training all these like small tiles but

00:23:02,190 --> 00:23:06,809
when we run it we run it on these huge

00:23:04,110 --> 00:23:09,390
tiles but then again UK there is some

00:23:06,809 --> 00:23:11,760
limitation by GPU memory what is the

00:23:09,390 --> 00:23:15,929
maximum image size so what we actually

00:23:11,760 --> 00:23:18,120
do we take predictions but there is

00:23:15,929 --> 00:23:22,590
always like this kind of small overlap

00:23:18,120 --> 00:23:23,820
between it and this is not like yeah

00:23:22,590 --> 00:23:32,100
that was the problem but we are

00:23:23,820 --> 00:23:33,990
resolving questions you know one about

00:23:32,100 --> 00:23:36,390
the validation so you you just ground

00:23:33,990 --> 00:23:37,980
through just OpenStreetMap data for the

00:23:36,390 --> 00:23:40,919
three messages you provided with

00:23:37,980 --> 00:23:44,520
rotation and differences and you show

00:23:40,919 --> 00:23:47,679
some outliers there the 90th

00:23:44,520 --> 00:23:54,340
you verify that that it's all babe I

00:23:47,679 --> 00:23:55,960
told you to the big some big and

00:23:54,340 --> 00:23:58,360
complicated buildings because in our

00:23:55,960 --> 00:24:01,299
polygon ization stage as I said we had

00:23:58,360 --> 00:24:04,000
some constraints so visually we make our

00:24:01,299 --> 00:24:05,620
add edges on our buildings to be like 90

00:24:04,000 --> 00:24:07,330
degrees but there are some funny looking

00:24:05,620 --> 00:24:08,830
buildings that actually don't look like

00:24:07,330 --> 00:24:13,090
buildings and our colleague organization

00:24:08,830 --> 00:24:15,840
fits them as we designed it so yeah you

00:24:13,090 --> 00:24:18,340
didn't use other like external

00:24:15,840 --> 00:24:22,149
footprints or buildings to check if

00:24:18,340 --> 00:24:24,549
those are not OS m means so actually our

00:24:22,149 --> 00:24:26,740
labels of these bolder buildings okay so

00:24:24,549 --> 00:24:28,539
these are very high quality when we do

00:24:26,740 --> 00:24:30,880
side-by-side comparison this between

00:24:28,539 --> 00:24:37,659
Boulder and the OS and Boulder was

00:24:30,880 --> 00:24:39,789
better in most of the cases okay how big

00:24:37,659 --> 00:24:42,159
was a misalignment entertaining data

00:24:39,789 --> 00:24:47,380
data between the satellite imagery and

00:24:42,159 --> 00:24:50,440
the building mass so we had luck with

00:24:47,380 --> 00:24:53,950
this dataset it seems like they used our

00:24:50,440 --> 00:24:55,450
Bing map to label to the buildings so we

00:24:53,950 --> 00:24:57,880
didn't have any problems with

00:24:55,450 --> 00:25:00,100
misalignment but misalignment will

00:24:57,880 --> 00:25:02,440
become problem very here then we

00:25:00,100 --> 00:25:04,899
obtained a new set of images new fresh

00:25:02,440 --> 00:25:06,880
images that might be out not any more

00:25:04,899 --> 00:25:08,470
aligned with Boulder buildings so we

00:25:06,880 --> 00:25:12,429
have we are now making some solution

00:25:08,470 --> 00:25:15,730
which uses live dislike in photography

00:25:12,429 --> 00:25:19,779
when you make like this 360 degree is

00:25:15,730 --> 00:25:22,000
how to align two similar so these images

00:25:19,779 --> 00:25:24,399
are similar like a one-year difference

00:25:22,000 --> 00:25:29,440
between they are taken and so our level

00:25:24,399 --> 00:25:31,059
can remain aligned thank you no time for

00:25:29,440 --> 00:25:33,240
other question so we need to pass your

00:25:31,059 --> 00:25:33,240
new

00:25:33,970 --> 00:25:37,499

YouTube URL: https://www.youtube.com/watch?v=1byoe8jbuTU


