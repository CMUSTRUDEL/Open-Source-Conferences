Title: Comprehensive OpenStreetMap History Data Analyses- for and with the OSM community
Publication date: 2018-09-28
Playlist: SotM 2018, Academic Track
Description: 
	Michael Auer, Melanie Eckle, Sascha Fendrich, Fabian Kowatsch, Sabrina Marx, Martin Raifer, Moritz Schott, Rafael Troilo, Alexander Zipf, State of the Map 2018
Heidelberg University, Heidelberg Institute for Geoinformation Technology, GIScience Research Group, Department of Geography
https://2018.stateofthemap.org/2018/A33-Comprehensive_OpenStreetMap_History_Data_Analyses-_for_and_with_the_OSM_community/

Increasing use of OpenStreetMap (OSM) data in various applications and use cases lead to a growing number of research studies in which OSM data, its contributors and usage, and the quality of OSM data are analyzed. These studies include extrinsic and intrinsic data analyses, and provide interesting results for and about the community and data users. Such studies were mostly limited to analyzing either small samples of the OSM database or to simple types of analyses due to the capabilities of existing services and software operating on OSM’s full history data, which includes all data and every change made on a global scale. OSM data and contributor monitoring over longer timeframes and covering larger areas could however provide more comprehensive insights about our community and the data we are producing, also including spatial variations and evolution over time. In order to better monitor and understand OSM mapping, mappers and the produced data quality, we developed a software platform that applies big data technology to OSM full history data. This framework allows detailed analyses of the OSM data evolution and the detection of remarkable patterns over time. The setup of the framework, including openly accessible APIs, will allow interested community members and researchers of varying levels of experience to perform analyses with different levels of complexity. Initial analyses that have been conducted using the framework focused on user activity monitoring, addressing the following questions: Are we retaining new mappers? How many people participated in a specific area over time? How many new mappers have been active in contrast to experienced mappers? This kind of analyses enable the detection of user activity patterns in different regions over time. Results provide an overview of less and more actively mapped places in the OSM database. As all changes are considered, temporal patterns can be detected as well as changes due to individual users´ activity. Such insights help to understand user behaviors and to get a better sense of areas that are not well represented in the OSM database and community. These analyses are initial examples that show the potential of our framework. Further analyses will be discussed and addressed in collaboration with the OSM community and interested researchers. The talk will also be complemented by a workshop to enable attendees to learn how to make use of the framework for their own research questions and analyses.
Captions: 
	00:00:00,990 --> 00:00:11,070
yes I'm happy to be here now and to

00:00:05,200 --> 00:00:13,419
present some ideas our approach of

00:00:11,070 --> 00:00:16,090
numeracy of Heidelberg and I'm

00:00:13,419 --> 00:00:18,190
presenting this not for me so on behalf

00:00:16,090 --> 00:00:22,060
of a whole group of researchers of

00:00:18,190 --> 00:00:24,570
Heidelberg were related to this and yeah

00:00:22,060 --> 00:00:28,420
I would like to talk about this a new

00:00:24,570 --> 00:00:32,289
framework we call it awesome which you

00:00:28,420 --> 00:00:34,780
can use to analyze history data and we

00:00:32,289 --> 00:00:39,550
have already hear about the challenges a

00:00:34,780 --> 00:00:41,040
little bit and yeah maybe we can solve

00:00:39,550 --> 00:00:44,590
some of the challenges with our

00:00:41,040 --> 00:00:47,710
framework so what does awesome actually

00:00:44,590 --> 00:00:50,410
means a we this is a artificial name of

00:00:47,710 --> 00:00:52,899
our framework and this composed of

00:00:50,410 --> 00:00:55,930
several ideas so there's osm inside

00:00:52,899 --> 00:00:58,980
obviously form Street Map then there's

00:00:55,930 --> 00:01:02,410
this H something is to read something

00:00:58,980 --> 00:01:04,600
inside so we have a tool that you can do

00:01:02,410 --> 00:01:07,690
something with history of Street no date

00:01:04,600 --> 00:01:13,210
and yeah obviously it just sounds

00:01:07,690 --> 00:01:15,820
awesome I would like to take the

00:01:13,210 --> 00:01:17,619
advantage to announce the workshop that

00:01:15,820 --> 00:01:21,610
is taking place right after the next

00:01:17,619 --> 00:01:23,530
break where you can learn how to use

00:01:21,610 --> 00:01:27,729
this framework to do this kind of

00:01:23,530 --> 00:01:32,700
analysis yourself and it will be in the

00:01:27,729 --> 00:01:37,210
other room s15 just after the next break

00:01:32,700 --> 00:01:40,990
so who are we who is actually developing

00:01:37,210 --> 00:01:42,850
this awesome thing we are from the

00:01:40,990 --> 00:01:46,240
University of Heidelberg in Germany and

00:01:42,850 --> 00:01:52,090
as the research group of GI science led

00:01:46,240 --> 00:01:54,729
by professorship and recently he created

00:01:52,090 --> 00:01:58,170
a new institute called Highgate and in

00:01:54,729 --> 00:02:01,780
this Highgate group three yeah focus

00:01:58,170 --> 00:02:03,579
research groups one is dealing or

00:02:01,780 --> 00:02:06,610
developing the open route service you

00:02:03,579 --> 00:02:09,509
may know this is around like 10 years

00:02:06,610 --> 00:02:12,220
using Open Street Map data for routing

00:02:09,509 --> 00:02:12,920
another group is dealing with disaster a

00:02:12,220 --> 00:02:15,170
new medium

00:02:12,920 --> 00:02:16,970
Taron management disclosed related also

00:02:15,170 --> 00:02:19,940
to the humanitarian OpenStreetMap team

00:02:16,970 --> 00:02:21,830
and so on and our team the big spatial

00:02:19,940 --> 00:02:26,870
data analytics group that i'm working

00:02:21,830 --> 00:02:29,780
also in is doing this awesome framework

00:02:26,870 --> 00:02:32,030
so before showing some examples some

00:02:29,780 --> 00:02:36,650
some details of the framework i would

00:02:32,030 --> 00:02:39,560
like to yeah share some thoughts of why

00:02:36,650 --> 00:02:41,270
should we analyze the history so what

00:02:39,560 --> 00:02:45,620
can we what can we learn from the

00:02:41,270 --> 00:02:48,820
history and yeah there are so many

00:02:45,620 --> 00:02:52,220
different possibilities to to make

00:02:48,820 --> 00:02:54,290
analyst and analysis with the history

00:02:52,220 --> 00:02:57,200
data so i thought it would be maybe a

00:02:54,290 --> 00:02:58,820
good idea to have some graphic in mind

00:02:57,200 --> 00:03:01,010
or some simple things so i draw this

00:02:58,820 --> 00:03:04,000
triangle here to have the different

00:03:01,010 --> 00:03:08,060
aspects that are always found in an

00:03:04,000 --> 00:03:10,850
history analysis so we have the data and

00:03:08,060 --> 00:03:13,010
then the user that created the data and

00:03:10,850 --> 00:03:17,269
the time stamp when the data was created

00:03:13,010 --> 00:03:21,230
so somehow these things are always in in

00:03:17,269 --> 00:03:23,120
history analysis and if you only look at

00:03:21,230 --> 00:03:25,400
the map and you just look at the data

00:03:23,120 --> 00:03:26,989
and you don't know who created the data

00:03:25,400 --> 00:03:28,549
you don't know when the data was created

00:03:26,989 --> 00:03:31,959
but in the history analysis we are

00:03:28,549 --> 00:03:36,620
always relating these things to the time

00:03:31,959 --> 00:03:39,709
property and you can for example just

00:03:36,620 --> 00:03:44,329
have a look at the relation between data

00:03:39,709 --> 00:03:46,310
and time and then make a nice graph of

00:03:44,329 --> 00:03:49,190
the development of the data how the data

00:03:46,310 --> 00:03:51,890
grew and so on but you can also look

00:03:49,190 --> 00:03:54,110
have a look at how the users developed

00:03:51,890 --> 00:03:56,570
over time or you can just put everything

00:03:54,110 --> 00:04:01,610
together in a more complex analysis and

00:03:56,570 --> 00:04:03,769
see who did what when and where so the

00:04:01,610 --> 00:04:06,590
data is actually composed of several

00:04:03,769 --> 00:04:08,630
things and you have the geometry thing

00:04:06,590 --> 00:04:11,299
the geometry part of the data and you

00:04:08,630 --> 00:04:14,840
also have the the topic that the the

00:04:11,299 --> 00:04:17,930
tags of the data so there's a lot of

00:04:14,840 --> 00:04:21,019
possibilities how you can you know

00:04:17,930 --> 00:04:24,320
create different types of history

00:04:21,019 --> 00:04:26,060
analysis and what is the in the end we

00:04:24,320 --> 00:04:30,260
are interested

00:04:26,060 --> 00:04:34,720
in is you know not just seeing how the

00:04:30,260 --> 00:04:38,180
data develops or evolves but we want to

00:04:34,720 --> 00:04:40,970
yeah get some deeper knowledge about

00:04:38,180 --> 00:04:42,919
data quality we've heard it in the talk

00:04:40,970 --> 00:04:46,760
before there's this intrinsic data

00:04:42,919 --> 00:04:49,669
quality also so which means yeah you

00:04:46,760 --> 00:04:54,580
just have a look at the data at certain

00:04:49,669 --> 00:04:59,300
point in time and then you compare maybe

00:04:54,580 --> 00:05:02,900
the data from one time with the same

00:04:59,300 --> 00:05:07,820
area of a former time and see how it's

00:05:02,900 --> 00:05:10,790
how it evolved and yeah to find

00:05:07,820 --> 00:05:14,180
something some useful measures for data

00:05:10,790 --> 00:05:15,950
quality that's not an easy task there so

00:05:14,180 --> 00:05:19,130
many different measures around and it's

00:05:15,950 --> 00:05:20,810
always difficult to interpret yeah and

00:05:19,130 --> 00:05:22,940
the other interesting part of history

00:05:20,810 --> 00:05:25,190
analysis is to know more about the

00:05:22,940 --> 00:05:30,290
community how the community actually

00:05:25,190 --> 00:05:33,080
creates the map and yeah maybe find out

00:05:30,290 --> 00:05:35,650
if there are sub communities that are

00:05:33,080 --> 00:05:38,090
dealing with special topics or maybe

00:05:35,650 --> 00:05:40,310
communities that are active in a certain

00:05:38,090 --> 00:05:42,650
region or sometimes maybe you have

00:05:40,310 --> 00:05:45,110
communities that are just existing for a

00:05:42,650 --> 00:05:48,140
certain amount of time if you think of

00:05:45,110 --> 00:05:51,200
map atones for example people gather do

00:05:48,140 --> 00:05:52,880
something on a specific topic just for a

00:05:51,200 --> 00:05:58,070
short period of time and then the

00:05:52,880 --> 00:06:00,350
community is gonna away so this a lot of

00:05:58,070 --> 00:06:03,169
dynamics in this who does what when and

00:06:00,350 --> 00:06:06,140
where why and all these are questions

00:06:03,169 --> 00:06:14,090
that you can maybe answer with history

00:06:06,140 --> 00:06:16,990
data analysis okay so yeah there are a

00:06:14,090 --> 00:06:21,800
lot of challenges we heard it before so

00:06:16,990 --> 00:06:26,979
our idea is how can we yeah make this

00:06:21,800 --> 00:06:30,880
data treasure available to everybody so

00:06:26,979 --> 00:06:34,610
how can we query this this data at every

00:06:30,880 --> 00:06:36,770
arbitrary temporal or spatial resolution

00:06:34,610 --> 00:06:38,540
so there are some approaches that do

00:06:36,770 --> 00:06:42,440
some pre-processing then

00:06:38,540 --> 00:06:45,230
maybe only yearly or quarterly views on

00:06:42,440 --> 00:06:47,060
the data but we wanted to create a

00:06:45,230 --> 00:06:49,850
framework where you can say I want to

00:06:47,060 --> 00:06:54,380
know how did the data look like at this

00:06:49,850 --> 00:06:56,630
specific point in time and what was also

00:06:54,380 --> 00:06:59,600
important for us that we do not pre

00:06:56,630 --> 00:07:04,700
filter the data or interpret the data

00:06:59,600 --> 00:07:07,850
before when we set up our data format so

00:07:04,700 --> 00:07:10,490
every thing should be in the data also

00:07:07,850 --> 00:07:12,770
the the mistakes and also the bad data

00:07:10,490 --> 00:07:15,050
everything should be findable in the

00:07:12,770 --> 00:07:17,870
data to be analyzed later

00:07:15,050 --> 00:07:20,090
so no cleanup is made no repair of

00:07:17,870 --> 00:07:21,890
geometries or something everything

00:07:20,090 --> 00:07:27,800
should be there like in the original

00:07:21,890 --> 00:07:30,380
data and then yeah the other challenge

00:07:27,800 --> 00:07:33,110
is that it's quite a lot of data and

00:07:30,380 --> 00:07:36,080
many people may be scared a little bit

00:07:33,110 --> 00:07:38,960
to do them themselves these kind of

00:07:36,080 --> 00:07:41,630
things because yeah you need quite a lot

00:07:38,960 --> 00:07:44,330
of resources if you want to analyze a

00:07:41,630 --> 00:07:46,700
larger region many people can do it for

00:07:44,330 --> 00:07:50,930
just small extracts maybe for city or so

00:07:46,700 --> 00:07:55,970
that's okay but if you want to have a

00:07:50,930 --> 00:07:57,650
bigger area analyzed and it gets quite

00:07:55,970 --> 00:08:01,850
complicated if you just do it on your

00:07:57,650 --> 00:08:06,380
own computer and therefore also the

00:08:01,850 --> 00:08:10,640
performance is a big topic so the idea

00:08:06,380 --> 00:08:14,080
was to find a data format or data scheme

00:08:10,640 --> 00:08:17,150
database scheme then it allows us to

00:08:14,080 --> 00:08:19,310
process the data not only in parallel on

00:08:17,150 --> 00:08:22,190
one machine but also split up the data

00:08:19,310 --> 00:08:25,340
and distribute it on many machines and

00:08:22,190 --> 00:08:30,680
scale up with the with the amount of

00:08:25,340 --> 00:08:32,630
data so but not everybody has a

00:08:30,680 --> 00:08:36,849
computing cluster at home so this is

00:08:32,630 --> 00:08:39,979
also something that we wanted our

00:08:36,849 --> 00:08:42,650
framework to be very flexible so you can

00:08:39,979 --> 00:08:45,770
either run it on your local PC maybe

00:08:42,650 --> 00:08:49,400
with an extract of the data of a certain

00:08:45,770 --> 00:08:51,950
area like a country or city or whatever

00:08:49,400 --> 00:08:54,830
but fits onto your local PC

00:08:51,950 --> 00:08:58,090
but also be able to distribute it on a

00:08:54,830 --> 00:09:01,850
larger computing cluster and maybe offer

00:08:58,090 --> 00:09:03,710
these kind of analysis as a service for

00:09:01,850 --> 00:09:06,410
the community so not everybody has to

00:09:03,710 --> 00:09:10,970
set up its own computing thing but just

00:09:06,410 --> 00:09:14,810
can access history data over some

00:09:10,970 --> 00:09:18,350
services and the usability is also a big

00:09:14,810 --> 00:09:21,620
question so we would like to make it

00:09:18,350 --> 00:09:23,860
easy to use this thing for different

00:09:21,620 --> 00:09:28,940
target groups or everybody's programmer

00:09:23,860 --> 00:09:34,520
but some are so there are different API

00:09:28,940 --> 00:09:39,830
levels that we that we have so everybody

00:09:34,520 --> 00:09:43,990
can find its point to to get involved

00:09:39,830 --> 00:09:47,390
and yeah obviously there are different

00:09:43,990 --> 00:09:51,260
you know flexibility of the kind of

00:09:47,390 --> 00:09:55,130
analysis that you can do but later we'll

00:09:51,260 --> 00:10:01,070
talk more about that now I would like to

00:09:55,130 --> 00:10:04,160
go on for some examples one example

00:10:01,070 --> 00:10:05,990
about data quality that has been done in

00:10:04,160 --> 00:10:11,870
our group and presented earlier this

00:10:05,990 --> 00:10:17,060
year and the experience is about data

00:10:11,870 --> 00:10:21,830
quality after the the disaster that

00:10:17,060 --> 00:10:25,340
happened in Nepal in 2015 and the

00:10:21,830 --> 00:10:30,320
colleagues from our disaster group have

00:10:25,340 --> 00:10:34,220
made these graphs with our framework and

00:10:30,320 --> 00:10:39,340
what you can see here is one year of the

00:10:34,220 --> 00:10:42,800
Nepal data set and how how the data

00:10:39,340 --> 00:10:44,720
evolved there and you see the red bar on

00:10:42,800 --> 00:10:46,880
the graph which is the time when the

00:10:44,720 --> 00:10:49,790
earthquake happened and then afterwards

00:10:46,880 --> 00:10:53,510
we already hit it a lot of a lot of

00:10:49,790 --> 00:10:56,950
people contributed to edit the data in

00:10:53,510 --> 00:11:01,250
Nepal you see the number of contributors

00:10:56,950 --> 00:11:04,760
going up very quickly after the the

00:11:01,250 --> 00:11:05,699
event wake event and then going slowly

00:11:04,760 --> 00:11:09,389
down again so

00:11:05,699 --> 00:11:12,540
this is what we expected and then you

00:11:09,389 --> 00:11:17,220
see a little a little bit shifted in

00:11:12,540 --> 00:11:22,980
time that first highways were created

00:11:17,220 --> 00:11:26,100
from remote mapping and then the

00:11:22,980 --> 00:11:28,290
impassable information was added a

00:11:26,100 --> 00:11:33,329
little bit later to the highways that

00:11:28,290 --> 00:11:36,449
were mapped so there's some some people

00:11:33,329 --> 00:11:40,139
that get this information is this

00:11:36,449 --> 00:11:42,449
highway intact or not and put that

00:11:40,139 --> 00:11:46,079
information a little later on that roads

00:11:42,449 --> 00:11:47,819
and what you can see here is that the

00:11:46,079 --> 00:11:51,600
number of contributors goes down after

00:11:47,819 --> 00:11:54,089
the event very quickly but the quality

00:11:51,600 --> 00:11:56,309
of the data is actually maintained over

00:11:54,089 --> 00:11:58,379
several months and the number of

00:11:56,309 --> 00:12:00,540
impassable highways goes down again so

00:11:58,379 --> 00:12:03,179
there there's not just one event

00:12:00,540 --> 00:12:05,730
creating a lot of data and then leave

00:12:03,179 --> 00:12:08,160
leave them alone and then it it gets

00:12:05,730 --> 00:12:10,619
wrong when the streets are repaired

00:12:08,160 --> 00:12:14,789
again but it seems that it is also

00:12:10,619 --> 00:12:16,350
updated and the quality is a good sign

00:12:14,789 --> 00:12:18,299
for the quality that the level of the

00:12:16,350 --> 00:12:20,189
impassable highways this goes back like

00:12:18,299 --> 00:12:25,079
the same level as in the beginning

00:12:20,189 --> 00:12:26,879
before the earthquake so you already see

00:12:25,079 --> 00:12:28,619
you have to if you want to make some

00:12:26,879 --> 00:12:31,799
statement about data quality you have to

00:12:28,619 --> 00:12:33,480
have a look at really a lot of different

00:12:31,799 --> 00:12:36,389
aspects of the same thing it's not just

00:12:33,480 --> 00:12:38,100
one measure and then it's good quality

00:12:36,389 --> 00:12:40,919
or bad quality you have to always look

00:12:38,100 --> 00:12:44,910
at a lot of different things what we see

00:12:40,919 --> 00:12:48,480
here is on the top the length of the

00:12:44,910 --> 00:12:53,609
highways which has this classical curve

00:12:48,480 --> 00:12:56,759
of getting complete so a lot of growth

00:12:53,609 --> 00:12:59,399
in the beginning and then yeah getting

00:12:56,759 --> 00:13:01,649
to some completeness point which is also

00:12:59,399 --> 00:13:05,160
a good sign for data quality if you have

00:13:01,649 --> 00:13:07,129
this flat curve in the end and then

00:13:05,160 --> 00:13:12,299
something very interesting in the middle

00:13:07,129 --> 00:13:15,360
we have a high peak of highways

00:13:12,299 --> 00:13:17,519
classified as highway road so highway

00:13:15,360 --> 00:13:19,300
road is actually not a really good

00:13:17,519 --> 00:13:21,880
highway class

00:13:19,300 --> 00:13:23,709
but it has been used to just say there

00:13:21,880 --> 00:13:25,990
is a road but we don't actually know

00:13:23,709 --> 00:13:28,600
which kind of road is it is it

00:13:25,990 --> 00:13:31,360
residential service secondary tannery

00:13:28,600 --> 00:13:34,450
you don't really notice from area images

00:13:31,360 --> 00:13:36,519
so but it's important to tell the first

00:13:34,450 --> 00:13:38,649
responders you can go somewhere there is

00:13:36,519 --> 00:13:42,010
a street but we don't know what kind it

00:13:38,649 --> 00:13:45,760
is but what you can see is going down

00:13:42,010 --> 00:13:48,600
and this means these highways at first

00:13:45,760 --> 00:13:52,000
classified as something highway road

00:13:48,600 --> 00:13:56,649
getting tagged and classified afterwards

00:13:52,000 --> 00:13:58,630
so this is also a pattern which tells

00:13:56,649 --> 00:14:01,480
you the quality is increasing over time

00:13:58,630 --> 00:14:04,390
and over also over several month after

00:14:01,480 --> 00:14:08,470
day the event is still maintained the

00:14:04,390 --> 00:14:10,360
data someone from from there must have

00:14:08,470 --> 00:14:17,230
had a look on the streets and classified

00:14:10,360 --> 00:14:19,779
them the start a graph here you see then

00:14:17,230 --> 00:14:22,990
the percentage of highways that have a

00:14:19,779 --> 00:14:25,600
name which was very high before the

00:14:22,990 --> 00:14:27,550
event so you can say oh cool good data

00:14:25,600 --> 00:14:29,560
quality but actually it is because there

00:14:27,550 --> 00:14:32,200
were very few highways at all

00:14:29,560 --> 00:14:35,890
so you must be careful this

00:14:32,200 --> 00:14:38,140
interpretation always and actually the

00:14:35,890 --> 00:14:41,560
number of highways which name goes down

00:14:38,140 --> 00:14:46,630
after he went but what does this mean

00:14:41,560 --> 00:14:50,560
actually this would be interesting to

00:14:46,630 --> 00:14:53,440
see and what level at other areas this

00:14:50,560 --> 00:14:55,959
highway with or without names is to to

00:14:53,440 --> 00:14:58,899
make a statement is this normal or is it

00:14:55,959 --> 00:15:01,839
bad or good or maybe the highways just

00:14:58,899 --> 00:15:04,140
don't have names there's many many

00:15:01,839 --> 00:15:04,140
reasons

00:15:08,540 --> 00:15:17,810
and then there's these two graphs so the

00:15:13,760 --> 00:15:21,649
idea of us also to make a statement

00:15:17,810 --> 00:15:24,820
about the data quality in relation to

00:15:21,649 --> 00:15:29,029
how good is it to use for routing

00:15:24,820 --> 00:15:32,449
applications and then you find that a

00:15:29,029 --> 00:15:35,600
lot of roads were created with junctions

00:15:32,449 --> 00:15:38,230
that were not noted so you cannot see

00:15:35,600 --> 00:15:41,480
this on the map so you have a nice nice

00:15:38,230 --> 00:15:44,769
map with all the Street crossings and so

00:15:41,480 --> 00:15:47,149
on but the algorithm of the routing

00:15:44,769 --> 00:15:49,550
application has problem with these kind

00:15:47,149 --> 00:15:52,250
of crossings where you cannot turn to

00:15:49,550 --> 00:15:55,579
the other Street then so obviously a lot

00:15:52,250 --> 00:16:03,139
of streets have been created but no real

00:15:55,579 --> 00:16:05,839
junction was put in and same thing is

00:16:03,139 --> 00:16:09,829
with endpoints that are really close to

00:16:05,839 --> 00:16:12,949
the the next way so the assumption is if

00:16:09,829 --> 00:16:15,620
there is an endpoint to a street very

00:16:12,949 --> 00:16:17,089
very close probably it should be

00:16:15,620 --> 00:16:19,550
connected and if there are reasons why

00:16:17,089 --> 00:16:23,000
they are not in several cases but for

00:16:19,550 --> 00:16:24,800
this analyst and analysis the assumption

00:16:23,000 --> 00:16:29,360
was okay let's count all these nearby

00:16:24,800 --> 00:16:31,640
nodes to have an idea of how good is the

00:16:29,360 --> 00:16:35,209
street network for the routing

00:16:31,640 --> 00:16:38,930
application and yeah and do as you can

00:16:35,209 --> 00:16:41,870
see the classification of highways

00:16:38,930 --> 00:16:44,690
before this is recognizable on the map

00:16:41,870 --> 00:16:48,050
it's not classified as a primary or

00:16:44,690 --> 00:16:51,290
whatever you can see this and very fast

00:16:48,050 --> 00:16:53,510
after the event the classification took

00:16:51,290 --> 00:16:56,420
place but this kind of thing you cannot

00:16:53,510 --> 00:16:58,459
see visually on the map and people just

00:16:56,420 --> 00:17:02,569
don't know maybe that there is a problem

00:16:58,459 --> 00:17:06,890
and so I don't know actually which kind

00:17:02,569 --> 00:17:10,130
of row type causes these problems I have

00:17:06,890 --> 00:17:14,720
an idea so maybe there if you if you

00:17:10,130 --> 00:17:16,819
want to yeah create a lot of street

00:17:14,720 --> 00:17:17,340
streets in a short period of time

00:17:16,819 --> 00:17:20,760
because

00:17:17,340 --> 00:17:23,040
the disaster then maybe you just yeah

00:17:20,760 --> 00:17:26,880
put long lines over the residential

00:17:23,040 --> 00:17:29,520
areas put a put all the the things in

00:17:26,880 --> 00:17:32,850
one large line thrinng and then just

00:17:29,520 --> 00:17:34,770
don't make the crossings and then yeah

00:17:32,850 --> 00:17:36,390
this but this is just an assumption so

00:17:34,770 --> 00:17:40,920
this is something that should be

00:17:36,390 --> 00:17:44,490
investigated closer okay this is an

00:17:40,920 --> 00:17:48,630
example of how to tackle data quality

00:17:44,490 --> 00:17:51,870
for certain topic so always keep in mind

00:17:48,630 --> 00:17:53,730
it's not simple you have to look at

00:17:51,870 --> 00:17:56,090
really a lot of different aspects of the

00:17:53,730 --> 00:18:02,160
data and be very careful with the

00:17:56,090 --> 00:18:04,680
interpretation then an example of for

00:18:02,160 --> 00:18:07,650
the other big topic the community

00:18:04,680 --> 00:18:11,760
dynamics dynamics as we call it what you

00:18:07,650 --> 00:18:15,360
see here is the number of unique mappers

00:18:11,760 --> 00:18:22,890
that are editing highways on a monthly

00:18:15,360 --> 00:18:27,660
base in Italy and you see the community

00:18:22,890 --> 00:18:31,890
is growing over time and it seems that

00:18:27,660 --> 00:18:36,000
is also some saturation taking place

00:18:31,890 --> 00:18:39,540
that around I don't know like thousand

00:18:36,000 --> 00:18:42,750
users per month are editing the roads in

00:18:39,540 --> 00:18:44,310
Italy it's quite interesting but the

00:18:42,750 --> 00:18:45,630
second there's a second pattern in this

00:18:44,310 --> 00:18:47,970
in this graph which is also

00:18:45,630 --> 00:18:51,210
interestingly that there are some

00:18:47,970 --> 00:18:53,870
seasonal effects so every summer time

00:18:51,210 --> 00:18:58,650
there's a little peak in the in the

00:18:53,870 --> 00:19:01,770
activity so we can just yeah don't know

00:18:58,650 --> 00:19:05,340
why this is but maybe this is people who

00:19:01,770 --> 00:19:07,620
are bored of lying on the beach and go

00:19:05,340 --> 00:19:11,670
out and do some mapping in their

00:19:07,620 --> 00:19:14,580
vacations or whatever so so maybe it's

00:19:11,670 --> 00:19:17,670
tourists or maybe it's just I don't know

00:19:14,580 --> 00:19:19,620
people go to the mountains in the in the

00:19:17,670 --> 00:19:23,460
hot summer month and do some mapping

00:19:19,620 --> 00:19:26,640
there I don't know yeah but these

00:19:23,460 --> 00:19:28,740
parents are like interesting in this

00:19:26,640 --> 00:19:30,100
what's what what makes it's so

00:19:28,740 --> 00:19:36,390
interesting to analyze

00:19:30,100 --> 00:19:39,820
history data okay and then another

00:19:36,390 --> 00:19:41,470
example is yeah this is quite a

00:19:39,820 --> 00:19:44,200
classical graph this is this typical

00:19:41,470 --> 00:19:51,640
longtail thing that there's a lot of

00:19:44,200 --> 00:19:56,380
mappers here that just have edited on

00:19:51,640 --> 00:20:02,820
one day awesome data like the green bar

00:19:56,380 --> 00:20:06,700
is formal an area like over 60% of the

00:20:02,820 --> 00:20:11,320
mappers that map Milan just edited one

00:20:06,700 --> 00:20:14,230
day and then the people that are really

00:20:11,320 --> 00:20:15,909
doing doing a lot of work very few like

00:20:14,230 --> 00:20:21,940
here like five if you have a look here

00:20:15,909 --> 00:20:25,450
five up to five hundred edit days it's

00:20:21,940 --> 00:20:28,240
just a very small group and if you have

00:20:25,450 --> 00:20:31,179
another look on these same groups of

00:20:28,240 --> 00:20:38,850
people in the next graph it's the same

00:20:31,179 --> 00:20:44,350
groups of people okay yeah you can see

00:20:38,850 --> 00:20:46,750
that these few people have edited more

00:20:44,350 --> 00:20:55,419
days or together than other all the one

00:20:46,750 --> 00:20:58,570
day mappers yeah okay so I may repeat

00:20:55,419 --> 00:21:01,360
the reminder to go to the workshop in

00:20:58,570 --> 00:21:04,270
the next session and get more to know

00:21:01,360 --> 00:21:08,760
about our how to use it so out a

00:21:04,270 --> 00:21:14,260
different API said we are providing and

00:21:08,760 --> 00:21:19,419
yeah one last announcement the framework

00:21:14,260 --> 00:21:22,120
is going open-source today and you can

00:21:19,419 --> 00:21:24,820
go on github and get the software there

00:21:22,120 --> 00:21:30,330
and do these things on your own

00:21:24,820 --> 00:21:30,330
yeah thanks really thank you very much

00:21:31,040 --> 00:21:44,610
[Applause]

00:21:33,770 --> 00:21:47,550
we have four minutes for questions there

00:21:44,610 --> 00:21:50,210
are discussions on the last state of the

00:21:47,550 --> 00:21:54,300
map concerning the data model of

00:21:50,210 --> 00:21:59,120
OpenStreetMap to change it and on this

00:21:54,300 --> 00:22:02,580
occasion to Anna anonymized the users

00:21:59,120 --> 00:22:08,490
would this take home to your to your

00:22:02,580 --> 00:22:11,460
project well if you do some anonymous

00:22:08,490 --> 00:22:14,750
ation on users you could still maybe use

00:22:11,460 --> 00:22:19,640
some artificial user IDs that are not

00:22:14,750 --> 00:22:22,500
traceable to username maybe and find out

00:22:19,640 --> 00:22:25,170
about communities without knowing

00:22:22,500 --> 00:22:27,720
exactly who is part of that community so

00:22:25,170 --> 00:22:30,480
this would be one possible solution for

00:22:27,720 --> 00:22:33,480
that don't don't track this individual

00:22:30,480 --> 00:22:39,360
users what has this user done over his

00:22:33,480 --> 00:22:43,530
his career so say but just yeah have a

00:22:39,360 --> 00:22:46,680
more generalized view on the community

00:22:43,530 --> 00:22:50,070
without yeah knowing each individual

00:22:46,680 --> 00:22:52,670
than in these groups maybe this is one

00:22:50,070 --> 00:22:52,670
solution to that

00:23:01,210 --> 00:23:08,240
I'm just curious if you have done the

00:23:04,130 --> 00:23:10,669
same analysis differentiating expert

00:23:08,240 --> 00:23:13,130
let's say users on the data quality

00:23:10,669 --> 00:23:15,049
expert users and new ones if you see

00:23:13,130 --> 00:23:17,480
that that the patterns of data quality

00:23:15,049 --> 00:23:22,309
is different on the new net edits after

00:23:17,480 --> 00:23:26,059
disasters actually we do not have made a

00:23:22,309 --> 00:23:28,580
lot of special analysis is concrete

00:23:26,059 --> 00:23:30,559
analysis so the focus is to bring this

00:23:28,580 --> 00:23:35,659
framework to everybody that everybody

00:23:30,559 --> 00:23:38,330
can do his own special measure and find

00:23:35,659 --> 00:23:40,429
out what's the best measure may be to to

00:23:38,330 --> 00:23:43,309
have these kind of take this kind of

00:23:40,429 --> 00:23:46,179
questions so now we did not do this

00:23:43,309 --> 00:23:46,179
especially

00:23:55,530 --> 00:24:03,010
any more questions

00:23:58,740 --> 00:24:05,280
maybe so what what is really really nice

00:24:03,010 --> 00:24:09,040
is that we have these these different

00:24:05,280 --> 00:24:11,350
levels here of access and the the most

00:24:09,040 --> 00:24:16,650
upper one here the awesome API is a Web

00:24:11,350 --> 00:24:20,530
API which has a different kind of

00:24:16,650 --> 00:24:22,840
aggregation analysis built in so we can

00:24:20,530 --> 00:24:27,220
can can make your own application is

00:24:22,840 --> 00:24:29,800
just you know requesting this API this

00:24:27,220 --> 00:24:32,350
is the most simple access but you can

00:24:29,800 --> 00:24:36,160
also go deeper if you have special

00:24:32,350 --> 00:24:39,850
requirements for for an analysis then

00:24:36,160 --> 00:24:42,490
you you can download the framework and

00:24:39,850 --> 00:24:46,840
the data and just have a few lines of

00:24:42,490 --> 00:24:51,880
code like this to to create the data you

00:24:46,840 --> 00:24:54,460
want so this is the Java API for

00:24:51,880 --> 00:24:57,130
analyzing this kind of data so it's

00:24:54,460 --> 00:24:59,020
really interesting and it's worth to go

00:24:57,130 --> 00:25:02,050
to the workshop and learn more about

00:24:59,020 --> 00:25:05,080
this just a few lines of code and then

00:25:02,050 --> 00:25:07,480
you have the data for such graphs thank

00:25:05,080 --> 00:25:09,760
you very much please do well to join the

00:25:07,480 --> 00:25:11,620
workshop and learn more thank you very

00:25:09,760 --> 00:25:15,900
much Michael and thank you all for your

00:25:11,620 --> 00:25:19,090
participation please the organizers has

00:25:15,900 --> 00:25:22,680
diverted me that you you are not voting

00:25:19,090 --> 00:25:22,680

YouTube URL: https://www.youtube.com/watch?v=IjAeH_qtc2s


