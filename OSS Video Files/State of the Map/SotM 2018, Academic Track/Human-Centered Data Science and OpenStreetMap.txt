Title: Human-Centered Data Science and OpenStreetMap
Publication date: 2018-09-28
Playlist: SotM 2018, Academic Track
Description: 
	Jennings Anderson (University of Colorado Boulder, United States),  State of the Map 2018
https://2018.stateofthemap.org/2018/A10-Human-Centered_Data_Science_and_OpenStreetMap__Contributor-Centric_OpenStreetMap_Analysis_Infrastructure/

OpenStreetMap is growing rapidly with one million users registering in the past year. As the number of registered users increases, so does the research interest on both the data and the community, anecdotally evidenced by a new research-oriented mailing list and presentations at conferences. With more research attention comes the need for consistent, reproducible, and scalable approaches to analyzing OpenStreetMap data—a massive non-homogeneous database of volunteered geographic information that each researcher approaches differently, many viewing it as just a map. OpenStreetMap, however, is more than a static map of the world: it is the aggregate product of billions of edits by hundreds of thousands of active contributors for over thirteen years. OpenStreetMap data analysis therefore needs to look beneath the current map and account for this rich historical context and evolution of both the data and the community to help researchers understand the complete story of how the map came to be. We are therefore designing and implementing a contributor-centric OpenStreetMap analysis infrastructure. This involves human-centered data science approaches that reimagine the research process all the way down the stack to preserve and explore the context surrounding the data genesis. This requires enhancing the data schema, extending analysis tools, and creating new interactive visualization tools to conveniently store, reproduce, and efficiently analyze the entire history of individual OpenStreetMap objects and the contributors that edit them. This infrastructure is built on vector tile analysis. It extends an existing vector tile schema to embed the history of each map object’s individual evolution into the object itself so that analysts can see which contributors modified specific attributes and when. These historically accurate datasets have a number of advantages over current OpenStreetMap datasets. First, they can include a representation of all historical geometries; no current dataset is able to represent this well, especially geometry-only changes. Second, they are optimized for efficient, parallelized processing for data analysis with existing tools. Finally, the approach is simpler than other planet-scale analysis systems: the dataset is one file (no running database), and the processing runs on one machine. This enables data sharing and reproducibility among researchers, improving both the accuracy and the scientific rigor of OpenStreetMap data analysis.
Captions: 
	00:00:01,600 --> 00:00:06,950
great thank you all my name is Jennings

00:00:04,819 --> 00:00:08,180
Anderson I'm a PhD candidate in computer

00:00:06,950 --> 00:00:11,360
science at the University of Colorado

00:00:08,180 --> 00:00:12,950
Boulder I've been doing a seven data

00:00:11,360 --> 00:00:15,080
analysis research for a number of years

00:00:12,950 --> 00:00:17,390
now it's become a topic of my

00:00:15,080 --> 00:00:18,859
dissertation so let me first motivate

00:00:17,390 --> 00:00:24,310
this talk with a bit of my own research

00:00:18,859 --> 00:00:27,220
trajectory so I was first introduced to

00:00:24,310 --> 00:00:29,630
osm through studying disaster mapping

00:00:27,220 --> 00:00:33,770
specifically the response to the 2010

00:00:29,630 --> 00:00:36,050
Haiti earthquake in January 2010 nearly

00:00:33,770 --> 00:00:37,700
500 volunteers created the most detailed

00:00:36,050 --> 00:00:40,760
digital map of Haiti in existence by

00:00:37,700 --> 00:00:42,890
tracing satellite imagery on MSM this

00:00:40,760 --> 00:00:44,870
then went on to inspire does not global

00:00:42,890 --> 00:00:45,879
form of disaster of humanitarian

00:00:44,870 --> 00:00:48,110
response

00:00:45,879 --> 00:00:50,719
three years later tech boom Yolanda

00:00:48,110 --> 00:00:52,850
struck the Philippines this animation

00:00:50,719 --> 00:00:54,530
shows the work of now about 1500

00:00:52,850 --> 00:00:56,539
volunteers tracing buildings from

00:00:54,530 --> 00:00:59,809
satellite imagery these data were used

00:00:56,539 --> 00:01:02,270
for damage assessment and recovery and

00:00:59,809 --> 00:01:04,160
then in April 2015 about 8,000

00:01:02,270 --> 00:01:06,049
volunteers responded to the Nepal

00:01:04,160 --> 00:01:08,150
earthquake by tracing thousands of

00:01:06,049 --> 00:01:10,520
kilometres of roads and hundreds of

00:01:08,150 --> 00:01:11,990
thousands of buildings you can see the

00:01:10,520 --> 00:01:14,540
mountainous geography reflected by the

00:01:11,990 --> 00:01:16,520
geometry of the roads being traced here

00:01:14,540 --> 00:01:18,409
these are roads to villages and

00:01:16,520 --> 00:01:20,990
buildings within that are literally

00:01:18,409 --> 00:01:22,909
being put on a map for the first time so

00:01:20,990 --> 00:01:27,650
this was the largest Rapid Response

00:01:22,909 --> 00:01:29,240
Disaster mapping event in history so in

00:01:27,650 --> 00:01:31,850
researching these events it quickly

00:01:29,240 --> 00:01:34,070
became obvious that osm is much more

00:01:31,850 --> 00:01:36,259
than a map the current map is the

00:01:34,070 --> 00:01:38,450
aggregate product of billions of edits

00:01:36,259 --> 00:01:40,100
across millions of objects performed by

00:01:38,450 --> 00:01:43,640
hundreds of thousands of contributors

00:01:40,100 --> 00:01:45,590
since 2004 so how do we better

00:01:43,640 --> 00:01:49,159
understand the nuances of the evolution

00:01:45,590 --> 00:01:51,229
of the map and the community for one

00:01:49,159 --> 00:01:54,110
historical context of different regions

00:01:51,229 --> 00:01:55,490
in osm is critical to understand these

00:01:54,110 --> 00:01:56,780
are graphs showing the daily editing

00:01:55,490 --> 00:01:58,850
activity for four different countries

00:01:56,780 --> 00:02:01,369
each one tells a distinctly different

00:01:58,850 --> 00:02:04,579
story about the daily editing daily

00:02:01,369 --> 00:02:06,380
activity and the community growth the

00:02:04,579 --> 00:02:08,509
number of daily users is represented by

00:02:06,380 --> 00:02:09,830
the red dots the scale for which is on

00:02:08,509 --> 00:02:10,780
the right of each graph and the blue

00:02:09,830 --> 00:02:13,240
lines

00:02:10,780 --> 00:02:16,630
represent the daily activity scale is on

00:02:13,240 --> 00:02:19,569
the left side do you know that these

00:02:16,630 --> 00:02:21,400
scales are dramatically different given

00:02:19,569 --> 00:02:22,870
relative editing activity comparing rod

00:02:21,400 --> 00:02:24,760
numbers doesn't how they work here but

00:02:22,870 --> 00:02:29,140
look at the shape to these distributions

00:02:24,760 --> 00:02:31,120
so Germany grew a very active osm

00:02:29,140 --> 00:02:34,150
community quickly and has continued to

00:02:31,120 --> 00:02:36,130
edit consistently and contrast the bulk

00:02:34,150 --> 00:02:39,069
of map United States was a data import

00:02:36,130 --> 00:02:40,660
in 2007-2008 and the community has

00:02:39,069 --> 00:02:42,580
consistently grown since then but that

00:02:40,660 --> 00:02:47,140
initial import greatly skews any

00:02:42,580 --> 00:02:49,600
relative activity since for both Haiti

00:02:47,140 --> 00:02:51,370
and Nepal we see pronounced spike in

00:02:49,600 --> 00:02:53,200
both editing activity and the number of

00:02:51,370 --> 00:02:55,750
users during their respective disaster

00:02:53,200 --> 00:02:57,130
mapping events these varying histories

00:02:55,750 --> 00:03:00,550
are essential to consider when looking

00:02:57,130 --> 00:03:02,290
at the map as it exists today the

00:03:00,550 --> 00:03:03,610
activities surrounding the mapping is

00:03:02,290 --> 00:03:05,800
going to have implications for how the

00:03:03,610 --> 00:03:07,510
data was contributed the daily work of a

00:03:05,800 --> 00:03:09,910
paved mapper for example is likely

00:03:07,510 --> 00:03:11,620
detail-oriented high-quality edits aimed

00:03:09,910 --> 00:03:14,080
at increasing coverage and completeness

00:03:11,620 --> 00:03:15,610
a local mapping party is going to

00:03:14,080 --> 00:03:18,220
provide a lot of localized highly

00:03:15,610 --> 00:03:20,019
detailed information about an area while

00:03:18,220 --> 00:03:22,000
a disaster mapping event is going to

00:03:20,019 --> 00:03:23,980
involve a lot of remote tracing little

00:03:22,000 --> 00:03:25,989
local knowledge and a lot of new mappers

00:03:23,980 --> 00:03:27,700
so referring back to the histories of

00:03:25,989 --> 00:03:29,860
Nepal in the US the map has grown

00:03:27,700 --> 00:03:31,630
through very different contexts again

00:03:29,860 --> 00:03:34,120
recall the scales are different but it's

00:03:31,630 --> 00:03:37,540
the shape of the distribution so in the

00:03:34,120 --> 00:03:39,070
US refers to the road import and then we

00:03:37,540 --> 00:03:41,170
have the subsequent spikes as we have

00:03:39,070 --> 00:03:43,260
different imports across various cities

00:03:41,170 --> 00:03:45,010
lottie's or do the building imports

00:03:43,260 --> 00:03:46,660
however there's been this kind of

00:03:45,010 --> 00:03:50,590
consistent community growth since the

00:03:46,660 --> 00:03:53,079
beginning and compared to Nepal where

00:03:50,590 --> 00:03:55,720
you do have an act of an active local

00:03:53,079 --> 00:03:57,850
community and then that activity is just

00:03:55,720 --> 00:04:00,790
totally dwarfed by the response to the

00:03:57,850 --> 00:04:03,430
earthquake the next piece to consider

00:04:00,790 --> 00:04:05,049
here is contributor context who are the

00:04:03,430 --> 00:04:06,880
contributors and what type of experience

00:04:05,049 --> 00:04:09,430
do they have how long have they been

00:04:06,880 --> 00:04:11,019
active on the platform moreover what's a

00:04:09,430 --> 00:04:15,670
more descriptive indicator of experience

00:04:11,019 --> 00:04:17,289
day's active or raw number of edits and

00:04:15,670 --> 00:04:19,930
then what about the type of expertise

00:04:17,289 --> 00:04:21,340
that users have so consider the

00:04:19,930 --> 00:04:22,750
following example

00:04:21,340 --> 00:04:24,820
contributor editing profile

00:04:22,750 --> 00:04:27,130
as this user makes more edits their

00:04:24,820 --> 00:04:28,690
total kilometres of roads Road edits

00:04:27,130 --> 00:04:31,690
increases but the number of buildings

00:04:28,690 --> 00:04:34,860
stays the same this user then would have

00:04:31,690 --> 00:04:36,730
specific kind of Road editing expertise

00:04:34,860 --> 00:04:38,500
alternatively someone who primarily

00:04:36,730 --> 00:04:42,640
focuses on buildings would look more

00:04:38,500 --> 00:04:44,530
like that a recent study of ours shows

00:04:42,640 --> 00:04:46,630
that many new mappers tend to focus on

00:04:44,530 --> 00:04:48,370
specific object types but as they become

00:04:46,630 --> 00:04:49,690
more experienced with the platform will

00:04:48,370 --> 00:04:52,210
lose this affinity for roads or

00:04:49,690 --> 00:04:53,620
buildings most part contributors don't

00:04:52,210 --> 00:04:57,700
have any object preference they at

00:04:53,620 --> 00:04:59,350
everything so beyond contextualizing

00:04:57,700 --> 00:05:01,630
editing histories of the map and the

00:04:59,350 --> 00:05:03,760
contributors we need to qualify exactly

00:05:01,630 --> 00:05:05,020
what it is that we're measuring here I

00:05:03,760 --> 00:05:06,700
make the distinction between questions

00:05:05,020 --> 00:05:08,650
and the indicators that can be measured

00:05:06,700 --> 00:05:09,970
relating to the question take this

00:05:08,650 --> 00:05:12,100
seemingly simple question for example

00:05:09,970 --> 00:05:15,640
how active is the mapping community in

00:05:12,100 --> 00:05:17,290
New York City as proposed this question

00:05:15,640 --> 00:05:19,600
is not possible to simply imagine there

00:05:17,290 --> 00:05:22,030
is no single activity attribute in the

00:05:19,600 --> 00:05:23,169
database that we can query however here

00:05:22,030 --> 00:05:25,960
are some various measurable metrics

00:05:23,169 --> 00:05:28,000
across New York City over 1,800 users

00:05:25,960 --> 00:05:30,100
have ever edited in New York City in

00:05:28,000 --> 00:05:33,180
winter 2017 there were about 20 users

00:05:30,100 --> 00:05:36,070
editing per square kilometer in the city

00:05:33,180 --> 00:05:38,140
so if you break this question down each

00:05:36,070 --> 00:05:40,450
of the following become indicators of

00:05:38,140 --> 00:05:42,040
community activity each of these are

00:05:40,450 --> 00:05:44,320
quantifiable attributes in the database

00:05:42,040 --> 00:05:46,180
only by explicitly identifying the

00:05:44,320 --> 00:05:47,830
indicators being measured and how they

00:05:46,180 --> 00:05:50,020
relate to the larger questions can we go

00:05:47,830 --> 00:05:54,040
on to perform robust reproducible

00:05:50,020 --> 00:05:56,380
analysis beyond questions and indicators

00:05:54,040 --> 00:05:58,810
what's the best format from which we can

00:05:56,380 --> 00:06:01,780
measure these attributes I'm personally

00:05:58,810 --> 00:06:04,090
a huge fan of osmq a tiles these are

00:06:01,780 --> 00:06:09,760
vector tiles with the majority of OS m

00:06:04,090 --> 00:06:15,880
object metadata included her object is

00:06:09,760 --> 00:06:18,190
that better oh no it's working ok so

00:06:15,880 --> 00:06:20,080
however these tiles don't show history

00:06:18,190 --> 00:06:21,850
very well tiles when they contain the

00:06:20,080 --> 00:06:23,919
latest version of an object so history

00:06:21,850 --> 00:06:25,440
is not currently tracked that said

00:06:23,919 --> 00:06:27,490
annual snapshots were made available

00:06:25,440 --> 00:06:29,560
separate datasets that showed what the

00:06:27,490 --> 00:06:30,820
map looked like on the first of each

00:06:29,560 --> 00:06:33,310
year for the last decade

00:06:30,820 --> 00:06:36,460
so this animation shows the density of

00:06:33,310 --> 00:06:38,979
edits from 2005 to 2016 in the US

00:06:36,460 --> 00:06:40,750
computed using these snapshots however I

00:06:38,979 --> 00:06:42,460
found the community is far too active

00:06:40,750 --> 00:06:43,000
for annual snapshots to capture the full

00:06:42,460 --> 00:06:44,380
story

00:06:43,000 --> 00:06:45,639
there's too much editing activity

00:06:44,380 --> 00:06:47,650
happening throughout the year that

00:06:45,639 --> 00:06:49,300
doesn't get counted and some urban areas

00:06:47,650 --> 00:06:51,430
as much as 80 percent of the editing

00:06:49,300 --> 00:06:53,199
activity is effectively masked by any

00:06:51,430 --> 00:06:56,169
edits that happen to occur at the end of

00:06:53,199 --> 00:06:56,919
the year when the snapshots made to

00:06:56,169 --> 00:06:58,720
address this problem

00:06:56,919 --> 00:07:00,940
I created quarterly snapshots for

00:06:58,720 --> 00:07:02,319
analysis doesn't include the full

00:07:00,940 --> 00:07:06,190
editing history but you have four times

00:07:02,319 --> 00:07:07,930
more granularity so here's an example of

00:07:06,190 --> 00:07:10,419
those quarterly snapshots in my hometown

00:07:07,930 --> 00:07:12,160
Boulder the top row shows the growth of

00:07:10,419 --> 00:07:16,090
the road network over the second half of

00:07:12,160 --> 00:07:17,770
2007 in the beginning of 2008 this shows

00:07:16,090 --> 00:07:20,410
the importance of quarterly resolution

00:07:17,770 --> 00:07:22,000
to see that actual growth the bottom row

00:07:20,410 --> 00:07:24,580
shows than the densification of the map

00:07:22,000 --> 00:07:26,710
as it filled in over the last six years

00:07:24,580 --> 00:07:28,570
so these quarterly snapshots might give

00:07:26,710 --> 00:07:30,669
us better resolution than the annual

00:07:28,570 --> 00:07:33,090
snapshots but it primarily shows growth

00:07:30,669 --> 00:07:36,009
over time requiring time series analysis

00:07:33,090 --> 00:07:40,870
and it makes it then difficult to know

00:07:36,009 --> 00:07:42,880
who did what work when so all of this is

00:07:40,870 --> 00:07:44,560
just to say that OS and data analysis is

00:07:42,880 --> 00:07:46,330
nuanced difficult and full of

00:07:44,560 --> 00:07:49,599
uncertainty there are many ways we can

00:07:46,330 --> 00:07:50,979
analyze the data and we do furthermore

00:07:49,599 --> 00:07:54,310
there's a need to expand this

00:07:50,979 --> 00:07:56,770
conversation this track is part of that

00:07:54,310 --> 00:07:58,389
the speaker goat shows the growth and

00:07:56,770 --> 00:08:01,030
research articles as indexed by Google

00:07:58,389 --> 00:08:02,639
Scholar over the past eight years these

00:08:01,030 --> 00:08:05,909
numbers show article per year

00:08:02,639 --> 00:08:08,919
non-cumulative just for that search term

00:08:05,909 --> 00:08:10,650
OpenStreetMap data analysis with more

00:08:08,919 --> 00:08:13,330
research attention has come a variety of

00:08:10,650 --> 00:08:15,099
protists to the data so each of these

00:08:13,330 --> 00:08:17,800
different approaches and combinations of

00:08:15,099 --> 00:08:19,750
tools makes the reproducibility validity

00:08:17,800 --> 00:08:24,909
and rigor difficult to maintain and

00:08:19,750 --> 00:08:28,960
evaluate okay so what is contributor

00:08:24,909 --> 00:08:30,460
centric as the title of the talk is so I

00:08:28,960 --> 00:08:33,099
consider a contributor centric approach

00:08:30,460 --> 00:08:35,169
to reimagine osm data analysis

00:08:33,099 --> 00:08:37,149
all the way down the stack by creating

00:08:35,169 --> 00:08:39,159
new datasets specifically formatted to

00:08:37,149 --> 00:08:41,229
unveil these rich histories of the

00:08:39,159 --> 00:08:43,240
objects on the map and allow researchers

00:08:41,229 --> 00:08:45,220
to define and describe a set of

00:08:43,240 --> 00:08:47,120
measurable indicators from the data to

00:08:45,220 --> 00:08:49,009
encourage more reproducible analysis

00:08:47,120 --> 00:08:50,749
we need to remember that the map is not

00:08:49,009 --> 00:08:53,149
a single static rendering of geospatial

00:08:50,749 --> 00:08:55,370
data but instead of dynamic and growing

00:08:53,149 --> 00:08:58,370
community of contributors that maintains

00:08:55,370 --> 00:09:00,079
an open map of the world ensure OS M is

00:08:58,370 --> 00:09:02,389
more than a single map so it needs to be

00:09:00,079 --> 00:09:04,339
analyzed in multiple dimensions as more

00:09:02,389 --> 00:09:06,620
than a map and I hope this approach will

00:09:04,339 --> 00:09:08,329
provide more detail to enable better

00:09:06,620 --> 00:09:09,949
data quality assessment and community

00:09:08,329 --> 00:09:13,029
analysis ultimately opening up more

00:09:09,949 --> 00:09:14,600
research opportunities around OS m

00:09:13,029 --> 00:09:16,309
extrapolating the number of articles

00:09:14,600 --> 00:09:18,559
already present on Google Scholar for

00:09:16,309 --> 00:09:19,399
2018 we're on track to continue growing

00:09:18,559 --> 00:09:22,459
at this rate

00:09:19,399 --> 00:09:24,769
so I say here the data scientists are

00:09:22,459 --> 00:09:26,689
coming it's important that the analysis

00:09:24,769 --> 00:09:29,300
tools available to these new researchers

00:09:26,689 --> 00:09:31,910
can expose the expansive history of the

00:09:29,300 --> 00:09:33,680
map to help them see to that OS M is

00:09:31,910 --> 00:09:35,420
more than just a map and the history is

00:09:33,680 --> 00:09:38,930
imperative to understand for complete

00:09:35,420 --> 00:09:40,939
and accurate analysis so now I'll

00:09:38,930 --> 00:09:42,819
transition into my kind of current

00:09:40,939 --> 00:09:45,050
approach to to try to solve this problem

00:09:42,819 --> 00:09:46,819
the first step is to make sure we have

00:09:45,050 --> 00:09:48,470
all the information we need to

00:09:46,819 --> 00:09:51,439
reconstruct the development of the map

00:09:48,470 --> 00:09:53,899
for this currently use the full planet

00:09:51,439 --> 00:09:56,629
history file so at the Edit level here

00:09:53,899 --> 00:09:59,949
were most interested in what changed on

00:09:56,629 --> 00:10:02,360
the map instead of what's on the map

00:09:59,949 --> 00:10:03,980
also unfortunately the number of

00:10:02,360 --> 00:10:06,649
historical OS and data sources is

00:10:03,980 --> 00:10:09,170
growing projects like OS and Mesa and

00:10:06,649 --> 00:10:11,209
awesome from Heidelberg provide new ways

00:10:09,170 --> 00:10:13,639
to process and analyze historical data

00:10:11,209 --> 00:10:15,949
so I see this is a great time to adopt

00:10:13,639 --> 00:10:18,170
and standardize specific OSF data

00:10:15,949 --> 00:10:20,540
analysis language and schemas to

00:10:18,170 --> 00:10:22,220
represent change in the map if we're all

00:10:20,540 --> 00:10:25,809
measuring the same indicators we can

00:10:22,220 --> 00:10:27,860
then better share interpret our results

00:10:25,809 --> 00:10:29,629
the crux of this work is then to

00:10:27,860 --> 00:10:31,490
represent the data in a familiar and

00:10:29,629 --> 00:10:33,679
usable form while also tracking this

00:10:31,490 --> 00:10:35,839
historical information for this I'm

00:10:33,679 --> 00:10:39,079
currently extending a Geo JSON osm data

00:10:35,839 --> 00:10:41,480
model so here's the geo json

00:10:39,079 --> 00:10:43,160
representation of the usm object for the

00:10:41,480 --> 00:10:46,279
pedestrian path out front of the

00:10:43,160 --> 00:10:47,839
university this is a lossless conversion

00:10:46,279 --> 00:10:50,269
from the OSM data model with all the

00:10:47,839 --> 00:10:54,529
attributes and metadata and today I'm

00:10:50,269 --> 00:10:56,449
adding this history attribute the

00:10:54,529 --> 00:10:58,579
history property contains a list of all

00:10:56,449 --> 00:10:59,839
previous edits to the object instead of

00:10:58,579 --> 00:11:00,259
just recording the tags and attributes

00:10:59,839 --> 00:11:02,359
to be

00:11:00,259 --> 00:11:04,309
version it also computes and stores the

00:11:02,359 --> 00:11:06,109
differences between the versions in this

00:11:04,309 --> 00:11:08,720
example we see that one version one was

00:11:06,109 --> 00:11:11,509
created it was marked as area yes given

00:11:08,720 --> 00:11:14,179
a name and then three other tags later

00:11:11,509 --> 00:11:15,259
on version 6 and 7 added the surface

00:11:14,179 --> 00:11:19,789
equals paving-stones

00:11:15,259 --> 00:11:21,859
and litt equals yes tags for the small

00:11:19,789 --> 00:11:24,470
connector road you can see instances of

00:11:21,859 --> 00:11:25,879
attributes being deleted and modified we

00:11:24,470 --> 00:11:27,889
can see that three different users were

00:11:25,879 --> 00:11:31,609
involved in naming this road the first

00:11:27,889 --> 00:11:33,679
user deleted the name VLA are gone and

00:11:31,609 --> 00:11:36,139
then two months later add name via

00:11:33,679 --> 00:11:37,910
Corelli the second user modified the

00:11:36,139 --> 00:11:39,769
attribute to its current name nine days

00:11:37,910 --> 00:11:41,660
later than a month after that a third

00:11:39,769 --> 00:11:44,589
user came in changed capitalisation to

00:11:41,660 --> 00:11:44,589
maintain consistency

00:11:44,889 --> 00:11:49,189
note that each version here also

00:11:47,029 --> 00:11:51,829
includes both a valid simps and valid

00:11:49,189 --> 00:11:53,629
until timestamp this enables us to

00:11:51,829 --> 00:11:57,799
easily filter by time to render features

00:11:53,629 --> 00:11:59,329
as they existed at any point so this is

00:11:57,799 --> 00:12:01,999
the first step for generating historical

00:11:59,329 --> 00:12:03,439
geometries computing historical

00:12:01,999 --> 00:12:05,660
geometries is a very difficult problem

00:12:03,439 --> 00:12:09,949
given the node way relation to Paulo G

00:12:05,660 --> 00:12:11,689
of OS m data but why is this important

00:12:09,949 --> 00:12:13,759
consider these new buildings added to

00:12:11,689 --> 00:12:16,939
the map in the week father in the 2015

00:12:13,759 --> 00:12:20,059
Nepal earthquake let's look closer and

00:12:16,939 --> 00:12:21,439
keep an eye on these buildings and

00:12:20,059 --> 00:12:23,449
here's that same area ten days later

00:12:21,439 --> 00:12:27,230
notice that these buildings are now more

00:12:23,449 --> 00:12:28,459
square so in the database however the

00:12:27,230 --> 00:12:30,709
buildings are all still recorded those

00:12:28,459 --> 00:12:32,539
version equals one this is because the

00:12:30,709 --> 00:12:34,429
reference node was moved but no change

00:12:32,539 --> 00:12:36,799
occurred to the metadata of the building

00:12:34,429 --> 00:12:38,989
object most current tools don't track

00:12:36,799 --> 00:12:40,759
these geometry only changes and make

00:12:38,989 --> 00:12:42,919
this point because this particular edit

00:12:40,759 --> 00:12:44,629
is especially important to count these

00:12:42,919 --> 00:12:47,329
changes to geometry represent a form of

00:12:44,629 --> 00:12:48,859
explicit data validation a second user

00:12:47,329 --> 00:12:51,019
here is correcting the data of a

00:12:48,859 --> 00:12:52,519
previous user and more importantly

00:12:51,019 --> 00:12:54,230
there's also this implicit map

00:12:52,519 --> 00:12:56,149
validation occurring here as well

00:12:54,230 --> 00:12:58,669
all of those buildings between the

00:12:56,149 --> 00:13:01,720
edited buildings are effectively being

00:12:58,669 --> 00:13:04,069
deemed good or passing quality by this

00:13:01,720 --> 00:13:05,809
validator so tracking these types of

00:13:04,069 --> 00:13:09,309
edits has huge implications for quality

00:13:05,809 --> 00:13:09,309
analysis and validation on the map

00:13:09,630 --> 00:13:13,560
so to track these intermediate changes

00:13:11,279 --> 00:13:16,319
we can then add an attribute the minor

00:13:13,560 --> 00:13:18,029
version minor versions are written into

00:13:16,319 --> 00:13:20,040
the objects history alongside major

00:13:18,029 --> 00:13:21,480
versions and gives credit where credit

00:13:20,040 --> 00:13:26,579
is due to the contributor who adjusts

00:13:21,480 --> 00:13:28,589
the geometry so with all the incremental

00:13:26,579 --> 00:13:31,319
geometry changes recorded and valid time

00:13:28,589 --> 00:13:32,880
stamps in the objects metadata we can

00:13:31,319 --> 00:13:34,440
really dig into the history of what the

00:13:32,880 --> 00:13:37,110
map looked like at any given second

00:13:34,440 --> 00:13:38,430
throughout history this animation shows

00:13:37,110 --> 00:13:40,980
buildings and roads in Alameda

00:13:38,430 --> 00:13:42,600
California each side of this map simply

00:13:40,980 --> 00:13:44,720
queries for objects with the appropriate

00:13:42,600 --> 00:13:48,709
valid since and valid until time stamps

00:13:44,720 --> 00:13:48,709
corresponding to the time slider above

00:13:50,149 --> 00:13:55,220
so how does this all come together and

00:13:53,370 --> 00:13:57,660
they get technical for a second here

00:13:55,220 --> 00:13:59,339
there's a tool that Lucas Martinelli and

00:13:57,660 --> 00:14:00,779
I started developing that can do all of

00:13:59,339 --> 00:14:02,910
this from the historical planet file

00:14:00,779 --> 00:14:04,470
with fairly decent performance it's

00:14:02,910 --> 00:14:06,389
called the OS Emily back utility a

00:14:04,470 --> 00:14:08,459
reference to previous osm project the

00:14:06,389 --> 00:14:12,269
same way back machine developed years

00:14:08,459 --> 00:14:14,519
ago by Sajjad and Sanjay so it works

00:14:12,269 --> 00:14:17,939
like this and I'll walk through these

00:14:14,519 --> 00:14:20,130
four steps briefly osm way back first

00:14:17,939 --> 00:14:22,050
populates a rocks DB index from an OS M

00:14:20,130 --> 00:14:23,790
history file there's separate column

00:14:22,050 --> 00:14:25,769
families for each type of Assam element

00:14:23,790 --> 00:14:28,139
and additional node locations column for

00:14:25,769 --> 00:14:30,389
geometry reconstruction rocks DB is a

00:14:28,139 --> 00:14:31,470
non disk key value storage system

00:14:30,389 --> 00:14:33,810
developed by Facebook

00:14:31,470 --> 00:14:36,089
the result is an index with super fast

00:14:33,810 --> 00:14:38,550
lookups based on object IDs and most

00:14:36,089 --> 00:14:41,670
importantly this can run on one machine

00:14:38,550 --> 00:14:46,589
and is not computationally intensive

00:14:41,670 --> 00:14:48,360
only disk space intensive so next of the

00:14:46,589 --> 00:14:50,759
stream of motifs and objects such as the

00:14:48,360 --> 00:14:52,470
product of the osmium export tool we

00:14:50,759 --> 00:14:54,779
then look up all previous versions of an

00:14:52,470 --> 00:14:56,550
object the tool computes the discs

00:14:54,779 --> 00:14:58,620
between each version and records both

00:14:56,550 --> 00:15:00,959
the differences and editing metadata for

00:14:58,620 --> 00:15:02,610
who made the changes and when this

00:15:00,959 --> 00:15:05,910
information then becomes that additional

00:15:02,610 --> 00:15:07,439
history property if we're concerned

00:15:05,910 --> 00:15:09,540
about historical geometries the current

00:15:07,439 --> 00:15:11,819
utility can account for that as well and

00:15:09,540 --> 00:15:13,920
performs a lookup of all nodes ever

00:15:11,819 --> 00:15:15,870
associated with a particular SMI object

00:15:13,920 --> 00:15:18,149
does take a bit longer to perform this

00:15:15,870 --> 00:15:20,270
lookup but at no point is the utility

00:15:18,149 --> 00:15:21,860
hold a location index in memory

00:15:20,270 --> 00:15:25,190
so again it can be run on smaller

00:15:21,860 --> 00:15:27,380
machines the output of this step is a

00:15:25,190 --> 00:15:29,149
much larger JSON object with the list of

00:15:27,380 --> 00:15:31,310
all node locations ever possibly

00:15:29,149 --> 00:15:34,070
associated and there are edit metadata

00:15:31,310 --> 00:15:35,870
and then finally we compute the

00:15:34,070 --> 00:15:37,850
historical geometries perversion and

00:15:35,870 --> 00:15:39,649
minor version augmenting that history

00:15:37,850 --> 00:15:41,779
array to include minor versions in the

00:15:39,649 --> 00:15:43,880
appropriate order this ensures that all

00:15:41,779 --> 00:15:46,330
users ever associated with this object

00:15:43,880 --> 00:15:48,529
of represented in the objects history

00:15:46,330 --> 00:15:51,230
computing minor versions can require

00:15:48,529 --> 00:15:53,240
calculating a massive cross-product this

00:15:51,230 --> 00:15:55,190
can be a bottleneck in processing but

00:15:53,240 --> 00:16:00,290
can easily be scaled to parallelization

00:15:55,190 --> 00:16:02,000
if needed so what gets produced the new

00:16:00,290 --> 00:16:04,100
JSON objects have substantially more

00:16:02,000 --> 00:16:08,060
information embedded in them these extra

00:16:04,100 --> 00:16:09,800
attributes can be written out as is with

00:16:08,060 --> 00:16:12,080
or without geometries for the most human

00:16:09,800 --> 00:16:15,740
readable output or they can be encoded

00:16:12,080 --> 00:16:17,660
as topo JSON to save space so topo JSON

00:16:15,740 --> 00:16:20,360
will remove the redundancies and all the

00:16:17,660 --> 00:16:21,950
historical geometries since topo JSON

00:16:20,360 --> 00:16:24,260
encodes each point once and then

00:16:21,950 --> 00:16:26,510
references single points across multiple

00:16:24,260 --> 00:16:28,279
features we can encode many historical

00:16:26,510 --> 00:16:31,520
versions of an object and yet only

00:16:28,279 --> 00:16:34,940
record the coordinates once so the

00:16:31,520 --> 00:16:36,829
resulting geo JSON format at OS and

00:16:34,940 --> 00:16:39,470
objects with embedded topo JSON history

00:16:36,829 --> 00:16:42,829
can be fed into Tippecanoe to create

00:16:39,470 --> 00:16:46,220
historical vector tiles the result is a

00:16:42,829 --> 00:16:47,870
single MB tiles file which is really

00:16:46,220 --> 00:16:49,430
just a sequel light database that can be

00:16:47,870 --> 00:16:52,550
easily shared and stored without and

00:16:49,430 --> 00:16:54,320
many without any major infrastructure so

00:16:52,550 --> 00:16:56,029
rendered a zoom 15 these are fairly

00:16:54,320 --> 00:16:56,870
compact and even renderable tiles as

00:16:56,029 --> 00:17:00,680
shown

00:16:56,870 --> 00:17:02,779
here's Milan and they're ideal for

00:17:00,680 --> 00:17:05,260
feeding into a processing utility like

00:17:02,779 --> 00:17:07,490
tile reduce for parallelized analysis

00:17:05,260 --> 00:17:08,990
there's definitely limitations of this

00:17:07,490 --> 00:17:11,089
approach to historical osm data

00:17:08,990 --> 00:17:13,640
management it's best for buildings roads

00:17:11,089 --> 00:17:15,199
points of interest large polygons to

00:17:13,640 --> 00:17:18,110
span multiple tiles can certainly cause

00:17:15,199 --> 00:17:20,270
problems and relations that are really

00:17:18,110 --> 00:17:22,670
Multi polygons can be handled as long as

00:17:20,270 --> 00:17:23,630
osmium can handle them but relations

00:17:22,670 --> 00:17:24,800
without inherent geometric

00:17:23,630 --> 00:17:27,410
representations such as turn

00:17:24,800 --> 00:17:32,179
restrictions are more difficult to

00:17:27,410 --> 00:17:32,960
handle so let's talk about results for

00:17:32,179 --> 00:17:35,630
like I'm

00:17:32,960 --> 00:17:37,490
and a half I ran a few different numbers

00:17:35,630 --> 00:17:40,549
for the map and Milan just to kind of

00:17:37,490 --> 00:17:42,830
try a couple things here I should note

00:17:40,549 --> 00:17:45,049
that once I was able to once I have the

00:17:42,830 --> 00:17:47,270
history extract from Milan I could do

00:17:45,049 --> 00:17:49,820
all the rest of the utility running

00:17:47,270 --> 00:17:53,990
locally no servers no cloud

00:17:49,820 --> 00:17:55,610
infrastructure to maintain okay so first

00:17:53,990 --> 00:17:57,409
I searched for edits to buildings where

00:17:55,610 --> 00:17:59,630
the attribute of building equals yes had

00:17:57,409 --> 00:18:01,490
been updated and so here's the user

00:17:59,630 --> 00:18:03,409
names the top building editors in Milan

00:18:01,490 --> 00:18:05,390
who modified this particular building

00:18:03,409 --> 00:18:11,299
tag adding valuable local knowledge to

00:18:05,390 --> 00:18:13,490
the map is anyone in this room here's a

00:18:11,299 --> 00:18:15,770
list of the top users by kilometres of

00:18:13,490 --> 00:18:21,110
roads they've at they with which they've

00:18:15,770 --> 00:18:22,970
added the name tag to a highway tag the

00:18:21,110 --> 00:18:26,000
map in the back shows these ways kind of

00:18:22,970 --> 00:18:27,799
colored by user I always like searching

00:18:26,000 --> 00:18:29,480
for the name tag when it's when that's

00:18:27,799 --> 00:18:31,010
an attribute that's been added because

00:18:29,480 --> 00:18:34,490
it hints towards local knowledge being

00:18:31,010 --> 00:18:36,380
put on the map here's a rendering of

00:18:34,490 --> 00:18:39,860
polygons in Milan that have been edited

00:18:36,380 --> 00:18:42,409
than by multiple different users the

00:18:39,860 --> 00:18:45,289
opacity reflects the number of users I'm

00:18:42,409 --> 00:18:47,510
not entirely sure how useful this is and

00:18:45,289 --> 00:18:49,820
what it's showing but there are a few

00:18:47,510 --> 00:18:51,740
outliers like some random buildings have

00:18:49,820 --> 00:18:53,510
been edited a lot so there could be

00:18:51,740 --> 00:18:57,049
something interesting there to explore

00:18:53,510 --> 00:18:59,270
further I talked about minor versions of

00:18:57,049 --> 00:19:02,870
geometries here's a graph of when they

00:18:59,270 --> 00:19:04,940
occur in Milan I'm really curious if

00:19:02,870 --> 00:19:07,610
these spikes then correspond to the

00:19:04,940 --> 00:19:09,409
release of new imagery and if we can see

00:19:07,610 --> 00:19:12,020
the effect that new imagery then has on

00:19:09,409 --> 00:19:14,539
the map as people get better imagery and

00:19:12,020 --> 00:19:20,149
they're changing the change in the

00:19:14,539 --> 00:19:21,470
geometries of these objects I know way

00:19:20,149 --> 00:19:23,720
better than to show a hairball Network

00:19:21,470 --> 00:19:24,919
they're hard to interpret so apologies

00:19:23,720 --> 00:19:27,110
to any network scientists in the room

00:19:24,919 --> 00:19:30,080
what you see here though is the directed

00:19:27,110 --> 00:19:32,059
editing network for objects in Milan the

00:19:30,080 --> 00:19:34,100
vertices are sized by out degree meaning

00:19:32,059 --> 00:19:36,950
that the larger vertices have edited the

00:19:34,100 --> 00:19:39,410
work of more different users but if we

00:19:36,950 --> 00:19:41,870
run a community detection algorithm

00:19:39,410 --> 00:19:43,970
we actually see the split in the data so

00:19:41,870 --> 00:19:45,500
I had a chance to dig into this yet but

00:19:43,970 --> 00:19:47,720
I wonder if these two communities break

00:19:45,500 --> 00:19:51,980
down across specific object types in osm

00:19:47,720 --> 00:19:54,230
so perhaps we're seeing like the editing

00:19:51,980 --> 00:19:57,500
networks of building mappers versus like

00:19:54,230 --> 00:20:00,320
highway mappers in the area not sure I'm

00:19:57,500 --> 00:20:02,750
excited to dig into that so where to

00:20:00,320 --> 00:20:04,880
from here our vector tile is a good

00:20:02,750 --> 00:20:06,230
approach with a new vector tile spec in

00:20:04,880 --> 00:20:09,590
the works perhaps there's room for a

00:20:06,230 --> 00:20:14,120
larger discussion about extraneous

00:20:09,590 --> 00:20:16,520
metadata such as history and yeah

00:20:14,120 --> 00:20:18,740
excited to talk about other historical

00:20:16,520 --> 00:20:19,970
data approaches perhaps the schema or

00:20:18,740 --> 00:20:21,860
one similar can be incorporated to

00:20:19,970 --> 00:20:24,650
better standardize how we do some of

00:20:21,860 --> 00:20:26,510
this analysis of that huge thank you to

00:20:24,650 --> 00:20:27,860
all my mentors colleagues and

00:20:26,510 --> 00:20:29,630
collaborators across academia and

00:20:27,860 --> 00:20:30,920
Industry I've mapped this for a few

00:20:29,630 --> 00:20:33,680
years now and then this work would be

00:20:30,920 --> 00:20:36,710
possible without these wonderful people

00:20:33,680 --> 00:20:46,010
organizations the tools and communities

00:20:36,710 --> 00:20:48,530
that they built and foster thank you

00:20:46,010 --> 00:20:52,250
very much for the site and presentation

00:20:48,530 --> 00:20:54,350
I think it's time for five minutes

00:20:52,250 --> 00:20:58,240
question time so you can raise your hand

00:20:54,350 --> 00:20:58,240
and you can bring the mighty

00:21:09,120 --> 00:21:16,240
Genesis is hyper fascinating um so how

00:21:12,700 --> 00:21:18,610
would we translate this back to mappers

00:21:16,240 --> 00:21:20,200
who want to perhaps meet others like how

00:21:18,610 --> 00:21:22,990
could you how could you translate this

00:21:20,200 --> 00:21:25,360
back to Nephi how people kind of connect

00:21:22,990 --> 00:21:28,060
to each other so how can regular mappers

00:21:25,360 --> 00:21:31,870
use this in a meaningful way you see

00:21:28,060 --> 00:21:34,030
that yeah I think there's I think

00:21:31,870 --> 00:21:36,550
there's a lot to be done and discount a

00:21:34,030 --> 00:21:38,590
larger discussion many talks today of

00:21:36,550 --> 00:21:43,690
like communicating this back to the to

00:21:38,590 --> 00:21:46,510
the community I'm hoping that approaches

00:21:43,690 --> 00:21:49,290
like this can kind of standardize how we

00:21:46,510 --> 00:21:53,050
do some of these analyses and make that

00:21:49,290 --> 00:21:55,510
easier language to understand in such a

00:21:53,050 --> 00:21:56,620
way that it could be shared and just

00:21:55,510 --> 00:22:00,880
sharing sharing the results of such

00:21:56,620 --> 00:22:04,960
these analyses a diary post etc and

00:22:00,880 --> 00:22:08,560
maybe finding ways to incorporate more

00:22:04,960 --> 00:22:10,780
users into it I think this is a really

00:22:08,560 --> 00:22:13,360
important topic and I and I'm not

00:22:10,780 --> 00:22:17,220
entirely sure what the best route is but

00:22:13,360 --> 00:22:17,220
excited to talk more about that

00:22:27,400 --> 00:22:31,400
this is just a comment it's looked

00:22:29,510 --> 00:22:37,340
absolutely brilliant please do something

00:22:31,400 --> 00:22:39,560
like this for Tanzania I would say

00:22:37,340 --> 00:22:41,570
please do this for every country in

00:22:39,560 --> 00:22:43,730
every region it's it's something I think

00:22:41,570 --> 00:22:45,590
everyone in the community struggles with

00:22:43,730 --> 00:22:48,620
it's it's very hard to just get the

00:22:45,590 --> 00:22:51,020
number of I didn't know what's going of

00:22:48,620 --> 00:22:53,960
just how many people are active what are

00:22:51,020 --> 00:22:56,450
they doing how is our road network still

00:22:53,960 --> 00:22:59,150
growing is it being edited stuff like

00:22:56,450 --> 00:23:01,160
that it's a huge amount of work to do

00:22:59,150 --> 00:23:03,320
this yourself for your own country and

00:23:01,160 --> 00:23:05,810
then some people do it and then they

00:23:03,320 --> 00:23:08,180
give up because it's too much work so if

00:23:05,810 --> 00:23:11,200
we have a platform we can build upon it

00:23:08,180 --> 00:23:11,200
would be way easier

00:23:31,710 --> 00:23:35,950
maybe just a technical question how big

00:23:34,360 --> 00:23:38,200
does the output of this cat for the food

00:23:35,950 --> 00:23:40,600
planet because if you still elude minor

00:23:38,200 --> 00:23:48,059
versions it obviously has to be quite

00:23:40,600 --> 00:23:48,059
large yeah still under a terabyte okay

00:23:49,799 --> 00:23:57,790
the size of the final historical vector

00:23:54,010 --> 00:24:00,610
tile is right so that's the JSON output

00:23:57,790 --> 00:24:02,350
is under terabyte the size of the

00:24:00,610 --> 00:24:03,970
historical vector tile actually

00:24:02,350 --> 00:24:10,330
corresponds pretty well to the size of

00:24:03,970 --> 00:24:11,710
the history OSN PBF so yeah this and and

00:24:10,330 --> 00:24:13,960
that can be improved with different

00:24:11,710 --> 00:24:15,250
compression right now just a lot of JSON

00:24:13,960 --> 00:24:18,870
and I'm sure there's some merit

00:24:15,250 --> 00:24:18,870

YouTube URL: https://www.youtube.com/watch?v=30R0AKvvbRA


