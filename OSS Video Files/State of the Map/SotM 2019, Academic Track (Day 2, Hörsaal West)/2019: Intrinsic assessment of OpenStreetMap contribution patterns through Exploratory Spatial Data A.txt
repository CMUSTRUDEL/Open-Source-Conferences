Title: 2019: Intrinsic assessment of OpenStreetMap contribution patterns through Exploratory Spatial Data A
Publication date: 2019-09-28
Playlist: SotM 2019, Academic Track (Day 2, Hörsaal West)
Description: 
	This study adopts a statistical approach based on Exploratory Spatial Data Analysis to identify underlying contribution patterns of OpenStreetMap (OSM). Univariate and multivariate analyses on a number of variables computed from OSM history on a regular hexagonal grid in Milan (Italy) allow to detect a number of both local clusters and local outliers, which shed light on the complexity of OSM temporal evolution driven by active local contributors and communities, data imports and mapping parties.

Compared to traditional geospatial data sources, a major advantage of OpenStreetMap (OSM) is the availability of its full history. In literature, OSM history has been exploited for a number of purposes. The most frequent is intrinsic quality assessment, which - in contrast to extrinsic assessment, where OSM quality is evaluated through comparison against a reference dataset - estimates OSM quality by only looking at its temporal evolution. OSM history has been also explored to gain insights into the project's contribution patterns, e.g. history and profiling of contributors; origin, amount, nature and frequency of edits; spatio-temporal evolution of the whole OSM database - or parts thereof, such as road networks and buildings - in specific areas, or after specific events like natural disasters; and spatial analysis of contributor and contribution patterns.
This work fits into the context of OSM intrinsic assessment by proposing a statistical approach based on Exploratory Spatial Data Analysis, and in particular spatial association, aimed at uncovering underlying history-based patterns of OSM data. More in detail, spatial association is investigated in both the univariate and multivariate contexts, i.e. in the cases - respectively - when one variable and multiple variables (together) are examined. The univariate analysis is performed using the Local Moran’s I indicator, which provides a robust classification method to detect statistically significant patterns (compared to the hypothesis of randomness) and define the spatial association type at each location in the dataset. The association type reflects the local characteristics of the variable at each location and its surroundings. Hence it allows detecting clusters, i.e. local patterns of similar (either high or low) values, as well as outliers, i.e. local patterns of dissimilar values (either low values surrounded by high values or viceversa). Instead, the multivariate Geary’s c indicator is employed to detect local association patterns resulting from the joint spatial interaction of two or more variables. A multivariate pattern classification comparable to the one of the univariate case is achieved through a novel classification method developed by the authors. This consists of a comparison of local and global centrality measures (means and medians) for the computed distribution of the multivariate Geary’s c, to produce classification maps of clusters and outliers.
The analysis is performed on Milan Province (Northern Italy), counting a population of more than 3 million inhabitants on a surface of about 1.500 km². This area is sampled using a regular hexagonal grid with side of the hexagon equal to 1000 m, producing a total of 684 cells. The analysis is focused on the history of OSM nodes only, with the following hypotheses: only nodes with at least one tag are considered; a new version of a node is counted only when there is a change in tags (not in geometry); only the nodes which currently exist in the OSM database are considered. With this in mind, for each grid cell a number of history-based variables are computed: total number of different contributors who have edited OSM nodes; average number of different contributors who have edited each OSM node; average date of creation of the OSM nodes; average date of last edit of the OSM nodes; average number of versions of the OSM nodes; average frequency of update of the OSM nodes. These values are derived from the processing of the OSM Full History Planet file (downloaded in May 2019) and its conversion into a SpatiaLite database after an intersection with the study area, followed by the computation of the variables for each grid cell.
The univariate analysis, performed using the QGIS Hotspot Analysis plugin developed by the authors, highlights different spatial associations for the different variables. While some of them (such as total and average number of contributors and average version) clearly show clusters of high values in correspondence of the most urbanized areas and clusters of low values in the non-urban peripheral areas, spatial association patterns are more heterogeneous for other variables such as the average update frequency. Multivariate analyses are then performed to detect the spatial patterns derived from the joint interaction between two and more of
Captions: 
	00:00:08,000 --> 00:00:13,610
my name is Godwin

00:00:10,080 --> 00:00:13,610
for this session and work

00:00:15,460 --> 00:00:20,110
we just get on with the with the

00:00:17,560 --> 00:00:22,840
presentations so the first presentation

00:00:20,110 --> 00:00:25,090
is about Marco is about intrinsic

00:00:22,840 --> 00:00:27,070
assessment and it's about intrinsic

00:00:25,090 --> 00:00:29,650
assessment of contribution patterns

00:00:27,070 --> 00:00:33,309
through exploratory special data

00:00:29,650 --> 00:00:34,000
analysis so please Thanks good afternoon

00:00:33,309 --> 00:00:36,129
everyone

00:00:34,000 --> 00:00:38,710
yes my name is Marco Nagini and I work

00:00:36,129 --> 00:00:40,000
at the joint research centre of the

00:00:38,710 --> 00:00:42,730
European Commission in Italy

00:00:40,000 --> 00:00:44,410
this work is kossler by colleagues from

00:00:42,730 --> 00:00:46,870
Politecnico di Milano

00:00:44,410 --> 00:00:50,290
the neox Olli and Marie Antonio Valley

00:00:46,870 --> 00:00:51,909
and also by Francesco Christina Lee from

00:00:50,290 --> 00:00:53,710
the Norwegian Institute for natural

00:00:51,909 --> 00:00:55,750
research I would like to thank all of

00:00:53,710 --> 00:01:00,190
them for their contributions to this

00:00:55,750 --> 00:01:02,710
work so we all know that compared to the

00:01:00,190 --> 00:01:04,570
traditional geospatial data source is

00:01:02,710 --> 00:01:06,280
one of the major advantages of

00:01:04,570 --> 00:01:08,560
OpenStreetMap is the availability of the

00:01:06,280 --> 00:01:11,320
full history of the data and in

00:01:08,560 --> 00:01:12,640
literature we know that the history has

00:01:11,320 --> 00:01:14,549
been traditionally used for many

00:01:12,640 --> 00:01:17,380
purposes mainly the so called intrinsic

00:01:14,549 --> 00:01:19,600
quality assessment which means in

00:01:17,380 --> 00:01:22,050
contrast to extrinsic assessments of the

00:01:19,600 --> 00:01:24,570
quality that the quality is assessed not

00:01:22,050 --> 00:01:27,400
through comparison against some external

00:01:24,570 --> 00:01:30,250
reference dataset but by only looking at

00:01:27,400 --> 00:01:32,080
osm itself in particular the history by

00:01:30,250 --> 00:01:33,790
the way Sam Easter has been also used to

00:01:32,080 --> 00:01:35,950
study the temporal evolution of the data

00:01:33,790 --> 00:01:39,430
in a region in a country or after a

00:01:35,950 --> 00:01:42,040
disaster and also to investigate the OSM

00:01:39,430 --> 00:01:43,690
contribution patterns usually in

00:01:42,040 --> 00:01:46,780
literature these studies have been

00:01:43,690 --> 00:01:50,020
focused on some of these variables so

00:01:46,780 --> 00:01:51,790
information on the contributors where do

00:01:50,020 --> 00:01:54,730
they come from what they edit how much

00:01:51,790 --> 00:01:56,860
of frequently how many contributors are

00:01:54,730 --> 00:02:00,430
there or how many contributors edit

00:01:56,860 --> 00:02:03,159
specific objects how much objects are

00:02:00,430 --> 00:02:05,110
edited so many versions or revisions and

00:02:03,159 --> 00:02:08,289
also information on the edits themselves

00:02:05,110 --> 00:02:11,379
so what is changed where how much how

00:02:08,289 --> 00:02:13,720
frequently so in this context that the

00:02:11,379 --> 00:02:15,790
purpose of this work is to contribute to

00:02:13,720 --> 00:02:18,959
the study of voice and contribution

00:02:15,790 --> 00:02:21,909
patterns not from the point of view of

00:02:18,959 --> 00:02:22,910
creating new variables or new indicators

00:02:21,909 --> 00:02:24,500
based on the history

00:02:22,910 --> 00:02:26,120
because this has been already well done

00:02:24,500 --> 00:02:27,590
in literature but from the

00:02:26,120 --> 00:02:30,260
methodological point of view and in

00:02:27,590 --> 00:02:32,300
particular by using exploratory spatial

00:02:30,260 --> 00:02:34,580
data analysis to our knowledge this is

00:02:32,300 --> 00:02:37,250
the first time that this is applied to

00:02:34,580 --> 00:02:40,310
osm first of all what is exploratory

00:02:37,250 --> 00:02:42,920
spatial data analysis or ESDA it's a

00:02:40,310 --> 00:02:45,620
statistical framework which is aimed at

00:02:42,920 --> 00:02:47,450
identifying or uncovering spatial

00:02:45,620 --> 00:02:50,180
patterns and trends that exist in

00:02:47,450 --> 00:02:51,800
geospatial datasets and in particular I

00:02:50,180 --> 00:02:53,390
will focus on the concept of spatial

00:02:51,800 --> 00:02:54,880
Association which is nothing but the

00:02:53,390 --> 00:02:56,750
degree of similarity between

00:02:54,880 --> 00:02:59,720
observations which are close to each

00:02:56,750 --> 00:03:01,970
other in space and the related family of

00:02:59,720 --> 00:03:05,720
indicators is called Lisa so local

00:03:01,970 --> 00:03:08,570
indicators of spatial Association in

00:03:05,720 --> 00:03:11,300
particular our case study is the area of

00:03:08,570 --> 00:03:13,580
Milan province in northern Italy where

00:03:11,300 --> 00:03:17,060
we had a great state of the map last

00:03:13,580 --> 00:03:19,580
year this area is almost 1.5 square

00:03:17,060 --> 00:03:22,580
kilometers and is sampled using a

00:03:19,580 --> 00:03:28,459
regular examiner grid with actually we

00:03:22,580 --> 00:03:35,180
tried three sizes of excellence aside of

00:03:28,459 --> 00:03:38,450
500 a side of 1000 and 2500 meters then

00:03:35,180 --> 00:03:41,630
we consider for this analysis only the

00:03:38,450 --> 00:03:43,430
history of osm note so we we extract for

00:03:41,630 --> 00:03:45,530
each cell only the Austin notes with

00:03:43,430 --> 00:03:47,450
from hypothesis we only consider the

00:03:45,530 --> 00:03:49,250
notes that are currently existing in the

00:03:47,450 --> 00:03:52,670
database we only consider the note

00:03:49,250 --> 00:03:55,190
having at least one tag and for us a new

00:03:52,670 --> 00:03:57,380
version of a node is counted only when

00:03:55,190 --> 00:04:01,310
there is a change in the tags and not in

00:03:57,380 --> 00:04:03,470
the geometry and we assess the spatial

00:04:01,310 --> 00:04:04,910
association between these variables

00:04:03,470 --> 00:04:06,860
related from the history which are

00:04:04,910 --> 00:04:08,420
mainly taken from the literature the

00:04:06,860 --> 00:04:11,390
total number of different contributors

00:04:08,420 --> 00:04:13,370
who edited osm notes the average number

00:04:11,390 --> 00:04:15,770
of different contributors who edited

00:04:13,370 --> 00:04:18,140
each Orson note the average date of

00:04:15,770 --> 00:04:20,239
creation and last edit of each osm note

00:04:18,140 --> 00:04:21,709
the average number of versions avoid

00:04:20,239 --> 00:04:23,660
some notes and the average frequency of

00:04:21,709 --> 00:04:26,870
updates avoid some notes for each cell

00:04:23,660 --> 00:04:28,400
of our grid how do we do that using an

00:04:26,870 --> 00:04:30,590
open-source software architecture that

00:04:28,400 --> 00:04:32,090
you can see here so the history of note

00:04:30,590 --> 00:04:34,220
is extracted from the full history

00:04:32,090 --> 00:04:36,290
planet file it is converted

00:04:34,220 --> 00:04:39,580
to an SQLite database with a specialized

00:04:36,290 --> 00:04:43,580
extension using a custom Python script

00:04:39,580 --> 00:04:45,740
which is called Oh sh to SQL based on

00:04:43,580 --> 00:04:47,750
the osmium library and finally basically

00:04:45,740 --> 00:04:50,750
we are grenade the note to the cells of

00:04:47,750 --> 00:04:55,970
the grid and we use QGIS to process the

00:04:50,750 --> 00:04:58,190
results and in particular we use the hot

00:04:55,970 --> 00:04:59,840
spot analysis plugin as you will see in

00:04:58,190 --> 00:05:04,370
a minute to compute the spatial

00:04:59,840 --> 00:05:05,930
Association how to do that so first

00:05:04,370 --> 00:05:08,030
analysis is the univariate one

00:05:05,930 --> 00:05:09,860
univariate means that we consider this

00:05:08,030 --> 00:05:11,930
partial Association and locals partial

00:05:09,860 --> 00:05:14,570
associations between the values of one

00:05:11,930 --> 00:05:16,160
single variable and in literature the

00:05:14,570 --> 00:05:19,100
most popular indicator is the local

00:05:16,160 --> 00:05:21,920
morons I here basically the indicator

00:05:19,100 --> 00:05:25,790
computes for each location I so let's

00:05:21,920 --> 00:05:28,280
say for each grid cell I the product

00:05:25,790 --> 00:05:31,660
between the value of the observation at

00:05:28,280 --> 00:05:34,430
that location multiplied by the sum of

00:05:31,660 --> 00:05:37,910
basically the observations which are

00:05:34,430 --> 00:05:39,770
neighboring okay said W is just a

00:05:37,910 --> 00:05:41,240
special weights matrix which makes sure

00:05:39,770 --> 00:05:42,380
to only consider the neighboring

00:05:41,240 --> 00:05:44,360
observations and not all the

00:05:42,380 --> 00:05:47,630
observations what is important is that

00:05:44,360 --> 00:05:49,760
this indicator basically tells us if the

00:05:47,630 --> 00:05:52,460
numerical similarity between the

00:05:49,760 --> 00:05:54,830
observation is statistically significant

00:05:52,460 --> 00:05:56,450
compared to the hypothesis of against

00:05:54,830 --> 00:05:58,970
the apologies of complete spatial

00:05:56,450 --> 00:06:02,270
randomness in few words this means that

00:05:58,970 --> 00:06:05,330
if this similarity is significant we may

00:06:02,270 --> 00:06:07,580
have clusters clusters are either high

00:06:05,330 --> 00:06:10,280
values of the variable surrounded by

00:06:07,580 --> 00:06:13,729
high values or low values surrounded by

00:06:10,280 --> 00:06:16,370
low values or outliers outliers are

00:06:13,729 --> 00:06:19,580
local patterns of dissimilar values so

00:06:16,370 --> 00:06:22,010
either a high value surrounded by low

00:06:19,580 --> 00:06:24,560
values or a low value surrounded by high

00:06:22,010 --> 00:06:26,660
values and this is implemented in the

00:06:24,560 --> 00:06:28,850
QGIS hot spot analysis plug-in that is

00:06:26,660 --> 00:06:32,000
developed by daniil one of the quarters

00:06:28,850 --> 00:06:34,220
of this work so let's see actually how

00:06:32,000 --> 00:06:36,880
these works first of all we consider

00:06:34,220 --> 00:06:40,090
here only the X agon

00:06:36,880 --> 00:06:41,290
the hexagon side a 1,000 meter later you

00:06:40,090 --> 00:06:43,810
will see something also for the other

00:06:41,290 --> 00:06:46,030
grid sizes and we consider as the

00:06:43,810 --> 00:06:49,240
neighboring observations to each eggs

00:06:46,030 --> 00:06:50,980
egg on the six adjacent excellence so

00:06:49,240 --> 00:06:53,050
this is the average number of different

00:06:50,980 --> 00:06:55,150
contributors who edited each osm know

00:06:53,050 --> 00:06:57,940
the meal and how to read this map so

00:06:55,150 --> 00:07:01,540
basically in red we have the high value

00:06:57,940 --> 00:07:04,870
cluster so hh-hi high which means again

00:07:01,540 --> 00:07:07,570
cluster so basically as a cell having a

00:07:04,870 --> 00:07:09,580
high value so a high average number of

00:07:07,570 --> 00:07:12,970
different contributor surrounded by

00:07:09,580 --> 00:07:14,620
cells having again a high value so

00:07:12,970 --> 00:07:16,510
basically not surprisingly this is the

00:07:14,620 --> 00:07:18,100
city center of Milan we know from the

00:07:16,510 --> 00:07:20,440
literature that the city centers are

00:07:18,100 --> 00:07:21,820
clearly the areas were more people map

00:07:20,440 --> 00:07:25,300
because more people live there are more

00:07:21,820 --> 00:07:26,890
people visit these errors and so on lot

00:07:25,300 --> 00:07:29,620
value clusters the blue areas you see

00:07:26,890 --> 00:07:31,810
that these are actually peripheral areas

00:07:29,620 --> 00:07:33,610
mainly agricultural areas and again in

00:07:31,810 --> 00:07:35,500
these errors there is rather less node

00:07:33,610 --> 00:07:37,840
to be met with a subject in general but

00:07:35,500 --> 00:07:40,300
also less people active this is again

00:07:37,840 --> 00:07:41,770
somehow expected let's have a look at

00:07:40,300 --> 00:07:44,170
the outliers because the all traders can

00:07:41,770 --> 00:07:45,550
really tell us the local nature can

00:07:44,170 --> 00:07:48,940
really explain the local nature of voice

00:07:45,550 --> 00:07:51,310
and so H L means a cell having a high

00:07:48,940 --> 00:07:53,980
value surrounded by cells having a low

00:07:51,310 --> 00:07:55,720
value so errors were mapping or into at

00:07:53,980 --> 00:07:58,150
least average number of contributors is

00:07:55,720 --> 00:07:59,980
particularly high LH are more

00:07:58,150 --> 00:08:01,810
interesting probably for the community

00:07:59,980 --> 00:08:03,940
because it means areas with the low

00:08:01,810 --> 00:08:05,620
values or low average number of

00:08:03,940 --> 00:08:07,570
contributors but surrounded by areas

00:08:05,620 --> 00:08:10,270
where this is high so may be areas that

00:08:07,570 --> 00:08:12,160
might need some mapping all the other

00:08:10,270 --> 00:08:14,110
cells are not significant so they are

00:08:12,160 --> 00:08:18,250
transparent not significant according to

00:08:14,110 --> 00:08:20,080
a significance level of 5% just to give

00:08:18,250 --> 00:08:23,170
you an idea of the added value of these

00:08:20,080 --> 00:08:24,970
spatial Association analysis I'm showing

00:08:23,170 --> 00:08:26,860
now a more traditional approach which is

00:08:24,970 --> 00:08:28,990
based on the same data supplied on the

00:08:26,860 --> 00:08:31,960
very same data but based on quantiles

00:08:28,990 --> 00:08:34,900
in particular five you see five classes

00:08:31,960 --> 00:08:36,700
of a quantile so it's clear that we can

00:08:34,900 --> 00:08:38,289
still see where the highest values are

00:08:36,700 --> 00:08:40,270
and where the lowest values are we can

00:08:38,289 --> 00:08:41,919
clearly see the city center of million

00:08:40,270 --> 00:08:43,680
but it's also clear that with the

00:08:41,919 --> 00:08:45,480
quantile we can not easy

00:08:43,680 --> 00:08:47,760
immediately immediately identify the

00:08:45,480 --> 00:08:49,350
clusters and the outliers and in

00:08:47,760 --> 00:08:51,870
addition as you will see later upon ties

00:08:49,350 --> 00:08:55,589
are not applicable in two or three or

00:08:51,870 --> 00:08:57,600
multiple dimensions okay let's come back

00:08:55,589 --> 00:08:59,399
to the our univariate analysis this is

00:08:57,600 --> 00:09:01,200
the total number of contributors who

00:08:59,399 --> 00:09:04,709
edited the notes so you can see that it

00:09:01,200 --> 00:09:06,570
is very very clear so when we speak

00:09:04,709 --> 00:09:08,670
about the total number the city centre

00:09:06,570 --> 00:09:11,910
of Milan is clearly visible and

00:09:08,670 --> 00:09:14,160
curiously no low value clusters and no

00:09:11,910 --> 00:09:15,570
liars are detected this is the average

00:09:14,160 --> 00:09:17,940
number of versions of or some nodes

00:09:15,570 --> 00:09:20,010
again a very similar pattern with the

00:09:17,940 --> 00:09:22,589
city center of Milan very well visible

00:09:20,010 --> 00:09:24,839
again these low value clusters somehow

00:09:22,589 --> 00:09:26,940
scattered in the periphery and also the

00:09:24,839 --> 00:09:28,589
outliers again the outliers can tell us

00:09:26,940 --> 00:09:32,040
something specific about what is

00:09:28,589 --> 00:09:34,140
happening their average date of creation

00:09:32,040 --> 00:09:36,570
so this is computed as the number of

00:09:34,140 --> 00:09:38,190
days between the date of creation of the

00:09:36,570 --> 00:09:40,820
nodes and the date we downloaded the

00:09:38,190 --> 00:09:43,020
planet which is last week basically so

00:09:40,820 --> 00:09:44,820
first of all we notice that there are no

00:09:43,020 --> 00:09:47,339
high value clusters in the city centre

00:09:44,820 --> 00:09:49,290
if you are wondering why this is because

00:09:47,339 --> 00:09:52,170
in the city centre there are also many

00:09:49,290 --> 00:09:55,290
many nodes created recently so when we

00:09:52,170 --> 00:09:57,959
compute the average date of creation

00:09:55,290 --> 00:10:01,080
this is of course not significant what

00:09:57,959 --> 00:10:03,260
is significant here is this large blue

00:10:01,080 --> 00:10:07,020
area of low value clusters which means

00:10:03,260 --> 00:10:09,120
node created very recently I'll tell you

00:10:07,020 --> 00:10:11,730
something more about that later this is

00:10:09,120 --> 00:10:14,010
the average date of last edit so similar

00:10:11,730 --> 00:10:16,529
story so this is the number of days

00:10:14,010 --> 00:10:18,690
between the date of last edit and the

00:10:16,529 --> 00:10:21,839
date we downloaded the planet so you see

00:10:18,690 --> 00:10:26,430
that again these area shows very very

00:10:21,839 --> 00:10:28,350
recent date of last edit and the reason

00:10:26,430 --> 00:10:30,150
of this is actually then of course this

00:10:28,350 --> 00:10:31,410
means try to look at the date and see

00:10:30,150 --> 00:10:34,740
what's happening there the reason is

00:10:31,410 --> 00:10:37,500
that there is a user one single user

00:10:34,740 --> 00:10:40,350
that over the last year or so started to

00:10:37,500 --> 00:10:43,170
map all the building

00:10:40,350 --> 00:10:46,980
numbers so the address is basically one

00:10:43,170 --> 00:10:49,230
single user this is frequency average

00:10:46,980 --> 00:10:53,040
frequency of update which again points

00:10:49,230 --> 00:10:55,770
us to that area with high value clusters

00:10:53,040 --> 00:10:59,340
so again high frequency of update mostly

00:10:55,770 --> 00:11:00,570
again due to this user but we want to

00:10:59,340 --> 00:11:03,150
also to extend this in the bivariate

00:11:00,570 --> 00:11:04,650
case which means we measure the locus

00:11:03,150 --> 00:11:07,280
partial similarity between the

00:11:04,650 --> 00:11:09,420
standardized values for two variables

00:11:07,280 --> 00:11:11,160
standardized is very important of course

00:11:09,420 --> 00:11:12,900
when we compare the values of two

00:11:11,160 --> 00:11:14,910
variables we need to standardized an

00:11:12,900 --> 00:11:17,190
standardized means mean equal to zero

00:11:14,910 --> 00:11:18,900
and variance equal to one and again we

00:11:17,190 --> 00:11:21,720
use the local morons I the formula is

00:11:18,900 --> 00:11:24,900
the very same formula as before but now

00:11:21,720 --> 00:11:26,760
the ZJ so the nave boring observations

00:11:24,900 --> 00:11:28,320
are not observations of the same

00:11:26,760 --> 00:11:31,140
variable but of course of the second

00:11:28,320 --> 00:11:33,000
variables so the the interpretation is

00:11:31,140 --> 00:11:34,370
that now the clusters are for example a

00:11:33,000 --> 00:11:36,750
high value of the first variable

00:11:34,370 --> 00:11:39,000
surrounded by high values of the second

00:11:36,750 --> 00:11:41,010
or low value of the first surrounded by

00:11:39,000 --> 00:11:42,840
low values of the second outliers are

00:11:41,010 --> 00:11:44,880
the same so low value of the first

00:11:42,840 --> 00:11:46,920
surrounded by high values of the second

00:11:44,880 --> 00:11:48,720
or vice versa and this is also

00:11:46,920 --> 00:11:51,540
implemented in the QGIS hot spot

00:11:48,720 --> 00:11:53,550
analysis plug in some examples again

00:11:51,540 --> 00:11:56,040
here we put together the average number

00:11:53,550 --> 00:11:57,540
of different contributors and the

00:11:56,040 --> 00:12:00,150
average number of versions if you

00:11:57,540 --> 00:12:02,730
remember in the univariate case these

00:12:00,150 --> 00:12:04,170
had a very similar pattern and it's not

00:12:02,730 --> 00:12:05,880
surprising that also when we put them

00:12:04,170 --> 00:12:07,620
together this is the pattern so

00:12:05,880 --> 00:12:11,190
basically the same city center of Milan

00:12:07,620 --> 00:12:13,530
clearly highlighted this is the average

00:12:11,190 --> 00:12:16,680
date of creation of osm nodes and the

00:12:13,530 --> 00:12:19,890
average date of last edit so inverted

00:12:16,680 --> 00:12:22,470
inverted means that in this case the

00:12:19,890 --> 00:12:24,870
high value clusters correspond to areas

00:12:22,470 --> 00:12:26,430
where the nodes have been created less

00:12:24,870 --> 00:12:27,990
too recently I mean very early in time

00:12:26,430 --> 00:12:29,910
and updated very recently

00:12:27,990 --> 00:12:32,580
so this basically never happens the only

00:12:29,910 --> 00:12:34,110
some somewhere here what happens what is

00:12:32,580 --> 00:12:36,150
confirmed basically because we already

00:12:34,110 --> 00:12:38,640
knew that from the univariate case is

00:12:36,150 --> 00:12:41,220
that this area had not created very

00:12:38,640 --> 00:12:43,050
recently and updated very recently most

00:12:41,220 --> 00:12:46,380
of them actually have one single single

00:12:43,050 --> 00:12:48,030
version so the date of creation is also

00:12:46,380 --> 00:12:50,730
the date of last edit because these are

00:12:48,030 --> 00:12:51,810
addresses then these two variables are

00:12:50,730 --> 00:12:54,030
quite different

00:12:51,810 --> 00:12:55,770
pattern in the univariate case because

00:12:54,030 --> 00:12:57,840
the average number of versions showed us

00:12:55,770 --> 00:12:59,400
the city center of Milan with the high

00:12:57,840 --> 00:13:01,980
value clusters the average frequency

00:12:59,400 --> 00:13:03,510
showed us that area you see that when we

00:13:01,980 --> 00:13:05,520
combine these basically we have

00:13:03,510 --> 00:13:08,250
something in between so this area is

00:13:05,520 --> 00:13:09,450
also well visible what I want to draw

00:13:08,250 --> 00:13:12,120
your attention on is also the

00:13:09,450 --> 00:13:15,450
significance level that is lower when we

00:13:12,120 --> 00:13:17,730
have two or more variables the reason is

00:13:15,450 --> 00:13:20,340
that it is suggested to use a rule of

00:13:17,730 --> 00:13:22,380
thumb saying use a significance level

00:13:20,340 --> 00:13:24,150
the standard one in our case five

00:13:22,380 --> 00:13:26,130
percent divided by the number of

00:13:24,150 --> 00:13:28,050
variables because when we have more

00:13:26,130 --> 00:13:30,030
variables it is higher the chance that

00:13:28,050 --> 00:13:32,370
the combination of the two

00:13:30,030 --> 00:13:34,290
generate some false positive so clusters

00:13:32,370 --> 00:13:35,940
which in the reality are not cluster so

00:13:34,290 --> 00:13:39,420
to limit these we basically make the

00:13:35,940 --> 00:13:41,280
test a little bit more strict finally

00:13:39,420 --> 00:13:45,690
multivariate case means when we put

00:13:41,280 --> 00:13:47,640
together multiple variables to again

00:13:45,690 --> 00:13:49,140
assess their local special associations

00:13:47,640 --> 00:13:51,030
in this case the indicator is a

00:13:49,140 --> 00:13:55,080
different one is the local gara C which

00:13:51,030 --> 00:13:57,650
is actually based on the Sun you see

00:13:55,080 --> 00:14:00,750
here the sum of the squared distances

00:13:57,650 --> 00:14:02,130
between the observations the

00:14:00,750 --> 00:14:03,900
standardized observations of the

00:14:02,130 --> 00:14:06,120
different variables so now the logic is

00:14:03,900 --> 00:14:08,520
a bit different because we have clusters

00:14:06,120 --> 00:14:10,200
when suppose that clusters means similar

00:14:08,520 --> 00:14:11,850
value so when we have different

00:14:10,200 --> 00:14:14,310
variables with similar standardized

00:14:11,850 --> 00:14:16,710
values the distances between the values

00:14:14,310 --> 00:14:18,210
in the let's say if we have five

00:14:16,710 --> 00:14:19,950
variables for instance it's the five

00:14:18,210 --> 00:14:22,350
dimensional space of the variables so

00:14:19,950 --> 00:14:24,750
the distance is small the sum of the

00:14:22,350 --> 00:14:27,630
distance is small so we have low values

00:14:24,750 --> 00:14:29,220
of C meaning clusters the opposite e for

00:14:27,630 --> 00:14:32,030
the outline so we basically look at the

00:14:29,220 --> 00:14:36,750
distance between the observations in the

00:14:32,030 --> 00:14:38,190
space of the observations if you think

00:14:36,750 --> 00:14:40,530
at the distance you might well

00:14:38,190 --> 00:14:42,330
understand that we can determine if

00:14:40,530 --> 00:14:44,640
these are clusters or outliers but not

00:14:42,330 --> 00:14:46,280
the type of clusters or outliers because

00:14:44,640 --> 00:14:49,050
it's only based on the distance okay

00:14:46,280 --> 00:14:52,980
this is why we actually created two new

00:14:49,050 --> 00:14:54,540
indicators that have to be computed

00:14:52,980 --> 00:14:57,000
after the application of the local

00:14:54,540 --> 00:14:59,490
galaxy to actually classify the type of

00:14:57,000 --> 00:15:01,050
cluster or the type of outlier I will

00:14:59,490 --> 00:15:03,210
not spend too much time because it's

00:15:01,050 --> 00:15:06,300
quite impossible to really explain these

00:15:03,210 --> 00:15:08,850
in one minute I just say that it's

00:15:06,300 --> 00:15:11,550
basically based on a comparison between

00:15:08,850 --> 00:15:13,470
so this is the first one and then see

00:15:11,550 --> 00:15:15,510
four clusters comparison between the

00:15:13,470 --> 00:15:17,550
mean of the medians of the standardized

00:15:15,510 --> 00:15:20,070
observation values at the location of

00:15:17,550 --> 00:15:21,930
the cluster and its neighbors compared

00:15:20,070 --> 00:15:24,840
with the mean of the medians of the

00:15:21,930 --> 00:15:28,200
observations in the whole space this one

00:15:24,840 --> 00:15:29,940
for outliers basically looks and

00:15:28,200 --> 00:15:31,860
compares the mean of the standardized

00:15:29,940 --> 00:15:34,290
observation values at the location of

00:15:31,860 --> 00:15:36,000
the outlier with the mean of the medians

00:15:34,290 --> 00:15:38,820
of the standardized observation values

00:15:36,000 --> 00:15:40,470
at all the neighbors this is actually to

00:15:38,820 --> 00:15:42,690
understand the type of cluster and the

00:15:40,470 --> 00:15:44,910
type of outliers the code of these for

00:15:42,690 --> 00:15:47,100
this is on github let's see basically

00:15:44,910 --> 00:15:49,410
what happens when we do it in practice

00:15:47,100 --> 00:15:51,360
this is three variables average number

00:15:49,410 --> 00:15:54,320
of different contributors average number

00:15:51,360 --> 00:15:56,520
of versions average frequency of update

00:15:54,320 --> 00:15:58,680
the first two add a very similar

00:15:56,520 --> 00:16:01,740
distribution in the univariate case the

00:15:58,680 --> 00:16:04,140
third did not any because the frequency

00:16:01,740 --> 00:16:05,880
if you remember showed us this area so

00:16:04,140 --> 00:16:08,270
overall we can see that the high value

00:16:05,880 --> 00:16:11,100
cluster so when these three variables

00:16:08,270 --> 00:16:12,990
taken together shows the highest value

00:16:11,100 --> 00:16:16,980
surrounded by highest values are still

00:16:12,990 --> 00:16:18,780
present in the city center of Milan low

00:16:16,980 --> 00:16:20,760
value clusters are again areas

00:16:18,780 --> 00:16:22,560
characterized by low values of all the

00:16:20,760 --> 00:16:24,630
three variables together but surrounded

00:16:22,560 --> 00:16:27,930
also by low values of these three

00:16:24,630 --> 00:16:30,990
variables outliers outliers here shows a

00:16:27,930 --> 00:16:33,300
mixed behavior basically so high values

00:16:30,990 --> 00:16:36,540
of the three variables surrounded by low

00:16:33,300 --> 00:16:39,180
values or vice-versa this is five

00:16:36,540 --> 00:16:40,620
variables together of course when we

00:16:39,180 --> 00:16:42,090
increase the variable we increase the

00:16:40,620 --> 00:16:43,950
variability of results and this is why

00:16:42,090 --> 00:16:46,500
again according to the same rule of

00:16:43,950 --> 00:16:48,780
thumb we consider now significance level

00:16:46,500 --> 00:16:50,550
of one percent here you can see five

00:16:48,780 --> 00:16:52,560
variables together so we also included

00:16:50,550 --> 00:16:54,660
the average date of creation and last

00:16:52,560 --> 00:16:57,120
edit and you can see that basically the

00:16:54,660 --> 00:16:59,580
patterns are are these so in the line we

00:16:57,120 --> 00:17:01,320
can say it's there's a clear activity in

00:16:59,580 --> 00:17:03,780
the city center there are these areas in

00:17:01,320 --> 00:17:05,850
the periphery agricultural error so

00:17:03,780 --> 00:17:07,230
these should be analyzed to see

00:17:05,850 --> 00:17:08,940
basically what's there the reason is

00:17:07,230 --> 00:17:11,160
that as I said there's there are a few

00:17:08,940 --> 00:17:13,270
things to be mad basically and then

00:17:11,160 --> 00:17:15,370
there is also this area where the active

00:17:13,270 --> 00:17:19,750
this single contributor is really

00:17:15,370 --> 00:17:22,810
evident here finally of course I showed

00:17:19,750 --> 00:17:24,610
you just results using a grid size of

00:17:22,810 --> 00:17:26,500
1000 meters but you can ask okay what

00:17:24,610 --> 00:17:29,830
what happens if we change the grid size

00:17:26,500 --> 00:17:31,480
that's a good question of course and you

00:17:29,830 --> 00:17:37,090
can see here on the left two

00:17:31,480 --> 00:17:39,550
distributions done with the excellent

00:17:37,090 --> 00:17:41,590
sides of 500 meters in the middle to the

00:17:39,550 --> 00:17:43,150
corresponding two with the side of the

00:17:41,590 --> 00:17:44,770
eggs on equal to 1000 meters so

00:17:43,150 --> 00:17:46,720
basically those I showed you before and

00:17:44,770 --> 00:17:49,990
on the right the same distribution with

00:17:46,720 --> 00:17:52,110
a higher grid sides 2500 so you can see

00:17:49,990 --> 00:17:54,520
that clearly the results of the special

00:17:52,110 --> 00:17:57,340
associations detected are different is

00:17:54,520 --> 00:17:59,920
this obvious so in general we can say

00:17:57,340 --> 00:18:02,590
that if we are studying a phenomenon

00:17:59,920 --> 00:18:04,060
that is associated to a specific special

00:18:02,590 --> 00:18:06,700
unit suppose that we are studying the

00:18:04,060 --> 00:18:08,500
votes in an election the votes in an

00:18:06,700 --> 00:18:11,470
election are associated to a district to

00:18:08,500 --> 00:18:12,880
province so usually you should use the

00:18:11,470 --> 00:18:15,220
the district or the province as a

00:18:12,880 --> 00:18:16,600
special unit you know SM of course if

00:18:15,220 --> 00:18:18,670
these things change because this is a

00:18:16,600 --> 00:18:20,560
kind of a continuous phenomenon so it's

00:18:18,670 --> 00:18:22,120
up to you really to choose the dimension

00:18:20,560 --> 00:18:23,560
of the grid according to that type or

00:18:22,120 --> 00:18:26,380
the scale of the analysis you want to

00:18:23,560 --> 00:18:29,200
make if you want to detect some very

00:18:26,380 --> 00:18:31,450
local behavior you would probably go for

00:18:29,200 --> 00:18:33,010
a very dense grid like this one so

00:18:31,450 --> 00:18:35,380
probably here the activity of each

00:18:33,010 --> 00:18:37,870
single contributor would be even more

00:18:35,380 --> 00:18:40,240
evident with a very very dense grid side

00:18:37,870 --> 00:18:42,970
so on the other hand if you want to make

00:18:40,240 --> 00:18:44,740
an analysis at the sub urban Europe on a

00:18:42,970 --> 00:18:48,490
regional scale you should probably go

00:18:44,740 --> 00:18:50,550
for a less and less dense grid so to

00:18:48,490 --> 00:18:52,600
conclude this is the first time

00:18:50,550 --> 00:18:55,270
application at least to our knowledge of

00:18:52,600 --> 00:18:57,790
the exploratory spatial data analysis to

00:18:55,270 --> 00:19:00,880
the study of the OSM contribution

00:18:57,790 --> 00:19:03,250
patterns it's still experimental but I

00:19:00,880 --> 00:19:06,940
would say it's quite promising to really

00:19:03,250 --> 00:19:10,030
understand the patterns underlying OSM

00:19:06,940 --> 00:19:13,920
local contributors even to the point

00:19:10,030 --> 00:19:16,450
wanna lies and to to really identify

00:19:13,920 --> 00:19:18,850
contribution from single users or from

00:19:16,450 --> 00:19:20,500
imports or from mapping parties of

00:19:18,850 --> 00:19:23,820
course

00:19:20,500 --> 00:19:26,320
we can also make some that say

00:19:23,820 --> 00:19:28,450
assumptions on the quality of our sin

00:19:26,320 --> 00:19:30,970
because of course if there are few

00:19:28,450 --> 00:19:32,620
contributors if sorry if there are many

00:19:30,970 --> 00:19:35,050
contributors if the frequency of updates

00:19:32,620 --> 00:19:37,210
is high if the data are very very

00:19:35,050 --> 00:19:39,100
up-to-date of course we can make some

00:19:37,210 --> 00:19:40,990
considerations of the possible quality

00:19:39,100 --> 00:19:42,730
of the data as I said this really

00:19:40,990 --> 00:19:44,820
outperforms the analysis based on

00:19:42,730 --> 00:19:47,530
quantiles because it does not directly

00:19:44,820 --> 00:19:49,900
identify clusters and outliers and it

00:19:47,530 --> 00:19:52,270
cannot be applied to the B value at a

00:19:49,900 --> 00:19:53,650
multivariate case as I said the choice

00:19:52,270 --> 00:19:55,810
of the grid is very important and should

00:19:53,650 --> 00:19:57,970
be done according to the type of

00:19:55,810 --> 00:19:59,830
analysis you want to make in this regard

00:19:57,970 --> 00:20:01,990
the next step would be to try to make

00:19:59,830 --> 00:20:04,060
some kind of sensitivity analysis that

00:20:01,990 --> 00:20:07,050
is to see really quantitatively how

00:20:04,060 --> 00:20:10,180
results change when you change the grid

00:20:07,050 --> 00:20:12,790
okay this is the abstract publishing the

00:20:10,180 --> 00:20:22,180
conference proceedings that's it

00:20:12,790 --> 00:20:25,060
I'm open to questions Thank You Marco

00:20:22,180 --> 00:20:28,030
for this excellent presentation of any

00:20:25,060 --> 00:20:34,330
questions we have one mic there the ones

00:20:28,030 --> 00:20:36,610
here like smoke and I was super

00:20:34,330 --> 00:20:39,940
interesting really interesting to see

00:20:36,610 --> 00:20:42,550
exploratory data analysis applied to osm

00:20:39,940 --> 00:20:45,100
editing history I just wondered given

00:20:42,550 --> 00:20:48,010
the inherent bias of population

00:20:45,100 --> 00:20:50,620
population density number of mappers had

00:20:48,010 --> 00:20:53,860
you considered normalizing the analysis

00:20:50,620 --> 00:20:55,420
based on population yeah this is this is

00:20:53,860 --> 00:20:57,510
interesting thanks for the question of

00:20:55,420 --> 00:20:59,290
course we didn't consider it in this

00:20:57,510 --> 00:21:01,570
experiment because it was just an

00:20:59,290 --> 00:21:03,340
initial experiment but again if you know

00:21:01,570 --> 00:21:05,830
what is the population for each single

00:21:03,340 --> 00:21:07,720
hexagon so for each cell of the grid of

00:21:05,830 --> 00:21:10,600
reefs partial unit of course you could

00:21:07,720 --> 00:21:12,490
just give each these as a and as an

00:21:10,600 --> 00:21:14,380
additional variable as input to the

00:21:12,490 --> 00:21:16,690
today to the algorithm and you will get

00:21:14,380 --> 00:21:18,760
exactly how for example the number of

00:21:16,690 --> 00:21:20,800
the average number of users varies with

00:21:18,760 --> 00:21:22,330
the population so would be for sure in

00:21:20,800 --> 00:21:24,490
the interesting to see it would remove

00:21:22,330 --> 00:21:25,930
that bias right even better if you could

00:21:24,490 --> 00:21:26,530
have the number of mappers in the

00:21:25,930 --> 00:21:28,060
hexagon

00:21:26,530 --> 00:21:29,740
yeah but this is something in general

00:21:28,060 --> 00:21:32,450
different to have because the number of

00:21:29,740 --> 00:21:34,490
mappers you you never know where and MA

00:21:32,450 --> 00:21:38,240
to leave so you can look at the day at

00:21:34,490 --> 00:21:40,850
the the place of the first edit or you

00:21:38,240 --> 00:21:42,890
know you mean it's always something

00:21:40,850 --> 00:21:46,580
difficult to understand really were I'm

00:21:42,890 --> 00:21:48,950
a Paris based but you can probably you

00:21:46,580 --> 00:21:50,720
can probably yes find it at least maybe

00:21:48,950 --> 00:21:52,760
on a on a euro burns case so you can

00:21:50,720 --> 00:21:54,680
know if I'm a Paris from Milan or

00:21:52,760 --> 00:22:01,070
outside me land for sure that's true

00:21:54,680 --> 00:22:04,040
thank you thanks we'd have a question

00:22:01,070 --> 00:22:06,110
there and then one more here you

00:22:04,040 --> 00:22:09,230
mentioned earlier on that you discarded

00:22:06,110 --> 00:22:11,720
any position changes for notes as an

00:22:09,230 --> 00:22:14,240
input can you elaborate why that is why

00:22:11,720 --> 00:22:16,610
you're only looking at tech changes yeah

00:22:14,240 --> 00:22:19,550
this was just a choice that we made at

00:22:16,610 --> 00:22:21,560
the very beginning there's no particular

00:22:19,550 --> 00:22:23,390
reason why we wanted just fast for this

00:22:21,560 --> 00:22:25,520
first experiment to look at the changes

00:22:23,390 --> 00:22:28,070
in the tags so not to look at all the

00:22:25,520 --> 00:22:29,510
small changes where a node is just moved

00:22:28,070 --> 00:22:31,250
basically from a place to another

00:22:29,510 --> 00:22:33,410
there's no particular reason for that

00:22:31,250 --> 00:22:35,750
there's a reason why we only focus on

00:22:33,410 --> 00:22:37,840
notes and not own ways and relations and

00:22:35,750 --> 00:22:41,540
this is because basically you know and

00:22:37,840 --> 00:22:44,600
ways means could mean oh so highways so

00:22:41,540 --> 00:22:46,760
highways basically staying more or even

00:22:44,600 --> 00:22:48,830
multiple cells of the grid so at that

00:22:46,760 --> 00:22:51,050
point we should have we would have they

00:22:48,830 --> 00:22:53,720
would have to take a decision on how to

00:22:51,050 --> 00:22:55,730
manage this or if considering away as

00:22:53,720 --> 00:22:57,110
belonging to many grades relations is

00:22:55,730 --> 00:23:00,440
even worse because relations are

00:22:57,110 --> 00:23:03,110
basically made of notes and ways so the

00:23:00,440 --> 00:23:04,940
updating of the relation means basically

00:23:03,110 --> 00:23:07,790
almost updating on same way so this is

00:23:04,940 --> 00:23:10,490
something we've decided we just focused

00:23:07,790 --> 00:23:13,070
on notes just to see sorry we focus on

00:23:10,490 --> 00:23:15,770
tags just to see if we could get some

00:23:13,070 --> 00:23:18,290
patterns of course next step would be to

00:23:15,770 --> 00:23:20,150
focus also on geometry and to plug

00:23:18,290 --> 00:23:21,920
basically these into the software

00:23:20,150 --> 00:23:26,530
architecture into the scripts and but

00:23:21,920 --> 00:23:26,530
for sure this can be done thanks

00:23:30,370 --> 00:23:36,110
was a technical one how did you manage

00:23:33,800 --> 00:23:38,150
to avoid a problem around the edge of

00:23:36,110 --> 00:23:40,790
your area so I saw that you had

00:23:38,150 --> 00:23:42,830
hexagonal tiles which were cut in half

00:23:40,790 --> 00:23:45,080
by the edge of your the area you were

00:23:42,830 --> 00:23:47,810
considering but what do you mean in the

00:23:45,080 --> 00:23:49,640
algorithm or yeah so I'm so if you have

00:23:47,810 --> 00:23:53,120
one of your hexagonal tiles which only

00:23:49,640 --> 00:23:56,120
have as part of its showing yeah did you

00:23:53,120 --> 00:23:58,850
look at the whole tile then here we just

00:23:56,120 --> 00:24:00,460
know this is at the beginning we created

00:23:58,850 --> 00:24:03,380
the grid and we intersect it with the

00:24:00,460 --> 00:24:05,360
province of milan and that basically if

00:24:03,380 --> 00:24:08,000
we have half of the eggs egg on this is

00:24:05,360 --> 00:24:09,650
our special unit so this is the place

00:24:08,000 --> 00:24:11,420
where we count the notes so we didn't

00:24:09,650 --> 00:24:13,310
look at the whole egg second because it

00:24:11,420 --> 00:24:16,100
was part was outside the province so

00:24:13,310 --> 00:24:16,940
when we aggregate the notes to the to

00:24:16,100 --> 00:24:19,430
their cells

00:24:16,940 --> 00:24:21,170
we just aggregate notes to that polygon

00:24:19,430 --> 00:24:22,880
sole because then I would have expected

00:24:21,170 --> 00:24:24,890
that all of those ones around the edge

00:24:22,880 --> 00:24:30,350
would be low value because they are

00:24:24,890 --> 00:24:31,910
smaller it could be now I mean it might

00:24:30,350 --> 00:24:34,340
depends on what's inside the notes

00:24:31,910 --> 00:24:36,710
basically not really depends on what's

00:24:34,340 --> 00:24:40,370
inside its remember that it's basically

00:24:36,710 --> 00:24:43,520
the value but always computed compared

00:24:40,370 --> 00:24:45,350
to what is surrounding what is so what

00:24:43,520 --> 00:24:48,260
in the neighboring observations are so

00:24:45,350 --> 00:24:51,880
it's always something relative to what

00:24:48,260 --> 00:24:55,430
is basically surrounding that

00:24:51,880 --> 00:24:57,290
observation so usually of course when

00:24:55,430 --> 00:24:59,450
you have cases like these you need

00:24:57,290 --> 00:25:00,800
always to take care of what happens at

00:24:59,450 --> 00:25:03,290
the borders because of course there are

00:25:00,800 --> 00:25:05,240
like border effects usually both because

00:25:03,290 --> 00:25:08,170
of the geometry so there's nothing and

00:25:05,240 --> 00:25:11,030
because of the of the algorithm itself

00:25:08,170 --> 00:25:13,880
that of course looks at the the

00:25:11,030 --> 00:25:15,530
neighboring cells but there are few

00:25:13,880 --> 00:25:17,330
neighboring cells so basically what

00:25:15,530 --> 00:25:21,670
happens at the borders should always be

00:25:17,330 --> 00:25:21,670
considered with the you know kosher

00:25:23,050 --> 00:25:26,170
thank you

00:25:26,350 --> 00:25:28,410

YouTube URL: https://www.youtube.com/watch?v=kU3CVTKE4YI


