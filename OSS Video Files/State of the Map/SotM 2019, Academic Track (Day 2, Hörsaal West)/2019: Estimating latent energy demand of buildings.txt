Title: 2019: Estimating latent energy demand of buildings
Publication date: 2019-09-28
Playlist: SotM 2019, Academic Track (Day 2, Hörsaal West)
Description: 
	We propose a model that uses only open data for estimating the minimal energy use of individual buildings for heating and cooling at scale. The workflow is divided in two main blocks: (i) predicting at scale a 3D building stock using OpenStreetMap data, and (ii) estimating the energy use of buildings individually with a back-end model.

For mitigating climate change, it is crucial to minimize energy use in buildings while maintaining decent wellbeing (1). Buildings contribute more than a quarter of the global energy-related emissions (2). On the one hand, buildings are among the lowest-hanging fruit for mitigation—with technological solutions available for energy-neutral or even energy-positive buildings. On the other hand, deep decarbonization is challenging because of the heterogeneity in the building stock (2). Moreover, achieving decent well-being conditions for everyone could increase the energy consumption of buildings. For instance, in countries like India where global warming is likely is intensify the deadly effects of heatwaves, air-conditioning is considered an important adaptation strategy.

Currently, building energy demand, e.g. for heating and cooling, is mostly estimated for individual buildings with data-intensive models, or with fudge factors in highly aggregated models globally. For rapid, wide-spread mitigation, such methods are not sufficient either to develop solutions that can be transferred across urban areas, nor to develop tailored solutions based on local data everywhere (3). This situation confines impactful climate action to a limited group of cities. There are entire regions with pressing mitigation and development issues that are left behind, in particular rapidly-urbanizing urban areas in the global South (4). The rise of big data and artificial intelligence offer new opportunities to model building energy demand for urban areas, and can even take into account geographical diversity (3). Such data could inform municipal policymakers on spatially stratified but city-wide strategies for climate change mitigation in buildings. However, large-scale spatially-explicit models that compute the minimum energy demand satisfying essential thermal comfort in buildings are still missing. 

Here, we propose a model that uses only open data for estimating the minimal energy use of individual buildings for heating and cooling at scale. The workflow is divided in two main blocks: (i) predicting at scale a 3D building stock, and (ii) estimating the energy use of buildings individually with a back-end model. 

In a first step, we use a machine learning model to learn and predict buildings heights from OpenStreetMap and ground truth LiDAR data. The latent energy use of a building can be approximated by its shape and its height (5). However, the height attribute is sparsely populated in OSM. Previous research (6) has demonstrated that building heights can be predicted from features describing buildings and their surroundings. We compare several machine learning architectures and input features spaces. In particular, we are developing a hybrid convolutional neural network based on OSM raster data and scalar features (7). Such architecture enables to take full advantage of both the spatial and the higher-level information available in OSM. We will test how well the model transfers to new cities, and how to improve generalization.

In a second step, a back-end model computes the latent, minimal energy demand of each building for heating and cooling. The model accounts for simplified building characteristics, local climatological conditions and ancillary factors. Climatological conditions strongly influence building energy use. The latent energy demand is a simple metric that does not account for occupancy patterns or appliance use. However, this metric provides a lower bound for the energy necessary for thermal well-being, and it is simple enough so that it can be computed for any building. 

Preliminary results indicate that on average the energy demand in the studied areas can be reduced by a large amount while maintaining decent thermal comfort. However, strategies for energy demand reduction depend on building stock vintage, cultural standards, and the local climate. Our framework provides new insights from OpenStreetMap for more detailed and globally consistent analyses of mitigation strategies in cities.

1.  N. D. Rao, P. Baer, “Decent Living” Emissions: A Conceptual Framework. Sustainability (2012)
2.  O. Lucon et al., in Climate Change 2014: Mitigation of Climate Change. Contribution of Working Group III to the Fifth Assessment Report of the Intergovernmental Panel on Climate Change [Edenhofer, O., R. Pichs-Madruga, Y. Sokona, E. Farahani, S. Kadner, K. Seyboth, A. Adler, I. Baum, S. Brunner, P. Eickemeier, B. Kriemann, J. Savolainen, S. Schlömer, C. von Stechow, T. Zwickel and J.C. Minx (eds.)] (2014)
Captions: 
	00:00:08,110 --> 00:00:13,820
so welcome welcome back to the academic

00:00:11,210 --> 00:00:16,189
track we have a second talk and the

00:00:13,820 --> 00:00:18,259
presenter is Nicola Molloy image from

00:00:16,189 --> 00:00:20,960
the Mercator Research Institute on

00:00:18,259 --> 00:00:24,650
global Commons and climate change you

00:00:20,960 --> 00:00:29,689
have 20 minutes for the talk thank you

00:00:24,650 --> 00:00:34,100
very much okay so this is a

00:00:29,689 --> 00:00:37,239
collaborative work where we try to help

00:00:34,100 --> 00:00:39,680
cities reduce the energy use in building

00:00:37,239 --> 00:00:44,320
in the context of mitigating climate

00:00:39,680 --> 00:00:46,670
change and this is work in progress so I

00:00:44,320 --> 00:00:48,769
will mostly be presenting a framework

00:00:46,670 --> 00:00:52,309
and experiments more than final results

00:00:48,769 --> 00:00:54,019
and and I will also focus on the our use

00:00:52,309 --> 00:00:57,409
of the OpenStreetMap data in our

00:00:54,019 --> 00:01:00,920
framework so I want to talk about three

00:00:57,409 --> 00:01:03,140
things today first is why are we using

00:01:00,920 --> 00:01:05,600
open shipment data and this is because

00:01:03,140 --> 00:01:11,240
it's a great source for us to find data

00:01:05,600 --> 00:01:13,369
on infrastructure then the second thing

00:01:11,240 --> 00:01:15,229
is that as a sustainability researcher

00:01:13,369 --> 00:01:17,409
one it's like there are certain features

00:01:15,229 --> 00:01:20,270
in OpenStreetMap that we like to have

00:01:17,409 --> 00:01:22,789
and for which the coverage is currently

00:01:20,270 --> 00:01:25,280
limited and so the first third thing is

00:01:22,789 --> 00:01:26,420
gonna I will show you or we try to use

00:01:25,280 --> 00:01:30,259
machine learning to predict these

00:01:26,420 --> 00:01:33,140
features so first I roll why are we

00:01:30,259 --> 00:01:35,359
interesting in infrastructure because

00:01:33,140 --> 00:01:39,200
infrastructure frames to a large extent

00:01:35,359 --> 00:01:41,960
energy use in cities so infrastructure

00:01:39,200 --> 00:01:44,329
can lock people in certain habits and

00:01:41,960 --> 00:01:46,420
that can be a problem is this habits are

00:01:44,329 --> 00:01:51,049
unsustainable for instance for mobility

00:01:46,420 --> 00:01:53,479
if you have the the iOS to the left it's

00:01:51,049 --> 00:01:56,800
hardly like is difficult to move in the

00:01:53,479 --> 00:01:58,720
city without taking a car well if you

00:01:56,800 --> 00:02:02,530
this back lines to the right then it

00:01:58,720 --> 00:02:04,720
enables people to take the bike and

00:02:02,530 --> 00:02:08,320
that's the same thing for for buildings

00:02:04,720 --> 00:02:11,740
if you have large rooms then you need a

00:02:08,320 --> 00:02:14,080
lot of energy to even call those those

00:02:11,740 --> 00:02:16,990
rooms well if you are smaller rooms it

00:02:14,080 --> 00:02:18,880
requires less and so I leave myself in

00:02:16,990 --> 00:02:20,170
an old building with high ceiling it's

00:02:18,880 --> 00:02:23,950
very nice but it's it's not

00:02:20,170 --> 00:02:26,110
energy-efficient and at the end of the

00:02:23,950 --> 00:02:27,580
year if we sum up all the energy that we

00:02:26,110 --> 00:02:30,540
use for building a superstation it's

00:02:27,580 --> 00:02:33,820
about sixty percent of our energy demand

00:02:30,540 --> 00:02:36,550
which translate into a more or less 50

00:02:33,820 --> 00:02:39,220
percent of the greenhouse gas emission

00:02:36,550 --> 00:02:41,350
that we use to produce energy so that we

00:02:39,220 --> 00:02:44,320
emit to produce energy so that's that's

00:02:41,350 --> 00:02:47,080
really a lot and our use of building and

00:02:44,320 --> 00:02:50,950
transplant infrastructure are really key

00:02:47,080 --> 00:02:54,460
contributors to climate change and and

00:02:50,950 --> 00:02:56,530
so this this demand this energy demand

00:02:54,460 --> 00:02:58,720
is expected to keep on growing while we

00:02:56,530 --> 00:03:03,280
are still struggling to to make this

00:02:58,720 --> 00:03:05,650
energy production decarbonized so we

00:03:03,280 --> 00:03:09,040
really need to to reduce our energy use

00:03:05,650 --> 00:03:10,989
and we need to do that very fast because

00:03:09,040 --> 00:03:14,320
we basically have something like 30

00:03:10,989 --> 00:03:19,410
years to get to zero net emissions if we

00:03:14,320 --> 00:03:23,010
want to remain below 1.5 degree of

00:03:19,410 --> 00:03:24,510
global mean temperature increase and

00:03:23,010 --> 00:03:30,850
[Music]

00:03:24,510 --> 00:03:33,880
yeah and and in order to do that like

00:03:30,850 --> 00:03:37,300
one of the solution that we think can be

00:03:33,880 --> 00:03:39,340
really efficient is to to use

00:03:37,300 --> 00:03:41,770
infrastructure planning as a way to

00:03:39,340 --> 00:03:46,000
reframe the energy demand of C in cities

00:03:41,770 --> 00:03:47,370
so this is about dealing with our

00:03:46,000 --> 00:03:52,150
current infrastructure and also

00:03:47,370 --> 00:03:54,580
designing the new addition both in in

00:03:52,150 --> 00:03:59,040
order to to help people having a lower

00:03:54,580 --> 00:03:59,040
carbon lifestyle and

00:04:01,270 --> 00:04:06,800
policymakers in order to design policies

00:04:04,310 --> 00:04:09,430
for that lady information they need

00:04:06,800 --> 00:04:12,680
information like where's the energy use

00:04:09,430 --> 00:04:15,950
what what is the reason for a certain

00:04:12,680 --> 00:04:18,170
energy use and imaginary use is there in

00:04:15,950 --> 00:04:23,000
their cities so they need geographical

00:04:18,170 --> 00:04:24,980
models to see that and and in addition

00:04:23,000 --> 00:04:26,450
if we can have common frameworks with

00:04:24,980 --> 00:04:29,180
models that are applicable to different

00:04:26,450 --> 00:04:32,690
cities then we can compare the problem

00:04:29,180 --> 00:04:35,300
and solutions in different cities so

00:04:32,690 --> 00:04:40,250
that's where we wanted to do something

00:04:35,300 --> 00:04:43,700
and so currently for buildings there is

00:04:40,250 --> 00:04:45,050
a certain gap between a lot of studies

00:04:43,700 --> 00:04:50,240
that are looking at buildings that

00:04:45,050 --> 00:04:52,670
really at the building level and other

00:04:50,240 --> 00:04:55,220
studies that work at a much more

00:04:52,670 --> 00:04:58,160
aggregated level so there is a gap in

00:04:55,220 --> 00:05:00,440
the literature in having building other

00:04:58,160 --> 00:05:04,360
energy models for buildings both at the

00:05:00,440 --> 00:05:07,250
building level but at the city scale so

00:05:04,360 --> 00:05:12,860
our framework in our homework we try to

00:05:07,250 --> 00:05:16,610
have building energy models for all City

00:05:12,860 --> 00:05:20,030
where we have individual buildings so

00:05:16,610 --> 00:05:22,730
first or as a 3d model where you can see

00:05:20,030 --> 00:05:26,380
its building and then for each building

00:05:22,730 --> 00:05:29,150
we which were compute the energy use as

00:05:26,380 --> 00:05:30,860
and so during the rest of the talk I

00:05:29,150 --> 00:05:33,800
will focus on the first part or we use

00:05:30,860 --> 00:05:36,250
OpenStreetMap data to generate the 3d

00:05:33,800 --> 00:05:39,140
building model and so just as a note

00:05:36,250 --> 00:05:42,320
there is one of the reason why most of

00:05:39,140 --> 00:05:43,250
the models so far that the building

00:05:42,320 --> 00:05:45,950
level is that because it's very

00:05:43,250 --> 00:05:48,770
difficult to compute the energy use of a

00:05:45,950 --> 00:05:50,600
building so in our framework which would

00:05:48,770 --> 00:05:53,470
try to make it very simple and

00:05:50,600 --> 00:05:57,020
absolutely very simplified metrics so

00:05:53,470 --> 00:06:00,560
and then like I have a modular framework

00:05:57,020 --> 00:06:03,680
that could be made more complex later

00:06:00,560 --> 00:06:05,000
and so for an as an example so far we

00:06:03,680 --> 00:06:09,940
just look as

00:06:05,000 --> 00:06:13,520
and it's use for eating and cooling so

00:06:09,940 --> 00:06:15,850
so if we want to have a model for each

00:06:13,520 --> 00:06:19,060
building in a city we thought that

00:06:15,850 --> 00:06:27,230
OpenStreetMap is a very nice opportunity

00:06:19,060 --> 00:06:29,420
to start the model with the data so so

00:06:27,230 --> 00:06:31,940
the the 3d model that we try to do first

00:06:29,420 --> 00:06:34,400
year is like at the LOD one so this is

00:06:31,940 --> 00:06:38,570
basically if like a footprint multiplied

00:06:34,400 --> 00:06:40,640
by a single ID value for building and

00:06:38,570 --> 00:06:42,290
the rationale here is that we basically

00:06:40,640 --> 00:06:44,600
want to get the volume of the building

00:06:42,290 --> 00:06:47,900
that you need to eat and cool and we

00:06:44,600 --> 00:06:50,180
also need to the surface of auto walls

00:06:47,900 --> 00:06:52,460
because this is where you have thermal

00:06:50,180 --> 00:06:56,120
loss and so these two elements are

00:06:52,460 --> 00:06:59,360
important to compute the energy use with

00:06:56,120 --> 00:07:02,240
different sources of data remote sensing

00:06:59,360 --> 00:07:06,680
cadastre and particularly dar data can

00:07:02,240 --> 00:07:08,960
provide accurate models but it's it's

00:07:06,680 --> 00:07:13,370
expensive and it's on it's only limited

00:07:08,960 --> 00:07:17,060
to few cities that the financial means

00:07:13,370 --> 00:07:20,030
to have such models so so can we use

00:07:17,060 --> 00:07:22,460
Open Street Map and so in a pan Sigma

00:07:20,030 --> 00:07:26,150
will basically have many of the

00:07:22,460 --> 00:07:30,080
footprints but and the Ides are stored

00:07:26,150 --> 00:07:34,430
in the key height but it turns that this

00:07:30,080 --> 00:07:38,900
key is rather sparse so we have 20 12

00:07:34,430 --> 00:07:41,810
millions key I'd feel worthwhile so this

00:07:38,900 --> 00:07:44,840
is more or less 25 times the size of a

00:07:41,810 --> 00:07:47,930
city like Berlin so the question was can

00:07:44,840 --> 00:07:52,520
we predict over other IDEs out of this

00:07:47,930 --> 00:07:55,820
data so I will show you the the either

00:07:52,520 --> 00:07:59,450
available in OpenStreetMap in europe so

00:07:55,820 --> 00:08:02,360
we have more as 1.5 million building we

00:07:59,450 --> 00:08:05,270
write in OpenStreetMap in europe where

00:08:02,360 --> 00:08:10,100
it's quite easier genius Italy Germany

00:08:05,270 --> 00:08:13,060
in France are the top one but in all

00:08:10,100 --> 00:08:15,740
Scandinavia and all the Balkans there is

00:08:13,060 --> 00:08:16,660
about the same number of buildings that

00:08:15,740 --> 00:08:19,870
you have an idol

00:08:16,660 --> 00:08:21,940
that would have I'd in in each country

00:08:19,870 --> 00:08:25,060
so this is quite little

00:08:21,940 --> 00:08:28,000
now if we zoom for instance in Germany

00:08:25,060 --> 00:08:30,550
we see that building can be fairly well

00:08:28,000 --> 00:08:32,260
distributed the building with heights

00:08:30,550 --> 00:08:33,760
can be very well distributed so this is

00:08:32,260 --> 00:08:36,150
important in the context of a prediction

00:08:33,760 --> 00:08:39,220
because that gives us at the origin 80

00:08:36,150 --> 00:08:41,650
about different context but if we zoom

00:08:39,220 --> 00:08:44,080
even more on Berlin we see that it's

00:08:41,650 --> 00:08:46,600
quite a small proportion of the

00:08:44,080 --> 00:08:51,760
buildings that right and this is the

00:08:46,600 --> 00:08:53,650
case in most for most cities but still

00:08:51,760 --> 00:08:57,630
we have many cities in Europe where we

00:08:53,650 --> 00:09:00,520
have what the wall city is pretty much

00:08:57,630 --> 00:09:05,820
mapped with it-- so this is particularly

00:09:00,520 --> 00:09:10,120
the case in north eastern Italy where we

00:09:05,820 --> 00:09:13,630
in even in remote region good mapping

00:09:10,120 --> 00:09:16,150
and another interesting point for us is

00:09:13,630 --> 00:09:19,390
that we have really points we really

00:09:16,150 --> 00:09:20,890
have cities with some points in various

00:09:19,390 --> 00:09:25,780
digital graphics in Europe we have in

00:09:20,890 --> 00:09:29,110
Majorca up to Belarus no way Istanbul so

00:09:25,780 --> 00:09:32,830
that's that's very different less point

00:09:29,110 --> 00:09:36,730
on the data is of course the quality and

00:09:32,830 --> 00:09:38,920
so the question is here are the do we

00:09:36,730 --> 00:09:40,810
have representative samples in the

00:09:38,920 --> 00:09:44,380
OpenStreetMap data but the real

00:09:40,810 --> 00:09:47,770
distribution of ayats in europe so here

00:09:44,380 --> 00:09:50,320
i given it again an example of berlin so

00:09:47,770 --> 00:09:53,010
the top chart is the official the

00:09:50,320 --> 00:09:57,060
distribution of the official data and

00:09:53,010 --> 00:10:00,430
bottom one is the wine OpenStreetMap so

00:09:57,060 --> 00:10:03,340
first question here is like is there a

00:10:00,430 --> 00:10:05,860
selection bias because mappers could

00:10:03,340 --> 00:10:08,140
tend to map specific building more than

00:10:05,860 --> 00:10:10,590
others for instance more iconic or

00:10:08,140 --> 00:10:13,390
central buildings and yes we see that

00:10:10,590 --> 00:10:17,290
for instance there is a lack very like

00:10:13,390 --> 00:10:23,200
the building between 5 and 12 meters are

00:10:17,290 --> 00:10:26,920
bit under represented then the second

00:10:23,200 --> 00:10:28,300
question is what is the what is the

00:10:26,920 --> 00:10:30,170
definition that we take of a building

00:10:28,300 --> 00:10:31,190
and so for us

00:10:30,170 --> 00:10:32,540
building that is interesting is a

00:10:31,190 --> 00:10:35,600
building where there was an energy

00:10:32,540 --> 00:10:40,160
consumption and so in Berlin I remove

00:10:35,600 --> 00:10:42,320
like this 2,600 objects because there

00:10:40,160 --> 00:10:45,100
were stones of a million monument that

00:10:42,320 --> 00:10:47,959
do not the not relevant in in our

00:10:45,100 --> 00:10:49,760
conception of a building and also in a

00:10:47,959 --> 00:10:53,360
position a large complex building of

00:10:49,760 --> 00:10:56,800
different objects each with a single

00:10:53,360 --> 00:11:00,980
light so in this case we need to merge

00:10:56,800 --> 00:11:03,740
these these different objects and the

00:11:00,980 --> 00:11:05,750
last issue is of course that they can

00:11:03,740 --> 00:11:07,910
also be wrong entries or different level

00:11:05,750 --> 00:11:10,579
of precision so we have certain cities

00:11:07,910 --> 00:11:14,480
where we have eyes only by 5 meters

00:11:10,579 --> 00:11:17,180
interval versus other cities where it

00:11:14,480 --> 00:11:20,180
continues so we need to account for all

00:11:17,180 --> 00:11:22,820
this but also it all together if we look

00:11:20,180 --> 00:11:27,260
again at this distribution the good news

00:11:22,820 --> 00:11:29,690
is that we have data for each building

00:11:27,260 --> 00:11:34,790
level but we also have to be aware that

00:11:29,690 --> 00:11:41,949
like it's very noisy so from that we'll

00:11:34,790 --> 00:11:41,949
try to predict building height and so

00:11:43,060 --> 00:11:49,240
the goal is to use machine learning to

00:11:45,560 --> 00:11:52,519
predict I taught you across Europe and

00:11:49,240 --> 00:11:54,230
for this using only predictive features

00:11:52,519 --> 00:11:57,769
that we compute for OpenStreetMap and

00:11:54,230 --> 00:12:00,589
train those features on all available I

00:11:57,769 --> 00:12:03,560
did from apartment also from other

00:12:00,589 --> 00:12:07,220
sources so there has been already

00:12:03,560 --> 00:12:12,790
similar work at the regional level for

00:12:07,220 --> 00:12:15,589
the USA and finally tab open city model

00:12:12,790 --> 00:12:18,440
so they use also a machine learning

00:12:15,589 --> 00:12:22,310
approach where they map the footprint to

00:12:18,440 --> 00:12:25,579
the to the ID but they do not report

00:12:22,310 --> 00:12:26,990
accuracy and in fact they they said that

00:12:25,579 --> 00:12:30,110
they expect that the model is rather

00:12:26,990 --> 00:12:31,519
good on average but that it's not so

00:12:30,110 --> 00:12:33,860
good for dense

00:12:31,519 --> 00:12:37,819
European regions that are very important

00:12:33,860 --> 00:12:41,139
for us and ability and colleague is also

00:12:37,819 --> 00:12:43,850
did it in a scientific paper I can

00:12:41,139 --> 00:12:47,300
mostly matically for two Dutch cities

00:12:43,850 --> 00:12:50,480
and so for this similar type of model

00:12:47,300 --> 00:12:53,329
where they map the the footprint to the

00:12:50,480 --> 00:12:55,579
ID they find they have an error around

00:12:53,329 --> 00:12:58,329
three meters and they're our best model

00:12:55,579 --> 00:13:02,170
in which they use the number of floors

00:12:58,329 --> 00:13:07,129
goes down to less than one meter error

00:13:02,170 --> 00:13:10,579
so in our case we we do something

00:13:07,129 --> 00:13:16,069
similar we try to map different features

00:13:10,579 --> 00:13:18,110
about our building that are so we try to

00:13:16,069 --> 00:13:19,309
map a different building feature about

00:13:18,110 --> 00:13:21,170
the building to the eye it's using a

00:13:19,309 --> 00:13:23,240
machine learning regressions and these

00:13:21,170 --> 00:13:26,839
features are either about the building

00:13:23,240 --> 00:13:31,339
itself or the surroundings and here the

00:13:26,839 --> 00:13:35,240
idea is - - that there is a local

00:13:31,339 --> 00:13:38,059
context within around buildings that we

00:13:35,240 --> 00:13:44,629
may be able to harvest for prediction

00:13:38,059 --> 00:13:46,699
and so all these features we found them

00:13:44,629 --> 00:13:48,139
in a way that they can be computed for

00:13:46,699 --> 00:13:51,170
any building in OpenStreetMap so there

00:13:48,139 --> 00:13:52,600
must be a simple geometrical operation

00:13:51,170 --> 00:13:56,870
on polygon cell lines

00:13:52,600 --> 00:14:00,170
so it's scalable and then so it is

00:13:56,870 --> 00:14:01,959
supervised learning so we trained for

00:14:00,170 --> 00:14:06,110
the buildings for which we have add

00:14:01,959 --> 00:14:08,870
these features to the actual site so

00:14:06,110 --> 00:14:13,420
that we get the parameters and then once

00:14:08,870 --> 00:14:15,980
the model is fitted and carefully

00:14:13,420 --> 00:14:19,459
validated they might be used

00:14:15,980 --> 00:14:24,620
- it might be used to predict the ID for

00:14:19,459 --> 00:14:27,709
building that don't have it so I will

00:14:24,620 --> 00:14:29,480
show you an example so this is just as

00:14:27,709 --> 00:14:35,740
an example these are not definitive

00:14:29,480 --> 00:14:38,720
results so I used random forests with

00:14:35,740 --> 00:14:41,470
really standard settings and I try to

00:14:38,720 --> 00:14:45,250
predict for two cities

00:14:41,470 --> 00:14:48,340
so a Czech city and Italian city is

00:14:45,250 --> 00:14:51,400
ready to predict the ayats within and

00:14:48,340 --> 00:14:57,660
across the two cities so the two cities

00:14:51,400 --> 00:15:01,180
are like both more than 220 thousand

00:14:57,660 --> 00:15:04,720
buildings with ID and they so they have

00:15:01,180 --> 00:15:06,280
like a standard deviation of less than 5

00:15:04,720 --> 00:15:10,540
meters so the data is where they are

00:15:06,280 --> 00:15:14,650
concentrated below 10 meters and the two

00:15:10,540 --> 00:15:16,690
cities are rather similar distribution

00:15:14,650 --> 00:15:20,830
although they are also a bit different

00:15:16,690 --> 00:15:24,570
so this is a priori we can think that

00:15:20,830 --> 00:15:28,210
equal work but it will also not work so

00:15:24,570 --> 00:15:30,430
here are the result within cities so if

00:15:28,210 --> 00:15:32,200
we look at the error it's between 2 and

00:15:30,430 --> 00:15:34,810
3 meters and now if we look at the

00:15:32,200 --> 00:15:37,420
distribution of the era so on the x-axis

00:15:34,810 --> 00:15:40,510
you have the predicted values on the

00:15:37,420 --> 00:15:43,510
y-axis you have the real values and blue

00:15:40,510 --> 00:15:46,060
line is the point where the predicted

00:15:43,510 --> 00:15:47,980
value equals the real value so if we are

00:15:46,060 --> 00:15:49,540
on that line it's good the further away

00:15:47,980 --> 00:15:53,800
we are from that line the further it's

00:15:49,540 --> 00:15:58,240
either under or over predicted so for

00:15:53,800 --> 00:16:02,200
those two cities we have points that are

00:15:58,240 --> 00:16:04,270
rather next to this lines and if you

00:16:02,200 --> 00:16:05,980
look at the Italian city we see that

00:16:04,270 --> 00:16:08,980
most of the points are actually

00:16:05,980 --> 00:16:13,450
predicted in this corridor like plus or

00:16:08,980 --> 00:16:18,940
minus 2 point 5 meter but now if we look

00:16:13,450 --> 00:16:22,690
at cross city so for the so those cities

00:16:18,940 --> 00:16:24,250
those results are prediction made a city

00:16:22,690 --> 00:16:28,330
with a model that with China the other

00:16:24,250 --> 00:16:32,380
city so for for the check city it's the

00:16:28,330 --> 00:16:33,370
the the error seems not not too bad but

00:16:32,380 --> 00:16:36,130
actually when you look at the

00:16:33,370 --> 00:16:39,310
distribution we see that the model

00:16:36,130 --> 00:16:42,100
doesn't get it right I mean like almost

00:16:39,310 --> 00:16:44,830
never and it's actually predicting

00:16:42,100 --> 00:16:47,050
almost everything to be 5 meter and for

00:16:44,830 --> 00:16:47,420
the other is the similar thing as a

00:16:47,050 --> 00:16:51,710
penny

00:16:47,420 --> 00:16:53,270
so so we see that the the generalization

00:16:51,710 --> 00:16:56,720
didn't work so this is really like

00:16:53,270 --> 00:16:59,750
something that is that is very tricky

00:16:56,720 --> 00:17:01,250
and on which were working so the there

00:16:59,750 --> 00:17:03,650
is some research on like whether the

00:17:01,250 --> 00:17:08,510
conditions for specialization within

00:17:03,650 --> 00:17:11,950
special models and and so the different

00:17:08,510 --> 00:17:15,920
things that we are looking at is like

00:17:11,950 --> 00:17:18,140
which kind of something could be better

00:17:15,920 --> 00:17:21,440
like we need to develop specific models

00:17:18,140 --> 00:17:22,190
that are good at predicting in urban

00:17:21,440 --> 00:17:24,050
settings

00:17:22,190 --> 00:17:27,050
versus rural is it something that has

00:17:24,050 --> 00:17:31,370
something to do with the region etc and

00:17:27,050 --> 00:17:32,950
then also the work in finding the right

00:17:31,370 --> 00:17:35,690
algorithm

00:17:32,950 --> 00:17:40,580
finding the right features that do not

00:17:35,690 --> 00:17:45,890
overfeed for instance coordinates tend

00:17:40,580 --> 00:17:48,500
to over fit and also using procedures to

00:17:45,890 --> 00:17:49,160
validate the model such as special cross

00:17:48,500 --> 00:17:54,020
validation

00:17:49,160 --> 00:17:58,160
instead of random cross validation a few

00:17:54,020 --> 00:18:02,990
last point before computing for some

00:17:58,160 --> 00:18:06,230
implication so in this kind of exercise

00:18:02,990 --> 00:18:08,810
every building maps counts as and

00:18:06,230 --> 00:18:12,170
specifically the building in low data

00:18:08,810 --> 00:18:16,040
settings are most important so the the

00:18:12,170 --> 00:18:18,170
big issues we will infrastructures are

00:18:16,040 --> 00:18:20,540
alive specifically in the global south

00:18:18,170 --> 00:18:23,570
so for instance in Africa the very few

00:18:20,540 --> 00:18:26,500
buildings with with it--so these

00:18:23,570 --> 00:18:30,920
buildings are even more important and

00:18:26,500 --> 00:18:32,420
the number of floor is also like a very

00:18:30,920 --> 00:18:35,600
good predictor that is easier to map

00:18:32,420 --> 00:18:37,400
than building heights and it's

00:18:35,600 --> 00:18:39,440
interesting that there are as many

00:18:37,400 --> 00:18:44,570
buildings in OpenStreetMap that number

00:18:39,440 --> 00:18:47,450
of floor as building the height so yeah

00:18:44,570 --> 00:18:49,250
just to conclude infrastructure data of

00:18:47,450 --> 00:18:52,280
OpenStreetMap this video.i relevance for

00:18:49,250 --> 00:18:54,410
us and so we are very grateful the the

00:18:52,280 --> 00:18:55,410
great work is done by the open

00:18:54,410 --> 00:18:59,250
shoot-'em-up community

00:18:55,410 --> 00:19:01,410
and so so yes stability researcher we

00:18:59,250 --> 00:19:06,530
would love to get closer from from this

00:19:01,410 --> 00:19:09,840
community and find ways to are mutually

00:19:06,530 --> 00:19:13,490
productive for for the first nobility

00:19:09,840 --> 00:19:13,490
for position thank you

00:19:17,930 --> 00:19:29,200
thank you very much Nicola are there

00:19:20,660 --> 00:19:33,140
questions thank you

00:19:29,200 --> 00:19:36,590
I appreciate that is difficult to get

00:19:33,140 --> 00:19:55,400
hold of but standard aerial imagery

00:19:36,590 --> 00:19:57,200
should be good but so yeah we did this

00:19:55,400 --> 00:19:58,940
is like we're trying to see if also if

00:19:57,200 --> 00:20:02,710
we can combine the different type of

00:19:58,940 --> 00:20:16,840
data so for instance with yes stereo

00:20:02,710 --> 00:20:16,840
images other questions

00:20:19,210 --> 00:20:25,100
Thanks interesting talk and my question

00:20:23,150 --> 00:20:27,110
is about installation how do you account

00:20:25,100 --> 00:20:29,390
for that because like if I insulator

00:20:27,110 --> 00:20:32,120
uninsulated building something the

00:20:29,390 --> 00:20:36,770
footprint is like I don't know fifth or

00:20:32,120 --> 00:20:41,929
fourth so yeah so far it's really like

00:20:36,770 --> 00:20:44,360
very very simplified matrix where we

00:20:41,929 --> 00:20:45,950
just try to say in a certain scenario if

00:20:44,360 --> 00:20:49,070
we assume that all buildings in this

00:20:45,950 --> 00:20:51,679
area would be the same what would be the

00:20:49,070 --> 00:20:53,390
energy footprint and then so that that's

00:20:51,679 --> 00:20:55,210
why I was saying that so first is a very

00:20:53,390 --> 00:20:57,470
simple framework and then if we get

00:20:55,210 --> 00:21:03,679
information on the building types then

00:20:57,470 --> 00:21:07,309
we would add it so this is yeah I had a

00:21:03,679 --> 00:21:08,780
question since you also used the context

00:21:07,309 --> 00:21:13,220
as feature as a predictor building

00:21:08,780 --> 00:21:14,929
height and then wouldn't is this I can

00:21:13,220 --> 00:21:17,559
expect that there are other indicators

00:21:14,929 --> 00:21:20,630
for the to describe the context like

00:21:17,559 --> 00:21:22,330
socio-economic development things like

00:21:20,630 --> 00:21:25,480
this is that influence right

00:21:22,330 --> 00:21:25,480
[Music]

00:21:25,710 --> 00:21:29,039
we're trying to find also features that

00:21:27,120 --> 00:21:30,419
we can find everywhere like for instance

00:21:29,039 --> 00:21:33,330
in Europe the where we have the data

00:21:30,419 --> 00:21:38,039
everywhere but and also that is really

00:21:33,330 --> 00:21:41,130
at the block level but yeah old all sort

00:21:38,039 --> 00:21:44,279
of I mean this is for sure probably very

00:21:41,130 --> 00:21:55,370
well correlated so yeah but it's a city

00:21:44,279 --> 00:21:59,070
level here it's not yeah best yeah okay

00:21:55,370 --> 00:22:01,710
so the you've been talking mostly about

00:21:59,070 --> 00:22:06,659
the building heights are there any other

00:22:01,710 --> 00:22:12,029
tags you would like us to add yeah so

00:22:06,659 --> 00:22:14,010
like of course the like 1m and actually

00:22:12,029 --> 00:22:18,570
so I've been focusing a lot on these tag

00:22:14,010 --> 00:22:20,250
so I'm not sure like for instance one

00:22:18,570 --> 00:22:23,039
thing that is very important for in

00:22:20,250 --> 00:22:24,690
buildings is like how many windows there

00:22:23,039 --> 00:22:25,679
are on the building I don't know this is

00:22:24,690 --> 00:22:35,159
something that is tagging up and

00:22:25,679 --> 00:22:39,510
sweetheart but apparently enough other

00:22:35,159 --> 00:22:42,179
questions yes a question you showed that

00:22:39,510 --> 00:22:45,990
the building height is different than

00:22:42,179 --> 00:22:49,320
the official the real height and that

00:22:45,990 --> 00:22:51,539
can be bias in the data have you looked

00:22:49,320 --> 00:22:53,100
at like what part of it is noise of the

00:22:51,539 --> 00:22:56,370
data because if there is a lot of noise

00:22:53,100 --> 00:22:58,260
there all the predictions also so your

00:22:56,370 --> 00:23:01,289
question is which are the sources of

00:22:58,260 --> 00:23:04,860
noise yeah any idea how much noise we

00:23:01,289 --> 00:23:07,200
have yeah my rough estimation for

00:23:04,860 --> 00:23:10,010
building I haven't and then he has like

00:23:07,200 --> 00:23:16,500
his City specific right is very small as

00:23:10,010 --> 00:23:19,799
between 10 and 20 percent that sir and

00:23:16,500 --> 00:23:22,799
that do not have the right right but

00:23:19,799 --> 00:23:25,110
this is really roughly I haven't

00:23:22,799 --> 00:23:27,960
finished this analysis yet yeah so that

00:23:25,110 --> 00:23:29,640
which you have like quite a good number

00:23:27,960 --> 00:23:31,650
of buildings that

00:23:29,640 --> 00:23:33,000
good and the the and you can also see

00:23:31,650 --> 00:23:35,040
from like certain cities where

00:23:33,000 --> 00:23:37,590
everything else seems to have been

00:23:35,040 --> 00:23:40,049
mapped by the same entity then you can

00:23:37,590 --> 00:23:42,299
have more confidence so this is diffic

00:23:40,049 --> 00:23:51,559
but that there can be cities where you

00:23:42,299 --> 00:24:05,309
are thank you any other quick question

00:23:51,559 --> 00:24:08,210
yes you you trained the models on two

00:24:05,309 --> 00:24:11,460
different cities and applied them to the

00:24:08,210 --> 00:24:15,929
other cities do you think it would make

00:24:11,460 --> 00:24:18,510
sense to train models for certain parts

00:24:15,929 --> 00:24:21,210
of the town like an old town or suburbs

00:24:18,510 --> 00:24:23,610
and do you think that this way you could

00:24:21,210 --> 00:24:30,809
get better results when applying them

00:24:23,610 --> 00:24:40,740
through other cities a way to find in

00:24:30,809 --> 00:24:46,919
the cities to know which part okay thank

00:24:40,740 --> 00:24:48,870
you any other question okay I would

00:24:46,919 --> 00:24:50,610
right just to close with a remark you

00:24:48,870 --> 00:24:53,100
already said that but this is a very

00:24:50,610 --> 00:24:55,890
clear example of how much each single

00:24:53,100 --> 00:24:58,230
mapper can do for some scientific

00:24:55,890 --> 00:25:00,450
applications and sometimes a great thing

00:24:58,230 --> 00:25:02,520
about SEM is that some data added by a

00:25:00,450 --> 00:25:04,260
mapper can be used for other

00:25:02,520 --> 00:25:07,650
applications that the mapper could even

00:25:04,260 --> 00:25:09,960
not think of so thanks every building

00:25:07,650 --> 00:25:12,900
counts and this is really important here

00:25:09,960 --> 00:25:15,050
so thanks again kala and thank you for

00:25:12,900 --> 00:25:23,170
participating in this session

00:25:15,050 --> 00:25:23,170

YouTube URL: https://www.youtube.com/watch?v=vCkJ46i0ygc


