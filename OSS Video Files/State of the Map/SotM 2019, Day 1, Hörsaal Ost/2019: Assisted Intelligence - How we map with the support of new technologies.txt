Title: 2019: Assisted Intelligence - How we map with the support of new technologies
Publication date: 2019-09-26
Playlist: SotM 2019, Day 1, HÃ¶rsaal Ost
Description: 
	Deep learning methods for feature extraction using computer vision are giving concrete results. This talk provides an overview of feature detection from satellite imagery and how machine learning can provide a seamless mapping experience for mappers, allowing them to utilize their knowledge to enrich maps further.

Deep learning methods for feature extraction using computer vision are giving concrete results. This talk provides an overview of feature detection from satellite imagery and how machine learning can provide a seamless experience for mappers, allowing them to utilize their knowledge to enrich maps further. 

When using OpenStreetMap for disaster response and humanitarian action, time and data quality are critical. Automated tools around OpenStreetMap have revolutionized disaster response, allowing mappers to leverage their local knowledge to organize themselves in action and to contribute to highly relevant maps. Over the last year, Tasking Manager team at HOT conceptualized an initiative to test using data derived using machine learning models to improve task creation, quality of edits and overall experience for mappers.

Within Tasking Manager Working Groups we are collaborating between different actors to test concepts directly applied to two countries: Tanzania and Uganda. Two tools in pilot phase to assist mappers are around 1) Task Creation and facilitation: identify areas that need most work and/ or need an experienced mapper based on task complexity and gap analysis 2) Assisted Mapping: leverage machines to do the heavy lifting around digitizing features, such as buildings and roads, one by one and allowing mappers to focus on their essential craft of mapping. 

In this talk the presenters  will share their findings, learning and existing challenges with the technology.

Felix Delattre Surabhi Singh

https://pretalx.com/sotm2019/talk/DPGWFB/
Captions: 
	00:00:08,700 --> 00:00:16,780
okay welcome everybody I am CD I am the

00:00:12,910 --> 00:00:20,440
chair for today's session and I would

00:00:16,780 --> 00:00:22,690
like to introduce Felix and sue B they

00:00:20,440 --> 00:00:25,330
are doing your presentation on insisted

00:00:22,690 --> 00:00:28,300
intelligence how we met with the support

00:00:25,330 --> 00:00:30,339
of new technology and they will have 20

00:00:28,300 --> 00:00:36,160
minutes for presentation and 5 minutes

00:00:30,339 --> 00:00:39,370
for questions thank you hey everyone I'm

00:00:36,160 --> 00:00:41,170
Sur B from Microsoft and I'm Felix I'm

00:00:39,370 --> 00:00:42,789
after humanitarian OpenStreetMap team

00:00:41,170 --> 00:00:44,260
and we want to share with you the work

00:00:42,789 --> 00:00:47,170
that we have been doing over the last

00:00:44,260 --> 00:00:49,449
month's around building tools to assist

00:00:47,170 --> 00:00:57,730
our mappers with machine learning

00:00:49,449 --> 00:00:59,230
technology ok so artificial intelligence

00:00:57,730 --> 00:01:05,320
and machine learning is becoming more

00:00:59,230 --> 00:01:06,640
and more ok so artificial intelligence

00:01:05,320 --> 00:01:09,250
and machine learning is really becoming

00:01:06,640 --> 00:01:12,370
a reality for us now right since we are

00:01:09,250 --> 00:01:15,520
using or people are using Alexa Siri and

00:01:12,370 --> 00:01:18,040
Google Weiss automatic text translation

00:01:15,520 --> 00:01:20,590
work really well with tools like DPL and

00:01:18,040 --> 00:01:23,770
computer vision and image processing has

00:01:20,590 --> 00:01:25,540
really reached our devices it is a new

00:01:23,770 --> 00:01:27,310
and promising technology but like all

00:01:25,540 --> 00:01:29,830
other technology it will not be the

00:01:27,310 --> 00:01:32,020
solution for everything and is a lot of

00:01:29,830 --> 00:01:32,920
human involvement needed to make the

00:01:32,020 --> 00:01:35,920
best out of it

00:01:32,920 --> 00:01:38,680
some things can be done very well other

00:01:35,920 --> 00:01:40,570
things are really hard to do in the

00:01:38,680 --> 00:01:44,290
context of Open Street Map and maps in

00:01:40,570 --> 00:01:45,670
general we talked about when you talk

00:01:44,290 --> 00:01:47,440
about machine learning and assisted

00:01:45,670 --> 00:01:49,470
intelligence we mostly refer to

00:01:47,440 --> 00:01:53,100
advancements in computer vision and

00:01:49,470 --> 00:01:57,100
combined with deep learning techniques

00:01:53,100 --> 00:01:59,440
this has become like much better than

00:01:57,100 --> 00:02:02,710
with regular computer vision than before

00:01:59,440 --> 00:02:05,590
now with those techniques but is also

00:02:02,710 --> 00:02:08,349
becoming a lot more complex and I

00:02:05,590 --> 00:02:11,340
personally don't see that AI or m/l is

00:02:08,349 --> 00:02:13,440
like replacing the human in any way

00:02:11,340 --> 00:02:16,160
but it is definitely like a progress

00:02:13,440 --> 00:02:20,460
that we are doing in this fields of

00:02:16,160 --> 00:02:23,790
being able to detect detect features

00:02:20,460 --> 00:02:25,620
from imagery this data derived from

00:02:23,790 --> 00:02:26,970
machine learning can actually help to

00:02:25,620 --> 00:02:29,850
assist a mapper to have a more

00:02:26,970 --> 00:02:32,520
comfortable experience and being able to

00:02:29,850 --> 00:02:33,959
focus on what humans are best at it's

00:02:32,520 --> 00:02:36,930
like bringing in the local knowledge

00:02:33,959 --> 00:02:39,510
check data accuracy and improve of

00:02:36,930 --> 00:02:42,300
street map as a whole our approach

00:02:39,510 --> 00:02:44,220
therefore is to assist the mapper not

00:02:42,300 --> 00:02:46,590
changing what the mapper is doing and

00:02:44,220 --> 00:02:49,410
not taking away the responsibility of

00:02:46,590 --> 00:02:51,350
the data for the humanitarian

00:02:49,410 --> 00:02:53,760
OpenStreetMap team and our our

00:02:51,350 --> 00:02:57,600
humanitarian partners it is important to

00:02:53,760 --> 00:03:00,350
map highly vulnerable areas where people

00:02:57,600 --> 00:03:03,480
have the risk to face life-threatening

00:03:00,350 --> 00:03:05,670
situations and for example when fighting

00:03:03,480 --> 00:03:08,220
against Ebola it is essential to have

00:03:05,670 --> 00:03:10,580
information about buildings to respond

00:03:08,220 --> 00:03:13,440
to infections and take needed measures

00:03:10,580 --> 00:03:15,390
like sending this infection teams or

00:03:13,440 --> 00:03:20,519
taking care of the other people that are

00:03:15,390 --> 00:03:24,120
living in this house so the recent

00:03:20,519 --> 00:03:25,920
outbreak in Ebola in Uganda showed again

00:03:24,120 --> 00:03:28,950
the importance for the need of those

00:03:25,920 --> 00:03:31,110
building data in this region and in

00:03:28,950 --> 00:03:34,019
addition hard works in Tanzania also for

00:03:31,110 --> 00:03:37,410
several reasons most prominently because

00:03:34,019 --> 00:03:40,680
of floods hygiene issues and spreading

00:03:37,410 --> 00:03:42,329
diseases that can happen there we and

00:03:40,680 --> 00:03:43,920
our partners are highly interested in an

00:03:42,329 --> 00:03:47,880
open datasets for building for

00:03:43,920 --> 00:03:50,820
OpenStreetMap in this area and so we

00:03:47,880 --> 00:03:52,769
teamed up with Microsoft and in order to

00:03:50,820 --> 00:03:54,989
produce an open datasets derived from

00:03:52,769 --> 00:03:57,630
deep learning techniques and start

00:03:54,989 --> 00:03:59,700
testing and evaluating how these really

00:03:57,630 --> 00:04:03,209
can be used in a responsible way to

00:03:59,700 --> 00:04:05,130
assist mapping and OpenStreetMap our

00:04:03,209 --> 00:04:07,290
initial goal was also to leverage our

00:04:05,130 --> 00:04:09,570
SEM data for the training of the models

00:04:07,290 --> 00:04:13,140
and so we will show you in a bit how

00:04:09,570 --> 00:04:14,940
this turned out the building blocks of

00:04:13,140 --> 00:04:17,729
our work and this is also going to be

00:04:14,940 --> 00:04:19,680
the structure of our presentation are

00:04:17,729 --> 00:04:22,680
those four so first we wanted to create

00:04:19,680 --> 00:04:23,630
an open buildings data set using the

00:04:22,680 --> 00:04:27,890
latest deep

00:04:23,630 --> 00:04:31,640
learning techniques we wanted to build a

00:04:27,890 --> 00:04:33,910
pipeline to connect ml models to

00:04:31,640 --> 00:04:36,260
applications around OpenStreetMap

00:04:33,910 --> 00:04:39,200
experiment and pilot different ways of

00:04:36,260 --> 00:04:41,720
using these predictions for assisted

00:04:39,200 --> 00:04:44,630
mapping and making sure it is very

00:04:41,720 --> 00:04:46,670
important for us that the experience of

00:04:44,630 --> 00:04:49,100
mapping with ml derived data is

00:04:46,670 --> 00:04:52,760
comfortably intuitive and understandable

00:04:49,100 --> 00:04:54,410
for the people we will go now through

00:04:52,760 --> 00:04:56,480
each of these four points in this

00:04:54,410 --> 00:04:57,920
presentation I'm handing it over to Sir

00:04:56,480 --> 00:05:01,370
me to talk about the first building

00:04:57,920 --> 00:05:03,710
block great so with that context in mind

00:05:01,370 --> 00:05:05,650
let's talk about what it takes to create

00:05:03,710 --> 00:05:08,600
an open buildings data set using

00:05:05,650 --> 00:05:09,680
technologies like computer vision I'll

00:05:08,600 --> 00:05:11,210
introduce you guys to some of the

00:05:09,680 --> 00:05:13,640
concepts and some of the challenges that

00:05:11,210 --> 00:05:15,770
we faced in actually making this happen

00:05:13,640 --> 00:05:18,260
in Uganda and Tanzania and share some of

00:05:15,770 --> 00:05:21,110
the results so when we say computer

00:05:18,260 --> 00:05:24,170
vision what does it actually what does

00:05:21,110 --> 00:05:25,280
it actually require so from a future

00:05:24,170 --> 00:05:27,560
extraction point of view if you're

00:05:25,280 --> 00:05:30,170
interested in getting buildings out of

00:05:27,560 --> 00:05:31,820
aerial imagery the input would be that

00:05:30,170 --> 00:05:34,520
in an aerial imagery and the output

00:05:31,820 --> 00:05:36,890
would be vectorized data the steps in

00:05:34,520 --> 00:05:39,230
between are essentially getting that

00:05:36,890 --> 00:05:42,280
input of aerial imagery running it

00:05:39,230 --> 00:05:44,300
through a model to generate pixel

00:05:42,280 --> 00:05:48,170
classification so you get you see these

00:05:44,300 --> 00:05:50,690
red pixel blobs and those are generated

00:05:48,170 --> 00:05:53,060
by classifying each pixel on the imagery

00:05:50,690 --> 00:05:54,620
as whether it's a building or not once

00:05:53,060 --> 00:05:56,360
that is generated they are not all

00:05:54,620 --> 00:06:00,260
perfect as you can see some of the rough

00:05:56,360 --> 00:06:02,030
edges around here and that's why it runs

00:06:00,260 --> 00:06:03,770
through we run it through a second

00:06:02,030 --> 00:06:07,190
algorithm which is called polygon

00:06:03,770 --> 00:06:08,990
ization what that does is smooth out all

00:06:07,190 --> 00:06:12,260
these rough edges and actually make

00:06:08,990 --> 00:06:14,450
those pixel blobs look like polygons so

00:06:12,260 --> 00:06:16,280
that's the high level of like what

00:06:14,450 --> 00:06:19,760
actually happens to generate the vector

00:06:16,280 --> 00:06:21,980
data one of the big concepts is the

00:06:19,760 --> 00:06:24,230
training data so what what does what is

00:06:21,980 --> 00:06:28,520
training data mean then training it

00:06:24,230 --> 00:06:32,300
essentially is a combination or a pair

00:06:28,520 --> 00:06:34,160
of raw imagery with corresponding level

00:06:32,300 --> 00:06:35,639
masks and when we say labeling that

00:06:34,160 --> 00:06:39,669
essentially as

00:06:35,639 --> 00:06:41,459
tracing of the outline of any object of

00:06:39,669 --> 00:06:43,929
interest on top of the aerial imagery

00:06:41,459 --> 00:06:47,439
and it's really important to have this

00:06:43,929 --> 00:06:49,089
pair together so the to the mast will do

00:06:47,439 --> 00:06:50,409
the label that is there actually

00:06:49,089 --> 00:06:52,749
corresponds to the under underlying

00:06:50,409 --> 00:06:54,849
imagery one without the other is not

00:06:52,749 --> 00:06:58,330
really useful from a computer vision

00:06:54,849 --> 00:07:00,249
perspective so essentially there will be

00:06:58,330 --> 00:07:02,319
an image that is read through an image

00:07:00,249 --> 00:07:04,769
API and then there is a corresponding

00:07:02,319 --> 00:07:06,989
mask and those two together are

00:07:04,769 --> 00:07:11,379
basically what she is to train the model

00:07:06,989 --> 00:07:13,989
and that model then is then shown

00:07:11,379 --> 00:07:15,999
imagery from places that it's not seen

00:07:13,989 --> 00:07:20,110
before and that's where the whole scale

00:07:15,999 --> 00:07:22,209
happens so to give you a context in when

00:07:20,110 --> 00:07:26,499
we started working in Uganda in Tanzania

00:07:22,209 --> 00:07:30,909
the model the training data was about it

00:07:26,499 --> 00:07:33,309
the imagery was about 450 5,600 square

00:07:30,909 --> 00:07:35,050
kilometers when we ran the extractions

00:07:33,309 --> 00:07:37,569
we're talking about the area of 1.2

00:07:35,050 --> 00:07:38,769
million square kilometers and that's the

00:07:37,569 --> 00:07:40,869
result that we gonna share so that's

00:07:38,769 --> 00:07:43,360
sort of the scale of what computer

00:07:40,869 --> 00:07:45,069
vision does but when we started running

00:07:43,360 --> 00:07:49,119
so we had a bunch of experience doing

00:07:45,069 --> 00:07:50,860
this in Canada and us and we thought we

00:07:49,119 --> 00:07:52,990
could really leverage that and use some

00:07:50,860 --> 00:07:55,509
of the data high quality data from osm

00:07:52,990 --> 00:07:59,499
to train the models or granted some

00:07:55,509 --> 00:08:00,669
challenges what are the big ones was the

00:07:59,499 --> 00:08:03,369
uniqueness of the imagery in the

00:08:00,669 --> 00:08:06,039
settlements in Uganda in Tanzania so

00:08:03,369 --> 00:08:09,519
here I'm sharing an example from our

00:08:06,039 --> 00:08:12,909
imagery in the US so on the on the right

00:08:09,519 --> 00:08:15,360
side you see the imagery in the US and

00:08:12,909 --> 00:08:17,529
on the left hand side you see these

00:08:15,360 --> 00:08:19,959
interesting structures so the two cools

00:08:17,529 --> 00:08:23,589
that exists in you can that has any and

00:08:19,959 --> 00:08:24,999
not so much in the US yet and the

00:08:23,589 --> 00:08:28,329
connected buildings wouldn't was another

00:08:24,999 --> 00:08:30,099
issue so if you remember it's really the

00:08:28,329 --> 00:08:31,839
model is just taking pixel information

00:08:30,099 --> 00:08:35,829
and classifying building or not building

00:08:31,839 --> 00:08:37,599
so it was not a surprise that the model

00:08:35,829 --> 00:08:39,670
that was trained on US and Canada

00:08:37,599 --> 00:08:48,280
wouldn't just work in you can

00:08:39,670 --> 00:08:50,050
zhenia okay and so this this domain

00:08:48,280 --> 00:08:52,660
switching created a lot of problem for

00:08:50,050 --> 00:08:54,910
us other challenges that we face when we

00:08:52,660 --> 00:08:58,060
were utilizing when we initially thought

00:08:54,910 --> 00:09:01,420
that we could use the OSM data for model

00:08:58,060 --> 00:09:04,510
trading was like the data itself was

00:09:01,420 --> 00:09:05,830
really good but if you remember the pair

00:09:04,510 --> 00:09:08,260
that I was talking about my cancer

00:09:05,830 --> 00:09:11,860
training it did not work really well

00:09:08,260 --> 00:09:13,750
with being a misery one of our inputs

00:09:11,860 --> 00:09:16,240
and so we ran into all kinds of issue

00:09:13,750 --> 00:09:18,310
where the tiles were incomplete when

00:09:16,240 --> 00:09:20,970
when we were aligning the label data on

00:09:18,310 --> 00:09:24,280
top of being imagery there were offsets

00:09:20,970 --> 00:09:26,800
there was vintage issues and obviously

00:09:24,280 --> 00:09:28,990
if there is clouds there is no way for

00:09:26,800 --> 00:09:32,020
the model to know that there's buildings

00:09:28,990 --> 00:09:35,580
underneath it so that's why not a

00:09:32,020 --> 00:09:39,460
surprise this is what we got when we ran

00:09:35,580 --> 00:09:40,690
our model and you got an Tanzania's our

00:09:39,460 --> 00:09:42,640
initial output

00:09:40,690 --> 00:09:44,470
it was very spotty as you can see the

00:09:42,640 --> 00:09:47,650
coverage is not that great we missed a

00:09:44,470 --> 00:09:50,290
lot of buildings so we knew that to

00:09:47,650 --> 00:09:53,890
really make this work we need to retrain

00:09:50,290 --> 00:09:56,950
the model and generate some new data

00:09:53,890 --> 00:09:59,250
that aligns with the bing imagery and we

00:09:56,950 --> 00:10:05,620
did that with a couple of different

00:09:59,250 --> 00:10:07,500
approaches to for labeling so we we

00:10:05,620 --> 00:10:09,940
reached out to our partners in hot and

00:10:07,500 --> 00:10:13,780
sought their help in terms of getting

00:10:09,940 --> 00:10:17,350
really high quality data from scratch so

00:10:13,780 --> 00:10:19,960
they were deployed as excellent data but

00:10:17,350 --> 00:10:22,870
when you're being really when quality is

00:10:19,960 --> 00:10:25,630
really high coverage was not that much

00:10:22,870 --> 00:10:28,720
because you can only do so much and from

00:10:25,630 --> 00:10:30,580
the MA from modeling perspective what we

00:10:28,720 --> 00:10:33,460
found was some of the other approaches

00:10:30,580 --> 00:10:35,170
that were not that superior in terms of

00:10:33,460 --> 00:10:39,010
generating really excellent quality

00:10:35,170 --> 00:10:42,130
training data actually worked for for

00:10:39,010 --> 00:10:43,930
computer vision models so the the three

00:10:42,130 --> 00:10:47,140
rules that you see at the bottom are

00:10:43,930 --> 00:10:50,290
much less hands-on especially the one

00:10:47,140 --> 00:10:52,310
where we did this binary labeling where

00:10:50,290 --> 00:10:54,920
we just expose the data

00:10:52,310 --> 00:10:58,460
to our labelers and asked them to say

00:10:54,920 --> 00:11:01,040
our slavery to classify whether this is

00:10:58,460 --> 00:11:03,110
a good sample or not a good sample and

00:11:01,040 --> 00:11:05,330
that was really quick and we were able

00:11:03,110 --> 00:11:08,480
to generate about two 45,000 labels out

00:11:05,330 --> 00:11:15,320
of that so with all this in mind we then

00:11:08,480 --> 00:11:17,180
started to train our model so here are

00:11:15,320 --> 00:11:19,040
the results between what happened

00:11:17,180 --> 00:11:20,779
between first and final output you can

00:11:19,040 --> 00:11:23,960
see how the coverage really improved

00:11:20,779 --> 00:11:27,710
we went from 800,000 to 18 million

00:11:23,960 --> 00:11:30,790
buildings and a couple of metrics that

00:11:27,710 --> 00:11:33,050
are really important to look at our

00:11:30,790 --> 00:11:35,570
precision and recall in the context of

00:11:33,050 --> 00:11:37,310
computer vision and precision

00:11:35,570 --> 00:11:39,950
essentially means how accurate were we

00:11:37,310 --> 00:11:44,270
so it was a we call it a building it was

00:11:39,950 --> 00:11:45,800
actually a building recall is if it if

00:11:44,270 --> 00:11:47,029
there were 100 buildings presence how

00:11:45,800 --> 00:11:50,660
many will be able to successfully

00:11:47,029 --> 00:11:52,790
identify so if you see between the

00:11:50,660 --> 00:11:55,430
initial and the final iteration with all

00:11:52,790 --> 00:11:58,910
the labeled data that we were able to

00:11:55,430 --> 00:12:01,700
get we really but got a boost in the

00:11:58,910 --> 00:12:04,220
recall and that was really that's sort

00:12:01,700 --> 00:12:10,120
of the power of what human input can

00:12:04,220 --> 00:12:12,950
actually do and so I want to share

00:12:10,120 --> 00:12:16,010
another interesting view that we took so

00:12:12,950 --> 00:12:17,990
when we were going through training that

00:12:16,010 --> 00:12:20,690
the different iterations that we did in

00:12:17,990 --> 00:12:23,300
the very first iteration we took there

00:12:20,690 --> 00:12:27,200
from there are hot partners provided us

00:12:23,300 --> 00:12:30,290
from Darussalam very urban the model was

00:12:27,200 --> 00:12:32,600
all fitted to urban you you can see we

00:12:30,290 --> 00:12:34,730
got eight million results and here we

00:12:32,600 --> 00:12:38,120
tried to I'm gonna focus on the polygon

00:12:34,730 --> 00:12:40,640
metrics and here you can see how the

00:12:38,120 --> 00:12:43,070
there is difference in urban and rural

00:12:40,640 --> 00:12:45,500
output so the orange bars are

00:12:43,070 --> 00:12:48,140
essentially recalled which is a measure

00:12:45,500 --> 00:12:50,630
of coverage you can see how in the first

00:12:48,140 --> 00:12:52,970
iteration the rural coverage was not

00:12:50,630 --> 00:12:54,830
that great and then we started going to

00:12:52,970 --> 00:12:56,990
more and more rural areas and getting

00:12:54,830 --> 00:13:00,050
data for that exposing that to the model

00:12:56,990 --> 00:13:02,779
and as a result the recall started

00:13:00,050 --> 00:13:04,570
really jumping up in the later area

00:13:02,779 --> 00:13:07,010
we also experiment with some of the new

00:13:04,570 --> 00:13:08,649
technologies that are coming for

00:13:07,010 --> 00:13:11,839
computer vision and that also helped us

00:13:08,649 --> 00:13:15,290
make the jump between 12 to 8 12 to 18

00:13:11,839 --> 00:13:17,449
million buildings so with all this data

00:13:15,290 --> 00:13:21,860
this data is now going to be made

00:13:17,449 --> 00:13:24,440
available for open to be used in OSI or

00:13:21,860 --> 00:13:27,769
to be used in any context when it comes

00:13:24,440 --> 00:13:31,519
to open data Felix is now going to talk

00:13:27,769 --> 00:13:33,920
about how to how to how to have some how

00:13:31,519 --> 00:13:37,760
to actually apply this in the context of

00:13:33,920 --> 00:13:40,070
OS M so when we started the project in

00:13:37,760 --> 00:13:42,320
hot we saw a lot of ml integration

00:13:40,070 --> 00:13:43,699
happening in parallel and we wanted to

00:13:42,320 --> 00:13:45,139
take on the role of uniting the

00:13:43,699 --> 00:13:47,690
scattered efforts and bring that

00:13:45,139 --> 00:13:49,940
together for this we developed a

00:13:47,690 --> 00:13:52,310
centerpiece for integrating machine

00:13:49,940 --> 00:13:55,160
learning models and the application in

00:13:52,310 --> 00:13:58,550
the OSM ecosystem and this is called tml

00:13:55,160 --> 00:14:00,589
enabler it is basically a programming

00:13:58,550 --> 00:14:02,149
framework to include all different kinds

00:14:00,589 --> 00:14:05,930
of machine learning models and make them

00:14:02,149 --> 00:14:08,440
available through one consistent API we

00:14:05,930 --> 00:14:10,940
see this as a registry for ml models

00:14:08,440 --> 00:14:13,760
that can be used by the tasking manager

00:14:10,940 --> 00:14:16,399
but also other application in the OSM

00:14:13,760 --> 00:14:18,110
world it is open source it is an

00:14:16,399 --> 00:14:20,360
inclusive effort and everybody who is

00:14:18,110 --> 00:14:23,899
working as field we really encourage you

00:14:20,360 --> 00:14:28,670
to to join us and take part in in this

00:14:23,899 --> 00:14:31,490
because it makes things a lot easier to

00:14:28,670 --> 00:14:35,240
recap ml enabler integrates different

00:14:31,490 --> 00:14:37,250
models into it it supports different

00:14:35,240 --> 00:14:39,260
schematics i'm--if different schematics

00:14:37,250 --> 00:14:41,930
I mean it it doesn't have to be only

00:14:39,260 --> 00:14:44,240
geometry it can be basically everything

00:14:41,930 --> 00:14:46,819
it can be any kind of calculation

00:14:44,240 --> 00:14:49,910
statistics or numbers you get from ml

00:14:46,819 --> 00:14:52,100
derived data and when you have this data

00:14:49,910 --> 00:14:53,470
you probably want to augment it in a

00:14:52,100 --> 00:14:57,589
certain way

00:14:53,470 --> 00:14:59,600
augmenting this could mean to compare it

00:14:57,589 --> 00:15:01,310
with OpenStreetMap data to see what is

00:14:59,600 --> 00:15:05,500
the gap between the day that it came

00:15:01,310 --> 00:15:08,180
from ml and the one that is available or

00:15:05,500 --> 00:15:10,610
hitting a different source to get like

00:15:08,180 --> 00:15:11,700
additional information to the data that

00:15:10,610 --> 00:15:15,450
is present

00:15:11,700 --> 00:15:17,640
this is then available for consuming

00:15:15,450 --> 00:15:21,420
applications through one consistent API

00:15:17,640 --> 00:15:23,940
so instead of implementing one API for

00:15:21,420 --> 00:15:26,130
each of the models you can basically

00:15:23,940 --> 00:15:28,170
implement one and you get the whole

00:15:26,130 --> 00:15:31,770
boilerplate of models that are available

00:15:28,170 --> 00:15:35,340
in the ml enabler currently there are

00:15:31,770 --> 00:15:37,380
two ml sources in India I'm an enabler

00:15:35,340 --> 00:15:40,650
one is to Microsoft buildings that sir

00:15:37,380 --> 00:15:42,810
we talked about this is available for

00:15:40,650 --> 00:15:46,500
several countries in the world US Canada

00:15:42,810 --> 00:15:48,920
and now attend Sony and organda and the

00:15:46,500 --> 00:15:52,980
Looking Glass machine learning models

00:15:48,920 --> 00:15:54,690
from development seat that is free and

00:15:52,980 --> 00:15:57,600
open source software so you can actually

00:15:54,690 --> 00:16:01,440
use it yourself to train your models and

00:15:57,600 --> 00:16:04,290
to generate data and and play with it

00:16:01,440 --> 00:16:05,970
and really get your hands on the third

00:16:04,290 --> 00:16:09,420
one is on its way it's the Facebook

00:16:05,970 --> 00:16:12,810
roads and I hope that soon more models

00:16:09,420 --> 00:16:14,340
are going to be there please get in

00:16:12,810 --> 00:16:16,380
contact with us if you have any

00:16:14,340 --> 00:16:18,810
questions about that and you want to to

00:16:16,380 --> 00:16:21,180
bring your models in if you are a

00:16:18,810 --> 00:16:24,540
maintainer of an application in the OSM

00:16:21,180 --> 00:16:27,660
world and you want to include ml models

00:16:24,540 --> 00:16:33,060
consider implementing one MPI instead of

00:16:27,660 --> 00:16:34,830
one for each model the next step that we

00:16:33,060 --> 00:16:37,560
wanted to do is to see how can we

00:16:34,830 --> 00:16:41,760
actually use this in OpenStreetMap and

00:16:37,560 --> 00:16:43,890
the first use of ml derived data and the

00:16:41,760 --> 00:16:46,440
humanitarian OpenStreetMap team was this

00:16:43,890 --> 00:16:49,020
gap analysis that is available in OS m

00:16:46,440 --> 00:16:51,810
analytics and it's comparing buildings

00:16:49,020 --> 00:16:54,060
mapped in OS m and the ones detected by

00:16:51,810 --> 00:16:56,430
an open data set from the European

00:16:54,060 --> 00:17:00,080
Commission called the global human

00:16:56,430 --> 00:17:02,970
settlement layer that is made by

00:17:00,080 --> 00:17:04,770
symbolic machine learning techniques it

00:17:02,970 --> 00:17:07,590
shows in dark purple the probably

00:17:04,770 --> 00:17:10,890
missing areas where buildings need to be

00:17:07,590 --> 00:17:14,670
mapped and in green it shows two areas

00:17:10,890 --> 00:17:18,150
that we can we can expect that the data

00:17:14,670 --> 00:17:19,449
is more or less complete this gives

00:17:18,150 --> 00:17:21,250
everybody

00:17:19,449 --> 00:17:24,329
good and easy overview and it helps a

00:17:21,250 --> 00:17:27,250
lot in a coordination of mapping efforts

00:17:24,329 --> 00:17:29,440
in hot our main entrance for mapping is

00:17:27,250 --> 00:17:31,210
the tasking manager and this is a tool

00:17:29,440 --> 00:17:33,250
for coordination of volunteers and

00:17:31,210 --> 00:17:37,059
groups that want to map an open street

00:17:33,250 --> 00:17:39,549
map most of you probably know it just to

00:17:37,059 --> 00:17:42,610
quickly explain if it works the way that

00:17:39,549 --> 00:17:47,919
if there's an area that somebody wants

00:17:42,610 --> 00:17:49,570
to you data from you can like defined

00:17:47,919 --> 00:17:52,769
this area and tasking manager would

00:17:49,570 --> 00:17:55,690
split this up into tasks that then

00:17:52,769 --> 00:17:57,250
mappers can lock work on it and then

00:17:55,690 --> 00:18:01,389
later it's getting verified from

00:17:57,250 --> 00:18:03,850
experienced mappers for the ML

00:18:01,389 --> 00:18:05,500
integration and testing we set up a

00:18:03,850 --> 00:18:09,519
playground instance which we call the

00:18:05,500 --> 00:18:12,149
assisted task manager and this is and

00:18:09,519 --> 00:18:14,769
yeah and I tell you what we did there

00:18:12,149 --> 00:18:18,250
one of the challenges that we have in a

00:18:14,769 --> 00:18:20,350
tasking manager is to define a good task

00:18:18,250 --> 00:18:23,500
size because if it's too big it can be

00:18:20,350 --> 00:18:25,419
very frustrating for mappers if they

00:18:23,500 --> 00:18:29,380
don't finish it up in a good amount of

00:18:25,419 --> 00:18:31,059
time and it's if it's very small then it

00:18:29,380 --> 00:18:33,399
will it's a lot of extra work and

00:18:31,059 --> 00:18:35,380
overhead and actually to know what is a

00:18:33,399 --> 00:18:37,029
good task size you have to go into the

00:18:35,380 --> 00:18:41,440
map and you have to know what is going

00:18:37,029 --> 00:18:43,840
on there and this but with machine

00:18:41,440 --> 00:18:46,630
learning we could potentially use this

00:18:43,840 --> 00:18:49,240
data and let this to be our eyes on the

00:18:46,630 --> 00:18:51,490
map and give feedback of how complex

00:18:49,240 --> 00:18:55,120
this task and how much effort that is

00:18:51,490 --> 00:18:57,460
going to be to map it in dark red you

00:18:55,120 --> 00:19:00,039
see the ones that are probably too too

00:18:57,460 --> 00:19:02,139
high effort and in light color the ones

00:19:00,039 --> 00:19:05,380
that are too less and you want to get

00:19:02,139 --> 00:19:07,659
like a balance there so this is a

00:19:05,380 --> 00:19:11,200
similar gap analysis as an osm analytics

00:19:07,659 --> 00:19:13,929
and it and it helped us a lot to get

00:19:11,200 --> 00:19:17,409
project creation a little bit more

00:19:13,929 --> 00:19:19,840
efficiently and in a better quality the

00:19:17,409 --> 00:19:22,090
other showcase that we want to present

00:19:19,840 --> 00:19:24,639
to you is the open source idea extension

00:19:22,090 --> 00:19:26,559
from Facebook called rapid it allows

00:19:24,639 --> 00:19:28,960
previously generated data from machine

00:19:26,559 --> 00:19:30,370
learning models to be included into Open

00:19:28,960 --> 00:19:32,070
Street Map most-used

00:19:30,370 --> 00:19:34,200
editor ID

00:19:32,070 --> 00:19:36,960
piece by piece the mapper can decide

00:19:34,200 --> 00:19:39,749
whether to use or to discard each of the

00:19:36,960 --> 00:19:42,149
predictions and step by step they can

00:19:39,749 --> 00:19:45,029
correct the shape if necessary classify

00:19:42,149 --> 00:19:52,139
it with hours M tags and then save it to

00:19:45,029 --> 00:19:54,179
the OSM database the fourth building

00:19:52,139 --> 00:19:56,999
block was actually that we wanted to

00:19:54,179 --> 00:19:58,679
improve the user experience because it's

00:19:56,999 --> 00:20:00,779
important for us the tools are

00:19:58,679 --> 00:20:03,059
understandable to our users especially

00:20:00,779 --> 00:20:05,249
when introducing a disruptive new

00:20:03,059 --> 00:20:08,249
technology for this reason we had a

00:20:05,249 --> 00:20:10,529
deeper look analysis and user focus to

00:20:08,249 --> 00:20:13,019
redesign if you are interested in this

00:20:10,529 --> 00:20:15,899
we will present about that tomorrow at

00:20:13,019 --> 00:20:21,440
10:30 and now I'm handing it over to

00:20:15,899 --> 00:20:24,570
survey to talk about our next steps so

00:20:21,440 --> 00:20:25,919
we are I wouldn't even say that we are

00:20:24,570 --> 00:20:27,450
halfway through our journey like we

00:20:25,919 --> 00:20:29,940
understand there is a lot of challenges

00:20:27,450 --> 00:20:32,700
that we face when we started working in

00:20:29,940 --> 00:20:35,489
Africa and some of the ideas that we

00:20:32,700 --> 00:20:37,529
have are mostly around addressing those

00:20:35,489 --> 00:20:40,309
challenges that we face so we really

00:20:37,529 --> 00:20:43,440
think that the data once it's out there

00:20:40,309 --> 00:20:45,629
there will be there is a huge component

00:20:43,440 --> 00:20:48,509
of validation on that data and we want

00:20:45,629 --> 00:20:50,879
to make this virtuous cycle where any

00:20:48,509 --> 00:20:53,070
feedback that comes on that data gets

00:20:50,879 --> 00:20:55,919
looped in to retraining the model and

00:20:53,070 --> 00:20:57,479
have that virtuous cycle going the other

00:20:55,919 --> 00:20:59,729
issue that we want to talk from a

00:20:57,479 --> 00:21:01,080
technical perspective is deploying some

00:20:59,729 --> 00:21:03,599
of the technologies and experimenting

00:21:01,080 --> 00:21:05,399
with them in figuring out when we move

00:21:03,599 --> 00:21:07,379
from one country to another one

00:21:05,399 --> 00:21:11,940
landscape to another how do we eliminate

00:21:07,379 --> 00:21:13,349
or at least reduce the need for more and

00:21:11,940 --> 00:21:16,259
more targeted training data because that

00:21:13,349 --> 00:21:18,629
is very expensive to create the other

00:21:16,259 --> 00:21:21,840
two two aspects that kind of go

00:21:18,629 --> 00:21:25,289
hand-in-hand is how do we tap into the

00:21:21,840 --> 00:21:28,169
really rich and really good quality data

00:21:25,289 --> 00:21:30,690
that exists in OS m and and figure out

00:21:28,169 --> 00:21:35,140
how to really tap into the value of it

00:21:30,690 --> 00:21:38,260
with the corresponding imagery to scale

00:21:35,140 --> 00:21:40,450
the and improve the quality of the

00:21:38,260 --> 00:21:44,559
output and then just to have

00:21:40,450 --> 00:21:46,480
conversations around how the AI could be

00:21:44,559 --> 00:21:47,700
helpful in the context of this and just

00:21:46,480 --> 00:21:51,760
continue having those conversations

00:21:47,700 --> 00:21:53,260
engage with the community lastly I just

00:21:51,760 --> 00:21:55,210
want to introduce you guys to some of

00:21:53,260 --> 00:21:58,029
the resources that are out there if you

00:21:55,210 --> 00:22:01,720
want it to go play out with tasks

00:21:58,029 --> 00:22:05,440
instead instances that we showed you you

00:22:01,720 --> 00:22:06,760
can go test the demos there there isn't

00:22:05,440 --> 00:22:10,600
for more information about the project

00:22:06,760 --> 00:22:13,419
and how you could a cage with us with

00:22:10,600 --> 00:22:19,920
that want to open it up to any questions

00:22:13,419 --> 00:22:23,279
I can leave this one here

00:22:19,920 --> 00:22:23,279
[Applause]

00:22:27,850 --> 00:22:35,960
and thanks Phillips and Sylvie with an

00:22:32,200 --> 00:22:42,999
informative presentation and we'll have

00:22:35,960 --> 00:22:42,999
five minutes questions at the back there

00:22:52,510 --> 00:22:56,970
[Music]

00:22:54,140 --> 00:22:59,310
thank you for the the work in Tanzania

00:22:56,970 --> 00:23:02,360
in previous efforts that I've seen in

00:22:59,310 --> 00:23:05,010
Tanzania they did the work was

00:23:02,360 --> 00:23:07,770
specifically excluding shipping

00:23:05,010 --> 00:23:09,900
containers in addition to the round

00:23:07,770 --> 00:23:13,560
buildings did you address the issue of

00:23:09,900 --> 00:23:16,470
shipping containers should not show up

00:23:13,560 --> 00:23:19,530
as a building I wouldn't say we have

00:23:16,470 --> 00:23:23,430
because if you saw the recall that we

00:23:19,530 --> 00:23:26,420
have I we're at 60% so we are missing

00:23:23,430 --> 00:23:28,620
some of the building still we didn't

00:23:26,420 --> 00:23:30,990
categorically exclude some of the

00:23:28,620 --> 00:23:32,880
buildings but we know that the training

00:23:30,990 --> 00:23:34,920
data was is still not representative of

00:23:32,880 --> 00:23:36,570
the entire mass so that's that's where

00:23:34,920 --> 00:23:37,980
we want to factor in the whole feedback

00:23:36,570 --> 00:23:40,470
loop we really believe that that'll

00:23:37,980 --> 00:23:42,650
uncover some of the value that's still

00:23:40,470 --> 00:23:42,650
out there

00:23:52,720 --> 00:23:59,870
have you built one Global Learning modem

00:23:56,690 --> 00:24:01,309
or do you have local models for

00:23:59,870 --> 00:24:04,370
different regions with different

00:24:01,309 --> 00:24:06,530
architectures for example so the

00:24:04,370 --> 00:24:11,210
question is do we have a global model or

00:24:06,530 --> 00:24:15,410
do we have a local that's something we

00:24:11,210 --> 00:24:17,360
aspire for very much but it's not what

00:24:15,410 --> 00:24:19,490
we're finding is that it's not that easy

00:24:17,360 --> 00:24:22,630
and simple because every time we have

00:24:19,490 --> 00:24:26,500
moved from u.s. to Canada Canada to

00:24:22,630 --> 00:24:30,260
Africa and Uganda Tanzania as well it's

00:24:26,500 --> 00:24:31,910
it's every time we switch the input it's

00:24:30,260 --> 00:24:35,210
really hard and you acquire all that

00:24:31,910 --> 00:24:37,700
training data but we do think that with

00:24:35,210 --> 00:24:39,200
some new approaches like the domain

00:24:37,700 --> 00:24:43,040
adaptation technologies that we're gonna

00:24:39,200 --> 00:24:44,990
evaluate and if those to figure out how

00:24:43,040 --> 00:24:47,390
we could reduce the need for training

00:24:44,990 --> 00:24:49,220
data every time we do that switch we

00:24:47,390 --> 00:24:56,240
believe could allow us to scale even

00:24:49,220 --> 00:24:58,100
more hopefully globally yeah I have two

00:24:56,240 --> 00:25:00,260
questions the first one is related to

00:24:58,100 --> 00:25:02,120
data it's like which were the common

00:25:00,260 --> 00:25:03,620
problems who found out their data said

00:25:02,120 --> 00:25:05,990
in order to be discarded from training

00:25:03,620 --> 00:25:07,940
data and send it to the machine learning

00:25:05,990 --> 00:25:10,280
model and the second one was related to

00:25:07,940 --> 00:25:11,780
the quality control because there was

00:25:10,280 --> 00:25:14,450
the Cairo say that there was like a

00:25:11,780 --> 00:25:16,669
binary criteria so which was that

00:25:14,450 --> 00:25:19,179
criteria is like the buildings were okay

00:25:16,669 --> 00:25:21,410
or the number of buildings were found

00:25:19,179 --> 00:25:24,169
could you repeat your first question

00:25:21,410 --> 00:25:26,480
okay which was which were the common

00:25:24,169 --> 00:25:28,340
problems found in the data set for the

00:25:26,480 --> 00:25:32,540
training data on the yeah on their

00:25:28,340 --> 00:25:37,130
building services yeah so the problems

00:25:32,540 --> 00:25:39,320
in training data set were so treated I

00:25:37,130 --> 00:25:42,080
said was when we started we started with

00:25:39,320 --> 00:25:46,640
dar islam and dar Salam is very it's

00:25:42,080 --> 00:25:49,760
comparatively very urban and when we

00:25:46,640 --> 00:25:53,570
started using that and running the model

00:25:49,760 --> 00:25:56,120
on rural areas that it did not catch the

00:25:53,570 --> 00:25:58,810
buildings in earlier and so recall was

00:25:56,120 --> 00:26:01,630
really low the other

00:25:58,810 --> 00:26:04,030
so that there was a diversity issue in

00:26:01,630 --> 00:26:05,670
the training area that we ran into some

00:26:04,030 --> 00:26:09,250
of the other ones that I mentioned were

00:26:05,670 --> 00:26:13,630
more related to being imagery so the

00:26:09,250 --> 00:26:16,630
data that we had from from hot partners

00:26:13,630 --> 00:26:19,390
and osm did not align to being imagery

00:26:16,630 --> 00:26:21,280
so that's where the pair thing comes

00:26:19,390 --> 00:26:24,220
into play that you need to have the

00:26:21,280 --> 00:26:25,870
imagery but also the the you need to

00:26:24,220 --> 00:26:28,060
have the true label data but also the

00:26:25,870 --> 00:26:33,640
corresponding imagery to go together so

00:26:28,060 --> 00:26:36,370
that's another issue that we faced it

00:26:33,640 --> 00:26:38,590
looks like Adam is over but I think you

00:26:36,370 --> 00:26:40,960
will find the true presenters in the

00:26:38,590 --> 00:26:43,180
foyer in the breakout sessions for

00:26:40,960 --> 00:26:46,120
further questions thanks everybody for

00:26:43,180 --> 00:26:48,300
attending I feel nice time tonight thank

00:26:46,120 --> 00:26:48,300

YouTube URL: https://www.youtube.com/watch?v=UniTGbXooCI


