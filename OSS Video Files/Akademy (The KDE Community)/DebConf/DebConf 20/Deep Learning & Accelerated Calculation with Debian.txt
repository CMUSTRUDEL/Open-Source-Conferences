Title: Deep Learning & Accelerated Calculation with Debian
Publication date: 2020-10-23
Playlist: DebConf 20
Description: 
	by Mo Zhou

At: DebConf20
https://debconf20.debconf.org/talks/23-deep-learning-accelerated-calculation-with-debian/

Artificial Intelligence is a world-wide trend of technology development, where
deep learning manifests its promising power on solving problems impossible for
traditional algorithms to solve. Yet powerful, the artificial intelligent software
deeply rely on hardware computational capability and software optimization. Thus,
the artificial intelligence software are highly related with computation acceleration.

This talk will focus on summarizing the current status of deep learning framework
packaging in Debian, and that of the computation acceleration libraries (e.g. BLAS/LAPACK
and ROCm). The existing problems and challenges will also be summarized.

I'm not physically attending the conference. So this will be a pre-recorded video.

Room: Talks
Scheduled start: 2020-08-25 10:30:00
Captions: 
	00:00:10,960 --> 00:00:13,920
hi

00:00:11,360 --> 00:00:14,799
everyone today i'm going to talk about

00:00:13,920 --> 00:00:18,960
deep learning

00:00:14,799 --> 00:00:21,920
and accelerated calculation with debian

00:00:18,960 --> 00:00:22,480
so what is deep learning in the recent

00:00:21,920 --> 00:00:24,720
years

00:00:22,480 --> 00:00:26,000
we have seen a trend of technology

00:00:24,720 --> 00:00:29,039
development about

00:00:26,000 --> 00:00:32,559
artificial intelligence for example the

00:00:29,039 --> 00:00:35,680
emergence of alphago an artificial agent

00:00:32,559 --> 00:00:36,320
that is able to play chess apart from

00:00:35,680 --> 00:00:39,520
that

00:00:36,320 --> 00:00:40,079
new technologies about autopilot also

00:00:39,520 --> 00:00:43,360
appeared

00:00:40,079 --> 00:00:45,600
for for the vehicles

00:00:43,360 --> 00:00:47,280
and actually deep learning is a key part

00:00:45,600 --> 00:00:50,480
of the state-of-the-art

00:00:47,280 --> 00:00:50,879
artificial intelligence you can see when

00:00:50,480 --> 00:00:53,360
we

00:00:50,879 --> 00:00:55,760
talk about deep learning it is basically

00:00:53,360 --> 00:00:58,000
about deep neural networks

00:00:55,760 --> 00:00:58,960
the most typical application of deep

00:00:58,000 --> 00:01:02,559
neural networks

00:00:58,960 --> 00:01:06,080
is classification for example

00:01:02,559 --> 00:01:08,240
we have an image presenting a cat we use

00:01:06,080 --> 00:01:09,760
this image as the input to the deep

00:01:08,240 --> 00:01:12,799
neural network

00:01:09,760 --> 00:01:16,000
and the network will try to crunch

00:01:12,799 --> 00:01:19,040
many numbers do lots of mathematical

00:01:16,000 --> 00:01:21,680
operations and calculate the

00:01:19,040 --> 00:01:26,240
probabilities for a predefined

00:01:21,680 --> 00:01:27,040
list of categories in our case the

00:01:26,240 --> 00:01:30,320
category

00:01:27,040 --> 00:01:34,159
cat has a highest probability

00:01:30,320 --> 00:01:38,159
so our input image is classified

00:01:34,159 --> 00:01:41,439
as the cat now let's take a closer look

00:01:38,159 --> 00:01:44,079
to the internal of neural networks

00:01:41,439 --> 00:01:45,040
a basic deep neural network can be

00:01:44,079 --> 00:01:47,600
decomposed

00:01:45,040 --> 00:01:48,399
into a series of network layers

00:01:47,600 --> 00:01:51,360
organized

00:01:48,399 --> 00:01:53,520
in a sequential pipeline where the

00:01:51,360 --> 00:01:57,360
output of the previous layer

00:01:53,520 --> 00:02:00,399
is used as the input of the next layer

00:01:57,360 --> 00:02:02,880
the most frequently used network layers

00:02:00,399 --> 00:02:04,399
include the convolution layer which is

00:02:02,880 --> 00:02:07,759
common to networks

00:02:04,399 --> 00:02:09,599
that take image as the input and the

00:02:07,759 --> 00:02:12,879
linear layer which

00:02:09,599 --> 00:02:15,440
performs a fine transformation

00:02:12,879 --> 00:02:16,560
and the activation layer which

00:02:15,440 --> 00:02:19,200
introduces

00:02:16,560 --> 00:02:19,760
a non-linearity to the neural network

00:02:19,200 --> 00:02:22,400
and

00:02:19,760 --> 00:02:24,239
the pulling layer which introduces

00:02:22,400 --> 00:02:28,959
translation environs to the

00:02:24,239 --> 00:02:30,000
deep neural network when we try to use a

00:02:28,959 --> 00:02:32,959
neural network

00:02:30,000 --> 00:02:34,400
we can immediately identify some

00:02:32,959 --> 00:02:36,800
performance bottlenecks

00:02:34,400 --> 00:02:38,640
for example the linear layer and the

00:02:36,800 --> 00:02:41,040
convolution layer

00:02:38,640 --> 00:02:42,720
in the linear layer the core

00:02:41,040 --> 00:02:46,160
mathematical operation

00:02:42,720 --> 00:02:48,640
is general matrix multiplication while

00:02:46,160 --> 00:02:50,800
in the convolution layer the quorum

00:02:48,640 --> 00:02:53,680
operation is convolution

00:02:50,800 --> 00:02:54,480
but the convolution itself is

00:02:53,680 --> 00:02:57,920
essentially

00:02:54,480 --> 00:03:00,239
a linear operation so it can be seen

00:02:57,920 --> 00:03:02,400
as a special kind of matrix

00:03:00,239 --> 00:03:05,360
multiplication

00:03:02,400 --> 00:03:06,239
so how do we mitigate such performance

00:03:05,360 --> 00:03:09,200
bottlenecks

00:03:06,239 --> 00:03:11,120
and accelerate the calculation the

00:03:09,200 --> 00:03:14,440
solutions can be categorized

00:03:11,120 --> 00:03:18,400
into three groups the first one is

00:03:14,440 --> 00:03:21,760
parallelization for example openmp

00:03:18,400 --> 00:03:24,400
which spawns a number of threads in

00:03:21,760 --> 00:03:26,000
at the runtime to calculate the matrix

00:03:24,400 --> 00:03:29,120
in parallel

00:03:26,000 --> 00:03:32,799
the second group is some techniques for

00:03:29,120 --> 00:03:33,519
optimizing the cache access in order to

00:03:32,799 --> 00:03:36,640
reduce

00:03:33,519 --> 00:03:38,560
the rate of cash misses

00:03:36,640 --> 00:03:40,239
the third group is hardware

00:03:38,560 --> 00:03:44,840
accelerations and

00:03:40,239 --> 00:03:46,159
there are many available hardware

00:03:44,840 --> 00:03:48,239
solutions

00:03:46,159 --> 00:03:50,000
let's look at the first one single

00:03:48,239 --> 00:03:52,480
instruction multiple data

00:03:50,000 --> 00:03:54,159
namely the vectorized instruction set

00:03:52,480 --> 00:03:57,760
provided by the cpu

00:03:54,159 --> 00:04:01,280
for example the abx 2 instruction set

00:03:57,760 --> 00:04:02,959
however even if we leverage all the cpu

00:04:01,280 --> 00:04:05,599
capabilities

00:04:02,959 --> 00:04:06,319
the floating point performance is still

00:04:05,599 --> 00:04:09,920
far not

00:04:06,319 --> 00:04:13,280
enough for neural network applications

00:04:09,920 --> 00:04:14,319
the next one is opencl it is a feasible

00:04:13,280 --> 00:04:17,600
solution but

00:04:14,319 --> 00:04:18,079
opencl has a complicated programming

00:04:17,600 --> 00:04:20,479
model

00:04:18,079 --> 00:04:22,800
and lacks support from the machine

00:04:20,479 --> 00:04:25,919
learning communities

00:04:22,800 --> 00:04:28,720
in contrast nvidia cuda is the most

00:04:25,919 --> 00:04:29,520
mature solution and it's it is most

00:04:28,720 --> 00:04:31,360
widely

00:04:29,520 --> 00:04:32,880
adopted by the machine learning

00:04:31,360 --> 00:04:37,040
community but

00:04:32,880 --> 00:04:39,919
it is proprietary to compete with nvidia

00:04:37,040 --> 00:04:40,320
md proposed its own kuda counterpart

00:04:39,919 --> 00:04:43,600
named

00:04:40,320 --> 00:04:46,560
jocam the software stack is open sourced

00:04:43,600 --> 00:04:46,880
and but it is still not mature enough

00:04:46,560 --> 00:04:50,000
like

00:04:46,880 --> 00:04:51,600
cuda intel has recently announced their

00:04:50,000 --> 00:04:54,080
plans for the discrete

00:04:51,600 --> 00:04:56,479
graphics card and the software support

00:04:54,080 --> 00:04:59,520
were based on sql which is a

00:04:56,479 --> 00:05:01,919
high level programming model for opencl

00:04:59,520 --> 00:05:04,160
the software stack is also open source

00:05:01,919 --> 00:05:06,720
and not mature enough

00:05:04,160 --> 00:05:08,240
there are also other types of hardware

00:05:06,720 --> 00:05:11,199
acceleration solutions

00:05:08,240 --> 00:05:12,400
such as fpga but i'm not quite familiar

00:05:11,199 --> 00:05:14,639
with them

00:05:12,400 --> 00:05:15,520
given this background we still have lots

00:05:14,639 --> 00:05:18,080
of work to do

00:05:15,520 --> 00:05:18,880
to integrate these libraries and

00:05:18,080 --> 00:05:22,800
applications

00:05:18,880 --> 00:05:24,160
into debian actually i participated in

00:05:22,800 --> 00:05:26,639
all the works related

00:05:24,160 --> 00:05:27,520
to deep learning and hardware

00:05:26,639 --> 00:05:31,120
acceleration

00:05:27,520 --> 00:05:32,880
in debian and i divide these works into

00:05:31,120 --> 00:05:36,240
four aspects

00:05:32,880 --> 00:05:37,759
the first aspect is the maintenance of

00:05:36,240 --> 00:05:41,199
performance libraries

00:05:37,759 --> 00:05:43,840
and introducing new ones for example the

00:05:41,199 --> 00:05:45,840
the fundamental mathematical libraries

00:05:43,840 --> 00:05:47,440
including the blast and lab hack

00:05:45,840 --> 00:05:49,520
libraries

00:05:47,440 --> 00:05:51,840
the second aspect is deep learning

00:05:49,520 --> 00:05:53,280
frameworks for example the tensorflow

00:05:51,840 --> 00:05:55,360
and pytorch

00:05:53,280 --> 00:05:57,919
you know the top two they are the top

00:05:55,360 --> 00:06:01,440
two deep learning frameworks

00:05:57,919 --> 00:06:03,680
the third aspect is amd raw cam

00:06:01,440 --> 00:06:06,319
it is the free and open source

00:06:03,680 --> 00:06:09,440
counterpart to nvidia's cuda

00:06:06,319 --> 00:06:11,360
and i think supporting amd raw cam from

00:06:09,440 --> 00:06:13,199
debian is beneficial to the free

00:06:11,360 --> 00:06:16,160
software community

00:06:13,199 --> 00:06:16,960
the last aspect is intel sql it is

00:06:16,160 --> 00:06:20,160
actually an

00:06:16,960 --> 00:06:23,199
additional module to the llvm project

00:06:20,160 --> 00:06:25,520
but intel still has not yet upstreamed

00:06:23,199 --> 00:06:28,639
their code to llvm project

00:06:25,520 --> 00:06:31,120
so i'm still waiting next

00:06:28,639 --> 00:06:32,720
i'll give you a brief summary about

00:06:31,120 --> 00:06:34,960
packaging works

00:06:32,720 --> 00:06:36,080
for the performance libraries deep

00:06:34,960 --> 00:06:39,360
learning frameworks

00:06:36,080 --> 00:06:42,000
and amd raw cam

00:06:39,360 --> 00:06:43,919
generally there is nothing special for

00:06:42,000 --> 00:06:46,639
the packaging work for the

00:06:43,919 --> 00:06:49,039
performance libraries but plus and

00:06:46,639 --> 00:06:52,319
lapack is an exception

00:06:49,039 --> 00:06:52,720
the two libraries implemented a number

00:06:52,319 --> 00:06:54,960
of

00:06:52,720 --> 00:06:55,919
very fundamental numerical linear

00:06:54,960 --> 00:06:58,960
algebra

00:06:55,919 --> 00:06:59,599
subroutines the corresponding shared

00:06:58,960 --> 00:07:03,199
objects

00:06:59,599 --> 00:07:05,680
are the live blocks and live live hack

00:07:03,199 --> 00:07:06,319
actually there are two virtual shared

00:07:05,680 --> 00:07:08,479
objects

00:07:06,319 --> 00:07:10,000
managed by the update alternatives

00:07:08,479 --> 00:07:12,960
mechanism

00:07:10,000 --> 00:07:13,680
the fastest open source implementation

00:07:12,960 --> 00:07:16,960
of plus

00:07:13,680 --> 00:07:19,440
is open blast but actually

00:07:16,960 --> 00:07:20,880
intel's math kernel library is faster

00:07:19,440 --> 00:07:24,000
than open blocks but it

00:07:20,880 --> 00:07:25,599
but it is not free so i registered intel

00:07:24,000 --> 00:07:28,840
mkl

00:07:25,599 --> 00:07:30,720
in the mechanism with the lowest

00:07:28,840 --> 00:07:33,199
priority likewise

00:07:30,720 --> 00:07:34,880
the recommended implementation of live

00:07:33,199 --> 00:07:37,440
lab pack in debian is

00:07:34,880 --> 00:07:38,319
also open blast but actually there is a

00:07:37,440 --> 00:07:40,400
faster

00:07:38,319 --> 00:07:41,360
free software implementation named

00:07:40,400 --> 00:07:44,800
landflame

00:07:41,360 --> 00:07:47,440
and currently i'm working on it so

00:07:44,800 --> 00:07:48,639
the two linear algebra libraries blast

00:07:47,440 --> 00:07:50,160
and lab pack

00:07:48,639 --> 00:07:52,720
they have some notable reverse

00:07:50,160 --> 00:07:56,080
dependencies such as gnu octave

00:07:52,720 --> 00:07:58,560
numpy and scipy that means

00:07:56,080 --> 00:08:01,199
there are two very important libraries

00:07:58,560 --> 00:08:04,160
and their performance can greatly impact

00:08:01,199 --> 00:08:06,160
the productivity of scientific computing

00:08:04,160 --> 00:08:08,800
users

00:08:06,160 --> 00:08:09,440
okay deep learning frameworks currently

00:08:08,800 --> 00:08:11,440
the

00:08:09,440 --> 00:08:13,919
top two deep learning frameworks are

00:08:11,440 --> 00:08:17,120
tensorflow and pytorch

00:08:13,919 --> 00:08:20,560
the blocker for tensorflow packaging is

00:08:17,120 --> 00:08:22,960
bazel a build system written in java

00:08:20,560 --> 00:08:23,599
it is currently under heavy development

00:08:22,960 --> 00:08:26,879
and this

00:08:23,599 --> 00:08:28,000
is not easy a preliminary packaging

00:08:26,879 --> 00:08:31,840
based on bezel

00:08:28,000 --> 00:08:34,320
is available on sales on the other hand

00:08:31,840 --> 00:08:35,440
the packaging process of pytorch is a

00:08:34,320 --> 00:08:37,599
good news

00:08:35,440 --> 00:08:38,640
the dependency libraries are already

00:08:37,599 --> 00:08:41,919
queued in new

00:08:38,640 --> 00:08:45,040
and pytorch itself is also new

00:08:41,919 --> 00:08:47,760
but i have to point out that i have no

00:08:45,040 --> 00:08:49,600
plan for the cuda version of pytorch

00:08:47,760 --> 00:08:51,120
but i have planned for the raw cam

00:08:49,600 --> 00:08:53,360
version once

00:08:51,120 --> 00:08:54,560
the jocam compiler is available in

00:08:53,360 --> 00:08:56,640
debian

00:08:54,560 --> 00:08:58,959
in the debian archive there are also

00:08:56,640 --> 00:08:59,839
some other deployment frameworks such as

00:08:58,959 --> 00:09:03,279
cafe

00:08:59,839 --> 00:09:03,760
but nowadays i personally regard a cafe

00:09:03,279 --> 00:09:06,880
as an

00:09:03,760 --> 00:09:09,279
educational code base apart from this

00:09:06,880 --> 00:09:12,160
we also have some plans for other

00:09:09,279 --> 00:09:14,880
deployment frameworks such as mxnet

00:09:12,160 --> 00:09:17,040
if you are interested in this you can

00:09:14,880 --> 00:09:18,640
browse the salsa page of debian deep

00:09:17,040 --> 00:09:21,760
learning team

00:09:18,640 --> 00:09:24,399
next is amd rawcan the software stack

00:09:21,760 --> 00:09:26,320
is open source but they depend on the

00:09:24,399 --> 00:09:28,640
non-free firmware

00:09:26,320 --> 00:09:29,680
anyway at least the kernel module is

00:09:28,640 --> 00:09:32,720
already present

00:09:29,680 --> 00:09:33,839
in the mainline kernel and it is already

00:09:32,720 --> 00:09:36,320
usable

00:09:33,839 --> 00:09:37,120
however apart from the kernel package

00:09:36,320 --> 00:09:39,120
the

00:09:37,120 --> 00:09:40,959
packaging work for the rawcam software

00:09:39,120 --> 00:09:43,839
stack is literally

00:09:40,959 --> 00:09:45,279
from scratch so it is still under heavy

00:09:43,839 --> 00:09:47,760
development

00:09:45,279 --> 00:09:48,560
if you are interested in the packaging

00:09:47,760 --> 00:09:51,200
details

00:09:48,560 --> 00:09:52,480
you can browse the salsa page of debian

00:09:51,200 --> 00:09:55,120
rook mt

00:09:52,480 --> 00:09:57,360
that is a brief summary of debian works

00:09:55,120 --> 00:09:59,519
related to deep learning and hardware

00:09:57,360 --> 00:10:01,279
acceleration apart from that there are

00:09:59,519 --> 00:10:04,160
also some interesting issues

00:10:01,279 --> 00:10:05,600
for example can we better leverage the

00:10:04,160 --> 00:10:09,120
sim the instruction set

00:10:05,600 --> 00:10:12,399
system-wide in the past i proposed

00:10:09,120 --> 00:10:16,160
a project named simdabian

00:10:12,399 --> 00:10:19,519
it aims to bump the isa baseline

00:10:16,160 --> 00:10:22,240
in the system default build flags but

00:10:19,519 --> 00:10:24,560
currently this project is stored because

00:10:22,240 --> 00:10:25,040
we don't have a definite conclusion

00:10:24,560 --> 00:10:27,600
about

00:10:25,040 --> 00:10:28,640
the performance gain from the

00:10:27,600 --> 00:10:32,320
system-wide

00:10:28,640 --> 00:10:34,160
build flags another example is the

00:10:32,320 --> 00:10:35,680
software freedom issues in the deep

00:10:34,160 --> 00:10:37,519
learning practice

00:10:35,680 --> 00:10:38,720
we had lots of discussions in the

00:10:37,519 --> 00:10:41,040
mailing list and

00:10:38,720 --> 00:10:42,640
eventually i drafted a machine learning

00:10:41,040 --> 00:10:44,880
policy

00:10:42,640 --> 00:10:46,000
it aims to sort out the issues between

00:10:44,880 --> 00:10:49,760
the machine learning

00:10:46,000 --> 00:10:52,880
practice and software freedom this

00:10:49,760 --> 00:10:56,399
document project is stable currently

00:10:52,880 --> 00:10:58,800
but it is not official

00:10:56,399 --> 00:11:00,079
okay thank you for listening i hope the

00:10:58,800 --> 00:11:03,440
information presented

00:11:00,079 --> 00:11:06,240
in this talk is helpful

00:11:03,440 --> 00:11:07,839
um the first question i mean this has

00:11:06,240 --> 00:11:10,000
been answered already in the etherpet

00:11:07,839 --> 00:11:13,120
but maybe for everybody's

00:11:10,000 --> 00:11:15,040
um yeah for everybody is rock a

00:11:13,120 --> 00:11:18,320
or however it's called packaged and if

00:11:15,040 --> 00:11:18,320
not would it be hard to do

00:11:25,600 --> 00:11:29,839
you're still muted

00:11:31,120 --> 00:11:35,680
okay let's start the q a session

00:11:39,360 --> 00:11:42,959
well there are only three

00:11:44,980 --> 00:11:48,039
[Music]

00:11:54,480 --> 00:11:58,240
uh yes i actually i have already

00:11:56,880 --> 00:12:02,079
answered him

00:11:58,240 --> 00:12:03,680
i'm the uh i'm looming so i have already

00:12:02,079 --> 00:12:07,680
answered him

00:12:03,680 --> 00:12:09,519
yeah hello right i hear you but i think

00:12:07,680 --> 00:12:14,079
there's a huge delay

00:12:09,519 --> 00:12:14,079
um so

00:12:15,040 --> 00:12:20,240
yes well if you answered all the

00:12:17,519 --> 00:12:23,279
questions already on the ethernet then

00:12:20,240 --> 00:12:24,000
i mean maybe you can just rephrase your

00:12:23,279 --> 00:12:27,680
answer

00:12:24,000 --> 00:12:29,600
so i guess um

00:12:27,680 --> 00:12:31,839
you should there was one mentioning

00:12:29,600 --> 00:12:32,240
about bazel so what's the current status

00:12:31,839 --> 00:12:36,000
and

00:12:32,240 --> 00:12:36,000
um can you can you comment on

00:12:36,959 --> 00:12:44,560
i'm that so i have already

00:12:40,560 --> 00:12:46,000
yeah uh right i need you but i think

00:12:44,560 --> 00:12:50,480
there's a huge delay

00:12:46,000 --> 00:12:50,480
yeah yeah i think there is some weird

00:12:50,839 --> 00:12:54,800
delay yes um

00:12:53,360 --> 00:12:57,120
well if you answered all the questions

00:12:54,800 --> 00:12:59,839
already on the internet then

00:12:57,120 --> 00:13:00,880
i mean maybe you can just um rephrase

00:12:59,839 --> 00:13:04,639
your answer

00:13:00,880 --> 00:13:06,480
so i guess um

00:13:04,639 --> 00:13:08,800
you should there was one mentioning

00:13:06,480 --> 00:13:12,399
about bazel so what's the current status

00:13:08,800 --> 00:13:15,920
and um can you remember that

00:13:12,399 --> 00:13:15,920
okay i'm writing the answer

00:13:18,839 --> 00:13:21,839
uh

00:13:30,639 --> 00:13:36,079
okay so um it may be the point

00:13:34,320 --> 00:13:37,839
the problem that lumen is not hearing me

00:13:36,079 --> 00:13:40,160
via jitsi but only via the stream so

00:13:37,839 --> 00:13:43,920
there's a huge delay here going on

00:13:40,160 --> 00:13:47,040
um so i don't see

00:13:43,920 --> 00:13:48,959
any um obvious questions that haven't

00:13:47,040 --> 00:13:51,760
been answered but we do have

00:13:48,959 --> 00:13:53,600
quite a lot of time left another five

00:13:51,760 --> 00:13:54,720
minutes so if you want to ask some more

00:13:53,600 --> 00:13:58,160
questions

00:13:54,720 --> 00:14:01,040
please write them on the other pad and

00:13:58,160 --> 00:14:03,040
we can try to answer them so i'll leave

00:14:01,040 --> 00:14:17,839
this open for a couple of seconds

00:14:03,040 --> 00:14:17,839
for further questions

00:14:42,839 --> 00:14:45,839
okay

00:14:46,639 --> 00:14:52,880
so we have um

00:14:49,680 --> 00:14:54,720
we have one more question which is uh

00:14:52,880 --> 00:14:56,320
what was the other library that's faster

00:14:54,720 --> 00:14:59,839
than open blouse

00:14:56,320 --> 00:14:59,839
and still work in progress

00:18:59,360 --> 00:19:03,120
okay so we are slowly running out of

00:19:02,160 --> 00:19:05,440
time

00:19:03,120 --> 00:19:08,240
the there was a levy discussion on the

00:19:05,440 --> 00:19:12,000
other pet and the discussion can go on

00:19:08,240 --> 00:19:15,120
um after this talk um i would like to

00:19:12,000 --> 00:19:17,840
thank mojo again for the talk

00:19:15,120 --> 00:19:18,559
and um the next talk will be in 10

00:19:17,840 --> 00:19:25,840
minutes

00:19:18,559 --> 00:19:25,840
so everybody see you bye bye

00:19:27,360 --> 00:19:29,440

YouTube URL: https://www.youtube.com/watch?v=VzkNKounPZY


