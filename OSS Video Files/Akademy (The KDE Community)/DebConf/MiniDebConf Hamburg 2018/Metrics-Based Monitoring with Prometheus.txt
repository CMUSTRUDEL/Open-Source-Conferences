Title: Metrics-Based Monitoring with Prometheus
Publication date: 2018-05-27
Playlist: MiniDebConf Hamburg 2018
Description: 
	by Ben Kochie

At: MiniDebConf Hamburg
https://wiki.debian.org/DebianEvents/de/2018/MiniDebConfHamburg
Room: main
Scheduled start: 2018-05-19 12:00:00
Captions: 
	00:00:04,900 --> 00:00:11,660
so we're talk by a non gala person and

00:00:09,650 --> 00:00:14,180
get lap now we have a talk but I get let

00:00:11,660 --> 00:00:18,650
person on on gift lap or something like

00:00:14,180 --> 00:00:20,930
that um the the ccchhh hackerspace is

00:00:18,650 --> 00:00:23,300
now open or from now on if you want to

00:00:20,930 --> 00:00:25,790
go there that's the announcement and the

00:00:23,300 --> 00:00:28,000
next truck will be my Ben Kochi or

00:00:25,790 --> 00:00:31,000
metric space monitoring with Prometheus

00:00:28,000 --> 00:00:31,000
welcome

00:00:31,230 --> 00:00:35,109
[Applause]

00:01:38,770 --> 00:01:44,429
ah there we go that's better all right

00:01:44,789 --> 00:01:52,929
all right so so blackbox monitoring is a

00:01:51,130 --> 00:01:56,140
probe it just kind of looks from the

00:01:52,929 --> 00:01:58,689
outside to your software and it has no

00:01:56,140 --> 00:02:00,369
knowledge the internals and but and its

00:01:58,689 --> 00:02:02,770
really good for end-to-end testing so if

00:02:00,369 --> 00:02:05,200
you've got a fairly complicated service

00:02:02,770 --> 00:02:06,909
you come in from the outside you go

00:02:05,200 --> 00:02:08,739
through the load balancer you hit the

00:02:06,909 --> 00:02:11,530
API server the API server might hit a

00:02:08,739 --> 00:02:12,849
database and you go all the way through

00:02:11,530 --> 00:02:14,170
to the back of the stack and then all

00:02:12,849 --> 00:02:16,209
the way back outs you know that

00:02:14,170 --> 00:02:18,340
everything is working and to end but you

00:02:16,209 --> 00:02:21,459
only know about it for that one request

00:02:18,340 --> 00:02:25,959
so in order to find out if your service

00:02:21,459 --> 00:02:26,860
is working from the end end for every

00:02:25,959 --> 00:02:29,140
single request

00:02:26,860 --> 00:02:32,410
this requires white box instrumentation

00:02:29,140 --> 00:02:34,209
so basically every every event that

00:02:32,410 --> 00:02:39,160
happens inside your software inside a

00:02:34,209 --> 00:02:41,769
serving stack gets collected and it gets

00:02:39,160 --> 00:02:44,109
counted so you know that every request

00:02:41,769 --> 00:02:45,819
hits the load balancer every request

00:02:44,109 --> 00:02:48,010
hits your application service every

00:02:45,819 --> 00:02:50,560
request hits the database so you know

00:02:48,010 --> 00:02:53,049
that everything matches up and this is

00:02:50,560 --> 00:02:54,850
this is called white box bar white box

00:02:53,049 --> 00:02:57,040
and like our metrics based monitoring

00:02:54,850 --> 00:02:59,940
and so there's different examples of

00:02:57,040 --> 00:03:02,410
like the kind of software that does

00:02:59,940 --> 00:03:04,720
black box and white box monitoring so

00:03:02,410 --> 00:03:08,500
you have software like Nagios that you

00:03:04,720 --> 00:03:10,329
can configure checks and you can or

00:03:08,500 --> 00:03:13,750
Pingdom that Pingdom will do a ping of

00:03:10,329 --> 00:03:15,639
your website and then there's metrics

00:03:13,750 --> 00:03:18,340
based monitoring things like Prometheus

00:03:15,639 --> 00:03:19,989
things like the tick stack from influx

00:03:18,340 --> 00:03:22,720
data New Relic

00:03:19,989 --> 00:03:24,400
and other like commercial solutions but

00:03:22,720 --> 00:03:26,500
of course I like to talk about the open

00:03:24,400 --> 00:03:28,569
source solution so we're going to talk a

00:03:26,500 --> 00:03:32,169
little bit about Prometheus and so

00:03:28,569 --> 00:03:33,880
Prometheus came out of the idea that we

00:03:32,169 --> 00:03:37,180
needed a monitoring system that could

00:03:33,880 --> 00:03:40,630
collect all this white box metric data

00:03:37,180 --> 00:03:42,190
and do something useful with it and not

00:03:40,630 --> 00:03:44,049
just give us a pretty graph but we also

00:03:42,190 --> 00:03:49,060
want to be able to alert on it so we

00:03:44,049 --> 00:03:51,639
needed both we needed we needed both a

00:03:49,060 --> 00:03:52,480
data gathering and an analytic system in

00:03:51,639 --> 00:03:55,780
the same

00:03:52,480 --> 00:03:58,480
instance and so to do this we built this

00:03:55,780 --> 00:04:00,400
thing and we we looked at the way that

00:03:58,480 --> 00:04:02,769
data was being generated by the

00:04:00,400 --> 00:04:04,840
applications and there are advantages

00:04:02,769 --> 00:04:07,750
and disadvantages to this push versus

00:04:04,840 --> 00:04:09,910
pull model for metrics and we decided to

00:04:07,750 --> 00:04:12,700
go with the polling model because there

00:04:09,910 --> 00:04:16,870
are some slight advantages for polling

00:04:12,700 --> 00:04:18,280
over over pushing with polling you get

00:04:16,870 --> 00:04:20,109
this free black box check that the

00:04:18,280 --> 00:04:22,389
application is running so you when you

00:04:20,109 --> 00:04:24,280
do a when you pull your application you

00:04:22,389 --> 00:04:26,410
know that the process is running if you

00:04:24,280 --> 00:04:27,639
are doing push based you can't tell the

00:04:26,410 --> 00:04:30,940
difference between your application

00:04:27,639 --> 00:04:33,250
doing no work and your application not

00:04:30,940 --> 00:04:36,970
running so you don't know if it's stuck

00:04:33,250 --> 00:04:44,949
or is it or is it just not having to do

00:04:36,970 --> 00:04:48,490
any work with pulling the the polling

00:04:44,949 --> 00:04:51,070
system knows that the the state of your

00:04:48,490 --> 00:04:55,600
network so if you have a defined set of

00:04:51,070 --> 00:04:58,870
services that that inventory drives what

00:04:55,600 --> 00:05:00,789
should be there and again it's like the

00:04:58,870 --> 00:05:03,220
the disappearing is the is the is the

00:05:00,789 --> 00:05:04,960
process dead or is it just not doing

00:05:03,220 --> 00:05:06,940
anything with polling you know for a

00:05:04,960 --> 00:05:09,160
fact what processes should be there and

00:05:06,940 --> 00:05:11,380
it gives you a little bit of an

00:05:09,160 --> 00:05:13,570
advantage there with polling there's

00:05:11,380 --> 00:05:15,880
really easy testing with push based

00:05:13,570 --> 00:05:17,830
metrics you have to like figure out if

00:05:15,880 --> 00:05:19,150
you want to test a new version of the

00:05:17,830 --> 00:05:22,479
monitoring system or you want to test

00:05:19,150 --> 00:05:25,389
something new you have to like tee off a

00:05:22,479 --> 00:05:26,830
copy of the data with polling you can

00:05:25,389 --> 00:05:29,830
just set up another instance of your

00:05:26,830 --> 00:05:30,970
monitoring and just test it or you don't

00:05:29,830 --> 00:05:32,800
even have it doesn't even have to be

00:05:30,970 --> 00:05:38,080
martyring you can just use curl to poll

00:05:32,800 --> 00:05:40,510
the the metrics endpoint so it's it's

00:05:38,080 --> 00:05:46,000
significantly easier to test the other

00:05:40,510 --> 00:05:47,410
thing with the is the other the other

00:05:46,000 --> 00:05:49,419
nice thing is the client is really

00:05:47,410 --> 00:05:50,860
simple the client doesn't have to know

00:05:49,419 --> 00:05:52,930
where the monitoring system is it

00:05:50,860 --> 00:05:54,639
doesn't have to know about hae it just

00:05:52,930 --> 00:05:56,470
has to sit and collect the data about

00:05:54,639 --> 00:05:57,729
itself so it doesn't have to know

00:05:56,470 --> 00:05:59,889
anything about the topology of the

00:05:57,729 --> 00:06:04,120
network so as an application developer

00:05:59,889 --> 00:06:05,650
if you're writing a DNS server or some

00:06:04,120 --> 00:06:08,800
other piece of software you

00:06:05,650 --> 00:06:10,660
have to know anything about monitoring

00:06:08,800 --> 00:06:13,750
software you can just implement it

00:06:10,660 --> 00:06:15,400
inside your application and the the

00:06:13,750 --> 00:06:17,410
monitoring software whether it's

00:06:15,400 --> 00:06:19,900
prometheus or something else can just

00:06:17,410 --> 00:06:21,789
come and collect that data from you and

00:06:19,900 --> 00:06:24,580
that's kind of similar to the very old

00:06:21,789 --> 00:06:27,580
modern system called SNMP but SNMP is a

00:06:24,580 --> 00:06:31,150
significantly less friendly data model

00:06:27,580 --> 00:06:34,120
for developers so this is the basic

00:06:31,150 --> 00:06:35,979
layout of a Prometheus server so at the

00:06:34,120 --> 00:06:38,620
core there's a Prometheus server and it

00:06:35,979 --> 00:06:43,479
it deals with all of the data collection

00:06:38,620 --> 00:06:45,250
and analytics and basically this one

00:06:43,479 --> 00:06:48,940
binder it's a sync it's all written in

00:06:45,250 --> 00:06:50,830
going it's a single binary it knows how

00:06:48,940 --> 00:06:51,910
to read from your inventory there's a

00:06:50,830 --> 00:06:57,120
bunch of different methods whether

00:06:51,910 --> 00:07:01,570
you've got a kubernetes cluster or an a

00:06:57,120 --> 00:07:04,030
cloud platform or you'd have your own

00:07:01,570 --> 00:07:07,180
customized thing with ansible you can

00:07:04,030 --> 00:07:09,840
tell yeah ansible can take your layout

00:07:07,180 --> 00:07:15,610
drop that into a config file and

00:07:09,840 --> 00:07:17,500
Prometheus can pick that up it once it

00:07:15,610 --> 00:07:20,080
has the layout it goes out and collects

00:07:17,500 --> 00:07:22,979
all the data it has a storage and a time

00:07:20,080 --> 00:07:25,240
series database to store all that data

00:07:22,979 --> 00:07:27,669
locally and then it has a thing called

00:07:25,240 --> 00:07:31,930
prom QL which is a query language design

00:07:27,669 --> 00:07:36,220
for metrics analytics and then from that

00:07:31,930 --> 00:07:38,590
prom ql you can add front ends that will

00:07:36,220 --> 00:07:40,690
whether it's a simple API client to run

00:07:38,590 --> 00:07:42,940
reports you can use things like ref

00:07:40,690 --> 00:07:45,220
on/off or creating dashboards it's got a

00:07:42,940 --> 00:07:50,250
simple web UI built in you can plug in

00:07:45,220 --> 00:07:53,050
anything you want on that side and then

00:07:50,250 --> 00:07:55,690
it also has the ability to continuously

00:07:53,050 --> 00:07:57,580
execute queries called recording rules

00:07:55,690 --> 00:07:59,430
and these recording rules have two

00:07:57,580 --> 00:08:01,990
different modes you can either record

00:07:59,430 --> 00:08:04,000
you can take a query and it will

00:08:01,990 --> 00:08:05,979
generate new data from that query or you

00:08:04,000 --> 00:08:08,680
can take a query and if it returns

00:08:05,979 --> 00:08:11,830
results it will return alert and that

00:08:08,680 --> 00:08:14,169
alert is a push message to the alert

00:08:11,830 --> 00:08:17,529
manager and so this allows us to

00:08:14,169 --> 00:08:19,310
separate the generating of alerts from

00:08:17,529 --> 00:08:22,190
the routing of alerts and so

00:08:19,310 --> 00:08:24,020
you can have one or hundreds of

00:08:22,190 --> 00:08:25,940
Prometheus servers all generating alerts

00:08:24,020 --> 00:08:28,460
and it goes into an alert manager

00:08:25,940 --> 00:08:30,890
cluster and sends does the deduplication

00:08:28,460 --> 00:08:33,790
and the routing to the human because of

00:08:30,890 --> 00:08:37,250
course the the thing that we wanted was

00:08:33,790 --> 00:08:38,330
we had dashboards with graphs but in

00:08:37,250 --> 00:08:39,860
order to find out if something was

00:08:38,330 --> 00:08:41,750
broken yet to have a human looking at

00:08:39,860 --> 00:08:43,700
the graph so with Prometheus we don't

00:08:41,750 --> 00:08:45,830
have to do that anymore we can we can

00:08:43,700 --> 00:08:48,410
simply let the software tell us that we

00:08:45,830 --> 00:08:49,790
need to go investigate our problems so

00:08:48,410 --> 00:08:51,020
we don't have to sit there and stare at

00:08:49,790 --> 00:08:53,350
dashboards all day because that's really

00:08:51,020 --> 00:08:53,350
boring

00:08:53,750 --> 00:08:57,980
so what does it look like to actually

00:08:55,490 --> 00:09:01,490
get data into Prometheus so this is a

00:08:57,980 --> 00:09:03,740
very basic output of the of a Prometheus

00:09:01,490 --> 00:09:06,080
metric and so this is a very simple

00:09:03,740 --> 00:09:08,540
thing if you know much about the Linux

00:09:06,080 --> 00:09:11,270
kernel it tracked the Linux kernel

00:09:08,540 --> 00:09:14,120
tracks in proc stat all the state of all

00:09:11,270 --> 00:09:16,520
the CP CPUs in your system and we

00:09:14,120 --> 00:09:21,160
express this by having the the name of

00:09:16,520 --> 00:09:21,160
the metric which is this this whoops

00:09:21,250 --> 00:09:26,840
node CPU seconds total and so this is a

00:09:24,080 --> 00:09:28,190
self describing metric that that like

00:09:26,840 --> 00:09:29,810
you can just read the metric name and

00:09:28,190 --> 00:09:33,680
you understand a little bit about what's

00:09:29,810 --> 00:09:37,520
going on here so it's the Linux kernel

00:09:33,680 --> 00:09:39,470
and other kernels track their usage by

00:09:37,520 --> 00:09:41,210
the number of seconds spent doing

00:09:39,470 --> 00:09:44,750
different things and that could be

00:09:41,210 --> 00:09:48,830
whether it's in system or user space or

00:09:44,750 --> 00:09:50,830
IR Q's or IO wait or idle actually the

00:09:48,830 --> 00:09:54,440
kernel tracks how much idle time it has

00:09:50,830 --> 00:09:57,170
and it also attracted by the number of

00:09:54,440 --> 00:09:59,090
CPUs and with other monitoring systems

00:09:57,170 --> 00:10:02,690
they used to do this with a with a tree

00:09:59,090 --> 00:10:04,820
structure and this was cause a lot of

00:10:02,690 --> 00:10:07,460
problems for like how do you get how do

00:10:04,820 --> 00:10:11,360
you mix and match data so by switching

00:10:07,460 --> 00:10:13,820
from a tree structure to a tag based

00:10:11,360 --> 00:10:18,200
structure we can do some really

00:10:13,820 --> 00:10:20,930
interesting powerful data analytics so

00:10:18,200 --> 00:10:26,030
here's a nice example of temp taking

00:10:20,930 --> 00:10:27,830
that those CPU seconds counters and then

00:10:26,030 --> 00:10:30,700
converting them into a graph by using

00:10:27,830 --> 00:10:30,700
prompt QL

00:10:31,630 --> 00:10:36,200
now we can get into metrics based

00:10:33,770 --> 00:10:38,150
alerting so now that we have this graph

00:10:36,200 --> 00:10:39,890
we have this thing we can look and see

00:10:38,150 --> 00:10:41,870
here oh that well there's like some

00:10:39,890 --> 00:10:43,520
little spike here we might want to know

00:10:41,870 --> 00:10:46,700
about that so now we can get a new

00:10:43,520 --> 00:10:49,220
metrics based learning so I used to be a

00:10:46,700 --> 00:10:51,250
site reliability engineer or I still am

00:10:49,220 --> 00:10:55,250
a site reliability engineer at heart and

00:10:51,250 --> 00:10:58,720
we have this concept of the the things

00:10:55,250 --> 00:11:01,610
that you need to run a site or a service

00:10:58,720 --> 00:11:02,930
reliably and the most important thing

00:11:01,610 --> 00:11:05,150
you need is down at the bottom is

00:11:02,930 --> 00:11:06,980
monitoring because if you don't have

00:11:05,150 --> 00:11:11,690
monitoring of your service how do you

00:11:06,980 --> 00:11:13,910
know it's even working so there's a

00:11:11,690 --> 00:11:16,100
couple of techniques here and we want to

00:11:13,910 --> 00:11:19,040
alert based on data and not just those

00:11:16,100 --> 00:11:20,780
and end tests and there's a couple of

00:11:19,040 --> 00:11:22,400
things that you know there's a couple of

00:11:20,780 --> 00:11:23,540
techniques there's a thing called the

00:11:22,400 --> 00:11:25,970
read method and there's a thing called

00:11:23,540 --> 00:11:28,040
the use method and there's a couple nice

00:11:25,970 --> 00:11:30,920
links to some blog posts about this and

00:11:28,040 --> 00:11:32,960
basically it defines that for example

00:11:30,920 --> 00:11:34,730
the read method is it talks about the

00:11:32,960 --> 00:11:37,340
requests that your system is handling

00:11:34,730 --> 00:11:39,170
and there are three things there's the

00:11:37,340 --> 00:11:40,970
number of requests there's the number of

00:11:39,170 --> 00:11:43,490
errors and then there's how long it

00:11:40,970 --> 00:11:46,370
takes a duration and with a combination

00:11:43,490 --> 00:11:49,670
of those three things you can determine

00:11:46,370 --> 00:11:51,650
most of what your users see is did my

00:11:49,670 --> 00:11:56,150
request go through did it return an

00:11:51,650 --> 00:11:57,410
error and was it fast and most most

00:11:56,150 --> 00:11:59,420
people that up that's all they care

00:11:57,410 --> 00:12:02,210
about I made a request to a website and

00:11:59,420 --> 00:12:05,600
it came back and and it was fast like if

00:12:02,210 --> 00:12:08,030
it's the it's a very simple method of

00:12:05,600 --> 00:12:09,350
just like those are the important things

00:12:08,030 --> 00:12:13,130
to like determine if your site is

00:12:09,350 --> 00:12:15,950
healthy but we can go back to some more

00:12:13,130 --> 00:12:18,290
traditional sysadmin style

00:12:15,950 --> 00:12:20,900
alerts and so this is basically taking

00:12:18,290 --> 00:12:23,900
the file system available space divide

00:12:20,900 --> 00:12:26,090
it by the file system size that becomes

00:12:23,900 --> 00:12:28,670
the ratio of file system availability

00:12:26,090 --> 00:12:31,100
from zero to one multiply it by a

00:12:28,670 --> 00:12:33,200
hundred we now have a percentage and if

00:12:31,100 --> 00:12:36,800
it's less than 1 less than or equal to

00:12:33,200 --> 00:12:39,170
one percent for 15 minutes this is less

00:12:36,800 --> 00:12:41,720
than one percent space we should

00:12:39,170 --> 00:12:43,910
probably tell it sysadmin to go check to

00:12:41,720 --> 00:12:46,399
find out why that file systems full

00:12:43,910 --> 00:12:50,029
and it's super nice and simple we can

00:12:46,399 --> 00:12:52,100
also tag we can we can include excerpts

00:12:50,029 --> 00:12:55,940
we get too late we every alert includes

00:12:52,100 --> 00:12:57,860
all the extraneous labels can that

00:12:55,940 --> 00:13:00,380
prometheus adds to your to your metrics

00:12:57,860 --> 00:13:03,220
so when you add a metric in Prometheus

00:13:00,380 --> 00:13:06,589
if we go back and we look at this

00:13:03,220 --> 00:13:08,620
we look at this metric this metric only

00:13:06,589 --> 00:13:11,360
contains the information about the

00:13:08,620 --> 00:13:13,610
internals of the application anything

00:13:11,360 --> 00:13:15,290
else about like what server it's on is

00:13:13,610 --> 00:13:18,199
it running in a container what cluster

00:13:15,290 --> 00:13:20,750
does it come from what continent is it

00:13:18,199 --> 00:13:22,310
on that's all extra annotations that are

00:13:20,750 --> 00:13:25,310
added by the Prometheus server at this

00:13:22,310 --> 00:13:27,589
discovery time first I don't have a good

00:13:25,310 --> 00:13:30,259
example of that of what those labels

00:13:27,589 --> 00:13:32,899
look like but every every metric hat

00:13:30,259 --> 00:13:37,190
gets annotated with with location

00:13:32,899 --> 00:13:40,009
information and so that location

00:13:37,190 --> 00:13:46,040
information also comes through as labels

00:13:40,009 --> 00:13:48,230
in the alert so if you have a message

00:13:46,040 --> 00:13:50,480
coming into your alert manager the alert

00:13:48,230 --> 00:13:52,370
manager can look and go oh that's coming

00:13:50,480 --> 00:13:57,040
from this data center and it can include

00:13:52,370 --> 00:14:00,680
that in the email or IRC message or or

00:13:57,040 --> 00:14:02,660
SMS message so it can include like file

00:14:00,680 --> 00:14:04,910
system is out of space on this host from

00:14:02,660 --> 00:14:06,769
this data center and like all those

00:14:04,910 --> 00:14:08,360
labels get passed through and then you

00:14:06,769 --> 00:14:11,149
can append additional labels like

00:14:08,360 --> 00:14:13,100
severity critical to that alert and

00:14:11,149 --> 00:14:15,319
include that in the message to the human

00:14:13,100 --> 00:14:17,240
because of course this is the this is

00:14:15,319 --> 00:14:19,550
how you define getting getting the

00:14:17,240 --> 00:14:22,880
message from the from the monitoring to

00:14:19,550 --> 00:14:25,220
the human and you can even include nice

00:14:22,880 --> 00:14:26,269
things like if you've got documentation

00:14:25,220 --> 00:14:28,939
you can include a link to the

00:14:26,269 --> 00:14:31,939
documentation as an annotation and the

00:14:28,939 --> 00:14:34,459
alert manager can take that basic URL

00:14:31,939 --> 00:14:36,050
and you know massage it into whatever it

00:14:34,459 --> 00:14:38,990
needs to look like to actually get to

00:14:36,050 --> 00:14:42,290
the operator to the to the document

00:14:38,990 --> 00:14:44,120
eight correct documentation we can also

00:14:42,290 --> 00:14:45,740
do more fun things since we actually are

00:14:44,120 --> 00:14:47,839
not just checking what is the space

00:14:45,740 --> 00:14:52,490
right now we're tracking data over time

00:14:47,839 --> 00:14:54,079
we can use predict linear and predict

00:14:52,490 --> 00:14:56,630
linear just takes and does a simple

00:14:54,079 --> 00:14:57,590
linear regression and this example is it

00:14:56,630 --> 00:15:00,250
takes the file system

00:14:57,590 --> 00:15:03,170
available space over the last hour and

00:15:00,250 --> 00:15:06,710
does a linear regression prediction says

00:15:03,170 --> 00:15:09,740
well it's going that way and four hours

00:15:06,710 --> 00:15:11,960
from now based on the one hour of

00:15:09,740 --> 00:15:15,700
history it's going to be less than 0

00:15:11,960 --> 00:15:19,490
which means full so we know that within

00:15:15,700 --> 00:15:21,080
the within within the next 4 hours the

00:15:19,490 --> 00:15:23,960
disk is going to be full so we can tell

00:15:21,080 --> 00:15:25,430
the operator ahead of time that it's

00:15:23,960 --> 00:15:27,320
gonna be full and not just tell them

00:15:25,430 --> 00:15:29,620
that it's full right now so that they

00:15:27,320 --> 00:15:32,120
have some they have some window of

00:15:29,620 --> 00:15:34,130
ability to fix it before it fails and

00:15:32,120 --> 00:15:36,260
this is really important because if

00:15:34,130 --> 00:15:37,520
you're running a site you want to be

00:15:36,260 --> 00:15:40,910
able to put you want to be able to have

00:15:37,520 --> 00:15:43,640
alerts that tell you that your system is

00:15:40,910 --> 00:15:47,260
failing before it actually fails because

00:15:43,640 --> 00:15:49,610
if it fails you're out of SLO or SLA and

00:15:47,260 --> 00:15:51,110
your users are going to be unhappy and

00:15:49,610 --> 00:15:52,730
you don't want the users to tell you

00:15:51,110 --> 00:15:54,410
that your site is down you want to know

00:15:52,730 --> 00:15:58,910
about it before your users can even tell

00:15:54,410 --> 00:16:01,730
so this allows you to do that and also

00:15:58,910 --> 00:16:04,460
of course prometheus being a modern

00:16:01,730 --> 00:16:09,020
system we support full utf-8 in all of

00:16:04,460 --> 00:16:10,850
our labels and then here's another one

00:16:09,020 --> 00:16:14,600
here's here's a good example from the

00:16:10,850 --> 00:16:16,070
the used method so this is a rate of 500

00:16:14,600 --> 00:16:18,350
errors coming from an application and

00:16:16,070 --> 00:16:21,350
you can simply alert that there's more

00:16:18,350 --> 00:16:22,910
than one 500 error per second coming out

00:16:21,350 --> 00:16:26,300
of the application if that's your

00:16:22,910 --> 00:16:27,860
threshold for her pain and you can do

00:16:26,300 --> 00:16:30,410
other things like you could convert that

00:16:27,860 --> 00:16:33,380
from just a rate of errors to a percent

00:16:30,410 --> 00:16:37,580
of errors so you could say I have an SLA

00:16:33,380 --> 00:16:42,410
of three nines and so you can say if the

00:16:37,580 --> 00:16:44,000
error if the rate of errors so error the

00:16:42,410 --> 00:16:47,870
rate of errors divided by the rate of

00:16:44,000 --> 00:16:50,270
requests is 0.01 or is more than 0.01

00:16:47,870 --> 00:16:53,570
then that's a problem so you can you can

00:16:50,270 --> 00:16:55,400
you can you can include that level of

00:16:53,570 --> 00:16:58,610
error granularity and if you're just

00:16:55,400 --> 00:17:02,480
doing a black box test you wouldn't know

00:16:58,610 --> 00:17:04,160
this you would only get if you got you

00:17:02,480 --> 00:17:05,540
got an error from the system then you

00:17:04,160 --> 00:17:07,730
got another air from the system well but

00:17:05,540 --> 00:17:09,860
then you fire an alert but if those

00:17:07,730 --> 00:17:11,420
checks are one minute apart and you're

00:17:09,860 --> 00:17:16,040
serving a thousand requests

00:17:11,420 --> 00:17:20,120
and you could you could be serving

00:17:16,040 --> 00:17:22,340
10,000 errors before the before you even

00:17:20,120 --> 00:17:24,410
get an alert and you might miss it

00:17:22,340 --> 00:17:27,260
because what if you only get one random

00:17:24,410 --> 00:17:29,090
error and then the next time yeah you're

00:17:27,260 --> 00:17:30,680
serving 25 percent errors you only have

00:17:29,090 --> 00:17:33,080
a 25 percent chance of that check

00:17:30,680 --> 00:17:35,570
failing again so you you really need

00:17:33,080 --> 00:17:38,120
this move these metrics in order to get

00:17:35,570 --> 00:17:42,530
proper reports of the status of your

00:17:38,120 --> 00:17:44,390
system and you can

00:17:42,530 --> 00:17:46,340
there's even option so you can you can

00:17:44,390 --> 00:17:49,070
slice and dice those labels so if you

00:17:46,340 --> 00:17:51,770
have a label on all of your applications

00:17:49,070 --> 00:17:53,750
called service you can send that service

00:17:51,770 --> 00:17:55,910
label through to the message and you can

00:17:53,750 --> 00:17:58,190
say hey this service is broken you can

00:17:55,910 --> 00:18:03,170
include that that service label in your

00:17:58,190 --> 00:18:18,080
alert messages and that's it I can go to

00:18:03,170 --> 00:18:23,710
a demo in QA so - any questions so far

00:18:18,080 --> 00:18:23,710
or anybody want to see a demo microphone

00:18:24,430 --> 00:18:30,780
[Music]

00:18:32,920 --> 00:18:38,750
discovery site containers or do I have

00:18:35,450 --> 00:18:42,310
to implement the metrics myself so for

00:18:38,750 --> 00:18:45,470
metrics and containers so there are

00:18:42,310 --> 00:18:47,840
there's already things that expose the

00:18:45,470 --> 00:18:49,550
metrics of some container of the of the

00:18:47,840 --> 00:18:52,250
container system itself so there's a

00:18:49,550 --> 00:18:54,740
utility called C advisor and C advisor

00:18:52,250 --> 00:18:57,890
takes the link C Group data and expose

00:18:54,740 --> 00:19:00,080
it as as exposes it as metrics so you

00:18:57,890 --> 00:19:02,150
can get the data about how much CPU time

00:19:00,080 --> 00:19:03,440
is being spent in your container how

00:19:02,150 --> 00:19:06,500
much memory is being used by your

00:19:03,440 --> 00:19:10,100
container about the application just

00:19:06,500 --> 00:19:11,780
about the container right so because the

00:19:10,100 --> 00:19:13,850
container has no idea whether your

00:19:11,780 --> 00:19:18,890
application is written in Ruby or NGO or

00:19:13,850 --> 00:19:20,720
Python or whatever you have to build

00:19:18,890 --> 00:19:26,020
that into your application in order to

00:19:20,720 --> 00:19:26,020
get the data so so for Prometheus

00:19:27,380 --> 00:19:33,560
we've written client libraries that can

00:19:29,240 --> 00:19:36,200
be included in it in your application

00:19:33,560 --> 00:19:39,920
directly so you can get that data out so

00:19:36,200 --> 00:19:41,570
if you go to the Prometheus website we

00:19:39,920 --> 00:19:45,980
have a whole series of client libraries

00:19:41,570 --> 00:19:50,470
and basically we cover a pretty pretty

00:19:45,980 --> 00:19:50,470
good selection of popular soft software

00:19:55,360 --> 00:20:01,070
what is the current state of long-term

00:19:57,590 --> 00:20:04,220
data storage and very good question

00:20:01,070 --> 00:20:05,180
so they just they there's been several

00:20:04,220 --> 00:20:10,270
there's actually several different

00:20:05,180 --> 00:20:13,520
methods of doing this so Prometheus

00:20:10,270 --> 00:20:16,460
stores all this data locally in its own

00:20:13,520 --> 00:20:18,320
data storage on the local disk and but

00:20:16,460 --> 00:20:20,420
that's only as durable as that server is

00:20:18,320 --> 00:20:22,280
durable so if you've got a really

00:20:20,420 --> 00:20:23,660
durable server you can store as much

00:20:22,280 --> 00:20:25,280
data as you want you could store years

00:20:23,660 --> 00:20:26,840
and years and years of data locally on a

00:20:25,280 --> 00:20:28,790
Prometheus server that's not a problem

00:20:26,840 --> 00:20:31,490
that there's a there's a bunch of

00:20:28,790 --> 00:20:33,230
misconceptions because because of our

00:20:31,490 --> 00:20:35,650
defaults and not the language on our

00:20:33,230 --> 00:20:39,980
website said it's not long-term storage

00:20:35,650 --> 00:20:43,490
simply because we leave that problem up

00:20:39,980 --> 00:20:45,440
to the person running the server but the

00:20:43,490 --> 00:20:46,790
the data time series database that

00:20:45,440 --> 00:20:49,850
parenthese includes is actually quite

00:20:46,790 --> 00:20:51,980
durable but it's only as durable as the

00:20:49,850 --> 00:20:53,780
server underneath it so if you've got a

00:20:51,980 --> 00:20:56,540
very large cluster and you want really

00:20:53,780 --> 00:20:58,730
high durability you need to have some

00:20:56,540 --> 00:21:00,380
kind of clustered software but because

00:20:58,730 --> 00:21:03,470
we wanted Prometheus to be very simple

00:21:00,380 --> 00:21:07,040
to deploy and very simple to operate it

00:21:03,470 --> 00:21:08,660
and and also very robust we didn't want

00:21:07,040 --> 00:21:10,940
to include any clustering in Prometheus

00:21:08,660 --> 00:21:12,950
itself because any time you have a

00:21:10,940 --> 00:21:15,830
clustered software what happens if your

00:21:12,950 --> 00:21:17,410
network is little wonky the first thing

00:21:15,830 --> 00:21:20,600
that goes down is all your all of your

00:21:17,410 --> 00:21:22,250
distributed systems fail and building

00:21:20,600 --> 00:21:25,490
distributed systems to be really robust

00:21:22,250 --> 00:21:27,620
is really hard so Prometheus is is what

00:21:25,490 --> 00:21:30,050
we call uncoordinated an uncoordinated

00:21:27,620 --> 00:21:31,580
distributed system so if you've got to

00:21:30,050 --> 00:21:35,030
Prometheus servers monitoring all of

00:21:31,580 --> 00:21:37,550
your targets in an H a mode in a cluster

00:21:35,030 --> 00:21:38,600
and there's a split brain each

00:21:37,550 --> 00:21:41,600
Prometheus can

00:21:38,600 --> 00:21:43,100
see half of the cluster and it can see

00:21:41,600 --> 00:21:44,780
that the other half of the cluster is

00:21:43,100 --> 00:21:46,880
down they can both try and get alerts

00:21:44,780 --> 00:21:49,160
out to the alert manager and this is a

00:21:46,880 --> 00:21:51,740
really really robust way of handling

00:21:49,160 --> 00:21:54,110
split brains and bad network failures

00:21:51,740 --> 00:21:56,299
and bad problems in a cluster so it's

00:21:54,110 --> 00:21:58,700
it's designed to be super super robust

00:21:56,299 --> 00:22:00,380
and so that the two individual previous

00:21:58,700 --> 00:22:02,059
servers in your cluster don't have to

00:22:00,380 --> 00:22:04,850
talk to each other to do this they can

00:22:02,059 --> 00:22:06,980
just do it independently but if you want

00:22:04,850 --> 00:22:09,230
to be able to correlate data between

00:22:06,980 --> 00:22:11,530
many different previous servers you need

00:22:09,230 --> 00:22:14,419
an external data source for to do this

00:22:11,530 --> 00:22:15,500
and also you may not have very big

00:22:14,419 --> 00:22:17,330
servers you might be running your

00:22:15,500 --> 00:22:18,860
Prometheus in a container and it's only

00:22:17,330 --> 00:22:21,440
got a little bit of local storage space

00:22:18,860 --> 00:22:24,320
so you want to send all that data up to

00:22:21,440 --> 00:22:26,179
a big cluster data store for for bigger

00:22:24,320 --> 00:22:27,620
use and so we've got a couple we have

00:22:26,179 --> 00:22:29,570
several different ways of doing this

00:22:27,620 --> 00:22:31,460
there's the the classic way which is

00:22:29,570 --> 00:22:34,039
called Federation where you have one

00:22:31,460 --> 00:22:35,299
Prometheus server pulling in summary

00:22:34,039 --> 00:22:37,490
data from each of the individual

00:22:35,299 --> 00:22:39,679
Prometheus servers and this is useful if

00:22:37,490 --> 00:22:42,620
you want to run alerts against data

00:22:39,679 --> 00:22:44,900
coming from multiple media servers but

00:22:42,620 --> 00:22:46,610
Federation is not replication so it only

00:22:44,900 --> 00:22:48,049
can do a little bit of data from each

00:22:46,610 --> 00:22:49,909
Prometheus servers so if you've got a

00:22:48,049 --> 00:22:52,010
million metrics on a beach each year in

00:22:49,909 --> 00:22:56,150
Prometheus servers you can't pull in a

00:22:52,010 --> 00:22:56,960
million metrics and do it see if you've

00:22:56,150 --> 00:22:58,640
got ten of those

00:22:56,960 --> 00:23:00,049
you can't pull in ten million metrics

00:22:58,640 --> 00:23:02,809
simultaneously into one Prometheus

00:23:00,049 --> 00:23:04,730
server it's just too much data so

00:23:02,809 --> 00:23:07,190
there's two up there's a couple of other

00:23:04,730 --> 00:23:10,510
nice options there's a piece of software

00:23:07,190 --> 00:23:13,429
called cortex and cortex is of is a

00:23:10,510 --> 00:23:18,470
Prometheus server that stores its data

00:23:13,429 --> 00:23:20,929
in a database all right a distributed

00:23:18,470 --> 00:23:24,850
database so things that are based on the

00:23:20,929 --> 00:23:33,440
Google BigTable model like Cassandra or

00:23:24,850 --> 00:23:35,090
what's the Amazon one yeah DynamoDB so

00:23:33,440 --> 00:23:36,919
you can if you have a dynamo DB or a

00:23:35,090 --> 00:23:38,809
Cassandra cluster or one of these other

00:23:36,919 --> 00:23:41,780
really big distributed storage clusters

00:23:38,809 --> 00:23:43,280
cortex can run and the Prometheus

00:23:41,780 --> 00:23:45,350
servers will stream their data up to

00:23:43,280 --> 00:23:47,510
cortex and it will make a copy it will

00:23:45,350 --> 00:23:49,370
keep a copy of that across all of your

00:23:47,510 --> 00:23:50,840
all of your Prometheus servers and it's

00:23:49,370 --> 00:23:52,070
because it's based on things like

00:23:50,840 --> 00:23:55,820
Cassandra it's

00:23:52,070 --> 00:23:58,280
super scalable but it's kind of it's a

00:23:55,820 --> 00:23:59,720
little complex to run and many people

00:23:58,280 --> 00:24:02,060
don't want to run that complex and

00:23:59,720 --> 00:24:04,640
infrastructure the other new we have

00:24:02,060 --> 00:24:06,590
another new one that has it was just

00:24:04,640 --> 00:24:09,530
blogged about yesterday is a thing

00:24:06,590 --> 00:24:11,690
called Thanos and Thanos is prometheus

00:24:09,530 --> 00:24:15,430
at scale and basically with the way it

00:24:11,690 --> 00:24:15,430
works actually why don't I bring that up

00:24:23,570 --> 00:24:27,460
so this was this was developed by a

00:24:28,540 --> 00:24:36,250
company called improbable and they they

00:24:31,850 --> 00:24:38,900
wanted to they want they they had

00:24:36,250 --> 00:24:40,940
billions of metrics coming from hundreds

00:24:38,900 --> 00:24:43,610
of Prometheus servers and so they

00:24:40,940 --> 00:24:46,670
developed this in in collaboration with

00:24:43,610 --> 00:24:48,500
the Prometheus team to build a super

00:24:46,670 --> 00:24:52,670
highly scalable Prometheus servers so

00:24:48,500 --> 00:24:54,950
the Prometheus itself stores the

00:24:52,670 --> 00:24:57,200
incoming metrics data in AI right ahead

00:24:54,950 --> 00:25:00,260
log and then every two hours it creates

00:24:57,200 --> 00:25:02,180
a compaction cycle and it creates an

00:25:00,260 --> 00:25:05,210
immutable time series block of data

00:25:02,180 --> 00:25:07,220
which is all the time all the time

00:25:05,210 --> 00:25:10,150
series blocks themselves and then an

00:25:07,220 --> 00:25:12,950
index in you know into that data and

00:25:10,150 --> 00:25:14,900
those blows to our windows are all

00:25:12,950 --> 00:25:17,360
immutable so what balance does is it has

00:25:14,900 --> 00:25:19,310
a little sidecar binary that watches for

00:25:17,360 --> 00:25:21,230
those new directories and uploads them

00:25:19,310 --> 00:25:24,830
into a blob store so you could put them

00:25:21,230 --> 00:25:27,950
in s3 or Mineo or some other simple

00:25:24,830 --> 00:25:31,520
object storage and then now you have all

00:25:27,950 --> 00:25:34,700
of your data all this index data already

00:25:31,520 --> 00:25:37,760
ready to go and then the Thanos sidecar

00:25:34,700 --> 00:25:39,530
creates a little mesh cluster that can

00:25:37,760 --> 00:25:44,150
read from all those as three blocks and

00:25:39,530 --> 00:25:46,880
so now you have this Sigma now you have

00:25:44,150 --> 00:25:49,730
this like super global view all stored

00:25:46,880 --> 00:25:53,450
in a big bucket storage and things that

00:25:49,730 --> 00:25:55,120
like s3 or mini oh our bucket storages

00:25:53,450 --> 00:25:57,400
and not databases so they're

00:25:55,120 --> 00:26:00,530
operationally a little easier to operate

00:25:57,400 --> 00:26:03,230
plus and now now that we have all this

00:26:00,530 --> 00:26:05,389
data in the bucket store and the the

00:26:03,230 --> 00:26:07,309
Thanos sidecars can talk to

00:26:05,389 --> 00:26:10,129
each other we can now have a single

00:26:07,309 --> 00:26:11,539
entry point so you can query Thanos and

00:26:10,129 --> 00:26:13,399
Thanos will distribute your quarry

00:26:11,539 --> 00:26:15,200
across all your Prometheus servers so

00:26:13,399 --> 00:26:18,709
now you can do global queries across all

00:26:15,200 --> 00:26:21,139
of your servers but it's it's very new

00:26:18,709 --> 00:26:24,259
they just released their first release

00:26:21,139 --> 00:26:26,269
candidate yesterday so but it is looking

00:26:24,259 --> 00:26:28,940
to be like the coolest thing ever

00:26:26,269 --> 00:26:33,589
for for running large-scale Prometheus

00:26:28,940 --> 00:26:37,339
and so here's an example of of how that

00:26:33,589 --> 00:26:41,049
is laid out and this will bring let you

00:26:37,339 --> 00:26:41,049
have a billion metric Prometheus cluster

00:26:41,559 --> 00:26:55,369
and it's got a bunch of other cool

00:26:43,309 --> 00:27:05,089
features any more questions all right

00:26:55,369 --> 00:27:08,209
maybe I'll do a quick little demo so so

00:27:05,089 --> 00:27:11,019
here's here's a Prometheus server that's

00:27:08,209 --> 00:27:14,959
provided by an this group that just as a

00:27:11,019 --> 00:27:16,789
ansible deployment for Prometheus and

00:27:14,959 --> 00:27:21,499
you can just simply query for something

00:27:16,789 --> 00:27:24,049
like node CPU this is actually the old

00:27:21,499 --> 00:27:29,409
name for the for that metric and you can

00:27:24,049 --> 00:27:33,649
see here's exactly the CPU metrics from

00:27:29,409 --> 00:27:35,329
from some servers and it's yeah just a

00:27:33,649 --> 00:27:36,919
bunch of stuff and there's so there's

00:27:35,329 --> 00:27:39,320
actually two servers here there's

00:27:36,919 --> 00:27:42,829
there's an influx cloud alchemy and then

00:27:39,320 --> 00:27:45,039
there's a demo cloud alchemy oh yeah

00:27:42,829 --> 00:27:45,039
sure

00:27:45,789 --> 00:27:48,789
oops

00:27:52,510 --> 00:28:02,019
so you can see all the extra labels and

00:27:58,890 --> 00:28:03,940
we can also do some things like let's

00:28:02,019 --> 00:28:06,640
take a look at say the last 30 seconds

00:28:03,940 --> 00:28:09,700
so we can just add this little time

00:28:06,640 --> 00:28:12,309
window it's called a arrange request and

00:28:09,700 --> 00:28:14,380
you can see the individual samples so

00:28:12,309 --> 00:28:16,690
you can see that all Prometheus is doing

00:28:14,380 --> 00:28:19,240
is storing the the sample and an

00:28:16,690 --> 00:28:22,990
timestamp and all of the timestamps are

00:28:19,240 --> 00:28:25,049
in milliseconds and it's all epochs so

00:28:22,990 --> 00:28:27,220
it's super easy to manipulate but

00:28:25,049 --> 00:28:30,070
looking at the individual samples and

00:28:27,220 --> 00:28:34,149
and looking at these you can see that if

00:28:30,070 --> 00:28:39,760
we go back and just take and look at the

00:28:34,149 --> 00:28:44,260
raw data and we graph the raw data oops

00:28:39,760 --> 00:28:46,990
that's not it that's a syntax error and

00:28:44,260 --> 00:28:48,820
we look at this graph come on there we

00:28:46,990 --> 00:28:50,620
go well that's kind of boring that's

00:28:48,820 --> 00:28:52,990
just a flat line because it's just a

00:28:50,620 --> 00:28:54,760
counter going up very slowly so what we

00:28:52,990 --> 00:28:58,389
really want to do is we want to take and

00:28:54,760 --> 00:29:02,409
we want to apply a rate function to this

00:28:58,389 --> 00:29:05,500
counter so let's look at the rate over

00:29:02,409 --> 00:29:08,320
the last 1 minute and there we go now we

00:29:05,500 --> 00:29:13,539
get a nice little graph and so you can

00:29:08,320 --> 00:29:18,399
see that this is 0.6 CPU seconds per

00:29:13,539 --> 00:29:19,750
second for that set of labels but this

00:29:18,399 --> 00:29:21,730
is pretty noisy there's a lot of lines

00:29:19,750 --> 00:29:24,370
on this graph and there's still a lot of

00:29:21,730 --> 00:29:27,220
data here so let's let's start let's

00:29:24,370 --> 00:29:29,260
start doing some filtering so one of the

00:29:27,220 --> 00:29:30,880
things we see here is well there's idle

00:29:29,260 --> 00:29:32,710
well we don't really care about the

00:29:30,880 --> 00:29:35,679
Machine being idle so let's just add a

00:29:32,710 --> 00:29:39,159
label filter week so we can say idle or

00:29:35,679 --> 00:29:45,309
sorry mode is the label name and it's

00:29:39,159 --> 00:29:47,169
not equal to idle done and if I could

00:29:45,309 --> 00:29:51,159
type what did I miss

00:29:47,169 --> 00:29:52,990
oh I I erased my bracket there we go so

00:29:51,159 --> 00:29:54,580
now we've removed idle from that from

00:29:52,990 --> 00:29:55,809
the graph that looks that looks a little

00:29:54,580 --> 00:29:57,730
more sane

00:29:55,809 --> 00:30:00,610
oh wow look at that that's a nice big

00:29:57,730 --> 00:30:02,720
spike in user space on the influx server

00:30:00,610 --> 00:30:06,230
ok well

00:30:02,720 --> 00:30:10,460
that's that's pretty cool what about but

00:30:06,230 --> 00:30:12,710
this is still quite a lot of lines how

00:30:10,460 --> 00:30:14,120
much CPU is in the use total across all

00:30:12,710 --> 00:30:18,380
of the servers that we have so we can

00:30:14,120 --> 00:30:23,870
just sum up that rate and we can just

00:30:18,380 --> 00:30:25,670
see that there is a sum total of 0.6 CPU

00:30:23,870 --> 00:30:29,240
seconds per second across the the

00:30:25,670 --> 00:30:32,240
servers we have but that's that's a

00:30:29,240 --> 00:30:40,760
little too coarse what if we want to see

00:30:32,240 --> 00:30:43,040
it by instance now we can see the two

00:30:40,760 --> 00:30:45,790
servers we can see that we were left

00:30:43,040 --> 00:30:48,650
with this that just that label is the

00:30:45,790 --> 00:30:51,590
influx label or the instance influx and

00:30:48,650 --> 00:30:55,160
the instance demo and so that's a super

00:30:51,590 --> 00:30:57,080
easy way to see that but we can also do

00:30:55,160 --> 00:31:01,460
this the other way around you can say

00:30:57,080 --> 00:31:04,460
without mode comma CPU so we can drop

00:31:01,460 --> 00:31:06,110
those modes and see all the labels that

00:31:04,460 --> 00:31:09,740
we have so we can still see the

00:31:06,110 --> 00:31:12,350
environment label and the job label on

00:31:09,740 --> 00:31:14,240
all this on all this data so that you

00:31:12,350 --> 00:31:15,980
can you can go either way with this with

00:31:14,240 --> 00:31:18,890
the summary functions there's a whole

00:31:15,980 --> 00:31:20,450
bunch of different there's a whole bunch

00:31:18,890 --> 00:31:25,640
of different functions it's all in our

00:31:20,450 --> 00:31:31,940
documentation but what if we wanted to

00:31:25,640 --> 00:31:34,310
see it what if we wanted to see how

00:31:31,940 --> 00:31:36,950
which cpus are in use well now we can

00:31:34,310 --> 00:31:38,420
see that it's only CPU 0 because

00:31:36,950 --> 00:31:44,240
apparently these are only one core

00:31:38,420 --> 00:31:49,850
instances so you can add and remove

00:31:44,240 --> 00:31:52,600
labels and do all these queries any

00:31:49,850 --> 00:31:52,600
other questions so far

00:31:54,930 --> 00:31:58,960
yeah I don't have a question but I have

00:31:58,000 --> 00:32:01,330
something bad

00:31:58,960 --> 00:32:03,400
Puma traumatise is really nice but it's

00:32:01,330 --> 00:32:07,150
a lot better if you combine it with an

00:32:03,400 --> 00:32:10,240
ax yes yes Griffin oh so in the

00:32:07,150 --> 00:32:11,530
beginning when we were working on we

00:32:10,240 --> 00:32:13,630
were creating Prometheus we actually

00:32:11,530 --> 00:32:16,630
built a piece of dashboard software

00:32:13,630 --> 00:32:19,540
called prom - and it was like it was a

00:32:16,630 --> 00:32:21,010
simple little Ruby on Rails app to

00:32:19,540 --> 00:32:23,710
create dashboards and it had a bunch of

00:32:21,010 --> 00:32:26,650
JavaScript and then Griffin I came out

00:32:23,710 --> 00:32:29,050
and we're like oh that's interesting it

00:32:26,650 --> 00:32:30,460
doesn't support Prometheus so we were

00:32:29,050 --> 00:32:33,130
like hey can you support prometheus and

00:32:30,460 --> 00:32:35,830
they're like yeah you got a REST API get

00:32:33,130 --> 00:32:37,150
the data done okay here boom done now

00:32:35,830 --> 00:32:41,560
gravano supports Prometheus and we're

00:32:37,150 --> 00:32:45,550
like well prom - this is crap bleeped so

00:32:41,560 --> 00:32:48,520
we the Prometheus development team we're

00:32:45,550 --> 00:32:51,450
all back-end developers and s eries and

00:32:48,520 --> 00:32:54,160
we have no JavaScript skills at all so

00:32:51,450 --> 00:32:55,510
we're like well let somebody else deal

00:32:54,160 --> 00:32:56,650
with that so that's one of the nice

00:32:55,510 --> 00:32:58,510
things about working on this kind of

00:32:56,650 --> 00:33:00,520
project is we can do things that we're

00:32:58,510 --> 00:33:03,280
good at and we don't know we don't try

00:33:00,520 --> 00:33:04,360
and do a lot of we don't have any

00:33:03,280 --> 00:33:06,730
marketing people it's just an

00:33:04,360 --> 00:33:07,900
open-source project there's no company

00:33:06,730 --> 00:33:12,220
there's no single company behind

00:33:07,900 --> 00:33:16,360
Prometheus I work for get lab improbable

00:33:12,220 --> 00:33:20,500
paid for the sanno system other

00:33:16,360 --> 00:33:23,230
companies are like Red Hat now pays

00:33:20,500 --> 00:33:27,010
people that were used to work on the the

00:33:23,230 --> 00:33:28,480
four core OS to work on Prometheus so

00:33:27,010 --> 00:33:31,270
there's lots and lots of collaboration

00:33:28,480 --> 00:33:36,280
between many companies to keep to build

00:33:31,270 --> 00:33:39,630
the Prometheus ecosystem but yeah gorf

00:33:36,280 --> 00:33:41,100
on is great actually grow fauna now has

00:33:39,630 --> 00:33:43,870
[Music]

00:33:41,100 --> 00:33:50,030
to to to permeate to full time

00:33:43,870 --> 00:33:52,660
Prometheus developers all right

00:33:50,030 --> 00:33:59,419
that's it

00:33:52,660 --> 00:33:59,419

YouTube URL: https://www.youtube.com/watch?v=HuG_n4cy5D8


