Title: Berlin Buzzwords 2014: Anca Kopetz & André Bois-Crettez - How to tackle performance issues when ...
Publication date: 2014-05-28
Playlist: Berlin Buzzwords 2014 #bbuzz
Description: 
	Anca Kopetz & André Bois-Crettez talking about "How to tackle performance issues when implementing high traffic multi-language search engine with Solr/Lucene"

This presentation will summarize the experience gained during an amazing journey of our team that implemented, deployed, and monitored the new search platform in production, which replaced a proprietary search engine with the popular open source Apache Solr.

Our company Kelkoo is an e-shopping platform that connects merchants and customers in different countries all over the world. The core of this platform is the search engine that allows clients and partners to execute full-text queries in order to find the best offers for their search. The queries could be pretty complex: range, filter & function queries, facets etc. They are executed on indexes of more than 15 millions of documents.

We used scalable and feature-rich technologies (Apache Solr/Lucene) to implement the search platform. We had to deal with exciting problems ranging from which features to implement, how to scale out and up the system, to SOLR and JVM tweaking in order to guarantee fast responses with high traffic on search cluster.

About Anca Kopetz:
https://2014.berlinbuzzwords.de/user/172/event/1

About André Bois-Crettez:
https://2014.berlinbuzzwords.de/user/212/event/1

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              hello everyone I am on the way back fit                               is I'm a software architect at Cal group                               hi I'm Monica kopites i'm a software                               engineer at calico and today we are                               going to talk about query performance                               optimization actually we are going to                               talk about it our experience gained                               during one year of trying to implement                               our search engine with a solar you see                                before we start the talk and maybe a                                quick rise of hands how many of you have                                already used the solar or elastic search                                there are most everybody nice and the                                old many of you add the performance                                problems not everybody but quite at                                thanks so you are at the right talk a                                quick got outline first we will explain                                the context and the set up with what is                                calc you what is our on engine usage and                                why it's important                                                  will present the the way we benchmarked                                on our our system and what we should be                                prepared because we knew that that                                furnace was critical we will present                                performance traditions we came up with                                most of the performance solution depend                                on the usage so not all will be o'clock                                appt liable for you but I hope you will                                see the way we walked and solve problems                                and the last we will explain what                                happened in production because of course                                if even if you are well prepared and                                there are still surprises in prediction                                so before going into the details so                                brief description of Cal cool calc with                                a shopping platform that connects                                customers with merchants actually we                                have a repository of merchants offers                                that are indexed on our                                search platform and then the end users                                via calcio website and the partner                                website they can search for good as you                                can see the source platform is the core                                of our system we have an index of more                                than                                                                   deployed worldwide in                                                 from the traffic point of view we had                                Peaks over                                                            better understand what we do a small                                example French people love cheese fondue                                oh yes yes they do a lot so they can                                connect on the cal cool website and type                                in a query like a pariah Fond du UNS                                arabic and they will find plenty of                                fondue kids as you can see we are                                displayed the offers from different                                merchants and the user have the                                possibility to filter by category we                                display statistic like a min max price                                and they can do faceted navigation you                                may be already familiar with the facets                                oh yeah I think so because we are going                                to talk a lot I'll just say that we we                                display all the margins that have offers                                associated to this query and for each                                member merchant will display the number                                of offers that are indexed and so as you                                can see a search engine is central to                                the way it works and in the past we as                                we were bought by yahoo we had access to                                ya there yahoo internal search engine                                worked pretty well but when they sold us                                we no longer had updates and the                                technology and nobody knew it so it was                                not even possible to very support for                                that and in                                                           with the implementation using solar                                for in the past month we selected the                                what was the best engine we could try                                and registry to go with Sura                                   important points where we should be able                                to sustain the Christmas high traffic at                                the end of the year but we don't have                                strong constraints around the near                                real-time indexation Cassandra explained                                for                                                                   was the Christmas traffic we are                                concentrated on query performance so we                                said before putting everything into                                production we like to do some                                performance evaluation so we chose as a                                target our big bigger country cluster                                which was composed of                                                more than                                                             index and we expected a traffic of more                                than                                                              wanted an average query time less than                                                                                                        we have to develop our search                                 application deploy it on the pre                                 production servers the pre production                                 servers should be as close as possible                                 to the production servers from the                                 hardware point of view and then we had                                 to identify the monitoring tools and we                                 wanted to execute the benchmarks in                                 order to simulate the real traffic we                                 interacted frequently with the solar                                 community which is a very active and                                 useful community so we decided to                                 structure our clusters separately for                                 each country most of you should be                                 familiar with the the wall structure but                                 I will work up so we have such clients                                 in our case it's a each server using                                 HTTP so our server connecting through a                                 virtual IP                                                             distributes the query                                 all the server's of country cluster                                 depending on the countries we have                                 different volume different crazies                                 performance so keeping that separated                                 was easier for us for prediction on the                                 other side we have our affairs                                 repository that goes through Houston                                 enjoy by indexers with Ted concurred in                                 it using cloud server to distribute the                                 annexation of documents in the in the                                 clusters for the benchmarks we had to                                 find the stress tool in order to execute                                 them so we chose Gatling we get link you                                 can define sonari that read the data set                                 and then they will send many many                                 queries in parallel to to the pre                                 production servers in order to simulate                                 the real traffic for the data set what                                 we did we took the production log looks                                 from the previous search engine Yahoo                                 search engine and then we transformed                                 into solar query in order to be as close                                 as a possible to the production traffic                                 when setting the benchmarks there are                                 two parameters that we found very                                 important is the number of users in                                 parallel and the duration of test for                                 the number of users in parallel just pay                                 attention when you set it because if it                                 is too small then your cpu load on the                                 pre production servers will be small so                                 the servers will be under loaded so for                                 example a good beginning we had a lot of                                 forty percent and then if it is too high                                 the servers are overloaded so it has an                                 impact on the average response times of                                 the benchmark the results were were                                 irrelevant so what we did with chose a                                 value so that the cpu load to be of                                 eighty percent the duration of test it                                 should be long enough in order to have                                 relevant results and not too long in                                 order not to have to wait too much for                                 your results                                 it has an impact of the productivity so                                 for example what we choose we set it one                                 hour two hours and it was so enough                                 jetLink generates reports of this kind                                 so these are the metrics that we used in                                 order to evaluate the performance is the                                 mean response time for example or or the                                 mean number of requests per second get                                 link shows the graphs like response time                                 distribution of our number of queries                                 per second distribution and so at the                                 end of the benchmark of course we have                                 the average Crispus account as first                                 time in and those graph but it was also                                 important to closely monitor a lot of                                 matrix about what happened on the                                 servers to verify where on the                                 bottlenecks because and if you opened                                 benchmarks its lot the disk everything                                 else will not be used at its full                                 potential so having those graphs during                                 the benchmarks allowed to verify if the                                 load was evenly distributed among all                                 the resources memory CPU discs etc and                                 not only the system matrix but also the                                 matrix from solar using genomics to have                                 caches it ratio verified by that they                                 are config you are configured properly                                 and as well we monitored the the one                                 that time is the time after commit                                 before the data is available and caches                                 around it should not be too long we                                 monitor the numbers and text segments                                 too because the most index signals you                                 have is the way the loosing index is                                 done the more segments you have the less                                 performance you get so this was the                                 phase where we prepared our benchmarks                                 we identify the tools to monitor our                                 system we installed our products                                 servers we developed hours the first                                 features on our search application and                                 we prepared the benchmark scenario so we                                 already we said let's go let's launch                                 the benchmarks in order to do the query                                 performance evaluation to end to explore                                 new ways to improve it so what happened                                 first we explode different possibility                                 around the shutting and replicating and                                 I guess the most of you should have an                                 idea about that but simply simple sketch                                 is the replication what happens is that                                 the index is the same of on each server                                 and the queries are load balanced among                                 all of those it scales linearly it's                                 very efficient when you have a very high                                 number of crisp a seconds that's a very                                 good scalability system however when                                 each of the queries is too long for                                 example when the index is very big and                                 what you can do is shard tion splits the                                 documents among the different entities                                 so here we have servers that contains                                 two short one is our replicas of shot                                 one same for Shawn to here and when the                                 crew arrives on any of the servers it                                 will be split into two or more depending                                 on number of shots surprise so each of                                 those will run in parallel and the                                 results are merged and we turn to the                                 new client so the croix times gets                                 roughly equivalent to the maximum tank                                 on each of the shots so doing benchmarks                                 we have the situation where the hardware                                 used either in this configuration on                                 this one with the same number of servers                                 the results were fairly the same but                                 that wasn't the ideal case what happens                                 in case of a failure                                 in this case it's very different when                                 purely replicating somewhere down means                                 that the other servers will have to work                                 a bit more but all others will be loaded                                 the same what happens with shouting is                                 that the load is distributed on the                                 remaining observers of the shot so here                                 we have only one poor server doing all                                 the work and it means that the cluster                                 will be water depth much faster despite                                 having the same hard way so when you                                 need resilience keep a lot of                                 replication and avoid shouting if you                                 can during back benchmarks we are the                                 surprise too we expect it to have very                                 emotional hardware and configuration and                                 sometimes we saw some servers                                 moreloading than others and what happens                                 is that with the solar and cluster all                                 the servers will work the same                                 especially with reputation the load is                                 really really intent car and perfectly                                 disparate distributed among all the                                 servers and shouting if you have bad                                 luck you can have a sharp slightly more                                 legit than the others but mostly the the                                 load is evenly distributed that means                                 that if any server is a bit slower than                                 losers the others will not work for him                                 so it will be a bottleneck very quickly                                 that's how we discovered that some of                                 the servers had energy saving option in                                 the viewers so the CPU was not fully                                 used we were able to fix that another                                 aspect that we analyzed was the                                 indexation actually the question was how                                 often should we commit how often our                                 months should be visible to search and                                 how often they should be stored in the                                 index for the visibility part we choose                                 the soft commit implemented with                                 committee within common of                                            and open search r equals true so open                                 search i equals to it means that offers                                 are immediately visible to search what                                 happens during soft commit is that the                                 transaction logs are not truncated so                                 they are accumulating and the caches are                                 flushed and auto one so just pay                                 attention because it has an impact on                                 your performance in terms of query time                                 for the durability we implemented the                                 hardcore meet via autocommit of                                    minutes and open searcher equals false                                 so it means that the document will be                                 stored on the on the index own on the                                 illusion index but they won't be visible                                 to search what happens here is that the                                 transaction logs are truncated and the                                 segment merges are initiated which was                                 very good because actually during our                                 benchmarks we realized that the number                                 of em in the same segment impacts the                                 query performance so we said okay let's                                 optimize our index let's optimize it                                 very often so if you launch the                                 optimized command the number of segments                                 in your index will be reduced to                                      immediately after executing this comment                                 we had pretty good results in terms of                                 query performance but actually we                                 realized very fast that this operation                                 has short-term benefits because after                                 one hour of commit the every response                                 time and the average number of query per                                 seconds in our benchmark increased so we                                 said let's try something                                 let's try something else let's set a                                 more aggressive Marsh policy so what we                                 did we modify the parameters of the key                                 our mesh policy in order to have during                                 the committee when the segments are                                 married to for loose listen to merge as                                 many segments as possible and actually                                 what happened is that we had pretty good                                 results in terms of number of queries                                 per second and every response time so                                 actually it worked for us in our case so                                 how to explain what we did on the                                 exertion side to improve the performance                                 to improve performance also important to                                 look at search caches first of all the                                 recent files are accessed so through a                                 memory mapping system it allows the                                 operating system to cache the files in                                 once so you have much less I oh wait and                                 you don't have to increase the JVM                                 memory and all cases we had enough run                                 to not even need SS these disks we were                                 still using the spinning disks we have                                 from the previous project and the                                 document cash was the disabled because                                 it was quite redundant with the memory                                 mapping of recent files and we didn't                                 see any improvement in in the benchmark                                 of using discussion and the credit cash                                 was kept very small and as we already                                 have a great gash above the calculor API                                 it was redundant with Sola and I didn't                                 bring any visible improvement in                                 benchmarks and it just consumed too much                                 java memory so in the given query                                 different caches are used here we have a                                 query with the                                 fulltext an iphone and we have a filter                                 on the merchant ID we asked for facets                                 and in category as well as colors under                                 offers displayed so filters used filter                                 cash of course and facets on single                                 valued feel like we have here for                                 category ID use the field cache of                                 listen you cannot tune it through solar                                 by default the facets on dynamic feeds                                 dynamic multivalued fields like we have                                 here with the future star cetera to                                 store a great number of different fields                                 that depends on new category except i'll                                 uncle chris not limited by the schema                                 but by default the FC means that field                                 value cash is used and the cash on trees                                 are very big wet issues the on memory                                 usage with that and it's not very good                                 for local energy fields no case the                                 color could be maybe a dozen different                                 value so we tried the set method inner                                 which is not always advised that but it                                 works very well for the the fields with                                 the law cardinality and in benchmarks we                                 saw that the memorization what was much                                 lower and position pain was the same and                                 just spared more memory it's just that                                 it's it uses the filter cash same as the                                 filters but you have to put a lot of                                 countries each value of the facet will                                 have a occasion tree so we get to a                                 better at we also analyze the impact of                                 the query features on the performance so                                 we you have to add many features in                                 order to meet your relevancy                                 requirements but what happens is that                                 most of the time or sometimes these                                 features have                                 an impact on the performance so what you                                 should do you should get a good balance                                 between relevancy and performance when                                 you measure the relevancy you measure                                 video manual test or a be testing but                                 what you should do you should also                                 measure the impact on the performance so                                 you should monitor your cluster in order                                 to see what's the impact on each future                                 on the mini response time or a query                                 time to better understand that I would                                 like to give you an example of                                 transformations of features that we                                 implemented a tail at Cal cool so if the                                 user types in a query like leather                                 accessories for iphone will transform                                 this query we will apply some                                 transformations like lower case leather                                 becomes leather stemming the accessories                                 is reduced to accessory will remove the                                 stop words and then we split on modeling                                 so iphone                                                          iphone &                                                        operatories end so all search terms are                                 mandatory but what happens if the query                                 doesn't return any result what we do we                                 execute an or query and actually during                                 our benchmarks we realize that the or                                 query is very expensive it takes a lot                                 of time because it returns lots of a                                 result and it searches through the whole                                 index so what we did for example we                                 implemented the mean should max that                                 will match that reduce the number of                                 work queries the facets are also very                                 expensive the fact that we display                                 offers from different merchants so we                                 had to youth group I merchant ID it's                                 also expensive the main idea behind this                                 every time you add a new feature try to                                 test the impact on the performance                                 so this was the end of our benchmarks                                 phase we analyzed many aspects we did                                 many improvements on the solar cloud                                 configuration on the indexation policy                                 Marsh policy search caches and solar                                 features so we were all ready to go in                                 prediction right and of course we had                                 surprises for example we had out of                                 memories due to filter cash we tune the                                 values so that the memory usage was                                 adequate in the benchmark but the actual                                 number you put in the configuration is                                 not doing more used the memo euros will                                 depend on the data you have in the index                                 as well as what are the queries that are                                 done and this despite we used the                                 prediction initiation data and pollution                                 queries we were quite surprised also by                                 that and so we increase the memory it's                                 important that you also wash game as the                                 same start and maximum given each                                 settings that was very evident in the                                 benchmarks                                                          twenty percent person performance                                 improvement and we turn down the a bit                                 the number of education trees and we                                 solve that about garbage collecting                                 parameters and they are quite a few it's                                 a bit of a black art I refer you to the                                 page of Shannon I see that some apps the                                 various settings that works some are                                 more detail than others and the main                                 takeaway is that confront my mark and                                 sweep CMS is important for solar that's                                 the group that works to the best we had                                 a few bad surprises with the service                                 going to recovery that means that one of                                 the server in a cluster things is it's                                 lagging behind the indexation so it will                                 stop serving queries and a lot would be                                 distributed among those us                                 and it will stop serving queries and                                 retrieve index from another server and                                 during the during that time the index is                                 retrieved it means that you have one one                                 less server doing the query work and we                                 followed closely the solar versions that                                 were released to benefit from                                 improvement and features bug fixes or                                 performance improvements son we had some                                 cases of regression in your cases so we                                 discussed on the solo is our mailing                                 list file server and i'll put fix the                                 those case and the subsequent versions                                 were perfect for us so this is our story                                 it was a success story with a leucine                                 solar it took was one year we had a high                                 traffic peaks of more                                                  second with an index of more than                                    millions documents the main ideas that                                 we would like to share with you when                                 tackling performance issues is benchmark                                 your system before putting into                                 production keep monitoring your                                 production cluster just pay attention                                 that the configuration that you do                                 during the benchmark might not be the                                 same in the production as Andre                                 explained with the setting of the filter                                 cash we had an out of memory in                                 production and identify the bad guy and                                 deal with him it wasn't one of my                                 colleagues at killed who actually what                                 I'm trying to say is that try to                                 identify the resource or the component                                 or the behavior that might have an                                 impact on your performance we have new                                 ideas for performance optimization that                                 we would like to implement them like                                 analyzing the incoming                                 african see if we can remove some bad                                 queries or test some other types of                                 caches so we still have work to do                                 that's it thank you a lot so if you are                                 interested as you can see we are doing                                 many interesting stuff would kill you if                                 you are interested in sort or big data                                 please join us the engineering team is                                 in Grenoble in France which is a very                                 nice city near the mountains and now if                                 you have questions yeah okay any                                 questions so finally you use short or                                 what was the decision it depends so for                                 one of most loaded cluster we as we were                                 using bit old servers we had two shot so                                 I guess it was something like no more                                 than two shots and and don't remember                                 the number of replicas maybe                                             was so                                                                   service for example it was really the                                 one of the biggest cluster most                                 problematic one and all those were only                                 for example two servers in a reputation                                 and such and we ended it with bringing                                 new hardware so that's why quite old and                                 this time we did not lead the shouting                                 to simplify that so when when you use                                 the for the case where you used shorts                                 did you use something like routing like                                 I mean to yeah yeah it was the default                                 word witching so we didn't custom it                                 that that to your question yeah yeah my                                 question is what are you customize the                                 routing or no no no it was the default                                 one another side solo crowd you're                                 seeing                                 no it didn't thank you hi on one slide                                 you showed that you have regionalized                                 server cluster so you there was one                                 cluster for France and one for Russia do                                 you split any content according to                                 language to each cluster so does the                                 Russian cluster just has Russian                                 language content or do you have a multi                                 language environment in each cluster                                 know right now we separate that the                                 configuration for Sara we have a common                                 part and customized part for each each                                 region country and there is no inside                                 the cluster there is no multi-language                                 stuff and the only thing we shared                                 between clusters was the sole keeper in                                 December so with five servers spread                                 among all those and even for the follow                                 dogs / even if it was only one and                                 somewhere for all the countries there                                 are split using a zookeeper would I                                 don't know if you are familiar so it                                 allows to supply the configuration and                                 for us it was much easier to change the                                 configuration for a country sobriety you                                 with that okay so and so any any content                                 you have in for example French you put                                 in just one cluster and have a language                                 specific indexing and schema in this                                 cluster with Zola yes yeah for example                                 the stabbing we apply different stammers                                 for each language so depending on the                                 country cluster we the schema is adapted                                 to that country so it will include the                                 appropriate stemmer okay and have you                                 ever thought about mixing it up and how                                 that would have an                                 impact on performance the advantage of                                 mixing different countries and the only                                 single cluster would be to spread the                                 load in an easier way problems are that                                 memory wise it's not it's not easy and                                 the the caching is different and we                                 added a bit with the previous engine to                                 keep a country separate so we kinda went                                 with the same type of architecture                                 sometimes we discuss about what can we                                 merge what can happen is that when the                                 single server we can have multiple                                 instances of solar some four countries                                 on furniture when we don't mix the two                                 settings or the the JVM we don't mix                                 languages in a single garage again thank                                 you how do you replicate your index                                 using the HTTP replication or the arson                                 connection yeah we are using the solar                                 cloud feature so it's all done by nice                                 flow itself that your question we don't                                 use the old style replication or first                                 prototypes use the master-slave and                                 search and it was a bit of going back                                 compared to the yahoo proprietary search                                 engine and we were all glad that solar                                 cloud arrived and we moved any single                                 point of failure in case compared to the                                 previous search engine there is much                                 less sports in in solar compared to a                                 lot of photos hi I'm question about                                 hardware so those clusters look really                                 impressive so do you host everything                                 internally or maybe use some services                                 like Amazon services no we have our own                                 space in data centers with a physical                                 hardware it's a submitter for                                 early gassy we are studying what we can                                 move to a amazon services or in such                                 it's not that easy because the volume of                                 that I windex as a cost and well for                                 formations that one's                                                    it's not so interesting to go to Amazon                                 you have change the feather message to                                 enum and did it reduce the amount of                                 memory you have to need further caches                                 so what do you say you change the pass                                 up method yeah from FC to enum yeah it                                 reduces this is this ridiculous amount                                 of memory unique occasion yeah because                                 actually the for each type of cash the                                 type of information that is stored is                                 different from one to another so it                                 reduced the amount of memory how much                                 good actually depends a lot on the                                 number of entries that you said but for                                 the same for the same number of entries                                 yeah sorry in fact it really depends on                                 the data so if you have a few different                                 values for fill the field value cash is                                 really not efficient and the enemy                                 really shined for that it's some                                 difference is that photo setting you you                                 enter one for each field in field value                                 cash whereas for the enemy mode with                                 filter cash it's one for each value for                                 each field so the dough number is a                                 minor thousands more sound sound bigger                                 it just that the memory usage is much                                 smaller / eaten thanks any more                                 questions                                 hi so I know you said during your                                 benchmarking you were targeting meme                                 response time did you ever have to                                 consider things like                                                     maximum response time as well just for                                 looking at those yes it was to simplify                                 the presentation but of course the the                                                                                                          with with the gvm garbage collector it                                 can also that the                                                        saw in the                                                              I guess one of the benchmarks under the                                 value is it at the beginning begin                                 actually in this way we realize that the                                 all queries takes a lot of time so we                                 had to analyze them in details and you                                 had to find a solution to D reduce the                                 number of or queries as i said the                                 metrics that i show there it was just an                                 example of the metrics that we are using                                 but actually we are analyzing the whole                                 report it's very important as you as you                                 are saying ok any more questions going                                 once twice three salt ok thank you thank                                 you guys enjoy your lunch                                 you
YouTube URL: https://www.youtube.com/watch?v=AFbWhjr5bho


