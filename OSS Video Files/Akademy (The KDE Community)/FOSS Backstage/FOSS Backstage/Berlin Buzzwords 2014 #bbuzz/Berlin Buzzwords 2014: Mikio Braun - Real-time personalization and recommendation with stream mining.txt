Title: Berlin Buzzwords 2014: Mikio Braun - Real-time personalization and recommendation with stream mining
Publication date: 2014-05-28
Playlist: Berlin Buzzwords 2014 #bbuzz
Description: 
	Recommendation and personalization system usually use elaborate store and batch algorithms to periodically crunch user event data like views, ratings, or purchases to compute predictions. A downside of this approach is that recommendations do not reflect the current user behavior, leading to missed opportunities in making good recommendations, or out-dated recommendations, for example when the purchase has already been made. 

We discuss novel systems based on stream mining algorithms which accumulate statistics on user behavior in real-time in a streaming fashion, this way always reflecting the most recent user behavior. Comparing profiles accross different time-scales, one is also able to classify recent behavior which deviates from the long-term trend and might be particularly interesting. Such algorithms have applications in ad targeting, recommendation, retail, monitoring, some of which will be discussed in more detail.

Read more:
https://2014.berlinbuzzwords.de/session/real-time-personalization-and-recommendation-stream-mining

About Mikio Braun:
https://2014.berlinbuzzwords.de/user/320/event/1

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              hello everyone thanks for showing up to                               my talk my name is Michael Brown                               the I'm a so-called                               scientist and today I want to talk a bit                               about new approaches to achieving real                               time which don't just rely on scaling                               okay so reacting to user behavior so                               there's a lot of data which is generated                               these days and which which you also use                                a big data for to analyze and that's                                actually use a data right so there's a                                data user here he goes to website and                                then everything he does there is written                                into some logs so this might be                                pageviews when he clicks on something                                when you put something into is into a                                card or something and based on this                                right so you want to analyze it to                                crunch the data and for example you get                                out okay right now this user's                                interested in cars okay so I could you                                know show him an ad for some car maker                                or I could do no recommend him a nice                                new car or I can show articles based on                                these cars and then I can feed all these                                things back to the user so that the                                website is adapting to him and showing                                him the stuff which is interesting for                                him right so it's it's a form of a                                conversation with the user and                                apparently this has to be in real time                                right so if I if I look for cars now and                                then the system like after an hour                                decides over there somebody who's just                                interested in cars then maybe I already                                stopped using the website so this is                                somehow yeah okay and then the year out                                up there there's also like these are                                like items on your website so this is                                this also goes into the database so                                there's a whole lot of data in there                                okay so far so good now if you if you                                would do this nowadays right then                                usually would do it may be like                                         ago or five years ago you would do it                                like this you collect all your data into                                a big sequel database and then you                                analyze it and analyzing usually means                                you do some you know some counting over                                certain time frames like you take the                                last month of data and then for each                                user you count how often he has looked                                at certain categories and stuff like                                that and then the result again ends up                                in a data database and then if the users                                there you can actually show them the                                stuff you think he's interested in but                                the problem here of causes that this                                it's a lot long time right so if you                                have several I know                                                  day then alone calculating this thing                                over whole week will take more time also                                it won't work that well so as new data                                comes in the database gets even slower                                so this is yeah something we apparently                                no doesn't work very well okay so maybe                                like three years ago you actually do it                                like this you don't put it into a sequel                                database but instead you just write the                                locks on your Hadoop cluster and then                                you can run a MapReduce job to analyze                                the data and then stall the results                                again in some database and you can get                                it out this is a like it's an                                improvement so at least it's scalable                                but again these jobs take a long time to                                run so probably longer than the user is                                actually interacting with the website so                                then the next thing so if you do it like                                this year maybe you have something you                                switch from this batch oriented                                processing to a stream oriented                                processing where you process all the                                events is they come in so you probably                                use something like a patchy Kafka which                                is basically a piece of infrastructure                                which lets you collect lots of data in a                                reliable fashion and then you would use                                something like storm for example so this                                is a string processing framework which                                you probably all know where you can                                process the events as they come in and                                you can also scale that out and then you                                would probably not use HBase but some                                memory based database like red is                                because it has to be fast right so here                                at this point now you're in a position                                like if you have a lot of money you can                                actually get to a point where you can                                 like see what the user is doing and and                                 react to it in real time okay so far so                                 good but the problem is so there are                                 some problems with this so one is that                                 actually if there's a lot of data then                                 this needs to be quite big because                                 because like it's processing unit and                                 because of all the distribution overhead                                 is actually not that fast so each of                                 your nodes can process maybe know a few                                 hundred a few thousand events per second                                 something like this this so basically it                                 all comes down to this right so one does                                 not simply scale into real time I                                 you can if you have a lot of money but                                 still the question is whether you should                                 or not and so in a way like so far all                                 of this big data has been about scaling                                 so it has been like thinking about how                                 we can break down the stuff we do with                                 our data into pieces of infrastructure                                 which each individually can scale out                                 and then you can put them together like                                 this right so each of this is a piece of                                 infrastructure this is for storage this                                 is for computing another storage layer                                 and we've sort of like build all this                                 tool box of stuff and if you put it                                 together we can build something which                                 skates but maybe isn't so fast in the in                                 the long run right because yeah okay I                                 come to the because later on okay so in                                 a way like usually when it is usually                                 works and technology is like you're in                                 some field there is a certain approach                                 and you optimize this approach and                                 optimize and optimize it until you come                                 to a point where you sort us have have                                 reached like the optimum which you can                                 achieve with a certain kind of approach                                 and you basically at a at a dead end so                                 I'm not saying big data is at the data                                 android but this specific approach can                                 only get use it that far and then what's                                 needed you know in order to go somewhere                                 else actually you have to jump outside                                 of this box or it's not really a box                                 with a blob you have to jump outside you                                 know by looking back at your problem                                 problem and realizing that there are                                 some things which you've always assumed                                 which which are not always true and that                                 then helps you to move over then for                                 example so before we we had like this                                 asset compliant databases right so the                                 like the traditional database with                                 transactions which were basically                                 designed in the time where you would use                                 them to really like keep track of money                                 and other things and that was really                                 important that like if like two people                                 are making deposits then they don't                                 interact and all kind of stuff but then                                 as they as we started to build all these                                 websites based on this database                                 technology it became clear that like not                                 everything here is really necessary for                                 particular people realize that the                                 consistency you don't need strong                                 consistent                                 see so if you use your database just to                                 store like user interactions or                                 something it's okay if some of the users                                 see the results only after                                              so so we dropped consistency and we made                                 this jump and came out at the with no                                 sequel in a similar way so we all had                                 these laptops and then like Steve Jobs                                 realized okay you don't for many                                 applications you don't really need a                                 keyboard you don't need all this                                 processing power or you even don't need                                 a file system right and then he said                                 okay so maybe instead we want something                                 like which is basically screen which                                 which we can carry it out anywhere we go                                 and then we came up with a new solution                                 right so this doesn't solve all the                                 problems which this also in a way it's a                                 it's a it's a change so you lose                                 something but actually for some                                 application it makes a lot of sense to                                 do that and then you end up with                                 something which is much better fitted                                 than the solution you had before okay                                 and in a way so this is maybe the first                                 main point of this talk so the thing for                                 there are applications out there and I                                 think reducer profiling is one of them a                                 reactance user we're actually actually                                 exactness is not necessary so in                                 classical big data so in particular                                 these exact aggregates like count an                                 average which we like which originally                                 came from the database you know all                                 these so you select from the table and                                 you say you want to count and then you                                 group by on the user and then you get in                                 the end out how often the user is                                 interacting with your website or                                 something so these are these basic                                 aggregate operations and that's that's                                 something we have since the                                          yeah there they are exact right in the                                 sense that they really return the number                                 of examples they are but in a way they                                 also restrict our abilities to deal with                                 lots of data because for like even for                                 events which are very not not very often                                 you still have to keep that count so if                                 you get that if you if you lose that and                                 you say okay from my application it's                                 not necessary just as you wouldn't use a                                 laptop to surf from your couch okay then                                 you end up with so-called stream mining                                 algorithms which is a class of                                 approximate algorithms so yeah okay yeah                                 sorry                                 to talk about this later on okay but                                 just to come back so right so this is                                 how it's done right now actually what                                 I'm saying is if you say I don't need                                 exact results you can actually replace                                 all of this with a solution which is                                 much better integrated and fitted and                                 has much higher performance so this has                                 also work and there are cases where this                                 is exactly what you need but if you go                                 the other way there are solutions which                                 I must simply and I'm going to talk                                 about how to do that now okay so just to                                 say why why is it that if you if you                                 deal with user data while is it that in                                 many cases you can actually live with                                 having a proximate result and the main                                 reason is that the dislike the activity                                 of users is usually look something like                                 this so there are a few users who are                                 interacting a lot and then there are                                 many who just you know show up once a                                 week or so and of course like this is                                 the part where you really want to spend                                 all your computing time and your money                                 to be able to react in real time because                                 these are the users who are most active                                 we know who who actually buying things                                 whereas these are users so right it                                 doesn't really make sense to to have                                 like a whole class that just all these                                 are results for these users or you can                                 just do it in the old way but then you                                 don't have to speed it up then that that                                 that you don't have to spend so much                                 money to speed it up okay yeah okay so                                 what is three mining so stream mining is                                 a class of algorithms which has been                                 developed in the mid-                                                   the questions of answering so-called                                 stream queries with finite resources so                                 you have an event stream which comes by                                 and you say you don't have enough memory                                 so neither in them dealer like ram or                                 disk to store all of the information is                                 in there but you're still interested for                                 example to count how often the items the                                 different items appear in the stream                                 right so if the event stream is a user                                 so you want to know which user has                                 viewed how many web pages or the events                                 are web pages you want to know like                                 pages on your website you want to know                                 which page has been you too often and                                 you say but i but i only have so i know                                 there                                 like I have                                                            have one megabyte of RAM to count it ok                                 and this these are these are algorithms                                 which right so extreme pastor data you                                 you have this analyzer which has a only                                 uses bounded resources and then you get                                 another result which is approximate but                                 usually comes with a theoretical                                 guarantee which says if I have this                                 amount of memory then the error i'm                                 going to make it smaller than that so                                 how do these algorithms look like so                                 luckily they actually quite easy to                                 understand so here's an algorithm which                                 counts these activities of a large item                                 sets of millions of users IP addresses                                 Twitter users whatever and the algorithm                                 works as follows so you have a fixed                                 table of counts so in this case I say I                                 have I only have room for six numbers                                 okay and these are the names of the                                 users who came to my website so and if i                                 have a new user coming by actually i                                 have two cases so either he's already in                                 the table then I just increased his                                 count so Paul goes from all those                                 actually Paul from                                                       user that which was not in there then I                                 take the the one which has the least                                 activity in here and I dropped him but I                                 take his count as a starting point for                                 the new one and then you can prove that                                 this is the like the the worst case of                                 times that Nico was already in the table                                 but then I removed him because somebody                                 else showed up right so it might be that                                 Nico had been in here yeah at most three                                 times in a way okay and then there's a                                 paper you know you can as you want to                                 really interested in the proves you can                                 all read all that but so the interesting                                 thing is it's actually it's quite a                                 simple algorithm which yeah which sort                                 of also does its own memory management                                 so I'm not saying you can you cannot                                 implement this using existing                                 architecture okay it's just you could if                                 you wanted to another algorithm which                                 you probably Oh towns are these count                                 min sketches and so the algorithm before                                 actually has like a count so here you                                 really have a list of all the people who                                 are on a website and you can later on go                                 in and say okay so tell me who is the                                 user who was your most often                                 so count min sketches work differently                                 so you don't actually store the IDS of                                 the things you count it's only a data                                 structure which you can query so if I                                 have that user I can ask so how often is                                 that user been here and then I get an                                 approximation for how often he's been                                 there but I cannot go in and say okay                                 tell me who's the like the most active                                 one it works like this so you have a                                 certain number of bins and then we have                                 a certain number so you have these end                                 times M bins and times and each row here                                 in this matrix corresponds to a                                 different hash function and if you have                                 a new entry you compute for each row the                                 hash functions and then you count the                                 the entries to which these different                                 hash functions point right so here I                                 have some new entry i don't know maybe                                 niko is coming by again so here it's the                                 the third pin the second the falls and                                 the third and then I count these things                                 up and when I query it actually I go I                                 do the same thing but then I take the                                 the count which the smallest count I get                                 in all of those and that way I'm                                 minimizing the collisions right so                                 another user might also end up counting                                 up this pin here but I do it like n                                 different times and therefore I get                                 better and better the more the more                                 memory I have okay it's very again like                                 a very simple it's not yeah it's i said                                 i'd say it's quite easy to understand                                 how it works so if you want implement                                 actually it's a bit different a                                 difficult how you get n different hash                                 functions for their techniques for that                                 but that's the basic idea here right and                                 this is very good if you write if you                                 really only want to query if you don't                                 want the trend but you just want to                                 query and you want to have like a better                                 approximation of all of those where it's                                 like in this case for the smaller ones                                 there i will usually quite not okay so                                 counting we already have like so instead                                 of doing these average things and                                 counting like in is in sequel you could                                 use a data structure like this and get                                 approximate count and not use a lot of                                 memory then the other thing but with the                                 sequel count is always that you usually                                 want to have activity over time frame                                 you want to say okay who the most which                                 page has been visited most often today                                 and then                                 had like this where clause where you                                 where you bracket the x terms of your                                 events and that's also something where                                 you can do a approximation right so if                                 you do this exactly you really have to                                 keep all the events in there so that you                                 can you know go through it or if you try                                 to optimize it a bit then you probably                                 you have a data structure where you put                                 in the event when it occurs and then you                                 take it out like a like a day later or a                                 week later but then you still have to                                 keep all the events in here or what you                                 can do is you can like dump the whole                                 table at certain intervals and then you                                 can look at like what is the count today                                 what is the counter day ago and then the                                 difference between those will be the                                 actual number of times this a conduct                                 occurred so by doing instead instead of                                 doing this you could also do do an                                 exponential decay that is so if the                                 counter-curse it starts with one but                                 then over time it will just decay right                                 in that way so each time you so event a                                 curse and then if no event occurs the                                 count will just decay and then when a                                 new one comes you add them up and then                                 you have a new thing which again the                                 case okay and this is not the same thing                                 as having these exact numbers but it's                                 again a good approximation over if you                                 use different time scales here you get                                 different approximations over these                                 counters with time scales okay and there                                 that sort of gives you like a way to                                 very quickly okay and the good thing                                 here is you really just need to store                                 the score and the last time stamp you've                                 seen here so just two numbers pantry                                 okay and you can you can combine this                                 with this data structure to get                                 something where you have you                                 automatically have a trend over a                                 certain time scale so you get the most                                 active users over the time scale you                                 have defined and that way you you                                 already to get an approximation to the                                 Select statement I talked about it in                                 the first slide okay there's one more                                 thing so usually so here right it's just                                 it's just entries I just seem simple in                                 its single names but it could also be                                 tuple so it could also be like not not                                 the vendors not just that Frank has                                 occurred but Frank Frank Cain                                 from that refer or something and he                                 looked at web that website so you could                                 actually have something where you the                                 event is not just a single object but                                 it's actually like a combination of a                                 page a referral and an IP address and                                 then you count how often this specific                                 combination has occurred and then what                                 you can do is you can have secondary                                 indices so this is already about getting                                 it's starting to get a bit more                                 complicated so we still like the sings a                                 single table with accounts the bounded                                 table but you also have like a binary                                 search tree like on the side of it which                                 keeps this thing sorry for each of these                                 columns you have like another entry                                 which says okay now and then what you                                 can do is you can say now give me only                                 the entries which have been referred                                 from google or give me only the entries                                 for the index page okay and that way you                                 are already getting to a point where you                                 cannot just have one count but actually                                 you have like a data structure which you                                 can query for all kinds of relationships                                 between your data ok so now using that                                 right you can't start to to store data                                 in a very compact manner to do all kinds                                 of analysis so for example if you just                                 want to store like an array in here so                                 you have counts for XD and a so it the                                 council XS or window course I've times D                                 or a two and A or III you put them into                                 the strand and then it's already it's                                 automatically sorted but that way it's                                 also like you can put a whole vector in                                 there like a whole matrix and whole                                 array and then the data structure                                 automatically only keeps the the largest                                 items okay if that's and if that's what                                 you're interested in then everything is                                 good so you can you know just count                                 stuff over very large scales and you                                 have something which automatically                                 focuses on the things which are most                                 active you can extend this and for                                 example if you have whole profile so for                                 X for example you're not only have one                                 count but you say so some so X might be                                 user and a might be some category he's                                 been looking at so this he has seen a                                 five times b                                                          why has seen a eight times and d                                         right and then you just sort these you                                 put these in and then you get this sort                                 of thing out here sorted by the you the                                 actual score and if you then do the                                 query on these column indices you will                                 like you will reconstruct these these                                 profiles yeah okay and then again if you                                 have more proof more data then you have                                 like a memory then it will start to                                 automatically discard the ones which are                                 very small and you sort of get an                                 approximation which focuses on the most                                 active ones okay so one more thing so                                 you can even like store matrix in there                                 so like the eyes are the the rows and                                 jays are the columns and if you store it                                 in here right you get the so                                          largest so I i equal                                                     entry so                                                             there at                                                                you can store the sparse matrix in here                                 and if you sort of like if you then use                                 the the industry's you have on the                                 columns of this matrix here you can                                 either get the row so too would be like                                 like in the second row you have a two                                 and one and the first row you have the                                   into                                                                     this column here okay so I'm just saying                                 so you can you can actually take a                                 sparse matrix which is something you                                 know you would use to sana and I'm going                                 to discuss in the middle towel would you                                 how that occurs in recommendation but                                 you can also take like a mathematical                                 data structure like that and actually                                 store it into one of these tables in                                 this data structures industry money data                                 structures and get an approximation at                                 all time which never grows in memory                                 which is always very stable okay so                                 actually we've built this thing called                                 spin drill can look at it there is a                                 demo version you can download which is a                                 exactly a real-time analysis engine                                 which has been built using these data                                 structures so it's based on a heavy                                 hitters counting exponential decay as I                                 explained and you get instant counts and                                 top K results over time windows just by                                 carrying the data structure it                                 all in memory in written and scada and                                 we also started to construct modules                                 based on this which are also going to                                 talk about a bit now so the interesting                                 thing is so so people always when I say                                 let's talk about three people always ask                                 me whether you can't can't you just do                                 that in storm right but I think that's                                 the wrong question so of course we could                                 also have implemented this a storm but                                 actually we could just implement these                                 things not using any of the big data                                 infrastructure's but we still ended up                                 with something that we can deal with                                 lots of data and millions of events just                                 on a single machine you know without                                 eating without eating scaling out and I                                 think that's the that sort of shows what                                 the power of this approaches okay so                                 real time user profiles so how would you                                 do this now actually it is it's quite                                 simple actually so like if the vendors                                 the event is that the user has looked at                                 a certain category you have a trend                                 which tracks users in categories and                                 then if you if you look for a certain                                 user using these indices you get the                                 profile out okay and then if you so you                                 update the corresponding combinations                                 here and just as I said period i said                                 before how you would store profile                                 information you can just get it out by                                 consulting these entities and this                                 doesn't take any additional computation                                 time you just get it out and so these                                 would be like for neuronal cars you have                                 that much inactivity for video games you                                 get this and so on so we actually have                                 that in production with a grave a pilot                                 project with the company who does                                 behavioral targeting so they they are                                 the ones who follow you over website and                                 set cookies and then they analyse the                                 website you look at and then they build                                 a profile of you too and then like if                                 you when you show weird you can actually                                 say okay show me an ad so this ad only                                 two people were interested in cars                                 something like that ok but so the thing                                 that the system they had was                                 specifically designed to integrate all                                 this information over time so that they                                 get a profile like a basic profile of                                 you and it could not deal with these                                 real-time things but this way so this                                 the way this works right now is they                                 have a big about                                                        of these nodes run a number of ruby                                 rayes on                                 Ruby in style in census which actually                                 do the analysis and then all these                                 packages these notes send by UDP the                                 events to stream draw and then for that                                 for each user you can get this profile                                 out for the last over the last day the                                 last week and then you can directly you                                 know make the comparison and see okay so                                 there is a category here which over the                                 last week of the day wasn't very active                                 but now it's very active so this is                                 something which is interesting which you                                 could use to show in something okay so                                 this looks like this it's very small so                                 there's a dashboard but the dashboard is                                 not really like it's just for us to see                                 whether it's working or not so this the                                 real value comes from having a rest                                 interface on the back where you can                                 within milliseconds for each user get                                 these profiles and make the decision                                 whether you there's something you want                                 to show this one or not so these are the                                 users down here and the activities and                                 these are these these fingerprints and                                 these heroes are show the histograms                                 over the different categories and for a                                 single user can look like this so these                                 are the categories chat information                                 entertainment media of a weekday an hour                                 and here are the the differences in                                 percentage points between those and then                                 you can say okay here like entertainment                                 media he's currently interested in that                                 so that show him that okay and so we                                 were a we were able to process about                                                                                                      machine which has like                                                 RAM okay so it's really not much sorry                                 we haven't even thought of so you could                                 even chart this very easily just by                                 users but it hasn't been necessary so                                 far so with one gigabyte of so with                                    gigabytes you can track about                                            like single counts which already gets                                 you a lot of information about the most                                 active users those which you want to                                 engage with I think this is really so                                 this is for us it's always very like I                                 think it's very impressive right so you                                 so you're not sticking together together                                 stuff like from this different in fact                                 parts which you could also do but even                                 if you just say okay I just have a                                 single machine which which also has                                 advantages like you don't have to deal                                 with just this is distribution with a                                 networking overhead you can build                                 something which I can actually process                                 assyrians amount of data okay right okay                                 so recommendation recommendations are a                                 bit more difficult of course so the                                 basic idea is so we're using this on a                                 website called zero in junkies which is                                 a german website of a TV shows and the                                 goal was that you for one TV show you                                 want to see here in real time what are                                 we like related TV shows down here so i                                 mean a recommendation is very hard i                                 know that ok but so the the good thing                                 about this specific approach was that                                 there weren't so many items you could                                 rank recommend fun from there were only                                 like four thousand TV shows in the                                 database and most of them of course are                                 not not active anymore so it's a i mean                                 recommendation gets very hard when you                                 really have to recommend for millions of                                 objects because the machine is no way to                                 tell what is related to what because the                                 data source parts but in this case if                                 you have like a few thousand items then                                 like a normal collaborative filtering                                 like approach works very well okay and                                 usually so recommendation like the basic                                 entity in all of recommendation is this                                 matrix here so on this axis you have                                 users so each row is a user and these                                 are items and every time a user looks at                                 a certain item you put a one in year or                                 your counted up or whatever okay and                                 then from this you can actually compute                                 the relationships between like the witch                                 if I have this item which item to                                 recommend based on the scores here by by                                 summing up the information here so you                                 take this item and then you go up you                                 look at the users who have also looked                                 at that item and then you aggregate                                 information like which other items has                                 looked at so it's like it's like what M                                 isn't always says right so people who                                 have bought this oh I've also bought                                 this they do something much more                                 complicated in the back but if you want                                 to implement it then this is its is this                                 like that's the basic the simplest way                                 of doing a recommendation okay normally                                 you would do this so first you have like                                 all your log data                                 you construct this matrix which is very                                 big like millions of users thousands of                                 items and then basically that you have                                 to do one big matrix-matrix                                 multiplication and there are ways to do                                 this and I do very easily but now the                                 interesting thing is you can do the same                                 thing again in a streaming fashion using                                 exactly these data structures if you                                 don't do this if you do it like online                                 so you store this is Hollywood you store                                 this matrix as a sparse matrix as a user                                 item friend and you store this matrix                                 again as an item item trend i said as i                                 described you know by having the the                                 coordinates and the count only in there                                 and no no                                                               conventicle you actually so this user                                 looks at this item then you just do the                                 update you know along this line using                                 the other items this user head looked at                                 so it's like instead of you know taking                                 a we a month of data and looking how                                 often like all the items have been                                 really used in relationship to everyone                                 you just do this for each user just as                                 he comes along right in that way your                                 approximate like the real thing again                                 the idea of approximation in a way where                                 you just have to do like a finite amount                                 of computation for each event and the                                 good thing is also that like if you                                 bring these this time for time scales                                 into account and actually this this                                 matrix is changing all the time so it's                                 really a reflection of which user looked                                 at which wich items in the last week and                                 if user behavior changes then the whole                                 matrix also changes right and the same                                 also for this side here so you have a                                 system which is not not only you know                                 doesn't require you to do these batches                                 but which still adapts over time and                                 reacts in a very quick point fashion so                                 and the nice thing is also that it's                                 actually if you want to build this it's                                 actually quite easy because it's really                                 like when you put a bit of JavaScript on                                 the web page then when the web page                                 loads it sends the click event via a                                 rest call to stream Doyle and then as a                                 result already gets the recommendation                                 and then you just rent                                 recommendation you can also of course                                 seed the recommender by sampling past                                 clicks so you just you know have if you                                 have lots of clicks you just sample                                 randomly from that and let it run off                                 for time as I said a dramatic adepts                                 over time yeah and there are so many                                 other things so you because trends are                                 very easy to compute yes thank you you                                 can also use all kinds of other                                 information like what are trending                                 trending if you would just usually want                                 to make a mixed of like user dependent                                 recommendations item-based                                 recommendations and trending items you                                 can also do that very easily because                                 it's all it all are in the end breaks                                 down to these data searches okay so yeah                                 I hope the one thing you saw was dad it                                 doesn't always have to be scaling right                                 so there are some applications are not                                 saying this works for everything but                                 they are important applications i think                                 or it's not really necessary that you                                 have the exact numbers especially if you                                 do like any kind of data analysis which                                 which has a large margin of error anyway                                 and if you go that direction there are                                 algorithms these three mining based                                 algorithms which at first look very                                 simple because you say they're just                                 counting just counting counts but                                 actually like in the way I described you                                 can use these to build data structures                                 to store all kinds of informations and                                 correlations between data and you can                                 create them quite effectively and solve                                 these problems like and especially if                                 you're so if you're like as large as in                                 the last talk maybe then you have no way                                 but really do it like the old way and a                                 big cluster but for I think there are                                 many many applications where you have                                 like a medium-sized website and you want                                 to have recommendations recommendations                                 would add a lot but you cannot really                                 afford you know                                                         cluster with                                                            this kind of thing can we get you a long                                 way yeah that's it thank you very much                                 so think of time for questions yep and                                 does this type of free commander so                                 first from self-fulfilling prophecy I                                 mean if it recommends something and then                                 the recommended the objects will be                                 clicked more yeah it's out this kind of                                 feedback oh yeah yes but I think they                                 like all systems more less suffer from                                 that so then it's more the question like                                 like if you know that people clicked on                                 a recommendation you know you should do                                 it in a way that you can distinguish                                 between those clicks and other cliques                                 and they're not feed those back or                                 something right so I have also talked                                 about it i know of course you need to                                 you know do the a B testing and                                 everything to see whether it works yeah                                 you can think okay that was one                                 so if I understand correctly the way you                                 parameterize your system is by defining                                 how many hash functions you have right                                 to have more precision or not maybe                                 think so depends okay now sovaldi for                                 this heavy hitters this top k our group                                 no I hash function that's just the size                                 of the table for these count min                                 sketches there to there the number of                                 bins and the number of hash functions                                 okay so what yeah usually it would                                 probably I don't know whether you have                                 to look at there's a formula for the                                 error and then you have to look what it                                 is but there there's a way how with like                                 with two hash functions you can actually                                 construct an arbitrary number of hash                                 functions or of hash hash values you can                                 use for that kind of algorithm grats on                                 really restriction okay is there a type                                 of algorithms that you parameterize your                                 system by the time of computation                                 instead of the use of memory okay so for                                 example well I i can accept two minutes                                 well it must be more precise ah yeah                                 sorry I mean in a way right as these                                 data structures get larger they also get                                 a bit slower but I think maybe you're                                 thinking more like in a kind of like                                 sampling or literature approximation and                                 then you stopped earlier just say the                                 time is all there may be no yeah I think                                 they're I don't already know but I think                                 so like that would be the kind of                                 algorithms where you start a computation                                 and as the computation goes on you're                                 already constructing your result right                                 and you could stop if you stop earlier                                 then actually there will be larger error                                 so you said like like yeah as I said                                 sampling kind of algorithms so you don't                                 you're not using this kind of algorithms                                 in in stream drill no oh you know okay                                 um here                                 um right here I already have a mic hi                                 how do you backup your data when you                                 have your memory data structures you                                 know you make sure your system crashes                                 do you think it periodically to disk yes                                 yeah so you want it okay sorry in what                                 system that could be anything I mean you                                 could just put it on disk so the the                                 good thing is solid it's like like it's                                 it's less than the amount of RAM you                                 have so the files are not really that                                 big so if you have like                                          gigabytes of RAM because of all the                                 indices and you wouldn't store this you                                 just or the raw data then maybe have                                 like three gigabytes of data you can                                 store and then you would do this like I                                 don't know once an hour and then go back                                 from there all you do like like a master                                 like you have to running in parallel and                                 one phase then the other takes over and                                 then you can you know that kind of thing                                 thank you recall yeah so could you tell                                 us more on how you right here hi hi                                 could you tell us more on how you                                 propagate these changes in the user item                                 matrix to the item item matrix in a                                 localized manner like only partial                                 changes to the idea meet with the                                 recommendation yes I think actually it's                                 likely as I described so what I mean                                 what like in the weird thing you also be                                 do a bit of normalization so that your                                 but it's basically like this so when                                 when there is an event you sort of look                                 at other users who have also looked at                                 that and then you extract like the                                 things they also looked at and then you                                 just add them up is it either you can                                 talk probably tour after the talk yeah                                 we're gonna measure easier yeah thanks                                 thanks for the hike actually yeah I                                 remember that you referred to the good                                 recommendations as up to date and the                                 ones that are already made based on the                                 recent activities Android users but I                                 you know it's a bit complicated because                                 sometimes I am just looking for coffee                                 makers but just for fun see something on                                 top of the page and about juicers or                                 whatever and just go there and bro                                 something but i am not really interested                                 in juicers and i don't need them so you                                 know it's a bit you know like a                                 complicated things to see what what we                                 have to recommend to users and also if                                 even you you make that based on the                                 recent activities but they really want                                 something else like some new things on                                 their past behaviors you know is i mean                                 how you can figure it out you know your                                 yeah that's always always difficult I                                 mean one thing you could do here you                                 could have the recommendation running                                 for different time scales right and then                                 you say like based on your behavior over                                 the last week this is the thing which is                                 probably interesting and already like                                 the user profiling so you would like if                                 you're looking at the juicer you would                                 see okay this is something you never                                 looked at but right now there's a lot of                                 activity there but if you stop then it's                                 also something which decays very quickly                                 so it disappears very quickly okay yes                                 it was it like often it's like so once                                 it's in the database then you're stuck                                 with it for a week or so yeah but I mean                                 in general so if you really wanted a                                 recommendation then I think you also                                 have to look at the items themselves                                 because there are some items you just                                 like by                                                                  you by very often and then I think that                                 way you can you can try to make it more                                 high quality recommendations in yes of                                 course here yet so it's about more like                                 dimensions that we are you have to take                                 you to an interac on because bull calm                                 also refers to good recommendations like                                 a most accurate owns or something like                                 that so I think we really need to fix                                 you know what we need from that in such                                 context or website resistance oh yeah                                 that visit you Thanks okay all right I                                 think one more okay                                 hi in the last presentation he talked                                 about multi-factor recommendations you                                 know that different kind of events would                                 affect the user profiles differently                                 here since it's your share and item on                                 facebook or if you oh yeah look into the                                 product for a long time if you buy it if                                 you just you know so you're different                                 kind of events that effects the                                 underlying recommendation model                                 differently would with your approach                                  Harriet streams will be able to handle                                  multi-factor rooms yeah I mean you can                                  no I mean you can for example you could                                  have a different record so I'm you can                                  have different scores like how important                                  it is that you did something but you                                  could also have like to recommend us                                  running one looking at social activities                                  and the other looking at that and then                                  in the end you do a combination of those                                  things so yeah so you could write say                                  okay if there is something like based on                                  your social behavior something I would                                  really recommend and I do that and I                                  also do a mix of like the thing I think                                  you would want to have based on what you                                  clicked on on our website so they are                                  infinite ways of I mean this is not                                  something that's in there so the signals                                  in there is this basic thing I had here                                  so if you would put it in production                                  than you yeah I would have to think                                  about different ways of using it and                                  then integrating it in the in the end                                  thank you thanks okay yeah thank you                                  very much                                  you
YouTube URL: https://www.youtube.com/watch?v=pYRdK8O2GMs


