Title: Berlin Buzzwords 2014: Roman Shaposhnik - Apache Giraph: start analyzing graph relationships (...)
Publication date: 2014-05-28
Playlist: Berlin Buzzwords 2014 #bbuzz
Description: 
	Roman Shaposhnik talking about "Apache Giraph: start analyzing graph relationships in your big data in 45 minutes"

The genesis of Hadoop was in analyzing massive amounts of data with a mapreduce framework. SQL-­on-­Hadoop has followed shortly after that, paving a way to the whole schema­on­read notion. Discovering graph relationship in your data is the next logical step. Apache Giraph (modeled on Google’s Pregel) lets you apply the power of BSP approach to the unstructured data. 

In this talk we will focus on practical advice of how to get up and running with Apache Giraph in minutes, start analyzing simple data sets with built-­in algorithms and finally how to implement your own graph processing using the APIs provided by the project. We will then dive into how Giraph integrates with the Hadoop ecosystem projects (Hive, HBase, Accumulo, etc.) and will also provide a whirlwind tour of Giraph architecture. 

Read more:
https://2014.berlinbuzzwords.de/session/apache-giraph-start-analyzing-graph-relationships-your-bigdata-45-minutes-or-your-money-back

About Roman Shaposhnik:
https://2014.berlinbuzzwords.de/user/214/event/1

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              all right hello thank you for coming to                               my talk today we will be talking about                               apache giraffe and this will be a                               presentation you know which is a                               combination of sort of graph processing                               and big data a lot of times you know you                               can actually do very interesting things                               with small crafts but you know giraffe                               is particularly good when you actually                                have sort of big data in mind so before                                I actually start talking about giraffe                                itself let me just introduce myself so                                this is me I've been at the SF you know                                for quite some time so doing you know                                various things used to be at closed era                                doing Hadoop you know before that at                                Yahoo and I actually have two pitches                                you know so I'll just get them out of                                the way we will continue with the                                presentation so I am also one of the                                three authors in the you know upcoming                                book called giraffe in action from                                Manning so you know support hungry                                authors you know by our book let's pitch                                number one and pitch number                                      working for the company called pivotal                                we're pretty interesting startup company                                you know doing a big data and                                platform-as-a-service things and i'm                                really hiring so i'm actually building a                                team of people who can help me figuring                                out next generation platform so yeah if                                you're interested you know catch me                                after the presentation and that was my                                second pitch so let's actually get right                                to the giraffe business so in this                                presentation I will actually try to talk                                a little bit about you know where it all                                came from and sort of backgrounds of you                                know Hadoop and big data processing how                                it relates to graphs how you know graph                                workloads actually tend to be different                                compared to MapReduce will introduce                                this thing called bulk synchronous                                parallel you know for those of you who                                don't know it you know it will be a                                primer on what it is and then we will                                talk about in a browser I've implemented                                and I will also try to show you code you                                know samples so my real goal that at the                                end of this presentation you can                                actually start writing your graph                                processing applications in top of                                Sheriff because it's really quite simple                                so let's get down to business as all you                                know of us now doc cutting was the                                original implementation                                of Hadoop Hadoop consisted of HDFS and                                MapReduce but he duped really came from                                google papers so what Doug did he                                basically read two papers one on gfs in                                a Google file system and another one on                                MapReduce and he implemented both in the                                free and open source java based Hadoop                                thing so if you think about you know                                where gfs and MapReduce sort of what                                kind of constraints or what kind of you                                know use cases they were aiming to solve                                at Google gfs was really meant to be you                                know fully highly distributed so it's                                kind of like scale-out storage it was                                meant to be replicated because you know                                it was again them to run on the                                commodity hardware which fails all the                                time and in order to achieve the first                                two they basically had to make a                                compromise by not making by not making a                                deposit file system so gfs actually                                happens to be an on politics file system                                one of the biggest limitations in gfs                                and you know hence HDFS you cannot                                really for example seek in the file and                                then right you know you can only keep                                writing to a file you know maybe you can                                keep a pending depending on the version                                of Hadoop but most of the time your data                                is you know sort of right and then read                                multiple times and read is actually                                optimized for streaming so if you're                                trying to seek in the file all the time                                again not a good idea a streaming and                                you know scale out is what gfs and HDFS                                are really good at on top of the file                                system google and you know hadoop by                                extension implemented the compute                                framework called MapReduce with a big                                inside that you know you don't really                                have to get your data to where the                                computation is you know if your                                 computation is portable and that was                                 part of the reason you know Hadoop aziah                                 actually implemented in Java you can                                 actually push your computation to where                                 your data is because you know with                                 Charlie doesn't really matter you know                                 where you compute as long as there is a                                 JVM on that node so MapReduce was meant                                 to be distributed it was also meant to                                 be batch oriented in a sense that you                                 would actually expect your jobs to take                                 a long time and produce sort of a final                                 result and at the end of the day you                                 know the algorithm that you would run on                                 MapReduce by and large tend to be                                 embarrassingly parallel                                 well so you can share the computation as                                 wide as possible and it is only during                                 the reduced phase where you would                                 actually combine you know some of the                                 results from different computational                                 units so as you can see I mean there are                                 quite a few limitations but amazingly                                 enough the map videos has been a really                                 successful framework so there is all                                 sorts of you know computational                                 paradigms that map into it well                                 unfortunately referencing is just not                                 one of them so one size really doesn't                                 fit all in that reduce you know you have                                 this key value approach so map is how we                                 get the key so you extract the keys from                                 the data and that's the math phase then                                 there is a shuffle phase essentially you                                 know soaring the keys and the reduced                                 phase is you know a single bit of code                                 gets to see all of the values                                 corresponding to a single key in one go                                 so that's you know the reduce phase but                                 in graphs it's not really even clear                                 what would be you know a natural sort of                                 key you know well I guess you can come                                 up with some but it doesn't really lend                                 itself easily to that type of processing                                 so MapReduce also has this idea of                                 pipelining in a sense that once a phase                                 of map reducing you know map shuffle                                 reduces done the only place where you                                 would store result is an HDFS all of the                                 you know very at times complex Maori                                 states that you've created but you know                                 for doing all these three phases is now                                 gone so if you actually have you know                                 multistage MapReduce job you basically                                 will keep going in and out of HDFS all                                 the time for intermediate results which                                 is a little bit like you know imagine if                                 UNIX didn't have pipes right you know                                 you would basically have to create you                                 know temporary files all the time you                                 know not really efficient and especially                                 if you create again non-trivial memory                                 state you know not really good idea and                                 again MapReduce is a very particular API                                 for working with your data there been a                                 couple of attempts to actually map graph                                 processing into MapReduce the craziest                                 attempt I've seen was at Facebook so                                 before sheriff existed at Facebook they                                 actually needed to you know analyze the                                 social graph and the only thing they had                                 was high so high with a sequel on top of                                 Hadoop so there was this really smart                                 dude who used to do c++ who came up with                                 this idea that you know literally all                                 graft                                 could be mapped into a linear algebra                                 you know you can basically do pretty                                 much anything using you know a jason c                                 matrix and you know operations from the                                 adjacency matrix but when he came up                                 with Dan was he could actually map                                 linear algebra into sequel you know in a                                 very very convoluted way but it worked                                 and you know the kind of sequel that he                                 showed me which is like I haven't seen                                 that type of sequel you know being                                 produced by like you know hibernate you                                 know it's really mind-boggling but                                 apparently it works so they actually                                 started analyzing their social graph way                                 back well before you're off using hive                                 but of course you know giraffe is a much                                 more natural fit so let's actually talk                                 a little bit about you know why is it                                 different with graphs compare it to the                                 your typical big data use case so when                                 you store unstructured data in HDFS and                                 that's what you know it's really all                                 about you talk about tuples right and                                 these samples are essentially you know                                 points in a some kind of a space you                                 know solution typically space and it                                 can't describe you know those tuples can                                 describe your customer data your product                                 data your interaction data but when you                                 start talking graphs what you're really                                 doing you're basically coming up with                                 connection between those doubles and two                                 things happen first of all unlike in                                 most traditional graph databases you                                 actually do just that you come up with                                 those connections upfront you might not                                 even know that you know two tuples might                                 necessarily be connected by some kind of                                 an edge it may actually be knowledge                                 that you somehow you know derive later                                 on by the time that all the data is                                 stored in your HDFS and as we all know                                 you know Big Data is all about sort of                                 growth but if your data size growth                                 linearly your connection sort of edge                                 you know number actually grows                                 exponentially so even storing all of the                                 edges would not be a good idea you                                 actually have to come up with what the                                 edges are you know most of the time sort                                 of on the fly and that's what really to                                 me is different about applying graph                                 processing techniques to Big Data right                                 you know you don't actually up front                                 know what the connections are you                                 actually extract them sort of based on                                 some kind of you know model and it may                                 very well be that you know one run on                                 the same data will actually come up with                                 one set of edges and                                 front run on the very same set of data                                 would come up with a different set of                                 edges and a good example of that would                                 be you know suppose you are trying to                                 match customers to products right so you                                 might actually have a social graph of                                 your customers and that's just fine you                                 essentially connecting all the tuples                                 you know between the customers but then                                 you know the next iteration could be you                                 could actually be matching customers to                                 the products and then you know the very                                 sort of same data set would produce you                                 know different drafts so what kind of                                 challenges do we actually have while                                 doing it well again data is dynamic so                                 there's no way of doing sort of scheme                                 on right the algorithms that we tend to                                 run again tend to be explorative and                                 iterative like I said I mean you might                                 actually load up the data you know come                                 up with one set of you know edges but                                 then it will sound like oh man I                                 actually need a different set of edges                                 and it would be really wastefully if you                                 actually had to reload all of the data                                 you know just like mapreduce makes you                                 do so we have to do something else and                                 you know you can actually use graph                                 database which is one of the things that                                 you know I believe there's a few our                                 talks in this track and you know they're                                 good in the sense that I cannot really                                 use them that much but you know my                                 friends tell me that you know if you                                 really need neo                                                         it where to moving on it's it's                                 available their benefits you know to me                                 the biggest benefit is that it's a                                 self-contained system right you know if                                 you install it if you maintain a time in                                 it kinda gives you what you need right                                 but the short coming is it is a                                 self-contained system so like the                                 internal state of that system can only                                 be gotten you know from calling the                                 api's that that system provides so                                 unlike what you're off is doing where it                                 basically just looks at your                                 unstructured data wherever the data is                                 you actually have to feed the data into                                 the graph database first and only then                                 you can work with it but again the flip                                 side is you know it's highly optimized                                 for that type of workload so there is                                 all sorts of interesting things that                                 they do it really depends on your use                                 case if you can define a schema for your                                 graph up front I mean there's no reason                                 not to try you know something like this                                 but if your scheme is not actually                                 definable giraffe is pretty much the                                 only game in town for now actually it's                                 not I mean there is also a project                                 called Apache Hama                                 but I'm not sure how active it is it                                 used to be pretty active but now that                                 Facebook invested in giraffe basically a                                 few engineers just full time you know                                 optimizing and working in it I think                                 Sheriff is way ahead of you know most of                                 the graph processing tools on top of                                 Hadoop Sookie insides that you have                                 brought to this problem is well let's                                 keep a state in memory for as long as we                                 actually need it so there is absolutely                                 no reason to throw away that state you                                 know that you know what MapReduce does                                 we can actually keep it in memory and                                 that makes it interesting from the                                 implementation side because you're off                                 actually happens to be the first                                 framework that can leverage this next                                 generation who do scheduling and you                                 know provisioning capability called yarn                                 where it's not really about fitting                                 everything into MapReduce you actually                                 can have different frameworks running on                                 top of the very same could do so                                 MapReduce just being one of them giraffe                                 would be a different framework we are                                 leveraging HDFS as I said you know the                                 repository from structured data but the                                 biggest sort of insight is how do we                                 actually do the computation and that's                                 you know BSP so what's BSB BSB stands                                 for bulk synchronous parallel and it's                                 really a very simple idea of how you can                                 achieve a middle ground between                                 something as restrictive as MapReduce                                 where there's absolutely you know few                                 points of communicating any kind of data                                 between you know workers again if you                                 think about it I mean the only two                                 points where communication happens is                                 once mappers are done you know the data                                 needs to be sorted that's the one you                                 know one point of communication and once                                 the data is sorted that data needs to be                                 streamed to reducers and that's the only                                 other point of communication so suppose                                 we want you know to have workers be able                                 to communicate with each other so a                                 different extreme would be to let them                                 communicate absolutely freely you know                                 at any time anything can send a message                                 to anything else and the only thing that                                 you are waiting in such a system is you                                 know a bunch of deadlocks you know those                                 of you who've done MPI know what I'm                                 talking about ah so what's the middle                                 ground well bulk synchronous parallel                                 you know BSP offers the middle ground                                 where you actually partition your                                 communication on computation phases                                 basically separating them by by barriers                                 so you have as many local processing you                                 know within the unit's done as possible                                 but once all of the processing is done                                 that's when you hit barrier number two                                 right so a barrier number one is when                                 everything starts computing theory or                                 number two is when everything is done                                 doing the local computation and between                                 barrier number two and beer number three                                 there is an absolutely unrestricted                                 communication happening you know                                 essentially Massachusetts and but there                                 is no computation happening at all so                                 the computation will start happening                                 once all of the messages get delivered                                 and the computation will take into                                 account the messages that were delivered                                 to the local state but while the                                 communication is in flight no                                 computation has happened so you                                 basically have this you know kind of                                 like kitchen giving States right you                                 know you do computation the messages                                 then computation the message is the                                 computation and messages and it actually                                 happens to be a rather nice model for                                 graph processing so let's actually try                                 to apply two graphs so this is an                                 example graph of my Twitter you know                                 network so my twitter handle is air                                 harder and I'm falling you know apache                                 software foundation obviously so there                                 is a friend of mine Constantine who I                                 you know we both follow each other and                                 he also is you know following SF but SF                                 is not following anybody so with BS be                                 applied to graphs you know at every                                 given vertex is where you have your                                 local computational sort of power right                                 you know you actually have to think like                                 a vertex so what does a vertex think                                 well verdicts nose and things you know                                 it's local state basically has control                                 over any kind of you know local state                                 and memory it can create just happens to                                 be a java thread right I it knows its                                 neighbors so the verdicts knows you know                                 who who the vertex is connected to and                                 it can traverse the network of the                                 neighbors but it does not necessarily                                 know who is connected to the verdicts so                                 you can basically get all of the                                 outgoing edges but you don't know                                 incoming edges so verdicts can also send                                 a message to just about anything you                                 know within the network so there does                                 not need to be a connection for a                                 message to arrive from point A to point                                 B you can just send arbitrary messages                                 verdicts can declare that it's done and                                 amazingly enough                                 verdicts can actually mutate the graph                                 topology so it would not be out of the                                 question to start with absolutely no                                 vertices and just build out the complete                                 graph in memory you know based on                                 certain criteria because you know a                                 single products can't just build you                                 know portions of the graph that topology                                 is considered to be sort of most of the                                 time at least is considered to be kind                                 of like messaging so it will be                                 available to the workers you know for                                 the next step so they will not start                                 computing immediately so these five                                 points is the fool giraffe sort of API                                 right there basically BSP happens like I                                 said at the level of individual vertices                                 then there is a bunch of you know                                 message delivery and it keeps keeps                                 going on so without further ado here is                                 the first you know you're off hello                                 world so what this little snippet of                                 code does well first of all it shows                                 that Java makes you right tons of                                 boilerplate but you know we'll get to it                                 in a minute but you know the real bit                                 happens right here so we basically have                                 extended the basic computation abstract                                 class which is an entry entry point into                                 writing anything in Sheriff you know and                                 the only method that we actually have to                                 refine to make it work is compute so                                 compute is what happens at the level of                                 fish individual verdicts so we didn't                                 compute like I said I mean we can do I                                 don't know println so we can train the                                 our ID you know who we are and then we                                 can iterate over all of the edges you                                 know that are outgoing edges from this                                 vertex you know printing the IDS of the                                 neighbors we're connected to so this is                                 basically the smallest I guess you know                                 think that you can do with your off                                 pretty simple actually did get cut out                                 so at the end there is also a halting                                 state so hopefully on the further slides                                 it will not get cut out so all of these                                 variables all of these type variables                                 that you actually have to write their                                 very important and they define you know                                 this sort of my t                                                      first of all you basically have to                                 declare a type of what is the vertex ID                                 so vertex ID is what you use to actually                                 reference the verdicts if you're trying                                 to                                 the message for example if you're trying                                 to connect you know yourself or some                                 other verdicts you actually have to know                                 vortex ID this is how you reference them                                 then products actually happens to have                                 the data you know piece of data                                 associated with it and that's the second                                 type variable then you have to define                                 what is the piece of data associated                                 with each edge third one and fourth one                                 what is the type of message that                                 vertices will be sending to each other                                 so you know for all together now of                                 course if you don't if you are not                                 interested in you know some subset of                                 these api's you can just you know                                 declare it now writable and speaking of                                 rider balls the only thing that in this                                 that needs to be something other than                                 just writable is the first one vertex ID                                 so it also needs to be comparable uh why                                 because you basically your grant gets                                 partitioned based on vortex IDs and the                                 vertices get assigned to workers within                                 your giraffe application based on the                                 partitioning that happens at the level                                 of the first type variable so let's                                 actually take a step back and again like                                 I showed you a bit of code but like what                                 what is the input for that bit of code                                 so where do ages and you know vertices                                 come from when you actually start your                                 giraffe application well like I said I                                 mean you don't actually even have to                                 define graph a front because you can                                 actually build it in memory so you can                                 you know your first sort of super step                                 you know your first BSP step could be                                 you just invent the graph right you know                                 based on some criteria or whatever you                                 just build in the memory and the next                                 one the next step will have that apology                                 as the working topology of the graph so                                 that's one but it's not really                                 particularly interesting what's really                                 interesting is you know how you can                                 extract the verdicts and edge                                 information from the raw data that you                                 have a new HDFS and for that you know                                 giraffe follows a very Hadoop like                                 approach ah so it defines edge input                                 format and verdicts input format you                                 know there is a built-in support for                                 most of the storage subsystems that                                 happen to give ailable on Hadoop so                                 there is what is the one for HDFS there                                 is one for each base and accumulo there                                 is a you know backend for Gore ax                                 high-wage catalog you know all the usual                                 bits and pieces of the Hadoop ecosystem                                 but you can write your own I mean that's                                 that's that's absolutely possible so I                                 mean I know that somebody wrote one for                                 Cassandra and that should fine so the                                 way it actually happens is I suppose you                                 have you know some data in HDFS and for                                 our simple example the type of data that                                 I decided to use and the type of input                                 format that i decided to specify is                                 essentially an adjacency matrix you know                                 just encoding the first integer here is                                 the vertex ID and all the integer is                                 following it are the verdicts ideas of                                 the neighbors for this guy so suppose                                 you have two files in HDFS you know one                                 looking like this and the other one                                 looking like that you give it to giraffe                                 when you start it up input format kicks                                 in input format builds the graph                                 topology based on the vertex idea that                                 graph topology gets partitioned between                                 different workers you know the workers                                 get assigned to different nodes bunch of                                 computation happens up until the point                                 where we don't have any messages left to                                 be processed if we don't and if all of                                 the vertices voted to halt so you know                                 they are not doing any computation the                                 entire giraffe application exits and if                                 you specify output format at that point                                 you can actually have your state of the                                 graph written you know being flashed                                 back into whatever storage you decide to                                 use what's interesting is you don't                                 actually have to use you know the same                                 storage so in my example I'm taking data                                 from HDFS and I'm putting data back into                                 HDFS but this could have been HBase and                                 that is you know fine to be HDFS so you                                 can have any combinations so what really                                 is what really is happening you have                                 some kind of you know storage you have                                 some kind of storage for output and                                 these little guys they actually happen                                 to be either yarn containers uh you know                                 this is next generation Hadoop Hadoop                                 two or the hack that refuses to run on                                 just pure Hadoop is those can be mapped                                 on the jobs so you know here's a cool                                 pro tip of how you can abuse Hadoop                                 you don't actually have to generate any                                 Keys you know in your map where you can                                 just sit there right and that's                                 essentially what you're of that so it's                                 it's forever and for as long as it does                                 it you know Hadoop is happy because like                                 you know something is running before we                                 move along let's hash again just recap                                 you know what is the vertex view so the                                 vertex U is rather simple messages come                                 in messages come out and by the way the                                 message data you know for an interest of                                 full disclosure could have different                                 types for incoming and outgoing messages                                 there are very few really obscure use                                 cases where it's useful so for all of                                 the practical purposes you can just                                 assume that you know message data one                                 type and message data to type are the                                 same but other than that again you have                                 vertex data associated with your                                 verdicts you have vertex ID and you can                                 outgoing a sort of arrows you know edge                                 each labeled with edge data so for our                                 next example I think I have a little bit                                 of time left so far the next example let                                 us actually do something interesting so                                 we all know that the only difference                                 between Twitter and Facebook is that one                                 uses you know directional drafts and the                                 other one doesn't but it's pretty easy                                 to turn one and two the other and it's                                 actually necessary operation because                                 giraffe by default supports directional                                 graphs right so if you want to simulate                                 in you know something else you can just                                 build additional edges you know into                                 your graph and that's exactly what we                                 will be doing here so again here's you                                 know compute that we are redefining you                                 know the same way we did for hello world                                 first of all this is a very common                                 pattern in giraffe application first of                                 all we try to see what is the number of                                 the super step super step is that block                                 you know of bsp processing and every                                 single blog gets numbered starting from                                 zero all the way to you know application                                 exits so if we happen to be just                                 starting so you know the super static                                 will                                                                     all the edges that we have thus                                 notifying them that we are connected to                                 to them right because as I said we do                                 know all of our neighbors but neighbors                                 do not know that we are connected to                                 them so if we want to build back                                 reference you know sort of back edge                                 into the graph we actually have to know                                 if I the neighbor that we are connected                                 to them and once this is done we                                 basically vote to help but because there                                 are messages in the queue the                                 application doesn't exit so we go to the                                 super step one so super step one                                 basically happens over here so we get                                 all of the messages that were sent to us                                 from previous super sub zero we iterate                                 all of those messages and what those                                 messages tell us is who is connected to                                 us so the only thing that we need to do                                 is we need to build an edge in a vertex                                 add edge to be connected back to that                                 guy so essentially you know if you take                                 this node what it will do it will send                                 the notifications to this one and this                                 one and the in the next super step you                                 know those guys will kick in they will                                 process the messages and we'll build the                                 back reference you know so that's pretty                                 simple running giraffe is actually                                 pretty simple but before but it's a                                 little bit time-consuming so let me                                 actually let me actually show you what I                                 will be doing here so the only thing                                 that you need to do is you would need to                                 set up environment and here I basically                                 have Hadoop you know i just downloaded                                 it from you know a apache web site you                                 know download a giraffe release you know                                 this is interesting because not a lot of                                 people realize that you can actually run                                 Hadoop without any kind of cluster or                                 anything running at all so there is a                                 mode in Hadoop called local mode where                                 everything happens within the same jvm                                 it's kind of the same processing but you                                 are limited to just one single mapper                                 well one single reducer for that matter                                 but your app doesn't care because you                                 know we can just like map everything                                 into the single processing unit an                                 interesting way of considering it is to                                 point Hadoop Commodore at the empty                                 subdirectory which happens to be this                                 guy because by default local mode is                                 what to do this it's only when you                                 actually configure it to do something                                 else it will do something else but by                                 default it does local mode so with that                                 bit of configuration in place the way                                 you would actually execute giraffe is                                 something like the                                 and I will go over you know what every                                 single line in here means and does but                                 for now let's just let just started so                                 you can see you know who do kicked in                                 and unfortunately this process will take                                 a little bit longer than you know I                                 might like and I will tell you how to                                 avoid this in a minute so let's get back                                 to slides so yeah Apache Hadoop                                        what I'm using you know Apache giraffe                                                                                                      this release so like hopefully we will                                 push it down soon so it will be released                                 artifact but for now I really highly                                 recommend using the snapshot you know                                 the previous release of giraffe is good                                 but this one had tender loving care from                                 facebook so it really is better than you                                 know what we had on                                                     the build system you know for sure off                                 so if you want to tinker with it you                                 know maven is highly recommended and                                 this is a subtle points so sheriff                                 actual requires jdk                                                      change that for this release but at                                 least it used to and what it really                                 means is that if you actually have a                                 fully distributed cluster you actually                                 have to run jdk                                                       right so giraffe you know being a                                 MapReduce application actually would be                                 executed within the same jvm that you                                 are running all over your cluster and if                                 that JVM happens to be jadek you know                                 JVM six it will not work so like I said                                 you know just downloading things and you                                 know setting environment variables is                                 all you need to do to actually kick                                 started if you are using maven and if                                 you're just developing your project I                                 mean the only dependency that you have                                 to declare is unsure of core and on                                 Hadoop core that'll they'll get you                                 going right away and here's how I ran it                                 so basically sheriff is a command line                                 utility you give it the jar file you                                 know that contains the code that you've                                 developed you give it the name of the                                 class that has the compute method that                                 you want to execute on your graph you                                 give it an input path in whatever file                                 system is configured and again an                                 interesting aspect of HDFS is that HDFS                                 doesn't really have to be h DX s right                                 you know kadu can actually work with                                 your local filesystem just fine that's                                 what I'm using here                                 a local sort of mode of hadoop would be                                 absolutely fine to just use your local                                 files but this is essentially you know                                 pass to all of my all of my files this                                 is the input format that i'm specifying                                 and unfortunately this is the bit where                                 giraffe gets you know kind of clunky                                 because this input format have actually                                 it has to match the code you know this                                 for type variable that you know you put                                 in your code and if you mess it up here                                 it will not give you a helpful message                                 message it will just fail so you know be                                 careful but this input format you know                                 produces the edges and you know nodes                                 and exactly the types that we've encoded                                 so this is this is an interesting bit                                 you know you have to you have to specify                                 that the number of workers is one                                 because that's all we get in local mode                                 but you can specify as many workers as                                 you want so you know typically if you                                 have a big loop cluster I guess this is                                 better now you can specify you know                                 typically like you know slightly less                                 than a total number of nodes if you want                                 to fully utilize utilize your cluster so                                 this is your responsibility sheriff will                                 not figure out how many workers you want                                 in this you know distributed network to                                 be instantiated and these are settings                                 for sheriff itself so this is the only                                 really useful one if your half split                                 master worker we setting it to false                                 again because we are running in the                                 Hadoop local mode we only ever get a                                 single mapper so we cannot really split                                 different functionality because your app                                 has you know different services we                                 actually have to run everything in a                                 single mapper and off you go it's pretty                                 much it again the troubling bit is that                                 you know i started at some time ago but                                 it's still going on so it like it takes                                 you know up to a minute or two even on                                 the local data which is really tiny data                                 set and the reason for that is much more                                 about than giraffe so hopefully we will                                 you know change some of it but the best                                 way to be as productive as possible                                 developing on sheriff it actually goes                                 back to how you would unit test your                                 application you know your sheriff                                 application and in giraffe if you write                                 something like this and you will use                                 internal verdicts run                                 what it will do it will basically run                                 the same code on essentially a mocked                                 environment right so you will give the                                 graph seed which is a array of strings                                 you know simulating your input data you                                 will expect you know iterable essential                                 of strings that will be the simulation                                 of your output data and you can                                 configure your f you know however you                                 want you know just like I did on the                                 command line using the configuration                                 object so if you have this bit of code                                 this your you know let's say unit test                                 right you know you can totally step                                 through it you know in your ID you don't                                 actually have to bootstrap anything you                                 know from khoob you know it's really                                 easy way of how you can like just really                                 start you know developing on giraffe but                                 of course you know when it's time to                                 actually execute it on Hadoop you will                                 still go through the same set of steps                                 that hopefully finished by now no it's                                 not this is really taking a long thing                                 and I think I'm actually out of time so                                 I will not cover some of the more you                                 know advanced use cases yeah so yeah so                                 what it's really doing and again like                                 I'm saying anything as way more to do                                 with Hadoop than with giraffe so like if                                 you look into it it's basically copying                                 the jar files into the distributed cache                                 and of course you know the local mode it                                 has absolutely no business doing it                                 because you know everything is available                                 on my local machine anyway and because                                 again you know for real Hadoop                                 deployments this step doesn't really                                 have to be all that performant you know                                 nobody really you know looked into you                                 know optimizing it so i think they are                                 reading like bite by bite you know each                                 jar file or something you know so again                                 it's just to do copying a bunch of data                                 from point A to point B without                                 absolutely any need you know whatsoever                                 for doing it so but you know once it's                                 done the giraffe application will                                 basically kick in will print you know a                                 bunch of messages so you know the whole                                 point is just to demonstrate how you                                 could actually run it on your laptop and                                 if you're interested in you know making                                 Hadoop nicer in this setup you know well                                 help us you know some patches um so yeah                                 questions                                 okay hello okay any questions yeah I                                 think over the right extra didn't quite                                 get how do you collect data from the                                 compute method because you you showed                                 how you send messages but how do you                                 finally collect them to HDFS or whatever                                 yeah that's that's this one this slide                                 let me show it to you again so basically                                 just a good yes so this is this step and                                 it's actually optional it may very well                                 be the case that you are absolutely not                                 interested in the state of the graph at                                 the end of the it you know at the end of                                 the entire run you may be looking for a                                 maximum value of some kind right so                                 maybe the whole point of your graph                                 processing is to find you know some                                 local maximum and once you've done it                                 like that's the output of your job so                                 like there's absolutely no need to                                 actually output anything that has to do                                 with your graph but you can do that and                                 that's how you get the data out of the                                 giraffe application and how would you                                 collect the maximum then that's actually                                 the slides that I had to skip so sheriff                                 has the notion of aggregators so it's                                 kind of like think of it as a global                                 variable that you can keep a throughout                                 the run and the state of the global                                 variable can be output just like you                                 know at any random point and in fact                                 what you could also do if you're                                 actually doing you know a bit of                                 heuristic you can actually look at it                                 and say like well maybe it's not you                                 know the optimal one but it's good                                 enough so I'm quitting the entire                                 application because it basically fits                                 into some kind of you know range so you                                 can output it and on top of that you can                                 even quit the entire application at that                                 point it may be the last question what                                 happens if the no didn't vote to out and                                 but it doesn't receive any messages                                 later on it keeps running essentially oh                                 yes so the method will be called anyway                                 yeah                                 Thanks so with the bsp model right                                 you're super step is only as fast as the                                 slowest task within that super step how                                 does giraffe determine how to split the                                 individual tasks so that they complete                                 at roughly the same time yeah so like I                                 said there is a partitioning based on                                 the initially based on the verdicts ID                                 so the partitioner is pluggable so the                                 default one is just you know cash-based                                 you know pretty simple you can you can                                 write your own implementation and you                                 know you can partition based on any                                 criteria giraffe actually does rebalance                                 so that's what you have to keep in mind                                 so if you look at                                                       these guys get assigned you know like                                 this for one super step doesn't mean                                 that they have to keep you know being                                 assigned you know like this for the next                                 super step before you're off transitions                                 to the next super step it actually does                                 rebalancing you know based on whatever                                 criteria you can built in and if you you                                 know start detecting that you know you                                 have some slowpokes in your execution                                 you can notify the you know rebalance ur                                 through your partitioning scheme that                                 you know repartitioning is necessary                                 okay any more questions wellsir yeah                                 yeah                                 high somewhere early in your slide you                                 had a slide about graph databases yes                                 isn't it a bit of strange comparison                                 because graph databases artist or graphs                                 and and this is to process graphs so for                                 just to give an example to to find all                                 incoming edges for a note is only now in                                 l                                                                     whereas in in giraffe you'll have to                                 actually go through all the nodes to run                                 all this the things so isn't a bit like                                 PHP in MySQL what is better there is                                 it's not grub databases to store grafts                                 and this framework is to process graph                                 on large scale well I mean if the only                                 thing that graph databases did was you                                 know they would basically let you store                                 and retrieve the node then I would agree                                 with you right you know but it actually                                 it actually I'll execute the queries                                 right it actually lets you do the                                 queries so once you start doing the                                 queries I mean it's processing right                                 it's kind of like you know saying sequel                                 is not Turing complete well it probably                                 isn't but it still doesn't prevent                                 facebook from essentially using sequel                                 to the graph processing so yes I mean it                                 may be a strange comparison you know                                 once you kind of like just visualize it                                 and try to partition everything but to                                 me it's not because it's actually the                                 very same things that you know again                                 like I'm saying facebook today is doing                                 with giraffe it could have totally done                                 with the graph database exactly the same                                 type of processing part of the reason                                 they are not doing it is you know what I                                 mentioned in you know some of the slides                                 when I did the comparison is because                                 once you store your data in a graph                                 database you basically lose the ability                                 to efficiently work on that data using                                 the tools that are available in Hadoop                                 ecosystem so if you want that same data                                 to be available to your pig and your                                 hive and your presto and your impala and                                 your sequel on top of Hadoop like it's                                 tough because it's not really stored in                                  a central location it stored on a graph                                  database like you know what do you do so                                  they opted out you know for using graph                                  and they're pretty happy with it but                                  like I'm saying if all you do is graph                                  processing graph databases could                                  actually be a good choice it's just that                                  they do much more than just graph                                  processing on                                  same data set and maybe a very good                                  question you've mentioned two times that                                  as a graph grows linearly the number of                                  relations grow exponentially put do you                                  mean better so if you basically double                                  the number of nodes in your graph the                                  potential again not not the ones that                                  you know of but the potential number of                                  connections grows exponentially it grows                                  quadratically as far as I remember this                                  well different different types of I                                  smell some fight outside of this room                                  which is good so correct me if I'm wrong                                  or if I took this incorrectly if I want                                  to use a graph based data store to                                  process and give me result sets for a                                  real time system which has not really                                  low latency but low enough to be in the                                  one millisecond under sorry one second                                  or under one second range is this a good                                  fit or as some other graph database best                                  suited for that or is this more like for                                  batch processing and getting sort of bi                                  sort of information out it's it really                                  is more for batch processing I mean                                  there is there is ways to get data out                                  of the you know sheriff and you can                                  actually like the intermediate you know                                  points of execution you know certain                                  super steps you can actually dumped you                                  know the graph state but it really is                                  more for batch processing there is                                  actually an alternative now on Hadoop                                  called spark that does you know                                  full-fledged memory processing so on                                  spark they have a reimplement ation of                                  essentially I believe you know the same                                  BSP model called graph X I think uh                                  which then you can actually inspect that                                  model so the whole idea of spark is that                                  it does computation but it actually lets                                  you sort of tinker with the model is you                                  know is it compute the output format are                                  then the output formats which take                                  graphs like which output graphs into                                  standard graph formats which other tools                                  can be visualized oh yeah absolutely so                                  there's actually it's fine funny funnily                                  enough I mean there is like a dot                                  you know pretty popular you know graphic                                  Fe and everything all other tools can                                  sort of lower these Arabs in pretty much                                  here you probably get like you know I'm                                  cautious you know when I like mentioned                                  dog because you know you don't really                                  want to overwhelm it with you know like                                  really big graph but if you you know                                  somehow like I don't know if you do any                                  kind of clustering or whatnot and you                                  end up with a much smaller graph yeah                                  that's that's not be fine i mean people                                  do it all the time okay maybe time for                                  last question because actually we have a                                  coffee break so if someone prefers                                  discussion to coffee now then thank you                                  thank you
YouTube URL: https://www.youtube.com/watch?v=alegx3sP7hc


