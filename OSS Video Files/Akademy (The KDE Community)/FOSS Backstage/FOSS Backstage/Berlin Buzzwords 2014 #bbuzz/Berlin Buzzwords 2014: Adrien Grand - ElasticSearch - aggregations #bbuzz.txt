Title: Berlin Buzzwords 2014: Adrien Grand - ElasticSearch - aggregations #bbuzz
Publication date: 2014-05-28
Playlist: Berlin Buzzwords 2014 #bbuzz
Description: 
	No need anymore to present faceted search, which is used on most search engines in order to give insights about the hits that matched the query.

ElasticSearch 1.0 introduced aggregations, which have been designed to be the successor of facets. What do aggregations bring that wasn't possible with facets? Composability. Although this may not look like a huge improvement, this actually opens up many possibilities, that this presentation will discuss.

Read more:
https://2014.berlinbuzzwords.de/session/elasticsearch-aggregations

About Adrien Grand:
https://2014.berlinbuzzwords.de/user/298/event/1

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              so against it stop oops                               time to stop okay so we are going to                               start so my name is adilyn gong                               I'm a software engineer at elasticsearch                               as well as a committee on the upper                               tributes in project and today we are                               going to talk together for about                                  minutes about aggregation elasticsearch                               so aggregations are a new feature of                                elasticsearch which have been introduced                                in the release                                                       four months ago in January and today I'm                                going to talk about what aggregations                                are why we built them and also about how                                they work I think it's important to get                                some basic understanding of how they                                work because it will help you understand                                the trade-offs that we made that when                                designing allegations and gematria is                                going to be helpful for you in order to                                understand whether or not it's also                                right to the right tool to tackle the                                problem that you are having so I won't                                give any example about syntax and                                response format it's only going to be                                about what you can do in how they work                                so aggregations are all about analytics                                as they allow you to compute histograms                                distributions of value and you can know                                statistics and have some key features                                one key feature is that they can be run                                on any portion of your data which means                                that whatever you can match with a gray                                such as what that work can you hear me                                okay any query that Clinton just                                described can be used and you can use                                this core in order to select the set of                                document that you'd like to aggregate on                                and everything is going to be computed                                in near-real-time so what does                                near-real-time mean it means that                                one two it works out okay okay                                that work okay so so what's this refresh                                interval means that when you index data                                you might need to wait until this amount                                of time before data becomes visible for                                search and by default it would be one                                second for elasticsearch so that might                                sound like a very hard row back for last                                week's search especially if you compare                                it to more traditional databases that                                would ensure that everything that you                                put into the system becomes visible                                instantly but actually this allows for a                                lot of mutation that we are going to                                discuss briefly and finally aggregations                                can be composed this is something that                                means in something that can be described                                as a previous stage of aggregations that                                we call facet elasticsearch and this is                                the reason why we build applications                                which was to be able to compose                                aggregations together so what does this                                compressibility mean in the context of                                recreations there are two main kinds of                                aggregations with an elastic search the                                first kind can be described as bucket                                operations which are basically                                responsible for taking a set of                                documents and partitioning into                                different sets of document and on the                                other hand we also have matrix                                aggregation whose responsibly is to take                                a set of document to compute something                                on this type of document so this can be                                anything from basic statistics like                                minimum maximum average and some to                                something which are a bit more                                interesting like approximate percentiles                                and unique counts so when I said                                compressibility what does it mean every                                bucket aggregation can have one zero one                                or many subrogation that can be either                                bucket a creation or matrix aggregation                                and matrix aggregation can only be used                                as a leaf of an aggregation tree so numb                                this might still sound a bit abstract at                                that point at that point and I'm going                                to give a few example of things that you                                can do with a relation that you couldn't                                do we said before because they lacked                                this composability so first let's try to                                think about traffic analysis let's                                imagine that you are willing to build                                something that would look like Google                                Analytics and really would have                                documents about your traffic that look                                like this you which would have to field                                one about the                                 IP of the requests that have been                                 performed on your website and another                                 one about the timestamp at which the                                 request has been performed and if your                                 documents look like this you can run the                                 the following aggregation which puts a                                 cutting in aggregation on the source IP                                 field and the an Instagram aggregation                                 and those attempts time field in order                                 to build this kind of chart that                                 displays a unique amount of visitors                                 that you got on a per day basis okay so                                 let's try another example because you                                 don't you do not need to put a cardinal                                 segregation and our histogram you get                                 something else in that case we what we                                 would like to do is some kind of                                 performance analysis let's imagine that                                 you are looking performance analysis of                                 your application it will last Excel                                 documents would typically look like this                                 with a response time and a timestamp in                                 that case if you put a person test                                 application under Instagram aggregation                                 and if so for example interval of the                                 histogram aggregation is                                             would be able to have the percentiles                                 for example the median                                               percentile of the response half of your                                 application for every bucket of                                         so we just saw two basic aggregations so                                 let's try to see one that would be a bit                                 more complex and you know that you                                 understand do you do use case so this                                 time we are dealing about e-commerce and                                 for example you are either a price                                 comparison website or marketplace and                                 you are indexing all files from                                 different sites and you would like to                                 build aggregations on this document so                                 your documents typically have one                                 category one site where they come from                                 one brand a designation and a price and                                 so let's try to run this aggregation to                                 see what it does so first we are going                                 to partition on a data according to the                                 category the offer belongs to in that                                 case elasticsearch figured out that                                 there are                                                              skirts and then for each category we                                 want to compute two things first we want                                 to compute the number of unique sites                                 that sell items in this category and for                                 example we got nine for the dresses                                 category three for the shoes category                                 and five category but we also want to                                 compute the unique set of brands                                 who have items in this category and I                                 only put it for the dresses category but                                 you would have a very small output for                                 Susan skirt and for example we found out                                 that there are eight dresses of the                                 brand is equal under this category in                                 the end for each brand in each category                                 will sound to compute the minimum price                                 so you would also have it in the output                                 of elasticsearch okay so now that you                                 should understand a bit more what                                 aggregations are about and what you can                                 do I'm going to try to explain why it                                 makes sense to implement them                                 elasticsearch this might be a bit                                 surprising if you come from more                                 traditional databases where you would                                 have expected this kind of feature to be                                 implemented but actually these are very                                 good reasons to implement it                                 elasticsearch one of them is that it's                                 very powerful if you combine it with                                 search because everything is computed                                 dynamically and on the fly which means                                 that if you are user initially entered                                 one query                                 if you refine this curry you can                                 recompute or discounts dynamically                                 another reason is more on the                                 implementation side and the reason is                                 that search engine have had faceted                                 search for a long time and one                                 consequence of that is that search                                 engines such as a loose in elastic                                 search have a storage that is highly                                 optimized for such cases I'm going to                                 talk about it a bit more in the next                                 slider and aggregation basically are not                                 a revolution in the sense that we don't                                 we didn't need to rebuild loosing in                                 order to make it possible but instead we                                 are building an existing work which has                                 been done in loose in and in particular                                 in order to support efficient compressed                                 columnar storage so I agree if that are                                 fast and I'm going to try to explain a                                 bit why so one reason why original fast                                 is that they are built on lucina                                 and the trade-off of leucine is to make                                 search as fast as possible so it's a                                 very conscious trade-off and if you've                                 seen that someday has the choice to make                                 either indexing faster or search faster                                 it's going to make search faster and                                 then work on in the inkling side in                                 order to improve algorithm and maybe                                 that data structure to make it as fast                                 as possible but the right the focus is                                 really on search so this is                                 true for the inverted index which is                                 used in order to find a matching                                 document but this is also true for some                                 columnist right that losing has which is                                 called dock values on which are                                 efficiently compressed so if you come                                 from loosing you would call it that                                 values if you come from elasticsearch                                 would collect field data but basically                                 it is exactly the same and one important                                 optimization a political master h is                                 that we've seen an elastic search never                                 never manipulate string base directly                                 instead a Lucene index is actually made                                 of a few typically around fifty                                 immutable indices which are smaller and                                 in each of these immutable indices                                 strings are restored as enums                                 so why is it important because if you                                 saw the strings installer stole them                                 somewhere in the file system you can                                 refer to these strings according to the                                 ordinal and for example if you want to                                 compare string a again string b you                                 don't need to compare the bite but you                                 can directly compare the ordinals in                                 order to understand if they are equal or                                 which one is greater than the other one                                 so this is a an optimization which is                                 used everywhere in missing in one less                                 return why is that fast is that no                                 matter how many levels of aggregations                                 you have everything is going to be                                 computed in one single pass so how does                                 it work so when you were on the crack                                 with elastic search basically you have                                 something which is called inverted index                                 which comes from Mussina which when                                 combined with the clay is going to be                                 able to tell you what are the matching                                 documents and typically these matching                                 documents are going to be written to you                                 as an information and you are going to                                 be able to to have listeners on this                                 iteration and they are called collectors                                 typically by default you would only                                 always have the top his collector which                                 is used in order to collect the top hit                                 but you can also plug in a collector for                                 aggregations and the important part                                 about this slide is that I wanted to                                 explain that computing aggregations and                                 computing hit top hit is actually going                                 to happen at the same time in the same                                 collection of your document matches okay                                 now let's focus a bit more on the                                 regions bottom                                 and basically as I said no I didn't say                                 it's right something which is useful to                                 know is that everything is tossed                                 sequentially in Massena so this for girl                                 parties could be one segment and even                                 about it index is going to give you                                 ideas of document that matched and for                                 this index which is stored sequentially                                 we could have welcomed for example two                                 columns for to filter so in that case we                                 have two field which are a category in                                 the price which store the category in                                 the price of the document and in order                                 to compute aggregation elasticsearch is                                 going to take the bucket of the parent                                 aggregation so it could be the root but                                 it could also be the result of any other                                 bucket or creation it would work exactly                                 the same way and then it will iterate                                 over all the matches so first we                                 discover a new category for which the                                 one there was no bucket so we need to                                 create a new bucket with the document                                 count of one and the price is going to                                 be the price of the single offer that                                 this bucket contains then the same                                 happens with clothing which is a                                 category that we have never never seen                                 before                                 and then we see shoes again so instead                                 of creating a new bucket what we are                                 going to do is that we are going to go                                 to the existing Birkett increment the                                 document count by one and update if                                 necessary is the minimum price the                                 minimum price is lower than the previous                                 minimum price so we need to update it                                 again a new category so we create a new                                 bucket and for the last document in this                                 set of matching document the price was                                 higher than the previous buy price so we                                 don't need to update the minimum price                                 so this is really how it works                                 at the shower level however                                 elasticsearch is not a single shard                                 single block so changing everything can                                 run distributed which means that you                                 need to be able to mount results of                                 several aggregations together and the                                 way it works that elastic search is                                 going to take the tree of aggregations                                 for every shot and to melt them together                                 recursively                                 by merging together buckets that have                                 the same level so let's take back this                                 example and try to match it with the                                 results that we got on another shadow                                 and basically we are going to mount                                 recursively so first we're going to                                 choose a category and we can see that                                 it's present in two variations so we                                 need to set up cots                                 so the total count would be                                   and then recursively we'll go to the                                 child aberration which is a mean price                                 and so we need to take the minimum of                                    and                                                                     with Slovenia and it can also happen                                 that some buckets are only present in                                 one aggression in what in which case                                 it's going to be very easy we just need                                 to recopy                                 it verbatim and we'll have our                                 separation and the same is true for                                 accessories and that's it                                 so we just get merged result for the                                 aggregations so on the sub slide I give                                 you examples the aggregation that exists                                 in elasticsearch in particular a bucket                                 and metrics a creation myth actually you                                 have a few aggregation that I did not                                 mention so something you could be                                 interested in is that elasticsearch has                                 some basic support for document                                 relations when it comes to aggregation                                 in particular if you know about nested                                 documents in a stick search you need to                                 know that we also have nested and                                 reversed nested agree aggregations in                                 order to make it very easy to leverage                                 these relations in aggregations we also                                 have something which is called                                 significant terms that maca would who                                 created them usually said that it's                                 helpful to find what is uncommonly come                                 on                                 in late data set this can be typically                                 useful in order to detect fraud for                                 example or to if you take database of                                 comments about products sorry a database                                 about of comments about products to get                                 what are the most important terms about                                 a particular product a for example for a                                 particular cow it could be that it's                                 done rules or it very nice draw so this                                 is a kind of information that you could                                 extract there is also a new aggregation                                 which is going to come investigate                                     thanks to Martina                                 with RIA which is the top piece                                 aggregation which is going to help you                                 compute the top each or every bucket so                                 if we go back to our example about                                 e-commerce this means that for example                                 for every category you could return the                                 top it to the user so let's imagine that                                 your user enter the query you would be                                 able to return the top hits for the                                 shoes category for the scales categories                                 etcetera                                 and one last thing that I wanted to                                 mention is that performance and memory                                 usage of aggregations improve a lot in                                 general and in particular in the last                                 release which was done last week which                                 was elasticsearch                                                   already using a relation I highly                                 recommend to to upgrade and in                                 particular if you are using discs                                 biggest field data performance will be                                 much better now so that's everything I                                 wanted to say so thank you for your                                 attention and if you have questions I                                 think we still have a few minutes to try                                 to answer them                                 yeah hello yeah the aggregation starts                                 really very cool very nice you mentioned                                 about aggregating across segments                                 individual segments or across different                                 I guess in indices how about things like                                 cardinality you know where you're going                                 to get some sort of error right because                                 you don't know exactly what what what                                 the cardinality things when you merge                                 and how do you cope with that                                 so actually we're not using a crate                                 algorithm to do that so for both the                                 cardinality and the percentiles                                 aggregations were using approximate                                 algorithm and so if you know about hyper                                 loglog papers this is the name of the                                 algorithm that we are using in order to                                 compute carnality and if you know about                                 tea digest deadening is here is the                                 father of this algorithm this is an                                 algorithm that we are using for the                                 percentile segregation so this algorithm                                 have been designed to be to not be fully                                 accurate but to be able to work in a                                 distributed environment when you where                                 you cannot know about all the data at                                 the same time                                 you could keep all of the items unique                                 items if you really wanted to get an                                 exact unique count it just would be                                 tremendously expensive right yes and                                 that's something we do rule to to allow                                 users to do it that make sense hello all                                 the plans to do efficient pagination on                                 aggregations so sorry congratu nation                                 okay Suzuka she is about pagination and                                 aggregation on aggregations okay so I                                 think this is something that we are not                                 going to do okay because it's                                 complicated if you want to do pagination                                 it typically typically means that for                                 example let's take the example of a term                                 segregation it typically means that you                                 have a lots and lots of terms that you                                 would like to be able to paginate honor                                 and the thing is that it's not an easy                                 issue not an easy problem to solve to                                 have a crate counts of a large Canyon                                 cut energy field                                 if data current comes from several                                 shards and you don't want to forward the                                 whole set of values to the note that                                 coordinates the search so if we find a                                 way to make it cheap shall we will do it                                 but so far we I mean we we not know I                                 don't think we can solve it but if                                 someone has an idea how to solve it I                                 deserve welcome three question here                                 I what would be the influence on the                                 memory consumption if we are going to                                 use aggregation sorry what would be                                 zillions the influence on the memory                                 consumption so in in addition to the                                 won't we have anyways so sorry I didn't                                 understand everything so it's about                                 memory consumption yeah and with regard                                 to what right cost of memory                                 okay what's the cost of memory so it                                 depends on the aggregation that you                                 you'd like to right now and so typically                                 if you are running a term separation the                                 memory usage is going to depend on the                                 unique number of terms that your fill                                 has so under each other so on every shot                                 so I give this example typically and I                                 never shall the elasticsearch is going                                 to build one bucket for every existing                                 term in your shop so the more and                                 everything is going to be loaded into                                 memory                                 hopefully it's highly compressed and                                 something so as I said earlier loosie                                 loosie an elastic search almost never                                 used biased in order to actually                                 represent terms the user ordinarily                                 instead and what would happen in that                                 case is that we would first not choose                                 the string representation of the                                 category here but it's odd nor okay and                                 we would then we would prove in order to                                 find the top categories and only after                                 that and after that would replace the                                 ordinal with the actual bytes of the                                 category so it's optimized but obviously                                 you have a memory usage which is linear                                 with the number of buckets that you have                                 so this is an example for the term                                 segregation and we also have variations                                 where you can trade some memory for                                 crazy so this is typically the case for                                 percentiles and cardinality well you you                                 have so you have a way to say please use                                 more memory in order to improve accuracy                                 or less memory because I want it to be                                 as light as possible so it depends on                                 the aggregation we are talking about                                 yeah so basically it's just a running                                 function now okay so the question is                                 about how do Instagram work okay so can                                 I use my own back mapping function in                                 lot to map time stamp to make it so we                                 are to in the context of the histogram                                 aggregation which is going to build back                                 it                                 so by default there are there is a high                                 number of intervals that are predefined                                 that which are typically minute or you                                 can say                                       I've -                                                                   do something which is more complicated                                 and in particular which would not be a                                 fixed side to advocate what you would                                 need to do would be to use a term                                 segregation and to use a script in order                                 to define your bracket so typically you                                 could say for that here I want because                                 of one months and for that job on                                 buckets of one week this is something                                 you could do with the term saturation so                                 the reason why I'm saying that is that                                 instagrams histogram migration is                                 actually a specialized term segregation                                 which is going to merge the robecca's                                 together okay and this is something that                                 you can do with a script in a stick                                 search the reason why we have histogram                                 aggression is that is typically used for                                 days and we need the special handling                                 for that in particular we want to allow                                 you to use that math okay but you could                                 definitely use term saturation with a                                 script in order to generate the levels                                 of your bucket and you could do pretty                                 much everything that you want Adrian                                 thank you very much for your talk as                                 always we are running out of time now                                 there are large break so have a good                                 meal and thanks                                 thank you
YouTube URL: https://www.youtube.com/watch?v=kKqIXvoQpbY


