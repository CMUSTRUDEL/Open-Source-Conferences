Title: Berlin Buzzwords 2014: Michael Hausenblas - Lambda Architecture or How I Learned to Stop Worrying...
Publication date: 2014-05-28
Playlist: Berlin Buzzwords 2014 #bbuzz
Description: 
	When Nathan Marz coined the term Lambda Architecture back in 2012 he might have only been in search for a somewhat sensical title for his upcoming book. No doubt, the Lambda Architecture has since gained traction, functioning as a blueprint to build large-scale, distributed data processing systems in a flexible and extensible manner. But it also turns out that there is a sometimes overlooked aspect of the Lambda Architecture: human fault tolerance. Humans make mistakes. Machines don't. Machines scale. Humans don't.

Read more:
https://2014.berlinbuzzwords.de/session/lambda-architecture-or-how-i-learned-stop-worrying-and-love-human-fault-tolerance

About Michael Hausenblas:
https://2014.berlinbuzzwords.de/user/303/event/1

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              all right good afternoon my name is                               Michael Howsam glass                               chief data engineer at web Arctic                               and today I'm going to talk about                               or how i learned to stop worrying and                               love human fault tolerance so let's have                               a look at what fault tolerance might                               mean in a distributed system like Hadoop                               for example we generally consider the                                hardware being commodity hardware not                                that reliable right we compensate for                                that using software right whoever you                                look HDFS HBase whatever you have                                mechanisms in there that compensate for                                Hardware going down right but what about                                a developer let's talk about developers                                I'm not talking about that kind we know                                that I would argue it all started when                                we invited the elephant somehow we                                presenting Hadoop to the tea party so                                things got complicated you know you                                develop something in the previous talk                                if you've been around we saw some very                                good examples they're having a staging                                area having can erase around and so on                                and so forth but then at the end of the                                day you still experience problems and                                part of that talk is to raise awareness                                around that problem and the other part                                is a suggestion how to combat that we                                also know that and I'm not going to talk                                about this necktie interface problem I'm                                talking about this right worked fine in                                death it's an ops problem now so let's                                maybe talk about human fault currents                                rather than developers in general                                developers are smart motivated DevOps                                they know what they're doing                                most of the time so when things go wrong                                and they can go wrong at any scale we                                see that quite interesting things with                                her bangs even Google has these down                                types quite often something that when                                nicely and smoothly in a development                                environment maybe even in the staging                                environment somehow blows up into your                                face when you do it when you put it into                                production so I would argue from an                                architectural level if we can provide                                some guidance there if we can equip                                people with a way of thinking about                                systems at least half of that challenge                                is already addressed so let's step back                                a bit the lambda architecture at least                                the name the idea was probably around                                for for quite some time ready was                                established by a smart used to work at                                Twitter early on back type and the                                creator of many goodies out there storm                                and Kass clapping two examples he                                started to ride on a book and if you                                haven't checked it out yet you might                                want to it's available under this URL                                and you had to come up with some name                                for that architecture so it chose to                                name it lambda architecture and why that                                is so we will see in a minute so looking                                at the experience he had both the back                                type and Twitter he essentially or his                                team andent himself put together a                                number of requirements they thought                                would be desirable to have for such a                                system it should be fault tolerance                                against both the hardware failures or                                well and human errors that's probably                                something you obviously if you look at                                the workload twitter has not only this                                batch mode should be supported but also                                a low latency query it should have the                                capability of scaling outliner ii so                                throwing more commodity hardware i did                                and it should also be extensible right                                the business requirements might change                                your environment might change your user                                bass might change so you want to have an                                accessible system being able to not only                                manage it and operate it but also                                accommodate new features and that's what                                he ended up with suggesting and if you                                look at that part that pretty much if                                you turn your head                                                      a lambda right that's why it's called                                lambda architecture so you essentially                                have new data streaming in here which is                                presented to or offered to both the                                batch layer on the one hand and the                                speed layer in the batch layer you have                                an immutable master data set and a batch                                process that computes the views I come                                 to an example and concrete                                 implementations in a minute so bear with                                 me for now just trying to appreciate                                 some of the key terms like this                                 immutable master data set on the other                                 hand you have the speed layer that                                 essentially processes the stream and                                 updates the real-time views and then you                                 want to combine these two to satisfy any                                 given query right very very                                 straightforward and simple at least on                                 an architectural level so as I said the                                 batch layer the main task there is to                                 manage that master date and said                                 hopefully because they're all your your                                 data should be available in a reliable                                 way and given the scale of the operation                                 it should be you know typically                                 distributed master data set and it                                 should be able to pre-compute any                                 arbitrary query yeah that one could                                 imagine and the result of that are the                                 batch views and the certain layer                                 essentially indexes these views and can                                 be cured in an ad hoc fashion so there                                 you have the low latency realized and                                 the speed layer essentially and I come                                 back to it this slide compensates for                                 the latency of the batch layer so the                                 bench layer let's say runs once a day or                                 every two hours and all the                                 data that has arrived between the last                                 run and now which has not been absorbed                                 into the master data set is catered                                 forest served by speed layer okay so                                 that's the kind of the things that how                                 the architecture deals with ya the                                 incoming data and if you think about                                 that if you deploy if you're a developer                                 and you have something some some bug in                                 there it even if that back shows it only                                 shows for that time the next time the                                 bed run goes over that you're may be                                 inconsistent data or whatever goes away                                 right the batch layer essentially always                                 takes the entire master data set into                                 account so let's have a look at a                                 concrete example took that from open                                 flight store org because they have to                                 date available you might want to do                                 something else there but for something                                 concrete and mutual master data set and                                 you have always these time stems there                                 and that essentially says this slide eii                                                                                                     point in time and you got that for all                                 the others and you always ever append                                 you never update anything in place so                                 rather than updating for the next day                                 this record here you would upend another                                 record there you always keep the data                                 around in its rawest form you never                                 update anything in place the implication                                 being you better have a good data                                 platform that can handle that data okay                                 so let's have a look at the fuse so one                                 very very simple view well I had that                                 one sentence in mind that counting is                                 actually not a very simple operation but                                 a very simple way of looking at things                                 is I'm asking how many airport how many                                 planes are airborne right so I go                                 through that list and all of them who                                 have taken off but not landed yet are                                 hopefully airborne                                 yeah there might be exceptions or                                 another view might be provide me an                                 overview about airborne airplanes her                                 airline so I would go over that and take                                 into account the airline identifier                                 there or another one would be the                                 airport load so how this is an airport                                 right but you get the idea you always                                 look over the entire master data set and                                 by applying a certain query you get the                                 views that are then served by well                                 serving there in case you want to learn                                 more about lambda architecture we put                                 together a website called lambda dead                                 dosch adesh architecture net where we're                                 trying to document implementations use                                 cases and so on and if you're in a                                 position that you have already                                 implemented this lambda architecture                                 somewhere please let me know or my                                 colleague Nathan Mason violence and                                 we're happy to put that there as well                                 it's really meant to serve the community                                 to ya document good practices so coming                                 to implementation of the land or                                 architecture so far you know an                                 architecture that's nice that's not very                                 useful you want to implement that right                                 you've got a deadline you've got people                                 and you want to implement that so one of                                 the things we're doing there on that                                 website and the advocacy website is                                 essentially listing the components that                                 you could use for realizing certain                                 layers that alone is probably not too                                 useful you still need some experience                                 around that and a gun essentially                                 through a number of architectures are                                 found in our user base and and outside                                 and one of these things that somehow is                                 a recurrent pattern is not very                                 surprisingly that for you to master data                                 set hdfs is used for the bachelor quite                                 often life is used or pig and wife                                 sorry Fred in the speed layer you would                                 typically see Kafka in front of storm                                 right so these are things that we quite                                 often find in implementations and then                                 the serving layer would for example be                                 realized with page base where you merge                                 both the badge view and the real-time                                 view right so who besides Ted can spot                                 the problem with this approach using                                 hive deploying a storm topology and so                                 on and so forth I'm not saying it's                                 impossible obviously customers have that                                 in production but what could you imagine                                 also in terms of minimizing the                                 potential errors someone could you know                                 introduce there what's the problem with                                 this approach having many many different                                 systems environments languages and so on                                 well the problem is that you're                                 essentially repeating the business logic                                 both in the badge layer and in the speed                                 layer once you write your hive query                                 once you write your storm topology in                                 Java it's the same business logic                                 implemented including testing and so on                                 and so forth you're essentially                                 maintaining parallel and illogical level                                 parallel things which leads us to the                                 question how about an integrated                                 approach is there anything out there                                 that allows me to do to implement the                                 lambda architecture using one framework                                 on language one platform so there's                                 Twitter summing bird real least end of                                 last year which is a very nice approach                                 not going to comment on that further                                 just pointing it out it is there there                                 is lamb dupe which I believe will soon                                 be open sourced you can request a demo                                 there on their side and their spark who                                 a few have heard about spark already                                 apache spark cool                                 so I'm going to argue that apache spark                                 is probably the best way based on our                                 experience to implement the lambda                                 architecture recent being that it allows                                 you to pick your language scallop eyes                                 know whatever develop both sides the                                 batchview and the speed a data stream                                 screaming sidereal time views using one                                 framework one paradigm so bit of a                                 background to spark initially developed                                 by the emblem folks we heard it in the                                 earlier talk also message is quite close                                 to them there early this year it got                                 promoted into top level Apache project                                 and the kind of commercial shepherds our                                 data breaks and you can get enterprise                                 support from Hadoop distributions such                                 as this the spark community has actually                                 pretty rapidly grown and there you see                                 contributors committers users people who                                 in generally do something with spark by                                 to contribute and or use and from a kind                                 of hundred thousand feet view these deck                                 looks like that so you got a data                                 platform and spark in a sense is rather                                 agnostic to that it says I don't care                                 where the data comes from could be as                                 free could be H differs whatever but at                                 the end of the day you need that data                                 platform you need something that you                                 know for your master dataset is you have                                 an execution environment again very                                 flexible you can have mesos you can have                                 yarn you standalone mode where you don't                                 need any other framework for that that                                 essentially yeah execute the spark core                                 engine and there's a ecosystem and as                                 you can see there are things like sparks                                 equal or shark in its previous form a                                 streaming part which is micro batch                                 you've got a machine learning part here                                 you've graphics and number of other                                 upcoming things there so this allows you                                 if you look back at the challenge we had                                 they're implementing the business logic                                 both on                                 betch layer and on the speed layer to do                                 that with one framework again click your                                 language currently there are three of                                 them supported it's pison Scala Sparky's                                 itself is written in Scala and Java and                                 sorry as I know there are more to come                                 up so on the one hand at least to me                                 spark has a bit this MongoDB feeling you                                 download it and you immediately get                                 something that you immediately can do                                 some cool stuff it is in this range of                                 small data something that probably fits                                 on that laptop or on my mobile phone to                                 mid-sized data to a large data something                                 that very nicely also addresses this low                                 end part this mid-sized data part where                                 with a MapReduce implementation you                                 might sometimes run into a problem                                 justifying that it also has a very                                 expressive API we know that app API from                                 good old map reduce its map and reduce                                 right however that's what spark offers                                 so a very expressive API many of the                                 things that in MapReduce you would                                 essentially either defer to high level                                 languages like high for pig or cascading                                 or whatever you get directly from apache                                 spark there right and we've again put                                 together and advocacy side for that                                 sparks teknorg we're trying to keep up                                 with the news that come around that                                 spark sec and now i have still some time                                 for questions answers which i hope we                                 can enjoy                                 and have you ever tried using                                 stratosphere and how would you compare                                 it to spark like couple of years ago we                                 had a talk about stratosphere and that                                 game try to make a point of comparing                                 with spark what you can say about right                                 so the question was if I or we have used                                 stratosphere I personally haven't i'm                                 looking at tap ok would be very                                 interesting to see that comparison there                                 have you got some previous experience                                 with stratosphere yourself or sorry just                                 a bit ok and if you contrast that with                                 what i showed so far here would you say                                 there is a one-to-one fit sorry for                                 sending you run try to keep it a bit                                 interactive ok yeah it has a strong                                 support for iterative algorithms so                                 basically it's a strong feature i would                                 say and then also they emphasize the                                 optimizer which i'm not sure how spark                                 is comparison this respect right but                                 that's is that I would be very                                 interested in having your comparison and                                 I think Ted also has something that you                                 get back right we're running a fairly                                 large Hadoop cluster and it's                                 operationally fairly heavy we spend a                                 lot of time keeping it running spark                                 similar in that aspect is my first                                 question and the follow-up question is                                 if spark is easier to operate you still                                 need some storage layer like HDFS if you                                 want to avoid the operational elephant                                 of Hadoop or where you have suggestions                                 for other options right right I guess                                 that was even though i only made this                                 point shortly so this is                                 you for example download apache spark                                 from the apache web site or it comes                                 with memoir wherever you want to get it                                 this is what you get in its core but in                                 order to operate it as you rightly                                 pointed out you want that so the                                 question is for example if things like                                 high availability disaster recovery and                                 so on are important to you then I would                                 say well you probably want to have a                                 data platform that really can offer that                                 we can deliver on that so here in the                                 execution environment you have got a lot                                 of choices right so you might want to go                                 with yarn you might have been around in                                 the earlier talk want to try out methods                                 there or for certain applications the                                 standalone mode might be sufficient                                 depending on if you have different                                 workloads in your cluster or not but I                                 would argue that many of the enterprise                                 demands and features such as hey J and D                                 are are probably met in these two layers                                 I think Ted                                                          yeah zooming in on the storage are there                                 any alternatives to HDFS basically so                                 yeah there are alternatives so there are                                 commercial hardware alternatives EMC                                 netapp both provide reasonable storage                                 appliances and map are the guys with the                                 buttons falling off we have high                                 performance HDFS clone that provides a                                 che that provides much easier                                 operational setting than stock a dupe                                 and gives you read write access but this                                 is an open source conference so the                                 speaker is trying to be very good and                                 not say these things but we'll talk                                 about it later if you want offline and                                 spark itself is very easy to run yeah                                 that yeah yeah I guess that's what I'm                                 trying to point out with that's the                                 MongoDB experience to me that says                                 everything I don't mean it doesn't say                                 to everyone but you know download it and                                 you get at me I'm going to be                                 experienced                                 easy to install and it was data later                                 which progress you get you get you get                                 started easy right but Festus re-spark                                 doesn't do that ok oh yeah from my                                 experience with Spock we have been using                                 not yet in full production but for a few                                 months when it's a exactly it's very                                 easy to use the share is great to learn                                 it extra a main problem we had well                                 that's difficult to tune the memory                                 usage and it does not degrade gracefully                                 when that that's a data sets are big                                 sometimes the cash does end up a bit on                                 the disk right and sometimes you just                                 have out of memory errors vector on your                                 horse relations yeah so the thing is I                                 intentionally I I put in the second part                                 around spark to kind of such as the best                                 practice how the lambda architecture                                 which itself might you know look rather                                 abstract how you can realize that it's                                 intentionally not talk about spark and                                 unhappy too you know authentic that                                 further about the the main problem or                                 one of the architectural decisions there                                 is that essentially the driver program                                 in spark you know has has the whole                                 control there and it's also you know                                 consuming as much memory as as it can                                 find let's put it that way so as far as                                 I know there are a number of things                                 coming up that might address that in may                                 be already in one point oh I don't know                                 but as you said this is something where                                 there's still a lot of yeah community                                 resources that need to be documented and                                 shared necessary it's it's nothing that                                 you know there's a simple and clear                                 answer do XYZ it is as you said it's                                 it's a known issue there hi I had a                                 question up I yeah I I haven't really                                 used spark much but I was playing around                                 with it a couple of weeks ago and key                                 phrase that kept coming up in the                                 literature was in memory MapReduce right                                 so that                                 sort of made sense when I related little                                 more but could you you know elaborate a                                 little more on how it works when the                                 data is really large and doesn't really                                 fit in memory right right so yes the                                 core idea are these are dd's to                                 Switzerland destroy the data sets and                                 again it's not a spark talk so that's                                 why I spared out all these details so                                 you essentially have a data set that                                 apply some transformation and you apply                                 the transformation resulting into a new                                 data set you can pin these data sets                                 into memory and start up to you know                                 have it already cached which which is a                                 lot of speed up the question still what                                 if the data set does not fit in memory                                 the simple answers it will spill to disk                                 so it will become slow right that's what                                 I said in terms of you might not have a                                 big data problem but something that is                                 where your standard relational database                                 or whatever you're using essentially                                 chokes on so you want to have something                                 that as your data grows is able to deal                                 with this mid-sized data but also you                                 know you can crunch a bigger sizes of                                 data sets as well and I've seen some of                                 these migrations where you might have I                                 don't know pig and hive script together                                 and you might keep for you know going                                 through                                                                 pic script around there right because                                 it's a bad job it runs once a day so you                                 know the SLA is that whatever you need                                 to guarantee that the job is done at a                                 certain time or whatever is not that                                 critical and in terms of throughput the                                 MapReduce take by extension is still a                                 very good way to go right for not saying                                 that throw out head open MapReduce and                                 whatever but have a look at if you have                                 spark on the one hand and tests and                                 other things on the other hand maybe                                 spark can offer you more integrated                                 approaches i call it there might be                                 other people calling it different ways                                 but more easier way                                 also to address the last two comments                                 one of the purposes of land architecture                                 is that you don't run very large batches                                 you run in the real time layer a very                                 moderate load in the batch layer you run                                 the last partition to in a batch mode so                                 that you can always rerun that in a                                 partitioned way and so you don't wind up                                 having to handle a very very large                                 amount of data in one in one single run                                 and that makes spark a very natural                                 component in lambda because of that and                                 sparks equal i think is compatible with                                 hive syntactically so you can mix and                                 match the execution mode that's                                 appropriate for whatever you're doing                                 yep with one more question here yeah                                 just throw it                                 hey I one of the things you mentioned                                 about the lambda architecture is that                                 the there's an immutable storage it just                                 goes up and only right right isn't that                                 like sort of like an intrinsic scale                                 issue there as over time you like even                                 if your data set is like fairly fixed                                 rate aren't you saying that you're two                                 years from now three years from now                                 you're going to have like an increasing                                 and at some point you're going to hit a                                 scale limitation so I guess are two                                 answers to that sort of the question in                                 case it's not clear at least as far as I                                 understand isn't there a scalability                                 issue somehow implying okay you need to                                 keep all the data around right so on the                                 one hand obviously or not so obvious we                                 are dealing with schema and read in this                                 setup right so the application                                 essentially decides how to interpret                                 these things which offers a lot of                                 flexibility because what you saw today                                 in this application is the right schema                                 might change with a different use case                                 or different business requirement                                 whatever so that's definitely an upside                                 the down downside is yes you need a very                                 good data platform where you can keep                                 all of that around right you might if                                 you're really dealing with large you                                 know the hundreds of pepper potts                                 whatever might consider for example                                 formats like parquet or org that in                                 itself I have a more compact                                 representation binary compact                                 representation rather than a CSV file                                 for example right you might want to if                                 you say this to                                                         have it in operations for                                                                                                                       a bit for example I would argue that                                 there is not enough knowledge or                                 experience in the community yet to                                 really say okay here the three options                                 how to deal with that but yes we're                                 talking about a data platform that can                                 easily you know store and manage                                 petabytes of data which yeah hopefully                                 is available                                 how we're doing timewise there one more                                 question or yeah so one last question                                 okay oh there you go yeah so from what i                                 read about lambda architecture it was                                 more about that you would always                                 recalculate the batch over your entire                                 data set but now Ted mentioned that you                                 would rather always just recalculate the                                 last partition so are there just to                                 different protests that you can both go                                 or in the batch layer right so you look                                 at the at the entire master data set and                                 create the views over the enter master                                 data said yes right are you worried                                 about errors so every partition that                                 might have errors right then you should                                 recompute if you can partition it if you                                 can't partition it what fits an not an                                 aggregating sort of thing then maybe you                                 have to run every all every bit of data                                 all the time your choice okay thank                                 you
YouTube URL: https://www.youtube.com/watch?v=ZAImpQzTJiE


