Title: Berlin Buzzwords 2014: Oleg Zhurakousky - I Opener to the Big I O #bbuzz
Publication date: 2014-05-28
Playlist: Berlin Buzzwords 2014 #bbuzz
Description: 
	There are many mechanisms for storing and processing a collection of data sets so large and complex that we collectively refer to it as Big Data. From NoSQL data stores to the Distributed File Systems and Computation engines to columnar stores to flat files - its all about capture, storage, analysis, searches etc. 

We want it all and we want it fast and traditional data processing applications can no longer support our demands. And while technologies such as Hadoop and its ecosystem derivatives paved an initial path to solving Big Data problems the approaches and assumptions they are built on starting to show its limitations one could only overcome by radically changing the way we think about storing and accessing data in general. In the end its all about I/O and how to make it more efficient.

Read more:
https://2014.berlinbuzzwords.de/session/iopener-big-io

About Oleg Zhurakousky:
https://2014.berlinbuzzwords.de/user/226/event/1

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              so hello everybody my name is Allegra                               Kowski I work for Hortonworks                               prior that i was at spring source gas                               must be familiar with spring framework                               so i was one of the developers of it and                               what I do at Hortonworks actual is a                               very interesting story I quickly tell                               you about it after leaving springsource                               one of the reasons why I lab spring                                source for VMware at the time was that                                we kind of start looking into Hadoop and                                I just realized that while I can you                                know their own technology I can read the                                comment ation I really didn't know much                                about how Hadoop is applied how and what                                is big data altogether so um when I came                                to Horton rocks actual specifically                                requested that I don't want to go to                                engineering just yet I wanted to join                                the field so i spent about a year and a                                half doing various different consulting                                engagements for Hortonworks related to                                the big data and kind of have a pretty                                good grasp of on things related to how                                Hadoop is being applied in modern                                enterprises and as I was sharing with                                the previous speaker just a few minutes                                ago one of the things that I found very                                interesting while doing a lot of                                consulting around Hadoop is that I spent                                a lot of times writing custom code which                                was kind of strange to me because here's                                the hortonworks we are selling product                                and Hadoop ecosystem seems to be very                                rich with variety of products around                                could do you think around Hadoop itself                                but for whatever reason I can't find a                                specific match to fit a specific problem                                or even if I do find the match to fit a                                specific problem then that particular                                product or technology doesn't really do                                everything that I've wanted to do or                                doesn't perform the way I wanted to                                perform and when I use the word want                                it's not necessarily what I want it's                                what my customer needed and one of the                                problems we'll all talk a little later                                on was related to                                you know super fast data capture or data                                in jest but anyway so today the topic is                                I opener to the Big Al kind of a catchy                                title what is the big I oh I don't think                                it's all that different actually I think                                it's not different at all from a regular                                io because at the end of the day you                                have variety of distribution technology                                whether it's a hadoop hazel cast great                                gain what have you right but at the end                                of the day your data may be distributed                                your process may be distributed but at                                some point of time you have to get the                                data from the disk and you have to read                                it right so and a lot of the times when                                you're relying on some of the technology                                and this is what I noticed things aren't                                done very efficiently and what I'm going                                to do today I'm going to quickly go                                through the slide deck typically you                                know I'm used to sort of a spring one                                format where we got like                                              talk you can show a lot of things you                                can go on a tangent and come back and go                                again when you do                                                    that simple and the other extreme of                                that is what i did ask allah days which                                was                                                                  talk that i really had to prepare and                                rehearse but anyway so let's talk about                                the big I oh and here's kind of the                                agenda so we're going to talk a little                                bit about what is a big data kind of                                touched on a topic of what is structured                                versus unstructured how do we measure it                                store it access it and kind of looked at                                some of the high level architecture I'm                                going to talk about compute mechanics                                why things are slow how easy it is to                                either fixed time or get caught in them                                right and how many of you familiar with                                this concept called mechanical sympathy                                so good couple people I actually got                                very fascinated when I start reading it                                their blogs and things that they did                                with the helm a disrupter and other                                things it's very interesting but like I                                said we'll talk about it in a bit so um                                what is a big data I mean it's                                definitely a buzz word which is chi                                appropriate for this conference right so                                 um it means many things to many                                 different people just like SOA did just                                 like ESP edit and many other acronyms                                 and words that we have right so                                 according to wikipedia it's a data set                                 data sets there are two large and                                 complex the manipulator interrogate with                                 the standards methods or tools do you                                 agree with that definition I mean it's                                 not wrong but is it complete something                                 missing we'll get to it in a few minutes                                 so how do we measure big data is it just                                 by its size and again the next few                                 slides are going to be basically raising                                 some of these questions so um so what                                 does it mean big and if it's not the                                 size then what other parameters we look                                 at to define it we have a big data                                 problem so how do we store the data                                 again in other questions that was asked                                 very often because when i came to                                 Hortonworks I you know the sales pitch                                 was Hadoop is so great just dump the                                 data we provide the distribution                                 technology and the MapReduce will take                                 care of everything well you know                                 MapReduce can't take care of a lot of                                 things but at the end of the day it's                                 nothing more than just processing                                 individual parts of the data in a                                 distributed environment and if that if                                 such data is not written in the                                 efficient way then it's only a matter of                                 time before you're distributed                                 environment will perform as a single                                 thread on the cheap laptop right so how                                 do we access the data you know blind                                 scanning everything not necessarily all                                 that efficient I actually have an                                 interesting story on my wife and I flew                                 last year from Europe back home to the                                 United States and I actually had to go                                 straight on the business trip so I never                                 even made it home so we just landed in                                 the Philadelphia and I                                 to board another flight so I gave it our                                 passports so she went on the car and                                 left so obviously in the state's I don't                                 have to use passport to fly around I can                                 use my driver license but all of a                                 sudden I'm checking on my pockets I                                 could not find my driver license so I                                 can get on the flight so I'm looking                                 around everywhere and the question I'm                                 asking myself have I lost it or haven't                                 misplaced it because if I lost it if I                                 knew that I lost it then I would have                                 caught her back she would make a loop                                 come back give me my passport everything                                 would be fine right but if I misplaced                                 it well I don't know what I did or                                 didn't right so and that's kind of                                 gaming an interesting idea which is                                 actually very related to the data access                                 how how cool would it was every time we                                 look for something we may find it we may                                 not but wouldn't be easier for us to                                 store data organized data in such way                                 where the questions such as it's not                                 there or answered before the questions                                 that it is there and where it is think                                 about that so and then you can bring a                                 third-party technology to make sense of                                 the data that you're dealing with and a                                 big presence here from elasticsearch and                                 I actually talked to one of the guys                                 today the interesting thing is that what                                 I'm going to be showing you what I'm                                 going to be talking about not                                 necessarily contradict some of these                                 indexing and search algorithm that                                 provided by elastic search another                                 company it's actually more of a                                 compliment thing because at the end of                                 the day you can put elastic search you                                 can put other technologies on top of                                 your data but if your data is stored in                                 a very inefficient way at the end of the                                 day your data access and data analysis                                 will be affected so one thing is for                                 sure the data will live somewhere for us                                 to use so historically big data problem                                 was defined by its size and to speed up                                 the processing hadoop gave us two things                                 it gave us a distributed file system and                                 it gave us the distributed computation                                 frameworks such as MapReduce so um you                                 know the old moto is this you know we                                 distribute the data we also give you the                                 frame up to                                 bring your coat to data and that's kind                                 of the opposite of what we sort of used                                 to or being used at a time where you                                 know the data could be spread around and                                 we bring it to a centralized processing                                 so kind of innovative but was done                                 before many times but I found actually a                                 problem with that approach the problem                                 is not necessarily with the pro which                                 itself but rather with the fact that                                 there is a kind of a compute versus I yo                                 a mismatch right because you know blocks                                 we have blocks in Hadoop then we're good                                 the blocks you know we got into splits                                 then input formats reading dis plates                                 and then passing the units of data for                                 processing to the MapReduce task right                                 if you think about it these are two                                 orthogonal problems speed of serving the                                 unit of data right basically reading it                                 from the disk and then speed of                                 processing and if there is a mismatch                                 between the speed of one over the other                                 then you can actually have a problem                                 because you know we always very nice to                                 live in the world where let's just get                                 more hardware but are we using in this                                 is where that we go back to mechanical                                 sympathy are we using our existing                                 hardware efficiently to do things so um                                 how do we solve mismatch and this is                                 where we get to this very simple things                                 however we tend to forget about them a                                 lot especially when dealing with the                                 high level of frameworks such as may                                 produce and Hadoop in general so custom                                 data buffering in fact one of the first                                 samples i'm going to show you it kind of                                 deals with that data encoding and                                 compression so generalization of roses                                 specialization how many of you use                                 google protocol buffers all right do you                                 like it I do but certain things I've                                 learned that could be done much much                                 better and you're going to see it today                                 so data organization pages and efficient                                 page creation meta information about                                 your data this is actually very                                 interesting a point you guys familiar                                 with compression right so let's say                                 you've you ingesting the data you're                                 storing it and you decided                                 save space or whatever other reason you                                 decided to compress the data and then                                 your analysts asked you how many records                                 do we have how would you answer the                                 question all you have to unzip the file                                 and do a line count right just to answer                                 that one question some of the other                                 information about the data for example                                 how when it was written so and so forth                                 so a lot of this meta information is                                 being completely missed the irony of it                                 is during the actual capture of the data                                 that information was available to you                                 for free whether you capturing one                                 record at a time from socket you can                                 maintain a counter whether you're just                                 reading or copying the entire file all                                 that information was available to you                                 during the capture and in fact if you if                                 well if you're using a custom code a lot                                 of times you maintain those counters for                                 other reasons and then you completely                                 dismiss their values right so page                                 caching a lot of times you know when you                                 you're familiar with a page caching when                                 you read the data you know the way the                                 data is right into the page cache and                                 then essentially when you let's say try                                 to read it again all of a sudden becomes                                 much faster but that's because it's not                                 giving to you from the disk but rather                                 from the page cache so and another thing                                 is a data sampling which I had two                                 clients to work with and they wanted to                                 you know talk about do a simple sample                                 data right so the sample data means that                                 let's say they capture                                                   data for something and they wanted to                                 get ten percent of it but equally                                 representing you know evenly distributed                                 so I it's not so it means that I cannot                                 just take the first attempt the first                                 block of Records and say up to the ten                                 percent or the last block of Records up                                 to the ten percent so i have to equally                                 distribute it right but which I can't do                                 you know count how many records and then                                 divide by                                                                it's doable but if you are already know                                 for example that you're going to be                                 doing certain operations on this data                                 then certain things could be taken care                                 of in advance and for example with data                                 sampling all with data set                                 how you going to sample a ten percent                                 then let's just do one two three four in                                 the round robin and you know right ten                                 different files representing the same                                 data not the solution that will fit                                 every need but again to the custom                                 requirements that was a very easy way to                                 fix it now they have ten chunks                                 representing actually equal distribution                                 of data so efficient input formats                                 that's another interesting thing when it                                 comes to i/o how many of you use the                                 word count from Hadoop examples all                                 right that's not how you do word count I                                 mean that's the most inefficient way of                                 doing it in fact the first example will                                 kind of demonstrate some of the issues                                 with that so arm here is the mechanical                                 sympathy slide which is basically I                                 provided my own definition which is an                                 ability to write software with a deep                                 understanding of its impact or lack of                                 impact on the hardware it's running on                                 here's a link to the blog and the bottom                                 line is you want to ensure that all                                 available resources hardware and                                 software working balance to help achieve                                 your end goal and in this particular                                 case it all starts from a data capture                                 in fact I'm not going to spend too much                                 time giving the lack of time that we                                 have here but i did a series of talks on                                 fast ingest last year and one of them                                 was a hadoop summit another one was                                 looking for in the q con so you can get                                 it from there and i'll give it with                                 history of why this dog came about and                                 although the history goes back to my                                 original talk but and this one is sort                                 of a derivative but let's quickly talk                                 about the problem and why all of a                                 sudden some of the existing Hadoop                                 technologies couldn't help me so um I                                 was my first client when I came to                                 hortonworks worth one of the biggest or                                 if not the biggest wireless provider in                                 the US struts with the name a you can                                 guess what the rest of it is sorry i                                 can't look funny things that you can't                                 really mention but you can mention what                                 it is anyway so what the problem was                                 kind of described here but let me just                                 kind of                                 doing my own words so they had a devices                                 that were producing data records you                                 know the ones that NSA likes to look at                                 so and each device would produce                                 anywhere between the number here says                                                                                                         could double or triple during the spikes                                 right so that's why I was kind of                                 averaging out between                                                  to                                                                   record was about                                                         they had to for the first implementation                                 handle                                                                   and realize how much data we had to                                 ingest so when we try to use floom floom                                 even flume ng couldn't really handle                                 that volume of data with a hardware that                                 was provisioned for that particular                                 system so we had to improvise and we've                                 succeeded we actually got on our ec                                  cluster successful test where we were                                 ingesting                                                             per second for a continuous period of                                    hours course the company a lot of money                                 but at least we knew it's doable so and                                 that's really when I start one and again                                 the the way we did it and it's all                                 described in that talk is through a very                                 custom approach right but that's what I                                 that's the whole point of this                                 conversation is that a lot of times you                                 have products that will solve certain                                 problems and those are general                                 technologies and frameworks right just                                 like Google protocol buffers but then                                 there is certain things when you kind of                                 narrow down your domain could be done                                 much more efficient much better much                                 faster so we're pretty much approaching                                 the demo part so any questions so far                                 all right feel free to you know raise                                 your hand if you want to I don't want to                                 really wait till the very end because                                 you know we're not going to have enough                                 time anyway so here's my four Musketeers                                 the CPU the network the disk                                 and the memory right so these are the                                 four core resources of a computer right                                 and the speed of development or                                 evolution is not the same we might have                                 you know fast disk but slow processor or                                 maybe a processor is fast but what we're                                 trying to do with this needed requires                                 more power and so on and so forth the                                 same goes for the memory and other                                 things so in other words what I always                                 wanted to be able to do is to say listen                                 if I'm going to go to my company and say                                 I really need more hardware I want to be                                 able to prove and measure that what i                                 have is used up to the limit that i                                 cannot squeeze anything more out of my                                 hard work and in fact on that                                 presentation i'm showing some of the                                 things where I'm doing the ingest and                                 I'm choking but my cpu is relaxing it's                                 only operating at about four percent why                                 is that well because you know i started                                 my demo written in such way where I was                                 really I Oh bound so I had to start                                 improvising doing various different                                 things and all of a sudden i bringed it                                 brought it to balance where my memory                                 CPU and and I yo was working in tact and                                 essentially speed up the processing but                                 about five times so and that's                                 essentially what we wanted to do want to                                 be able to understand what our resources                                 are doing to address all these problems                                 so in other words while the topic is                                 title io or big big data i/o or whatever                                 it is titled right now at the end of the                                 day it's really more about understanding                                 how your resources are utilized during                                 big data processing so um for example I                                 mean like I said everything kind of                                 starts with the capture of the data                                 because how you capture the data will                                 affect how everything else is will                                 affect everything else down stream data                                 access searches and what have you so and                                 you know bond of things when it comes to                                 data capture especially when it comes to                                 streaming data capture you can never be                                 slower than the data source right                                 imagine what would happen if the data                                 source producing data faster than you                                 can                                 I'm going to have a big problem right so                                 like I said else is not good you always                                 have to be kind of come up with an                                 approach and you don't want to be at the                                 limit you want to be by several orders                                 of magnitude because like i said before                                 we had to deal with spikes right                                 sometimes you know it could be a natural                                 spike because of the holiday or                                 something like this or because you know                                 something going on in this big                                 metropolitan area it could be unnatural                                 spikes because a certain device was down                                 in itself accumulated the back like now                                 it's throws everything at you right so                                 you want to be able to handle that so                                 and this is where we get to the code                                 part which I'll try to spend the rest of                                 the time is doing coding so these demos                                 are all sort of a very trivial kind of a                                 simple demos and the whole purpose was                                 to kind of a draw awareness that simple                                 things that we sort of in the Big Data                                 world tend to forget are still relevant                                 right and you know even when you're                                 dealing with the third party technology                                 in your shop you kind of have to                                 understand what it does how it does it                                 and if it's going to fit if it's going                                 to fit your need so remember I said that                                 Ward count is not how you do things                                 right so here's a very simple demo                                 basically what I'm doing here I have a                                 test file i'm reading just a thousand                                 records or ten thousand records from it                                 loading it up in memory and basically                                 doing a regular expression search on                                 that file one on that what I can relate                                 is so now I have a data in memory it's                                 not even IL it's all in memory but the                                 first attempt I'm doing one line at a                                 time right the second attempt I'm doing                                 it in badge so let me run it a real                                 quick and we can set if you interested                                 we can look at the code more deeper so                                 it ran so here is the first set of                                 results I do it like ten times just to                                 get a good figure so it did get a little                                 faster that's one line at a time but                                 look at processing the same                                 accumulation the same buffer but doing                                 it as a whole the entire chunk we                                 basically improve our performance by                                 about five times right just a very                                 simple thing that could in the city if                                 you remember the word count in word                                 count one of the things that they do                                 they the reader passes you one record at                                 a time right and I understand if the                                 record is huge couple kilobytes or                                 something like that but in my case for                                 example when I was dealing with the                                     bytes worth of                                 look                                 spin off the entire map task just the                                 process one record to do string                                 tokenization and searches for searches                                 for words right so if if you simply were                                 to write at a different input file                                 format different reader then you would                                 end up in a situation where you would                                 tremendously improve the performance of                                 your of your MapReduce for this                                 particular problem but like I said this                                 is a trivial sample let's get into                                 something more interesting so yesterday                                 I actually wrote I have it had a                                 different demo but I decided to make it                                 a little more interesting so I have this                                 unoptimized right and I have an                                 optimized right so let me kind of give                                 you a little more real estate here so if                                 you look at optimize right and look at                                 unoptimized write the code is identical                                 right basically I am you know trying to                                 what I'm doing here I'm basically i had                                 this quote from Einstein and I'm just                                 writing it to a file                                                  creating a file files                                                 each right and I'm creating multiple                                 files i'm creating a thousand files and                                 i'm going to be doing it first with one                                 thread and then we're going to see what                                 the difference is between optimized                                 versus on optimized and then we're going                                 to increase the thread and see what our                                 improvements are going to be so let's                                 run unoptimized first and i expected to                                 run in about                                                            along those lines and while we waiting                                 let's look at the code here and this is                                 where this is pretty much all the code                                 right this is actually the i/o task that                                 does the actual right anybody I think                                 it's done now we're still writing come                                 on                                 alright um here we go it took                                            sometimes it's a guess my computer and I                                 don't know why but sometimes it goes                                 like the                                                                 seconds so fine anybody can look                                 actually before we do that let's do the                                 same with optimized like I said code is                                 the same but the difference in the i/o                                 task and so                                                       seconds right so obviously the one is                                 faster than the other one by about five                                 six times right so can anybody spot the                                 problem i mean i'm using buffered output                                 stream i am you know kind of doing                                 everything that I'm supposed to do when                                 I'm writing the file all right can                                 somebody spot the solution here it's                                 optimized right now all right let me                                 help you so let's look at this                                 particular line of code I'm creating a                                 new buffer output stream what's going on                                 behind the scenes buffered out with                                 stream by itself is just an object is                                 easier to create but internally because                                 it's buffered output stream it creates a                                 buffer which means it allocates the                                 memory what happens when I say close                                 garbage collection right and then the                                 next time I want to write another file                                 it creates another buffer instead of                                 possibly using or reusing the existing                                 buffer right look at the optimized                                 rights just that fact alone that I just                                 explained to you i'm using a byte                                 buffers how many of you using java niño                                 package okay so you're familiar with a                                 byte buffer so i'm using a byte buffer                                 but what I'm also doing sure I'm                                 allocating here right just like before                                 but as you can see it's for the if                                 statement which means I'm doing some                                 type of caching and you can see I have a                                 context object which is nothing more                                 than a threadlocal because it's only for                                 this thread i don't i'm going to have to                                 synchronize it right so thread once as                                 soon as a threat finishes by syncing the                                 file for the file in output stream it                                 releases that buffer that the next                                 attempt on this thread can reuse the                                 same buffer the same memory that was                                 allocated before right so all of a                                 sudden all of a sudden I've yes okay so                                 i got the question so the question is                                 that Emma locating the same size or and                                 what happens if I have a variable length                                 is a correct                                 yeah it's AK well no so I could play                                 though that ok the question is that a                                 buffer output stream the default buffer                                 size                                                                     increase it but the bottom line is that                                 even if I were to increase it I would                                 have to as soon as I get as soon as I                                 issue the clothes on the buffered output                                 stream its and create a new buffer                                 output stream the next the next cycle is                                 going to locate a new memory right we're                                 in this particular case I'm actually                                 reusing the same buffer because now I'm                                 managing the buffer separately from the                                 actual input from the actual output                                 stream so and then the other question I                                 think that I kind of formulated myself                                 in this case I kind of know the length                                 of my record and I know that I'm doing                                 it hundred thousand times so I've                                 calculated the size of the buffer but i                                 also have and implementation of byte                                 buffer kind of extended version where                                 you can create initial size and if it                                 has to expand they will expand                                 automatically by essentially copying                                 itself to a bigger buffer and so on and                                 so forth but again that's just in other                                 ways of doing things it's kind of                                 irrelevant to this particular topic so                                 so anyway um you see how simple things                                 like this could either fix your problem                                 or get you into a problem right so but                                 let's do something else let's try to                                 increase the amount of threads right                                 saying okay well I'm probably dealing                                 with IO intensive task so if I bump the                                 threads by four I should you know make                                 it much faster so let's try that and                                 again for the sake of saving time I                                 might as well keep on talking and I'll                                 tell you that it's not going to be that                                 much faster why because there is certain                                 things that are happening behind the                                 scene that will never give you give you                                 one plus one equals two so obviously I                                 your resources you're still contending                                 for the shared resource which is my                                 single disk on this machine right there                                 is still some string system.arraycopy                                 on with an internal implementation which                                 I actually learned the hard way that no                                 matter how many threads you throw at it                                 when you're dealing with native calls                                 it's not always going to equate to even                                 close to be one plus one equals two so                                 as you can see we saved were three                                 seconds right the same thing if I do an                                 optimized actually will show us a better                                 percentage which essentially shows you                                 or proves to you that the memory                                 allocation that we were requesting from                                 our buffered output stream was quite a                                 big problem for us so here we saved                                 almost a half right not necessarily four                                 times but almost a half and that's but                                 in quite interesting so another thing                                 that I was talking about and I think we                                 all have ten minutes left unfortunately                                 won't be able to show you everything but                                 another thing that I was going to talk                                 to you about is what I learned when I                                 terms of data organization and data                                 storage is that a lot of times like how                                 many how many of you one book one was                                 last time your accounting bits when it                                 came to the data like bits not bytes                                 bits okay so you think a little big data                                 why would I care about bits well think                                 about that way most of the day the                                 business data or machine data is going                                 to be in ASCII characters I know I'm in                                 Germany so I can be really you know but                                 you know most of the machine data will                                 deal with the ASCII characters how many                                 bits do I really need to store any ascii                                 character seven right so what is one bit                                 or                                                                      that fact alone could actually save you                                 a lot of storage and obviously improve                                 your because you're now                                                  but what I also learned that most of the                                 data most of the characters that are                                 used or majority of them could be storm                                 or between five and six and that                                 actually gives you into a better                                 situation now another thing is that                                 might as well turn back to the slides                                 real quick                                 kind of skipping a little bit but data                                 organization so for one of my clients we                                 have to come up with a custom file                                 format and this is kind of a pseudocode                                 showing you what this file format looked                                 like and I was trying to explain it I                                 was trying before to explain it and then                                 I decided I gotta put it in the slide so                                 you can guys it's better to understand                                 so here's kind of a sample record is it                                 slimmed down but this is actually the                                 records that I was dealing with from                                 call detail records right this is what                                 they kind of look like there's more to                                 that to the right but anyway so and so                                 that pro which is actually not new so                                 think about this way when you look at                                 this data when you analyze the data that                                 you're dealing with and it's a very                                 important topic you kind of understand                                 that because machine data and because of                                 what it represents there is going to be                                 huge amount of repetition and you can                                 say well fine if it's a huge amount of                                 repetition I can compress it right well                                 compression will give you certain                                 benefit but again when it comes to data                                 access you need to be able to you need                                 to decompress it before you can start                                 using it so what I start thinking about                                 what if I come up with a different sort                                 of a organization style which allows me                                 to achieve close to the same ratio as                                 compression or even beat the compression                                 ratio but at the same time give me an                                 uncompressed access to this data as a                                 sort of custom binary file formats so a                                 simple approach was to all those many                                 different variations that I had                                 throughout the year but one of the                                 approach that I'm kind of like more than                                 others is first of all you you you                                 chunking the data into blocks in this                                 case let's say                                                           right and for each block you're going to                                 create a dictionary of what's in that                                 block of values however you want to                                 parse them right and then you're going                                 to represent these values in other words                                 this is your dictionary and in green you                                 have basically the index just worried                                 it's not written but it's a for you to                                 read write and now I can actually write                                 the same for records as array of                                 integers or in a calendar format and                                 when you do that actually when I was                                 sort of playing with that I start                                 noticing that these things for example                                 here I have the same value this priority                                 code I've haven't seen them changing it                                 was kind of                                 radically encoded there the the                                 timestamp was constantly increasing but                                 because they produce                                                     second that means that it would be                                 increasing you know I may have the                                 entire batch that will either have one                                 branch or will have the same because I                                 have you know several hundred thousand                                 records per second and then you have                                 other values and so and so forth so the                                 bouncing black in the random and so on                                 and you know the the blue ones are same                                 and the orange ones are range which is                                 something that will change but will                                 continue to be the same until the                                 changes again so why am I saying that                                 well here's the demo so remember I                                 mentioned go ahead sure and well                                 actually not necessarily because while                                 you're you I agree with it it's                                 something confessional glutens do                                 compression algorithms some of them do                                 some of them don't give me access                                 because right now in order for me to                                 read this data and that's what I was                                 about to show you I don't have to                                 decompress the file I could actually                                 read the data as is right so no I want                                 to compress everything so so for example                                 the questions is it there or is it not                                 could be answered by simply doing a look                                 up in the dictionary without your                                 compressing anything okay some                                 compression algorithm do some don't                                 right but again these were I'm sort of                                 generalizing it for the purpose of                                 discussion there was some specific                                 requirements for example this and again                                 I can't unfortunately go through all the                                 damage they had specific requirement to                                 access data by certain fields and there                                 was other algorithms in place right but                                 the bottom line is that you know I'm not                                 saying that that that was the most                                 perfect approach actually it's not                                 because I'm working on something that                                 makes it even smoother but the bottom                                 line is that when you start thinking                                 about that it really the what I'm trying                                 to drive to is that                                 start thinking about how to organize                                 data more efficiently for the purpose of                                 doing something with it whatever it is                                 might be either use existing compression                                 algorithm that is you know as genocide                                 is splittable that you can read without                                 decompressing or a lot of times you                                 can't for variety of reasons and a lot                                 of times coming up with this simple file                                 formats they're not that difficult to                                 implement or not that difficult to                                 understand but let me show you for                                 example so when I had that string array                                 the column right so let me show you                                 integer encoding them so um Google                                 protocol buffers right so initially I                                 start using google protocol buffers to                                 encode my integers in the column so                                 here's the protocol buffer and I'm going                                 to do with it with the protocol buffer                                 i'm basically encoding i'm generating a                                 hundred thousand integers within the                                 range randomly within the range of fifty                                 thousand and we'll see from the output                                 that our sort of packing compression                                 ratio whatever it is you want to call it                                 is you know one point                                               basically so out of                                                  values Google started Google protocol                                 buffers store to this                                               right instead of four hundred thousand                                 bytes now if I go to and run the same so                                 but the thing is that what what I wanted                                 to pay attention to you too is that                                 while Google handles any type of                                 integers right in my case since I'm                                 storing offsets here I'm only dealing                                 with positive numbers so when you're                                 dealing with the positive numbers                                 there's a lot of room for improvement                                 and if you look at this demo then all of                                 a sudden                                 so we're almost to folks right almost                                 double with a google right then what the                                 Google did for this particular                                 specialized use case well now let's do                                 different things let's say like in my                                 case where I had the same value right if                                 I go with Google protocol buffers then I                                 got two times improvement but if I do                                 the same via my sort of a custom encoder                                 you see the numbers right all of a                                 sudden the entire what could be four                                 hundred thousand bytes array restored as                                 nine bites right and i have another sort                                 of a sample where I'm kind of addressing                                 the ranges of values so with the custom                                 I got you know                                                           and here i will have well i will have a                                 bigger number so you had a question you                                 had a question go ahead I mean I mean it                                 unfortunately we are getting to the end                                 of yeah I think we're on for you have                                    minutes is but let me just do a quick                                 conclusion so let me just throw I guess                                 yeah there was only a few sort of                                 conclusion slides left but I guess the                                 idea is that there's a lot of things                                 that you know products can handle or                                 product doesn't handle all that                                 efficiently and there's a lot of things                                 that simple things that you can do                                 yourself whether you meant in a custom                                 component within a framework within a                                 technology or whether it's a custom                                 solution all together that would allow                                 you to make your data simpler to use so                                 i guess that's it thank you very much                                 and I'll be around if you have asked and                                 you guys have any questions                                 you
YouTube URL: https://www.youtube.com/watch?v=fXCNz7ZKR-A


