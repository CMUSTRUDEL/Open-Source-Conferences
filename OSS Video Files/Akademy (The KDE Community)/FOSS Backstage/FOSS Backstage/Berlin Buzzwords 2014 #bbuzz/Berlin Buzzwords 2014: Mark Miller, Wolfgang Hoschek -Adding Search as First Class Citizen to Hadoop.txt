Title: Berlin Buzzwords 2014: Mark Miller, Wolfgang Hoschek -Adding Search as First Class Citizen to Hadoop
Publication date: 2014-05-28
Playlist: Berlin Buzzwords 2014 #bbuzz
Description: 
	So far Search has largely been missing as a first class citizen from the Hadoop ecosystem. We describe how Cloudera Search deeply integrates SolrCloud/Lucene with Hadoop. This enables rich user friendly low latency Search and Analytics over Big Data stored in HDFS and HBase as well as Near Real Time Search and Analytics over streaming data such as logs, social media, structured and unstructured data, all in a manner that is flexible, scalable, reliable, cost-effective and easy to operate.

GFS, MapReduce and BigTable were originally built to store and index the web. Apache Hadoop, HDFS and HBase implement these concepts in open source. Proprietary Google Search sits on top of this infrastructure, and we wanted to build something similar in open source for Hadoop.

Read more:
https://2014.berlinbuzzwords.de/session/adding-search-first-class-citizen-hadoop

About Mark Miller
https://2014.berlinbuzzwords.de/user/317/event/1

About Wolfgang Hoschek:
https://2014.berlinbuzzwords.de/user/299/event/1

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              all right hello everyone welcome to                               finding the needle in a big haystack                               we're going to be talking about                               integrating search with Hadoop this is a                               two-person talk so a lot Wolfgang start                               off with a little bit of his bio real                               quick and I'll take over all right my                               name is Wolfgang whooping hashtag and                               I'm a search engineer at the claddagh                                research team you know I previously                                worked in various different places                                including in Europe at CERN the European                                our research center for particle physics                                i worked on berkeley lab and then in                                various startups in a bay area including                                sky tag for example and what all of                                these things have in common is that you                                know I worked with you know large                                amounts of data I'm making sense out of                                game building VII servers building                                worldwide analytic systems that crunch a                                lot of data and producer you know real                                time results in the end you know after                                considerable amount of trials and errors                                and i'm also an Apache isoleucine                                committer from way back in the day I'm a                                flume commuter commit around the HBase                                indexer project will hear more on that                                later on kite and more plans and so                                forth and so without much further ado                                I'll hand it over to mark who's going to                                talk about some more yeah so quick                                introduction to me my name is Mark                                Miller I'm a leucine solar committer                                I've been playing around with we've seen                                since                                                                   two thousand eight previously I worked                                for lucid works which is the commercial                                entity behind or attempts to be the                                commercial entity behind we've seen in                                solar I was the core engineering manager                                there for a couple years and currently                                I'm a software engineer at Cloudera                                co-creator of solar cloud with yannick                                sealy which is solar's clustering and in                                distributed capabilities it's a little                                bit of the agenda first we're going to                                talk about Big Data and search kind of                                setting the stage the cloud era search                                architecture some near real-time in                                batch use cases and then finally                                conclusion                                in QA so first the enterprise data hub                                this is kind of Clara's vision of the                                future for for handling big data the                                idea is basically that you have all of                                your data in one place on HDFS and you                                bring in various processes to deal with                                that data allows you to store you know a                                ton a ton of data on commodity hardware                                very cheaply and then you know bring the                                processing to the data rather than                                moving the data around so you know you                                can see where search fits fits into this                                this is kind of taking the the single                                node model where where you need you want                                to be able to store data you want to be                                able to process data and of course you                                also want to be able to search data so                                Hadoop obviously deals with the storing                                and processing of data and you know this                                this is a talk about bringing search to                                that to that ball game so the idea of                                adding search is that search tends to be                                easy everybody knows search you see a                                Google box and it's pretty clear what to                                do type in some search terms hit return                                and you get your results back so you                                know dealing with MapReduce kind of                                takes an expert you're generally kind of                                a low level programmer you've got to                                understand some of the distributed                                paradigms you know you kind of got to be                                an expert SQL brings things up a level                                it's a little easier a lot of people                                know SQL but but generally not everybody                                knows SQL search you know everybody                                knows search everybody knows how to                                search google so bringing search the                                hadoop kind of opens up the range of                                what had you can do for people to it to                                a much wider audience so what is                                cloudera search it's basically full text                                search with faceted navigation we allow                                for for both you know efficient bat                                search batch indexing near real-time                                search where basically data is coming in                                and being indexed well people are                                searching for that data with with very                                 low latency from when it comes into when                                 you can search it on-demand indexing so                                 how we chose to integrate search the                                 Hadoop was to integrate with Apache Solr                                 Hadoop being an Apache project it's kind                                 of a nice marriage to bring in solar                                 which is also Apache based                                 very similar communities in a lot of                                 ways very similar architectures and how                                 things are configured has a very                                 established mature community it's always                                 been around since about two thousand six                                 so it's it's it's been you know fairly                                 hardened over time although some of the                                 distributed stuff is a little newer it                                 started out as a single box solution and                                 now people are using it you know up to                                 two hundreds of boxes in some cases um                                 so basically the idea to integrate                                 search you know what do we do first we                                 basically decided you know let's do some                                 first-class integrations with the rest                                 of the Hadoop ecosystem so let's                                 integrate solar with MapReduce let's                                 integrate solar with HBase let's                                 integrate solar with flume all of these                                 kind of well-known Hadoop projects lets                                 you know contribute back to to each of                                 these open source projects all of these                                 integration points to make it really                                 easy to to add search to what                                 everybody's already doing with Hadoop so                                 we're using one hundred percent solar                                 it's all you know unmodified open-source                                 solar we don't really have anything                                 proprietary we're just taking what's                                 already out there and integrating it                                 with the rest of these projects so that                                 the cloud our search architecture                                 overview in the middle there you have                                 the soil cloud cluster clad air search                                 only works in solar cloud mode solar                                 basically has two modes one which is                                 kind of the old mode is mostly a single                                 box solution it did it did offer                                 previously distributed search but it was                                 up to you to kind of index the data                                 yourself to deal with failover or fault                                 tolerance it was really easy to search                                 across multiple boxes but you know there                                 was a lot of do-it-yourself clue to make                                 that work well solar cloud kind of adds                                 a lot of the cluster and high                                 availability and automatic failover that                                 a lot of people kind of had to used to                                 build themselves now you get it out of                                 the box so as you can see we we've done                                 integrations both with HDFS so that                                 solar cloud will run natively on HDFS                                 rather than on the local file system                                 integration with                                 space so that as you're indexing dated                                 HBase that that can immediately be                                 available for search as well as flume                                 data can come into flume and be piped                                 off in the solar along with you know                                 whatever other sinks you have like HDFS                                 and then it's basically a high-level                                 overview will kind of dig into some of                                 these another great integration is with                                 Hugh he was a graphical interface into a                                 lot of the Hadoop projects and it's                                 added some really great support for                                 solar where you can kind of generate                                 these really quick GUI search                                 integrations where you can do drag and                                 drop to to kind of create a whole                                 interface for searching and and having                                 facets on the side where you just you                                 know drag over this is where I want my                                 facets this is the fields I want to pass                                 it on um if you haven't checked out you                                 really really awesome interface and do a                                 lot of Hadoop projects and and that the                                 job they've done integrating with solar                                 is it's pretty amazing so some of the                                 challenges we had in doing this                                 integration um you know making it very                                 scalable very reliable that's that's                                 kind of still an ongoing process but                                 what we've done a lot of work to improve                                 this just the fact that we run natively                                 on HDFS adds a lot of reliability                                 because HDFS is very hardened stable you                                 know has its own replication it's it's                                 been around long enough in bank online                                 enough by by many you know large                                 organizations people doing stuff with a                                 lot of data so it's it's it's just very                                 solid and adds an extra layer of                                 reliability you're not likely to Tino to                                 have a corrupt index or lose any data we                                 wanted to provide near real-time search                                 at at a very large scale you know people                                 dealing with the loop are generally                                 dealing with terabytes or petabytes of                                 data so you know we want to be able to                                 be able ingest data at a very high                                 volume while still providing your                                 real-time search at the same time we                                 want to be able to index a lot of data                                 very fast you know if you can't index                                 that fast and you're dealing with                                 terabytes of data then you know searches                                 is kind of useless you don't want to                                 take a year to be able to put in your                                 your terabytes of data so that's that's                                 really where kind of our MapReduce                                 integration come                                 where you can use your you know                                          MapReduce cluster to build indexes and                                 deploy them to Silver Cloud and then                                 another issue was usability there's this                                 kind of been a lot of usability issues                                 in terms of getting solar and solar                                 cloud up to speed the community is                                 focused a lot on some of the the core                                 problems that are kind of deeper and                                 more technical and not as much on the                                 usability so as we were doing these                                 integrations we spent a lot of time                                 making usability a little better we                                 worked on a tool called solar control                                 that makes it a lot easier to kind of                                 manage configuration and and creating                                 and collections and adding replicas and                                 things like that so we spend a lot of                                 time kind of improving lose a bit                                 usability so a little bit about apache                                 Lucene and solar which is what we're                                 integrating in we've seen is a full-text                                 search library extremely fast extremely                                 compact extremely efficient probably                                 most of you have heard of it it's pretty                                 much that a facto standard for search                                 these days easily the best open-source                                 search library I think I used to work a                                 lot more on the scene it's it's an                                 extremely fun project to work on has a                                 fantastic community I spend a lot more                                 time on solar now solar is basically a                                 search engine built on top of leucine                                 that adds things like highlighting                                 faceted search spell checking the                                 distributed capabilities that come with                                 solar cloud so kind of like the next                                 layer up and then so solar cloud itself                                 this this was basically an initiative                                 started in                                                         basically attempting to make large-scale                                 distributed search easier like I said                                 before solar even back in like                                      could do distributed search but it was                                 up to you to figure out how to get your                                 data on on the individual nodes solar                                 cloud basically starts doing that                                 automatically for you you just start                                 adding data to the cluster it figures                                 out where to put the data by hashing it                                 and it kind of handles you know if a                                 node goes down how to deal with that and                                 and and provide fault tolerance search                                 um yeah so so basically one of the first                                 things we started working on was making                                 solar                                 that it could natively work with HDFS                                 this was fairly important to Cloudera                                 because pretty much all of their systems                                 run on HDFS and you really don't want to                                 introduce a new component that relies on                                 local file system storage because it's                                 just a new level of complexity in terms                                 of dealing with configuration and                                 management you know you've got to look                                 at each node and you've got to make sure                                 they all have the right amount of                                 filesystem space and if you're running                                 out of space you've got a deal with each                                 of those nodes and add more space once                                 you're running on HDFS that becomes much                                 less of a headache you can use existing                                 tools and management applications just                                 add more space to your HDFS cluster and                                 it's kind of less of a management                                 headache you get some other benefits by                                 running out running off HDFS let's say                                 you want to use MapReduce to to build                                 indexes you can immediately start                                 serving those indexes straight from HDFS                                 with solar without having to like copy                                 them off to the local filesystem a                                 practice that there was pretty common in                                 the past you also can do some cool                                 things in terms of failover that I'll                                 talk about a little more so the HDFS                                 integration there's a low level                                         directory abstraction that allows you to                                 basically plug in a new class so that                                 you can write a leucine or really seen                                 index from almost anything if you                                 implement this directory structure so                                 what we did is there's a project called                                 Apache blur that kind of you know headed                                 the way in this and they they                                 implemented what's called an HDFS                                 directory which knows how to read and                                 write directly to HDFS so you know we                                 talked to them and we ended up borrowing                                 this code there can be some performance                                 issues reading writing the HDFS it's                                 actually faster than you think but in                                 some cases the reeds can be slower than                                 you'd like so they also introduced                                 what's called a block cash so that when                                 you're doing reads you know it'll read a                                 lot more and cash it locally you can you                                 can do this off heap as well using                                 direct buffers to to make sure you're                                 not running into garbage collection                                 issues and you don't have to size or                                 heap really huge this this block cash is                                 basically a replacement for the                                 the local file system cache in a lot of                                 cases so you tend to want it to be a ton                                 of RAM and having it off heap just                                 basically makes that a lot more                                 efficient so let's see so basically in                                 solar there's a directory factory                                 extraction that lets you plug in                                 different we've seen directory                                 implementations in the past there's                                 actually the only one that really worked                                 well with solar and that was the local                                 file system implementation and so like                                 if you wanted to replication or some of                                 the other more advanced features and                                 solar if only worked if you had this                                 local file system implementation so one                                 of the first things we did is we started                                 expanding solar so that it could                                 actually do replication and all of these                                 things with any valid directly                                 implementation so that led us basically                                 add this HDFS implementation and still                                 take advantage of replication which is                                 pretty important for solar cloud when a                                 node goes down and you bring it back up                                 later it's got to replicate from you                                 know an existing node that already has                                 the data so basically we kind of                                 borrowed this HDFS directly                                 implementation we plugged it into solar                                 we kind of spruced it up to make it work                                 a little better with solar there were a                                 couple of oddities that didn't really                                 fit with our architecture but we spruced                                 it up we got all that to work pretty                                 well there's still some optimizations we                                 want to do but we found performance was                                 actually fairly similar to local                                 filesystem solar cloud also has a                                 transaction log so that if a node goes                                 down if it crashes when it starts back                                 up it can replay from the transaction                                 log and not lose any data this was also                                 written to the local filesystem and so                                 having that in the local file system                                 again the same as having your index on                                 the local filesystem kind of added a                                 management headache so we also created                                 new implementation for that they can                                 write the transaction log directly to                                 HDFS there's there's kind of some side                                 benefits in this and that if you only                                 have if you have no replicas and you                                 only have one node serving a shard and                                 that shard goes down you can actually                                 bring that shard up on another node and                                 still have the transaction log to replay                                 and not actually lose any data so once                                 we implemented both of these                                 abstractions we basically had kind of                                 first-class support for HDFS with solar                                 when you start up solar you can pass a                                 couple system property                                 and basically be writing all of your                                 data other than the logs to HDFS so auto                                 auto replica failover this is kind of                                 actually one of the great things to come                                 out of running on HDFS um so when you're                                 running on a CFS if a node goes down                                 that that data is actually still                                 available in HDFS so in the standard                                 local filesystem case if a node goes                                 down you have to have a replica still                                 around to fall over to right and then                                 you can add more replicas and they'll                                 replicate the index from the existing                                 replica to get back up to speed but if                                 you lose all of your replicas for a                                 shard your kind of dead in the water                                 you're not going to be able to serve you                                 know one part of your index until                                 someone comes in and manually brings                                 those machines back up so this is a                                 feature that I'm actually currently                                 working on it's not finished but it's                                 pretty close it's going to be in soon                                 and and what this lets you do is even if                                 you lose every replica in your shard                                 because that data still served in HDFS                                 you can actually recreate a core a solar                                 core on a machine that's still up and it                                 can start serving that data from that                                 shard and and and this is kind of a big                                 advantage to the local file system and                                 that you can you you know you can                                 essentially if you start with                                          and you go down to only one node up and                                 all the rest died you can essentially                                 fall over to serving your entire index                                 from that one node in most cases it's                                 not going to be performing to go down to                                 one node but you know it's it's just                                 kind of cool that basically it's it's                                 you know fault tolerant down to pretty                                 much any level because the data is                                 always available in HDFS and it just                                 takes starting up a solar core on an                                 existing machine to start serving it                                 again I'll pass over to Wolfgang to talk                                 about a couple of the other integrations                                 all right so let's talk a little bit                                 about more of the injection site so we                                 have some integration points in a near                                 real-time indexing into solar using                                 apache flume as an ingestion system                                 flume is you know one of the projects as                                 part of the Hadoop ecosystem that got                                 very widely used                                 other people use it to stream data into                                 HDFS for SNA as a near real-time                                 mechanism to say like no load lock files                                 that are incrementally you know produced                                 in to HDFS other kinds of data not just                                 like text data but also binary data or                                 all sorts of things and so we took flume                                 and flume meaning a bunch of flume                                 agents and have them consume data such                                 as log files or other kind of in front                                 of data and then forward have this state                                 of being forwarded through various flume                                 agents in a distributed system you know                                 hierarchy or graph or tree shaped like                                 form into solar and into HDFS starting                                 from the notion initially that all data                                 lands in HDFS pretty much unmodified so                                 that is for her for a very long time has                                 been like the the common mode of                                 operating Hadoop that whatever data gets                                 ingested pretty much gets stored in its                                 raw form without with little or no                                 formatting or transformation and so                                 given that what we had to do is find a                                 way how to take that data in near real                                 time and also stuff it into solar but as                                 you will of course know it doesn't make                                 too much sense to store raw data and                                 solar but rather that data needs to                                 conform to some search application model                                 that actually makes sense and so there                                 is some some some step in between that                                 needs to be done in order to convert the                                 data from A to B into you know into the                                 format that's actually make sense for a                                 search application and so flume is a                                 very modular system you can plug                                 yourself in at various levels and very                                 various different points and the most                                 natural point for plugging in this solar                                 ingestion mechanism is at the sink level                                 where you have something called a flume                                 more flying solar sync which is sense                                 essentially a plug-in point that that                                 can take a stream of data and then                                 transform that theater into whatever                                 data model is required by solar and then                                 send it es / j standard solar api into                                 into solar for indexing and                                 you can type the very same data also                                 into HDFS so it's it you but usually                                 people use this you know to stream data                                 into both targets rather than it into                                 one target to retain the ability to have                                 all of their data in HDFS in its raw                                 form so later on you can run arbitrary                                 processing over it you can change your                                 mind you might not even know today what                                 you're going to do with your data are                                 year from now so you have all the                                 flexibility that specially Hadoop hoop                                 group gives you because you have rotted                                 in HDFS but you also have what is                                 currently today the best to the best of                                 our knowledge you know what's what's the                                 most meaningful way how to index that                                 data and solar and make it available to                                 searches through that near real-time                                 mechanism so it's not an either/or but                                 it's a you know both of these things are                                 available to you and typically what                                 people use both in convene combination                                 then flume is also reliable in the sense                                 that you know you can fail over between                                 hosts that go down it can also scale out                                 in that you can petition it so many                                 nodes can work on subsets of the in                                 potato at the same time so this is a                                 scalable and reliable ingestion system                                 that works very well and the way it gets                                 configured is by a little configuration                                 file that refers to what we call a more                                 flying configuration file yet another                                 configuration file that essentially in                                 some dsl small little dsl type like form                                 specifies how the input data and you                                 know whether it's a log line or whether                                 it's a you know a photo or whether it's                                 a microsoft word document or maybe it's                                 like some clickstream data or whatever                                 it may be how it gets transformed the                                 other thing that people told us you know                                 would be really interesting and really                                 very valuable is to make a HBase be able                                 to research over to a is to make a                                 system that can search data that's                                 stored in HBase so HBase is you know a                                 no sequel store that is different from                                 HDFS in that you know you can update                                 data in it which you can in HDFS very                                 easily so it's an incident it's a                                 storage manager if you like that you                                 can be used for old TP updates and so                                 these old EP updates you know should be                                 able should should be able to to get                                 searched take like for example you know                                 an online shopping store or something                                 like an ebay type like system or so you                                 can imagine that people as they update                                 their data into database you know other                                 applications would like to search that                                 data and they would rather like to do                                 that in year real time rather than say                                 like an hour down the road or maybe a                                 data on the road and so we came up with                                 this way of plugging ourselves into HBS                                 in one particular way that allows us to                                 do this in a non intrusive way meaning                                 that we don't actually perturb you know                                 their reliability and and the                                 performance of the HBase itself and the                                 applications that I do updates in a                                 space and we plugged ourselves in a way                                 that's fairly flexible so you can                                 specify how exactly you know the data                                 should be indexed and transformed on the                                 way it scales in that you can                                 horizontally scale it out and use any                                 number of nodes and processes to do that                                 kind of indexing step and it's also                                 reliable in that you know no data will                                 ever get lost and so the data that                                 hasn't been delivered yet to the                                 indexing system will be retried and it                                 will eventually make it intuitive solar                                 so you can keep your solar index                                 consistent with the HBase index as long                                 as you allow for a little bit of lag so                                 in other words this is an eventually                                 consistent system not a system that is                                 immediately consistent and the reason                                 why is because for search applications                                 more often than not indeed in the                                 overwhelming percentage of cases this is                                 good enough it's good enough to be                                 eventually consistent as long as you                                 will be consistent there I know in a                                 fairly reasonable amount of time and the                                 strong consistency is not required this                                 allow is the key feature that allows us                                 to put ourselves outside of the right                                 path of HBase so we don't we don't                                 impact another reliability and the                                 performance of the primary application                                 that applies updates to hbase                                 again this component called the lily                                 HBase indexer that I've been describing                                 is configured through this little                                 configuration file called more flying                                 configuration file this dsl that where                                 you can say like you know take that HP                                 seller that the HBase row and here's how                                 you transform it into a solar input                                 document or you know it conforms to the                                 leucine data model and you know what                                 what the data what the data actually is                                 that you want to index an indexing solar                                 for that HP cell and how it should be                                 analyzed and so forth this is actually                                 work that we've done with our good                                 colleagues at energy data we were very                                 very happy to find out that they had                                 goals that have a very similar to ours                                 and so we set out to do this together as                                 an open source project it's available in                                 github under and under the apache                                 license you're welcome to check that out                                 and chime in and help us move this                                 forward it's essentially you know                                 implemented as a listener to the                                 replication API that HBase has had for a                                 very long time and that is by now very                                 much hardened and reliable and in a                                 heavy used by many customers so we                                 pretend so H besides these API where you                                 can say we're similar to say my sequel                                 Oracle or indeed any any other database                                 you can listen in to the to the oltp                                 updates that are are applied to the base                                 table you know and so that a replication                                 API essentially look plays back you know                                 all the edits that happened to to a to H                                 base and we just simply you know plug                                 ourselves in at that level and pretend                                 to be a secondary HBase cluster you know                                 which we are not but we should pretend                                 to be an                                 hbase.regionserver.blockcachecount                                 rather than store it the data into into                                 yet another secondary HBase we store it                                 in solar you know after having                                 transformed it using this this                                 morpholine ETL step                                 alright so after having a this I should                                 I should mention also that we we early                                 on we decided that you know you know                                 Hadoop traditionally comes from comes                                 from a world of batch for from a world                                 of massive massively scalable batch                                 processing and year relative was going                                 to be important moving forward and                                 becoming more and more important over                                 time but the need for badge isn't going                                 to go away anytime soon and indeed you                                 know my impression is that you will                                 never go away and then the more near                                 real-time processing you do as a side                                 effect the more data you're going to                                 store and keep you know perhaps in some                                 historic archive in HDFS and the more                                 important actually a scale scalable                                 batch processing is is going to be so                                 batch isn't going going away at all and                                 we need to support pat bad batch in a in                                 a performant way it's just a fact of                                 life the batch processing is more cost                                 effective and                                                        item than a near real-time in a                                 processing could ever be so bad for                                 remains very important and so we                                 integrated solar and MapReduce and the                                 result is you know a system that again                                 is very flexible in that you can specify                                 how your data should be transformed and                                 massaged into something that solar                                 understands it's scalable in that it can                                 route take advantage of all the mapper                                 slots and reduces lots that you have on                                 your HBase class on your map reduce                                 cluster which can be you know very very                                 large even though you might your solar                                 installation might not be so so large                                 you can still take take advantage of                                 your a big MapReduce cluster that maybe                                 is you know also used for other purposes                                 and you will just use it for in some                                 massive indexing stat for for a little                                 while and then hand it off hand it back                                 to some some other tenants on that                                 cluster and so in this way by massively                                 parallel izing the indexing step in some                                 reliable way which is what you know                                 MapReduce basically stands for you know                                 we we allow people the flexibility to                                 re-index their data without having to                                 wait for a year which is                                 really unacceptable or to change their                                 mind later about how their you know                                 their index we should really look like                                 and so they're able to iterate you know                                 on their data model without it being                                 prohibitive and so the other interesting                                 thing about combining a batch and year                                 real-time here is that you were using                                 this you can implement something like a                                 lander architecture were you index you                                 know you know the last year of data that                                 you integrate that you have using                                 MapReduce into solar and then going                                 forward you know the the most recent                                 data that's streaming through some                                 online system with updates that happen                                 you know air yeah that happened                                 immediately though this this data can be                                 interested in near real time using flume                                 directly into solar so you can combine                                 those two approaches to you know to                                 index today ninety-nine percent of the                                 data using MapReduce and the remaining                                 remaining                                                              that's generated right now in near                                 real-time put them all into you know the                                 same solar a cloud cluster and then                                 answer queries that combine the historic                                 theater and the most recent data using a                                 uniform interface so that's a fairly                                 interesting so the way it would you can                                 also you can index data that's in HDFS                                 just plain files whatever files they may                                 be what are the sequence files the text                                 files or whether it's parquet files or                                 whether it's a ver files or you know                                 some other arbitrary data format or you                                 can you know index HBase tables using                                 MapReduce in this in this scalable batch                                 indexing step so the way it works is                                 that essentially a bunch of mappers get                                 fired off they munch you know take take                                 take a subset of the input files you                                 know transform the data produce solar                                 input documents you know they get sent                                 off to a bunch of reducers which run you                                 know like on that MapReduce cluster the                                 reducers actually each producer is like                                 a small little you know solar shard in                                 its own way like a micro chart it's it's                                 it's a fully fully functional solar                                 server that just happens to be                                 instantiated temporarily indexes data                                 writes it to HDFS and                                 and when all the reduces are finished                                 you can optionally you know merge a                                 large number of charge into a smaller                                 number of charge meaning that the number                                 of shots that you desire for your solar                                 for your solar cluster and then you can                                 using a go live step merge those                                 segments merge those indexes into the                                 solar cloud as an as a final step and                                 that final step is optional you don't                                 have to do that if you're happy to just                                 bring up your solar service in the way                                 mark already suggested that just like                                 read data from HDFS you can do that too                                 or you know using that goal of step you                                 can merge the data into a live customer                                 facing solid Rochester I already talked                                 about most of these things here one of                                 the interesting things ended up a little                                 bit more on the technical side is that                                 you know indeed you can use more reduces                                 that you have solar charge doing the                                 indexing this is pretty interesting                                 because it turns out that you know                                 indexing is by and large a CPU bound                                 step so it makes sense to use all of the                                 CPU cores that you have your available                                 in your cluster if that's what you                                 desire according to your you know your                                 your administration and policies if                                 that's what you desire to do the                                 indexing so you can use all reduces lots                                 on your cluster if you want to do the                                 indexing and then there's a final                                 merging step to merge the segment's into                                 a smaller number of segments which is                                 comparatively less expensive than the                                 indexing into in in in the reducers                                 itself so this is why these works in                                 such a scalable manner much more                                 scalable than you know having a map or                                 only job they would send the data                                 directly using solo che into a solar                                 client cluster all right a couple of                                 minutes on the little library that we                                 are that we developed in or for that                                 purpose there's a little library for                                 extraction transformation and loading in                                 a streaming manner that we developed is                                 called kite morph Lions it's an immortal                                 and life so Java library that was                                 developed as part of the cloud there a                                 search project and since we found out                                 you know that is actually applicable in                                 other contexts as well we you know so                                 like                                 moved it out of cloud air search and put                                 it into a general-purpose reusable forum                                 such as a kite and the way it works is                                 like and it's actually very simple it's                                 a Java library that allows you to take                                 any take data from any kind of data                                 source and process it in whatever                                 arbitrary way and then sent the results                                 you know into some kind of target and                                 whatever that target maybe you can you                                 can you can plug yourself in there                                 typically solar or it's HDFS or it might                                 be HBase or you know whatever and so at                                 its core it's basically a chain of                                 commands that take some in potato it's                                 just like similar to unix pipelines you                                 know at least conceptually take some you                                 know each of the components take some in                                 potato munches it spit something out and                                 the next component you know again takes                                 the output of the previous component                                 punches it in some way transforms it now                                 that's something with it and maybe                                 cleans the data or annotate it you know                                 and then sends it to the next command                                 and that that command might be a command                                 that's that actually sends the data into                                 solar you know such as the load solar                                 command so using this this mechanism                                 people are able to describe you know                                 like sort of item one item at a time                                 type like transformations in a very                                 simple way and lots of people are using                                 this especially because it doesn't                                 actually require any kind of you know                                 sophisticated programming it's it's a                                 DSL that can be used but simply by                                 editing a configuration file and so it's                                 fairly approachable and easy to get                                 started with and people have had lots of                                 success indexing data in you know in                                 various manners without actually having                                 to be Java programmers pre-flight to                                 mention that you know you could use this                                 library not just                                                       the solar cloud but into your own                                 application it's really you know has no                                 dependency on solar cloud at all except                                 for that there is one optional module                                 you know for solar and that module is                                 responsible to implement the commands                                 you know that relate to loading data                                 into solar but all the other commands                                 you know are in different maven modules                                 and so you can use this independently                                 for other search service if to if that's                                 what you wanted to do here's a brief                                 example of                                 more flying configuration file that you                                 know say assume you have a simple line                                 you know from some textual log file the                                 typical kind of thing that has you know                                 like a date in there and maybe some a                                 bunch of strings that relate to who what                                 process wrote this stuff is just looked                                 ye or something like this and and a                                 message or so in some say me say me                                 informal you know format and you'd like                                 to extract you know a bunch of fields                                 out of this you know select a priority                                 times and hostname so forth and so forth                                 people usually reside to regular                                 expressions to to parse this stuff in                                 more or less nasty manners out of these                                 text files that have they have never                                 been you know implemented you know with                                 much of an API in mind and so there's                                 one command such as that's called garage                                 and you can specify there some rules as                                 to know how some regular expressions as                                 to how to extract the data out of that                                 text line and where a field starts and                                 where it stops and so forth basically                                 the way it works is like it's like a                                 it's like a convenient you know user                                 interface to regular expressions you                                 don't specify a regular expression by                                 writing it down but rather by specify a                                 regular expression by name in this case                                 like say like positive integer or syslog                                 time stamp or syslog host and that                                 really is a regular expression that's in                                 some external configuration file that                                 you don't have to write that somebody                                 else already wrote so you can reuse                                 regular expressions simply by you know                                 using them by name and you might be                                 familiar with this because this is                                 actually as a matter of fact precisely                                 what locks tinged us and we felt free no                                 Twp you know this great idea that the                                 lock says people had and you know we                                 provide exactly the same kind of feature                                 set that locks those users you know to                                 do this it's a it's a great idea and we                                 were happy to take advantage of of that                                 similar leader commands say like to you                                 know split a file into a records you                                 know in case of women records are spread                                 across multiple lines and you don't spar                                 system line by line but rather you know                                 multi-line thing so there's a variety of                                 of of commands in there for all sorts of                                 purposes that you can you can pick up                                 and sort of sort of satisfies the                                      rule                                 if there's stuff that isn't part of the                                 predefined you know functionality set no                                 you can extend it in the library in a                                 bunch of ways the easiest way is to just                                 write some Java code as a snippet sort                                 of like directly in line in that                                 configuration file and it gets compiled                                 on the fly you know as the program runs                                 and then you know gets compiled in the                                 regular a java bytecodes you know and                                 then executed at runtime and so you can                                 if you're all your logic is like lower                                 casing some strings you know or doing a                                 little bit of you know processing and                                 one or two or three or four five lines                                 of Java it's entirely you know rational                                 to just dump a little bit of Java code                                 right into the configuration file and so                                 that gets gets executed and takes care                                 of your custom functionality there is a                                 as i try to hint that you know the usual                                 there's commands with the usual suspects                                 in the Hadoop ecosystem like parque                                 files are                                                             forth Jason XML and then you know we                                 also import into teka project so                                 whatever parses are available as part of                                 tika you can use those too so you know                                 they're there are thousands of parcels                                 out there that you don't want to write                                 trust me but you know you can take                                 advantage you know if tika with which                                 has already done you know a large                                 fraction of the hard work out there and                                 so we reuse that as i mentioned HBase                                 rose sales and importantly you can also                                 write your own commands of course so you                                 can plug in your own command and then                                 that command does it's just a regular                                 Java class and it would take like you                                 know                                                                    you want so that's essentially the idea                                 behind most lines a bunch of other                                 things you know like geo super Cheers                                 report I you know and so forth and I'm                                 not going to go to detail that the way                                 it works is in regarding performance is                                 that a very simple approach similar to                                 the map it whose approach you know each                                 more flying is really compiled on the                                 fly and runs in a single thread all the                                 commands run in the very same single                                 thread and sew handing off data from one                                 command to the other is truly just a                                 method call there's no serialization in                                 between                                 there's no Q in between there's no                                 threat context switches no nothing it's                                 just like you know one object calling                                 another object with nothing in between                                 this is the reason why this is really                                 fast and so the way you get scale out is                                 by simply running many instances of the                                 more flying typically one per cpu core a                                 two per cpu core among on many nodes so                                 that's a typical kind of embarrassing                                 lee parallel processing model okay and                                 then as Mark already hinted you know                                  there's a nice uui that's sits on top of                                  it that that you will use presumably to                                  get started super easy to get started                                  very very sweet and sexy but eventually                                  you might actually end up with your own                                  UI and you know he can you know of                                  course you don't have to use that you                                  know any solar application that runs on                                  standard solar will also run on Cloudera                                  search and sodas there's like no locking                                  or anything like this but this is a good                                  starting point we also integrated a                                  search with security infrastructure                                  called Apache century and so we worked                                  on cluster level access control index                                  level access control the latest thing is                                  document level access control and then                                  going forward there is going to be some                                  work on field level access control as                                  well as well to make sure that the only                                  substance of data are available to the                                  right kind of people with the right kind                                  of integrate kind of roles and                                  permissions and then you know the way it                                  gets managed is through clattering                                  manager I think that you know like I                                  mean the basic the basic scale out                                  preposition is that you can scale out                                  your data in your cluster and whatever                                  it is that you're doing by a factor XA                                  factor                                                                  having to employ                                                          hundred times more people that's you                                  know if you have to hire a no x times                                  more people to do you know x times more                                  more data working that's just a red flag                                  it's not going to happen and so cloudera                                  manager is like the letter answer to                                  this you know where we say like you know                                  you can operate manage you know your                                  cluster without having to you employ a                                  huge amount of new human resources and                                  so in in doing so you'll save a lot of                                  time and effort and so that's about it                                  you can try this out you know there                                  a new project that recently went live                                  called Cloudera life that's maybe the                                  the easiest way to get started is                                  basically just you know an online                                  website women of super already running                                  an instantiated and you can see with                                  some a bunch of sample data sets and all                                  of the you know services already up and                                  running and you can just click around                                  there and you know do your own your own                                  thing and try it out and then maybe you                                  know later on dry which on machine or                                  whatever so it's a new easy way to to                                  see how to our works in practice alright                                  thanks very much i think we've used up                                  by our time i hope we have still have                                  time for some questions you know                                  considering that this is the last                                  session so we get the little bit of an                                  advantage over everybody else out there                                  okay questions are you planning to run                                  solo on yon as well as it a standalone                                  demon we think with we're thinking about                                  it at this time you know we're not                                  running on yarn precisely because you                                  know as you know like people usually                                  expect solo two key results in pretty                                  much millisecond time frame in a very                                  very low latency results you know for                                  interactive theories and you know                                  integrating I know solar with a resource                                  management system that's more a                                  heavyweight so integrating into a system                                  that that's more heavyweights such as                                  yarn is a bit of a challenge so at this                                  point we are not providing yarn                                  integration but we're exploring and                                  considering in like how what's best way                                  to move form it is definitely the idea                                  is that we integrate in a resource                                  control system at this point in time it                                  is really just in look at the operating                                  system                                  you know where you know you're                                  statically in partition you know the                                  resources rather than dynamically                                  because this is justin billion no not                                  enough time to switch in a millisecond                                  time frame um yes maybe one for mark but                                  you mentioned that celeron HDFS had                                  before okay oh hello hi one for mark you                                  mentioned that sullen HDFS had                                  performance fairly similar to the local                                  phone system could you elaborate on that                                  a bit yeah I mean we haven't done their                                  extensive testing to where I i I'd be                                  willing to give like conclusive numbers                                  but in kind of my like you know kind of                                  off the cuff testing I've generally seen                                  you know maybe a ten to fifteen percent                                  performance loss although there's                                  there's a lot of optimizations that we                                  haven't done yet we haven't turned on                                  like you know short circuit reads we                                  have in there's various kind of coding                                  optimizations actually one of our HDFS                                  guys Todd with Kim I was looking through                                  some of the code that we did and he had                                  various ideas for some speed ups so                                  right now it's actually been pretty                                  close but we think that that will be                                  able to pretty much match it in a lot of                                  cases too because people using Hadoop                                  tend to have like                                                         of stripes across that you can actually                                  see greater performance although then                                  it's not really in an apples-to-apples                                  comparison on so there is some overhead                                  our goal is to actually you know meet                                  local file system performance exactly                                  and we think with with a couple changes                                  we can eventually get there we've                                  definitely focused more on I'm kind of                                  stability and initial features over                                  performance so far though okay follow-up                                  question with do you think you'll ever                                  support SSDs as well it's all kind of                                  splits HDFS to get kind of faster disk                                  i/o for the solo instances so will it                                  will be on a single HDFS Wolfgang's are                                  the end of SSDs                                  there's nothing that prevents you today                                  too you know configure and provision and                                  HDFS cluster sitting on SSDs if that's                                  what you want to do i do know that going                                  forward you know people are thinking                                  about introducing hsm there's a                                  hierarchical storage management instead                                  into a tube were you know you could you                                  know combined you know spindles you know                                  rotating disk and just ease and rob and                                  perhaps nug non-volatile memory and her                                  other storage tiers somewhere somewhere                                  in between but that is future talk hi                                  have you done any stats run lower                                  streaming files from s                                             because in in one of the case i am                                  trying like we have millions of files in                                  s                                                                     time to complete the whole indexing we                                  don't have an s                                                           of the box you know we don't provide                                  functionality of that but having said                                  that i don't anticipate you know that                                  any slower than whenever the network                                  link it gives you a coming out of                                  currently the marine dexter tool streams                                  their data in the morin map runner it                                  just reached a whole file and then                                  indexes it so why not we had a kind of a                                  combined file input format or something                                  because each file would not fill the                                  whole life hdfs block so that you know                                  the split allocations and other things                                  which are due provides so thus the file                                  is being streamed read in the streaming                                  fashion you know it's not like read all                                  into main memory and then once what's my                                  main memory gets indexed so it is read                                  in a string fashion if that's the                                  question                                  right so the current production                                  implementation is optimized for                                  flexibility in that it allows you to to                                  index and process arbitrary input data                                  formats including formats that are not                                  suitable in the in the head tube sense                                  so like you know like multi-line you                                  know you know input formats you know                                  where you know like sometimes you know a                                  record is like five lines line sometimes                                  it's like three lines lon and things                                  like that so our CSV files you know this                                  man multiple lines lines and so forth                                  but so it's optimized for flexibility                                  and also reads you know from reads one                                  file in one thread but we also have a                                  tool in the works you know that allows                                  you to combine you know splittable input                                  files and non splittable until input                                  files so that splittable input files can                                  be distributed among as many task as                                  they are input splits okay thank you                                  very much and I think we move on to the                                  party now and you can ask them questions                                  there thank you
YouTube URL: https://www.youtube.com/watch?v=DVo1SLbhNIE


