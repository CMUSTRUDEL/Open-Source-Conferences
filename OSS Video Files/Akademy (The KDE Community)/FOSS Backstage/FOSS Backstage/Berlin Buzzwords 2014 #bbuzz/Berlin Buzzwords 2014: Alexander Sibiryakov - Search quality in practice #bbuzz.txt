Title: Berlin Buzzwords 2014: Alexander Sibiryakov - Search quality in practice #bbuzz
Publication date: 2014-05-28
Playlist: Berlin Buzzwords 2014 #bbuzz
Description: 
	1. Understanding search quality: relevancy, snippets, user interface(?)
2. How to measure search quality: metrics, comparison of two search systems request-by-request, classic evaluation of top N, by-pair evaluation with swiss system. The cheapest way.
3. Examples of search quality problems.
4. Production system. Which data available: clicks, queries, shows in SERPs.
5. Text relevancy ranking: different approaches, absence of silver bullet. BM25, tf*idf, using hits of different types, using language models, quorum, words proximity in query and document.
6. How to effectively mix all signals: manual linear model, polynomial model, gradient decision trees, known implementations. Where to get labels?
7. Doing snippets well: candidates labeling, blind test, infrastructure for candidates ranking, features examples, infrastructure for candidates features and ranking, features examples.
8. How to measure search quality using clicks?
9. Other signals: comments, likes.
10. Example project: Filesystem path classifier based on search results.

Read more:
https://2014.berlinbuzzwords.de/session/search-quality-practice

About Alexander Sibiryakov:
https://2014.berlinbuzzwords.de/user/157/event/1

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              so hi everybody I'm really glad that you                               are all here my name is cibulkova                               Alexander and I will be talking about                               search quality in practice so here's the                               agenda of my talk please show me the                               hands who knows what Yandex is Wow let's                               get so Yandex is the biggest search                               engine in Russia it's like Google and I                               unfortunately I didn't have a slide                                about my my previous experience but I                                will just say it so i was working five                                years in the department of search                                quality in Yandex and my talk today will                                be about what kind of problems do exist                                with search quality what is it and also                                about how to improve the things without                                investing too much resources so first we                                will talk about what is search quality                                then we will take a look about examples                                of such quality problems when I will                                describe three methods of how to                                evaluate information retrieval systems                                then we will have a look at the signals                                I have some like bunch of signals for                                your inspiration and finally we will                                take a look about how to produce good                                snippets if we will have time and so                                let's go first of all search quality is                                abstract term it includes user                                experience or advance and the real sugar                                all effectiveness of search by humans at                                the same time relevance is a measure of                                conformity of user information need to                                document found so basically when user                                wants to find something he starts with                                his intent so for example when I first                                get a Hadoop I've faced with streaming                                jobs                                are breaking when the code in the                                streaming map is throwing a nonzero exit                                code so i started to google it my intent                                was to solve my problem now how to                                disable crushing of streaming of nonzero                                exit code so we see first the results                                they are about Hadoop and they are about                                nonzero exit code so it looks like a                                relevant and but the second two is                                completely out of sense you know so and                                you also see that the highlighting shows                                that words are distinct so it means like                                nothing useful is found so the question                                why am I see this here so about search                                we've searched we have quite a bunch of                                problems for example search has no                                definition formulation and it also has a                                considerable uncertainty what does it                                means for example it's it's not possible                                to define a search for example like a                                erp system or internet banking                                application it's a complex finks think                                because it's work it is working with                                Nader queries written in natural                                language and it also works with                                documents written in natural language                                which is kind of your rational hard to                                formulate formalize also search research                                working real users whose behavior is                                very hard to somehow explain or also                                formalize and also they have a kind of                                unexplainable and not predictable needs                                always in you                                mmm okay so also the developers are not                                prepared to tackle the search we cannot                                manage high tech step change in                                cross-functional and user to enter a                                challenge of course if we are not Google                                so what what am I talking about to build                                a good search system we need to know                                knowledge from different domains for                                example how to build high-performance                                system and the second thing is                                linguistics so we have to understand how                                morphology works how post taking works                                house in taxes and so on also we have to                                know how how much in learning works and                                finally we need to have some knowledge                                about the domain you know if we are                                searching for financial documents will                                have to know something about financial                                stuff so also the role of search and                                user experience is underestimated there                                followed by the measuring knows how good                                is it this is true especially for open                                source world where everybody just                                downloads the archive installs it and                                that's it job is done and so here's how                                it looks like basically this is a big                                elephant in relatively small room                                somebody is guessing that it is there                                 but nobody sees it and nobody knows how                                 exactly it looks like so this is how it                                 comes from emotional perspective of your                                 users so if users pessimistic the                                 beginning it could end up well with bad                                 user experience and finally defeating                                 and abandoning for a phone and email so                                 search will simply not work it could be                                 of other way which could be an                                 interesting source of discoveries and                                 finally user will end up with                                 satisfaction and using it again and                                 again so let's take a look closer to                                 examples of                                 problems these examples are for big                                 search systems like Google you can try                                 it but these examples these kind of                                 problems they are also persists in                                 lucene world and its ecosystem so first                                 thing is searching of model numbers and                                 articles so you see these two queries                                                                                                        telescopic muzzle for BMW                                              difference is replacement of spaces and                                 for the second query you will get much                                 more broader and interesting response of                                 research system then Tory first one so                                 basically if your typical user and you                                 don't know how our system works you will                                 not get it you know because user will                                 simply abandon the search and at the                                 same time search system itself knows                                 much more better for which kind of query                                 for which placement of spaces it has a                                 better results second question is                                 detection and correction of typing                                 errors this is kind of interesting thing                                 this is a Czech language and I'm                                 searching for a soap holder in your                                 bathroom and this differences in this                                 queries in one word in the in one letter                                 in the second word and the difference I                                 think here that first query was not                                 corrected why because the second word                                 Mila is means to wash and also basically                                 this is the right word the right form                                 but of a different word so we have two                                 different words and they that way we                                 were ending with lexical ambiguity so                                 search system simply has nothing to do                                 this it the only way to solve it is to                                 take into account                                 previous history of the queries or to                                 take into account with surroundings of a                                 query the surrounding words so first                                 example is requesting search for example                                 for some reason Google is completely                                 forgetting about what am i searching for                                 when I'm searching how to buy a used                                 Xperia experience the sony ericsson                                 mobile phone model name and so he's                                 starting to give me some results about                                 how to buy used things but the most                                 important word in this queries xperia if                                 you change it on how to buy used                                 smartphone it works ok so it means like                                 it has a different weighting of                                 important words sometimes it's wrong                                 there are also fundamental problems for                                 example stupid query berlin by tomatoes                                 where can i buy tomatoes in the big city                                 so now and google actually found funnel                                 in berlin total fail you know so the                                 thing here is when you are we when we                                 are putting search on our website we are                                 always presuming assuming that our users                                 have understanding of what they can                                 search and what we could found but this                                 is not always true and so try to figure                                 out if there was a zero given result                                 what user was found and you probably                                 will be inspired a lot what do they find                                 try to found this is a another example                                 systems is that example of bad user                                 experience so it systems is that anybody                                 knows what systems is that uh-huh only                                 few people systems is that is a check                                 search system and only four countries in                                 the world has its own search technology                                 and Czech Republic is there so Russians                                 checks china by do koreans and rest is                                 my google so this is a check sir system                                 and they tried experimented with this                                 new user interface what we did they put                                 a big snapshots of a website and so as a                                 result what they have because of big                                 snapshots which are giving not so much                                 in useful information it has big bad                                 user experience also order is not clear                                 you know which result is better by means                                 of search system right one or left one                                 hard to say also half of a page is spent                                 on advertisement and on domain Otto said                                 so basically if you have such problems                                 with interface well it could be not                                 surprising that your users will not use                                 your search too much yeah here's the                                 examples of Russian web interface so if                                 you are waiting too long for a loading                                 kaif additional page you also could                                 conclude that results are not so great                                 and abandon the search okay so let's                                 talk about evaluation so evaluation is                                 basement for improvement of the                                 relevance at the same time there is no                                 ideal measure and it's just much more                                 better to consult different measures                                 when you are making a decision but it's                                 matter has its own properties so you                                 have to think about it                                 yeah if it will show something wrong it                                 doesn't have to be something wrong also                                 evaluation of search could be done using                                 click behavior of your users so or other                                 interaction behavior if you have it so                                 basically recording of alex is the mast                                 so relevance is subjective why because                                 of the context of a problem you try and                                 solve for example if I would like to                                 move from point A to B and I'm searching                                 for a taxi so search system looks at it                                 okay he wants Texas so I will show him                                 the best website about the taxi so                                 basically the search system lacks of the                                 context of a problem and simply will not                                 give me a relevant result I need and the                                 second thing is awareness about a                                 problem so if you are interested for                                 example in the conflict on Ukraine and                                 you would like to know what is the last                                 news so you entering the query last news                                 in the advil Ukraine and if their system                                 will not give you a nothing Neal from                                 your perspective you could conclude with                                 relevance is bad for this query end and                                 so on so also user interface could                                 affect decisions about user about                                 relevance bad document annotations it                                 was shown on the previous slide that                                 good good lead also to decision of                                 battery levels presentation form and                                 previous experience with his search                                 system which is very important because                                 for example some search systems has its                                 own properties like Yandex Russians your                                 system works not so well for developers                                 queries I mean software development and                                 so system societies works                                 also badly for for English queries and                                 so on so we will talk talk about free                                 methods query by quick comparison of two                                 system classic clever dunne's cranfield                                 and pairwise evaluation that one is                                 experimental so queerly back video                                 comparison if you have to search systems                                 and you want to compare them you can                                 this method works only if the search                                 results do differ significantly for                                 minor changes it doesn't work so if you                                 take                                                                    system and will wait the whole syrup                                 search engines results page for top end                                 results with with one judgment and the                                 scale is simple good very good bad very                                 bad so and then count judgments of each                                 type so here I evaluated to queries of                                 Berlin buzzwords and Java byte output                                 stream and I see you see there but                                 Google is has a very good pink is kind                                 of good and finally there is a summary                                 we see that google is better because it                                 has to                                                                one for good and being has one for good                                 and one for that so of course numbers                                 will be much more higher when you                                 evaluate                                                              can make decision you can apply it for                                 example if you have the same search                                 engine but different search databases or                                 you can apply it if you have the same                                 search database but different engines                                 when you are testing which ranking is                                 better or if you are changing the                                 similarity model you can also test it by                                 the vet approach here's the thyroid                                 clever done he is the British librarian                                 and he is best known for his work on the                                 evaluation information retrieval system                                 that guy is responsible for the classic                                 approach which is heavily used in all                                 major search systems like Google bank                                 and yonder                                 so he's also responsible for he proved                                 that use H of a flexible terms list in                                 the index bringing more quality than the                                 fixed lexicon it wasn't clear before so                                 what components it has document                                 collection set of queries and then set                                 of relevance judgments so and then                                 measures precision is a fraction of                                 retrieve documents that are relevant and                                 recall percent of all relevant documents                                 returned by the search system so recall                                 is the kind of thing which is very hard                                 to measure because you need to know how                                 much relevant documents is in your                                 collection which is very hard to judge                                 manually because collections are big                                 usually so this is an example for query                                 Berlin buzzwords we have seven documents                                 and here is the judgment relevant is                                 relevant and our is not relevant so and                                 finally we calculate measures so                                 precision is about                                                      divide count of relevance on our account                                 and recall is not shown because when you                                 are working with web scale collection                                 you never know how many relevant results                                 the results do exist so when you are                                 evaluating many queries then you have to                                 somehow calculate the final measure                                 there are two ways you can do micro                                 averaging and microwave a raging the                                 difference here is imagine you have a                                 free classes of queries queries with                                 good precision we receive bad precision                                 and queries with an average precision so                                 simply micro average will will reflect                                 the class which represents the majority                                 know if you want to know something about                                 how it behaves on less popular classes                                 you have to use micro average there are                                 also add variations of this measure                                 which is working only counting four top                                                                                                   precision one precision five or                                 precision ten another measure is well                                 known in the information retrieval                                 community on all learning to rank                                 challenges is called nirmala is                                 discounted cumulative gain and it                                 measures usefulness or gain of document                                 based on its position in the result list                                 so the gain is accumulated from the top                                 afrezza a place to the bottom with a                                 gain of each result discounted at lower                                 ranks in the formula of dcg you see the                                 denominator and in denominator we have I                                 the position number so basically the                                 nominator does the discounting so the                                 core difference of this measure from                                 precision is that this measure takes                                 into account the position where document                                 was marked as relevant for example                                 precision if you have two queries for a                                 first query you have real relevant at                                 the bottom for the second query for                                 bottom of the list and for the second                                 where you have at the top of the list                                 relevance for precision then it will be                                 the same but this measure will reflect                                 the difference in the positions so in                                 order to calculate normalize discounted                                 cumulative give me you have to calculate                                 IDC tree which is for ideal ranking and                                 so this is done by simply searching the                                 results of your evaluated query by                                 relevance putting relevant on top that                                 way you will get ideal ranking and you                                 can calculate IDC G that way by the same                                 formula sorry so here's a pairwise                                 evaluation with Swiss tournament system                                 and guess the most complex fink in this                                 talk so                                 first of all this this kind of method is                                 true is working for document pairs so                                 you're not judging the whole query                                 you're judging with document pairs for                                 it simply you have to answer each time                                 the simple simple answer which document                                 is more relevant to the query X and then                                 I'm can say left right or equal and so                                 if you say left left one will get one                                 point if you say right right one will                                 get one point equal both will get by one                                 point so chosen document is getting okay                                 lready said it so the tricky thing here                                 is the preparation of pairs here's the                                 complex him about it so let's think we                                 started with initial set no ranking at                                 all so we we are randomly shuffling it                                 and dividing on two parts and then doing                                 the first pass so in the first pass you                                 have to compare first from the first                                 half and first from the second half and                                 so on so second from the first half                                 second from second half you see it on                                 the table in the right then you have the                                 results of a first pass so g                                             by one point doing the same for them so                                 only winners will participate in the                                 next pass so in the second pass we are                                 comparing drd                                                          vs do you want so d                                                   one point and go into the third pass                                 third passes needs one click so third                                 pass will get d one and the same and now                                 they see the final ranking by the points                                 so d                                                                     before and just six by one point so                                 pairwise evaluation which this system                                 takes about                                                           documenter interest for one query why am                                 I proposing this because it is much more                                 cheaper to click for two documents to to                                 judge from the two documents when to                                 judge the whole top top                                            because usually when you're doing                                 judgment you need to make it in some                                 reliable way because if two different                                 people will make judgments it's quite                                 likely that we will not agree so the                                 agreement between two judges using                                 pairwise evaluation is much more easier                                 to get than for classic approach so this                                 is kind of achievement replayed cheap                                 replacement of a classic approach after                                 judgment is finished ringing is built by                                 gifford points and according to position                                 weights are assigned to the documents                                 and then using weights the machine                                 learned model can be trained or you can                                 calculate and ECG NTC is perfectly fit                                 into this model so here's a example of                                 how weights could be assigned basically                                 i'm getting i'm giving more weight to                                 the first documents than to the last so                                 signals here is a small agenda of this                                 part the text relevance has a diversity                                 of tasks and use hmm any signals                                 including query classifiers it's the                                 only way and also production system if                                 you have it what data we have available                                 social signals and if we have some                                 integration of social media or we have a                                 some components of social social                                 networks you include it on your website                                 and finally how to mix the signals                                 simple menu linear model or the most                                 advanced a state of art technique                                 gradient this with the decision trees so                                 diversity of text for example if you                                 have a phrase search phrase search is a                                 when you are have a long sequence of                                 words and you want to find it exactly                                 matching and this is called the phrase                                 search search for named entities like                                 cities names search for of codes                                 articles telephone numbers search of                                 your questions search of set expressions                                 all these types of queries having                                 differences for example they are                                 different in length they are different                                 in turn frequency distributions they are                                 different in idf's anybody knows what                                 IDF is uh huh IDF is the inverted                                 document frequency it is calculated for                                 the whole search collection so basically                                 it's very hard for a one model to work                                 well on all types of these queries so                                 the solution is many signals so first                                 thing is a query type detection so we                                 can build a simple rules or classifiers                                 to somehow detect a query type and then                                 use it as a signal then we can calculate                                 the end                                                                HTML documents has a meta description                                 meta keywords field so we can calculate                                 especially for these fields at bn                                      can calculate the m                                             expansions so we can expand the existing                                 query using word forms we can expand it                                 using this                                 we can expand it using abbreviations                                 translate and fragments we can also use                                 properties of distribution of count of                                 subsequent query words found in the                                 document we can use the same thing for a                                 query order the same thing for a                                 distance plus minus three one two three                                 words and so on we can also build a                                 language model document and use it for                                 ranking or we can build a language model                                 for a query language model is simply a                                 dictionary of words this is language                                 would order a language model of first                                 order connected with a frequencies with                                 probabilities of a peer Inc of with                                 words and this probabilities can be used                                 to the signals so here is the simple                                 example of how we can mix signals so                                 ABCD can be estimated manually or it can                                 be estimated using relevance judgments                                 so ok if we have production system so                                 what does it means it means that we have                                 a real search and users are making                                 queries there and we're clicking results                                 so it means that we can record this data                                 process it in the background or in real                                 time if you can afford it and then                                 extract signals from it and use it in                                 ranking this is actually how do all big                                 search system works like Google binky                                 and X so first office that will be I                                 will just count examples of possible                                 features we could get from from clicks                                 so for documents we can get a                                 click-through rate of a document click                                 through rate is a count of clicks                                 divided on the count of shows we can get                                 absolute number of clicks count of times                                 when docking                                 was clicked first in search engine                                 results page the same time when it was                                 clicked last count of clicks on the same                                 search engines resulted before after the                                 document was clicked so then we can get                                 some features for our shows for example                                 count of times when document was                                 displayed on search engine results page                                 so is it popular or not count of unique                                 queries where document was displayed if                                 we are finding one document for a mini                                 queries it means something perhaps it                                 contains a lot of diverse information                                 and we should show it more for example                                 document position properties                                 distribution properties where do we show                                 this document in average on what                                 position for queries we can collect for                                 example absolute click count on query                                 this query clickable at all is this kind                                 of a successful query we can collect                                 abandonment rate and so how many times                                 user enters the query and then abandon                                 the search at all CGR of the query time                                 spent on search engines results page so                                 if users are looking something in this                                 well looking for a long time on this                                 query it means that there is something                                 to look so time spent till first last                                 click so is results accessible on this                                 query or not query frequency count of                                 words in the query finally count of                                 query reformulations our formulation is                                 when you have a subsequent queries but                                 you have a changes only in one word or                                 two words and so user is doing the same                                 query changing one word just                                 looking for something and try trying to                                 defeat the search system and so this                                 kind of stuff is very important and we                                 also can count of how much there was a                                 query reformulation for this query and                                 see char of these reformulations if we                                 have some integration with social media                                 or we have a some parts of social media                                 in on our website we can use count of                                 readers commenters of the content we can                                 also count the comments published during                                 some time period which is velocity so                                 how this big Grove we can count time                                 since last comment we can also count                                 speed of likes Grove we can count how                                 much time was since last like and so on                                 how to mix the signals there is a                                 well-known problem in machine learning                                 called learning to rank so basically                                 this is application of machine learning                                 studying how to construct the good                                 ranking model there are some in learning                                 to rank challenges for example yahoo                                 learning to rank challenge where they                                 are just giving them to participants the                                 judge data set and they create models                                 for this data set and then measure                                 somehow which model is better whose mod                                 model is better so this is a full-scale                                 process how it is done in Big Sur                                 systems like Google Bing and X first we                                 have a do prepared training set so they                                 prepare documents queries and relevance                                 judgments and they renew this so very                                 nude adding new documents adding new                                 queries and renewing relevance judgments                                 because it could happen that's for some                                 reason the document was irrelevant now                                 it is not then they have a framework                                 if you have a distributed search system                                 on many machines you need a way how to                                 get the feature vectors for your machine                                 learning it means it could be not so                                 easy to do if you if you have it                                 distributed then we do learning mode of                                 the model so it means we have some                                 implementation of how to say it some                                 implementation for learning algorithm                                 and they do use it and then they do                                 evaluation for example they have a be                                 testing where we can compare fifty to                                 fifty percent which ranking is better if                                 previous ranking is better we can we                                 learn the model and so on and they do                                 repetition of all that stuff because                                 internet is changing so features are                                 also changing so from time to time Model                                 Model degrades mm-hmm how to mix signals                                 do it yourself wave so we can do it                                 easier well we can try to do it cheaper                                 or indie way so we can choose manually                                 some set of features which we think a                                 good predictors and then we can create a                                 simple linear model this is an example                                 and here the coefficients needs to be                                 filled so we can take ten representative                                 query spent one hour and fit them                                 manually this is better than nothing of                                 course this is a dump approach but it                                 works it works somehow so gradient                                 boosted decision trees here basically is                                 the state-of-art method if you want to                                 mix them if you have relevance judgments                                 you can use this kind of algorithm to                                 mix these signals so do you have time                                 one minute so here's the                                 small explanation of how big and boosted                                 decision trees working this is simply an                                 example of the small trees not so deep                                 so basically each tree is learned on the                                 subsample of a whole training set it                                 means it means that you it will learn                                 the all nuances of your or trade                                 training set that way because it they                                 babe we are becoming more more like more                                 important when you're working with                                 subsample and then it adds the result of                                 each decision tree on each iteration so                                 there could be a thousand sophistries so                                 you see there are learning to rank                                 challenge of Yahoo and we can see the                                 expected reciprocal rank and normalized                                 ECG of comparison so as we see gradient                                 boosting trees is the best by all                                 measures and the difference in the                                 second digit after the coma is the most                                 important one so these results are of                                 the LGBT is with showing great                                 difference between B and                                               geeky rank is van is ranking sem and                                 bn                                                                     you see that very used a pretty big data                                 set like                                                                 training and about                                                   which is a lot so okay we have no time                                 for snippets but presentation will be                                 published if you are interested you can                                 download it and read it thank you                                 great thanks maybe we have time for one                                 quick really good question so who wants                                 to take it I'm no one so what's the                                 question so I have a question most of                                 your optimization is carried out for and                                 ECG right you look at other metrics like                                 diversity like in trollese diversity or                                 how popular are the websites you show                                 and things like that other properties                                 beyond pure accuracy and I did not hurt                                 you well for do you look more properties                                 besides end ECG and accuracy do you look                                 at things like diversity in the list or                                 or how popular are the results you are                                 showing because then I guess you might                                 fall into the filter bubble and not                                 allow users to discover new content may                                 be so it's come to get across in here                                 sorry for that I really don't hear very                                 curse maybe we can take it from outsides                                 better well I will definitely answer you                                 sure great well thank you very much                                 again
YouTube URL: https://www.youtube.com/watch?v=z8MJZaPbnHw


