Title: Berlin Buzzwords 2014: Peter Bourgon - CRDTs: Consistency without consensus #bbuzz
Publication date: 2014-05-28
Playlist: Berlin Buzzwords 2014 #bbuzz
Description: 
	When you think of distributed systems, you probably think in terms of consistency via consensus. That is, enabling a heterogeneous group of systems to agree on facts, while remaining robust in the face of failure. But, as any distributed systems developer can attest, it's harder than it sounds. Failure happens in myriad, byzantine ways, and failure modes interact unpredictably. 

Reliable distributed systems need more than competent engineering: they need a robust theoretical foundation. CRDTs, or Convergent Replicated Data Types, are a set of properties or behaviors, discovered more than invented, which enable a distributed system to achieve consistency without consensus, and sidestep entire classes of problems altogether. 

Read more:
https://2014.berlinbuzzwords.de/session/crdts-consistency-without-consensus

About Peter Bourgon:
https://2014.berlinbuzzwords.de/user/155/event/1

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              okay so i guess we're getting close to                               the time and we're at the time hi                               everybody my name is Peter and this talk                               is called consistency without consensus                               and I guess it's very similar to another                               talk that's coming up right after this                               which is has a very similar title I                               forget the name but that one's cool to                               go to that one as for this one it's it's                                ultimately a talk about distributed                                systems so I'd like to start by talking                                a bit distributed systems theory so if                                you're a distributed programmer or                                you're interested in distributed                                programming i guess i'd like to read a                                quote about the topic and that is this                                that distributed programming is the art                                of solving the same problem that you can                                solve on a single computer using                                multiple computers I have another quotes                                and that is that distributed programming                                is generally a bad idea and you should                                try to avoid it if possible and I think                                like this is pretty intuitive it's I                                think it's easier to understand why this                                is true so as an example consider the                                case of setting a single variable or to                                use a more academic term setting a                                register on a single computer it's                                pretty easy like you load up I Python                                and then you like type that and then you                                type that and then you get that right                                and if you didn't observe that behavior                                then there'd be something pretty                                fundamentally wrong with your computer                                like something totally devastating to                                your ability to continue to program now                                consider the same interaction against a                                distributed system there's many ways to                                do it but maybe here's one you would                                curl up a post request to some restful                                endpoint rest dish endpoint you pass                                some data and then of course you get you                                know this back it's like okay well maybe                                maybe worked anyway let's try doing a                                get and internal server error right it                                might as well be like an ascii art                                middle finger right for all the good it                                does you and this what's crucial here is                                this this isn't in any way abnormal                                right like whenever you interact with                                any distributed system you see errors                                like this all the time no matter how                                routine are conceptually simple what                                you're doing is you have to deal with                                stuff on this level and you have to deal                                with these failures in these errors that                                are                                like myriad complex ways that everything                                can go wrong and these are all things                                that we have to account for and defend                                against when we're doing distributed                                programming so I'm not the first guy to                                recognize this even back in the days of                                like ARPANET people knew distributing                                distributed programming was harder than                                regular programming and so I came up                                with all these nice primitives and                                patterns to deal with the complexity and                                in the early days I will argue the                                dominant primitive and pattern was its                                concept of the RPC so this is saying                                that network communication can be                                modeled like a function call and we can                                kind of treat it in the same conceptual                                way unfortunately there's a big                                difference between moving a program                                counter in a CPU and serializing a                                request sending it over network and                                getting it back and I'm going to argue                                that it's a difference not only in                                degree but also in kind fundamental                                difference so next leap forward I would                                argue is in the                                                       called korba and like all these                                middlewares does anybody who's korbo                                before I'm so sorry yeah korbo was                                really bad so there's like a lot of                                things in korba that they tried to use                                to make things easier there's this thing                                called a distributed object paradigm                                there's this thing they tried to                                implement called location transparency                                basically if you held a reference to                                some object or some data it shouldn't                                matter in the corba world if it was like                                on your local machine or if it was on                                the other end of the network connection                                I'm going to argue that middlewares like                                 went too far too fast and that they                                 treated the network kind of is more                                 reliable than an hour then it actually                                 was and they didn't provide good                                 abstractions for dealing with the errors                                 that networks inevitably would give you                                 so we build a lot of systems with                                 Corbett like if you're interacting with                                 your bank these days almost certainly                                 you're going through some korba layer at                                 some point and whether or not these                                 systems are reliable is open for debate                                 but we didn't have really anything                                 better to base our built engineered                                 systems on until I will argue the year                                                                                                         called the cap theorem by this                                 named Eric Brewer and often this is                                 summarized kind of incorrectly as                                 consistency availability partition                                 tolerance you can pick two of them so                                 more than anything I'm going to argue                                 that the cap theorem represented like a                                 fundamental shift in the way we look at                                 distributed programming specifically it                                 made us confront the reality of                                 partitions so partition tolerance is                                 that the system continues to operate                                 despite message loss due to network and                                 node failure that's one way to formulate                                 the idea and because we haven't yet                                 invented like perfect networks this is                                 an invariant in distributed programming                                 nodes are going to fail the network is                                 going to fail so when we're constructing                                 systems through the lens of the cap                                 theorem we must choose P we must choose                                 partition tolerance so you've probably                                 seen this diagram the cap theorem says                                 that we can't do that we can't have all                                 three we gotta pick two and because                                 networks are fallible we have to pick                                 partition tolerance so that leaves us                                 with basically two options right so                                 called CP systems that choose                                 consistency and so-called ap systems                                 that choose availability so CP systems                                 forgive me if this is like something                                 you've all heard before but it's                                 important to lay the groundwork CP                                 systems they're typically bill around                                 something called a consensus algorithm                                 or consensus protocol here are some                                 examples taxes is maybe the most famous                                 consensus protocol there's some systems                                 built on that Doozer is one written and                                 go chubby is the Google paxos                                 implementation there's another one                                 called Zab which is very similar but not                                 the same zookeeper uses Zab raft is a                                 new like hipster consensus algorithm                                 there's a couple of things using raft                                 console is the latest and greatest at CD                                 uses another implementation of raft                                 there's still another one which is super                                 academic it's called view stamp                                 replication I'm not aware of any systems                                 that are built using it but it's a fun                                 read if you're into this sort of thing                                 so these protocols are robust and CPW                                 systems are often provably reliable and                                 corrects but they're relatively                                 difficult to explain to the layperson                                 and as a result they're kind of                                 difficult to implement and debug and                                 maintain their also relatively slow with                                 kind of high operational Layton sees for                                 every interaction and low operational                                 throughput so they're kind of unsuitable                                 for a lot of things you want to use                                 distributed systems for for example you                                 probably couldn't store your tweet                                 stream in acp replicated log for those                                 kind of things we need to consider the                                 other side of the cap spectrum which is                                 AP and this is choosing availability and                                 partition tolerance but so called AP                                 systems like these don't actually                                 sacrifice consistency they just use a                                 different form of it so CP systems are                                 strongly consistent and AP systems are                                 less than strongly consistent and                                 there's different types of less than                                 strong consistency I'm going to talk                                 about one which you've all probably                                 heard of it's called eventual                                 consistency and that basically means                                 that you accept that some nodes in the                                 system may be stale at a given point in                                 the in time but you try to ensure that                                 no nodes will be completely wrong and so                                 if that's your constraint you can build                                 systems in a way that potentially                                 satisfies it so a lot of energy would                                 spend the last decade or so trying to                                 get eventual consistency to work well in                                 the general case and these are some                                 products of that of that energy a lot of                                 them do a really admirable job right but                                 the path to that current state where we                                 live today was a bumpy one especially in                                 the early days there was a lot of                                 frustration and public failure and I'm                                 going to argue that it was a function of                                 a gap between what people expected these                                 systems to be able to do and what the                                 system's actually promised or could do                                 and there was an I'm gonna argue                                 continues to be a large gap between the                                 theoretical state of the art and ap                                 information theory and the engineering                                 practice that produces products like                                 this now recently relatively recently                                 there's been kind of a conceptual                                 revolution there's been a big leap that                                 closes that gap and that's ultimately                                 what I'm going to talk about consists                                 see without consensus so I'll explain                                 what i think are two formulations of                                 that idea kind of the same idea and they                                 all have to do with this idea of failure                                 so partitioning in a network is one                                 broad class of failure but there's other                                 like more subtle ways that partitioning                                 can happen specifically messages in a                                 network can be delayed they can arrive                                 out of order from the way they were sent                                 they can be dropped all together and                                 they can be duplicated and these are all                                 invariants in our networks we have to                                 deal with these conditions and one way                                 maybe the best way to deal with in                                 variance is to allow them to happen                                 allow them to happen without corrupting                                 the system state that we build on top of                                 it so how can we how can we allow these                                 things to happen and that's what these                                 solutions that's what these big leaps                                 recent leaps attempt to implement so the                                 first one is this thing called the comm                                 principle has anybody heard of this good                                 great I'm introducing you to new ideas                                 calm stands for consistency as logical                                 monotonicity and that's a big an                                 abstract way of saying a system can only                                 really grow over time in one direction                                 or should rather so imagine a number                                 that you can only add to and never                                 subtract from the bloom language its                                 bloom lang org is built with this kind                                 of guiding principle in mind but it's a                                 little bit abstract so there's another                                 formulation of this idea which i think                                 is a little easier to get your head                                 around and that's so called acid two                                 point oh so probably you guys have heard                                 of acid in the context of databases                                 right and I think I forget what it's                                 like atomicity what's the other one                                 consistency is all right independent                                 isolation and durability right okay so                                 that's cool that's like our DBMS is are                                 built on these principles so some smart                                 fella back around twenty two thousand                                 nine or                                                                this acronym and the field of                                 distributed systems so for him acid two                                 point O is associative associativity                                 commutativity idem potency and then he                                 kind of ran out of stream steam she was                                 like okay distributed yeah whatever and                                 it would argue this is basically the                                 same idea as calm but kind of reified                                 like made more concrete and it means                                 that a system which is acid two point O                                 compliant has operations which satisfy                                 all of these things well the first three                                 so that brings us kind of to the name of                                 the talk which is CR DTS crd T's are                                 conflict-free replicated data types and                                 it's a distributed data type like a                                 simple variable or a set which is                                 provably eventually consistent without                                 consensus so CR dt's achieve eventual                                 consistency by using acid to point o or                                 calm compliant operations the seminal                                 paper on CR DTS like the the source                                 material is by this guy named Mark                                 Shapiro he's I think of Microsoft or                                 formulae of Microsoft but it's super                                 academic and kind of difficult to wrap                                 your head around and for me at least the                                 best way to understand what is a crdt is                                 by example so let's try that let's pick                                 a motivating example and I think the                                 easiest one is something called an                                 increment only counter so you can                                 imagine what is the counter it's a                                 number you can add to our remove or                                 subtract from but an increment only                                 counter is a replicated integer                                 supporting operations increments to                                 update and value to query so plus one or                                 read so let's think about the reeds easy                                 right you just read the value that's                                 there that shouldn't be too tricky but                                 let's think about the the write                                 operation that the plus so what are the                                 properties of addition integer addition                                 well it turns out it is associative                                 meaning this statement is true and it is                                 commutative meaning that statement is                                 true but it's not I Denton because                                 that's that's false so addition by                                 itself isn't a crdt                                 acid to point o compliant calm compliant                                 operation so we have to play a little                                 bit of a game to get an increment only                                 counter into crdt semantics so in the                                 paper he describes a data type called a                                 G counter and it works like this let's                                 say you have a note distributed system                                 with three nodes each node is going to                                 hold an array and in the array there's                                 going to be three indices on node                                        say the red node is going to own the                                 first index in that array on node                                        going to own the second index in that                                 array and so forth now let's and                                 crucially the other indices represents                                 the state of the machine the                                 corresponding machine I hope I've made                                 that clear with the colors so let's                                 consider how a write operation would                                 work let's say someone comes along and                                 issues a increment operation into this                                 distributed system and it just so                                 happens to hit the second node so what's                                 going to happen is the second note is                                 going to update its owned index in the                                 array and now the value of this counter                                 is considered to be the sum of that                                 array so at the moment the value is now                                 one on the other nodes the value is                                 still                                                          inconsistent state but it's eventually                                 consistent we'll see how okay so that                                 write operation worked in now if someone                                 else read from the second node it would                                 see the correct value they read from                                 either the other two nodes they would                                 see an incorrect value the second note                                 is now going to take ownership of this                                 state change its going to propagate it                                 to the other nodes in the system whom he                                 presumably knows about knows how to                                 contact them so step one is to issue                                 this message up to node                                                 it's not a +                                                             am node want the second node and my                                 value is currently one that's it's                                 important that it not be +                                             as we saw isn't I'd impotent but what I                                 just said is if that happens three or                                 four times the Penn State is going to be                                 the same does the same thing down there                                 and now we have a consistent state no                                 matter which node you read from                                 you're going to see the same value which                                 is one okay let's consider another write                                 operation another increment this time to                                 this guy same thing now his state is too                                 it's going to propagate it there so far                                 so good but let's say that this message                                 is lost or delayed we have an                                 inconsistent state in the overall system                                 that's not great and it's going to stay                                 there absent the state's going to remain                                 inconsistent absent some motivating                                 condition but check out what happens now                                 if we get another plus one to the same                                 node it's going to replicate remember                                 with these like State semantics and if                                 it makes it down there we're suddenly                                 consistent again so that's cool and                                 again if it gets duplicated no change                                 we're still good so this is cool this is                                 a way to make an increment only counter                                 work unfortunately it does have some                                 downsides it requires that the list of                                 nodes be both known to all nodes in the                                 network and kind of static because it's                                 these arrays are like allocated in that                                 way so it turns out we can do a little                                 bit better and the paper talks about                                 this too let's consider a better way to                                 do an increment only counter and for                                 this we turn two sets so when you have a                                 set rather than an integer your addition                                 operator is a union so let's run through                                 the list again if we have a set                                 containing one union with a second                                 hanging two and three that statement is                                 true so Union operation is associative                                 that's also true I hope is clear so it's                                 commutative but handily it's also                                 adamant in other words if you Union a                                 set containing a value with a set                                 already containing that value you get                                 the identity set so that's cool can we                                 model this incremental encounter using                                 sets instead of integers well it turns                                 out we can but it's a bit interesting                                 because the expression plus one isn't by                                 itself unique and in order for something                                 to be in a set it should be unique so                                 what we have to do for this to work is                                 take a step back and consider our                                 application domain and we have to say                                 well your                                 longer just issuing a plus                                             increment only distributed counter you                                 have to do something else to it let's                                 say for example the counter represents                                 the number of unique plays that attract                                 on Soundcloud gets and so what we can do                                 is instead of saying plus                                               into the network the user ID of the                                 person who press play and so this would                                 be unique because I user ID can only                                 uniquely play a trap once and can never                                 revoke it so it kind of makes sense so                                 now here's what it looks like instead of                                 an array we have three sets currently                                 empty and in order to compute the value                                 of this replicated integer of this                                 increment only counter we're going to                                 compute the cardinality of these sets                                 and right now there's nothing in them so                                 the cardinality                                                       let's consider user ID                                                  this track remember this increment only                                 counter represents a single track so                                 that's going to hit one node randomly in                                 our distributed system it's going to                                 enter that set the same semantics for                                 replicating that to the other sets and                                 now we have consistent eventually                                 consistent cardinality of one across                                 this integer same thing you can imagine                                 ok exact same operations so this is                                 solving the same problem that the                                 original array based implementation did                                 but it relaxes this restriction of                                 having to know a priori the topology of                                 our distributed network so this is cool                                 and it works so now a brief interlude                                 crd T's give us these really nice                                 properties there a solution that gives                                 us these really nice properties that we                                 want to leverage and sometimes that                                 requires bending the definition of our                                 problem to fit them i think tomorrow                                 morning the keynote eyes by this beer                                 ops young woman and she's going to talk                                 about the DevOps revolution and i'd like                                 to draw a parallel here in analogy in                                 the battle days we used to build these                                 binary artifacts and                                 we would deal like works on my machine                                 and then like throw it to the sis                                 admin's right and then they say you guys                                 you should run this in production and                                 you should get paged when it goes down                                 and you should scale it out we                                 eventually recognized an invariant in                                 software engineering which was that as                                 the authors of a piece of software we                                 are like uniquely qualified to run that                                 piece of software in production we're                                 best whether we're going to be the best                                 person to do it so we had this like                                 DevOps revolution right and we changed                                 our methodology and we said programmers                                 are expected not just to type the code                                 in build a war file and throw it at                                 somebody but to deploy to production                                 ourselves and to monitor it and it get                                 paged when it breaks and to fix it and                                 then to scale it down when or scale it                                 up when we need more capacity and scale                                 it down when the next version of the                                 software comes along and we're better                                 for it as an industry I think all the                                 best software shops work this way I hope                                 you work this way if you don't it's bad                                 you should feel bad but we're better for                                 doing things this way similarly if we i                                 would i would say that we used to write                                 distributed systems very optimistically                                 we categorically dismissed huge numbers                                 of failure modes in our networks and                                 everything everything beneath that but                                 the cap theorem came around and showed                                 us that they are in variance and                                 distributed programming that we need to                                 design for and accommodate and see our                                 dt's i believe are the current like                                 best-in-class solution to deal with                                 those invariants so the fundamental                                 argument that I'm making here is that as                                 users and implementers of distributed                                 systems we should read we should be like                                 ready willing and able to change our                                 methodology to change our way of                                 thinking and not to hold our problem                                 invariant and say I need to be able to                                 say plus                                                           support that and then try to bend the                                 solutions that we have to fit that                                 problem instead hold your good solutions                                 invariant understand what is a crdt for                                 example understand what it requires of                                 the application domain and then bend                                 your problem to fit it I believe that                                 type of methodology that type of                                 thinking produces like better more best                                 more robust                                 blur more reliable software and I think                                 that benefits all of us okay that's my                                 little speech and now we get to the                                 interesting part which is crd tease in                                 production and the system I'm capable of                                 talking about so I for this company it's                                 also there that's cool we put your                                 sounds and cloud we're ultimately like                                 many websites we have a social network                                 component so if you create an account                                 and you log in you'll be encouraged to                                 follow other users other Creators on the                                 platform if you follow enough people                                 then you have this thing called the                                 stream and you see all the things that                                 they do all the tracks they upload all                                 the sets that they repost and and this                                 sort of thing so here's the stream and                                 the stream is not unlike the facebook                                 feed or the Twitter tweet timeline or                                 whatever you can imagine it's comprised                                 of things we call events and events all                                 take basically the same form an event is                                 the timestamp for the thing that happens                                 the user or the actor who did the thing                                 the verb of the thing that was done like                                 uploaded or reposted or something like                                 this and then a unique identifier of the                                 thing to which the verb was applied that                                 sounds awkward but I hope it's kind of                                 clear what I mean so as an example on                                 the                                                                     have reposted The Economist's podcast or                                 something like this so that's an example                                 of an event unlikely though it may be                                 okay so when you're constructing                                 timeline services tweet services this                                 sort of thing I'm going to argue there's                                 like two ways of doing it the first in                                 kind of most obvious way is so-called                                 fan out on right so imagine this is your                                 social network you have your Creator                                 here on the left and you have all of                                 these listeners on the right and in this                                 model all the listeners get kind of an                                 inbox and so what happens is when                                 assuming all these guys are following                                 this guy when he produces a new track or                                 repost something what's going to happen                                 is that's going to get written to the                                 inboxes of everybody who follows him                                 and that's kind of the data model fan                                 out on right so the locality of the data                                 is very close to the consumer make sense                                 so there's another way and so called fan                                 and on read and in this model the                                 locality of the data is close to the                                 producer so when they upload or produce                                 a new track or whatever it goes into                                 their outbox and then what happens is                                 whenever I as a listener open up my                                 stream page what I'm going to do is                                 query to see who I follow go to all of                                 their out boxes and kind of pull in                                 their recent stuff merge it together and                                 then produce kind of a dynamic view of                                 what my stream is at that point in time                                 without getting into details this                                 carries a lot of advantages you can                                 think of that a product level it's much                                 easier to do fun dynamic things with                                 your stream if you can manipulate it                                 kind of on the fly so maybe there's a                                 way we can that our goal here is to                                 implement a fan in on read model of the                                 event stream perhaps using crd tease                                 otherwise this talk would be kind of                                 weird so let's see is there a way we can                                 do this with what we know already it                                 turns out that events are unique in the                                 sense that Snoop Dogg can only repost                                 that one economist podcast once like                                 that that is a unique formulation of                                 this idea should only appear in the                                 stream once so maybe we can use the set                                 we already know about this G set which                                 is when I described earlier but it                                 doesn't work because you can't delete                                 from it so increment only you can only                                 add things into it if you continue to                                 read the seminal paper on C or D tease                                 you describe some other sets which you                                 can delete from describes a two-piece                                 set so-called to pset you can add and                                 delete but then you can never re ad                                 again so that's not great there's                                 another one called an o are set and                                 observe remove set this does allow you                                 to add and delete kind of indefinitely                                 but it comes coupled with a storage                                 overhead that is not great for us and it                                 also imposes some restrictions on the                                 topology of the distributed system which                                 we we don't want but to summarize all                                 these                                 that's kind of work in roughly the same                                 way I'll crdt sets kind of work in                                 roughly the same way which is as follows                                 there is a concept of a logical a single                                 logical set which is split into two                                 physical sets there's a so-called add                                 set times like S Plus this contains all                                 the elements that should be considered                                 to be in the set there's a so-called                                 remove set s minus and then the way you                                 produce these are the two physical sets                                 the way you produce the logical set is                                 by doing a semantic merge of them so                                 kind of if you merge ABC with B then                                 this is the result right so this is kind                                 of conceptually how they all work so now                                 I'm going to describe a new set that we                                 kind of came up with which powers the                                 soundcloud stream so here are the                                 semantics is very similar we still have                                 an ad set but we couple each element                                 with a piece of metadata which you can                                 consider a score it's actually the                                 timestamp I'll just have them as                                 integers here for simplicity there's                                 still a remove set exactly the same                                 thing and we still do the same semantic                                 merging but these timestamp metod it has                                 give us a bonus feature or a set of                                 bonus features so asn't just to make it                                 clear what these things mean the the set                                 key the logical set name would be the                                 the user ID for whom the out box                                 represents so for example Snoop Dogg's                                 outbox the elements in that set or a                                 unique combination of the actor the                                 thing that was done and the identifier                                 so this is actually the unique thing                                 that should only appear once in any                                 given stream and then the extra piece of                                 metadata the score is the timestamp of                                 the event as it was done ok so if that's                                 our data model then reading is easy you                                 just read the two sets you'd perform the                                 semantic marriage and then you're done                                 what's interesting is writing and                                 particularly what's interesting is                                 writing while ensuring these asset to                                 point o calm semantics are maintained so                                 to consider the insert operation answer                                 it is always key element score here's                                 how it works if either the ad set or the                                 remove set for that key already contains                                 the element then you look at it and if                                 the existing score is greater than or                                 equal to the score that you're trying to                                 put in the score you're trying to insert                                 it then it's a no op and you exit this                                 means that if you insert an older                                 elements into your sets then it doesn't                                 beat the existing element so it's kind                                 of dropped on the floor otherwise you                                 insert the incoming element into your ad                                 set and you delete any matching element                                 from the delete set and in this way                                 ensure that each element each unique                                 thing only exists once in this outbox ok                                 that's insert you can imagine delete is                                 actually exactly the same thing except                                 you swap inserting into the ad set with                                 inserting into the delete set and vice                                 versa ok that's a bit abstract let's                                 look at an example let's consider that                                 this is our current state we have a set                                 s consider it Snoop Dogg's outbox he has                                 ADD set elements a and B with scores of                                 one and two delete set helmets he score                                 of three and here comes a write                                 operation answer D for show of hands who                                 knows what happens where does it go                                 where does it go that's plus correct                                 bonus point you get a t-shirt you                                 already have a t-shirt ok that's clear                                 right because D doesn't exist anywhere                                 it can go into the ad set cool here                                 comes to know it's the same operation                                 again what happens right it detects it's                                 there the score doesn't beat it so it's                                 a no op in no state change insert d                                  correct d                                                           change delete d                                                       that's right we checked both sets                                 because d                                                               d                                                                        it's a no op haha but d                                          not a--not up so we check both sets we                                 see d exists but five beats four so five                                 kills                                                                    now this is our state if we had an                                 uncertainty                                                           it's a no op but a delete d                                             the effect of updating the score on that                                 element ok so that's how it feels right                                 everyone kind of like understand how                                 that works and it looks pretty good on                                 keynote slides but the question now is                                 we have to make it real we have to make                                 it perform so let's talk about making it                                 real and the question was do we need to                                 write new software to implement these                                 kind of set semantics and the script out                                 is I described and it turns out we don't                                 it turns out that there's this thing in                                 the world called Redis and Redis is a                                 data structure server not to be confused                                 with the database and Retta supports                                 directly something called a sorted set                                 AZ set which provides exactly the                                 semantics we want its add remove get the                                 score retta's also has this atomic Lua                                 scripting so that's really cool so we                                 can safely implement our write                                 operations but our data set is large                                 revis is an in-memory server and so we                                 don't fit the whole data set into one                                 reticence tins so naturally we do what                                 most people do and we shard rightist                                 instances and this actually works really                                 great because we have a natural shard                                 key every operation is always based                                 around this set ID set a name and we can                                 hash that and do really nice easy                                 sharting at the application layer so we                                 wrote a very simple library sit in front                                 of a bunch of Redis instances do that                                 charting for us and on top of that we                                 wrote something called cluster and that                                 provides us a very simple insert delete                                 select API on top of the shard API ok                                 but remember this still represents a                                 single logical node in our distributed                                 system and by that I mean that Snoop                                 Dogg's outbox only appears once in all                                 of these boxes so in order for this to                                 be a nice reliable distributed system we                                 want to replicate this picture and                                 indeed that's what we do as many times                                 as you want to replicate this stack                                 and what's interesting about this                                 architecture is that none of these                                 components talk to each other right none                                 of the Redis is talk to each other none                                 of these cluster stacks talk to each                                 other and in this way it's I think a bit                                 of a different architecture than you see                                 in a lot of distributed systems where                                 the nodes will kind of gossip with each                                 other I'm going to argue that this makes                                 this system better more reliable easy to                                 your easier to understand easier to                                 reason about because it exposes it to                                 fewer modes of failure okay so we have                                 finally this final layer which                                 communicates with all the clusters and                                 we package the farm the clusters in the                                 pool into a single binary stateless                                 binary which communicates with all of                                 the stateful reddest instances so we can                                 do horizontal scaling with this binary                                 talking to the stateless stateful layer                                 and that's basically our system so at                                 this high level writing is very easy                                 every write operation should be sent to                                 every cluster because that's what you                                 want straightforward but reading is                                 interesting here because we have options                                 when a read request hits the farm layer                                 what do you do well the easiest thing to                                 do is just to send it to one cluster                                 pick randomly shore and return the                                 result directly this is cool it's easy                                 to understand it's fast you only have                                 one network round trip but you only get                                 one response back and if that cluster                                 happens to be out of sync inconsistent                                 we have no way to know we just have to                                 return the data back the alternative is                                 to send it to all the clusters we can do                                 this kind of efficiently we can do a                                 scatter gather that bounds us to the                                 slowest of the three in this case but it                                 means that we get multiple responses                                 back and remember these are sets that                                 are coming back so we can do interesting                                 things with that let's say this is what                                 we got back from such a request                                 everything looks good except as last                                 cluster apparently missed the B element                                 somehow so that means it's inconsistent                                 what do we do well you can Union all of                                 these responses and this is what you get                                 and it's interesting that this is                                 actually the correct response right this                                 is this is this is what is provable                                 be totally correct and it turns out if                                 you think about it as long as each                                 cluster has representation for each                                 element at least once at least once then                                 the union is going to be the overall                                 correct response and that's cool we can                                 send that back to the client he doesn't                                 see the inconsistency there's another                                 operation and set world called I think a                                 symmetric difference which selects the                                 things which aren't in perfect agreement                                 across all the sets in this case it's                                 the element B and knowing that we can                                 then compare we can ask each cluster                                 what do you know about the element B get                                 responses back compare the responses                                 based on the scores we can determine                                 which one is the most correct and then                                 perform a diff against the responses we                                 got back and then reissue write                                 operations to the clusters that were                                 inconsistent so this is very similar to                                 Cassandra read repair in fact it's                                 called read repair and so what we can do                                 then is build this hybrid read strategy                                 where we send the request to all the                                 clusters we return the first one that                                 comes back maybe it's inconsistent but                                 we linger around and we receive all the                                 responses from the other clusters we                                 perform this diff and when we see that                                 there's anything inconsistent we send                                 the right operations all asynchronously                                 and in this way the system kind of                                 becomes self repairing assuming you have                                 sufficient read volume so that's like                                 the incredibly fast five minute overview                                 of this production crdt system the full                                 thing is totally open source we recently                                 did a blog post you can go to this                                 website we spent a lot of time building                                 a readme that I hope is easy to                                 understand the whole project is written                                 and go except for the reddest bit it's                                 about                                                                   big easy to understand I hope and we're                                 in the process of rolling it out to                                 serve this dream to all of our users                                 we've load tested it very significantly                                 to several orders of magnitude growth                                 beyond what we currently have several                                 years of exponential growth ahead so                                 we're very confident so this is a real                                 deal right it's a production crdt system                                 built from first principles simple and                                 fast take a look okay conclusions I                                 think I still have two minutes i think i                                 am ok cool so consistency without                                 consensus current best                                 option available in the field of                                 distributed systems is the crdt learn it                                 know it love it taking a step back                                 speaking a bit more generally every time                                 you write software you have to deal with                                 invariants whether they come from the                                 business world or whether they come from                                 like the structure the the grid of the                                 technology that you build on top of and                                 it's a fool's errand time arguing to try                                 to code them away rather you should                                 embrace them you should acknowledge them                                 as first order things and you should                                 bend what you're trying to do to them                                 rather than trying to abstract them away                                 I think this is the lesson of successful                                 distributed programming and I've just                                 said that so I won't say it again that's                                 basically a thank you so much I work for                                 soundcloud if you want to work on hard                                 problems like this we're hiring there's                                 the website and maybe I have like time                                 for one or two quick questions but thank                                 you very much                                 anybody                                                           there's one guy right behind you is like                                 oh okay how do you ensure that the time                                 stamps are consistent or that they are                                 monotonically increasing yes different                                 clusters yeah so this is the question                                 was how do you ensure the time stamps                                 are consistent indeed this is the                                 problem that I've totally like glassed                                 over the way we built the system was                                 that all of the events are coming from                                 kind of some source in the distance and                                 the assumption in the system is that the                                 time stamps that come with the events                                 and they then they do come with the                                 events are correct so if that's not true                                 we're totally subservient to those time                                 stamps and that is definitely a cheat in                                 the system there's this whole field to                                 study like Lamport clocks vector clocks                                 that deal with exactly this problem we                                 chose not to address it in this and this                                 in the system and like exported that                                 complexity to the producer side of the                                 system cool okay so I go what do you do                                 if one of your clusters is so                                 inconsistent that it's not acceptable                                 for the user so maybe you put up a new                                 note it's empty I mean it would repair                                 itself but you can't give that to the                                 user it might be annoyed yeah so this is                                 an interesting case you can't give it to                                 the user and in this hybrid mode that I                                 showed at the end you couldn't return                                 that right away so we hope in the                                 general case of the operation of the                                 system no cluster falls into this state                                 but one interesting way of growing the                                 system is indeed to boot up an empty                                 cluster and let read repair just kind of                                 fill it in overtime and this works great                                 except you have to put a kind of a flag                                 on that cluster and you have to say this                                 cluster can receive write operations but                                 it you should never serve read                                 operations from it directly and so                                 that's kind of how we do with it we                                 could have a way of like dynamically                                 deducing error rates from cluster by                                 cluster basis and at some threshold like                                 kick it out we don't currently do that                                 but that could be possible yeah okay I                                 think that's time thanks again                                 you
YouTube URL: https://www.youtube.com/watch?v=U6xLcIf1Qlw


