Title: Berlin Buzzwords 2014: Andrew Psaltis - Real-time Map Reduce: Exploring Clickstream Analytics ...
Publication date: 2014-05-28
Playlist: Berlin Buzzwords 2014 #bbuzz
Description: 
	Andrew Psaltis talking about "Real-time Map Reduce: Exploring Clickstream Analytics with Spark Streaming, Kafka and WebSockets"

Spark Streaming is an extension to Apache Spark that lets users seamlessly intermix streaming, batch and interactive queries through the use of a new programming model. Coupling this with strong consistency and efficient fault recovery, the opportunities to build robust streaming analytics systems is limited only by imagination. 

In this talk I'll show you how to practically use Apache Kafka to store clickstream data, Apache Spark Streaming to perform click stream analysis, and WebSockets to stream the results out to a client. 

Read more:
https://2014.berlinbuzzwords.de/session/real-time-map-reduce-exploring-clickstream-analytics-spark-streaming-kafka-and-websockets

About Andrew Psaltis:
https://2014.berlinbuzzwords.de/user/207/event/1

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              hi my name is Andrew sultisz title it                               talks can be real-time MapReduce                               exploring                               analytics with Kafka spark streaming and                               web sockets kind of a mouthful supposed                               to stay a loop about myself currently                               working at ensighten on agile marketing                               platform as of three months ago the                               previous five almost five years worked                                at web trends on visit on the linux and                                streaming analytics platforms some is                                where I kind of first fell in love with                                spark and then spark streaming so some                                of the stuff we'll talk about relates to                                that experiencing kind of just using                                spark in spark sharing kind of where                                we're going is talk about spark briefly                                spark streaming more particular kind of                                how I fell in love with it give a brief                                overview of an architectural picture I                                have in mind of the streaming platform                                give a birds eye view of kafka really                                quick in case people just haven't used                                it discuss spark streaming in more                                detail and they kind of walk through                                some clickstream examples and then kind                                of discuss getting data out of spark                                streaming so I'll FL for spark em it's                                kind of like three worlds collided on                                october fourteenth when felix                                baumgartner jumped from the edge of                                space for red bull red bull happen to be                                a web trends customer it was the largest                                single hour of traffic collected for web                                trends its                                                        happens that it also crash all the                                analytics machines trying to process                                that data at the same time around at the                                same time matei announced that spark was                                going to be standalone and no longer                                required mezzos so which was great for                                us because we had a Hadoop cluster                                didn't want to deploy mezzos and all of                                a sudden we had a problem that our                                production engines were crashing so we                                had a pretty big data set and we had a                                new toy to play with so we got spark                                installed locally got a local data set                                of the Red Bull data and started to play                                and realize that wow you could express                                things so easily we could actually                                answer questions that were crashing                                production and analytics tensions so                                from there just kind of continued and we                                start to pursue using spark for various                                things read the D streams paper and                                we're interested in then using spark                                streaming when that came out because we                                already running a storm cluster and                                already did some streaming analytics and                                want to see what this new thing could be                                so why spark streaming you know said we                                already had storm running in production                                at providing a streaming analytics                                solution for our customers really kind                                of event based so it's you know clicks                                coming in some light augmentation some                                light analytics and then data going out                                as fast as possible so pretty much from                                click to dashboard as quick as we can so                                we already have that yeah but we wanted                                to deliver an aggregate stream so we                                could do things like a rolling top end                                of pages or the top business on the side                                or the top countries or top browsers we                                wanted exactly once semantics to be able                                to do that we didn't want to risk double                                counting or have any issues like that                                and we were okay with things being at                                second scale latency we didn't need the                                storm speed of an event stream we're                                okay if things are delayed we want to                                see top pages say last                                                minute so things could be delayed it was                                just an aggregate of what's happening                                for your business at that time we also                                want to have state management for                                computations so if things did go south                                as they do we want to be able to recover                                and not lose our state and since we've                                been using spark and start a kind of                                fall in love with that and really                                appreciate using that coming from a                                hadoop mapreduce world we wanted the                                ability to combine that historical data                                that we're doing aggregations on with                                stuff that we're streaming so we could                                get a more holistic picture of a user do                                some other analytics on the data or do                                 something with it so in my mind this is                                 kind of a generic streaming data                                 pipeline if you will that we have a                                 browsers and device click on something                                 ends up going to some tags every wheel                                 some analytics vendor could be anything                                 else that collect that data possibly                                 drop it into the message queue of some                                 sort goes through some analysis tier                                 possibly in some sort of in-memory store                                 and then some data accessed here we                                 could get the data back out all rights                                 from the storm architecture I had talked                                 about it we pretty much had this type of                                 thing with the analysis here bein storm                                 and the data access to your being you                                 know a web sockets web service                                 we would then serve data through so as                                 we start to play with spark streaming we                                 thought can we do a similar type of                                 thing so that this is pretty much the                                 pipeline that I was thinking about you                                 know putting together code for this and                                 just kind of walking through it of you                                 know I wasn't going to put together a                                 real collection server but I just got                                 like MSNBC click traffic from the UCI QD                                 data set that has like you know real                                 visitor patterns for a day and you know                                 in September basically took that                                 replayed it into Kafka run it through                                 spark streaming back into Kafka and the                                 out via a WebSocket server okay it's                                 just quick overview of Kafka rights a                                 neutral develop it linkedin distributed                                 pub sub type messaging system is                                 specifically designed for real-time                                 activity streams it doesn't follow any                                 JMS standards use a JMS api's has AP is                                 in most languages some key features of                                 it persistent messaging that's high                                 throughput low overhead use a zookeeper                                 whether that's good or bad yeah it does                                 and supports both kind of queuing and                                 topic semantics as it really kind of is                                 designed to take this it kind of the                                 couple this mess into something that                                 looks more like this so it sits in the                                 middle because step will walk there is                                 you know coming in through here going to                                 Kafka in this case they go into                                 real-time or spark streaming you know                                 these arrows could go both ways you know                                 of having you come in this way and then                                 feedback through and come back out so                                 spark streaming sits on top of spark                                 with the goal of delivering large-scale                                 stream processing it's pretty efficient                                 and fault-tolerant staple stream                                 processing I integrates with sparks                                 batch interactive processing so you're                                 working with a stream of data and it                                 really feels no different than if you're                                 working with a file that's coming out if                                 I do so everything kind of feels the                                 same and they could kind of mix them and                                 you really                                 kind of lose sight of where that is and                                 it blends it really well provides a                                 simple batch like API so you can                                 implement complex algorithm so the same                                 thing you really don't know you're                                 working with a stream you just realize                                 you're working with a different type of                                 our dd's that's storm that spark exposes                                 you know with that it provides a certain                                 programming model so in spark there's a                                 notion of these are td's is resilient                                 data sets in spark streaming there's                                 these streams which is discretized                                 stream this is very similar ap is it                                 looks and feels very similar to it and                                 you pretty much live in a world that you                                 have input we have D streams are being                                 created from an input stream or from                                 another stream all right then you deal                                 with operations on them we're going to                                 transform and do something with them and                                 then you're going to output the result                                 to somewhere out of the box the data                                 sources for input are HDFS kafka flume                                 Twitter TCP sockets acha actor zeromq                                 and pretty easy to kind of roll your own                                 as well it's not that not that hard it's                                 pretty well extracted so they got a lot                                 of things pretty covered from and                                 out-of-the-box standpoint on getting                                 data in and as we'll see getting data                                 out is a little bit of a different story                                 at this time these are some of the                                 operations so I say transform it really                                 just allows you to build new strings                                 from existing ones so there's our DD                                 like operations it's the same thing                                 you've seen if you've looked at spark                                 before or other similar platforms where                                 this map flatmap filter count by value                                 reduce group bike key you know so a                                 variety of things so the ones in bold                                 we'll see as we kind of walk through to                                 some code examples of it some of the                                 things that are new that it does bring                                 to the table that don't exist in spark                                 are the windowing operations all right                                 we could do a window you could count by                                 window get reduced by window count by                                 value and the window reduced by key in                                 the window and you can update state                                 which is pretty interesting for doing                                 certain things so we saw that there were                                 a lot of different input options this is                                 all you get for output out of the box                                 you get print which will print to the                                 drivers screen so that doesn't really do                                 a whole lot for you you get for each rdd                                 where you could perform an arbitrary                                 operation on every rdd that came out of                                 the batch so it kind of gives you the                                 point that you could do something you                                 can save as an object file you save as a                                 text file you can save us a dupe files                                 so somewhat limited for a streaming                                 platform to only have that as the output                                 there's some different projects out                                 there to have other outputs it's just so                                 those been the focus to get data in and                                 so far not a whole lot that's going on                                 to get data out episodes we'll see what                                 what I've put in place at least what I                                 have here is it's just leveraging that                                 for each to send data out but it's still                                 not as clean as the inputs that they                                 offer because the inputs all seem the                                 same it's a generic way of getting data                                 in there's no really good generic way to                                 get data out so the discretized stream                                 processing we talked about so really                                 kind of how this looks in the picture                                 that we had as you imagine the stream of                                 data coming through Kafka and its really                                 kind of breaking it up into these batch                                 sizes the rights imagine each one of                                 these as the stream is flowing each one                                 of these representing a batch of data                                 coming through you know could have a                                 granularity of say like a half a second                                 right so as it flows through you end up                                 inside of spark streaming say we're                                 consuming this from Kafka working to do                                 some processing on it and then we're                                 going to send out these process results                                 again in some sort of batch increment                                 right so it's a stream that gets broken                                 up these little discretized pieces you                                 process them and send them out some of                                 the clickstream examples want to walk                                 through some common ones that you'll see                                 it just like pageviews per batch the                                 neck really just be anything rather just                                 counting number of page views in one of                                 those batches looking at page views by                                 URL over                                 I'm so maybe you want to see things like                                 I said if you want to see what are the                                 top page is over in the last                                            or the last                                                             for you may want to do the top end page                                 views over time so in that case they                                 just want like a top                                                    about seeing all of them but you just                                 want the top                                                             you want to see the top                                                want to see the top                                                 browsers or cities another one is                                 keeping a session up to date you just                                 say you have something where someone's                                 on a site and there's traffic coming                                 through and you have their session in                                 hand and you want to be able to keep                                 track of that session as more data flows                                 through so that's where we'll see where                                 update state by key we could hold on to                                 a session that's live and keep updating                                 that session as more traffic flows                                 through joining the current session with                                 historical so in this case say you have                                 again that same session someone's on the                                 site or doing something and then you                                 also want to be able to reach back and                                 grab whatever their history maybe and                                 pull that together maybe you want to run                                 some other computation on that maybe                                 want to do some sort of prediction as to                                 likelihood someone may buy the                                 likelihood they may abandon the                                 likelihood that they may do something                                 provide a recommendation forum so a lot                                 of different things you could do you                                 could also decide that maybe it's not                                 historical but in just this general                                 joining of two streams maybe you want to                                 join the current visitors stream with                                 the weather so now you could grab their                                 zip code grab the weather in their zip                                 code and have an idea if you're a pool                                 company of whether or not you should                                 provide them an offer or make some other                                 decision based upon this other data so                                 it could really be used to join any two                                 streams we could also be used to join a                                 stream with existing spark rdd as well                                 so it can be coming out of Hadoop or two                                 streams coming in from say Kafka or                                 cough gun Twitter so the first thing to                                 get things going to be able to do kind                                 of our page views that are just in each                                 batch you got to create a stream from                                 Kafka so this is another java api which                                 is painful at times m it shows you more                                 i think of what's going on and somewhat                                 gets hidden from scholar but this is                                 kind of working with this source of                                 feel like the old argument of C++ or                                 maybe they always start to write a whole                                 bunch here and the Scala API is a lot                                 more succinct but really where it all                                 starts is you have this coffee utilities                                 create stream once you do that and                                 that's going to take some configuration                                 that's going to be typically what you                                 would provide to have a coffee consumed                                 that's going to consume data so this                                 we're saying that the input sources are                                 really clean this is all you have to do                                 and you could have them for all                                 different types of sources you know from                                 a file from anything it's really clean                                 like that so it makes it nice so what                                 this is going to return to us is this                                 messages d stream right this is going to                                 have in it you know two pieces that's                                 going to be the couple that's coming out                                 of Kafka that our data is going to be in                                 here so we're going to do with that is                                 turn around in this case I had stuff the                                 data in that it was tab delimited with a                                 visitor ID and the URL that they're on                                 so we're going to is just map that to a                                 new rdd that's going to be composed of                                 this couple of visitor ID and URL                                 because all this does when it goes                                 through it's just basically split the                                 data and assign it ok so what it looks                                 like down here is pretty much what's                                 happening in this code is you have this                                 consumer you have some batch happening                                 at time T at time T plus                                                 going on this way we create this stream                                 and at each batch we're going through we                                 have this message d stream that we                                 created so here right we're going to                                 perform that map operation which is here                                 and we're going to end up with this                                 events d stream at this point it's all                                 just there nothing's really going on                                 right it's just we have this data in                                 hand or will once we go to act upon it                                 okay so now we have that we have our                                 stream setup from Kafka and we have just                                 all the raw events coming in from Kafka                                 now we can start to actually do                                 something with it so the first would be                                 ok let's do the page views per batch see                                 how much is coming in so again we take                                 that events d string that we had before                                 and now we're going to perform a map                                 operation on it                                 alright and we're going to take visitor                                 idea in the or other on and we're going                                 to return back the URL from here and                                 then we're just going to account by                                 value okay so what's going to end up                                 happening is we're going to get a count                                 of the URLs with their account of all                                 the URLs and their values ok so the same                                 thing that applies is data is moving                                 across this way you know at T equals                                     plus                                                                happens and then we turn around do a                                 count by value we end up with this page                                 counts d string okay so at this point we                                 have the count that's URL and the number                                 of times it appeared in that patch if we                                 want to take that little further and say                                 well now we have that that really                                 doesn't give you a whole lot except                                 knowing how many are elves were in a                                 batch so you could maybe see like a                                 number of events that are coming in but                                 you're really not doing a whole lot with                                 it so now if you actually want to say                                 you want to see the page views per Earl                                 overtime now we could take again that                                 same events d string that we had we're                                 going to perform a different map                                 operation on it do the same thing of                                 returning back the URL this time we're                                 going to account by value and window so                                 the same before we get account by value                                 this time we're going to count the                                 values as well which of the URLs and the                                 window and time ok so this is going to                                 be our window length that we want to do                                 it or a                                                                  batch interval to be at five seconds ok                                 so we'll have                                                        compute this computation every five                                 seconds when that's done we'll go ahead                                 and reduce it by key and then what we                                 end up here right is so coming out of                                 this every five seconds so mini                                 MapReduce job if you will runs bad job                                 that runs that we end up with the URLs                                 and their counts and then we're going to                                 turn around and reduce it by the key                                 which is the Earl's we get the values                                 which are coming in as the counts and we                                 just add them together and just return                                 okay so it ends up happening when this                                 comes out let me go to use this is we                                 end up having the sum of counts for URL                                 every five seconds so from here you                                 could just see all the URLs happening                                 every five seconds you have access to                                 all the URLs with their accounts of how                                 many times they were visited the one                                 thing that's different between one way                                 between say these pageviews per batch or                                 and this this doesn't do any sort of win                                 doing operation at all so there's                                 nothing that needs to get persisted                                 anywhere for failover but it sees notice                                 we do this and then we want to count by                                 value in window and we're going to need                                 to have a window of                                                  batch interval of every five seconds                                 that data needs to be stored somewhere                                 okay so there's a check pointing that                                 you need to basically set up in spark                                 streaming to tell it where to checkpoint                                 and then check point HDFS by default if                                 you run it locally just give it a local                                 directory and it'll check point two                                 there by default with count by value and                                 window it'll checkpoint every                                            and that again is that's controllable so                                 now we've got all the page views with                                 their accounts going on something that's                                 maybe more interesting or more useful                                 instead of just streaming a bunch of                                 data at someone is actually just have                                 the top and maybe you want the bottom in                                 this case we're just going to like the                                 top so in this we're going to take that                                 sliding page counts that we just had                                 before we're going to turn around and                                 basically just swap them so now instead                                 of having the Earl and its count we're                                 going to end up with a mat with a d                                 stream that has the count in the Earl                                 once we have them swapped we're going to                                 turn around transform that                                 and basically sort by key and then I                                 come I mistakenly deleted code that                                 should've been there have the last thing                                 that you would do is from there you                                 could do a take and take                                               get back just the top                                                 already in sorted order just can give                                 you back the first                                                       also an API to make this simpler that                                 you could just do a top you could do its                                 other ways you could solve the same                                 problem so say in this case that's all                                 great and you have a dashboard that has                                 a top end of things that are going on                                 maybe its pages maybe its cities                                 whatever you want it to be some other                                 data and now you want to basically have                                 the situation if you want to update a                                 current session as something's flowing                                 by say it's a user session as they're                                 active alright so what you could do with                                 update state by key is really specify a                                 generic function to modify previous                                 state with new data so in this case                                 we're going to declare this update                                 function that really all it's going to                                 do is just going to take this page view                                 assume that we have some page view                                 object is going to take the session and                                 then we're going to perform an operation                                 on it update in the session and return                                 that updated data so now we could take                                 in the new pages that were viewed and                                 the session object that we have that's                                 representing our state and update that                                 session with a new page information and                                 then return it okay so when we do that                                 this is the update function that will be                                 passed in and we just basically takes a                                 this page view d stream that we may have                                 call update state by key and pass in the                                 state information the function that                                 represents that ok when that returns                                 then we have visitor history in a                                 current visitor session that's currently                                 being kept as data is flowing through                                 you know there's still things to work                                 out that you would need to do as far as                                 what happens when you know that states                                 longer valid                                 you know in the case of say a visitor                                 session it times out so you still need                                 to do other things outside of this to                                 make sure that that's cleaned up or                                 taken care of so now you have a current                                 session maybe we want to be able to join                                 it with history so we can start to see                                 what's this user done over the last                                    days compared to what they're doing                                 today I maybe you want to make                                 predictions for maybe you want to                                 recommend something maybe you just want                                 to update some information not show them                                 anything so let's assume that we created                                 this current sessions d stream and we                                 created a historical sessions stream                                 this shouldn't say D string and that's                                 just an RDD from spark and say our                                 current sessions looks like this we have                                 you know it's composed of tuples a                                 visitor ID and some JSON object that                                 says that's current session another                                 visitor ID with a current session and                                 then we have historical sessions that                                 look something you know not too                                 different from it visitor ID one with a                                 historical session visitor ID                                          historical session so now we go ahead                                 and we just call current sessions join                                 and pass in the historical sections                                 what's going to end up doing is joining                                 on the keys so as long as these keys                                 match where they do here you're going to                                 end up coming back with a couple that                                 has a list in it of the sessions so you                                 end up with the visitors ID along with a                                 couple of all the sessions from the                                 joining of those two streams so in this                                 case it works if you want to do stuff                                 with visitors is that this could be from                                 a spark rtd that you created it could be                                 from another stream coming in from say                                 Twitter and you have someone's ID and                                 you want to combine things it could be                                 weather data it could be anything you                                 want where you have an RDD or D stream                                 in hand and the keys common you want to                                 combine them                                 so so where are we so we kind of walk                                 through kind of this flowing through                                 we're going to get these chunks of data                                 coming out right in these batches of a                                 half a second we did some processing on                                 it and then we have the process results                                 going out and then back into Kafka so in                                 this case that Joyce use Kafka there's                                 other ways that you could do it as well                                 you know so you see that there's some                                 problems that you need to consider if                                 you end up using Kafka and say you have                                 a WebSocket that's connecting and you                                 have some WebSocket server that's going                                 to be managing clients there's things                                 that you need to consider within spark                                 streaming that are unlike spark right so                                 for instance a regular spark job when                                 you run it it's like a MapReduce job the                                 computation is done the job's done in a                                 spark streaming job it runs forever                                 until you kill it so it doesn't end as                                 soon as the computations done so the                                 batches that you run it and it's just                                 sitting there in a loop if you will and                                 constantly running so imagine if we did                                 have say this going on we had those you                                 know handful of streams that we're going                                 over of say top end and we're updating                                 visitor history we're doing these things                                 those are always going to be running and                                 producing data to somewhere so there are                                 some things you need to consider you                                 know when you have it set up where                                 they're launched and run and they're                                 just going to be pushing data out of                                 there's no way to kind of shut things                                 down to clean it up so getting data out                                 of spark streaming so those are so                                 before that it only supports you know                                 print for each RDD you know save objects                                 to file save to HDFS save the text                                 doesn't and doesn't do anything more                                 than that so kind of the next step is                                 once we have this here of how do you get                                 it to Kafka                                 so this is one example of how you could                                 use a for each rdd em which again is not                                 a generic way of output but it's the                                 only way currently to be able to have                                 access to the collection of our duties                                 from a badge and to do something with                                 them this Kafka producer show is really                                 nothing more than just typical standard                                 Kafka producer code and just go through                                 and in this case you know we had say the                                 top end stream that we were doing and                                 we're going to attach this operation to                                 these sort accounts that we had from                                 before and we're going to loop over                                 these are dd's build up this map and                                 then basically turn around and send the                                 top                                                                     before that you know you could do this                                 if this rdd take so we had that sorted                                 counts you guys execute this take to be                                 able to take the top                                                  that sorted rdd so this is nothing                                 different here except just walking                                 through it then just call it a cough                                 great to produce and just to send it all                                 right so not as generic as getting data                                 in so websockets kind of chose of had                                 success with it it's kind of an abuse of                                 it at times if you use it for storm with                                 very high volume to have all that data                                 going out to web sockets you know it                                 really causes pain sometimes two                                 browsers when you're pushing a fast                                 stream across it yeah but it looks fun                                 it shows well it does pretty cool things                                 so it's a standard way to get data in                                 and out easy to prototype with a browser                                 so one way handling it oh wow sorry                                 wrong button                                 oh so one way of handling this WebSocket                                 server and someone connects it's                                 basically just starting to consume data                                 from Kafka I starting to consume on say                                 a top                                                               pulling that data back and sending it to                                 the browser as I said where this could                                 end up with problems is you run into                                 issues potentially with multi tendency                                 of how do you know how many clients are                                 going to have and you have different                                 data that's being processed through here                                 and then I got to have different data                                 that's through here and you have                                 resources being consumed in spark                                 streaming safer it's multiple customers                                 of how do you have all those jobs                                 running at one time and how do you make                                 sure you have enough resources and what                                 do you do if you're running computations                                 for say a customer's data but there's no                                 one over here even listening for it and                                 wanting it right something you sitting                                 there just computing stuff I'm not going                                 and isn't it going nowhere so another                                 option as opposed to doing this that                                 have seen be successful is half of that                                 WebSocket server have a client that                                 comes in and since you're already                                 infested with zookeeper if you have                                 Kafka of turning around and registering                                 this server in zookeeper with some sort                                 of ID about the browser that's connected                                 to her the WebSocket client that's                                 connected to it and then having spark                                 streaming and the code you have in there                                 watching zookeeper for changes and                                 seeing who has the data and who needs                                 the data and then via zeromq going from                                 spark streaming back to WebSocket server                                 so then you could handle things up                                 starting up a stream if someone connects                                 you could tear down the stream when                                 someone disconnects and you could handle                                 state and here as far as resource                                 consumption so this works well as this                                 kind of demo but it is it does have some                                 potential issues based on what you're                                 going to do here because those jobs                                 never stop running                                 skins is the summer and then we could                                 walk to the code show the demo running                                 if we want and spark streaming works                                 well for click stream analytics you know                                 I use it for doing those types of things                                 use it for trying to do some sort of                                 predictions it's kind of working with                                 that data it is still there's no good                                 out-of-the-box output operations for a                                 stream you know for each is just not                                 that great multi-tenancy it needs to be                                 thought through you know so there's                                 because those jobs have no end of life                                 you're stuck in the position of having                                 to control what happens to them how do                                 you bring them up how do you bring them                                 down with regular spark if you notice                                 there's now a job server that you could                                 submit jobs to we've done things or it                                 compiles MapReduce you know spark jobs                                 on the fly and launches them that's nice                                 that's easy because as soon as that job                                 finishes you get back the results it's                                 gone in this case even if you did the                                 same thing and you programmatically                                 launched a spark streaming job you have                                 to know when it needs to die or skin                                 continue to just kill resources that's                                 all that I had right now for know if I                                 good it's time for your Nathan sure okay                                 just give me a second so if I understand                                 you were using zookeeper as a control                                 channel in order to control what parts                                 of the streaming jobs were running or                                 not yeah yes could you zookeeper as a                                 control so have you considered using                                 kavkaza control Channel I've heard                                 people do that and published requests on                                 kefka and and then this spark in this                                 case would respond with a new stream I                                 haven't no actually have not looked at                                 dinner i'll show you where we did use it                                 we'd use zookeeper you know in place of                                 Kafka here so just kind of take this                                 other picture so if you imagine this                                 arrow going to hear /                                            zookeeper here and we do things where if                                 like say a client                                 connected via socket server we'd have                                 the client idea of who it is we know                                 what their rights are what they're                                 allowed to and then register in                                 zookeeper the IP address of this host                                 along with the client that's connected                                 and then from there inside of inside the                                 streaming basically listen for changes                                 in zookeeper and know that this hose                                 services this client and there's no data                                 through yeah that's the Samara furring                                 to I've heard people use Africa for                                 precisely that in your spark streaming                                 code you could have a like a calf occur                                 consumer listening to essentially                                 request servic Africa and then you                                 wouldn't have to connect spark and                                 zookeeper right yeah it's interesting                                 yeah I can see that working yeah then we                                 just handle like a disconnect which                                 could be another message in the kafka                                 topic certainly yes good idea we started                                 prototyping something similar with Spock                                 streaming but instead of Kafka we are                                 using the flume out the oval what                                 happens when the message is out dropped                                 our if what is the water than see on the                                 kafka side not to measure family always                                 kefka and up there are duplicated the                                 elements or drop to demands so you can                                 store the data based on time we had it                                 set up over to store data for                                          so we'd hold on to messages for                                          before anything we get deleted I'm sorry                                 to repeat your question think about you                                 does no no I was saying the flow of                                 events from the input of sparks trimming                                 so you say that kefka is what I ever                                 right mm-hmm / input she asking about                                 input into spark streaming yeah                                 so yeah Kafka is one of the input                                 sources yeah under compound to flume                                 that in case of a problem you can have                                 duplicated messages that happened with                                 kefka you could I mean it doesn't know                                 right so you could have multiple                                 producers producing the same data onto                                 Kafka yeah that's not my question but                                 what do it sorry                                 you were explaining how you were merging                                 or joining data with his own law on                                 streaming data we have the historical                                 data right right and you have pretty                                 short batches right and you can have                                 quite a bit of historical data so how                                 does this joint happen actually                                 efficiently because I thought it should                                 happen more or less immediately but you                                 can have lots of historical data so I                                 didn't quite get it right so the                                 question was how do you combine                                 historical data with the in flight data                                 fast that right so the historical day                                 that we had was just visitors in                                 partition by visitor so we would have                                 data that's routed to the right notes                                 with a visitor and then have all the                                 data that's there and in reality a lot                                 of visitors don't have that much data so                                 it ends up not being that big and                                 aggregate it's a lot there's a lot of                                 data across all the visitors but as                                 someone's trimming flip through it's                                 really not often do they have a lot of                                 data if you look at a lot of web                                 analytics everyone believes that people                                 come back all the time but the reality                                 is a lot of people don't and sessions                                 are small so the history really                                 sometimes isn't that big but can I kind                                 of pile up actually still if you are                                 lucky and certainly so in you're stuck                                 there right because if you're getting                                 this data from HDFS and that becomes a                                 ball neck of getting it into spark and                                 then at that point right you're bound by                                 memory or a putting it to disk addition                                 to this question if you have for example                                                                                                        million coming in in parallel so how can                                 be is this really efficient to figure                                 out do continuously filtering through                                 this even if it's partitioned by server                                 so so you have to go to a huge amount of                                 data every time because the data is                                 start and probably in something more                                 sequel right hammer in terms a partition                                 right to think question was so if the                                 data just kept exploding how do you do                                 it efficiently so we used a bloom filter                                 to keep track of whether or not the                                 visitor is poss                                 we on that note and to keep trying to do                                 checks to see if through there before                                 doing the operations so we would build                                 those and then have that there in hand                                 to see whether or not we actually had to                                 try and do some massive join what was                                 the problem with the mazes so why did                                 you have to wait for the standalone                                 feature in spark the question is why is                                 there a problem with my sauce and we had                                 a way there's no problem we're just a                                 small shop at least and we had a Hadoop                                 cluster that was just plain vanilla                                 Apache and we weren't we just weren't in                                 a position to be able to also deploy a                                 cluster that head meadows so once it was                                 stand alone and that was one less thing                                 we need to go asking for it just became                                 much easier for us so there was no                                 problem with it we didn't have issues                                 with it technically it was more of an                                 operational thing of chemical asked for                                 yet one more cluster so that was all if                                 I remember correctly you used to have a                                 storm cluster is that correct will you                                 try to out spark no so the question is                                 we used to have a storm cluster actually                                 still do it was kind of side by side so                                 we're using storm for an event stream                                 and then we want to explore using spark                                 streaming for doing more like an                                 aggregate stream and things that didn't                                 need that from click the dashboard as                                 short as possible okay so it was                                 something that's continues to run and is                                 out there one of them if the feature                                 some storm is is the you can push stuff                                 to backing store and have a cash in                                 front of it if you use tridon that is is                                 that something that you use have you                                 missed it in spark or how did you solve                                 it in that case M so the state you get                                 from trident we you know I had looked at                                 it didn't play much with it but you get                                 pretty efficient you know execution of                                 it with spark streaming so it's going to                                 store that data you could push data to                                 HDFS from spark streaming as well so it                                 doesn't always have to go out you could                                 save data off every time a batch runs                                 okay a question on join joining to these                                 streams a seal problem that when you                                 joined to the strips the problem is that                                 you have to have the same keys at the                                 same batch right do you see it really a                                 problem for you or do you solve it                                 somehow right so the question was                                 joining                                                                 same key in the same batch I the only                                 thing that we were attempting to do is                                 just make sure that visitors were routed                                 the same way and just that we'd end up                                 with the same visitor ideas on the same                                 spark notes so and I think it could be a                                 problem though if you don't have the key                                 there you kind of not gonna join and so                                 we handled it by trying to route visitor                                 IDs I believe we still have time for one                                 last question if no thank you very much                                 thank you
YouTube URL: https://www.youtube.com/watch?v=1QLVMwtoDog


