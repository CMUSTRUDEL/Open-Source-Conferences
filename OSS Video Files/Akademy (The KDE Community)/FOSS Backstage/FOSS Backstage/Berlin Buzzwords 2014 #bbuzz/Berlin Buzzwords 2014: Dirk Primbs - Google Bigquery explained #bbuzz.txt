Title: Berlin Buzzwords 2014: Dirk Primbs - Google Bigquery explained #bbuzz
Publication date: 2014-05-28
Playlist: Berlin Buzzwords 2014 #bbuzz
Description: 
	Google Bigquery is a data analysis tool, which can crunch terabytes of data on demand in seconds using SQL queries without using expensive in-memory technology.

It has been used extensively inside of Google for analyzing large datasets and log files for years and is also available externally.

The scientific paper about Dremel ("Dremel: Interactive Analysis of Web-Scale Datasets") explains the algorithms behind the tool.

This talk goes through the algorithms in a simplified and accessible way by visualizing how Dremel executes a query on a small dataset.

Read more:
https://2014.berlinbuzzwords.de/session/google-bigquery-explained

About Dirk Primbs:
https://2014.berlinbuzzwords.de/user/363/event/1

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              alright so Google's back on stage with                               the subject on big data again for the                               ones in room that followed my colleague                               earlier on you probably have seen a few                               of the bigquery well the curie queries                               okay already my name is Dirk prince i'm                               working for google and a developer                               relations department and i'm responsible                               for the whole german-speaking region and                                well cloud subjects are one of my                                favorites because you you get such a                                massive scale at eight points and for                                the next                                                                with you is I like to have a look at                                bigquery from the point of view for how                                do we actually accomplish queries that                                go over massive scales of data like                                gigabytes of data terabytes of data                                without having to well wait minutes and                                and and go grab a coffee or such but                                instead get a really almost real-time                                responses like in the                                                range as you have seen in n examples                                area so if you're interested in that                                type of thing then you probably have                                browsed the google for developers                                websites already might have browse for                                the cloud technology and you find                                something they are it is called the                                Dremel white paper so dremel is the code                                name or the former code name of bigquery                                and it explains in quite computer                                science tific terms how we actually do                                what we do in big fairy now when i                                prepared for that session i downloaded                                that paper and i had a good long look at                                that paper and i tell you that's kind of                                a little bit painful go to the                                experience yourself but the whole point                                of this session is actually to walk you                                through the through the algorithm in a                                way that's not that painful and                                interesting and well it's hopefully                                interesting and not that painful that                                was proper english i guess and well so                                you get an unfair advantage above me and                                others who have prepared for that                                session so you                                you have a general understanding already                                all right um so bigquery what is it for                                those of you who haven't been here on                                the session before how many of you in                                the audience have played with bigquery                                already like five or six people who                                knows what be curious yeah those were                                the ones who bear in the session earlier                                I guess and the few of those who played                                with it all right so you find bigquery                                on well or the interface to be query if                                you go to developers of google com you                                just follow the signs that this week are                                the links to the bigquery dashboard and                                basically think of it as a read-only                                database in the cloud that you can use                                to analyze your data to to fire or                                queries against to work with it and what                                you what you find is as an interface a                                bit query of course is an API that you                                can use a developer as well but if you                                go to the developer console you you find                                basically this type of interface you can                                type in a query you can select your data                                you can well upload your own your data                                that you want to analyze and such and                                the type of data set that we like to use                                for the next                                                       github archive so github basically                                publishes the whole the whole set of                                commits as a big database and we have it                                as a public data set in bigquery for you                                to play as well and it's a pretty useful                                data set if you like to play a little                                bit bit with it so put the starter um to                                get a feel for the type of technology we                                talked about I prepared a statement a                                query that basically counts all the                                languages or the commit in in github                                according to the languages that they are                                using and what you see here is kind of                                like runs through the data net process                                like well one dot                                                      in about three seconds so it's not too                                bad can also enable catching I disabled                                it for the for the for the demo to to                                get a real                                real timing here and what we see is the                                Curie results at the top at the bottom                                which basically is no surprise it's a                                well it basically says javascript is the                                 most common language in github                                 repository followed by Java followed by                                 Ruby followed by pipes and followed by                                 PHP and well I guess followed by all the                                 other stuff down there so we can loop                                 through                                                                right now but of course we can download                                 it as a CSV as you seen might have seen                                 earlier and we can continue to work with                                 that type of data so to give you an idea                                 about what's possible with it I prepared                                 a few other statements I thought that we                                 do a little scientific experiment here                                 and have a look at how you guys behave                                 when you submit code to the github                                 repository so I assemble the list of                                 quite colorful language read it                                 carefully you never used one of those in                                 your comments right and well make a make                                 a run through the github repository and                                 look for the overall setup in the                                 commitments and again it takes a few                                 seconds so we processed about six                                 gigabytes of data right now and yeah                                 you know god damn game developers and I                                 i I'm not I'm not sure any rimmel                                 developers in the room I've seen one                                 hand one hand I didn't even know that it                                 exists but when I think of why I I'm                                 yeah I can understand why you're in the                                 mood of colorful language and c-sharp                                 see some in the middle and again there                                 are there are more more data sets to                                 discover if he if we pay true that well                                 looking at this varying in github maybe                                 maybe we should have something to                                 balance it and look for the most polite                                 developer as well and the most polite                                 developer of course would be someone who                                 says thank you for while checking your                                 data before submitting it to my function                                 and please be respectfully when you add                                 and something to my data by a database                                 and so on and so forth and we do the                                 same exercise again and who would have                                 thought that the see developers are the                                 most polite developers in the github                                 repository so well that's data data                                 never lies as we all know so this is                                 scientific fact now and next time you                                 meet a see developer in your team think                                 of it when you are such swearing he                                 might just say thank you respectfully                                 yeah so the whole point of that exercise                                 is to give you an idea about the the                                 stunning speed the whole big hurry                                 engine on it's working behind the scenes                                 so that's not the only public data set                                 you can play with a number of public                                 data sets or your own data as well so                                 have a look you can browse through all                                 the Shakespeare works or through                                 Wikipedia as you as you might have seen                                 earlier and those are very interesting                                 data sets to work with and the question                                 raises how we are pulling that off                                 getting this type of speed and all now                                 it would be great if I get a internet                                 connection here                                 because I would like to continue                                 explaining the algorithm well anyway I                                 think it might not work that way may as                                 well so for the next                                                    walk you through how the whole thing                                 works and I were um basically you see                                 and there are two two types of data that                                 we can actually attempt to analyse and                                 especially in the web world one is the                                 realm of the rational database so you                                 have tables you have columns your froze                                 in there and it's a very straightforward                                 but struck a strict and structured type                                 of approach how you how you work with                                 your data the other way of date and the                                 other kind of data that you're often see                                 is the type of data use it might see in                                 in JSON objects or in XML or in that                                 example here and I try to get you that                                 that noise out of that should stop                                 breathing when I talk with that                                 microphone yeah that the type of type of                                 information that you find in JSON data                                 sets or in XML which is basically a                                 hierarchy call kind of data                                 representation where you you have not a                                 fixed structure up front but mere and                                 more or less you you add nodes as you                                 come along to your to your data items                                 both types of data can be found in in                                 bigquery but the more interesting ones                                 are certainly those hierarchical type of                                 data sets because if you think about it                                 if we if you try to solve the task to                                 analyze the web that which is kind of                                 what Google does on a day-to-day basis                                 and firing searches on the whole public                                 Internet what you find there is HTML and                                 XML which is that kind of hierarchical                                 representation of data that you need to                                 analyze so for the for the example we                                 walk you I walk you through a list of                                 books for simplicity it's a very simple                                 list of books but you you kind of walk                                 out here with the idea of the algorithm                                 and you can go as complex as you like to                                 be and those three books in my example                                 have a number of different items and                                 elements on it like                                 author title pricing and so on and so                                 forth and this is the kind of query that                                 we like to analyze here so when you                                 think about bigquery or when you think                                 about database in general but you try to                                 avoid it's usually a full table scan                                 right so you try to be efficient in your                                 query and normally databases build                                 indices and try to optimize your speed                                 of of selecting data by well by                                 selecting specifically what feels to                                 touch but in case of bigquery what you                                 what you essentially try to do is you do                                 a full table scan you analyze all there                                 is in their data set so if you use                                 bigquery then you say the battle is lost                                 I need a full table scan and bigquery is                                 the way to make the most efficient full                                 table scan that you can can use today                                 and that's the type of query that we                                 walk through in the next few slides okay                                 remember the books that I that I mention                                 and so I think I have yeah I have a                                 connection again so we can make that a                                 little bit larger so there are three                                 books with authors and titles attached                                 to it and the very first trick big big                                 furious is doing is it changes the                                 representation of that data from a row                                 based representation to a column based                                 presentation so instead of having all                                 those rules with all the data in it it                                 changes the way it stores the data                                 efficiently to column so we have an                                 author column a title column a price                                 column price that Europe rise have used                                 d and so on and so forth and if you look                                 at the the elements here those are the                                 data stored within and just ignore the                                 common the numbers in the apparent ease                                 is right now those are the magic cells                                 that make bigquery work be we go through                                 it in a bit but essentially that's the                                 very first trick changing the                                 representation of the data all right and                                 by thinking of the data the next the                                 next step bigquery is taking is                                 it analyzes the data set in it adds                                 metaphorical speaking the missing                                 elements to the other the other rows so                                 to speak so if you look at book                                          we have authors we have a title but                                 there is no price but there is a prize                                 in book                                                                 so it adds the price and the responsible                                 subnodes to the the representation I                                 write it in in paranthesis to make it                                 make it clear that they are actually not                                 data knots data in the bigquery                                 representation but is an added element                                 to the whole thing it start very                                 efficiently so it doesn't add up to the                                 data but at least conceptually for                                 walking through the data they need to be                                 there so they be query adds those                                 elements then in the next step um it                                 gives names to all elements and those                                 names are following a typical pattern                                 that you might recognize as a developer                                 so if you if you look at author well                                 it's pretty straightforward author title                                 or named quite directly then we have the                                 price dot discount property the price                                 USD property price toward euro and so on                                 and so forth in cases where we have more                                 than one element like here with authors                                 or down there with prices it's like an                                 array representation so you simply count                                 through the elements and again for                                 completeness here are the names in                                 parentheses further for the elements                                 that actually don't carry any value all                                 right now let's have a look at those                                 numbers and stay with me because we walk                                 through it in a bit and then it makes                                 all sense when it comes together there                                 are two numbers with each data element                                 and one is called the repetition counter                                 we repeat count and which is basically                                 the number of elements following or the                                 number of repetition a data element has                                 so if you look through that if you have                                 a repeat count of                                                        this means this is the first occurrence                                 of title                                 and the same is true for for price that                                 discount priced at USD price ural those                                 are all repeat count                                                    the first instance in that data set                                 going to book two it's pretty similar                                 author is the first occurrence so it's a                                 repeat cannot                                                          repeated once here and it's repeated                                 once there though it's a repetition of                                 that element so it's a repeat count                                      both cases and if you go all the way                                 down you find the same pattern like here                                 with the prices soda the price priced                                 element is curl is counted here as a                                 repetition the second number is it is a                                 definition number so what it basically                                 says how many of those elements are                                 defined so if we look here price dot                                 discounts are two elements as a prize                                 element in the discount element and if                                 it carries a value then prizes defined                                 and discount is defined so that the                                 definition count is two same here same                                 here if we go down here then we have an                                 author element which is just one element                                 and it is defined so it the finishing                                 count is                                                              have an interesting one which is a                                 definition count of two for price                                 discount I'm followed by a definition                                 count of one priced at USD this is                                 because the price tag actually is there                                 in that element but USD is not defined                                 so it's just one defined element and not                                 the second one I hope I you're still                                 with me did those of you who are                                 shortener yeah some nodding that's good                                 all right so let's bring it all together                                 because it's quite theoretical right now                                 and we play it through with one in the                                 first example with one column and then                                 we                                 use two columns because remember we are                                 actually trying to do a select star from                                 all books their prices in urine and USD                                 are in a certain amount that was a query                                 that I showed you earlier and we are                                 doing that right now by with the                                 author's column so um the author of the                                 first book what we see here is we see a                                 repeat count of                                                          we see a definition count of one so yes                                 the author is defined the value is Dumas                                 you have a book one author Dumas next                                 book because we have a repetition count                                 of zero it's a new book and we have a                                 definition kind of one so it's defined                                 and we have an author and now the author                                 gets repeated that's because the                                 repetition we see that in the repetition                                 come which is one now and it's also                                 defined and this of course is the same                                 case in the next author element then we                                 have a null and this is an undefined                                 author element and we see it's a new                                 book we see it's not copied so it's                                 essentially a new book which happens to                                 have no author attached to it all right                                 still are you still with me or anyone in                                 the room that likes me to go over that                                 one more time no everyone in room no                                 please um okay so now let's yeah yeah                                 I'm can say it again                                 the question was the the repetition                                 count can it be higher than one or can                                 it just true or false yeah it's true                                 false all right and so let's do it with                                 the euro price taken the u.s. dollar                                 price tag because that was what the                                 query was selecting for and that's where                                 the whole magic basically happens and it                                 gets more interesting or at least my                                 definition of interesting here so first                                 first item we see its repetition count                                 of                                                                 definition count of                                               defined and euro is defined we have a                                 Europe rise and here's the same see                                 Europe because it's a new book and two                                 because price is defined and USD is                                 defined the next element we see it's a                                 new book because repetition count is                                   and definition count                                                     euro price element to it and it's the                                 same case down here so we don't have a                                 price tag attached to book to next                                 because it's                                                           here the definition count is                                             euro are defined so we have an item here                                 the definition count is                                                 is defined we do have a price but it                                 happens to be not a US dollar price but                                 a Europe rise and this is a pretty                                 similar case so here we have we know Bo                                 another similar cases a similar case                                 than before we have a repetition so this                                 element happens to have another price in                                 Europe and again price and eurotech are                                 defined so it's two and we know it's a                                 euro price and down here we have a null                                 element which basically means the value                                 for USD is                                                             we know it's a repetition we know that                                 price is defined but USD is not defined                                 so it's                                                                                                          essentially what we are doing with that                                 type of algorithm is we optimize what                                 columns we need to touch in order to do                                 the full table scan instead of running                                 through the whole data set of I don't                                 know how many gigabyte or terabyte of                                 data you're essentially just touching                                 those elements that are used in the                                 query and those two counters and the way                                 you walk through the data enables you to                                 do that very efficiently without losing                                 the overall structure of the data so you                                 get as a as an reply you get the real                                 data set that you asked for and instead                                 of browsing through let's say five                                 gigabytes of data you're parsing just a                                 few hundred megabytes nevertheless those                                 few hundred megabytes need to be still                                 crawled and run through very efficiently                                 and of course we do that with the power                                 of the google data centers so that                                 usually what you do you spread out the                                 whole data you learn through across a                                 number of servers depending on the size                                 of your data set and then you have leaf                                 servers that do that process the query                                 for you accumulate that and sometimes                                 you have met several levels where you                                 run that query through and by the way                                 that was the query we just parse with                                 our three books of course this this is                                 quite efficient and how efficient can be                                 also seen in that a white paper I                                 mentioned and I took a few of the                                 statistics out there for you to look at                                 for instance there is a day of a                                 benchmark using                                                          by using bigquery you you official                                 efficiently eliminate like                                             terabytes of the data that doesn't need                                 to be processed any more of you if you                                 use bigquery as a data processing engine                                 and here's a comparison of the data sets                                 or the data retriever time the execution                                 time using different or a number of                                 technologies to retrieve the data                                 MapReduce being the first one then them                                 you have                                 dremel would just be query essentially                                 and the middle one is map reduced when                                 you use a color more representation so                                 you see that's a potential optimization                                 in its own right just to use a kilometer                                 presentation in dremel is it is able to                                 eliminate a lot of the data processing                                 that you usually have to do and this is                                 a similar case so here                                            records                                                                  the number of levels that you that you                                 use also defines how fast you basically                                 run your query all right and before you                                 start throwing things at me because I                                 run over time this was a quick tour                                 through the algorithm of bigquery I hope                                 you you kind of like well get interested                                 enough to have a look at bigquery and                                 maybe even at the right paper it's a                                 kind of like an interesting read if you                                 if you're in computer science and in any                                 case big furious an API should be worth                                 playing around with if you if you like                                 to analyze big data sets I'm around in                                 the in the break and today and tomorrow                                 you can find me on the on premise also                                 you can of course find me online and                                 shop me a question that might come later                                 to you and with that thank you very much                                 for your attention hope it was a little                                 bit of fun using decree                                 I might allow two questions if there are                                 a medient he might so only if the                                 questions right yeah so I'm I saw in the                                 previous demonstration that we was using                                 bigquery to do bigquery joins of                                 datasets and I'm wondering how that's                                 implemented because the way the                                 execution tree stuff works like only                                 imagine how it works for scanning over a                                 single data source obviously we joins                                 you go to scan multiple data sources i'm                                 already don't understand how that                                 combined works yeah that that's                                 basically where the magic of the Google                                 Data Center happens how we spread out                                 the data and run it simultaneously but                                 what I try to cover here is the                                 essential way how how the algorithm in                                 in South works and as you say it's not                                 that that complicated and if you if you                                 do joins or multiple joints you                                 basically pre process the data in the                                 data center where you have a larger                                 datasets then but again you can have                                 columns where you select items and bring                                 it together that way there was a second                                 one over there or here related question                                 how do you limit so is that could be                                 simple query could be complex square as                                 far as I understand to charge card                                 storage and / a size of data set which                                 was done audit but I can imagine that                                 you could very complex queries which                                 would consume a lot of processing power                                 how do you limit to how they solve these                                 cases we wouldn't become water net for                                 you or very cost efficient so oh my god                                 sorry so the question is how do you                                 control costs essentially i'm there                                 there are a number of things that year                                 that you can can look at there's a                                 control center where you can define                                 limits and how you how you actually want                                 to allow how many transactions you want                                 to allow how much workload you wanna                                 allow and secondly you have a full set                                 of api that you can script against the                                 right code against if you want to                                 control your your crease like how much                                 how much time is allowed how much                                 consumption before you get for instance                                 of warning in your you structure your                                 data                                 but on there I'm not I'm not really                                 familiar right now with the current                                 pricing on bigquery but uh that I think                                 there is quite a number of crew that you                                 can do at a relatively low price point                                 before it really becomes a virtual okay                                 one last one last I think yeah oh I have                                 a question about the filling out the                                 gaps so it seems that your records I                                 mean you infer the structure from the                                 data you've God so what if a new record                                 comes in and has a alters the existing                                 structure how do you feel it fill out                                 the gaps effectively because it feels                                 like it could be a massive fan out of                                 updates yeah so so what you do here is                                 read the only operation so you pump up                                 the whole thing and then you do analysis                                 against it if that would be a live data                                 set like processing in the back while                                 you're doing queries then then I'm with                                 you then it's probably a little bit                                 complicated to fill out the whole thing                                 so it happens once when you push the                                 data update and then the new video you                                 all set all right um I'm here for                                 questions after the session for those of                                 you who like to to show up I'm pretty                                 happy too oh yeah you you like to me to                                 give away those t-shirts anyone who                                 likes to have a t-shirt I tried to get                                 one really far oh yeah that was not                                 really far that was like middle and that                                 it's like in front row and all right                                 thank you very much guys                                 you
YouTube URL: https://www.youtube.com/watch?v=Pjd-ld1BI7A


