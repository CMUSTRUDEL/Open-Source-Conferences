Title: Berlin Buzzwords 2014: Steve Loughran - Secrets of YARN application development #bbuzz
Publication date: 2014-05-28
Playlist: Berlin Buzzwords 2014 #bbuzz
Description: 
	Learning from the mistakes of others is better than learning from your own -this talk lets anyone writing a Hadoop YARN application learn from mine. Client, Application Master, and worker design, RPC service interfaces, secure operation, failure-handling and test strategies, are all key issues you need to get right -the key points being "Model-View-Controller" is still a good architecture, while mock tests are the secret to testing that model.

This talk tells people writing YARN applications what they need to know -to help the build YARN applications that can work with all the data waiting for them in Hadoop clusters.

Read more:
https://2014.berlinbuzzwords.de/session/secrets-yarn-application-development

About Steve Loughran:
https://2014.berlinbuzzwords.de/user/228/event/1

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              hello everyone I am Steve breaker of                               things at horton works and i'm here to                               talk about the secret of yarn                               application development I've been doing                               some of this myself and these are these                               are some of the lessons I've learned                               before I start hands up who's heard of                               yarn tool hands up who's heard were this                               okay keep your hands off you've actually                                coded applications specifically for it                                okay those look at the hecklers all                                right over there a cow I will I will                                maybe except questions from you but I'm                                a bit hung over so don't shout them too                                much okay hey this is me back in the                                beginning there was Hadoop and there was                                Matt produce and it was incredibly                                successful because it it made writing                                distribute applications easier for the                                first time you didn't have to understand                                all the problems you just wrote a mapper                                and reducer and it works oh it worked                                provided your code was either a map or a                                reducer but as the amount of data the                                cluster collective got bigger and bigger                                other people in the organization wanted                                to run their code on it too and the only                                way you could do this was by having your                                code pretend to be a map or a reducer                                with her a hack known as the long-lived                                mapper we'd start a MapReduce job that                                would never ever finish it was an ugly                                hat with just about got things to work                                it but I had bad failure modes you                                couldn't really expand it on demand or                                shrink and or choose where things would                                be it would just be where your data                                walls around them and the Ops teams hate                                it too that job tracker it just wasn't                                designed for long live code it couldn't                                deal with the scam and failure so it was                                an ugly huh so along along comes a                                solution yarn yet another resource                                negotiator its aim in life is to let you                                run other other algorithms alongside                                MapReduce so Matt produces there it                                still runs happily we try and run                                existing code but we also you can rather                                the code there and now there are                                projects actually building the other                                tools to go alongside                                so if you view Hadoop as a kind of data                                central that they descend to level OS                                yarn yarn is part the execution engine                                right down the bottom there's almost a                                device driver level of the host OS and                                the networking stuff you don't need to                                understand the details you don't need to                                look at linux device driver internals                                but it is kind of handy to have a vague                                idea what's going on same for networking                                HDFS you can go back and read the                                original papers from the early                                        Juji computing you don't really care                                about that all you care about is it's                                very cost effective to store lots of                                data and it can come in remotely so                                yarns the same thing we've been trying                                while my colleagues I've been working on                                something lets you run your code within                                a hooded cluster it deals with the                                problem of getting de binary's to the                                machines to run the stuff to keep track                                of the health and you you can focus on                                the layers above you can decide what you                                want to run where you want to run it how                                failures happen so I like you this is                                kind of the land port there you work at                                this you have to go and read lamport's                                papers and then run away screaming you                                really want to focus at this level above                                the algorithms and even if you can the                                really high levels because ultimately                                you're trying to do useful work you know                                and MapReduce worked really well because                                it hid all the complexity all the stuff                                down below and said mappers and reducers                                ass that's what everything should we                                think about here is what can they do in                                terms of more reusable systems like this                                happy do inside yarn and just generally                                get your work done with the petabytes of                                data you're collecting so yawn it runs                                the code across the cluster summer in                                there there's a resource manager one of                                them or now there's two of them know                                Gotye I'll use machine your cluster you                                something called a node manager it talks                                the resource manager says it's there and                                 it manages these things called                                 containers and a container is really                                 currently a sea group managed execution                                 kind of process tree of code that you                                 running we're playing with docker a bit                                 and                                 the containers are the code that yarn                                 tells it to run resource manager sighs                                 what's going to run where tells no                                 managers they run it and something                                 happens here youngest told about again                                 resource manager and it gets to deal                                 with it the way it works is your code                                 your code talks to yarn say I want to                                 run something and you run something                                 called the application master it's worth                                 noting you have a whole new set of                                 acronyms here if you thought you                                 understood about data know the name node                                 and task tracker you've got more to                                 learn so the application master is                                 effectively the successor to the                                 jobtracker it's your own personal job                                 tracker to run your code with your                                 algorithms in your policies so what you                                 doing is tell yarn to say check your                                 application somewhere deploy this yarn                                 will deploy it it will keep an eye on it                                 if it fails and restart it somewhere                                 else based on the policy and I just it's                                 it's your code to make your decisions                                 about what you're going to do how do you                                 run this well the thing is what we're                                 doing there is we are running a remote                                 application to cluster and what we have                                 to do is tell you on what to do this is                                 something if you end up staring at a                                 code you'll discover these call to                                 contain the launch context but the key                                 point is is that you you just build up a                                 command to run you say oh yeah here my                                 environment variables i want to set up                                 here is my bash command line and here                                 are some of ours i want you to download                                 and that's basically it's a Utah yonce                                 download these binaries on ptolem if                                 need be run this command with this                                 environment variable and it goes away                                 and running there's some extra things                                 that get passed down various environment                                 variables and stuff to let your running                                 application bind bind to yarn bind to                                 the file system pick up things like                                 Kerberos keys but generally generally                                 that's it so you can run arbitrary code                                 there it does not have to be Java ok the                                 yarn code we help you do that but you                                 can run other bits of code in there I've                                 done groovy ones are then scar the ones                                 which was same jvm but also people done                                 them and go as well actually so it's                                 really it is a shade                                 of arbitrary applications one of the                                 cute things that you know where your                                 binaries come from the answer is you                                 copy all the artifacts you actually want                                 to run into HDFS or any other file                                 system that her dude can grab here's an                                 example here where I'm basically saying                                 I want to download an age-based our ball                                 somewhere and I just say right you're                                 going to download my H based are but                                 we're going to pick it up actually from                                 an Amazon s                                                           coat and hat that you can handle and it                                 will quite happily do this and that's                                 worth knowing if you'll say playing with                                 yarn on an Amazon EMR cluster is you                                 keep all your binary somewhere just on                                 s                                                                       so I say here's a tarball I say it's an                                 archive which tells the node manager                                 when this thing gets installed on tyrant                                 unzip it whatever and also give the                                 relative path I say install this into                                 the lib HBase what that means is when my                                 container comes up it's going to be                                 untied and stuck in the relative path of                                 somewhere somewhere in local filesystem                                 note manager when it's it gets told to                                 launch your container it gets that list                                 of resources there pulls them all down                                 expands them copies and whatever and                                 then it does it it's into a bit of the                                 file system that gets deleted once your                                 container finishes and then execs your                                 bash command line at the base of that                                 path so now that I've installed HBase                                 into the page base if I set my bash                                 command to be lib HBase fresh HBase no                                 point                                                               exact age base and that's the secret is                                 you download your binary you set                                 environmental and you run your code that                                 is the core concept there's one little                                 surprised they're called class parts ok                                 I won't go into details except to say it                                 hurt I will go into details actually he                                 does have the bear it if you're running                                 java code you want to set the classpath                                 of what to do it makes sense to use the                                 dhoop and other binaries that are on the                                 system at the far end which you can't                                 necessarily predict it on your client                                 because it's somebody else's cluster so                                 there's an environment variable Y on                                 application class path that you should                                 be able to grab from your yarn settings                                 site settings to say here's my classpath                                 the default one says use the big-top                                 path but different installations behave                                 differently you build your classpath                                 from that if it's wrong your application                                 doesn't start with a relatively                                 meaningless error message so that class                                 not found or something like that so it's                                 it's actually one of the the bigger                                 sources of paint you've got that little                                 problem there's another one which is                                 that that class path tends to include                                 everything that had to decide you want                                 that comes down to the version of log                                   you on the version of Jackson you want                                 whatever that rose in there you get all                                 the transient craft on the classpath                                 which due to an original to break things                                 when we ship to deep last year loop                                     is pretty out of date Anna that means                                 you really ought to be aware of what                                 you're building against and kind of keep                                 the old code the good news is it's been                                 annoying me so much of a right man code                                 up you've been upgrading your jars as I                                 go along so that's where I've used my                                 power so a loop                                                        let's update everything and we've done                                 that as much as we can now we've reached                                 the impasse we said we are stuck with                                 the latest versions of the binaries we                                 can do that still run on java                                           to get rid of that too and generally                                 work the thing was scared of his google                                 guava that's incredibly brittle but                                 you're going to have to deal with that                                 for now I'm afraid I would like someone                                 and that's particularly someone in the                                 audience over there that's their phone                                 over there to go and add osgi support                                 for us instead so we could run John run                                 yarn apps in osgi container that will                                 make life a lot simpler I think                                 everybody agrees that it's just nobody's                                 sat down to put in the hours from free                                 anyway ignoring little detail your                                 application master comes up and running                                 and was it doing                                 it's like the jobtracker its aim in life                                 is now to manage the work doesn't do the                                 work so much is coordinated it says it                                 works out it decides what it has to do                                 based in your request or whatever talks                                 to yarn and says okay I need some                                 containers to do the actual work it can                                 specify the capacity amount of memory                                 and CPU those containers have it can say                                 where you want them to be it builds up                                 the binaries and execute it it's also                                 somewhere you can provide IPC and rest                                 api calls web you eyes and it has to                                 handle responsibility dealing with                                 failures a little bit of work on the                                 side so it it's like it is exactly your                                 version of the job tracker in Hadoop to                                 the jobtracker replacement is a yarn am                                 it has all these features and other                                 applications do exactly the same thing                                 we ask for containers all we do is say                                 your requirements terms of memory and                                 CPU and you also get the same way you                                 want them so MapReduce it says it looks                                 at where the data is first here's my                                 data sources looks at the files that                                 looks at where the blocks are which you                                 can ask age do first phone and says I                                 want to run my code in it here in the                                 project I've been working on something                                 called slider we do it completely                                 differently we just say I want something                                 a random we don't really care where it                                 is at first but then we try and remember                                 where it is so next time your                                 application comes up a claim baked in                                 the same place it was that's so we can                                 reuse the data and we just say yeah we                                 try and remember but it's best effort as                                 a little flag here saying relax locality                                 versus strict strict says it must be on                                 a specific container relax as I'd like                                 it if you're asked for strict and the                                 machine's not there you're not good                                 enough you're not going to get a                                 container if that machine's busy you're                                 not going to get it so generally relax                                 is the only option that makes sense now                                 one of these yours but you you are                                 solution you get back something that may                                 be what you wanted or maybe close one of                                 the things that we've been working on                                 we've got improve as a failure tracking                                 if something fails what do we do in that                                 world can we say when we get it back we                                 don't want it do we list everything else                                 say I want everything but these nodes                                 right it's an interesting area for for                                 more code at least in my project but it                                 if you look at the big MapReduce engines                                 the job tracker in that they have a nice                                 simple notion of blacklisting so this is                                 machine is too slow I'm not going to use                                 it I think where they were being a bit                                 more subtle about it as we're trying to                                 keep moving averages I mind just say a                                 box works or doesn't work we have a                                 notion of this box is a bit unreliable                                 and you don't want to use it but you                                 have no other choice you might as well                                 take it in a small cluster don't sit                                 there saying oh no these things are no                                 good you want to say well I hate it but                                 i'll use it anyway okay its application                                 master it asks yarn for containers what                                 you want maybe where you want it and                                 eventually eventually i get satisfied                                 you get some containers and if it comes                                 up to your code you that you get them                                 back and you run them all right exactly                                 that same setup as for application                                 master where you say here's my                                 environment here my binaries here's mine                                 here's my command line and you run them                                 again they start off in there if you tip                                 the switches you get see group isolation                                 here that's a subset of what things like                                 darker does it doesn't hide the oh the                                 OS and the file system but it does put                                 limits on process and a CPU and memory                                 consumption and the policy there is if                                 you are if you start using more memory                                 allowed your program gets killed cpu get                                 throttled and that's that's a nice way                                 of stopping your application going wild                                 in a cluster and that what that does is                                 then let you you rock and run your                                 programs in a cluster without the ops                                 team getting too unhappy you know yeah                                 you're not going to kill the things                                 memory we're not doing i/o throttling                                 yet and that's an interesting problem                                 because the i/o is actually going here                                 in ex gfs not the container so we've got                                 to come up some plan fitting in that but                                 otherwise your containers run rather to                                 be isolated there is also project                                 underway running the contain                                 in Dhaka so you will just say run these                                 docker images around the cluster that                                 gives you better isolation although it                                 complicated networking setup no that's                                 it you run your containers now what                                 happens if something fails and node goes                                 away that is something that is not                                 directly your problem it happens it                                 happens the larger the cluster is just                                 based on machine failures disk failures                                 are kind of proportional number of disks                                 you have and then there's the risk that                                 the code fails as well the most                                 unreliable piece of code in the Hadoop                                 cluster is likely to be your own                                 application right so sometimes your                                 containers crash they fail no matter                                 what happens where the entire machine                                 goes away or just your process exit yon                                 finds out about it your process exits                                 the node manager says that process don't                                 entire machine fails yawn says hang on                                 this thing here hasn't heart beated in                                 for a while so that let's assume is dead                                 again the RM said is dead either way                                 your application master gets told what                                 what happens and it chooses how to react                                 to it ain't for us the slider stuff we                                 just asked a replacement but that's                                 that's now policy-driven if you look at                                 things like the jobtracker they try and                                 add some extra things to say this bit of                                 date to were working on caused the                                 failure they also look at the machine as                                 well and do blacklisting but you could                                 imagine application and says if any                                 machine fails if any container fails                                 then I just died completely enroll back                                 and that may seem a stupid policy but                                 for some applications it actually makes                                 a lot of sense so some of the people                                 doing MPI over yon project called                                 hamster I believe they do that they                                 basically say I'm going to run a job                                 anything happens to it will stop and                                 restart and they rely on the fact that                                 actually if your job is fast enough or                                 short-lived enough you don't need to                                 bother with checkpointing and restart is                                 good failure mode and if you don't save                                 things to disk you can actually get by a                                 lot faster so they're cheating and                                 saying actually our failure policy is                                 start from scratch so don't don't                                 dismiss the simple policies darling I                                 recommend is don't forget                                 model-view-controller as your                                 architecture for an application this is                                 very important because most people                                 writing yarn applications start with an                                 example piece of code called distributed                                 shell in the new code base and whoever                                 wrote it forgot about Model View                                 controller so you cut and paste that                                 code you start running with it and your                                 code gets a bit messy and then you add a                                 bit more stuff like fairy handling and                                 it gets a bit messy and you end up with                                 a class here that's about eight thousand                                 lines long with all these various data                                 structures in and synchronized blocks                                 and you have no idea what's happening                                 then you spend a week maybe even                                         stripping it all out and putting in two                                 places unless you like doing that I'd                                 say start from beginning and come up                                 with a model of what you're doing and                                 that's in a yarn application that piece                                 of code becomes your model of the                                 cluster what's happening in the notes                                 what their failure rate is that kind of                                 thing and what you actually want to do                                 so for our code we basically take a                                 specification saying I want to run this                                 binary like age base on these machines                                 now specification we asked for it and                                 then we keep track of where things are                                 let me have some we have some other                                 stats there and what's going on run a                                 bit of knowledge what we're doing but                                 it's all isolated and that lets us do a                                 few things one of the best things is we                                 can now test it heavily and simulate                                 scale and failure handling without                                 actually putting you on a real cluster                                 so even though I have access to big                                 clusters I have to argue with people to                                 get that time here I can just say right                                 I'm going to simulate a                                             cluster with some mock code that                                 generates the requests and the failures                                 and tries to even simulate kind of                                 asynchronous calls into it just just to                                 stress the code to find those failures                                 before you go into production that's                                 important because when you get into the                                 big distri                                 system you're into the world of                                 distributed debugging and it is a lot                                 easier to find the things on your local                                 machine first so do that Model View                                 controller the other thing is is that                                 you can add api's on top for us we're                                 hooking into we have a rest api an optic                                 capi and some zookeeper stuff on the                                 side here is some chatting going on with                                 yarn itself resource manager and then                                 the old manager that's all handled for                                 us by classes that they're coming yawn                                 so we we just some class something that                                 handles all that conversation this extra                                 stuff we added on ourselves again I                                 think we got a bit too late to splitting                                 it up so this this application master                                 class is just over large and it's become                                 the piece of code was scared of the most                                 so when we added the rest stuff we at                                 least it did it slightly better and                                 stuck it on the side well then we are                                 doing now we've got a separate model is                                 we're trying to do something which is                                 very leading edge which is handle                                 failures of the application master                                 itself until now yan hopes that your                                 application keeps running if it fails                                 you're at the key if your application                                 master fails then all your containers                                 get destroyed your application gets                                 queued for restart and yarn keeps track                                 of the fact your code failed if it has                                 not failed more than the cluster policy                                 says phase loud it will get restarted                                 somewhere else eventually when the space                                 on the cluster and you have to start                                 from scratch again that's actually fine                                 for things like say a job tracker or                                 similar where you may as well start and                                 rebuild all your complicated state in                                 your application master from scratch                                 what we were doing we tryna have long                                 live services we actually wanted to keep                                 things running the point being at say                                 for example running HBase or storm if                                 our out master fails we don't want HBase                                 to go down we want the storm session to                                 keep running so there is a new feature                                 my colleagues put in where we can say                                 set a flag called set to keep containers                                 across application attempts                                 in short well at least it does say what                                 it does you know in his favor and in                                 fact Lee what happens is the containers                                 keep running when you're am gets                                 restarted it gets given a list back of                                 what containers come in of what                                 containers you ready had and you can get                                 told what containers failed while you                                 were down if anyone's going to implement                                 this young people the corn if you're                                 going to do that synchronize everything                                 because it turns out you actually end up                                 getting those farrier callbacks before                                 you finished processing the answer but                                 in fact the you get this and you've got                                 to try and rebuild your state and that                                 there's an interesting problem if you're                                 going to do that you've got to think                                 where do I keep my state that that's                                 persistent we keep some of the stuff in                                 HDFS we have the kind of the original                                 what it is we want and we keep a history                                 of where things off if you've got                                 anything else I'd say look at zookeeper                                 but of course you cannot keep this in                                 one though zookeeper ephemeral notes                                 because once your RM goes down all its                                 snake goes away so there's one other                                 hook we actually do here which is we we                                 use the single field in a yarn container                                 its priority for when you ain't allocate                                 it as our single index into what kind of                                 role a container has in the cluster so                                 have different roles here like a master                                 like a worker like a monitor like a                                 garbage collector we would give them                                 four different priorities one two three                                 four and all we have to do is in                                 numerate that cluster and see what they                                 are we've gone a bit beyond that now                                 we're actually the code inside has a bit                                 of minimal state so what we're going to                                 do is ask them where they think they are                                 whether i say running or not running and                                 if then if they're not running we just                                 destroy them we don't bother to worry                                 about why they're running or what song                                 but if they if they are running we just                                 leave them alone anyway it is it's an                                 interesting feature if you're running                                 long live code if you're not running                                 long-lived applications I would just say                                 don't go near this all right it's just                                 extra pain and suffering and the real                                 problem is rebuilding your staked on a                                 phone                                 that's important because you have enough                                 to do and the a extra thing you have to                                 do is actually testing um I big fan of                                 testing I like writing tests I think                                 everyone should write more tests I don't                                 like waiting for tests to finish that's                                 where you can spend a lot of my life is                                 actually spent waiting for tests to                                 finish these days so like I said before                                 we move on application state into unit                                 tests and that's really nice because                                 they finish in about five minutes where                                 it gets harder it's actually the real                                 production tests in this world you                                 actually want to simulate a yarn cluster                                 you want your programs to be downloaded                                 from HDFS you want to talk to HDFS you                                 want to exact things you want to get the                                 errors back and there are there's one                                 thing can help you here is something                                 called mini yarn cluster zilla class                                 that actually runs all the yarn cluster                                 inside your jvm process so you can                                 actually host the the Resource Manager                                 the node manager the real running in                                 process you can also bring up HDFS                                 alongside that with a mini HDFS cluster                                 which I would not recommend doing it                                 first because when your test runs HDFS                                 gets taken away and any interesting logs                                 and other data you've connected goes                                 away too so did just run locally I only                                 discovered recently because I hadn't                                 read the guide propping up the something                                 called an unmanaged application last as                                 well which actually runs your                                 application master in in your J unit                                 code I I've not played with that but I                                 think it sounds like it would have made                                 my life a lot easier as it is a lot of                                 our simple tests are in the mini on                                 cluster it's nice it works with simple                                 tests but as every every test class                                 starts that cluster up and tears it down                                 it makes your application slow and slow                                 and slow so nowadays what we're actually                                 doing for most of our work is we're                                 actually we've designed it all to run                                 functionally against real hooded                                 clusters starting at VMS so                                 we have a whole test suite which                                 actually we're build process works is we                                 build up by Nuri's we build our archive                                 a tarball we untie r it then our                                 functional test suite actually exacts                                 the binary script as you would real real                                 client applications point it at some                                 settings files that dictates the real                                 clusters locally i run the amp but we                                 can run it against production clusters                                 to whether they're things on ec                                      Rackspace or where they're actually real                                 physical clusters over ssh tunnels it's                                 notable here that I actually have three                                 VMs a red hat machine with helloo                                     ubuntu                                                             up-to-date branch to and kerberos                                 windows thing in the corner by go near                                 sometimes that's enough to pretty much                                 create most of the configuration                                 problems in grief you're going to                                 encounter and particularly Kerberos and                                 Hadoop security hands up who's got a                                 Kerberos enabled secure secure Hadoop                                 cluster okay keep your hands up if you                                 like it okay one person the back it does                                 actually make sense and i would                                 recommend everybody stop being scared of                                 Kerberos and learn to understand it all                                 right it's just painful but well it                                 actually does make sense in some way but                                 it a crazed problem take great problems                                 of long-lived services it creates extra                                 work in your test and it creates lots of                                 interesting obscure messages a good run                                 being updated my cluster last week with                                 apt-get update and everything stopped                                 working and it turned out that there was                                 a new Java                                                         installed by Ubuntu which had not                                 included the latest US enabled                                 encryption mechanism so I can handle                                 long secure keys which is then causing                                 the client to fail with some error                                 message like couldn't talk to the server                                 you know things like that so that's why                                 I'd recommend you start playing with                                 this stuff sooner rather than later is                                 because you want that pain before it                                 ships anyway so testing testing is one                                 of the areas where we reading to a lot                                 more work I think it will be good if                                 someone and it might actually be a                                 he sits down and writes about a test                                 framework for this stuff I also think                                 actually testing large-scale distribute                                 systems is probably harder than actually                                 writing them in the first place it's                                 rare these to write an application it                                 runs across                                                         harder to show it work that's apparently                                 what testing is doing is trying to show                                 your code worked across a big cluster                                 and given that it doesn't for the first                                 few months of its life or whatever                                 trying to get the logs back and trying                                 to understand why it failed now and well                                 you know we're still in the dark ages                                 we're still using log statements you                                 know and that's basically printf for a                                 thousand machines so testing is fun if                                 you really want to work on it come and                                 find me and that's not just in                                 Hortonworks but if you're writing yarn                                 clusters bike to test frameworks and                                 share them so key point trying to avoid                                 doing as much of the work yourself okay                                 application masters are complicated and                                 the best way to avoid doing them is to                                 let somebody else do all the heavy                                 lifting there are various people                                 projects working on things like this and                                 working on this slide of stuff to run                                 existing apps colleagues are doing tez                                 there's a pipeline thing give read the                                 Microsoft dryad paper you'll understand                                 what they're doing there and there are                                 there are other things going along on                                 top Apache twill is going to be spoken                                 about next and I'm going to give a quick                                 demo of it here so twill is probably the                                 simplest way to run a yarn application                                 where it takes a normal was pretty much                                 looks like a normal Java runnable and                                 runs it elsewhere so this is me                                 launching it a yarn application master                                 is the client code I basically say                                 create some                                                              of my render class and something locally                                 to catch their logs and then just run it                                 and that runs that runs in the cluster                                 so this one mounted demo I've got a                                 little frame render app which will take                                 some an image file and some parameters                                 and it will render render a frame the                                 lies about as it scales well it's the                                 opposite of MapReduce because it goes                                 from a small file it goes normal map                                 reduces big input small output this goes                                 to small input massive output you can be                                 generating gigabytes a second if you've                                 got a big busy cluster and so it changes                                 the whole notion of where you want to                                 place things                                 and you know and Germany is your                                 scheduling and failure mode the nice                                 thing is you can restart anything that                                 fails you want to place things so that                                 time consecutive frames are close to                                 each other so that if your next step is                                 actually merging frames into a video                                 everything is reasonably local and this                                 is my demo the code is online and this                                 is where we actually have a piece of                                 code this is the runner I'm going to                                 find a string here show is not a rig                                 demo I want somebody in the audience to                                 come up with a sentence you come up with                                 a phrase to say developers sleepless                                 okay everybody saw that was what my                                 person I given this quote to earlier                                 said that it was sleep less okay right                                 let's bring up my terminal window this                                 is me running a mini yarn plus the test                                 here so it's starting up the cluster in                                 processes running on the local machine                                 if I bring a separate window I can do                                 see what Java proceeds are running and                                 it will tell me what tell me what's                                 happening GPS to launch is running okay                                 so that that's that's my application                                 master running there okay or yeah so                                 that's running my code this is still                                 busy running away this is all log junk                                 that comes out of the yarn application                                 master and don't managers it is all                                 generally meaningless until you're                                 trying to find out why your code doesn't                                 work and then you will start end up                                 learning to understand this stuff                                 they're just just an observation okay                                 now here we go developers sleepless see                                 so that was me that was me that is a                                 yarn application running there locally                                 to open a JPEG render some text over it                                 and then save it again to and from a                                 file system Viet local or do and that                                 was all it took so even though I've been                                 scaring people with all this stuff about                                 how it's hard and painful the rest of it                                 the point is you can                                 my applications that run on the yarn                                 cluster that was me rendering one frame                                 locally but if I extend that a bit to                                 take things like different texts                                 different frames I could run that over                                                                                                          right that's what the render looks like                                 actually I run I've got a little                                 runnable I got a context to get on my                                 application arguments on the command                                 line passed in I just create my little                                 render a code here run it and save it                                 and that's it it's a runnable it's not                                 doing anything complicated at all so                                 there you go yawn we now we can now let                                 you run whatever you want to inside the                                 cluster it hides a lot of the details                                 over the LAN port layering below but it                                 still takes work ok you have to start                                 handling policies of basement and fair                                 you're handling you have to deal with                                 building up those command lines and                                 executing it so my main recommendation                                 is find someone else to do the work and                                 that's the one fact that's the secret of                                 software engineering journal is find a                                 volunteer and those people that put                                 their hands up they are the volunteers                                 especially overs talking on twill next                                 basically someone you should take you                                 listening to because really what you                                 want to think about is what algorithms                                 you're going to run not how to integrate                                 with yarn but what is my code going to                                 do what is the high level stuff what do                                 I want to do to process data to generate                                 data to do useful things rather than                                 what do I need to do to integrate the                                 system and that that's what you have to                                 go home and do now and we're very short                                 amount of time for questions five                                 minutes                                 you see with slider you can run                                 arbitrary code has anyone tried running                                 are something like tom cat right                                 question there was with slide ecolgico                                 um that is the goal of slider win that                                 you run arbitrary code ok all right what                                 real friendly doing that is we've got a                                 new package format that you put things                                 in there with various metadata saying                                 here are the parameters you have to do                                 to help build up and execute the script                                 we've got some Python scripts through                                 launching and some templates and things                                 like that yeah people we have played a                                 tomcat we're doing HBase accumulo and                                 storm first and we're not actually                                 incubated project which I would                                 encourage you to get involved in if you                                 can well I think about Tom cotton that                                 is the goal is to let you run existing                                 up the caging as HDFS with saying you                                 know the nice idea of application is                                 saying Tom cut talking to hbase with my                                 client applications talking to tom cat                                 we've got to do a lot dynamic binding                                 there actually you don't know where                                 Tomcats going to be what ports is going                                 to be in front on that so one of the                                 things I mean a lot to work on is                                 service registries and getting                                 configurations so whilst we deploy                                 Tomcat we have to provide binding                                 information so that Tomcat clients can                                 work out what's going on so I mean also                                 serve as registry stuff I'm going to be                                 implementing a chunk of that in yon                                 later on this summer actually yarn                                     if you like so as registries get                                 involved ok another question over there                                 Stefan Stefan is one of the people of                                 volunteered he's doing stratosphere he                                 will gladly field your support calls                                 Stefan I'm how fast would you say Ken                                 yarn in its current architecture get to                                 you know bring up an application master                                 given that the application muscle starts                                 really faster locates a handful of                                 containers ok of course can it get well                                 what what delayed this yarn add on                                 whatever delay your application has ok                                 question is what is the startup delay                                 well your application starts okay                                 there's the overhead to download the                                 binaries which is why we're doing some                                 caching thing and then there is the                                 problem of just telling the load manager                                 what it is to have to run no manage is                                 reporting the heartbeat                                 that's when they get given the work yarn                                 can start the most up if it allocates as                                 many containers in one go and then when                                 and that that startup delay that                                 heartbeat can can slow things down the                                 problem being the bigger the cluster the                                 longer that heartbeat has to be in a                                 small class you can just a report in                                 faster as you get bigger you have to                                 make them slow but good news is also                                 that no managers report in whenever they                                 finish work so you've got a cluster                                 that's busy doing analytics applications                                 where containers finish rapidly then no                                 managers report in more which generates                                 more space a bigger issue is in a big                                 cluster that's busy you're not going to                                 get space for the containers especially                                 if you start asking for lots of memory                                 and lots of CPU you're not going to get                                 compute time because other people are                                 using it silence your question okay okay                                 so so yarn is pushing out work in                                 response two heartbeats it's pretty much                                 doing it like a dupe use to do right                                 yeah so Lisa in contrast in systems like                                 like spark and Status you try an eagerly                                 push out work from the from the Masters                                 to the workers which which just get down                                 the deployment latency yeah if you                                 actually get the trend one of the things                                 that we're going to mold away from                                 classic MapReduce is to hold                                 session-based things so storm and tears                                 in that they bring up a set of                                 containers that hang around for a while                                 and I stopped you having this workflow                                 saying bring up a lot of containers tear                                 them down req stuff bring them up again                                 so it's just if you can if your                                 application master can bring up a pool                                 of containers and hang onto them you can                                 then do a lot of productive work and                                 push work out to them however you want                                 zookeeper being a good example you can                                 just push things in the zookeeper saying                                 do this and as the information gets                                 picked up by watches the client nodes in                                 the watches they can pick things up and                                 actually think that's how to older stuff                                 actually they use zookeeper a lot there                                 okay                                 just how does have this young compared                                 with measles as a good question how does                                 young compared measles I'm not entirely                                 sure I haven't played enough with me                                 sauce I know measles come out saying we                                 are purely an execution framework yarn                                 is an evolution of more than my produce                                 wealth so I know yarn is very good at                                 running short to medium their services                                 and applications and announces jobs and                                 it has that integral notion of data or                                 where placement you want to run your                                 code near the data where we're your only                                 means it's a good of that but measles is                                 better at long-lived applications which                                 is where yarn is weaker so we're pushing                                 yarn I'm kind of leading in some of the                                 work and saying let's make yarn better                                 at hosting long-lived applications I                                 think it means it's going to the other                                 way of saying okay let's let's handle                                 short-lived applications better okay                                 more Christians then thank you very much
YouTube URL: https://www.youtube.com/watch?v=zQUGlgBfXzE


