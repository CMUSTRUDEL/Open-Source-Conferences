Title: Berlin Buzzwords 2014: Stephan Ewen - Big Data looks tiny from Stratosphere #bbuzz
Publication date: 2014-05-28
Playlist: Berlin Buzzwords 2014 #bbuzz
Description: 
	Stratosphere (http://stratosphere.eu) is a next-generation Apache licensed platform for Big Data Analysis. Stratosphere combines the flexibility and scalability of MapReduce-like systems with a high-performance runtime and automatic optimization technology inspired by MPP databases. 

Stratosphere offers fluent APIs in Java and Scala that extend the MapReduce model with arbitrarily long programs and more operators such as join, cogroup, cross, and iterate. Stratosphere’s runtime uses main memory efficiently, and gradually degrades to disk with good performance under memory pressure. Stratosphere’ cost-based optimizer automatically picks the best execution strategy for programs taking into account data and hardware characteristics. 

Finally, Stratosphere features end-to-end first class support for iterative programs, achieving similar performance to Giraph while still being a general (not graph-specific) system. Stratosphere is compatible with the Hadoop ecosystem, runs on top of YARN, and can use HDFS for data storage. Stratosphere is developed by a growing developer community, and is currently witnessing its first commercial installations and use cases.

Read more:
https://2014.berlinbuzzwords.de/session/big-data-looks-tiny-stratosphere

About Stephan Ewen:
https://2014.berlinbuzzwords.de/user/209/event/1 

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              let's get started the first talk in the                               afternoon session here I'm going to talk                               about stratosphere or if link I'm going                               to explain the name duality in a second                               I'm Stefan and I'm i hope i can tell you                               for him to sting things in the next                               minutes okay so yeah as I said I am I'm                               I'm Stefan I am currently a yeah PhD                               student at the hue from Berlin actually                                University of Technology last days so                                one of the guys was basically waiting                                for the professor's to find a a day                                where they can actually hold the defense                                can take longer than you might think I'm                                one of the people that has been involved                                with a stratosphere project I'm about                                which I'm going to talk from the from                                the very beginning I'm someone oughtta                                of the core developers okay and as you                                may have seen on the on the title slide                                actually it the the talk is announced as                                stratosphere on the title slide I wrote                                stratosphere / flank so by the time                                actually we we handed in the talk this                                was still the project the project                                stratosphere which started as a project                                here in dublin area in the mean time                                between handing in the torque and this                                event happening stratosphere is actually                                been accepted at the Apache Incubator so                                it's moving into the Apache Incubator                                it's actually taken on a new name                                because there's already a project called                                Apache Stratos so we um we voted on a                                new name and the new name is going to be                                flink so I'm going to say stratosphere                                for the remainder of the talk because                                this really describes the outcome of                                stratosphere which is going to be the                                starting point of the flink project but                                yeah you can think of stratosphere and                                flink being synonyms for the sake of                                this talk and um courtesy of Alan                                Freedman is the first draft of the of                                the mascot of of the flink project so                                for those of you are not German speakers                                flink is actually I it's a German word                                and means agile nimble so that's why we                                have the are the squirrel sa as a mascot                                ok so much for the introduction                                so whenever I say stratosphere you can                                just in your head think of link as well                                okay so what is what is status fear all                                about it's um it's a fairly new open                                source project as such so a few of you                                may have heard about it a few of you may                                not to sum it up in a few words it's a                                it's an analytics platform so it's in                                the in the space of systems like Hadoop                                and apache spark it is it is not built                                on top of a depend on top of spark so                                it's it's it's own system it's it's own                                stack it it runs very nicely on top of                                HDFS and yarn so it integrates very well                                with the with the Hadoop stack and it                                has its it has its focus on on a variety                                of use cases and especially on on ease                                of programming providing high level                                abstractions for um for programmers I'm                                to give you a little idea where this                                project is standing it's it started                                quite a while ago I say as a research                                project here in the berlin area shared                                between different universities um when                                the e when the research project was over                                we thought that the result was actually                                kind of nice and we got very good                                feedback on it so we we kind of pushed                                it to become an open source project so                                that's where it is right now it's um it                                is at this moment the the codebase that                                you can get is still hosted on github it                                Sam it's an open source project there it                                is as I said moving to the apache                                software foundation under the names                                linked we are in the in the fifth                                release so three threes is actually                                coming coming out right about now                                there's the second release candidate is                                I think online since the weekend and we                                begin pushing it into the open source                                about half a year ago and it has                                actually gotten quite a bit of adoption                                or adoption right now so we have a total                                of thirty-eight contributors already not                                all of them from Berlin actually so we                                also have people working on on projects                                related to a stratosphere in various                                places and yeah we also we're seeing                                 first companies                                 trying it out for four applications okay                                 so much as the background so what is                                 what is stratosphere really to you as                                 somebody who wants to try it out so what                                 is greatest view and why would actually                                 the ecosystem need something else then I                                 do Ben spark and there's so many other                                 systems drill and so when we started                                 stratosphere we thought they're kind of                                 these two spaces there's the MapReduce                                 style space with projects of course like                                 Hadoop MapReduce and apps Park getting                                 more more traction also fitting in my                                 opinion more in dead space and there's                                 the this the area of of databases                                 initially mostly relational databases                                 but also databases that go beyond that                                 like Apache drill and there is this                                 spectrum between those um between those                                 different types of technologies they                                 they each have their very individual                                 strengths so the the MapReduce space has                                 focused heavily on scalability on                                 user-defined functions as first-class                                 citizens are very complex data types all                                 of that the database area has focused I                                 since the                                                            very much on declare activity so you                                 you've right a very concise query you                                 delegate a lot of of the decisions to                                 the system how how should it be executed                                 it comes with this optimizer component                                 it has for what it does a fairly                                 efficient runtime and we kind of thought                                 there's there's a nice sweet spot                                 between those two systems where you can                                 combine some of the characteristics and                                 this is where stratosphere actually sits                                 it sits between the met reduced our                                 technologies and the database                                 technologies and it adds also it's very                                 own twist to it so we've added                                 functionality especially for iterative                                 algorithms which are very important if                                 you are looking into applications like                                 machine learning graph analysis I'm it                                 is fairly complex data flow programs and                                 it's adding it's adding um the a bit of                                 that decorative Attilan magic sauce that                                 is known from databases for                                 non-relational kind of programs                                 so this is what it what it looks like if                                 you look at it from                                                   it's a yeah it's a stack it sits on top                                 of a of various storage and unclassified                                 management solutions the storage being                                 serviced feed self does not store data                                 so it it reads files structured files                                 unstructured files it draws data from                                 databases in various formats or local                                 files HDFS cloud file systems and it can                                 be deployed on two clusters using our                                 frameworks like yarn or you can directly                                 install it on the bare metal and on top                                 of that sit sit status with runtime                                 optimizer and a series of api's and I                                 think this picture kind of tells quite a                                 bit about the system especially the is                                 very prominent component here which is                                 which is very much inspired by the                                 architecture of relational databases so                                 so this view has a has a common runtime                                 and a common optimizer for multiple                                 programming IP is their record oriented                                 programming api's in Java and Scala                                 graph oriented ap is a scripting                                 language for for Jason language all of                                 these go through the same optimized at                                 the same runtime and there you can some                                 of them you can actually even mix and                                 match in a very nice way so what what                                 what makes stratosphere kind of unique                                 in that in that space of Big Data                                 technology there I'll talk about the                                 individual appoints later in more detail                                 but I think you can sum it up in and                                 basically for four broad categories so                                 very strong focuses on easy to use                                 developer API sin different languages as                                 I said javis Carla graphs already there                                 there's Python and sequel under                                 development I think actually de python                                 api is in beta status right now with                                 these easy-to-use develop api's kind of                                 handed hand comes automatic optimization                                 so it is something that comes more from                                 the space of the relational databases um                                 what what strategy does it it it                                 implements an optimized that it is an                                 inspired although not the same as an                                 optimizer in relational databases so you                                 can write programs and                                 not worry about many of the individual                                 low-level decisions that you need to                                 need to make when you write something                                 for example in Hadoop burn spark and                                 just delegate it to the system and say                                 okay figure that out for me um it comes                                 with its own runtimes it's not sitting                                 on my produce a run time that is running                                 computation in memory as far as possible                                 it's going out of core when necessary it                                 that is fairly unique as well I think                                 streams data between operations even                                 though it has a batch API on top of it                                 so that might sound a little weird                                 initially but it does actually make                                 sense for a lot of operations because it                                 implies that if you have multiple steps                                 after another you can actually produce                                 pretty big intermediate results which                                 you'll never have to materialize in                                 store and yeah I mentioned that earlier                                 there's there's a very deep support for                                 iterative algorithms um they're very                                 deeply embedded both in the api's and in                                 the runtime and then I'm going to show                                 you a few examples later of or that                                 actually means to the program around our                                 track securing iterative algorithms okay                                 so from the high level view of the                                 system what is it you look like if you                                 actually write your first example                                 program um so here's the use the in                                 famous word count which is kinda there                                 you know it's kind of the hello world I                                 think of the of the parallel analytic                                 space so this is what it looks like and                                 so this use Java API it's the EAP is                                 both in Java and Scala are centered                                 around data sets that you create and on                                 which you apply arm yeah transformations                                 functions like mapping over them                                 grouping aggregating and so on you right                                 you right your these transformations                                 just as Java functions and unlike unlike                                 a dupe you can really use you can use                                 just plain java objects the basic types                                 you can also use your own your own                                 classes and so on there's a this kind of                                 a mixture of of analysis of the types                                 reflection analysis and general purpose                                 utilization going on to make                                 at work okay so this is where the this                                 is for the Java API I think this gives                                 you a rough impression of what it looks                                 like yeah data sets your types tuples                                 are kind of built in as a special                                 concept in the hadoop world everything                                 is around key value pairs but we thought                                 for more um more complicated program                                 this actually gets a little this gets a                                 little ugly after a while and for those                                 of you who are scholar program is                                 they'll actually noticed our concept                                 it's um is fairly powerful so in Scala                                 of course everything looks even a little                                 more beautiful if if you're a fan of                                 Scala this is the same program um it's                                 not one hundred percent the same but                                 roughly the same program written in                                 Scala so it's the same thing you start                                 with uh with the data set created from a                                 text file should form it here with an                                 operation that splits the lines in this                                 case you just group by the entire the                                 entire record so this is what this                                 lambda he expresses given that you get a                                 word which is group on the entire world                                 and then then count where it occurs so                                 far so good that should look for those                                 of you who have looked into frameworks                                 like let's say spark or crunch or so                                 somewhat familiar um the the whole api's                                 are designed to round a rich set of                                 operators so the runtime if you wish                                 knows knows a few operators MapReduce                                 join core group Union cross iterating                                 iterate delta i think the the first six                                 should be you should recognize probably                                 from if you work with cascading or so                                 before or actually spark iterate an                                 internet delta i might seem a little                                 special and i'm going to talk about them                                 in a bit this is what kind of the course                                 the system knows in the api's there's a                                 lot of derived operators as well                                 filtering flat mapping project                                 aggregating duplicate elimination server                                 forms of joints and also derived                                 operators which are kind of compositions                                 of other operators for example arm                                 vertex centric graph computations which                                 are a combination of iterations and co                                 group and                                 joints so what what does the system do                                 internally once you write such a program                                 so it's not breaking it down into a                                 series of MapReduce jobs it's not just                                 saying okay let me execute one step                                 materialize the result may execute the                                 other step the next step what it's                                 really doing is it's um it's taking the                                 entire program constructing a data flow                                 graph out of it so this operation                                 streaming data from here and streaming                                 it into the reduced operation to reduce                                 operation gathering it for hashing                                 sorting and streaming it into the join                                 and so this is this is really a pipeline                                 a dataflow with multiple operators                                 possibly executing at the same time that                                 is online if you execute the program and                                 it has the ability to also take take                                 data from from a later step and in a                                 controlled fashion feed it back to an                                 earlier stage there by closing a loop                                 and enabling to do a charity for                                 recursive computations okay um so much                                 as a as it as a teaser from from that                                 level let's actually walk through some                                 cases where um where the where what we                                 added to the system in terms of                                 optimization and um and ease of use                                 actually becomes a little more a little                                 more graspable so um here's a variant                                 that that that runs there a program                                 roughly sketched by this but this yeah                                 but this abstract algebra tree here it's                                 a it's a two joints between a large and                                 a medium sized table with a small table                                 and an aggregation afterwards something                                 that that does occur as I don't know                                 possibly intermediate steps and                                 algorithm or pre-processing steps so in                                 what you would do in stratosphere is you                                 would you'd write the program starting                                 to read the data in this case from csv                                 files which which would parse the data                                 into into some topple data structure and                                 then they would do two joints between                                 the large in the medium table the syntax                                 here if you have toppled data you can                                 just express okay which fields of the                                 able to use if you have don't have                                 toppled data you can throw in the lambda                                 that says okay here's how you get                                 actually the key from the data type so                                 you join these two tables a runner and                                 one day I grew by an aggregation over it                                 in the end and then you submit it to the                                 system and what does the system do with                                 it there for those of you have actually                                 spent some time with with Hadoop in hive                                 they might know that they're especially                                 for joints are there are several ways to                                 execute them so in i think in hive they                                 call them reduce that join or maps i                                 join they call them sort join and hash                                 join and so on and you can build a lot                                 of combinations to execute these to                                 execute these different joints with                                 different algorithms some of which are                                 good in one situation and some of which                                 are good in another situation so all in                                 all if you take this program and you                                 tell the programmer okay you need to                                 figure out now what exactly is the way                                 to execute the first joint and the                                 second join they're going to they're                                 going to be quite a few combinations and                                 actually figuring that out is not it's                                 not unusual because it really makes                                 difference in the execution times in                                 orders of magnitudes so below here is a                                 list of the strategies that status your                                 nose internally so partition join which                                 is kind of close to the reducer join                                 versus replicated joins which is close                                 to the maps are join sorting and hashing                                 algorithms underneath and trying to I'm                                 trying to come up with a good way there                                 is something that um that the system can                                 can hear do for you so your submit is                                 dropped to the optimizer and what the                                 optimizer does is it actually looks at                                 the it looks at the sizes of the of the                                 data that go into the operations it                                 makes an estimate how this operation and                                 behaves how it changes the data what the                                 data is Isis's that go into the success                                 of operations it's going to look                                 holistically on the plan i'm going to                                 see okay if I do this here can I                                 probably possibly reuse something here                                 so can I collapse these operations into                                 one operation can i maybe do something                                 here that can reuse later at that point                                 so it's going to take this yeah holistic                                 look at the plan and trying to optimize                                 it so here in example one way                                 that you could do it is say okay take                                 the first join here and make this a make                                 this a partition Tash join which is I                                 don't think it's something that you can                                 actually do in my produce it's closer to                                 the reducer joy and then do the maps are                                 join but it's not really it the second                                 join could be a what does it do                                 broadcast has shown here and that gives                                 you a very interesting very interesting                                 setup because it allows you if you if                                 you look here the grouping field that                                 use use it this position as kind of this                                 is the same field that you use you for                                 joining so given that you did something                                 like partitioning on this field here                                 broadcasting on the other side and for                                 this join you can simply reuse it so it                                 means that if that service we can come                                 up with a way of executing this program                                 that that this aggregation which is                                 normally a reducer comes basically for                                 free so you can you pay you pay                                         you pay a map search on here and you get                                 the successive aggregation kind of for                                 free this is how a damn stitches                                 together the execution plans so this is                                 a this is kind of the value you get out                                 of an optimizer I'm being added between                                 the api's at the runtime okay now that                                 you with that we've written a program                                 that we've looked at okay what can                                 actually happen in terms of executor of                                 optimization how do we execute it and                                 they're they're different ways of doing                                 that so I guess from the from the Hadoop                                 space the the natural thing would be                                 yeah package you can package it into a                                 jar file copy it onto your cluster and                                 invoke the invoke the script that                                 executes the execute the program so that                                 actually you can do the same thing here                                 something that's actually even more                                 comfortable in em and my opinion is                                 saying okay I'm just defining the                                 program to work on a remote environment                                 so I'm creating an environment that                                 points at the cluster where it's                                 supposed to be executed I'm going to                                 invoke it and it will be the program                                 will be serialized and thrown onto the                                 cluster where an RPC call and if you                                 just you know if you're writing the                                 program and you're not absolutely sure                                 you've gotten it right the first time or                                 want to play around a little bit you can                                 also just define a local environment and                                 execute it from from anywhere in your                                 IDE you can actually do this embed it                                 with another program so you have a                                 regular Java program you write something                                 then you're better part of a                                 stratosphere program say ok execute this                                 locally embedded in this JVM and then                                 you continue with the with the other                                 parts of the program so depending on how                                 you configure it will run single                                 threaded or multi-threaded all right um                                 let me show you two slides about the                                 runtime I personally can talk about the                                 runtime probably for two hours along                                 because this is my favorite part but um                                 let me try and give you just a just a                                 quick overview of what it looks like so                                 from from a high level the runtime I                                 said is fear and Hadoop don't look don't                                 look too different you have you have a                                 master to witches up my job's the master                                 does Resource Management scheduling and                                 you have the yep the task managers which                                 do the actual work ideally you co-locate                                 them with data nodes of the distributed                                 file system to get local reads but you                                 don't have to and um yeah the the master                                 pushes out work to the workers the                                 workers communicate with each other to                                 exchange intermediate results in in the                                 in the details this works a little                                 different than then it does for example                                 in in Hadoop worlds and spark as I said                                 earlier the the work is to actually do                                 that actually do a streaming exchange of                                 data and also all operations are written                                 such that they they allocate a lot of                                 memory when they start up there trying                                 to fill it up as much as possible once                                 they hit once they hit the level that                                 they've fused it up the operations are                                 written such that they gradually move                                 parts of the computation to disk so this                                 is also something that is if you wish                                 more inherited from the database systems                                 and the way these algorithms are                                 implemented most of them have a                                 characteristic that they have a very                                 gradual way of going to disk so if you                                 get this one record that actually makes                                 you go beyond the capacity of your                                 memory it's not all of a sudden that                                 everything goes to disk but the first                                 parts go to disk only                                 okay and there's um there's another                                 another mass spectra that makes it a bit                                 different than most of the systems that                                 are around there so we've placed kind of                                 an emphasis on on robustness here which                                 means that we don't we don't gather                                 let's say if you have an intermediate                                 result you gather data for sorting or                                 for a hash table or you just you want to                                 cache the data yeah in memory we don't                                 gather the the object that you work on                                 even though you write that you're                                 working on objects but what you actually                                 do is we gather binary representations                                 of the of the data that is somewhat                                 similar and the way that the Hadoop does                                 it which gathers this utilize data                                 although we do it a little differently                                 so status view has a chunk of memory                                 pages which are if you wish just by the                                 race of                                                                  the data but it um it does so in a                                 fairly transparent way so if you if you                                 use a class for example a temple we know                                 very specifically actually how the data                                 looks like how the data is laid out once                                 you create a binary representation what                                 this allows you to do is it allows you                                 to write certain algorithms of the                                 runtime to to work directly on this                                 binary data so you don't have to UM to                                 go to the arm to the binary data turn it                                 back into an object to whatever                                 operation you want on the object for                                 example comparing them if you want to                                 sort them but a lot of the arm of these                                 operations can actually happen directly                                 on the on the binary data and that goes                                 beyond let's say key value pairs or so                                 also for for more complicated structures                                 that you get from longer tuples or                                 nested objects                                 alright um let me tell you a little bit                                 okay I know I'm actually moving kind of                                 fast through it so if you have questions                                 and I'd be happy to spend some time in                                 the end actually going also back to the                                 individual sections and answering                                 questions there okay um let me spend a                                 few minutes on iterative algorithms                                 because that is that is something we're                                 where we added were added quite a bit of                                 functionality also in a way that I think                                 is quite different from what most                                 systems aren't there do so a turret of                                 algorithms are kind of an interesting                                 class of algorithms they were very                                 important for um for a lot of lot of                                 applications for a lot of algorithms                                 from the fields of clustering                                 optimization graph graph analysis graph                                 processing what they what they really do                                 is they are they do multiple passes over                                 the data most of the time these                                 algorithms start out with with a set of                                 parameters that is somewhat randomly                                 initialized or words the it's the state                                 of let's say the previous day or so and                                 what you then do is you go multiple                                 times over your over your data to                                 analyze and each time you refine the                                 parameters a little bit you go over them                                 the next time we find them a little bit                                 more until you've kind of reach your                                 convergence date and then you've you've                                 trained your classifier you have found                                 your clustering so the the way that is                                 that this is most often done this by                                 implementing the implementing that                                 functionality that makes one pass over                                 their data and really just invoking it a                                 lot of times which seems like the                                 natural thing to do so how do in Hadoop                                 you would say okay I have my data here                                 in the distributed file system I call                                 this the step function which would does                                 Switched is one computation of this                                 algorithm the result is again going to                                 the distributed file system so I'm going                                 to do that again from distributed file                                 system to distributed file system on and                                 on and on it works but you pay kind of a                                 kind of an high overhead for always                                 doing the full amount of work so the                                 Apache spark project is this in a in a                                 way that's a bit cleverer                                 by saying okay let's just do the it's                                 just do the computation from round to                                 round so we've read from this the first                                 time and ever always afterwards we just                                 read from Ram um this gives you also the                                 ability that if you have something that                                 doesn't change between iterations you                                 can specifically say okay pin me that                                 into the main memory and yeah reuse it                                 stratosphere does it a bit different um                                 what what stratosphere does is once you                                 bring up an iterative algorithm you                                 really bring up one instance of this                                 data flow that is the computation so if                                 the if the if the computation consists                                 your of a map reduced Joe and join then                                 instead of having it once and then                                 another time afterwards in another time                                 afterwards you really bring it up once                                 and just um close a a data flow edge                                 back from the last operation to the                                 first operation so in that sense it's                                 not it's not a loop that you kind of                                 roll out it's really a closed loop if                                 you wish and that is it has it has the                                 advantage that that you can actually                                 share part of the computation across                                 multiple steps so assume that that there                                 is some let's say some auxiliary data                                 set that you want to initialize inside                                 this operator once are once you once you                                 start the computation this can be lets                                 say a library with another classifier                                 that you you know you load and to bring                                 it up and then you invoke this function                                 instead of doing this every time while                                 you unroll that you can you really do                                 this once and the data just pipes                                 through the operator multiple times it                                 doesn't do they also in an uncontrolled                                 fashion so there's a clear                                 synchronization of super steps between                                 that but this is really you know just                                 lightweight coordination between the                                 operators there's no it's really                                 deployed into the cluster and                                 initialized only once um another thing                                 again coming back to this to this                                 optimizer that that stratosphere does is                                 it dumb it also can it also looks at                                 this at this loop the way you specify it                                 and figures out okay where's waste data                                 loop and very                                 so what happens let's say once across                                 all loops what can I cash in order to                                 say okay let me get back at this result                                 in every in every time I feedback the                                 data again instead of recomputing it so                                 it's it's placing such caches                                 automatically it's it's looking at the                                 data flow and trying to push some part                                 of the computation out of the loop if                                 possible so to do it before the entire                                 loop starts and um yeah if it has                                 actually first-class support for                                 maintaining state across iterations                                 which it can them where it can actually                                 use a an index to access it so this is a                                 screenshot actually from the from the                                 optimizers visualization tool where you                                 can have a look at at the way status for                                 execute programs okay and what that                                 allows us to do if we have a loop that                                 is so tightly integrated into the                                 runtime you can actually mix and match                                 certain paradigms so here is an example                                 program that that starts it starts with                                 an execution environment it creates two                                 data sets here for vertex studies for                                 edges it runs a regular maps tab                                 function is a pre-processing step and                                 then it invokes a vertex and Rick                                 iteration so you can actually embed                                 different paradigms even in in the                                 programs you start record oriented to                                 switch to vertex oriented you can if you                                 wish switch back to record oriented                                 later switch back to vertex oriented for                                 another time if you want so um this                                 year's something that is for those of                                 you who know appetitive Ravitz and                                 interface it's very similar to it it's                                 um you describe a graph computation by                                 by looking at the graph from the                                 perspective of a vertex and saying okay                                 I'm receiving some messages from my                                 neighbors and I'm sending messages to my                                 neighbors so this is this is really that                                 thing just embedded into into an or                                 encapsulated in an operator that you can                                 embed in this in this API and the                                 ability true to maintain state across                                 multiple steps of iteration is actually                                 something that is really powerful if you                                 use it so for many of these machine                                 learning algorithms you have a you have                                 this characteristic that not all of                                 these parameters that you that you                                 refine in terms of the computation                                 actually needs as much computation in                                 some parameters                                 converge very fast and some of them                                 converge very slow so it it is a good                                 thing to say okay once the parameters                                 converged I'm actually keeping it out of                                 the loop so I'm doing the loop only on                                 on whatever parameters are still                                 changing and because I can really keep                                 them let's say in the operator put it to                                 the side you can you can keep them                                 around without computing on them and                                 this if you implement this correctly                                 than the this can get your runtime down                                 arm by very much okay let me let me skip                                 over those slides and just give you a                                 rough road map of where the system is                                 going and if i have time actually a two                                 minute demo in the end so what we're                                 what i've shown you now is going to the                                 end of the stratosphere project so where                                 the flink project is going right now is                                 yeah of course it's moving to apache so                                 we're releasing the latest pre Apache                                 version of                                                            release candidates if you want so one of                                 the next things we're going to add this                                 mid query fault tolerance that is                                 something we didn't take from the                                 research project into the open source                                 yet support for interactive queries and                                 cross query cashing in our design those                                 two actually very closely related so um                                 this is a common effort and we've                                 started looking into tears or Tess is a                                 is a runtime that is developed by mainly                                 by the hive community I would say it's                                 the it's the new runtime under the under                                 the latest version of hive but it is                                 actually a general data for runtime so                                 we actually seeing how stratosphere is                                 let's say the api's and the optimizer                                 can be integrated with with tests and                                 adding it as an alternative execution                                 engine to our on to our own execution                                 engine because the good thing is heart                                 works is putting a lot into tears and                                 making sure it runs on ten thousand                                 notes that is honestly beyond what we                                 have done yet the                                                    makes sense to to say if we we have we                                 have strong higher-level layers and and                                 find kind of an orthogonal match on the                                 lower on the lower layers um the this                                 probably add                                 I'm sure if there's a talk about that                                 for those of you from Berlin the last                                 recommender get together on thursday                                 actually introduced the new Mahad                                 scholar dsl for linear algebra                                 operations they're running it currently                                 on spark what we're doing is we're also                                 adding operations underneath and trying                                 to integrate my out with stratosphere so                                 that you can that you can actually run                                 the linear algebra computations from                                 Iran stratosphere and then there's                                 there's a group in in Budapest that's                                 working on streaming so I mentioned that                                 the runtime the lower levels actually                                 have streaming like computation so what                                 we're trying to do is actually surface                                 them in them in a high level in a higher                                 level API some some stole my                                 capabilities the nice thing about that                                 is once you actually have this in the                                 same flow system you can kind of build a                                 lambda architecture kind of thing                                 without wiring together let's say how to                                 open store we can really build it in one                                 end to end integrated system where you                                 get you get time safe and efficient                                 interchange of data types and everything                                 yeah and at last thing they want to                                 mention on the roadmap is that we're                                 we're trying to actually improve the the                                 upper layer if api's to go more to to a                                 logical way of specifying queries so                                 what what that can give you is is shown                                 here there's only a prototype for that                                 it's not part of the release right now                                 um this is it's the same same word count                                 example instead of working on tablets it                                 works on a very simple class defined                                 here with two fields a word in a count                                 field and instead of writing functions                                 that that that really access these                                 fields or modify these fields you can                                 you can simply drop in the names of the                                 fields you say group by the word                                 aggregate or sum up the count field so                                 in order to ease the interchange at the                                 interplay between arm between objects                                 and empty API ok and with that I'd like                                 to conclude the talk our motorways big                                 data looks tiny from stratosphere we're                                 still looking for a good a good slogan                                 for Flinx of anybody's any suggestions                                 please drop us a note on the mailing                                 list you can find a lot of the resources                                 that                                 that show how to get started that give                                 you an idea of what the what the api's                                 looked like and so on at the status                                 field at you or on github so is your                                 stratosphere and yeah if you're                                 interested in release announcement and                                 use and so on follow us on twitter                                 thanks so we have a few minutes for                                 questions or a demo i'll actually leave                                 that up to you whatever you prefer so it                                 seems questions huh okay um hello I end                                 adventure of questions mostly about                                 comparison with spark but you mostly                                 answered the then unfortunately I only                                 learned about the stratosphere like few                                 weeks ago where I spark was a we are                                 working with it one year ago so we have                                 no expense do you consider stratosphere                                 was freeing to be a prediction you label                                 or two to consider it what is it                                 prediction ready is that comparing the                                 two spark because i vant leader both                                 projects are very similar there with                                 video streaming with stratosphere to the                                 most striking difference advantage I                                 would see for us atmosphere in the the                                 the way to degrade to disc we spark                                 sometimes you are out of memory it's                                 much harder to tune sometimes worse but                                 sometimes not yeah okay so there was a                                 bunch of questions I think the main the                                 main point was the comparison with with                                 spark what are the main advantages is a                                 test production ready and so on so the                                 projects have been going on similarly                                 long i think our open source effort                                 started quite a bit later only half a                                 year ago is it production ready I think                                 you'd have to you have to try it out at                                 the the part that we've shown here we've                                 actually we've tested them there are                                 some people that are using it I think                                 none of them has deployed it into                                 production pipeline most of that is the                                 say let's say clusters for you know data                                 exploration and so on that it does work                                 that much I can say the the streaming                                 parts are there currently under                                 development so everything what I'm                                 saying right now is about the batch                                 parts the stuff that I talked here not                                 about the roadmap part so I would say                                 it's in the shape that you can try it                                 out definitely also try it on you should                                 be able to to really do something with                                 it not spend debugging your entire                                 afternoon the the most striking                                 difference is if you would compare to                                 spark are yes so there's this the and                                 optimizes and we're putting a lot of                                 effort on making the high level API                                 simpler the optimizer also the in the                                 way the road map shows that we try to do                                 the the high level yeah I maybe I sauna                                 in a more logical and physical way the                                 way that you do iterations in a stateful                                 manners for you you can actually run the                                 same operators for multiple for multiple                                 loops you can stratosphere bundles a lot                                 of of operators into the same jvm so you                                 can share data structures across there                                 so if you have let's say libraries or                                 cache data or so that you share across                                 operators because you run multiple of                                 them together that can give you                                 definitely an advantage here and yeah                                 the the system has been designed from                                 the start would be actually a system                                 that can go nicely out of course so                                 except for one operator which is the                                 which is the iteration state index which                                 is an operator that they were just I                                 would say one of the really of the most                                 advanced features and they except for                                 that one all of them are actually                                 architected in a in a fairly clean way                                 of going to disk so except for that one                                 I don't think any of them precious if                                 you run out of memory and also without                                 joining so that should work if you find                                 it otherwise please let us know and                                 we'll fix it yeah thank thank you for                                 the dancers a a bit more first park it's                                 very easy to test with the shell the                                 command line interface and I guess you                                 talked about the stratosphere we'll have                                 that in the near future right yes so the                                 something like the like the calm                                 like an interactive way of doing the way                                 go back to the to the roadmap and an                                 interactive way of calm of writing                                 programs is on the way I mean still you                                 can already fairly easily use it armed                                 with this with the local environments                                 and just executing that executing them                                 locally so let me see if we can actually                                 see that here so if you see a program                                 here in and start this video can you can                                 just generate data sets by saying okay                                 um just say okay environment create a                                 data set from some elements that I throw                                 in I just give a local collection it's                                 going to move that collection into the                                 cluster and uses the data set you can                                 you can say just okay whatever print the                                 result here or gather it back in in a                                 collection output format and so on and                                 directly test against it and then                                 execute it locally so all of that you                                 can just you can can hit run and then                                 locally test it and then you can say                                 okay okay let's change the environment                                 point it to the cluster and move it into                                 the cluster instead so run it locally                                 and then pointed to some other place and                                 run it in the cluster so it's it is true                                 there are some cases where the                                 interactive shell makes it easier it's                                 going to come but i think is already                                 fairly easy to use to be honest for for                                 quite a few algorithms that question on                                 about how you handle the the the fourth                                 tolerance how do you work with photo in                                 in spark you have at least called                                 concept of our DD you have something                                 similar or is it something else ok let's                                 go to this slide here or maybe even two                                 to that one ok so as I said for                                 tolerance is actually coming up the                                 university project had for tolerance                                 which we decided not to move into the                                 open source because we weren't confident                                 enough that it was robust so we can                                 reworking this and adding it to the open                                 source and the way that the way that it                                 works is if you wish it's not so                                 different from our duties because the                                 data flows that that stratosphere                                 constructs kind of                                 between between the data flow in a                                 lineage it's kind of the same thing they                                 are the only difference in the between                                 the systems is that spark would execute                                 let's say these two together then that                                 one then that one and then that one                                 where a stratosphere may actually                                 execute them at the same time so if one                                 of those fails you can you can do the                                 same trick you can backtrack through the                                 graph and say okay where's the latest                                 point where I actually materialized my                                  results the difference here is that once                                  a later operator that you let's say                                  stream the data to has already been                                  started you might actually have to tell                                  that operator key reset and start from                                  the start from the beginning the later                                  operators because they may have consumed                                  some data that is that is invalid then                                  once you restart predecessors or so                                  that's really the I'd say the only big                                  difference other than that the two                                  concepts kind of kind of the same okay                                  we have one last question here mmm I you                                  mentioned that the optimizer try decides                                  what kind of joints to use in what order                                  to run sometimes based on the size of                                  the data yeah is there some trick that                                  you use this to find out what the size                                  of the data is beforehand yeah good                                  question there actually is so if if you                                  would look at this program the the input                                  formats that that that read the data                                  here in this case it's the CSV input                                  formats input formats being very similar                                  to the Hadoop input formats they have                                  actually the ability to say okay give me                                  some statistics over the data and the                                  default input formats implement                                  typically something something fairly                                  lightweight you know connecting to the                                  to the data node and                                                  name node summing up sizes of the files                                  gathering a lightweight sample of let's                                  say                                                                      average length of the line so so um to                                  figure out how many how many rows are                                  actually going to be in that file                                  roughly so that's what it that's what it                                  does to to get an idea of what they the                                  characteristics of these initial data                                  sets are once you actually go into into                                  a later operator there are some there's                                  some like and databases from some some                                  assumptions how these operators behave                                  that give you a                                  your rough estimate of what the data                                  says afterwards is so these can actually                                  be off they they're often off by quite a                                  bit but in many cases that's not totally                                  bad because they have to give you like a                                  rough book or ballpark it's going to be                                  a really huge one is it going to be very                                  small one and so on so as long as that                                  works that is fine what what we actually                                  doing and the are going to do in the                                  future in order to improve this is once                                  you have this let's say the interactive                                  mode of running queries the optimizer                                  can actually submit the quiz also                                  interactively so what you can do is you                                  can submit the first join and have a                                  look at the first data it produces                                  before you actually say okay I really am                                  executing the second row and also that                                  gives you a moment to reconsider before                                  before executing later parts of the join                                  and also you can say join and say system                                  please decide for me which way to go but                                  if you say okay I'm actually very I'm                                  very confident and know how to do this                                  correctly and I'd rather not take                                  chances with the system then just say                                  join with large or join with tiny or                                  join symmetric or so and you will tell                                  it exactly how to do it and give how                                  much I think we have to give the                                  audience a chance to go do our talks                                  Thank You chef on all right
YouTube URL: https://www.youtube.com/watch?v=HZRJ7l8hh3U


