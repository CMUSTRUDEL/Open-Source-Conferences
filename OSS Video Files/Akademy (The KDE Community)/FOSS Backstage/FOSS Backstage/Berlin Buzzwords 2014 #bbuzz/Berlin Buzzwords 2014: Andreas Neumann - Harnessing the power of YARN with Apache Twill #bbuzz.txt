Title: Berlin Buzzwords 2014: Andreas Neumann - Harnessing the power of YARN with Apache Twill #bbuzz
Publication date: 2014-05-28
Playlist: Berlin Buzzwords 2014 #bbuzz
Description: 
	When Apache Hadoop was first introduced to the Open Source, it was focused on implementing Google's Map/Reduce, a framework for batch processing of very large files in a distributed system. Built for running on large cluster of commodity hardware, Hadoop also included a cluster resource manager to divide the capacity of the cluster between the various Map/Reduce jobs that can run at a given time.

A Hadoop cluster, however, is not always fully utilized, and idle resources would best be used for other compute-intensive tasks like real-time stream processing, message passing, or graph algorithms. Unfortunately, the cluster resource manager was specialized in Map/Reduce execution and did not allow other types of workloads.  

Read more:
https://2014.berlinbuzzwords.de/session/harnessing-power-yarn-apache-twill

About Andreas Neumann:
https://2014.berlinbuzzwords.de/user/279/event/1

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              welcome to my talk it's gonna be about                               yarn and how to make your uneasier who                               knows what yarn is who was here an hour                               ago yeah I know it                               okay so yarn is part of the current                               release of Hadoop and I do this to                               elephant                               it can handle tons of data it's it does                               all the heavy lifting for you when you                                have to do massive data processing the                                distributed application in Hadoop                                classically looks like this                                you recognize this threat this is a                                MapReduce job you have a bunch of                                mappers each mapper reads the split from                                the filesystem spits out some tuples                                there's a shuffle phase in between them                                sorting and then there's a bunch of                                reducers and each of them spits out data                                writes them to filesystem into heart                                files this is a very powerful pattern we                                can do lots of different types of data                                analysis but we cannot do everything                                however hadoo gives us an infrastructure                                that allows to run many different kinds                                of MapReduce jobs in a single cluster so                                here I have four different MapReduce                                jobs running Hadoop has a scheduler that                                allows me to place my computations close                                to the data lots of useful features but                                if I look closely here in this picture I                                see that a lot of the nodes on my                                cluster are actually not used because                                not all the time do I have enough data                                to analyze sometimes I have peak amounts                                of data and I need all the notes in my                                cluster sometimes I don't so what could                                I do with these these grey little boxes                                here that are right now just sitting in                                the data center consuming power and                                question is well maybe I have a data                                scientist in my in my lab and and he                                says I I'd like to write a message                                passing application that's also this                                tubular application I have here I have                                six processors they all talk to each                                other they all interact with DES                                somehow locally and they need to run in                                a cluster I might have a stream                                processing app right I get events they                                come in in real time I have processors                                they consume these events in real time                                they may write some data to a data base                                and may read some data but again it's a                                distributed application it can run in a                                cluster and maybe I could run that in my                                Hadoop cluster and if I have no                                important applications to run maybe I                                could do some testing because for                                example if I have some web service I                                want to do some load testing I could                                just run a test on many nodes on my                                cluster when I have spare capacity so I                                actually know lots of ways to use the                                spare capacity in my cluster and if I                                could do that                                then my cluster looks more like this                                there's still some gray notes but it                                looks much more diverse and I can do all                                these different things in a single                                infrastructure so this would be ideal                                right I'd love to do that now let me                                quickly explain why I would love to do                                that my name is andreas Norman I worked                                for continuity and what we have built is                                a product that's a developer centric big                                data application platform and it pretty                                much runs any type of processing that                                you have to do in the Hadoop cluster it                                runs real-time stream processing it runs                                batch analytics like MapReduce it runs                                we run tests there we run web services                                there and when we built this platform we                                were kind of desperate say it's about                                two years ago two and a half years ago                                and we couldn't find an easy way to run                                all these different things in a single                                cluster and the answer that we found was                                yarn so that was about the time that                                Hadoop version                                                        one major advantage over Hadoop version                                one in Hadoop version one there was a                                job tracker and the job tracker was                                responsible for managing the resources                                of the cluster and for driving the                                execution of                                produced jobs so the programming                                paradigm and the resource management                                were very tightly coupled and that was                                 the reason why it was very hard to run                                 anything other than MapReduce and I had                                 to gesture with Hadoop to oh was new                                 resource manager yarn and that separates                                 these two tasks there's resource                                 management and there's the programming                                 and it allows to run pretty much                                 anything in your heart'll cluster and                                 how this works now is so here I have an                                 application that I could not run in                                 Hadoop previously and in yarn I will now                                 have a resource manager that runs on the                                 side and the only thing that I have to                                 do is I have to write an application                                 master this is one new process that I                                 have to run it negotiates resources with                                 the resource manager it acquires                                 containers to run tasks in and then it                                 drives the execution of the application                                 in those containers so now the logic of                                 how the application is executed is in                                 the application master and that's my own                                 code if I want to so I have all the                                 power and with yarn my cluster now looks                                 like this I have the resource manager on                                 the side and all of my applications have                                 their own little application master okay                                 so far all tier good so let's look a                                 little bit closer how yarn works so in                                 yarn I have my cluster every node of the                                 cluster runs a node manager and I have                                 the yarn resource manager which sits                                 there as the central point of control                                 and here I have a yarn client the yarn                                 client wants to start an application in                                 the cluster so the first thing that                                 happens is the yarn client needs to                                 submit the application master to the                                 resource manager which means it needs to                                 bundle up a jar and some configuration                                 and tell the resource manager I want to                                 start an application and here's the                                 master what the resource manager does                                 next is                                 find a free container in the cluster and                                 it talks to the node manager of the note                                 where that container is located and it                                 tells that note manager to start the                                 application master so now the                                 application master is alive it's running                                 and it can now start talking to the                                 resource manager and ask it for more                                 containers so now it could acquire say                                 three containers and once it has these                                 containers it then talks to the node                                 managers in the cluster to start its own                                 tasks in those containers that it                                 received from the resource manager so                                 this is the interaction and these steps                                 number three and number four they can                                 repeat so the application master can                                 dynamically ask for more resources can                                 give up resources and resources in this                                 sense are always containers in the                                 cluster with a given capacity in terms                                 of memory or virtual cores no this looks                                 fairly simple so I want to dive a little                                 bit deeper so let's just look at this                                 first step submitting the application                                 master what does that mean so we have a                                 we have a Java a jar file with some Java                                 code that's the application master and                                 we want to run that in one of the nodes                                 of the cluster now the client has this                                 jar file on this local file system if we                                 just tell the resource manager find the                                 container and run this jar in this                                 container is going to run somewhere here                                 and the jar is not going to be on its                                 local file system it's not it's not                                 gonna work it cannot access my local                                 file system that might be my laptop                                 right so what needs to happen is first                                 thing the client needs to copy this jar                                 file to the distributed file system then                                 it submits its request to the to the                                 resource manager and the resource                                 manager and node manager make sure that                                 this jar file gets copied to the local                                 file system off the machine that hosts                                 that container now the node manager can                                 start the application master which can                                 now load it from it                                 local file system so this interaction is                                 slightly more complex than you would                                 think at first and if we really list all                                 the things that the young client has to                                 do just to start the application master                                 its these eight steps I could go through                                 each of these steps and I actually have                                 roughly a page of code for each of these                                 steps not gonna show you all of this                                 because I mean I'm gonna show it but I'm                                 not going to talk to it there's some                                 interesting things here that you see                                 that in this code I have to set up the                                 class path for for that container I also                                 have to set up a command this is a shell                                 command that runs it runs a JVM and I'm                                 actually I'm in charge of making sure                                 that standard out and standard error are                                 captured somewhere properly and if I if                                 I built this command in the wrong way                                 then nothing is gonna work and I'm gonna                                 get a very unexpected behavior so it's                                 quite complex and there's a lot of                                 things you can do wrong and we if we                                 look at all this this was only the first                                 step right this was only submitting the                                 application master the same kind of code                                 is required again in the application                                 master when it wants to start containers                                 for the individual tasks right so we                                 have a duplication of this code it                                 happens once in the application master                                 and once in the aren't lined and so so                                 we end up writing quite a lot of Euler                                 plates code so yarn is great nothing                                 against that right but it is quite                                 complex in order to write an application                                 you need to learn three different                                 protocols and each of them is                                 complicated especially the protocol                                 between the application master and the                                 resource manager is it's an asynchronous                                 protocol and lots of interesting race                                 conditions can happen there what you get                                 is really full power you get full                                 control over all the different knobs                                 that you can twist and turn in Hadoop                                 but it's actually at the expense of                                 simplicity it's the the learning curve                                 is very very steep and I don't know how                                 many here have actually written a yarn                                 application once okay how many of you oh                                 wow I know Steve has written many of                                 them how about you did you like it yeah                                 so I I had a lot of fun when I first did                                 this and my first little application was                                 over                                                                   didn't do anything all it did was it was                                 logging a single line so um so at                                 continuity when we built our product we                                 found ourselves reimplemented that                                 boilerplate code over and over again for                                 all the different things we were doing                                 in yarn and we very quickly realized                                 that that there must be a better way it                                 must be there must be an easier way to                                 do this because there are common                                 patterns that we find again and again                                 and so we we looked at a class of yarn                                 applications or distributed applications                                 and we found the similarity to                                 multi-threaded applications many                                 distributed applications consists of                                 processes that run on different nodes in                                 the system but they don't actually need                                 to talk to each other a lot of times                                 each one runs autonomously like a Java                                 like a Java thread right and if I would                                 program this in in Java as a                                 multi-threaded application I would have                                 utilities in Java from from the                                 concurrent package I'd have executors                                 and things that manage my threats for me                                 so could we do something similar for                                 yarn and and the answer is yes the                                 answer is is Apache twill so this                                 started Azzam is an internal project                                 inside of continuity and we started                                 talking to some people about it and                                 there was there was a very high interest                                 because everybody who had written yarn                                 applications saw the need for this this                                 simplification and the programming model                                 we have is indeed just like Java threads                                 you define random                                 and then you run them in the cluster                                 instead of running them in in in thread                                 pools it was incubated about half a year                                 ago we have had two releases since then                                 the third one is in the making and the                                 community is growing so let me show you                                 a small example so this is an                                 application that will run a single                                 container or a single runnable in a                                 cluster and all that it's going to do is                                 it's going to log a message hello world                                 so in order to define the application                                 all I need to do is is define this                                 runnable so all I need to do and then I                                 need to start the application for that I                                 create a yarn twill runner service which                                 connects to the yawn resource manager                                 and it can then start the application                                 for me it's that simple a similar                                 application using raw yarn api's would                                 be Steve how many lines of code                                 hundreds five hundredths I don't know so                                 so this is the simplicity in the power                                 of twelve and it's easy now what's the                                 architecture here the idea is that in                                 your application you define twill run a                                 bolts and that's the only interface that                                 you need to define your application and                                 it's very similar to Java threads and                                 then there's at will run our service the                                 anything that's green here is part of                                 the twill framework once you submit your                                 jobs to the twill Runner that will                                 runner will start a generic application                                 master which knows how to negotiate with                                 the resource manager and then all of the                                 tasks that you've defined all the runner                                 bolts they will run in containers and                                 they'll be wrapped into at will we call                                 them twelve task runners or twill I'm                                 not exactly sure                                 and so you really don't need to worry                                 about how they are started all right all                                 that boilerplate code is in the green                                 areas in this diagram and the only                                 protocol that you need to learn now is                                 this API between the                                                                                                                           okay so the first example we have had                                 only a single runnable what if I need an                                 application that has more than one type                                 of task say I have a producer and a                                 consumer well I can do that of course in                                 swill so I can use a slightly more more                                 verbose Builder pattern to define my                                 application and now in this example it's                                 a crawler and an indexer and so I define                                 these two runner bolts add them to my                                 application build it and then I can                                 submit it just like I could submit that                                 simple hello world that before so a lot                                 of the complexity is is taken away and                                 that in itself is is very helpful I                                 think but in addition to that and and                                 Steve mentioned that in his talk there                                 are certain common patterns that many                                 distributed applications implement and                                 in addition to the simplicity that tool                                 provides it also provides some of these                                 patterns out of the box and you can just                                 reuse that in your application so one of                                 those is logging we saw in this example                                 that typically a Hadoop application logs                                 to some files and those files there on                                 the local file system of the notes in                                 the Hadoop cluster right and when the                                 job is done the the resource manager or                                 the node manager will copy them to HDFS                                 and then they're available for you and                                 so on but if you write say a real-time                                 event stream processing engine that's                                 never going to end it just keeps running                                 and your your job will never terminate                                 and so your logs will never be available                                 in a central place and plus if it's a                                 real-time application you would like to                                 have insights about the behavior of your                                 application in real time all right you                                 want your logs in real time and that's                                 not that easy to do                                 but fortunately there's a great                                 technology called Kafka so what we did                                 in drill is when you start a twill                                 application the application master will                                 actually run an embedded instance of                                 Kafka and all the tasks all the runner                                 bolts you have when they omit locks we                                 inject a specific log appender that                                 sends these log messages to Kafka and                                 now from your tool client you can                                 retrieve those log messages in real time                                 very nice and the good thing here again                                 is you don't need to know how Kafka                                 works all you need to know is how this                                 what's the twill API to do this right                                 and really the only thing you have to do                                 is you have to add a log Handler and log                                 Handler api's is just a very simple                                 callback api on error on warning on                                 debug and there you can just reuse an                                 existing handler that we have or you can                                 just write your own another feature we                                 call it the the resource report when you                                 start an application in the cluster it's                                 gonna run somewhere there but you don't                                 really know anything about it you know                                 maybe you know it has ten nodes maybe                                 you don't what                                 twill does is it gives you this                                 information as a resource report you can                                 talk to it using rest and it'll tell you                                 about the current state of the                                 application how much memory is it using                                 how many virtual course is it using how                                 many instances are there how many                                 instances are actually life and are                                 still sending heartbeats what are the                                 hosts where it's running and so on this                                 is then registered this rest endpoint is                                 registered as the tracking URL in yarn                                 so when you go to the yarn UI you can                                 just click through and you get that                                 information                                 but there's also a way to get that                                 information programmatically from your                                 client and we will see later how that                                 can be very useful okay so now it will                                 allows me to run an application let's                                 say my application runs for ten hours so                                 in the meantime I call it a day I go                                 home close my laptop I lose the                                 connection so now at home I open my                                 laptop again and I'm not connected                                 anymore right so what can I do I want to                                 I want to connect back to that                                 application right and of course that's a                                 that's a pattern that everybody wants                                 and it's will implements that using                                 zookeeper so again this application                                 master here it records the state                                 especially where it's running in                                 zookeeper and when you start a new twill                                 client it can use that information in                                 zookeeper to reconnect to a running                                 application and can then get a resource                                 report or to other things                                 lifecycle management and the API is                                 again our very simple command messages                                 another very common pattern I'm running                                 say I'm running tender scene indexers                                 and at some point I want to take a                                 snapshot I want them all to to flush at                                 the same time int well you can do that                                 by sending commands to every runnable                                 and again this happens through zookeeper                                 that's well client again just uses an                                 API of the tool runner but that will                                 then this arrow goes in the wrong                                 direction by the way that that will then                                 put that message into zookeeper all the                                 tasks because they are wrapped into at                                 will kind of a twill wrapper are                                 actually listening and watching those                                 zookeeper notes and when a command                                 appears there they all have a call back                                 and and then that command gets executed                                 so in the runnable I just have to                                 implement the method to handle a command                                 and the command is just a simple                                 simple string basically okay elastics                                 the scaling how often do I run an                                 application and I realize it doesn't                                 have enough capacity I want to add five                                 notes or maybe I'm using too much                                 capacity I want to remove for with twill                                 you can change that with a single call                                 so if I started with                                                   started with five instances I have a                                 simple API using the controller to                                 change the instances to ten internally                                 this is also implemented as a command                                 message but you don't need to know that                                 next interesting topic is service                                 discovery Steve already talked about                                 that if you say you've run a web service                                 in your cluster but because it runs in                                 yarn you don't know where it's running                                 you know it's in one of these                                           I have ten instances of my Tomcat but                                 you don't know exactly where to connect                                 to so you need a way to discover that                                 and int will you can do that and again                                 using zookeeper every task can register                                 itself and that adds this information to                                 zookeeper what's the host where it's                                 running and what's the port that it's                                 that it's listening on and then through                                 the troll client I can find out that                                 information all right and again very                                 simple API in that we'll we'll run a                                 bowl I have an initialized method and                                 that gets at will context that context                                 gives me access to zookeeper and there                                 for example I can announce my service                                 and then on the client side I can just                                 use the controller to discover that                                 service simplicity okay one problem that                                 we often have is I have an existing jar                                 existing Java code and it has                                 dependencies on some version of a                                 library and happens to conflict with a                                 newer version or an older version of                                 that same library that Hadoop uses                                 and now I'm I'm screwed I can't do                                 anything unless I find a way to avoid                                 loading the Hadoop one and one way of                                 doing that is to building it to build a                                 bundle jar that contains all the                                 dependencies of the application and then                                 so for example here I could have a main                                 class may be a record class and then I                                 need an explicit version of guava                                                                                                        points because they broke backward                                 compatibility in a recent version and                                 now this will be inside of my jar and I                                 can submit that to toot will there's                                 standard ways of building bundle jars is                                 actually an OS GI pattern and I can then                                 execute that int will we have done this                                 - to run presto inside of yarn presto is                                 a sequel engine that was built at                                 Facebook it's similar to hive or Impala                                 and we just wanted to know whether it's                                 possible to run something like this in                                 int will and it would have been                                 straightforward if not for the version                                 conflicts so this feature was explicitly                                 added for running existing applications                                 over which we don't have control                                 we can't change their dependency                                 versions okay last but not least                                 distributed debugging have you ever run                                 a distributed application and it crashes                                 and you don't know why never and all                                 right now you add lots of logging to the                                 application and it doesn't crash anymore                                 race confusion race condition is gone                                 who knows so um wouldn't it be nice if I                                 could just go into my IDE and say                                 connect the debugger to this application                                 so this runnable of that type and that's                                 what we did with                                                        application you can start it with                                 debugging enabled and when you do that                                 you actually say these are the runner                                 bolts for which I won                                 enable debugging what twill does is it                                 starts the JVM with the option to have                                 the debugging port open right there's a                                 standard Java protocol to do that by                                 default Java won't do that and so what                                 you have to do is you have to find the                                 free port on the machine and then start                                 the JVM with that option it's kind of                                 tricky to do that because the JVM that                                 is open for debugging does not know                                 about its own port Java is kind of weird                                 in that way right so um if you want to                                 implement that yourself it's going to                                 take you a couple days to get to make it                                 work so int will this is all this little                                 hacking has been done and we already saw                                 earlier there's a way to get a resource                                 report which informs you about where do                                 all my run doubles run how many are                                 there and so on right so through the                                 same report resource report you can also                                 find out about the debugging parts of                                 each of the containers all right and                                 then you just attach your IDE to it and                                 you can debug one note here is some                                 twill is as secure as the Hadoop cluster                                 that you run in so if you have Kerberos                                 enabled Twitter's totally fine with that                                 the moment you do this you lose security                                 because Java has no way no way to secure                                 that debugging port so don't do it in                                 production don't do it when you don't                                 know that the environment is safe okay                                 there's quite a few more features but I                                 want to come to an end there's also                                 quite a few features that we still have                                 to build so right now                                 twill has a nice API that you can use                                 but you have to program against it in                                 Java                                 just for usability we just want to build                                 lots and lots of command-line tools for                                 example finding out the debugging part                                 of that runnable that you started right                                 now you have to write a little Java                                 program                                 and printed would be nice to have a                                 little command-line tool so um yeah as a                                 reminder it's an open source project if                                 you want to contribute these are really                                 nice little things that you can start                                 with then there's some nice distributed                                 application patterns I wanna say like                                 distributed coordination let's say you                                 want to do leader election let's say you                                 want to do some synchronization barrier                                 this these are things that many                                 applications need and it's our goal to                                 add those things to twill there's                                 actually the the curator the Apache                                 curator that implements some of these                                 recipes we might include that through                                 curator we might we don't know yet how                                 to get it in anyway that's on the                                 roadmap we want to be able to run on                                 Java applications lots of data                                 scientists love to write Python code you                                 can't do that right now with                                           will assume we want to enhance the way                                 that you can do lifecycle management of                                 your application right now you can start                                 it you can send commands to it and you                                 can stop it but and you can wait for it                                 but it would be nice if you could pause                                 it and then resume it it would be nice                                 if you could collect metrics in the same                                 way that you can collect block messages                                 now that would give you much better                                 insights into the performance                                 characteristics of the application where                                 are the hot spots so I have a                                 distribution skew or not you need                                 metrics for that and here's one killer                                 feature that that's currently being                                 built is a local Runner service suppose                                 you could run any distributed                                 application on your laptop in memory and                                 threads and then you could just debug                                 you could just develop and test on your                                 laptop and you never need to deal with a                                 cluster until you really go and want to                                 try that large-scale because if you look                                 at the API sets we'll has there's not a                                 single yarn or Hadoop dependency in                                 all the api's are independent of yarn                                 then there's implementations of those                                 api's and one of them is the yarn                                 implementation but we're currently                                 working on a multi-threaded                                 implementation and that would be really                                 nice yeah and lots and lots of other                                 things to come                                 suggestions are welcome summary yarn is                                 powerful it allows you to run arbitrary                                 applications in a Hadoop cluster but                                 you're on this complex kind of difficult                                 to learn protocols a lot of boilerplate                                 code steep learning curve                                 twill makes yarn easy the programming                                 model is similar to Java threads and                                 everybody knows how to do that and so                                 what you get as a sum of all these is                                 you get a productivity boost you get all                                 the power of yarn with the simplicity of                                 Java threads you're going to develop                                 distributed applications in two hours                                 three hours instead of two months last                                 thing to mention twill is open source                                 it's in the Apache Incubator any open                                 source project can only live if it's                                 community lives so we need contributors                                 we need committers we need people who                                 want to volunteer to do the hard work I                                 want on tiered who else is volunteering                                 Steve is oh yeah you are you've you've                                 been volunteered so it's lots of fun to                                 work on these things so go to the                                 websites go to the mailing list maybe                                 you find something interesting and with                                 that I think we have about five minutes                                 for questions thank you                                 hey you talked about metrics I assumed                                 you meant the yarn metrics right like                                 metrics metrics yes                                 what about application metrics that                                 that's what I was okay so yarn gives you                                 some metrics but that is very useful                                 because you're on doesn't know what your                                 application is - exactly that's one                                 thing I'm right now that this is                                 probably not implemented yet what's your                                 recommendation to do that just have have                                 the application just talk to some                                 graphite server or something or what you                                 so so there's implementation and there's                                 ap is right so if you look at how we do                                 logging there's simply a simple way to                                 emit logs right so um I would think it's                                 not even here so it's it's simply a an                                 SL f                                                                 logging right for metrics I would think                                 there there is an open-source metrics                                 library I think it's just called metrics                                 and it's all dapper Dapper thing and                                 those api's are actually powerful enough                                 and what I see is that we'll we'll have                                 an implementation of those api's that in                                 the background maybe uses Kafka or maybe                                 uses something else to to collect those                                 metrics sounds good yeah thanks                                 hi in in Steve sake men should this the                                 spring XD I'm just wondering how easy it                                 would be to put Porter spring app into                                                                                                       yes that's a good question I haven't I                                 can't give you any on that I I feel that                                 spring is definitely more mature and and                                 more powerful at this time also much                                 more complex programming in XML no                                 questions I don't oh there's someone he                                 waited for you to come all the way to                                 the front to make you walk actually I'm                                 not a young expert but as far as I                                 understood that my produce                                 implementation then that runs on top of                                 yarn is also a yarn client right so                                 there is a young young client for                                 MapReduce yes there there is a an                                 application master the MapReduce                                 application master and there is a hadith                                 client that allows you to submit                                 MapReduce jobs so that comes built in                                 with Hadoop so do you know whether it                                 will make sense to port it to twill and                                 how much effort it will be and if you                                 have enough features and will right now                                 that you fully implement it I would say                                 in theory it's possible but MapReduce is                                 such an elaborate framework and so much                                 work has gone into how MapReduce                                 actually drives execution there is lots                                 of subtleties about in what I mean how                                 soon can you start with users there                                 speculative execution all these things                                 it's certainly possible int will but I                                 wouldn't say it's natural                                 any wallets thank you very much thank                                 you
YouTube URL: https://www.youtube.com/watch?v=IyjRVzKPhUo


