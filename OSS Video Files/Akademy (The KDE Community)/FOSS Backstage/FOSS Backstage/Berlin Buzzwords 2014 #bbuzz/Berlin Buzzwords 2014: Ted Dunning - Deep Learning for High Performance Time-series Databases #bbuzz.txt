Title: Berlin Buzzwords 2014: Ted Dunning - Deep Learning for High Performance Time-series Databases #bbuzz
Publication date: 2014-05-28
Playlist: Berlin Buzzwords 2014 #bbuzz
Description: 
	Recent developments in deep learning make it possible to improve time series databases. I will show how these methods work and how to implement them using Apache Mahout.

Systems such as the Open Time Series Database (Open TSDB) make good use of the ability of HBase and related databases to store columns sparsely. This allows a single row to store many time samples and allows raw scans to retrieve a large number of samples very quickly for visualization or analysis.  Typically, older data points are batched together and compressed to save space. At high insertion rates, this approach falters largely because of the limited insert/update rate of HBase.  In such situations, it is often better to short segments of data and insert batches that span short time ranges rather than inserting individual data points.

Read more:
https://2014.berlinbuzzwords.de/session/deep-learning-high-performance-time-series-databases

About Ted Dunning:
https://2014.berlinbuzzwords.de/user/323/event/1

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              okay so here we go this is going to be                               kind of a dense talk there's a lot in it                               there's practical aspects but there's                               also philosophy there's even a little                               bit of mathematics this time so what I                               want to talk about is how the insights                               of anomaly detection anomaly detection                               done right can lead to some very very                               interesting capabilities for time series                                database and the ideas lead to time                                series databases that are first of all                                ten or more times more performant but                                also which ultimately can do semantic                                search on sequences and that's that's an                                extraordinary capability for a time                                series database most of them just draw a                                graph and not much else so I'm me I                                worked for map our case most people                                can't read hats because they've blinded                                to advertising you can reach me a lot of                                different ways I'm involved with Apache                                as well and been involved with open                                source for a very long time now part                                makes Hadoop distribution which has a                                lot of cool technology we can talk about                                it later so anyway let's talk real quick                                and I'm going to go very quickly through                                the first part about what anomaly                                detection is and how it should be done                                so the problem here is not like many                                kinds of machine learning where we have                                examples of what we want to find and                                then we ask the machine to find more of                                that here we have to find the things we                                don't know to ask for it's difficult the                                machine has to figure out somehow                                something that stands outside the normal                                and there's a story that my own journey                                with anomaly detection started when we                                had a system at music Matt and the                                problem was that the CEO would wake up                                at two in the morning and go and check                                the system and call me and said Ted we                                haven't had a sale in five minutes do                                you think that's okay I'd go hey I mean                                else can you say two in the morning when                                somebody calls you and asks you a                                question like that and so this happened                                a lot                                and then the real problem was he was                                often right and this is very bad for                                CEOs because it leads them to believe                                that they're always right you should                                never trained a CEO that way so what we                                did is we built a system finally and it                                was difficult because we had people from                                Japan people from Europe people from the                                US buying and and so on and so what we                                did is we built a system that understood                                the patterns that we had and would call                                me first that there was if there was a                                real problem and then it would let him                                as dashboard turn red about                                           later and so he would call me then he go                                oh my god the dashboards red and and it                                hasn't had a sale in                                                Dennis where you been I've been on this                                already we got it fixed we know what's                                happening I mean just go to sleep you're                                you got better things to do and it took                                about four times and it trained him to                                sleep through the night it's very                                important to train these people to sleep                                through the night it's difficult                                Isabelle can talk about this sort of                                difficulty but with CEOs you have the                                same problem and the idea is once you                                get a sense of confidence once they know                                that you will find problems before they                                can it's calm and actually everything                                works better because they do what                                they're supposed to do which is not                                minute to minute observation so here's a                                signal this is random numbers you get an                                idea what is supposed to do and then you                                see something like this and you say                                anomaly what's happening there is you've                                built a model in your head e is normal                                woo is not now you might see the spikes                                like this periodically that might become                                part of your normal and then the step                                would be the anomaly so the idea here                                again is the the normal may not be a                                simple sort of thing it may not just be                                here plus or minus a little bit of noise                                here plus or minus a little bit of noise                                and occasionally short spikes but not                                long steps you elaborate very quickly                                this model in your head and so you want                                 to make take action when something                                 breaks when something abnormal happens                                 definition of breaks is not clear                                 definition of working is not clear                                 so we want to trade off action and                                 inaction in the right times and we want                                 to build a model that does this for us                                 here's another look we could                                 automatically using things like the T                                 digest that I talked about at the bar                                 camp set thresholds and then the system                                 can tell us that we can set a budget we                                 want to be woken up this many times per                                 month and we want the most of those to                                 be when there really is a problem and we                                 could build a system like that where the                                 online summarizer pics whatever                                 percentile we want to set and that's as                                 a threshold and mahout has this and                                 we're done right nine slides we're good                                 but not quite so much easy I mean people                                 have done this sort of thing at sea has                                 released skyline but what about signals                                 like this a is easy to find but B is                                 hard B is hiding in the valley and so a                                 simple threshold is definitely not going                                 to work you also have signals like this                                 where you get this big anomaly over                                 there I'd like to think of these signals                                 though is not so much anomalous because                                 that's a heartbeat those are important                                 to me and here's an anomaly this is an                                 anomaly and a real heartbeat there                                 you've got an irregular heartbeat the                                 the the shape and the timing changes so                                 again we have to have a bit more of an                                 elaborated model but model is still the                                 key word there we want a model of what                                 is normal and then we want to say                                 whatever doesn't fit that model is                                 anomaly and if we built a good and a                                 tight model it really does express                                 normality then the things that deviate                                 from that will be honest to god and                                 important deviations now for simple                                 things it could just be normal                                 distribution that's boring but for the                                 EKG signal the electrocardiogram we can                                 build a model of this signal by taking                                 short windows and mathematically the                                 windows are formed by a signal that's a                                 little bump like that it's really just                                 sine squared or cosine squared and we                                 multiplied element by element sample by                                 sample and so this it's zero everywhere                                 except in the bump and so the signal                                 goes away except inside the bump and so                                 as we move the the window along we get                                 different little views of that and                                 they're all the same length because the                                 window is always the same length now we                                 get see that signal looks a lot like the                                 one at the beginning and that one looks                                 a lot like the one at the beginning and                                 that one looks a lot like the beginning                                 the idea there is then we could cluster                                 these these little windowed signals now                                 that the the window is chosen very                                 cleverly not by me by a guy called                                 hemming who came up with lots of clever                                 things so that when you shift them just                                 by half thing and you add them up they                                 add up to exactly one and that means                                 that if you just add up all those little                                 windowed signals you get the original or                                 if you add up some nice approximation of                                 each of the windowed signals you wind up                                 with a nice approximation of the                                 original and here for instance are the                                 most common shapes for the EKG signal                                 several hours that I looked at and you                                 can see there's that mean that's the QRS                                 complex there's another version of it                                 slightly shifted slightly shifted                                 another one another one and then you                                 have the bumps afterwards and so on so                                 this thing has learned the shapes that                                 make up a normal EKG and we can now                                 build the original and we can build the                                 reconstruction it's probably hard to see                                 but you can see that the reconstruction                                 is just like the original except a                                 little bit smoother little noisy things                                 the hard things that the things that                                 don't repeat our fall away and the                                 reconstruction error here which is just                                 the subtraction of those it's pretty                                 small most of the time it's quite rare                                 in fact for it to be as large as a                                 hundred plus or minus so this                                 reconstruction thing is a measure of                                 deviation from our sig normal and the                                 reconstruction is now a nice signal kind                                 of like the first one we saw just                                 livity noise sitting there no Big D VA                                 shin so big deviations could be                                 considered anomalies because we can                                 reduce to that earlier problem here's an                                 anomaly for instance it's hard to see                                 what that anomaly is especially if                                 you're sitting over here behind the the                                 lectern but here it is magnified this is                                 this is indeed a really bad thing that's                                 the beginning of atrial fibrillation the                                 herd be went did it it stopped I mean                                 that's why the guys survived otherwise                                 probably would be dead because there's a                                 portable heart monitor this data is                                 coming from and the anomaly is that the                                 thing that understands and reconstructs                                 normal heart beats can't reconstruct                                 this thing that is so anomalous it's a                                 subtle difference between that and a                                 normal heartbeat but it's a difference                                 that the model cannot understand cannot                                 see cannot reconstruct and so the                                 reconstruction error has a large error                                 there there's another anomaly this is an                                 instrumentation bug and the system just                                 says I can't see this as a heartbeat and                                 so it's just this huge anomaly so here's                                 a revised system as a model we subtract                                 that off and now we do the online                                 summarizer yeah we got it well I got it                                 going and this is model Delta sort of                                 anomaly detector and it's fine it's good                                 it does what it says and the idea here                                 is thinking about probability                                 distributions is a really good way to do                                 this you can even do this with events                                 dreams where it isn't so much that they                                 follow a same shape but they happen at                                 irregular intervals but the irregular                                 intervals themselves have shape to them                                 they have a model that you can build a                                 Poisson thing with variable rate is a                                 commonly used model for that and so we                                 can build again a log a model like that                                 we can use log probability as the                                 anomaly for detecting it so if we just                                 slip back a little bit we're just a                                 little bit in here but we have this idea                                 of what anomaly detection is build a                                 model look for anomaly look for things                                 that don't Mitch and we can deal with a                                 lot of different kinds of signals                                 but here's here's where philosophy                                 starts up to now is totally practical                                 things you could build you know in the                                 kitchen at home sort of thing but the                                 idea here that's very very interesting                                 is when we build a model that fits our                                 observations we are also building a                                 compressor and it's an interesting                                 mathematical truth that the model that                                 builds the best compressor is also the                                 closest one to truth truth being some                                 unobservable what the probabilities                                 really are it's stunning thing but it's                                 it's based on the idea that the log is a                                 privileged function it's a special kind                                 of thing it is the best choice for                                 measuring information but it also                                 happens to be the thing that determines                                 how much we can compress things and so                                 taking it to minimal error making that                                 reconstruction error as small as                                 possible with whatever model structure                                 we have is maximum likelihood maximum                                 likelihood is maximum compression and by                                 accident also truth to get the best                                 understanding out of a signal we make it                                 as small as possible the residue of                                 things we don't understand is smaller                                 therefore the bulk of things we do                                 understand is larger now there's nice                                 wonderful mathematics here but the shape                                 of things is what's important this log                                 function here has a maximum relative to                                 this X minus                                                             when x equals                                                         that optimum unique and exact so if we                                 think about this clustering we windowed                                 we clustered we scaled the cluster just                                 the right size with the right number of                                 things there make it as big as so on we                                 were subtracted from the original thing                                 and we combine that with this idea of                                 information compression and some kind of                                 what we're doing there is we're taking                                 the original signal we're encoding it                                 with a probability model                                 and then reconstructing the signal and                                 we're trying to make that encoding as                                 tight as possible and in in the the                                 simple tiny naive Saturday afternoon                                 kind of thing that I built a year it's                                                                                                       compression it's quite quite tight but                                 this is also called an information                                 bottleneck and it's been studied for                                 many years in AI so we've moved from                                 anomaly detection now to an idea that by                                 reconstructing here we're building a                                 compact representation a compact                                 encoding and in fact the clustering that                                 we used is the first step in a deep                                 learning system if we look at a neural                                 network the way a neural network works                                 is we have this is a an auto encoding                                 neural network the input is the same as                                 the output except for the little primes                                 on it meaning it's not quite exact the                                 input our values those circles are                                 values every time we follow an edge we                                 multiply by the weight of that edge and                                 when we get to the things in the middle                                 we add up all the incoming lines and                                 then we do that again those intermediate                                 circles have values and we go to the                                 output things and we add up now in the                                 middle layer there's always some sort of                                 non-linearity in interesting neural nets                                 because if you didn't have the                                 non-linearity there you could just kind                                 of reduce it all to one light one step                                 and if you were to look at this one kind                                 of non-linearity that we can add is we                                 can say only one of these int interior                                 nodes can be nonzero exactly one of K                                 that's clustering we'd pick the biggest                                 one the reason it's clustering is                                 because this incoming step the sum of                                 weighted inputs it's a dot product to                                 dot product with the weights dot product                                 is the same as Euclidean distance I mean                                 X minus y square                                 x squared plus y squared minus two x dot                                 y and if the magnitudes of x and y are                                 the same then the dot product there                                 tells us when the distance is smallest                                 dot product its biggest or distance is                                 smallest as we're slipping through all                                 the wise and so what we're doing when we                                 say only one of these can be non-zero                                 we're saying pick the nearest cluster                                 the clusters are those weights on the                                 inputs and those are the same weights on                                 the outputs because this is a                                 symmetrical auto encoder so that                                 clustering that we did there which                                 seemed very simple and easily motivated                                 just from kind of look at it find                                 patterns sort of mode is actually inner                                 lip and it happens to be a special case                                 of a case barse encoder we could say                                 just pick the three of the things that                                 are the biggest and we can train them                                 the same way as we would with k-means                                 learning or with neural net learning and                                 then we have one of the more interesting                                 and advanced neural nets that we can                                 apply to this auto encoding signal and                                 by enforcing that sparsity we can have                                 many normally you would say you could                                 only have                                                              if you allow dense patterns the dense                                 reconstructions that's just vector space                                 sort of things but if I say only one of                                 k then I can do like I did here where I                                 have                                                                window size is                                                          so it's a                                                               have enough fingers to point in all the                                 directions and and so by having that                                 clustering having that one of K I can                                 use a very wide interior layer and by                                 using M of cake and cut it down a little                                 bit and I can encode very interesting                                 signal so i really have kind of this                                 neural net for the first window second                                 neural net for the second one to third                                 and so on and their reconstructions give                                 us this this is kind of cool                                 we can make these neural Nets deeper on                                 that interior one of K signal we can do                                 the same operation we can put a further                                 abstracted value which will give us an                                 iota better compression and it will give                                 us significantly better understanding                                 and we don't even need to keep the                                 reconstruction if we don't want to so                                 what we can do is we can just keep the                                 first level nodes values but of course                                 those are encoded by the second level                                 nodes so we can just keep those with or                                 without an error signal and so we can                                 just keep that interior node which is a                                 semantic representation now it's only                                 been in the last five to ten years that                                 how to learn these models has been                                 practical this is a new thing to be able                                 to do that but we have the potential now                                 going from anomaly detection this all                                 modeling idea to build time series                                 databases that do not store the signal                                 they store the meaning they use these                                 ideas of anomaly detection these ideas                                 of what normal is the ideas of maximum                                 compression being maximally much like                                 the true model to compress things and                                 allow now not just signal search like I                                 want this hour of data and plot it now                                 from that signal from that signal but                                 actually to say I want to find anomalies                                 that look like this one I want to say I                                 want to find where the pump failed with                                 this predecessor sort of signal want to                                 find more examples of that I can now say                                 give me semantic search and at the same                                 time the database becomes                                               somewhere in there depends on the                                 signals smaller which is the same thing                                 as the data rate coming in can be                                     larger this is really kind of cool and                                 the trick here is you have to be able to                                 build and they've been a lot of talks                                 about this you have to use real-time and                                 long time together to be able to build                                 these systems Michael talked yesterday                                 about lambda architecture which is an                                 approach to that I like extending it by                                 using a real time                                 log so that you can recover more quickly                                 than just waiting for the batch layer to                                 come back around and the trick of course                                 is that Hadoop in systems like it or not                                 very real time the the ordinary file                                 systems that you use with them are right                                 once and sooner or later people will see                                 the results but only after flushing or                                 closing and so what we want to do is win                                 or take the stuff that the batch system                                 has processed and the stuff that the                                 real times has processed storm sparks                                 equal whatever you like and we need to                                 provide this blended view and the                                 blended view now consists of taking                                 models built in the long time and                                 applying them in real time to the signal                                 so I've strove and mightily to stop as                                 quickly as possible so we could have                                 time for questions because it's always                                 the best part of any talk just hearing                                 what people do and what they're                                 interested in and what they'd like where                                 they'd like to drive some of this stuff                                 so there we are we're                                                  now got                                                                guys think what do you guys want to do                                 with this any thoughts it's a little bit                                 fast wasn't it it's kind of a lot to fit                                 into                                                                   and a man with a microphone it makes it                                 portable so I kind claimed it understand                                 all that of the mathematics but it seems                                 to be the into intuitively you know if                                 we were speaking about you know like the                                 model being some form of compression of                                 the underlying data that and there being                                 some approximation to it that the more                                 we compress it you know the less clear                                 the signal is going to be in the more                                 approximately it's going to be and they                                 know that the representation of a data                                 series that has the least amount of                                 error is the data itself so in order to                                 gain compression we lose you know                                 precision is that the right way of                                 thinking about it no but otherwise close                                 no because what I'm talking about is for                                 a particular constant accuracy what can                                 you do to compress the data the most and                                 if you hold accuracy constant                                 you can still compress signals that are                                 not totally random if they were totally                                 random then there is no the the entropy                                 of the signal is the bit rate in any                                 real signal certainly any real signal                                 that we want to interpret the way humans                                 interpret it and humans are doing a                                 massive job of compression as they see                                 things as they hear things what you can                                 do is you can hold the error you can                                 define what errors insignificant means                                 and I'll get back to that in a moment                                 and you can hold that constant and you                                 can still compress so compression does                                 not mean loss more compression does not                                 need mean more laws more compression                                 with constant loss means you have a                                 better model more compression with a                                 constant model that's totally general                                 purpose yeah eventually you get down to                                  and you just have one bit and you                                 go bit and and you've lost all the                                 information that is not what I'm talking                                 about I'm talking about holding the loss                                 constant and also if you think about it                                 this was originally                                                    error in reconstruction is typically                                 less than                                                              all of those less than one bit things                                 can be Justin coded as zero with a run                                 length and is round when you reconstruct                                 and then you could store the error when                                 there is a high error when the air gets                                 large you can actually store that now                                 anomalies by definition don't occur                                 often therefore you store at full                                 bandwidth you have the semantic                                 representation and you say but and so                                 you just search for but and find these                                 sorts of things and so you have perfect                                 fidelity there and you have almost                                 always very high compression every time                                 things are normal the system is going on                                 its normal type                                                         get it it's like the prisoners telling                                 jokes they've compressed it they have a                                 fine dictionary that encodes that and                                 then when there's an anomaly you go but                                 here's the error so you can still get                                 perfect fidelity or nearly perfect                                 fidelity or if                                 fidelity as good as your signal really                                 is there's always the question of                                 significance so yeah you don't have to                                 lose the signal so so if the variance in                                 the data is very high the mechanism is                                 not going to perform as well as if it                                 would be a very uniform signal with very                                 rare abnormalities well if the                                 abnormalities are not rare than they are                                 not abnormalities so you know you                                 there's a very slippery sort of semantic                                 arguments there and here in this                                 original signal variance is not the                                 issue variance is the the size of that                                 thing but that isn't the big deal I mean                                 a little bit if I have quantization here                                 see here's                                                         smallest thing I could do then clearly                                 having a variance this small means I                                 really have this value of                                               time if I really had a quantized signal                                 but that would look very different and I                                 would model it with a dictionary i would                                 say                                                                    there's a signal and so on and so                                 variance is really not the right concept                                 their entropy and relative entropy                                 relative to the model is the correct                                 concept now we may just be words you do                                 different things than this so words can                                 be an exact but a little bit of care is                                 careful is important they're right next                                 to you somebody had a finger up so my                                 question is about what the main sort of                                 to look at the data add because your                                 talk seems to be focusing on looking at                                 the time domain so all of your signals I                                 essential is sort of overtime if we                                 actually flip into some other domain                                 let's say you know we do wavelets you                                 know sort of decomposition or you know                                 Fourier transform you know we would                                 basically start looking at a different                                 sort of data sets is there any sort of                                 theory behind when one domain is better                                 than the other for the type of thing                                 that you were talking about yeah so so I                                 think there's kind of like three or four                                 questions in that oh and I'm going to                                 start with a simple most                                              sort of just question                                 you mentioned for you transform now ouya                                 transforms take a signal of some size or                                 or some period and if it's a discrete                                 signal you can use the discrete Fourier                                 transform and so on and it restates that                                 in terms of what are called basis                                 vectors these are sinusoidal Co Co                                 sinusoidal properties in that it loses                                 no information and it is optimal in many                                 ways but it misses the point you know                                 back in the                                                              and and in Hamilton and many others were                                 coming up with his idea from physics and                                 they hide the idea that sinusoids were                                 somehow special and that complete                                 orthogonal orthonormal basis were a very                                 very important thing but in fact that's                                 really not the right way to go they                                 started with that assumption because                                 they're building and inventing linear                                 algebra at the same time and they had                                 all these optimality prism but that                                 really wasn't the point and                                              or so Candace and others came up with                                 this idea that in fact signals of                                 interest very very often are better                                 expressed as a sparse value in a much                                 higher dimensional space so the idea of                                 over complete basis came up and that's                                 what we have here we have                                           vectors in a                                                           way over done and then we encode as                                 exactly one of them now maybe we could                                 encode is exactly thrilled but if Lee a                                 sort of thing would have encoded it as                                                                                                          there is apparently a strong correlation                                 to visual perception very very much                                 seems like humans do this and this is                                 how they see things and understand them                                 quickly now there's no reason the world                                 has to work that way we could imagine                                 worlds that don't work that way but our                                 world does                                 to work that way at least as we perceive                                 it and systems that have strong                                 regularities can be shown to have these                                 kinds of properties as well it isn't                                 just that the human visual cortex or                                 auditory cortex has to be able to see                                 these changes and so it's importantly                                 true that we don't worry about these                                 orthonormal basis sets so then we moved                                 to wavelets in the                                                  she's and things like that those are an                                 attempt to do the flu a thing with a new                                 set that has what's called compact                                 support meaning they're bounded in time                                 but it's still assuming orthonormal                                 complete basis and the point was                                 sparsity sparsity alone has been shown                                 to be a vastly important organizing                                 principle for these learning systems it                                 alone sparked the entire deep learning                                 revolution practically it alone has been                                 the driving principle that allows now                                 computers to do almost anything we can                                 do at a glance the the Android speech                                 recognition system is based on these and                                 so I really try to draw a line from the                                 the                                                                    based minimal basis it's a very                                 different kind of thing so that's one of                                                                                                      where does this work I'd spaced it out                                 entirely I don't know there's there's                                 lots of systems it does work in it works                                 in text it works based on my prior work                                 on genomes it works in time series                                 databases works in a video databases it                                 works in auditory databases but it has                                 to be things that have regularities they                                 can be discovered by these methods these                                 methods are not very well understood yet                                 and so we don't know what the bounds and                                 lemons are my guess is anything that                                 compresses super well with a nice                                 understandable compression scheme it's                                 basically the same as what I showed and                                 of course the stuff I showed here is                                 incredibly not even simple                                 it's what fits in                                                        there's somebody over here any                                 applications you could recommend to have                                 a closer look and weds implemented your                                 model like you said to not get the CEO a                                 CIO waked up during the night yo like                                 how skyline works from metzia is it                                 actually applying the workout sergeant                                 the connection between CEOs and scholar                                 but no skyline what you're scaling that                                 from FC skyline skyline I know how it                                 work out of it yes so skyline is a very                                 simple tool you set or I think you can                                 automatically set possibly thresholds                                 and things cross them so it doesn't do                                 any kind of learning along these lines                                 so that's an example of applications                                 which yeah yeah there's many                                 applications for this and it's always                                 places where systems are not possible to                                 be watched continuously implementations                                 examples wits actually power plants                                 medical instrumentation oil fields where                                 they're drilling and I'm in software                                 wise like what software wise with us no                                 I don't have an example I mean this all                                 is on github but it's a little bit small                                 to be used in any honest-to-god                                 applications I don't know of any open                                 source software that does this in a                                 general way now this is very very close                                 to things like the open TS DB which is                                 an HBase based system that uses the two                                 advantage the fact that HBase ranges                                 things with sequential Keys together and                                 so that you can read stuff and it also                                 uses this idea that you would compress                                 historical data into little blobs now it                                 has the problem that it stores the exact                                 data in the database and so it's limited                                 speed wise dramatically I mean without                                 that one limitation and with a real-time                                 data store of other forms you can get                                 and when we've had systems that exceed                                                                                                    but anywhere any industrial thing where                                 things break and cost people money they                                 want to find these and so those are the                                 applications i bet i don't have openly                                 available software we have several                                 customers doing this but they don't make                                 their software open source yeah we're                                 going to have to do traveling salesman I                                 thank you for the talk I have a question                                 concerning one of the the figures in the                                 beginning when you showed the neural                                 network so the input and the output                                 layer yeah he packin the individual                                 neurons in the input and the output                                 layer they are the signals at a                                 particular instance in time is this                                 right it signals at a particular                                 instance in time is after windowing so                                 of the different input nodes different                                 instances in time or yeah and so then we                                 have a separate neural net for each half                                 window step so here's the first window                                 there's this first half window step                                 here's the next window over yeah okay                                 thank you so I got the right and then                                 the the reduced representations the                                 compressions that you get in the middle                                 layer you talked about the sparseness                                 you're using their vote for the                                 clustering so I'm what is the intuition                                 of what is the reason for young is                                 sparse representation in the hidden                                 layer I don't you use a smaller hidden                                 layer smaller number of nodes nor a                                 dimension and the full space like why                                 don't you generate disputed it's kind of                                 disability representations yeah so that                                 that's a very interesting question and                                 that's taken mug I can't say that it's                                 been answered but since the early                                     and since even before with the early                                 distributed representation work in the                                 mathematical sphere people thought that                                 the best way to do a bottleneck                                 architecture                                 like this or you have the small center                                 thing that the best way to do that was                                 to have a small number of nodes that all                                 could be activated and you would put                                 some regularization on that by like try                                 to minimize the sum of the squares of                                 the weights yeah very very common idea                                 and there are results that show that the                                 representations you get something like                                 the house it totally whatever they do in                                 Latin semantic indexing the they're very                                 much like an SVD yeah yeah yeah and they                                 don't work sometimes sometimes they do                                 but not very well you know about current                                 work of Google like producing such                                 reduced representations for birds in                                 language or the old work yeah but the                                 best ones are all sparse and the                                 sparseness yeah I think you're talking                                 about nikka loves work this guy has some                                 very interesting ideas that that he                                 encodes words and the directions encode                                 relationships between words so you can                                 have words here and the plurals will all                                 be the same direction and distance from                                 the originals you can have man and woman                                 and you get king and queen same                                 direction same magnitude from these                                 others and he does use dense                                 representations yeah but very few people                                 have been able to replicate that work                                 and all of the operational systems that                                 that are doing seriously good work the                                 image recognition systems the speech                                 recognition systems use interior sparse                                 representations and so it isn't clear                                 really it isn't what's going on there in                                 a deep sense but it does seem that                                 sparsity is a very very important                                 property and that's been true it's been                                 very clear since the late                                              compressive sensing literature that                                 sparse representations were very very                                 powerful in encoding physical phenomena                                 you know if you want so in a dense                                 representation it if we're looking at a                                 time series we have some signal that's                                 band-limited we have to have samples                                 often enough that we have no aliasing of                                 signals that's the Nyquist sort of thing                                 we have to sample often to characterize                                 a real signal that was the                                 from                                                                  know though that the signal is say the                                 mixture of three sinusoids it has to be                                 no more than three sinusoids mixture not                                 a continuous mixture then you need very                                 few data points you need about five to                                 be able to characterize it regardless of                                 how long a period that is you can find                                 out which sinusoids it is well not                                 regardless but yeah a little bit of hand                                 waving there and so you can use far far                                 less information you have a massively                                 underdetermined system and with this                                 sparsity constraint you can represent                                 that now there's also some talk about                                 how rotationally symmetric                                 representations the dense                                 representations you talk about have                                 lower learning complexity and large                                 dimensions have higher dimensionals and                                 non rotational ones higher complexity                                 but the fact is that these high                                 dimensional non rotationally symmetric                                 systems that are sparse have very very                                 low learning complexity and the                                 representations are very easy to learn                                 and they're easy to interpret they're                                 easy to search for I mean like an                                 elastic search can take these first                                 things and drop them into an index and                                 so there's some very important                                 properties as i say not well understood                                 but it does appear to be very very                                 important and it does appear to be                                 biologically very plausible built into                                 us and we are better at this than any                                 computer so far so it's not a bad thing                                 to take a hint thank you not that                                 airplanes or birds but the hints are                                 still good                                 I was wondering um in one of the grass                                 he showed a zigzag line with a spike and                                 then a systemic shift that may occur if                                 for example you're measuring performance                                 of a website and there's a new release                                 and it turns out to be a lot slower or                                 faster how would you a update to model                                 would that be a continuous process or                                 would that be some other way what do you                                 think that should be done you know                                 somebody talks about the heart there's                                 very few hard problems in computer                                 science it's like naming schemes and                                 cache coherence and so on well there's                                 very few Universal answers but the the                                 universal answer is it depends and you                                 just earned that that answer but the the                                 it starts with the thing that when you                                 detect an anomaly it may be good or bad                                 you may have just released a new much                                 better system like you say and that                                 would be an anomaly this is not working                                 the way it used to system panics and it                                 goes uh huh and you go it's all right                                 it's all right I know I know about that                                 that's good you know you want the alarms                                 go off when you do something like that                                 it's a little bit of confidence                                 inspiring sort of thing how do you train                                 after that well if the world has changed                                 like the shift then training in an                                 online fashion that keeps a big memory                                 of the past is disastrously bad starting                                 over at zero and having a very low                                 resolution model for a short time that                                 gets better and better as you go along                                 now the past could give you prejudices                                 about what kinds of models you might see                                 some priors but the details of the                                 previous model are the things that need                                 to be rejected quite quite decisively at                                 that moment and then the classes of                                 models the priors can still inform you                                 so you can learn very quickly but yeah                                 if things change you have to change the                                 models otherwise the alarm will just go                                 off forever and that's not useful there                                 was somebody in the back yeah                                 there were several people in the back                                 actually yeah we'll have to make a                                 backside tour of the microphone you ever                                 noticed that when people smash into the                                 room they always stand in the back so                                 obviously shy people come late yeah um                                 so all the techniques that you that you                                 showed assume that some windowing                                 happens before so all of the time series                                 the edge techniques yeah so does that                                 mean that they apply to two scenarios                                 where the length of a period is constant                                 that is and is known upfront or does it                                 also work with the variable over the                                 varian varying period length or is there                                 simple trick to make it work with a                                 varying period length oh there's a                                 simple trick in the data that I had                                 which was pick a window length it's                                 small enough that the clustering will                                 finish before sunday comes so that was                                 that was my strategy there and then oh                                 my god it sunday morning it works that                                 was really good now the real world is                                 not usually so forgiving and and                                 requires a little bit more thought than                                 that then this is just what I can get to                                 work and there's there's two                                 considerations one is how large do you                                 make I'm trying to find you the prize                                 that I award for a good question yeah                                 because that really is a deep question                                 what size do you make that window and in                                 the signal that i showed you make it as                                 short as plausible to still capture the                                 short time scale features and then you                                 ask do you use a variable time window                                 and the answer is if you really have the                                 deep learning stuff so that the window i                                 had was too short for instance to detect                                 a change in the heart rate which is                                 pretty important you know as heart                                 signals go but the if we start building                                 the deep learning system where it has                                 higher and higher levels i'm totally                                 distracted here but if you start                                 building the the higher levels then you                                 there it is can you catch there you go                                 whoa no can I throw is the question I                                 was off by an entire meter out of                                    yeah so anyway the the question is that                                 the deeper layers will start in coding                                 regions of Windows and so they start                                 gathering data from a longer time string                                 and so the low level window can be                                 usually fixed and relatively short and                                 then the deeper layers start encoding                                 the larger signals basically symbol                                 co-occurrence heartbeat there nothing                                 heartbeat with a shift over here that                                 means heart rate buxom so that's the                                 idea now details will vary when this                                 really hits a real problem it won't be                                 looking quite like this so that was a                                 good question and then this lady over                                 there sorry running out of the talk                                 we're out of time catch me in the hall                                 such a sadness oh good she says one of                                 the questions was answered by pure luck                                 and and clever see you did deserve the                                 little now that is open-source candy so                                 you have to share                                 you
YouTube URL: https://www.youtube.com/watch?v=Q3DkkXElBtQ


