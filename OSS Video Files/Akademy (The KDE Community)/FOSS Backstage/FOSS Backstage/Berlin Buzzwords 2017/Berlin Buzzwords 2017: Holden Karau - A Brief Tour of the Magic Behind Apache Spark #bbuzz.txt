Title: Berlin Buzzwords 2017: Holden Karau - A Brief Tour of the Magic Behind Apache Spark #bbuzz
Publication date: 2017-06-15
Playlist: Berlin Buzzwords 2017
Description: 
	Apache Spark is one the most popular general purpose distributed systems in the past few years. Apache Spark has APIs in Scala, Java, Python and more recently a few different attempts to provide support for R, C#, and Julia.

This talk covers the core concepts of Apache Spark, but then switches gears into how the "magic" of Spark can have unintended consequences on your programs. You will gain a better understanding of impact of lazy evaluation, the shift away from arbitrary lambda expressions to "statements", and of course the "dreaded" shuffle (plus a few more concepts). 

Read more:
https://2017.berlinbuzzwords.de/17/session/brief-tour-magic-behind-apache-spark

About Holden Karau:
https://2017.berlinbuzzwords.de/users/holden-karau

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              thank you all for having me yeah so this                               is a very brief tour of the magic behind                               spark they only have                                                will leave a lot of the magic behind                               it's very sad and my co-presenter today                               is boo she is wearing her witch's hat                               for for the magic part she she's very                               she likes magic a lot yeah so my name is                               Holden my preferred pronouns are here                                her it's tattooed on my wrist in case                                you forget and I'm a principal software                                engineer at IBM's Park Technology Center                                in San Francisco I like it to give me                                money to work on open-source I'm a sport                                committer but I mostly work on Python so                                if things are broken I will blame                                someone else if they work I will take                                credit and they've worked with a bunch                                of other companies before IBM on a bunch                                of distributed system stuff if you want                                to follow me on Twitter it's just my                                name Holden Caro right now it's mostly                                someone being kind of really sad about                                living in America and the slides from                                today and my other talks will go on                                SlideShare and I have some other spark                                videos if you want to see a longer                                version with more magic you can you can                                find different magic on YouTube and some                                of my other talks and just to be like                                super clear I added the slide after the                                American election which made me really                                sad but I'm trans and queer and Canadian                                I'm on a work visa where I where I live                                and it's when they're talking about                                taking away so that's fun                                and I consider myself so part of the                                project out there community and this is                                really just like I mean if you don't                                know other people like me we're normal                                people we write the same shitty spark                                jobs that you do we get out of memory                                exceptions I don't have a secret Java                                garbage collector that I'm hiding from                                you right like we're all just software                                developers and we're nice people or I                                mean close enough right                                hmmm this is my employer they give me                                money so they get a slide we have this                                very nice-looking Lobby with a lot of                                green in it which is supposed to                                indicate that we make good software                                because green is the color of a passing                                Jenkins build so a green Lobby means                                that your software totally works if                                you're ever considering buying something                                from IBM I don't know what it is we sell                                anymore but please buy it                                yeah whatever the yeah oh and I'm not                                alone like I work with a bunch of other                                people on SPARC and they're nice people                                as well so we're going to talk about oh                                wait I'm going to assume you're nice                                people you've laughed at many of my                                really crappy jokes so we're off to a                                good start I am really curious though                                how many people are completely new to                                Apache spark is this anyone's for a                                spark talk okay cool um this talk is                                probably not the best introduction to                                spark um because we we try and look                                behind the covers a little bit hopefully                                you'll still find it useful but you                                probably want to check out Paco's                                introduction to spark talk after this if                                I don't scare you away from using spark                                how many people are like diehard our                                users really okay how many people are                                diehard our users that don't use Python                                I would thank God okay um so all of the                                examples are in the scholar Python ah so                                good                                I don't know R well enough yet so for                                the people who are new to spark why why                                should you be thinking about spark so I                                think a lot of people come to spark                                because they have a MapReduce job                                they're running it and sixteen well                                eight hours into their                                               they're like I bet I can learn spark                                before this finishes and yes you can you                                can learn spark in the time it takes a                                MapReduce job to run the API is pretty                                simple                                the other reason that people come is                                their data frames stopped fitting in                                their MacBook Pros main memory and                                they're like the solution to this                                problem is distributed systems                                 and now they have n problems but on the                                 other hand their code might work so                                 hopefully one of these things is                                 relevant to you but the reason we're all                                 here today is magic okay cool let's do                                 some magic um so there's a lot of magic                                 aids that makes spark as a distributed                                 system like work or like computer                                 answers the dag and the query planner                                 are really like the core building block                                 of the magic of spark it's the part                                 which lets us optimizer do all sorts of                                 really cool things                                 spark is lazily evaluated and as a                                 result spark doesn't actually do the                                 things we asked it to do until we                                 absolutely force it kind of like a                                 really lazy                                                             yeah I took out the garbage Oh fine okay                                 you want to go check I'll actually take                                 out the garbage                                 but unlike your lazy                                                   is able to use this to do pipelining and                                 combined a lot of steps and save time                                 the other thing which I think is really                                 important as a distributed system is                                 sparks resiliency model is a little bit                                 different than some other traditional                                 distributed systems so it depends on                                 recompute rather than sort of                                 traditional resiliency of saving results                                 out to a bunch of different machines and                                 this is really important because the                                 only thing slower than a disk is three                                 disks over our network and that's sort                                 of the traditional resiliency model and                                 we can also do interesting operations                                 without the serialization because                                 someone spent too much time drinking and                                 we'll we'll look at some of these                                 different pieces of magic but we're not                                 going to cover all of them there was a                                 really lovely talk yesterday which                                 talked about some of the stuff happening                                 in the shuffle and some other magic it                                 wasn't called magic for some reason but                                 I mean artistic differences I respect                                 that if you miss that talk I have a go                                 to Chicago talk which talks about some                                 of the same things and you should watch                                 that well yeah so the the reason we all                                 use spark is that our data is magically                                 distributed across our cluster along                                 with our work at some point our RTD or                                 our data frame there are                                 collection is forced to exist right if                                 spark was able to never do any work I                                 mean you could just not write your                                 software to begin with and it would run                                 much faster you probably wouldn't get                                 paid as much but you know would be                                 perfectly fine but when this happens                                 it's really important that our data is                                 distributed but there's this catch that                                 spark tries to understand where data is                                 to some degree and this is done with                                 this thing called partition errs and in                                 spark normally when we load our data in                                 it doesn't really understand what the                                 layout of our data is but if we need to                                 do something like a join or if we need                                 to sort our data spark has to understand                                 where the different keys for our data                                 are and partition errs are deterministic                                 and this deterministic nature of                                 partition errs breaks some of the really                                 cool magic right are magically                                 distributed collection that I can                                 pretend just works starts to have some                                 constraints so when I say magically                                 distributed what we actually mean is                                 that each of these gnomes is responsible                                 for some of my data and when I ask these                                 when I ask my spark cluster to do some                                 work each of these                                                      to go off and do that little work on                                 their own but once I like want to get my                                 keys in order these gnomes have to talk                                 to each other to figure out like hey                                 which which pieces of the data do you                                 have which pieces of the data you have                                 and at that point we start trying to                                 assign gnomes different pieces of the                                 data to deal with specifically rather                                 than each just giving them like some                                 random set of records the problem is we                                 might give one of those gnomes the                                 responsibility for the null key and at                                 that point that gnome gets really sad                                 because most of your data is no because                                 the client lied to you or didn't lie but                                 that they told you their data was evenly                                 distributed what they meant is the data                                 where my keys exist is evenly                                 distributed and then our gnome                                 responsible for the null key goes on a                                 bender it just doesn't come back and                                 then our job fails because we have this                                 very sad gnome or actually then the                                 gnome fails and we try it on another                                 gnome and then we try it on another                                 gnome and now we have three dead gnomes                                 and                                 thankfully gnomes don't count but you                                 know things things start to go poorly at                                 this point and so yeah this is this is                                 the key skew problem it's the sort of                                 part where sparks magic distribution of                                 data stops working and my traditional                                 example for this is handlebar moustaches                                 because as someone that lives in San                                 Francisco                                 I mean software is great and make a lot                                 of money on software it's pretty good                                 but there's this thing where I have to                                 do work and that's just like not really                                 my style so I want to get out of the                                 software business and start selling                                 moustache wax to people and they're                                 normally there's someone I knew you have                                 a pretty good moustache okay normally                                 there's someone in the front row is like                                 a really rockin moustache and we can we                                 can talk but in this case I want to know                                 where I should be opening my moustache                                 wax shop so I've got this this data and                                 this is American zip codes right and you                                 have postal codes that I assume have the                                 same problems but so once I try and sort                                 my data it's it's going to go really                                 poorly right but that was talked about                                 yesterday so we're going to talk about                                 what happens if I try and call goodbye                                 key in my data I might be trying to                                 figure out like I want to send some                                 really high quality targeted marketing                                 to all of the people in whichever zip                                 code has the most handlebar mustaches                                 because that's where I'm going to open                                 my moustache wax shop so I'm going to                                 take my RDD I'm going to call group by                                 key on it and then things are going to                                 go fabulously mmm okay well the problem                                 is when we call group by key spark                                 creates this RDD where it's this key and                                 this list of all of the records for that                                 key and then afterwards I can do some                                 type of arbitrary transformation on that                                 RDD and that's great but we think this                                 shouldn't be too bad right because                                 sparks optimizer doesn't actually have                                 to make that RDD exists until we get to                                 the point where we save it up to disk or                                 you know I start sending out my Flyers                                 to all of the people about my moustache                                 wax shop the problem is that spark can't                                 see inside of our lambda expressions                                 and this is this is like the limitation                                 of the magic of spark once we all of our                                 maps are flat maps all of the things                                 that we do spark and understand what                                 we're asking it to do except you can't                                 see inside of what we ask it to do so                                 let's okay I'm going to go to this one                                 really quickly so the problem is that                                 spark can't see inside of this lambda                                 expression so it doesn't know that we                                 want the counts here right even though                                 it's able to delay the computation until                                 we save this result out to disk spark                                 isn't able to move this counting up                                 above grouping the values together by                                 key because really what we want to do is                                 as we're going through for each zip code                                 I want to be keeping track of the number                                 of people who are in the zip code that I                                 can send these high-quality                                 advertisements to but because it can't                                 see inside of my lambda sparked into the                                 magic breaks and then boo gets very sad                                 and this happens again                                 so key skew plus black boxes equals                                 sadness but it's okay we're going to                                 we're going to make this work we're                                 going to do well okay we're going to see                                 an example with literally kilobytes of                                 data because I ran this on an airplane                                 rather than a real cluster but we can                                 see I've got some input data it's                                     kilobytes and my shuffle read is                                    kilobytes but if instead from my                                 handlebar mustache workshop I replace                                 this with reduced by key or aggregate by                                 key and was computing like the number of                                 people who were interested in handlebar                                 mustaches in San Francisco this would go                                 a lot better and this is because when I                                 replace my operations with things that                                 spark and get sort of more information                                 about spark is able to sort of start                                 pipelining these a lot better so instead                                 of constructing a giant list and                                 shuffling this list around spark is able                                 to compute the summary statistic on each                                 individual gnome and then each gnome                                 just has to tell the other gnomes                                 how many handlebar mustaches are in each                                 zip code and that's a lot more efficient                                 than having to go to every other node                                 when be like hey these are all of the                                 handlebar mustaches in San Francisco                                 because I                                 that's just going to take forever we get                                 a map side reduction for free which                                 means that this is                                                     than                                                                    mean kilobytes not a super exciting but                                 if we replace that K with the T or even                                 a G right like that could that could                                 make a difference but the really                                 important part is that like that                                    kilobytes was probably all going to one                                 gnome like that was all going to the                                 gnome responsible for San Francisco so                                 we had one very sad gnome and here you                                 know we we have like one kind of sad                                 gnome but it's only sad for the number                                 of other gnomes in our cluster not sad                                 for the number of handlebar mustaches in                                 San Francisco and in terms of we tend to                                 have more humans and computers sort of                                 more records than the number of                                 computers in our cluster if you have one                                 computer per record in your data                                 processing pipeline                                 I would love to sell you a support                                 contract but like this is not going to                                 fine if you want to write your code that                                 way so the other option is we can we can                                 spend a lot of time trying to make                                 things work and sparks our DD API and                                 like taking all of our lambda                                 expressions thinking carefully like oh I                                 did a group by key and then I had this                                 like reduction afterwards                                 can I rewrite my reduction into an                                 aggregate or we could just go ahead and                                 use sparks data set API and I really                                 think the data set API is a really cool                                 piece of magic where admittedly I have                                 to give up my like arbitrary lambda                                 expression some of the time I can still                                 use them but I can just like give spark                                 some hints and stuff will just work for                                 me and it's really cool magic and it's                                 the best kind of magic because I don't                                 have to think about it so let's look at                                 an example using the data set API so                                 here I'm interested in figuring out the                                 fuzziness                                 of the happy pandas and this is really                                 cool because these expressions are                                 mostly data set expressions but then at                                 the end because I forgot how to do some                                 I just used your reduce and so this is                                 an arbitrary lambda at the very end but                                 the first two bits are in sparks little                                 cool DFL and I mean trying to convince                                 people to use a DSL                                 like you know trying to sell people                                 handlebar moustache wax it's a little                                 difficult but it's really awesome                                 because the first bit like we've got                                 this filter expression where we're                                 looking for happy pandas spark is able                                 to do some really awesome things for us                                 so if I have data about pandas spark is                                 able to only load the information about                                 the happy pandas because it can                                 understand what's happening in here but                                 if I have a lambda expression inside of                                 their spark can't figure out what my                                 condition is but because we write it in                                 this DSL like the magical optimizer can                                 take care of it and the fastest data                                 that I can ever load is the data that I                                 don't load and the Select also means                                 like spark will automatically only if                                 we've got columnar data will only load                                 the attribute about how fuzzy the pandas                                 are so if we have a bunch of other                                 information about like where the Panda                                 came from we wouldn't have to load that                                 in from disk either yeah okay yeah um                                 and and if you want to still use your                                 cool arbitrary lambda expressions you                                 totally can                                 I do this a lot because it turns out I'm                                 pretty bad at writing things in sequel                                 like languages and I'm much better at                                 writing lambda expressions and it's                                 really convenient because this map is                                 still distributed across my cluster all                                 of the existing spark magic so works and                                 I don't have to switch paradigms or                                 something like that to get between the                                 different types of magic and here's an                                 example in Python for the Python people                                 it's not as pretty in Python this                                 party's actually my fault I've totally                                 been meaning to fix this but it turns                                 out it's kind of hard to do like static                                 typing in Python and a lot of the magic                                 that we do involve static typing so we                                 we have to be a little bit more verbose                                 in Python to just help spark out but                                 it's still not too bad right like we you                                 know mm-hm                                 I'm sorry Python users um but it's okay                                 it's okay this little bit of sadness                                 right like the fact that I have to write                                 this like weird ro expression here like                                 that's not the end of the world and it's                                 so much faster it's so much faster um so                                 bigger is bad                                 smaller is good unless you if you're                                 selling support contracts totally use                                 group by key if you build by it's like                                 the CPU cycle but otherwise you probably                                 actually want to use reduced by key or                                 you want to use data frames right and we                                 can see that data frames outperform                                 reduced by key right and this is for                                 really simple aggregate so even even                                 when I'm writing intelligent things                                 inside of rdd's things can get a lot                                 faster and sparks data frames and data                                 sets um so why is it so much faster so                                 the really cool thing is that we can                                 sort on the serialized data previously                                 when we were doing our sort we would                                 have to deserialize and re serialize our                                 data for pretty much every step inside                                 of the shuffle inspark the the really                                 important part though is that spark can                                 understand the aggregation that's                                 happening and this gives us a very space                                 efficient way to store things and also                                 allows the optimizer to do a lot of                                 really cool things and so what what can                                 we do in this DSL so we can do a lot of                                 the things that we're used to doing in                                 spark we can do filters we can do                                 drawings we can do group bys and we get                                 some new fancy things too if you want to                                 run arbitrary sequel statements on data                                 frames but it's totally doable you just                                 have to pretend you're living in the                                     because that's when we picked up the                                 sequel standard from but if you if you                                 don't mind a trip back to like sequel                                    tool and you can you can run arbitrary                                 sequel expressions and the other thing                                 which you can do now really easily is                                 you can do windowed expressions and you                                 can you can totally do all of these                                 things in rdd's right they're just                                 really painful to do right if I want to                                 compute a window in Rd deal and this                                 like magic distribution of my data like                                 it kind of breaks because I have to                                 think about like what happens when one                                 No                                 has like part of the window that I'd be                                 writing like it has records one through                                 four and the other gnome has records                                 four through six and my window is                                 computed over these two pieces I have to                                 like think about that and then the                                 really more important part is I have to                                 remember to test that because even                                 though I thought about it I probably got                                 it wrong or I can just use the window                                 expressions and let's sparks equal take                                 care of it for me um and as someone who                                 has like screwed up writing window                                 expressions on rdd's before like this is                                 something that I really enjoy doing                                 there's a long list of the built-in                                 aggregations at work if you want to                                 write your own custom aggregations is                                 totally supported you can go here and                                 see the list but group-by becomes safe                                 which i think is really important                                 because group by key sounds totally                                 reasonable except for the part where                                 normally destroys you inside of spark Rd                                 DeLand because it does terrible things                                 we can compute multiple aggregates I                                 know if you're coming from a traditional                                 single machine background this is not at                                 all impressive but when you have to                                 compute three different aggregates                                 inside of a reduced by key step you end                                 up with like five different variables                                 you're keeping track of and you really                                 hope you don't screw any of them up                                 um so hmm there is a catch though with                                 everything it explodes under certain                                 circumstances so all new magic is not                                 guaranteed to work                                                     you're doing iterative machine learning                                 come talk to me and we can talk about                                 how to work around this problem but                                 there's some extra knobs that you'll                                 have to tune in your spark cluster to                                 make it really work but other than that                                 the magic should be pretty fine oh and I                                 promised the conference organizers that                                 I would only have one slide of trying to                                 shamelessly pitch you on a book so this                                 is the one side of stream lessly trying                                 to pitch you on a book you can buy this                                 book it's about spark um                                 [Music]                                 presumably you're interested in spark if                                 you're not it is a great gift for cats                                 they love the box that comes in                                 hmm please don't return the book                                 afterwards so you definitely want to                                 keep it to remind yourself of the bucks                                 you got for your cat and it's actually                                 is on print on Amazon as of this morning                                 so don't don't don't worry about it                                 although if you want to give me your                                 email address to spam I always love                                 collecting people's email addresses um                                 there's supposed to be another side but                                 really really that's the most important                                 slide anyways um because my computer                                 just froze and will so yeah you to do I                                 think we're pretty much out of time                                 anyways right so does I Drive time for a                                 question very quickly does anyone have a                                 super quick question for boo no                                 questions okay wait three questions what                                 come on make up your mind                                 so is it often that you see a situation                                 that you want to rewrite your your data                                 frame query into rdd's because the                                 optimizer gets it wrong totally that's a                                 really good question and the answer is                                 not super often most of the time when I                                 see this happen I want to give some more                                 hints to the optimizer because you can                                 you can give the optimizer a bunch of                                 hence the times when I normally end up                                 wanting to rewrite it is this so I'm                                 doing an iterative graph algorithm or                                 something that looks kind of like an                                 iterative graph algorithm or like a CD                                 or something like this because then the                                 optimizer even if I give it all the ins                                 in the world still makes really                                 interesting decisions but it's it's                                 normally normally the answer is give it                                 more hints and then if you get really                                 stuck you can always just go back to our                                 deal and but like it's not something                                 that I find myself doing super often all                                 right thank you Holden cool                                 you
YouTube URL: https://www.youtube.com/watch?v=nZngJBdVK8g


