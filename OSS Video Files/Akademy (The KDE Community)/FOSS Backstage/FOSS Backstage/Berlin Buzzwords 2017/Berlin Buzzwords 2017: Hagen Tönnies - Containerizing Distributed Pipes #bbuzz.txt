Title: Berlin Buzzwords 2017: Hagen Tönnies - Containerizing Distributed Pipes #bbuzz
Publication date: 2017-06-15
Playlist: Berlin Buzzwords 2017
Description: 
	In this talk I will present how we enable distributed, Unix style programming using Docker and Apache Kafka. We will show how we can take the famous Unix Pipe Pattern and apply it to a Distributed Computing System. We will demonstrate the development of simple applications with the focus on "Do One Thing and Do It Well."

Afterwards we demonstrate how we make these two programs work to together using Apache Kafka. By encapsulating our applications in containers we will also show how that enables us to go from the limited resources of a development machine to cluster of computers in a data center without changing our applications or containers.

Read more:
https://2017.berlinbuzzwords.de/17/session/containerizing-distributed-pipes

About Hagen Tönnies:
https://2017.berlinbuzzwords.de/users/hagen-tonnies

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              yeah thanks so much yeah container is                               beautifies I'm hiring you can find me on                               LinkedIn I work for Sony we put                               PlayStation chips in racks and then we                               give you a streaming service where you                               can basically subscribe to PlayStation                               games and stream it to your device and                               so we have quite a fleet of computers                               and what I'm showing here though is just                                a proof of concept that's not actually                                related to what we do in production but                                it's kind of a research thingy that is                                ongoing see if we can make use of                                container writing distributed pipes and                                I have to give special thanks to                                Christian key that's the Twitter handle                                there he supports me a lot with the                                container writing part of everything                                basically but I'm just feeling his work                                and just use it so I give a bit of                                background then we talk about a tool                                chain which should run distributed and                                then we talked a little bit about                                container izing and then we have a recap                                so the background buzzwords first of                                course there was this talk from our team                                clapman way said the UNIX philosophy of                                distributed data and I got a bit of                                hooked and I thought oh that's kind of a                                cool idea can I actually use it and by                                that time he was doing Samsa and I was                                like are now sonzai runs on yarn and I                                needed to do but everything is ok not                                now and and then there were Kafka and                                Kepler came up with this streams API and                                I thought now now I can try it now it's                                every everything is simpler apparently                                so                                I thought okay what's the UNIX pipe so                                UNIX pipe is a process chain where the                                output of one process is the input of                                the next and I guess we all know it but                                I guess we also lost it a bit in the                                industry actually because programs that                                do one thing and do one thing well who                                has written a program lately that does                                this                                really good work together and I know                                working together kind of like yeah yeah                                we have an API we go this but using an                                API always couples our code to that API                                right so the other means basically by                                LNK we're working together in something                                a bit different and I think we need to                                go back and be simpler with everything                                to make that actually happen and then if                                here's something controversial a guy                                from Copenhagen BSD developers said use                                text streams because that's the                                universal interface and there's a lot of                                stuff on the internet that says app                                string oriented programming not my                                saying use types and then you're fine                                well I don't know the guy from the                                    has a lot of wisdom for me so I just put                                it out there because I think it's it's                                controversial and nice ok short recap on                                Kafka it's this thing that you put in                                your infrastructure and in your good                                because you can now shift around you can                                shift around tons of data and you don't                                worry just runs perfectly there are some                                some some problems with that I guess but                                everybody has to figure out my operation                                teams always said we don't have any                                problems with Kafka and I'm happy about                                that and I just believe them and                                actually we have no production outage of                                Kafka it's just just seems to be good                                yeah but you can you can travel around                                data in your organization and this is a                                data hub basically and it's also an                                append lock it looks something like that                                you have a zookeeper ensemble and you                                have a bunch of program that's like your                                cluster then you have the notion of a                                topic that's just a queue where you have                                your messages in then cascade has also                                this notion of partitioning where the                                topics get partitioned and that within a                                petition therefore strictly ordering                                some not too toxic itself is a straight                                ordering but the partition themselves                                they have so here is this what kind of                                petition key are you using and what does                                it mean for your data processing                                pipeline but                                so calcio will do also replications of                                your petition meaning I have one                                 petition then you can say make it three                                 times and then shift it around in the                                 cluster does that for you very nicely                                 actually and then it would look like                                 this here if your broker and then you                                 have your petitions and the copies of                                 your petition so there's also a notion                                 of data locality then of course you can                                 produce messages to the topic you can                                 consume messages from the topic and then                                 here's the nice thing you can actually                                 build groups of consumers where you can                                 then read from a topic and then say this                                 one group a that's your bi team reading                                 your events if you want and then group                                 two is your every team also reading the                                 events but very differently but they can                                 do this now without running into trouble                                 or I have any synchronization between                                 them they just can do this independently                                 of each other here the streaming API                                 just like you should go read the docs                                 that's highly simplification and highly                                 simplified here but whoever has used                                 storm in the beginning or other                                 processing libraries knows we have this                                 notion of a topology how should our data                                 processing pipeline look like and you                                 have it in a low-level API for Kafka                                 streams then you can build custom                                 aggregators you can build custom                                 processes which I think it actually                                 looks very similar to what storm offers                                 you from API wise but then you have also                                 high level API and there you have a                                 notion of streams and tables and you can                                 interchange them because if you have a                                 stream of changes then that actually is                                 always a view on a certain table at a                                 given time the high level API also comes                                 with something flat map and map and                                 reduce I guess and you can have joins                                 meaning if you convert a stream to a                                 table and another stream also to a table                                 and you can go join them and then                                 means a different stream or always look                                 at pairs of data from different Calcutta                                 topics very nice so the stream table                                 thingy looks a bit like okay this is my                                 first change then the table looks like                                 this is my second change my table looks                                 like this third change tail looks like                                 this so you can interchange them or you                                 have to change them streamflow setting                                 recap this is actually I guess from from                                 at equipment store or normal from some                                 talk from constant I all country                                 meddling it so in the beginning there                                 was a patchy storm and I used it quite a                                 bit we actually use it at Sony I guess                                 then we have spark and fleeing                                 I know there are up to                                           processors now actually but these are                                 the most important one they are quite                                 capable meaning the api's are vast broad                                 powerful I don't know if it's actually                                 powerful as they are that big but you                                 can do a lot about this and then                                 simplicity I don't know who had somebody                                 debug a storm topology in production                                 nice experience so yeah it's it's not                                 that easy for spark we had a talk today                                 and it's kind of like you need to know                                 the not source link I have never been in                                 touch with link actually I don't                                 I hope they learned a lot from the                                 former attempts and so everything is not                                 simple and easy and then on that on                                 these two dimensions character would go                                 and be very very simply because if you                                 have a running craft anyways what you do                                 is basically write your stream processor                                 put it in a jar and run the jar and done                                 you don't need like                                                    your Hadoop infrastructure so yeah you                                 don't so coming back to distributed                                 piping so one can think of a pipe as a                                 casket topic or petition depending on                                 how you layout your data and then the                                 function which processes the output of                                 one pipe                                 so in this case the pipe would be the                                 message broker Kafka and a string                                 processing job would be of course a                                 closure applications because nobody is                                 like why would you use any other                                 language and then you can have like this                                 would be like okay we we know this this                                 is Linux UNIX command-line stuff right                                 you do you do kind of this with cat and                                 a WK and less and whatever so this is                                 the goal can we build something that                                 looks like this and T is very similar so                                 to do that I built some tools and I call                                 them distributed because I can basically                                 have this one implementation and then                                 start multiple instances of my tool and                                 yep there will be Lisp lots of                                 parentheses but the good thing is after                                 half a year of doing that the                                 parentheses are no longer exist they                                 just vanish and then everything is very                                 clear so I built this cutter he takes a                                 string and he comes up with the list of                                 tokens and with some error handling and                                 some blah blah blah it's basically just                                 one function right you split anything at                                 the space and then for the string mapper                                 or the string builder it's called it's                                 the does that work if I wanted here so                                 you have the stream builder and then                                 what wrong button you need to do the                                 serialization and configure it and then                                 you say ah this is my input topic then                                 you can do some processing I mentioned a                                 sled math map and then at the end you                                 put it out and you can change the                                 realization here if you want but because                                 everything is string in Denmark we can                                 this is also a just string and then you                                 start your stream and that's about it                                 and I think it's in Java it would be                                 look a little bit different I guess in                                 scarlet would be way tighter and way                                 better and faster of course                                 I prefer this then I've got a heavy                                 hitter which is he takes in this list of                                 tokens and he estimates the frequencies                                 lips what have you hit at us and it uses                                 a count min sketch data structure where                                 you where you have your item and you                                 have a bunch of hash functions and the                                 table size basically and then you go and                                 hash that item through all hash                                 functions and place them at the given                                 position in your table and when you                                 retrieve them you get all your values                                 back and then choose to the minimum and                                 magically there's some F behind it it's                                 actually a good estimate of the                                 frequency of that token and then for                                 having a heavy hitter you just go and                                 take Tom in every X window time if you                                 want and then you have at                                              was my                                                                something like this so he I'm using the                                 low-level API because I need to add a                                 processor and I cheated a bit because                                 there's this tiny function where I said                                 get processor just does that and here's                                 something very cool which is also in Sam                                 so you can have a state store so every                                 time your processor is doing an handling                                 state in this case my data structure for                                 the heavy hitter it goes yeah yeah yeah                                 taking and taking and taking and now                                 zinc and then it spits out that state on                                 to another topic in Kafka and my other                                 processors will actually share that                                 state and so it's the bad new to shared                                 state but I guess if you do it in Kafka                                 it's completely fine and you don't need                                 to worry about it                                 and then yeah you just I guess it's on                                 the next slide here's the processor so                                 you have this init function it's it's                                 very Java raised right you have to                                 implement something here the processor                                 of course you have to do that and then                                 if you're in a function that's this one                                 here and then I set up my                                 have you hit a data structure and my min                                 sketch thingy then I process every value                                 that's coming through actually key value                                 pair sketch put to heavy hitter and on                                 punctuate I go and flush the data                                 structure on to the stream so back into                                 another topic and if the system closes                                 down I can gracefully shut down I forgot                                 I made this then at the end I need to                                 aggregate my results right so because I                                 have a distant I come back and have                                 these alvey's token eight times                                 estimated and                                                       that's not going to work so I need to                                 actually group by key and then count so                                 I need this refi educator I need to do                                 my custom aggregator here because                                 obviously you know cannot just simply                                 add I don't know if there are actually                                 high-level api's that don't require you                                 to implement an ad function but it's the                                 same thing you build your stream your                                 aggregate by key whatever you would like                                 to do you need an initializer engine                                 aggregator here for that serialization                                 again then you put it into a stream then                                 you map it back again I want to create a                                 jason here and then i put it up again so                                 it looks very similar like the first one                                 that's there's not much to it actually                                 it's just the api and at the end I think                                 everything to elasticsearch so I have                                 those estimated aggregated values ten                                 plus eight and then I come up with this                                 JSON structure and then to make more                                 propaganda about closure we steal this                                 goal thingy from the paper set the                                 author but I guess he just was envy of                                 golang so I guess it's that's the real                                 reason we have go loops which is awesome                                 you just consume from a Kafka topic get                                 your Jason's and with an es connection                                 and then just index it that's trivial so                                 it's almost keep it stupid simple                                 it's not because it's double gvn stuff                                 and there are billion lines of code                                 underneath you but from the top it looks                                 simple I think so recap we have the                                 string we have the list of tokens we                                 have the estimated values and then we                                 have the aggregated values and then we                                 end up with a JSON and elasticsearch                                 okay so that was a tooling the pipe now                                 how do we put this in containers and we                                 had to fiddle around with that a lots of                                 meaning we Christian and me and because                                 we had two attempts basically and when I                                 get to know Christian he was always                                 using console for service discovery and                                 I was gonna so complicated why do we                                 need another system to do that I don't                                 yeah but so the first over we will get                                 to that                                 so we basically tried two things how to                                 containerize and we come to that in a                                 minute but first of all recap operating                                 system-level virtualization multiple                                 isolated user spaces I know there's some                                 controversy about what actually means                                 isolated user space because if I am root                                 in a container will be rude on the host                                 and that's evil                                 anyway it's evil to be rude it has                                 nothing to do with a container or docker                                 it's shouldn't be rude you shouldn't run                                 it through so in the old school days                                 because yesterday I had a nice                                 conversation I was basically yeah we                                 have this container and the client wants                                 us to put Windows XP in the container so                                 they contain has some couple of tens of                                 gigabytes big and it kind of something                                 is gone wrong here but nobody notices so                                 the old school was basically yes that's                                 virtualization you have your server you                                 have your host corner then you have the                                 host user land where you do all your                                 magic so that meant stuff and then you                                 use the hypervisor and maybe you've got                                 an internship and then your hypervisors                                 also have a very efficient and then you                                 get the host corners and the user land                                 and the service on top so this is VM                                 virtualization and you know with docker                                 or containers we don't need it anymore                                 was it looks like this we have a host                                 colonel and we have the server and then                                 we still have the userland of the host                                 and maybe we have a service they are                                 like log collection or metric collection                                 or something but then we can have all                                 our containers with their own user land                                 and air service implementation and yet                                 so it's simpler and we all love                                 simplicity and you get rid of the whole                                 bunch of lines of code right because                                 there's a user let missing and a                                 hypervisor missing and a second instance                                 of the host color is also missing so I                                 think it's worth it now                                 the execution the execution of me as a                                 Java developer for some couple of years                                 now                                 Java - jar and then talk I run I don't                                 even need this ie I guess I want to put                                 a minus RM minus minus RM but it looks                                 very similar for mutant I execute                                 something and with docker I can just do                                 that and with a composed syntax which is                                 this yellow specification it still looks                                 something similar to just the command                                 line so I think it's actually good                                 looking yeah so for the development                                 setup our first attempt how do i as a                                 developer work with distributed pipes                                 and their applications so everything                                 needs to run on my one dhaka team and i                                 have on my laptop and then here's those                                 consoles yeah how to how to tell Kafka                                 about zookeeper and how to tell the                                 streaming application about my kaskell                                 zookeeper and elasticsearch without                                 having a domain name really or an IP                                 address of course I can configure the                                 network so that it binds to my host but                                 then what happens if I add more docker                                 engines to it okay consul to the rescue                                 there's a key value store you start up                                 you register yourself everybody else                                 asked yourself you put in templates in                                 your container then use console template                                 language to render your configuration                                 and there you have it and you block as                                 long as console is not absolutely will                                 council first and it was a start                                 zookeeper then you will start the                                 brokers and they come come up as a                                 cluster and you go and add elasticsearch                                 when you have more or less exertional                                 they were joined a cluster and then                                 afterwards you spawn out your individual                                 applications yeah of course you need to                                 set this chakra home spinning nothing                                 unusual                                 I guess composed some sects is quite                                 clear you want to expose some ports with                                 compost too and I get it still working                                 with Campos three specification you can                                 actually extend the base thing so in                                 that base yema there's a network                                 configuration it's not shown here again                                 you load up your systems                                 yeah and that's actually nice you load                                 up your system but still it's only                                 processes right you don't there's not a                                 Linux running it's just the process and                                 your host is providing the kernel okay                                 so that looks like that it was looking                                 on my laptop then you add the                                 applications we had a couple of minutes                                 ago very simple you have a base image                                 with Java and then you add your jar and                                 then you say entry point when Christian                                 was sitting here he would say basically                                 hugging that stupid don't do that in the                                 entry point don't do that bad practice                                 so everybody I have said that that's bad                                 practice I like it because I still see                                 what I'm doing and there's no hidden in                                 it                                 FH script in my container mounted                                 somewhere that it does it's magic or I                                 inherit and then I don't know what to do                                 or I need to relook into that container                                 and fix it and see what's my shell                                 script doing for me this is simple and                                 then to just get the container out there                                 we just okay I have my IDE I have my                                 builded my jar and then I check my                                 container I log into docker and then I                                 publish my container we all do this                                 without signing of course and we don't                                 have our own registries or find it's all                                 secure                                 don't wear is here so the complete tool                                 chain now looks like this and if you get                                 rid of this and just use the command we                                 are still at the command line interface                                 from what we just need to now say images                                 host name container name I just do this                                 actually I think you can get rid of it                                 if you want to but I think it kind of                                 works because I change together my                                 processes and because I did my programs                                 in that way that I can have the input                                 topic and the outputs are baked so I I                                 wouldn't model the pipe in here in                                 between these specifications right I                                 have to still do string args to say                                 where you're coming from what you're                                 reading it and where you're writing to                                 but then in the data center setup we                                 want to now have more talk engines and                                 want to distribute the containers across                                 more talk engines and now we figured out                                 we can something we can do something                                 else with compose                                                    launch services in program and maybe we                                 don't need console anymore because we                                 can now use some something inside the                                 swarm engine that gives us the server                                 name or the service names and the task                                 ID to identify a certain service and                                 that's what we do in the version                                   compose file yeah so you join you build                                 a network you then be on the network and                                 then for deploy doctor services you can                                 actually say how many replicas do you                                 want and what's the resource constraints                                 for any given service um here's another                                 thing where you can have the update                                 consequences when I do a rolling upgrade                                 of your service what would be the                                 intermediate delay and what's the                                 parallelism so how many shutdowns I do                                 at one given time imagined                                 this live feed fake producer would be                                 there                                                               rolling upgrade and it would go one by                                 one by one or it can go I shut                                 everything down and inside everything                                 you again you have control over that                                 with compulsory detention and then you                                 have a restart policy do you want to try                                 once and then say forever do you want to                                 always when you die you get restarted                                 again your control over that I think                                 it's very nice and then in the data                                 center it would look something like oops                                 sorry I create your network deploy the                                 stack and I've splitted my stake into                                 the back ends the distributed pipe the                                 Caprica thingy and the stream processors                                 and the front end so you can have both                                 running and because in that                                 specification they share the same                                 attachable network they all see each                                 other and they work together okay                                 having the recap it's basically a pipe                                 and a function can be expressed as a                                 message broker and a string processor                                 and you can put them all into Tokra and                                 then be happy now there are some things                                 that that I think are for tooling it's                                 good enough but I would never try this                                 actually in production I guess and I'm                                 fine with my I saris holding off and                                 saying hang on to that                                 so yeah the counting how do you count                                 something when you see something you                                 don't know how often you see something                                 that's how do you count what's ten now                                 this is ten eleven is ten twenty ten one                                 hundred something has gone wrong your                                 broker would be just restarting you                                 upgrade the codec if you were rolling                                 upgrade and boom ten thousand messages                                 are duplicated it said now ten thousand                                 oh one I estimated                                                      data here but exactly once is coming                                 there are some transaction API for Kafka                                 I saw one talk about it                                 didn't fully grasp but I guess it's                                 coming and that's obviously something                                 that fling provides you and yeah for                                 doing some exact calculation field you                                 need exactly once and it's not like                                 shouldn't be like that                                 so going from a laptop to a data center                                 you still need capacity planning there's                                 no way that you go I just like how do                                 you partition your data how many                                 processes can do what                                 where are your blocking where you                                 waiting on what you still need to do                                 that it's not like you like it it's a                                 difference if you try to process                                    terabytes or                                                          how many brokers do you want to apply                                 and how much stream processors do you                                 need and which stage is very costly and                                 whatnot and that's still not trivial and                                 there's still nothing I can see that                                 that helps you better or do it                                 automatically and testing and debugging                                 of course it runs on docker you can have                                 integration says you can have your                                 colleague and you build a docker swarm                                 and then you can go a bit bigger and                                 then you can take the data and then you                                 can look at it even and blah blah but in                                 production what about this consistency                                 in the state storage so it has the state                                 storage and it's a multi bi case by                                 default it's a key value store and then                                 it gets swing over the network and the                                 changes will be applied to all other                                 instances but it still kept that kicks                                 in and then it's still kind of like how                                 do I know in a production scenario that                                 everything is correct then it's not                                 trivial to debug in the you have now a                                 nice API but doing that it's necessary                                 processing time versus ease and time are                                 you sure that what you're counting here                                 is happening in the same time interval                                 or are you looking at something that the                                 event claimed yesterday or was created                                 yesterday but I'm now counting it in                                 that window of now does that make sense                                 so you need to be aware of where do I                                 put in my processing time when when do i                                 time stand like this is now and this is                                 for me now and then these                                 is yesterday or today like that there                                 are some tricky thing is that can go                                 with wrong with your sprinkler system                                 and what about Amdahl's law so yeah sure                                 I can build a hundred topics and cuff                                 that would happily do that and I                                 couldn't also process and through a                                 hundred car topic and it will heavily do                                 that but now I'm doing this a cricket by                                 key what's now like is this a single                                 point of aggregation and everybody wait                                 and hold off until my one job gets                                 together and basically has everything or                                 do we build these graphs where you go                                 and okay that's the first map step then                                 you reduce then you reduce further then                                 you reduce further are you need to                                 rebuild this now with kafka streams or                                 Kenya it does it just magically works I                                 think at a certain size I'm not law will                                 get you because there will be threads                                 waiting to get get the aggregation or                                 the final step done and then docker                                 volumes so you have a bare-metal hosts                                 and then you say yeah that's my root                                 server big data directory and everything                                 every container you build mounts into                                 that and puts in the application name                                 and then you write to the disk and the                                 container dies eventually he will be                                 back up again and hook to that same                                 volume and everything is good but for                                 elasticsearch does that work does it                                 actually will it pick up and then the                                 indexes and and then will elastic search                                 happily do the reallocation of the                                 shards again because it has now in                                 different note ID running on the same                                 volumes with the same I guess not                                 and then Kafka also can you like no so                                 docker volumes and like container rising                                 these these data systems still has some                                 issues and it's there were one idea I                                 don't know if it was in December or                                 January where somebody said basically                                 all our volumes now live in zest so this                                 is it                                 also distributed file system a bit                                 quicker than HDFS and then they would                                 put their volumes in there and they                                 could pinpoint the volumes to docker                                 container IDs and then when a container                                 dies we will come up at a different node                                 and then we'll know my volume is F and F                                 goes out I'm actually in your wreck here                                 you have your volume and that seems to                                 be a promising idea so I was a bit so I                                 think my my little example didn't work                                 for me I can do that and I will do that                                 for not production stuff but if I need                                 some big data processing cleaning up                                 some data I will actually write a small                                 stream processor and use my taka stack                                 here and then work with that in that way                                 but more importantly I think what it                                 also shows is you just need something                                 and you not often you need really Hadoop                                 I mean most companies have it but are                                 you really needing it for what you are                                 trying to solve here or just using it                                 because it's already there so yes                                 I'm not against a dig at all maybe it                                 comes around wrongly but it's just that                                 in order to have a simple system there                                 are lot of lines of code in the little                                 snake and it's not exactly a fresh                                 technology by now anymore so maybe you                                 don't need and maybe you can't can do                                 something with it without it and then                                 other Einstein also things that you need                                 to do the stuff needs to be simple but                                 not too simple I just let that sink and                                 then what what I told me at my                                 university was the sky very more calm                                 the fuels are some assumption right we                                 tend to do everything based on a lot of                                 assumptions and clean that out get rid                                 of it like you really if you look at                                 something if you want to have that fire                                 here it's                                 removing that fire there's no need to                                 spin up what's that called it's called a                                  laughter I heard you can shift                                 files around from one this to another or                                 spring boot you can use spring to copy                                 as files around yeah you can do that but                                 you shouldn't and wherever you think of                                 like how that's complicated maybe your                                 feeling is exactly right at that point                                 and yeah don't do it then we have enough                                 lines of code and they bothering all our                                 day it's like it's not worth it just if                                 you don't like also but now I'm getting                                 off the rails of it if you have a                                 five-week working week like five days of                                 work spend a Friday not committing a                                 line of code that would be your Friday                                 go not adding code just too much of them                                 so we have five minutes left but I guess                                 it's done thanks for this                                 why are any questions yeah so we have                                 some questions we have exactly five                                 minutes so there is some time first one                                 thank you for the talk first question is                                 have you considered using actually UNIX                                 pipes when when running lots of                                 container locally and maybe you don't                                 need containers because you have servers                                 with tens of processors and you can do                                 lots of parallel things just in one                                 single server the exact questioning yeah                                 no I get teached by my injuries                                 engineers all the time how to do not                                 that just do it on our house we have                                 pretty big holes and yeah it would work                                 the problem is we already put everything                                 in Hadoop and Kafka so I would need                                 something Bosch Kafka tale maybe a go                                 language                                 goldang program would solvent now                                 absolutely right of course for some of                                 this stuff you don't even need this but                                 like in some organizations we have this                                 data pipeline already here and you just                                 hook into it and you can maybe gain some                                 simplicity by using cough cough streams                                 Gracie thank you very much                                 [Applause]
YouTube URL: https://www.youtube.com/watch?v=-hlblcL46cQ


