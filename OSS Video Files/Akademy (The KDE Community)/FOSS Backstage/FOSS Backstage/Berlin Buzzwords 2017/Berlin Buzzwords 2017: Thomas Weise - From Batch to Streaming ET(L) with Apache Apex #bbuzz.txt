Title: Berlin Buzzwords 2017: Thomas Weise - From Batch to Streaming ET(L) with Apache Apex #bbuzz
Publication date: 2017-06-15
Playlist: Berlin Buzzwords 2017
Description: 
	Stream data processing is increasingly required to support business needs for faster actionable insight with growing volume of information from more sources. Apache Apex is a true stream processing framework for low-latency, high-throughput and reliable processing of complex analytics pipelines on clusters. Apex is designed for quick time-to-production, and is used in production by large companies for real-time and batch processing at scale.

This session will use an Apex production use case to walk through the incremental transition from a batch pipeline with hours of latency to an end-to-end streaming architecture with billions of events per day which are processed to deliver real-time analytical reports. The example is representative for many similar extract-transform-load (ETL) use cases with other data sets that can use a common library of building blocks. The transform (or analytics) piece of such pipelines varies in complexity and often involves business logic specific, custom components.

Topics include:
- Pipeline functionality from event source through queryable state for real-time insights.
- API for application development and development process.
- Library of building blocks including connectors for sources and sinks such as Kafka, JMS, Cassandra, HBase, JDBC and how they enable end-to-end exactly-once results.
- Stateful processing with event time windowing.
- Fault tolerance with exactly-once result semantics, checkpointing, incremental recovery
- Scalability and low-latency, high-throughput processing with advanced engine features for auto-scaling, dynamic changes, compute locality.
- Who is using Apex in production, and roadmap.

Read more:
https://2017.berlinbuzzwords.de/17/session/batch-streaming-etl-apache-apex

About Thomas Weise:
https://2017.berlinbuzzwords.de/users/thomas-weise

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              apache apex is a stream processing                               platform actually and so maybe first                               clarification regarding what is the ETL                               doing in the title right so ETL is                               something that extract state are does                               transformation on the data and then                               loads data somewhere right and this is                               very broad actually it applies to simply                               mirroring data from one database to                                another but the transformations can be                                also very complex and this is the type                                of use case that I'm going to talk about                                here so it's not a simple move data from                                A to B but actually take the data from                                somewhere then does some do some more                                processing with it and in this case                                actually not loaded into a target                                environment but use the data directly                                for visualization in the front end so                                where does Apex fit in you can see here                                that you have sources of events                                continuous streams of data mobile                                devices log sensors and so on you have                                systems that transport the data and                                Kafka is the most prominent one store                                and also transport the data there are                                other message queuing systems and those                                are the most common sources for stream                                data applications and then you point                                what the process the data and there are                                several options out there we just had                                presentation about spark there are the                                frameworks out there strong flaying and                                so on and really many choices just in                                the Apache Software Foundation alone so                                apex is a native stream processing                                engine and a framework - it has api's it                                has a library and it runs on clusters so                                right now it's a Hadoop based system                                that means it requires Yann how to be on                                as a resource manager just to get the                                compute resources it is also using HDFS                                for some storage needs but the platform                                itself the streaming of the data between                                different processes that is part of                                Apache apex and you can define your                                pipelines as direct                                a cyclic graph and that means you can                                take these smaller building blocks that                                we call operators and arrange them in                                various ways the year you see a simple                                sequence but most applications wouldn't                                be that simple that you built with Apex                                they would contain branches and joins                                and to express more complex logic and                                then the API there is a lower-level                                deck API in case you're familiar with                                storm you compose you take building                                blocks and you connect them with streams                                and then the other style of API that you                                find in spark for example is declarative                                API so fluent style API you would do                                your chain different calls and you                                define implicitly that a dak dak                                representation is present anyways in                                those systems just the style of                                declaration that is different and                                different style of API is actually                                preferred for different type of use                                cases and users as well apex is also                                Java based by the first iteration of                                allowing of supporting SQL has also come                                in in the last year and that's based on                                Apache color side so the operator                                library is pretty comprehensive it                                covers the connectors for the systems                                that you see here and more of those and                                just like you need a connect connector                                to read data or to write out data read                                from sources write to things what you                                really need them to express what you                                wanted to do or transformations so and                                some of those transformations are of                                course also built into the library most                                of the time data comes from a streaming                                source or from files which is also very                                common and then it goes into databases                                and those are the results and they are                                stored in databases for downstream                                pipelines one more though we see                                patterns there you have some smaller                                pipelines or microservices and they are                                connected with Kafka queues for example                                or what I believe will also happen more                                and more and you hear this mentioned in                                the last                                year also Kerbal state stream puzzles as                                the database their various names for                                those things but the idea is that you                                 have a stateful platform the memory that                                 you already have allocated to the                                 platform can hold data and if you can                                 get the data from there and use it for                                 visualization directly that's also a                                 nice idea so the use case that I want to                                 talk about is from online advertising                                 collection of impression and click data                                 and then aggregation of the data and                                 making it available for users to users                                 through dashboard and that is exactly                                 the case that I just mentioned where the                                 data is not first written into a                                 database but it's a real-time dashboard                                 where the user wants to see the data                                 basically as it's being computed the                                 reason why such system is needed in                                 attack the round-trip time from when an                                 event occurs in the front end until a                                 business user has insight in campaign                                 performance is critical so this is not                                 milliseconds but it's it should be fast                                 it shouldn't take five hours which the                                 system took when it was implemented as a                                 batch of pipeline first so there are                                 several components of the system that                                 the user or the company has implemented                                 the first part which is the focus here                                 is the real-time reporting get the                                 streams capture the streams to                                 aggregation have the data for time                                 series and have it available for one for                                 real-time reporting on monitoring                                 monitoring also to to define alerts that                                 is another use case then also for the                                 real-time learning to take the data that                                 is being aggregated and computed and                                 feed it back into the machine learning                                 and the final part and that's the most                                 important part is actually to be able to                                 take the data back into the allocation                                 engine which is where the decision is                                 made which ad will show up on which                                 website and that of course affects                                 the bottom line of the business what                                 what we bid for and how much we bid for                                 right so the transition from the batch                                 to the food streaming was done in stages                                 and that was because not everything                                 could be changed at one time the first                                 cut was to leave the e part of the ETL                                 there as it is that means the data comes                                 from files and just change the                                 transformation part so transformation                                 part means we move MapReduce jobs that                                 are in defined in service orchestration                                 chain together multiple MapReduce jobs                                 and other things take that away plus the                                 part that loads the data finally into a                                 database and replace that with the                                 parsing in Apex so that is the middle                                 part that took it to                                                     still a lot of time right                                                it's better than                                                        of a typical day let's take it a busy                                 shopping day right when you know                                 something within                                                        that well I can still make a change and                                 rest of the day might be better but if I                                 know only after                                                         not going so well then probably the peak                                 shopping day is sort of wasted for that                                 particular campaign nevertheless that                                 was an intermediate step it brought the                                 better transform it brought the                                 dashboard already with the real-time                                 reporting but it wasn't enter and                                 streaming yet and that was really cool                                 so the enter and streaming required them                                 to take out this extract thing that was                                 there that would take data actually form                                 a stream because it appears as a stream                                 the clicks and the impressions                                 that's a stream that that you have                                 naturally but what had to be done to                                 make it work earlier was the batch                                 system is to chop that stream into files                                 files that are I think it was                                            worth each each in                                                   chopped off stored into s                                               from there so that had to go go away and                                 the stream metric capture directly and                                 fed into the stream processing pipeline                                 so here this was the                                 first part and you see there's no                                 connection between the front end these                                 are the ad servers on the left side then                                 the data through rest box he pushed into                                 Kafka clusters and it's geo distributed                                 it's in all regions of the world they                                 have had data that have data centers                                 with those car clusters and then no link                                 between the Kafka cluster and the apex                                 the stream processing pipeline and                                 that's because the link wasn't there the                                 link was really the chopping of files                                 and the batching so I think they use                                 Camus and then Camus would pump it into                                 oh it would go into a MapReduce job in                                 the MapReduce job with dump it into s                                  and once it's in s                                                      scanner running and picks up the files                                 as soon as they show up they will be                                 processed and it will be turned back                                 into the stream right that's the irony                                 and but here you can see the processing                                 what happens the file reader then the                                 file reader will after the file read or                                 somethi compress and parsing will happen                                 some filtering pre aggregation shuffle                                 finally the full aggregation of the                                 partial aggregates and the in-memory                                 store and from the in-memory store the                                 results can be accessed of to the front                                 end so this myth box that is labeled                                 middleware as a as a front-end server                                 that has HTTP interface for the                                 dashboard and the back end is really                                 Kafka in this case then when you see a                                 widget on this screen and this will be                                 the this year I show what the end result                                 is first before I keep on talking about                                 it so you have this dashboard you can                                 see time series you can see top endless                                 of the data that you are interested in                                 you can filter and you can also define                                 time time arranges right so when you use                                 on growth here and open the dashboard                                 for particular combination of keys then                                 a query will automatically be placed                                 into this Kafka topic and it will be                                 received like any other data from Kafka                                 by the FX application and it's a pops up                                 mechanism as                                 somebody registers interest for a                                 particular data point or for time range                                 and key combination and everybody could                                 look at different things at the same                                 time and this dimension store operator                                 keeps track of it and sends periodic LED                                 responses and there's a keepalive on it                                 so if the browser gets closed and the                                 user goes away or just closes the laptop                                 that we don't do unnecessary work it                                 will just stop after some time there's a                                 countdown but this is how we can process                                 the queries pretty fast and it may look                                 very complicated well it has to go                                 through many hops front-end server Kafka                                 and then into apex out of AP expects                                 ROKAF back into wanted server but the                                 entire run to actually is only like                                    milliseconds and the benefit is that you                                 get to see the data immediately so once                                 there is a change here in that in-memory                                 state which happens all the time                                 impressions new clicks they're getting                                 aggregated so within second within a                                 second or so you can see changes right                                 on the front end and you get this nice                                 feedback loop working so the second                                 phase was a move away from s                                           Kafka right now                                 consume the Kafka topics directly why                                 was this not done in the first part it's                                 actually the worst challenges right                                 first of all at that point this was in                                                                                                        was work that needed to be done on the                                 Kafka connector in Apex to to make it                                 read from multiple clusters because the                                 multiple Kafka clusters and we don't                                 want to waste resources by having a set                                 of connectors and partitions for each of                                 the Kafka clusters because they have                                 peak times at different days in the                                 world and people sleep in Asia they are                                 awake in the US and they were interested                                 in even resource utilization rather than                                 allocating extra so that support had to                                 be added and this was also Kafka                                     still right                                                       challenges with the stability of the                                 Kafka class so so it was tricky because                                 when we go in here this potentially                                 affects the answer was if they cannot                                 push the data out anymore and there's a                                 backlog then it's not a good situation                                 because the real                                 the real work that they do is serving                                 ads not to produce logs for data                                 aggregation which is very important but                                 it cannot disturb the primary function                                 so the switch took a bit longer and                                 happened slower but the rest of the                                 pipeline remained the same right and                                 then finally we got rid of s                                      everything is Kafka based and we got                                 this nice end to end streaming a                                 pipeline also some interesting learning                                 was an Amazon unpredictable behavior at                                 least at that time with network                                 performance which really showed up with                                 a shared environment really yes young                                 controls young locate CPU and the                                 allocate memory and that's all good but                                 the network we saw really surprising                                 things happen that later and this when                                 some of these things are moved to                                 unprimed they magically disappeared so I                                 showed the dashboard already how that                                 looks like and here's some more detail                                 about the transformation pipeline so                                 data coming in from Kafka for efficiency                                 multiple log entries are actually                                 batched together combined together and                                 compressed into one Kafka message so                                 that needs to be unwrapped on this side                                 here so he comprised path split and then                                 enrichment to inject additional data                                 from lookup source into the data triples                                 and transformation and then the                                 aggregation finally pre aggregation to                                 shrink as much data as possible while                                 it's in this parallel pipeline I chose                                 three but I think in the final                                 configuration there were                                             pipes so we try to do as much as we can                                 in this fused or chained pipeline before                                 we have to hit a first shuffle we goes                                 to shuffle is expensive everything has                                 moved as we moved over the network so                                 the shuffle happens after pre                                 aggregation and then we have keyed data                                 so everything that belongs to one key                                 goes to one store I show one store this                                 actually there are many of those there                                 are                                                                     ingest and pre aggregate pipes                                 and there are                                                     already explained what this year is the                                 visualization this front-end server that                                 is interacting with calf you to                                 push Curie's in and retrieve results so                                 what is the aggregation doing think of                                 it as when you have a data warehouse you                                 might know fact tables and dimension                                 tables and those type of things right it                                 really is a matter of defining certain                                 key combinations that we want to                                 pre-compute                                 a for the reporting that means we don't                                 go to a relational database and do a                                 giant drawing over many many rows of                                 source data the impressions and the                                 clicks are not stored here all right                                 we are just storing aggregates the data                                 is really reduced a lot it's B if the                                 input is                                                                then the updates here or the data that                                 is being sought is much less just the                                 aggregate is not the source data so we                                 have these different dimension                                 combinations for example one is here                                 time is implicit based on the time stamp                                 then we have time and advertiser those                                 are the yellow columns then we have time                                 and location and we have time advertiser                                 and location those are the combinations                                 this is just an example of course there                                 are also different time pockets that are                                 being computed not just hours there are                                 minutes and there are days and so on so                                 you got all these things basically you                                 can show in here as a flat list and                                 actually in memory it looks like that                                 the in-memory store think of it like a                                 hash table hash table is composite keys                                 and when lookup has to happen we know                                 the time range we know exactly for which                                 time bucket to look for the keys will                                 tell us let's say we are looking for                                 subway we will have that key and we will                                 have a location maybe or maybe not the                                 point is we know these are just lookups                                 in the hash table then a new event comes                                 in we know what we have to update so we                                 know the advertise we know ok                                 we know the time and then it's a matter                                 of updating the metrics that are the                                 blue columns and then these are                                 available for reporting there's no                                 computation that needs to be done when                                 the data is retrieved into the front end                                 so scale                                                              data centers the all the systems they                                 collect a lot of stuff a lot of data                                    petabytes of data what is more                                 interesting here's                                                      that move in in a day and those are                                 about                                                                   billion                                                               individual bits right to analyze those                                 two and then average data flow of                                    days of                                                               said this is handled with                                             partitions to do the Kafka read                                 decompress filter enriched pre aggregate                                 and then                                                         instances that just keep the data in                                 memory so when I say keep data memory is                                 not just leave it there and and and it's                                 good of course things can fail right                                 process can go down this continuous                                 processing system continuous operator                                 models so the data has to be there when                                 it fails so the data is check pointed                                 it's safe periodically but it's after                                 the after the aggregation it's small                                 enough to actually do that while with                                 the source data would be very difficult                                 a lot of storage systems they max out is                                                                                                          scale right and in this case there's no                                 need to save all the source data and if                                 if there was then they are already safe                                 there in the Kafka topic you don't need                                 a database for that and total memory                                 consumption is                                                           pipeline so different processes that                                 contain those building blocks those                                 operators distributed over the cluster                                 and the total memory that those take up                                 is                                                                      epic use for this it's providing state                                 management and fault tolerance which is                                 needed for exactly this function to                                 serve the data out of memory                                 exactly once results semantics that we                                 don't double account since money is                                 involved here it's kind of important                                 it provides checkpointing windowing you                                 can do processing based on event time                                 it's the computations need to be done                                 based on the time stems that are there                                 in the source events they need to go to                                 the right time buckets if I run the same                                 computation tomorrow again I have to end                                 up with the same result                                 so that's event I'm processing then                                 recovery fine grained recovery you can                                 have an SLA because of the way of the                                 Veda recovery work and how you can                                 paralyze the processing if you use it                                 for speculative execution for example                                 not applicable here but you can do that                                 with Apex but then the option to do the                                 queryable state to do queryable state                                 you need the data in memory and                                 accessible directly right so you can do                                 it as a stateful stream processor only                                 processing based on event time I                                 mentioned that it's native streaming                                 with native streaming you can do low                                 latency processing you know micro                                 batching no unnecessary delay and as if                                 locking operations but in this case                                 nothing is blocking time buckets are                                 updated as the data comes in and the the                                 results even the SDS deck rates are                                 being computed you can see the changes                                 in the dashboard it's just constantly                                 changing that was the requirement here                                 on its pipeline processing so the data                                 moves through the pipeline it's not the                                 processing doesn't move it to where the                                 data is but it's the opposite the data                                 moves to the pipeline and because this                                 is done in a streaming way there are no                                 spikes read spikes write spikes and so                                 on this continues flow nicely evens out                                 the resource consumption it's scalable                                 you can add more processes to a Hadoop                                 Aeons a store and they could be used by                                 apex and that can also be done                                 dynamically apex lets you do it and then                                 it has the library of connectors many of                                 the things that I talked about here when                                 three years ago they were not there some                                 of the things actually built out of                                 learning from from that use case the                                 connectors                                 some of the file readers and so they've                                 improved a lot but the basic ideas they                                 basically they are reflected also in                                 ready-to-use building blocks the                                 different today in the library so this                                 is the library some categories                                 connectors for messaging Kafka and so on                                 file read file system in and out by the                                 reading for writing those are the common                                 very common things that people need and                                 do also database reading database                                 writing database some other connectors                                 no sequel of course and then the                                 transformation so stateless                                 transformations simple things that you                                 would also know from ETL tools like                                 filters and all of those things pauses                                 and so on                                 but then also the things that require                                 stateful platform alright and state                                 management and fault tolerance                                 windowing the accumulations the                                 triggering watermarks and so on so the                                 remaining time of when you used to just                                 mention a few things or show how you                                 could build something like this yourself                                 with Apex some of the components because                                 this was really a use case a case study                                 what a custom of the dashboard was                                 proprietary so let's get to some of the                                 details how you would do it if you were                                 using apex so I picked something that is                                 easy to understand for everyone because                                 it's real time consuming tweets from the                                 Twitter developer API you can tap into                                 that API you can create a developer                                 account and you can get                                                 stream for free so you can write your                                 own application and you can do some look                                 at the tweets do some analysis and it's                                 a fun project so this particular                                 application does two things it computes                                 the top hashtags of tweets over a                                                                                                         it computes the sum counts for the                                 tweets so on the top you see the top end                                 it extracts hashtags first then count by                                 key                                 top end and then a conversion that is                                 just necessary to you reuse another                                 operator that is this snapshot server                                 which is the piece that enables the                                 queryable State in Apex those two are                                 windows operation so Kentucky is a                                 window operation on top n                                             key is a Keith windows operation and top                                 n is you need all the need to see all                                 the keys to decide which one are the top                                 worlds right and in the other branch you                                 have timestamp assignment then we                                 compute counts just three starts just                                 three starts is a simple example total                                 counts in a window total number of total                                 tweets in the window total tweets with                                 hashtags and total tweets with URLs                                 three metrics we get there and then we                                 output those as time series into a                                 WebSocket operator and in the end after                                 the WebSocket there is a pub sub server                                 so in if you build your own system you                                 would probably use Kafka in this case                                 I'm using a simple WebSocket server that                                 is connecting HTTP on one side and                                 WebSocket on the other side to tap in                                 the visualization and the visualization                                 is done with Cortana Cortana is actually                                 very nice for such things as fast to set                                 up something and it was made for time                                 series for monitoring in time series and                                 it's really easy to visualize such data                                 and you can use it for tabular data too                                 and so the source code of the                                 application you can find it here I will                                 share the slides after the talk the end                                 product will be this simple dashboards                                 so on the top you see the top hash tags                                 you see on the left side the the hash                                 tags on the right side accounts and this                                 extra column is just there so that I can                                 sort it in Co Farnham that's really the                                 only reason why this label column is                                 here and then the lower panel you see                                 time series and what you will see later                                 when I run it you will see that the last                                 is a minute minute intervals                                 was altered yes these are minute                                 intervals and you will see that the last                                 interval already delight the last minute                                 will always update because that's in                                 basically under computation but we                                 visualize it just as in the previous                                 case we visualize the data as it's being                                 computed so you see the changes okay so                                 in terms of code and this is how it                                 would look like you have a window                                 operator in the apex library and then                                 you set all the different options that                                 you need on that window operator you                                 tell how large the window should be that                                 is happening here the five-minute window                                 you also set you have to define in this                                 case that you want to emit results                                 before the window is complete we do not                                 know want to know after five minutes                                 what it was but we want to know                                 immediately and we want to see the                                 changes for the visualizations or after                                 the current based trigger after every                                    changes is the limit the intermediate                                 result and it will keep on accumulating                                 that's what this option means this                                 actually follows closely how the beam                                 model is defined in terms of windowing                                 so I will not go into details here but                                 this is where you would find more                                 information what these things are and                                 why they are useful and then the second                                 windowed operation is the top and it                                 gets all the key key and count pals and                                 image the top and top                                                thing is then we have the count we have                                 counted we have the top and the data                                 goes into this snapshot server operator                                 and the snapshot server operators job is                                 to get a query and then to emit the                                 result and this result operator is just                                 a WebSocket think that could be                                 WebSocket thing that could be Kafka                                 think it could be anything that can talk                                 to the in this case it's WebSocket                                 because this is the pops up mechanism                                 that is used in this in this small                                 example WebSocket - HTTP and then qivana                                 on the other end - are continuously                                 pause I had that on the screen when I                                 show it again look on the upper right                                 corner you see one-second refresh so it                                 continuously will hit                                 pops-up server and look at the latest                                 data if you have something that is of                                 course push-based you could do that too                                 in different clients but this is                                 horrifying works the Cortana data source                                 adapter and the pops up server they are                                 here in get up to you can look at that                                 so the queryable state parts we                                 instantiate the snapshot server and then                                 we instantiate the WebSocket output                                 operator we need to tell the WebSocket                                 output operator via the addresses of the                                 snapshot server that's a configuration                                 that needs to happen we also need to                                 tell the snapshot server what the schema                                 is so remember upstream was computation                                 of hashtag count and in this extra label                                 column so we need to tell that those are                                 the fields that are available in the                                 incoming data stream and then the query                                 will say which fields we want in this                                 case pores are the same but they could                                 be different so I will I have recorded                                 the steps of just bringing up the                                 different bits and pieces so this is                                 launching the apex application it is not                                 running on the cluster is running as a                                 it was a unit test driver locally so you                                 see these exceptions rolling by this is                                 because it cannot connect to the pops-up                                 server yet so it fails to connect to                                 that address so it pops up server after                                 it started we will see those go away the                                 exceptions and we will see tweets being                                 processed okay so there's some extra                                 logging the other choice you see there                                 are not many tweets right it's just it                                 was just running on the laptop and and                                 also many tweets to consume from the API                                 but you can imagine any other data                                 source that produces more data like the                                 attack use case so here this is just a                                 test with curl to hit the HT with HTTP                                 now the pops up so the pops-up server                                 receives the data from the application                                 with WebSocket you can use curl to check                                 whether the data is available now we                                 will start the Ravana adapter and that                                 will now talk to the WebSocket or to the                                 Pops of server and do what we did                                 manually with                                 before it would put pull the data from                                 there and then you can see in the front                                 end the data updating but you probably                                 see more change than the patent panel                                 the top text they change slower but you                                 see the last two intervals changing I                                 think it's going to go back there this                                 was just a shawl so the multiple                                 components right there's the apex                                 application that would normally run on a                                 cluster in this case one as a insider                                 j-unit driver and embedded mode there is                                 a pops up mechanism that it provides an                                 endpoint that we can talk to from                                 Cortana and then the Cortana                                 sync and then here you see now the                                 updates happening as the data is being                                 computed right it's always the last two                                 intervals the change and then they will                                 be static so that's a way how you can do                                 time time serious computation and                                 aggregation and visualization with apex                                 so apex recent additions and roadmap                                 quickly what has happened over the over                                 the last year approximately there's an                                 apex runner in Apache beam now we added                                 support for iterative processing it                                 means you can do machine learning                                 algorithms too and to prove that an                                 integration was done with Apache Samoa                                 then the SQL supported already mentioned                                 the state management incremental state                                 management that means stateful                                 processing is good but we also need to                                 handle it very cope it's very large                                 state if historical data is involved if                                 you have to keep a lot and have to have                                 an efficient way to store that so                                 there's a component to do that in the                                 library the support for control tupis                                 was also added which enables podcasting                                 of tuples across partitions in a                                 consistent way which is needed for                                 watermarks but also for batch control                                 and then some things on the roadmap apex                                 is native streaming platform and it                                 really started as a with the goal of                                 doing real-time streaming right but you                                 can do batch processing too but there                                 are some enhancements to make this                                 simpler from a user's perspective then                                 support for other cluster manager                                 and also support for Python because                                 there really a lot of people that know                                 Python and they want to use the                                 libraries that are available and they                                 would like to be able to do that in a                                 JVM based environment also so and then a                                 few links and a few minutes for                                 questions hi so my question is if you                                 were to write this application now would                                 you write it in native apex or using the                                 beam API well today I would still use                                 the native apex API because the runner                                 isn't really on par with what FX                                 underneath can do right there is still                                 work that needs to go into Toronto but                                 what you see is these things are                                 converging right the beam model and the                                 semantics you already see them repeated                                 in multiple more than one stream                                 processing framework out there right so                                 the native API is and how beam looks                                 like they are getting closer to each                                 other and I think it will happen with                                 other features too that are more                                 non-functional right the scalability out                                 partitioning both efficiency so you will                                 as of today and I think this is not just                                 the case for apex but also for for the                                 other runner implementations you will                                 still see that the native API czar give                                 you better performance or certain things                                 that are not exposed to being yet or not                                 available so being yet but I think                                 that's probably going to change over                                 time so in other words you don't think                                 beam is really ready for primetime you                                 you'd be doing it on native and well I                                 would I would do it of course I would I                                 would do native because that's what I                                 know right from but from a user's                                 perspective look at your use case that                                 you have in what is your data volume                                 right that's the efficiency matter at                                 all because it's a similar discussion                                 like the latency right we say nginx does                                 so in so milliseconds and the other one                                 only seconds                                 doesn't matter for you that's the real                                 question so what kind of processing                                 logic do you need do you really need to                                 have a very large state it doesn't                                 matter if if the native API is a little                                 bit more efficient in your case so I                                 think that's what I would probably                                 approach it I would not say completely                                 go with native API because there are the                                 portability advantages to that certain                                 users like right for some users is                                 important for others not your pipeline                                 portability if you know how to run                                 multiple platforms in your opera in your                                 operational environment then the                                 portability might be interesting to you                                 right if you only know one thing and you                                 already know that you're going to run                                 spark right then well why not use the                                 native spark API if you decided that you                                 want to use the effects why not use the                                 native epics API and maybe it makes                                 certain things easier hi thanks for talk                                 at the in your work example you had a                                 Pope's observer your own Pope's observer                                 what why didn't you use in                                           because pops-up Silva was already there                                 I would say right                                 we have pops-up operators yes you can                                 you can use probably also Reddy's right                                 fault for this it's it's a demo alright                                 so in the real production application                                 that I talked about it's using Kafka and                                 the reason why is using Kafka is does a                                 cover class already they know how to run                                 Kafka and Kafka is good and in for this                                 so pick pick what is good and so if X                                 has the connectors to use different                                 message process right you can use active                                 MQ any JMS based thing RabbitMQ so it's                                 it's an example I'm not recommending you                                 to use WebSockets                                 okay so thank you thank you for now                                 [Applause]
YouTube URL: https://www.youtube.com/watch?v=mstkxTh18ME


