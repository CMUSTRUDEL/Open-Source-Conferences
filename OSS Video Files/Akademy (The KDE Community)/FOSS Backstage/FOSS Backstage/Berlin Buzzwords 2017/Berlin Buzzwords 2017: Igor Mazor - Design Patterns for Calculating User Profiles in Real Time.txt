Title: Berlin Buzzwords 2017: Igor Mazor - Design Patterns for Calculating User Profiles in Real Time
Publication date: 2017-06-15
Playlist: Berlin Buzzwords 2017
Description: 
	It’s important to know your users’ preferences and behavior in the E-marketplace world. If you can quickly understand who your users are, you can optimize the user journey on the E-marketplace platform by presenting relevant products to the user, and by improving the relevance of search results. One way to leverage user preferences is to calculate a passive user profile based on the user’s interactions with the E-marketplace platform. 

At mobile.de, Germany's largest online vehicle marketplace, we include in the user profile information such as likelihood of a user to select different car colors, price distribution, mileage distribution, etc. The real challenge is designing a scalable system that can calculate profiles for different users in real-time and serve those profiles via a REST API to other stakeholders.

At mobile.de, we reviewed some of the most popular open-source stream processing solutions to consider possible architecture designs for the problem. In a nutshell updating user profile in real time is actually a stateful stream processing system in which the state is the user profile, the state key could be the user ID, and the state update operation can be simple as counter increment or an average/variance update.

This talk is sponsored by our Gold partner ebay tech.

Read more:
https://2017.berlinbuzzwords.de/17/session/design-patterns-calculating-user-profiles-real-time

About Igor Mazor:
https://2017.berlinbuzzwords.de/users/igor-mazor

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              so I would not repeat the title again                               it's quite a long I would just add that                               i joined mobe ela which is part of ebay                               classified groups last year and my major                               goal is to leverage and to bring the                               mobile infrastructure from batch                               processing more towards real time and                               building scalable data products so a                               little bit about mobile a for those                                who've use it not familiar with mobile a                                if the Germany's largest online vehicle                                marketplace                                we have almost                                                          obviously we have also a lot of users                                each day millions of users that come                                into each to our plot from each day and                                looking for their next dream car and                                definitely when you have such a huge                                variety of cars finding to your next                                dream car is not an easy task and for                                that you will actually need a good data                                products and this is the main task of                                the data team at Mozilla to build good                                scalable amazing data products so today                                my focus would be exactly around one of                                the most challenging projects that we                                are working on and it is around the                                concept of the user profile so I would                                first start with some motivation why                                actually we need user profile                                we're from it cam what are the                                challenges that we had that you are                                still having some possible design                                patterns what eventually our                                architectures that we have selected and                                some final notes all right so let's                                start with motivation numbers everybody                                like numbers it's mobile our platform                                generates each day almost                                            events so maybe it's not so much but for                                our companies quite huge amount of                                events and it's great each event                                represents some sort of interaction with                                the user in our platform for example                                when you're looking for specific car it                                triggers ad view event radiating when                                you park a car we have a park event it's                                like adding a car to a wish list so you                                can remember it later but                                            events separately don't give us too much                                insight and we want to see how can we                                derive any kind of insights from those                                millions of events and reduce it to                                something more meaningful so we came up                                with the user profiles concept                                so eventually we take each day thus                                    million of row events and reduce them to                                                                                                    you're probably asking yourself all                                right what what is that user profile how                                it exactly look like how to expressed                                actually it's quite simple the user                                profile is just a set of attributes each                                attribute shows some kind of preference                                of the user towards those specific                                attributes for example an attribute can                                be color price range mileage range and                                each of those attributes we call                                dimension a profile dimension eventually                                we have approximately                                                   we collect for each user each dimension                                represent some sort of like loot of the                                user to favorite that attribute for                                example user had                                                       cars and the distribution of the prices                                between                                                                comes with the following question how do                                we calculate those user profiles why                                actually we need them in real time and                                actually the title here is new real time                                because real time is just by definition                                of the business for somebody real time                                is                                                                somebody else                                                       real time we can just make our life easy                                and make a daily batch jobs that                                calculate the profiles over the night                                put them to a database then we have nice                                rest service some kind of micro service                                that serve them and our life is easy but                                no we were not satisfied with that what                                we wanted is once you take any kind of                                action on the website so you trigger any                                kind of edgy event that it would be                                reflected immediately on your eat                                journey which mean if you had now some                                kind of ad view and you go back again to                                the search page we want that all the                                views that you had before                                           before                                                                  right away on your journey and effect on                                 the search results page in addition                                 imagine a situation that new user coming                                 to our website and he done is still                                 don't have any kind of profile and we                                 will view that profile over the night                                 what we have done is that maybe the user                                 will not come back again so the next day                                 what that profile over the night will                                 help me not at all that's the motivation                                 for the real time all right so some of                                 the challenges that we                                 of so as I mentioned we reduce all our                                 event each day to approximately two                                 million daily user profiles and each                                 profile have approximately not                                 approximately exactly                                            dimensions such as price color mileage                                 now one thing that important to mention                                 here that it is daily profile which                                 means that for each profile for each                                 user we calculate his profile on daily                                 base so if you have any kind of                                 interaction today you will get a profile                                 for today if you will come up to two                                 more days and you will have interaction                                 in the platform you will get another                                 profile for that specific day and then                                 each of those daily profiles we keep for                                 a period of                                                           daily approach simple because we want to                                 give the flexibility to our stakeholders                                 the users of the users of the user                                 profile series the option to say I want                                 a user profile based on the last                                         I want to use a profile based on the                                 last                                                                   the flexibility to apply different decay                                 function between those profiles between                                 those daily profiles so maybe we would                                 like to boost the data of a user from                                 the last week more than what happens in                                 the last month so if you will do a small                                 mass                                                                     days you getting approximately                                     billion records which is approximately                                                                                                         that is not some sort of unstructured                                 data its production data that need to be                                 maintained and updated constantly some                                 of our more requirements were the user                                 profile need to be updated up until                                    seconds from the minute from the moment                                 the user triggers any kind of event and                                 we need to be able to serve the user                                 profile in less than                                                 some of the more challenges that we have                                 in scaling out the system so now we are                                 keeping for                                                           imagine that tomorrow we decide no I                                 want to keep it for                                         no I want                                                             grows the user profile database is gross                                 we need to make sure that our data                                 storage can scale out easily without any                                 kind of down times also for the moment                                 we expecting                                                           for the user profile service but it can                                 easily go up tomorrow for                                               stick                                 others in the company would like to use                                 it we need to make sure our system                                 scales good enough for that without                                 downtimes high availability so imagine                                 you have a system that have two major                                 parts one part is updating the user                                 profiles in real time another part is                                 the one that serves them for us what was                                 important is the serving part we don't                                 want to have any kind of down times                                 there it could be sent the stream                                 processing system the system that                                 updates the profiles failed from some                                 reason doesn't important what kind of                                 reason we still want to be able to                                 continue in serve our profile even if                                 it's not the most updated it's better                                 than not serving it at all and if you                                 intensive calculations so we don't                                 really serve the profile as it is we                                 actually using the basin approach we                                 calculating posterior and prior for each                                 user profile and by that we sort of                                 creating likelihood a prediction how                                 much a user will favor it a car with a                                 specific set of dimensions those kind of                                 calculations are quite intensive and the                                 system need to be able to handle all of                                 those amounts of traffic through the                                 calculations all right so how we                                 actually build such a scalable system                                 that updates profile a new real-time                                 serve them in real-time in a nutshell                                 it's just stateful stream for setting                                 system right so we have our events                                 coming from caster this is our source                                 stream source we have some kind of                                 framework fling the cast of streams                                 sparked right the processing our events                                 and most of those frameworks have the                                 support for keeping the stream state in                                 some some manner and we have the stream                                 state and Street and stead update                                 operations so in the context of user                                 profile the set can be simple as user                                 profile the state key can be combined                                 for example from user ID date and                                 dimension because we would like to allow                                 our stakeholders to request the user                                 profile for specific date ranges or                                 maybe using only specific set of                                 dimensions and the set of debt operation                                 are quite straightforward to the simple                                 counter increment of some sort of                                 average variance update okay                                 but when visiting such a system there is                                 one major decision to do how the state                                 will be managed where the date will be                                 stored and there are two major                                 approaches                                 more traditional approach is to use                                 external storage                                 I call it external storage global                                 storage central storage you choose it                                 but storages such as Cassandra and edge                                 is a popular choice they are distributed                                 and scalable by themselves another                                 approach which is a little bit more                                 already innovative and I see and read a                                 lot of blogs and the confluent guys from                                 castor will give a talk tomorrow about                                 it changing the concept of micro                                 services is to use local storage so                                 local storage could be in the form of                                 embedded key value storage for example                                 rocks DB or any kind of simple in-memory                                 map okay let's dive a little bit deeper                                 so let's start with the local storage                                 option so as much as most of you                                 probably know a distributed stream                                 processing system is just a cluster with                                 multiple workers each worker is able to                                 process a subset of your stream events                                 and what you can do at that point that                                 once you starting a new worker one is                                 starting our stream processing system we                                 can attach we can create a local storage                                 to each of those workers if you have                                 multiple executors on each node then                                 each of these executors will have its                                 own local storage it's not just local                                 storage per machine ok and as I                                 mentioned this local search can be                                 embedded and can be on the file system                                 you choose it what is more comfortable                                 for you what the framework supports and                                 then once a specific worker executors                                 process a subset of the stream events he                                 will keep also and update the subset of                                 the stream state in that specific local                                 storage and the way how we can expose                                 that local storage in real-time is                                 simply by exposing rest endpoint on top                                 of that worker but one important thing                                 to understand and remember that the                                 worker and the rest point will have                                 access only and only to that local                                 storage on the other hand the more                                 traditional approach that we're using is                                 the external storage and the major                                 differences between the external storage                                 and local storage is in the external                                 storage all of the workers                                 subset of the workers can access in the                                 same time your external storage ok                                 so they continuously can continue an                                 updating the external storage there is                                 no kind of separation of their and it                                 could be an edge scenario that for                                 example two workers will try to update a                                 specific user profile exactly at the                                 same time and since our updates are                                 counter update it can lead really much                                 to rest conditions and eventually to                                 wrong state this is important to                                 remember and by the way this is a                                 problem but it can be solved by                                 corrected design and unfortunately I                                 don't have enough time or it's not the                                 focus of my talk today the other                                 advantage of using local storage is                                 decoupling so you can decouple fully                                 between the system that updates the user                                 profiles in real time and between the                                 system that serves the profile so                                 imagine that the system that updates                                 profiles getting approximately let's say                                                                                                        that serves them needs to the handle                                 with                                                                   be quite hard to scale the system if it                                 contains both to the both of those                                 operations together but if you have                                 issue of the system separately you can                                 scale it easily and you can debug it                                 also easily it also removes some                                 concerns for example for high                                 availability if the part that updates                                 the profile fail still you can continue                                 serving those profiles because both of                                 the systems are isolated from each other                                 completely                                 so some comparisons that I prepared so                                 if you look about letting this                                 definitely local storage is much better                                 option because its local storage is in                                 memory it's a file system we don't have                                 any kind of remote calls over the wire                                 we don't have any kind of network                                 bottlenecks like it's typical to                                 databases right so it's will win in that                                 for that one he will win definitely from                                 resource and maintains yeah maintaining                                 a stream processing system without                                 another piece of distributed database is                                 definitely much easier but if you don't                                 get from me all the five points because                                 after all managing the distributed                                 stream processing system it's not an                                 easy task it can be quite challenging                                 you need to monitor in to track you need                                 to make sure that everything works fine                                 so it is a dedicated resources and work                                 however of                                 if on top of it you add another                                 distributed data storage you need a                                 separate cluster for that you need maybe                                 a separate DevOps that you'll support                                 you and help you do that so it's more                                 challenging from resources point of view                                 recovery from failures so it's true that                                 many of the stream processing systems                                 slink and Casca streams and Apache                                 things have different mechanism for how                                 the recovery from failure one is better                                 one is work depend on the use case but                                 there are still not so much stable at                                 least in our opinion as distributed data                                 storages it can lead alpha easily to                                 duplications so let's start with the                                 example of duplications imagine you're                                 processing your stream of events and                                 your state is updated in memory but                                 something happened your entire system                                 fails and before you are able to commit                                 the offset back to Kafka done you you're                                 succeed to update your state but not to                                 commit offsets now once you restart the                                 stream processing system again what will                                 happen that you will reprocess those                                 events from Kafka again and will                                 continue update your state so you will                                 have over counting quite easily right                                 also if your state is big                                             keys in the state it can be quite slow                                 until you can recover from any kind of                                 those failures on the other hand                                 external storages distributed data                                 storages more precise the HBase                                 cassandra for example will build it from                                 the beginning from the fundamentals of                                 high availability and scalability so                                 scaling those out                                 they have much rubbish mechanism for                                 recovering from any kind of failures and                                 don't want to go too much deep into the                                 mechanism of distributed data storages                                 state availability as I mentioned if                                 you're using the local storage you're                                 actually getting one big system that                                 contains all the moving parts you have a                                 one system that contains the state                                 updates the user profile update and the                                 serving part if it fails you lose your                                 profile you use the ability to serve the                                 profile for some time or maybe for a                                 longer time depends what kind of failure                                 with the external storages you can just                                 put your rest endpoints your micro                                 services on top of the external storage                                 you completely decoupled it from your                                 systems that update                                 the profit and you reduce the risk from                                 race conditions okay I mentioned it                                 multiple workers can access the external                                 storage at the same time leading to race                                 conditions and scaling out so don't                                 forget the local storage attached to a                                 specific worker and if your state size                                 continuously growing you can be in a                                 problem because the amount of space and                                 memories that you can have dedicated to                                 that local storage is connected directly                                 to the worker and if the workers don't                                 have enough space capacity your only                                 problem the only way is to stop the                                 process                                 review your cluster but you have down                                 times on the other hand using databases                                 such as edge base with Cassandra you can                                 just add more nodes and scale linearly                                 so for us recovery from failures data                                 availability and scaling out was the                                 most important factors when we designing                                 our user profile system that's why we                                 decided to go with the external storage                                 with the following configuration so use                                 Cassandra as our external storage which                                 is quite typical because it's quite                                 battle proofed we have a lot of big                                 community around it so you can get a lot                                 of support we have                                             keys in our state which is quite a big                                 state to manage and we saw that                                 Cassandra able to manage even much                                 better                                 Cassandra structure allow really fast                                 reads and even faster right so we felt                                 that for us this is the perfect I will                                 not hit the perfect the correct database                                 to use for keeping our state's we select                                 the SPARC streaming as our stream                                 processing system the one that updates                                 user profiles in real time and why                                 exactly because of the near real-time                                 the micro batch approach perfectly fit                                 us we can create a micro basis of ten                                 seconds of                                                            requirements or update the user profile                                 amp until                                                               user have multiple events using the                                 micro batch approach we can actually                                 reduce all the amounts of those events                                 into single event and by that reduce the                                 load from our external storage on top of                                 on top of Cassandra we using akka and                                 Scala and more precisely akka HTTP in                                 our opinion Scala is just a wonderful                                 language for writing                                 kind of asking services scalable                                 services and we really didn't have too                                 much thinking over there I mean we just                                 considering different frameworks as                                 there are HTTP of collateral play but                                 eventually we saw that our cash TTP                                 works quite fine and again I can have a                                 good community so communities by the way                                 always a big factor because especially                                 once he work with open source                                 technologies you would definitely would                                 like to get a nice support and our micro                                 services are managed by a massive                                 cluster and well message is a great                                 scheduler but it's not was like our                                 choice simply when I came to the company                                 methyls already was there all our micro                                 services in the company are managed by                                 mesos we have two data centers for now                                 it works fine I don't have any kind of                                 complaints so to conclude when you                                 designing such a scalable system                                 especially if you have any kind of those                                 special requirements it is really                                 important to understand all the                                 different pros and cons of the                                 technologies of the available                                 technologies                                 it's not easy choice to do right unit on                                 Sand Village good which one works good                                 which one works better and how but even                                 much more important is also to                                 understand your business requirements                                 because it may be that you will find in                                 technology that it's shooting for you                                 but not for the business requirements                                 for example our our requirement to have                                 the user profile always available to be                                 served so in my opinion the winning                                 equation is to understand deeply all the                                 different technological pros and cons of                                 all available technologies which you                                 heard about open source or something                                 that you want to pay for to understand                                 your use case the business requirements                                 and only then you can build your                                 scalable system and even then I still                                 continuously to be honest always doubt                                 myself and checking what kind of new                                 technologies that are checking if I did                                 any kind of mistake if then I redesign                                 something cannot only by doubting                                 yourself you can always continue to                                 learn in finding bugs and improving your                                 system thank you for being here clay                                 [Applause]                                 Thank You Hugo are there any questions                                 to ego oh hello ego so my question is if                                 so you explained in earlier side that                                 when a new event from Kapaa comes you                                 update the user profile in the real time                                 but do you have any kind of snapshots                                 kind of for mechanism like for example                                 if you make a release in this user                                 profile quote and if two weeks you fire                                 realize that there is was a bug in last                                 release and the user profiles in last                                 two weeks updated are like kind of                                 culminated like they are not some very                                 correctly set up so yeah how do you                                 correct them it's a good question                                 definitely reprocessing is not always an                                 easy task especially if your retention                                 policy in Kafka is limited for example                                 for seven days but you need to reprocess                                 two days two weeks we have all the raw                                 data always in Hadoop and also one of                                 the reasons why we selected spark which                                 is now still in going process is that we                                 can easily leverage all the logic that                                 you have and just hook a different                                 source so instead of Kafka we can just                                 use Hadoop so then we can reprocess all                                 the events and push them to Cassandra                                 definitely we will need to be careful                                 there because if you push huge amounts                                 of events right away - Cassandra it can                                 be a problematic but it's a challenge                                 that we are still working with yeah so                                 you can just because of SPARC to have                                 support for many different sources                                 especially like a dupe and we proceed                                 our raw data anyway on Hadoop for any                                 other sort of analysis we can just reuse                                 it and just instead of using Kafka we                                 can use Hadoop to reprocess the events                                 yeah thank you very much you go thank                                 you                                 pick up load
YouTube URL: https://www.youtube.com/watch?v=uVOZbZXeVzY


