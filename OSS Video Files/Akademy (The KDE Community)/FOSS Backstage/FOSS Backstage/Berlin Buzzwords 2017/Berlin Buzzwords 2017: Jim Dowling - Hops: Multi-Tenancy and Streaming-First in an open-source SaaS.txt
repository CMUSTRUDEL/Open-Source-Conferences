Title: Berlin Buzzwords 2017: Jim Dowling - Hops: Multi-Tenancy and Streaming-First in an open-source SaaS
Publication date: 2017-06-15
Playlist: Berlin Buzzwords 2017
Description: 
	Hops is a new European version of Hadoop that introduces new concepts to Hadoop to enable multi-tenant Streaming-as-a-Service. In particular, Hops introduces the abstractions: projects, datasets and users. Projects are containers for datasets and users, and are aimed at removing the need for users to manage and launch clusters today, as clusters are currently the only strong mechanims for isolating users and their data from one another.

In this talk we will discuss the challenges in building multi-tenant streaming applications on both Spark and Flink over YARN using Hops concepts. Our platform, called Hopsworks, is in an entirely UI-driven environment built with only open-source software. We also show how we use the ELK stack (Elasticsearch, Logstash, and Kibana) for logging and debugging running Spark streaming applications, how we use Grafana and InfluxDB for monitoring Spark streaming applications, and finally how Apache Zeppelin can provide interactive visualizations and charts to end-users. We will also show how applications are run within a 'project' on a YARN cluster with the novel property that applications are metered and charged to projects. We will also discuss our experiences running streaming-as-a-service on a cluster in Sweden with over 150 users (as of early 2017).  

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              large yes thank you yes my name is Jim                               I'm actually representing logical clocks                               we have a start-up come                               using this work so we have our name down                               here at the bottom corner I'm going to                               talk so if you've you've heard of helps                               a lot of you haven't                               we're not based in the valley I guess                               you know we're European or European                                distribution Hadoop and we try to get                                the word out there we do our best but                                you know you should have heard of us I                                guess if you read a lot of the the you                                know the blogosphere because we have                                actually the fastest distribution of                                Hadoop by a good margin so we had a                                paper earlier this year at use next past                                where in collaboration with Spotify so                                on Spotify his workload their Hadoop                                workload we worked with Oracle on this                                as well but we had                                             throughput of HDFS the Hadoop file                                system so we got about                                            operations per second and we can have                                clusters that are more than an order of                                magnitude larger than existing to do                                cluster so you know                                                    of files is a reasonably conservative                                estimate of how big we can go recently                                the last couple of weeks we won the                                Triple E scale challenge for this year                                so like we have a bunch of them we've                                done a lot of research over number of                                years to get the the underlying file                                system to this level and the way we did                                it was that if you're familiar with                                Hadoop there's a in the Hadoop file                                system you have something called a name                                node which stores the metadata for the                                file system so we're distributed systems                                researchers what we did was we move the                                main the name node state from the heap                                of the JVM into an in-memory distributed                                database and basically that helped us to                                scale it it's a new sequel database                                called my sequel cluster now I'm not                                here to talk about that today if you                                want to read the opponent you can go to                                our website or we have papers and plenty                                of presentations to look at I'm going to                                talk about streaming on platform so                                streaming as a service in particular                                spark and flink they're the kind of main                                platforms that we support we also                                support cask as a service and we're                                working hard on tensorflow right now so                                this is running in production we're                                running we have a cluster in northern                                Sweden and Lully oh we've got                                          and we have a number of users doing                                streaming right now so I'm going to talk                                about is the journey we had in head to                                head you get to it like a production                                ready                                platform what do we need to add two hops                                to to bring it up to speed so the one                                thing that is interesting about Sweden                                is you know that labor is expensive                                right so cleaners taxi drivers sis                                admin's they're very expensive so what                                we wanted to build was a self-service                                streaming platform and that means that                                users like this is Obama here in case                                you're wondering if it is we want to                                help him to spark up by himself with a                                bit of slinky that should be Flint but                                flink whatever okay bad joke                                so the basics of a stream processing                                platform are that you're going to have                                some streaming engine maybe it's going                                to be flank or spark and there's other                                ones like data torrent product apex                                which I heard it talked about earlier                                and but they're going to take data in                                there's going to be ingress some pipes                                bringing data in from its source and                                then you're going to write your data out                                somewhere probably to a sink but there's                                more services a basic streaming platform                                will have some way of looking at online                                logs so you're going to have to debug                                applications as the Runnings you want                                them to be able to write to some log                                where you can read the logs as the                                streaming app is running similarly you                                want be able to monitor that application                                as it's running so you need to be able                                to get notifications if something's                                going around you might want to look at                                 the you know memory utilization CP                                 utilization if it's lagging or not and                                 then your streaming application will                                 also have to support high availability                                 so if it crashes it should restart and                                 it should reprocess any data that it                                 missed and typically streaming engines                                 will will need some form of storage on                                 which to write checkpoints we're also                                 right ahead log so there's a typical                                 mechanisms used to recover from failures                                 you're going to have a right ahead log                                 and then some checkpoints that you                                 recover from and then you may go back to                                 Kafka or wherever to to process any data                                 that you missed so that's a basic stream                                 platform but what we have to do is we                                 had wanted to build a platform around                                 hops since are kind of our clouded that                                 we're building on and we looked at                                 casket basically for the for the pipe                                 where the data is coming in we looked at                                 a lot of different tools for how to do                                 monitoring and we ended up on Griffin                                 and in Flex DB I know there are not lots                                 of other ones like for me                                 and so on I'm going to talk about the                                 ones that we've looked at we looked at                                 ELQ stock the elastic logstash in Cabana                                 for the logging and for doing UI so if                                 you're doing any interactive and                                 analytics of data maybe produced by your                                 streaming platform we're supporting                                 Jupiter and Zeppelin now there are a                                 number of other platforms out there                                 these are all open source stuff so the                                 whole platform is open source so that                                 was a kind of a prerequisite for us at                                 the beginning okay so this is our                                 platform if you if you want to look at                                 it I don't know if you can see that                                 particularly well resolutions a bit off                                 but we're basically supporting casket as                                 a pipe for for incoming data you can                                 write your streaming up in slink or                                 spark we support our HDFS compliance                                 it's a drop-in replacement for HDFS                                 helps FS and we also support your on                                 currently version                                                        support also structured streaming so you                                 can write your spark application to                                 park' and then do analytics directly                                 from park' I'll give an example do demo                                 that later and then finally for logging                                 and monitoring I'm going to talk a bit                                 about elastic and qivana that we use for                                 logging and then Griffin in folks DB for                                 monitoring now we also support my sequel                                 custard the back end so you can use that                                 if you want to as it as I think                                 typically people don't but it's there                                 the other thing that's interesting for                                 from our perspective and I guess from a                                 lot of people in the room is that we                                 have this new law coming into effect                                 next year GDP or and it is law right and                                 it will come into effect then and I                                 think a lot of people are in denial of                                 the implications of it so it has                                 implications for sensitive data private                                 data you have to think about issues like                                 data retention you have to have auditing                                 for the data and you need and then                                 citizens have rights you have right to                                 be forgotten and so on so this has a lot                                 of implications for how you build data                                 processing platforms and in particular                                 you have the issue of privacy by design                                 so that's kind of a requirement to GDP                                 or so how do you ensure that your system                                 will support privacy in its actual                                 architecture so I'm going to talk a bit                                 about how we do that we basically have a                                 number of new abstractions that we've                                 introduced into Hadoop and they make it                                 easier for us to                                 support multiple users on the same                                 platform so multiple tenants that is                                 over multi-tenancy for stream processing                                 so the first abstraction we have is                                 something called Project and what a                                 project is is is basically a github                                 project so if you want of a project you                                 can create us it's quick and cheap                                 degres you can manage the project if                                 you're the owner of the project you can                                 have datasets in your project you can                                 add members your project remove them I'm                                 going to actually split over and do a                                 little demo while we're doing the see                                 you get a feel for what I'm talking                                 about so actually let me go to a new                                 project this one I haven't logged in                                 here so just create new in here so ok so                                 it's actually we have we've got a bunch                                 of Tours I'm just going to close the                                 tour to the beginning but basically you                                 have projects in your platform so if I                                 want to create a new one called Bebo's                                 here you just create it like that we had                                 a number of services and my projects are                                 going to add up here on the right-hand                                 side so a project is the unit of                                 isolation so any programs I run within                                 it will be sandbox within that                                 environment and I won't be able to copy                                 data out of the project if I don't have                                 a role that's allowed to do dance from                                 another data owner and the projects will                                 basically be cheap and dirty so the                                 equivalent of clusters right if you're                                 running any s                                                         running on as you're typically you would                                 spin up a cluster to do this it would be                                 your kind of unit of isolation but this                                 is a managed platform so for us a                                 project is the unit of isolation and                                 because it being within you do your work                                 so when you have a project what we also                                 want to be able to do is share your                                 datasets across projects because I said                                 already it's like a universal ation but                                 what if I if I have a data set it could                                 be very large I'd like to share it with                                 another project I don't want to have to                                 copy the data and typically that's what                                 you would do in a you know a different                                 environment where you were worried about                                 maybe allowing users access the data                                 they might copy it to someplace so not                                 load copier - so if I create a data set                                 let's create two projects here another                                 one here we close up I just turn off                                 these towards for the moment                                 besides creating a second project called                                 hello and I'm going to go in now I'm                                 inside the project hello can you see                                 that there yes okay I think I'll just                                 make it fullscreen okay so what I'm                                 going to do is just create a little data                                 set here I'll call it my my data now                                 that data would set with searchable but                                 I can share it by basically                                 right-clicking on us I can share it with                                 the other project that I created earlier                                 Debose and that's all I basically need                                 to do so it's kind of like Dropbox to                                 share the data set if I go back to my                                 other project this one here Bebo's what                                 see happens is that it appeared in here                                 so this is the data set from below I can                                 click on it to say fine I accept us                                 because I didn't ask for that I have to                                 ask me if I want to include us and                                 that's basically sharing data set                                 security between projects so we could                                 have shared that data set right only or                                 read write or read only but it didn't                                 cost the for each project you also have                                 quotas I'll talk about it in a minute                                 but it's not going to add to the quota                                 of this project it's not going to add to                                 the amount of space that I consume so                                 we're sharing data is not copying data                                 it's basically linking in this case the                                 HTS data set across two projects I'll                                 show later on that we can do the same                                 thing for Casca topics an example now                                 our platform supports Lincoln spark as I                                 mentioned already we also support                                 tensorflow spark and tensor flow and                                 casket we're not we're not supporting                                 MapReduce right now we haven't had any                                 demand for us even though it's obviously                                 power to the platform that works but in                                 the UI we're not even supporting it I                                 don't know this is very little around                                 first so then the other thing that you                                 need to do in if you're going to have a                                 UI driven platform like we have is you                                 have to have notebooks at some levels so                                 we started supporting just Zeppelin and                                 we found that there are a lot of data                                 scientists and files and people who just                                 want to use Jupiter so we spend a lot of                                 effort adding Jupiter to it and then the                                 other way you can run jobs is you can                                 launch them in in a job launcher I can                                 jump back and have a quick look at a job                                 here so now I'm inside a project here so                                 the jobs basically look like these ones                                 here                                 you can create a new job you can pick a                                 name for it pick your platform                                 tensorflow sparks link and so on and                                 that those jobs can then be launched                                 it's like a job scheduler like you would                                 have in a and things like that you can                                 have them launch a particular times                                 schedules there's no way of changing                                 them together right now but it's                                 basically a way for launching and                                 managing jobs okay another thing that                                 pythons surprising is affecting                                 everything we're doing because python is                                 growing so much in use on Hadoop                                 platforms that Python users demand                                 access to different versions of                                 libraries and currently if you're                                 running in a Hadoop style environment                                 and you're running with Scala or Java                                 you can just build a big fat jar with                                 all your dependencies and run it and you                                 have no problems but in Python they                                 don't have a support for building hot                                 jars uber cars you can have eggs but                                 eggs may have dependencies and other                                 eggs and there's no way of packaging                                 them together no easy way for users so                                 what we've been doing a lot of with is                                 adding support for Conda to our                                 abstractions so I don't have you heard a                                 con that counters basically a package                                 manager for Python I'll just go back and                                 show you how it works here so here I                                 have a a particular project I don't you                                 can see it on the screen there's a                                 little bit I can zoom in a bit I guess                                 so I can search for a library in Conda                                 in so I just search for panda and                                 because that's the net and then I can                                 pick my version let's say take the Pam                                 the data reader and then that gets                                 installed so here I have some libraries                                 that were installed around this one is                                 currently installing you can see but                                 panda sequel is installed earlier on so                                 what's happening here is that each                                 project can have their own libraries                                 their own version of tensorflow their                                 own version of numpy and the way we're                                 doing it is that we're actually copying                                 the we're creating a virtual environment                                 on every node in the system so we have                                 an agent running on every node in the                                 system they get commands to basically                                 say create an environment for this                                 project and then they'll get commands                                 like install this particular library for                                 this count environment and when your                                 jobs launch on Pais parkour and                                 tensorflow then they launch within that                                 the environment no other Hadoop distros                                 supports anything like this                                 currently okay I'm going to move on a                                 bit now to I talked about projects                                 already but let's think of it from our                                 abstract perspective and then I look at                                 an example a use case that we have with                                 an IOT company so a project is basically                                 a grouping of users and data so here I                                 can see three different users and four                                 different data sets or data sources so                                 we can have Casca topics you can have a                                 database for example in hive or you                                 could have a subtree in HDFS so if you                                 draw any line around those users and                                 data sets you get a project so that's                                 the only restriction here is that a                                 project sorry a data set has to have a                                 home project but otherwise you can draw                                 any lines around these whatever way you                                 want so that you know when you when you                                 setup in a company you might have one                                 project for every one the company is a                                 member of it with the company database                                 and then when teams want to work on                                 different activities or maybe different                                 departments would have their own                                 projects but it becomes quite a natural                                 way of assigning responsibility and                                 ownership within an organization so as I                                 mentioned we work with Spotify quite a                                 lot and you may know that they're moving                                 towards Google Cloud so one of the big                                 problems they had was lots of orphaned                                 workflows on datasets so the workflows                                 are running everyday here we are                                 nobody knew who could run it you know                                 people who have a stay of turnover                                 within the company the same goes for                                 datasets you know who's responsible for                                 that so in our particular model there's                                 always going to be an owner for a                                 project and it would be quite clear if                                 that person leaves an organization you                                 can find someone to take over that role                                 okay so I'm going to give an example of                                 an IOT platform that we're working with                                 a noisy vendor in in Stockholm and what                                 they're doing is they have have IOT                                 devices there that like you know                                 factories or on cruise ships or lots of                                 different places and they have gateways                                 off site and the the gateways talk to a                                 number of cloud servers to aggregate the                                 data and then they want to pull all this                                 data into a you know a data like                                 platform where they can give access to                                 their users to do analysis of the of the                                 IOT data and so the main requirement                                 here is the multi-tenancy one                                 you could run a different cluster for                                 every company that you're supporting but                                 in this case we want to support a                                 multi-tenant platform world that                                 customers can run on the same single                                 platform now in reality it's going to be                                 a little bit more complex than this this                                 company have customers on AWS on Google                                 cloud on Azure and you need to bring all                                 those together into the same platform so                                 this is the way we do it with our                                 abstractions remember we have these                                 projects datasets and users the the                                 company has a casket topic for taking                                 all the data in from the sensors and                                 that data will flow into this casket                                 topic for every company so give a                                 company acne we'll have a project and                                 that project will have datasets in HDFS                                 but also topic in Casca that you can see                                 there so the first streaming application                                 takes the data coming in and then                                 redirects it to each company's topic and                                 data set and then the company itself can                                 manage access to that data so they can                                 add the users that they want to do in                                 our analytics owners and and so on the                                 company the IT company itself can then                                 generate you know generic analytics                                 reports and sell them to companies but                                 companies will often want to do their                                 own custom analytics which they can do                                 in our platform so the alternative to                                 this if you were thinking I'm going to                                 do this task                                                            would you know we'll write our data to a                                 bucket in s                                                             the company but in that case the company                                 loses control of the data they're                                 basically giving it to the company and                                 so this way the company the IT company                                 retains ownership of their data and                                 access to it so one thing I didn't                                 mention about users and projects is that                                 we have roles now you know when we talk                                 to different users and different domains                                 they all want to have lots and lots of                                 different roles GDP or talk specifically                                 about two particular roles a data                                 controller which we call a data owner                                 and then a data processor so somebody                                 who's doing analysis on the actual data                                 so in our case the data owner is able to                                 import data into a project exported apps                                 so they're responsible for the data                                 they're also responsible for adding and                                 removing people from the project and                                 sharing data or                                 topics in casco between projects data                                 processors our data scientists just do                                 analytics so they can upload jars they                                 can look at their logs that they've                                 generated but they can't copy data                                 anywhere so what's kind of unique in our                                 platform compared to others is that the                                 users do all this management themselves                                 you don't need to talk to assisted men                                 who add some rules in Ranger or in                                 century basically users can do all this                                 work by themselves in a kind of manner                                 that's familiar to them you get style I                                 guess I mentioned this already for                                 projects of quotas so because we have                                 all our metadata in our own database we                                 can extend that as we want to and one                                 thing we added was quotas to yarn so you                                 don't have quotas naturally in yarn                                 we've added them so basically when a                                 container starts when the container                                 stops we we can increment the time taken                                 within a database and charge that to a                                 project and I guess one interesting                                 feature of that is once you have quotas                                 it becomes an easy mechanism with which                                 you can handle elastic demand on your                                 cluster so in our cluster in northern                                 Sweden if it's highly loaded the price                                 goes up and if it slowly load the price                                 goes down and that way users can                                 hopefully expand and shrink their demand                                 based on the the time of day ok so I'm                                 going to look at the the tooling that                                 we've built around helps works or                                 platform so we're building as I said                                 before in our Hadoop distribution code                                 helps to do and we added Casca blck                                 stack and graph a Netflix TV and Jupiter                                 as well as a plan and all of this is                                 actually multi-tenant so you these                                 services themselves some of them support                                 authentication and authorization some of                                 them don't we front them with reverse                                 proxies so reverse proxy servers                                 WebSocket proxies where we can do the                                 actual access control as necessary but                                 if you look at Kafka this is not easy to                                 read so I'll do a demo                                 instead you bit easier okay so let's go                                 to Casca here                                 there's a Crashdown either okay one                                 second here so Google Chrome is appeared                                 to crash let me try us now okay                                 there's there's NetBeans decide to start                                 me there one second okay                                 here we go Kafka all right let me just                                 make it I think it's a full screen thing                                 this is a this is Kafka on in a project                                 the project I created around called bit                                 doors                                 there goes NetBeans okay I admit I use                                 NetBeans okay so what if you're you're                                 running caster you you probably be using                                 something like confluence Data Platform                                 or maybe am or I am but you've probably                                 familiar with the notion that topics                                 typically have a scheme associated with                                 them so if you're going to push data                                 into a topic often it's useful to have a                                 schema so we're supporting the same as                                 confluent and Avro schema registry for                                 topics so each topic let's say I create                                 a topic here called Steve because he's a                                 heckler down the back and I can pick a                                 schema to associated with it and schemas                                 can be versions of course you can                                 upgrade them so so now we've basically                                 created a topic now if your work at any                                 other Hadoop platform that's a                                 non-trivial thing to do you know you're                                 going to probably talk to an                                 administrator to do it for you to give                                 access to other people outside of this                                 project the ability to read or write to                                 that topic would also be a painful                                 experience in our case I can just select                                 another project and that project can                                 then so the project of crater there on                                 that project                                 hello can now read or write to that                                 topic if I wanted to get more funky with                                 the permissions I can get into the ACLs                                 that are supported by confluent it was                                 actually part of casket sales much as                                 confluent but typically we've never                                 experienced anyone once actually get                                 that low-level alright so you can you                                 can do quite a lot with these simple                                 abstractions of data owner data                                 scientist and then sharing topics across                                 projects so if I go back to the low                                 project will see the                                 that topic appears in here so Steve so                                 now I can write projects that can access                                 that particular project if any questions                                 just shout or heckle the school okay so                                 the summer experience with working with                                 Kafka on spark streaming or link it                                 doesn't really matter is that if you're                                 going to provide a self-service you                                 right the question is what do you want                                 users to be able to change what do you                                 want them to be able to optimize there's                                 lots of things in Kafka you can change                                 you can change data retention periods                                 you have quotas now recently in casco                                 where you can specify the amount of data                                 that can be read and written from a                                 topic basically we're only providing two                                 metrics they can tune one is how many                                 topics do I have in my project and then                                 how many partitions do I have per topic                                 and you can go a long way with this so                                 typically if you're building a streaming                                 app often you will try and match the                                 number of executors with the number of                                 partitions that's a pretty                                 straightforward thing to do and then you                                 want to make sure that the data that                                 you're reading and writing or your                                 writing that's being written to your                                 topics is balanced across the the                                 partitions so that no executors are                                 doing a lot more work than other                                 executors that you won't like some                                 balance there so if users get that prior                                 that they can do this and they still                                 need further optimization then you can                                 do it offline once with an administrator                                 but typically this is we found that this                                 is enough to give users for for                                 self-service so logging was the next                                 thing that I mention that the yarn if                                 you work on yarn as your cluster manager                                 and we we use yarn and if you're running                                 a spark streaming or a flink straining                                 job and it's writing to your logs and                                 yarn they will only get aggregated when                                 the application completes                                 so an application finishes and that's                                 pretty useless if you're in it in the                                 yarn environment because you want to get                                 the logs that they're being produced so                                 what you need to do is you need to add                                 some kind of support for collecting logs                                 and we looked at a bunch of different                                 ways of doing it and we we ended up                                 looking at log stash elastic in Cabana                                 and the way to do it if you're writing a                                 simple spark application is you we ought                                 to configure log                                                         that will just like to log stash so the                                 user doesn't even need to think about it                                 they just still log                                                    and it ends up in in in their particular                                 logs                                 that will see an example of later on                                 okay it kind of looks like that and I'll                                 do to do a demo of asana in a minute                                 then the other one is monitoring so if                                 you're familiar with SPARC this is first                                 Park in particular it is quite good                                 support for for monitoring you can                                 supply metrics our properties file when                                 you launch a spark job and what the                                 SPARC job will do is it will write its                                 metrics so things like related to the                                 JVM a heap size usage you can you can                                 extend it to your own add your own                                 custom metrics as well but basically you                                 can configure it to write to JMX graph                                 ice or to serve let's see a sphere                                 console or even a large output so in our                                 case we're writing to a graphite sink                                 plane graphite is actually just in flux                                 DB so there's no graphics or where it's                                 just an infix DB server the other ways                                 of getting doing resource monitoring                                 through applications at least in spark                                 in the latest version is you can for any                                 given spark application a structured                                 streaming application you can write a                                 structured query listener and that can                                 for example write to casket if you want                                 as well and you can inspect from a                                 casket topic or you can write to again                                 the same graphite sink if you running                                 many queries within a single spark                                 session so many structured streaming                                 creator the same session you can use the                                 streaming query listener to do something                                 similar okay I'll do demo this in a                                 second and then we do support notebooks                                 it may not seem relevant but you know at                                 some level you're going to somehow do an                                 analysis of the data coming out so you                                 may have a database or you may use Kafka                                 as a sink and from there then you serve                                 your data to your to your users but you                                 can also do quite a lot with just                                 appling and Jupiter and we support both                                 of them so Jupiter it just to give you                                 an idea of the kind of challenges you                                 have with supporting a notebook this is                                 a web application and if you allow users                                 to run spark in client mode or think for                                 that matter and what will happen is that                                 they can launch drivers on the same sir                                 as your web application server and if                                 you run a workshop with                                                 and they're all running drivers with                                     gigabytes then you can have trouble and                                 we did to begin with so what we did was                                 we moved over to using a rest server in                                 particular for spark called Livi so the                                 driver is never launched locally on the                                 web application server instead it's                                 launched on the iron cluster and if you                                 are familiar with Jupiter there's a                                 kernel called spark magic that windows                                 azure uses we're using the same one and                                 but then we had some issues in both in                                 Zeppelin and Jupiter related to how can                                 we get our notebooks to be you know                                 visible across to all users and across                                 all instances of our web application and                                 the simple solution is to put them in                                 HDFS but there was no content manager                                 for Jupiter of HDFS we wrote one and the                                 same goes for for Zeppelin so that code                                 is available if you're interested in                                 github                                 okay so Livi kind of looks like this you                                 know you basically a REST API and then                                 in our case the Jupiter kernel or the                                 the Zeppelin interpreter talk to Livia                                 and run the Jobson errand yarn okay                                 let's go back and have a quick look at                                 some of those things that I was talking                                 about right so I'm going to do a tour I                                 think just to generate some of them here                                 so I'll do a calf Couture because this                                 is both a bit streaming so what I                                 clicked on will start the tour we have a                                 number of tours rogue we're going to add                                 more tours you can see we have tours for                                 spark and Casca there but I'm going to                                 do the the casket or just to begin with                                 so what it's doing I'm just going to                                 click Next and follow along it's going                                 to create a schema for me an avro schema                                 it's just filled one in for me there                                 it's going to validate the schema that's                                 done and now it's going to create a                                 topic I did this already so I'm just                                 going to click through it you can change                                 number of partitions there and now it's                                 going to create a job so firstly it's                                 going to create a in this case it's                                 going to be a spark producer it's going                                 to write to that casket topic that I                                 created                                 it's just picking out a jar file from                                 the hdfs there                                 that jar file actually had the classpath                                 in its manifest so it picked that out                                 and then these are the configuration                                 parameters for free yarn such that one's                                 creators now going to create a consumer                                 it consumer name the jar file and then                                 the name of the class and you can see                                 here we have the an argument to call                                 consumer all this code is on github if                                 you're curious and then I'm going to run                                 us okay so I'll just run the this is our                                 pricing model that I mentioned the one X                                 and then I'm going to launch the                                 consumer so it's still one X o'clock                                 white lower load ok so they're running                                 now let me just show you the UI so when                                 the job starts running you can basically                                 inspect the spark UI for your job you                                 can go to yarn to have a look at the job                                 you can go to cabana to have a look at                                 the loads being written in what's                                 interesting with Cubana actually is that                                 if you if you can't use cabana to do                                 graphing you know you can use log for                                 data just to write out some state that                                 you're interested in your application                                 and then you can just visualize this by                                 using built in visualization tools in                                 cabana and then we have metrics there's                                 not that many metrics on this side but                                 there's more on the consumer so I can go                                 back to the consumer and see our                                 consumers running here in this case this                                 is SPARC structured streaming so you're                                 going to you're going to have this SQL                                 tab in SPARC structured streaming that                                 you don't have in normal streaming and                                 then the metrics if I go back we can see                                 here you can see a bunch of executors                                 have run and they've written to HDFS and                                 what this is done actually in this                                 example it's writing to a park a file                                 that's this one here or is it this one                                 here and so this this has basically a                                 bunch of parque files in there so I                                 could go and run it on Zeppelin but I've                                 run it earlier I'm running out of time                                 so I'll just show you what I did or oh                                 that was on the crashed okay let's go                                 back here I can log in as admin here I                                 guess it's a design user here                                 okay there wasn't that user other this                                 different user that too many accounts on                                 here okay so yeah just pull this one up                                 so this is basically doing analytics on                                 the on the the parks which structured                                 streaming file as the data is coming in                                 and you can you can even do an eigen                                 angular front-end to refresh that if you                                 want to while it's going in okay I want                                 to just talk a little because I have                                 only a few minutes left to talk about an                                 API we've added here to make it easier                                 to write streaming applications here we                                 go okay so this is a simple spark                                 structured streaming program you'll see                                 here you'll see it on the data breaks                                 blog                                 it basically says you need to have a                                 query you need to have an input source                                 and I put sync and you need to trigger                                 the query periodically you need to say                                 where you can write your check points to                                 and you think okay that's a nice example                                 but there's quite a lot of stuff missing                                 from us for example you know where's the                                 camp where the endpoint - Kafka                                 what are my credentials for connecting                                 to Kali I'm going to do this securely                                 automated schema registry endpoint how                                 do i shut down the application                                 gracefully where do I get my monitoring                                 their properties files so actually what                                 we're doing is hiding all of this                                 complexity inside an API so all that                                 code that you would write here on the                                 left the framework knows about all of                                 these things it knows the locations it                                 has a service registry it knows about                                 the certificates it's copied the                                 certificates out to the to the tasks to                                 the tasks in spark and the yarn is going                                 to clean them up when they're when                                 finished so basically all you need to do                                 is say get me a spark producer or flink                                 producer and then you can work with it                                 and the same goes for the for the                                 consumer so the consumer will be look                                 pretty simple so your programs will come                                 quite nice and straightforward but                                 they're more production ready and that                                 they're handling security and now you're                                 considering all the services that you're                                 going to use as syncs and you know these                                 schema registry and can cast a broker                                 and so on so the platform we have a lot                                 of people working on this in a decent                                 stop government we're going to produce                                 quite a few new features in the next few                                 months I'll just give you an                                 introduction to a couple of them they're                                 mostly aimed at data scientists so now                                 streaming so reduce interest in                                 streaming this may not be as relevant                                 for you one of them is sharing of                                 datasets globally so if you have image                                 in that for example which is a large                                 data set of several hundred gigabytes                                 and you'd like to get access to it what                                 we'll have here is a way for you to                                 search for that and just download it to                                 your house works cluster and if you have                                 your own data set you're interested in                                 and you want to publish it make it                                 available to people you just right click                                 on publish it and it will become                                 available for anyone else's to search                                 for so it's basically open data which                                 they do back end we hope that people in                                 ten surfer will look at assist well                                 because we have no support for GP you as                                 a resource in yarn we're supporting                                 tensorflow on SPARC and then we have                                 also distributed native tensorflow                                 version running our file system we've                                 lots of things going on one of them is                                 putting the small files in HDFS which is                                 a quite a source of a lot of problems                                 for a lot of people we're putting it in                                 the database so any file under a                                 kilobyte will be in memory in the                                 database and files between one kilobyte                                 and                                                                      the database supports on column disk so                                 that's basically how we're doing is                                 we're also supporting we're working on                                 this isn't finished work but we're                                 working on making the filesystem highly                                 available across availability zones so                                 if you want to run this on Amazon and                                 you would like to be resilient to it a                                 whole availability zone crashing this                                 might be of interest to you similarly if                                 you're running on Prem and you have two                                 big clusters and you'd like to replicate                                 across and then this may also be of                                 interest to you another thing that we've                                 done is related to the file system again                                 is that we have support for hive and the                                 hive metadata is typically you'll see in                                 a my sequel database well we're already                                 using in my sequel database so what we                                 did was we added foreign keys from the                                 hive metadata to the backing files in                                 HDFS and what that means is in practice                                 I don't know if you can see it here but                                 there's two tables here one is called                                 websites and web sales and what happens                                 if I do this and the command line if I                                 go to HDFS and remove the website web                                 sales subtree but if you did that in                                 normal hive your hive will be broken but                                 in our case it's just automatically                                 cleaned up because we have strong                                 consistency now for the hive meta data                                 so that's what foreign keys give us ok                                 so that's that's basically yes we have                                 ocular Europe's only Hadoop distribution                                 perhaps the dupe it's completely                                 open-source everything all Apache                                 licensed and it's not just there for                                 bigger and faster clusters we're also                                 building a data platform and in that                                 platform we're adding first-class                                 support for streaming and Python amongst                                 other things and we're agnostic to                                 whether you're interested in flicker                                 spark we love both of them so lots of                                 people worked on the project this is                                 there's some of them sitting down here                                 and we have a bunch of users and                                 customers and if you're interested in                                 the project you can if you're incident                                 contributing just talk to me interested                                 in doing research on the platform that's                                 great you can also just follow us on                                 Twitter like us on github or just talk                                 to us on on slack thank you                                 [Music]                                 we do have time for one maybe two                                 questions I actually have one under                                 health questions so our first question                                 is since you want to abstract away some                                 of those nitty gritty details and try to                                 provide this UI where everything does                                 right you've solved already a number of                                 things like the logging so that they                                 have access to things are you also                                 thinking about perhaps using beam as an                                 abstraction layer for for the                                 programming since I think it would                                 complement because you have solved some                                 of those operational things that are                                 around it now why not let users also get                                 a little bit away from those native                                 interfaces and then the other thing I                                 just wanted to have confirmation you can                                 deploy this on cloud and on cram it's                                 completely open source yes we expect                                 when we have automated support for                                 installing this with chef and we have a                                 tool called caramel you can click your                                 way to an Amazon cluster it's a bit four                                 clicks I think if you're doing it on                                 prime you just have to have your cluster                                 set up with SSH access into machines and                                 then this caramel which is nor                                 castration tool for chef will run all                                 the chef's cookbooks on the other point                                 beam we actually we implemented the                                 interpreter for beam in Zeppelin because                                 we wanted beam as a first-class we                                 believed in blame I wasn't saying we                                 don't believe in it anymore but we                                 weren't primarily looking at beam on                                 flink now if you're familiar with being                                 on slink you'll know that and it only                                 runs in what's called attached mode that                                 means that the same client has to be                                 attached well being runs there and it                                 doesn't work in detached mode now the                                 run would attach mode is that if we were                                 to run that then we have to think big                                 bunch of jars or dependencies including                                 nettie which is a different version                                  which conflicts with our version so we                                  can't run think in attached mode as it                                  currently stands there is a Jireh flip                                  on going for fling to provide a REST API                                  for yarn which would solve this for us                                  but it's not the area so we'll add it                                  when it's there                                  apex sure we could do the same thing you                                  know it's not it's not much work to add                                  support for for being to our platform                                  it's quite you know it's quite                                  straightforward I'm jiggly apex would be                                  easy enough so we gladly accept peers                                  and contributions we would love that                                  okay um first we're going to have a                                  break and I'll have an hour break you                                  feel free to stay here and continue the                                  discussion but officially thank you Jim                                  for the presentation                                  [Applause]
YouTube URL: https://www.youtube.com/watch?v=fNeks6JdrrE


