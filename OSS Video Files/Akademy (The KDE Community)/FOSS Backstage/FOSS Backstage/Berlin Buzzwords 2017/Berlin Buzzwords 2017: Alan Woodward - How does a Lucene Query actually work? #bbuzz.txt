Title: Berlin Buzzwords 2017: Alan Woodward - How does a Lucene Query actually work? #bbuzz
Publication date: 2017-06-15
Playlist: Berlin Buzzwords 2017
Description: 
	When you submit a query to Elasticsearch or Apache Solr, underneath all the shiny user interfaces it's a Lucene Query that's doing all the grunt work.  But what actually happens when your query is processed?  And how come it's so damn fast?

This talk will guide you through the code paths that Lucene takes when executing a query, explaining the data structures and algorithms used, from simple term queries to more complex booleans and custom scoring methods. It will be useful for people writing their own queries or similarities, wanting to optimize how their queries are run, or just plain curious about how an awesome piece of software actually works.

Read more:
https://2017.berlinbuzzwords.de/17/session/how-does-lucene-query-actually-work

About Alan Woodward:
https://2017.berlinbuzzwords.de/users/alan-woodward

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              hello everyone my name is Alan                               Woodward's I'm from flax such as                               Microsoft insultin see based in the UK                               and I'm going to talk to you about how                               you query at Lucene index what actually                               happens when you run a query to start                               off with we have the slide that the                               marketing manager makes me put up at all                               these talks so yes this is who flax is                                with open-source consultancy based in                                Cambridge we've been around for a while                                we do lots of interesting stuff hit me                                up if you want to see more details on                                that                                so the talk today what I'm going to                                basically do is take you through some of                                the the Java classes that that are                                required that you use when you run a                                query and Lucy will talk about how                                things match we'll talk about how you                                collect all those matching documents                                we'll go through the details of how                                Matthew and the simpler queries work and                                I'll talk about query caching as well                                this is sort of an entry level talk                                Adrian here is doing another talk at                                half-past four which is on more details                                on how particular types of slow queries                                work so that's a nice follow-up to this                                one but the most important thing is why                                should I care why you cares how things                                work this talk the idea initially came                                from a workshop I did at Glasgow                                Strathclyde University last year called                                loosing                                    which was introducing the Lucene open                                source java library to students                                academics working on information                                retrieval who tend to not use sort of                                commercial software in their own                                research they use things like Terrier                                which is lots of papers are published on                                this but then no one outside the                                academia uses it and all their research                                and the things they were doing based on                                this scene were making certain                                assumptions about how queries work how                                documents are collected how things are                                scored                                we don't actually apply to the scene so                                okay well if you're                                cont UN query if you're writing your own                                similarity for doing any kind of this                                this sort of level of research or                                implementation then it helps know what                                the framework does how it all fits                                together plus the main reason why you                                should care is because it's interesting                                no we're all geeks that's why we're here                                so here's some of the classes I'm going                                to talk about there's lots of them they                                have scary names some of them have names                                which is there because that's what they                                were called when leucine one was                                released fifteen years ago and their                                purpose has changed entirely but they                                still have the same name but there we go                                we start off with we have an index                                weekend so the illicit index is a bunch                                of files on disk and the the way that's                                stored is pluggable they have different                                codecs but the these are the                                implementation of how all these data                                structures are stored is incomplete                                hidden from the client generally                                speaking and you get access to                                everything through the index reader                                class so this gives you access to the                                inverted index to dock values document                                to like a database table to the KD tree                                which is nu and leucine                                             better way of storing dimensional data                                and Emeric data term vectors general                                corpus statistics things like number of                                documents that have a particular term or                                the number total number of terms total                                member tokens all the things you need                                when you're executing a query and when                                you're scoring a query so the index                                reader gives you access to all these                                things then the index search wraps our                                index reader and that exposes a bunch of                                methods helper methods to actually run                                your queries so this is the important                                thing when you're doing a search the                                first thing you need is a searcher and                                then the next thing you need is a query                                so we have a query object which is an                                abstract class and this defines what you                                 actually want to get from your index so                                 it's a term query or you're getting a                                 single clip searching for a single term                                 if it's a boolean you're searching for                                 this or that this and that the phrase                                 query you want to get things with                                 terms next to each other all the                                 different types of queries are had their                                 own class they're independent of your                                 index rate then you can run the same                                 query against lots of different index                                 readers lots of different indexes and                                 you're obviously at different documents                                 back because the documents and all these                                 different in this it's gonna be                                 different but the kind of fundamental                                 abstract notion and what it is that                                 you're going to retrieve it's                                 independent of the actual thing you're                                 searching over they're immutable this is                                 an important thing if you're using query                                 caches you need to make sure your query                                 is immutable                                 you can go and change it under the hood                                 then suddenly you're going to start                                 getting wrong results back from a cache                                 yes the body if you're writing your own                                 queries ever then bear in mind you need                                 to make it make sure it's immutable                                 otherwise you're gonna start getting                                 weird results back from from your index                                 searcher now that the important thing                                 here is the queries independent of the                                 index reader so under the hood when you                                 actually run the query delusi needs to                                 turn that query object into something                                 that is relevant to that particular                                 index and what we have here is it's                                 called a weight normally this is all hid                                 so when you're externally when you're                                 running a query you just run the query                                 against the index and you get your top                                 box back so that you don't see the                                 weight but this is used internally and                                 it's the specific representation of this                                 query for this index reader yeah and it                                 may take to your query that relates to                                 everything in the index so not specific                                 bits for the index it's kind of a global                                 view over your index you create it by                                 calling create up create weight not all                                 queries can do this it's a slight I know                                 it's kind of a bit of a rough edge in                                 the in the Lucene API that you can have                                 a query that can't actually create a                                 weight though I think that ought to be                                 sort of be two different classes there                                 but that's a another discussion there's                                 a certain types of queries you need to                                 rewrite them against in particular in                                 read the first so I talk on top of some                                 queries when I give an example here like                                 a wild card or a regular expression what                                 that will do is you give it an index                                 reader and it rewrites itself into a                                 disjunction query so we had the weight                                 and the weight looks at the global view                                 in the index next I need to talk to a                                 bit about that the the structure of the                                 the Lucene index so before I can sort of                                 go and explain what happens next so your                                 Lucene index structure it's not one big                                 monolithic thing it's actually consists                                 of lots of different segments of index                                 each segment itself is a kind of an                                 index in itself they're built in memory                                 when you add documents to your index                                 writer it'll match everything up buffer                                 everything it builds your segment and                                 then it flushes it out to disk when you                                 call commit and then also when you when                                 you call commit when something's to disk                                 you have another background process                                 which will merge segments together so                                 you don't end up with lots and lots of                                 tiny segments then someone will get most                                 together into larger segments behind the                                 scenes why do we do this                                 the main reason is to means that you can                                 do incremental indexing really easily so                                 when you're adding new documents you                                 don't have to go and change already                                 existing index structures you can just                                 add another index structure another                                 index onto the end of your existing                                 index and you don't add to update                                 anything so that's good for speed of                                 indexing and it's also good for                                 immutability and data integrity you can                                 make sure that you know what you can                                 must checksum and you know that when                                 you've got that set of bytes that                                 particular index segment once it's                                 written once you can read it in you've                                 got to check your checksum and you can                                 be sure that the data is correct so the                                 index reader gives you a view over all                                 these individual segments                                 we have a leaves method these returns a                                 bunch of leaf reader context the leaf                                 reader context is a view over an                                 individual segment but it also it has an                                 ID which has a an ordinal which means                                 they're ordered within the segment and                                 it also records a dock base so you can                                 map your ID from that segment in terms                                 of a global space of IDs over the whole                                 index and it also Polly feeder context                                 also exposes a leaf reader which is like                                 an index reader but it's for individual                                 segments so what does this all mean to                                 surging well your index reader gives you                                 the top level view that's the view over                                 the whole index you can get some access                                 to some data structures that way but                                 it's kind of an inefficient way of doing                                 it because it's going to have to merge                                 the data structures from all the                                 individual segments together so                                 generally the way we search things we                                 actually you iterate over all the                                 segments you do a search of a one                                 segment such as the next segment segment                                 search over the next one and then                                 combine those results wait again it                                 gives you that view over the whole thing                                 so if you want to actually do your                                 search you can't use a weight because                                 our weight is too too granular we need a                                 different object and that object is                                 called a scorer                                 so the scorer maintains your state for                                 the query for each individual leaf                                 reader so you have the query top level                                 which is index reader independence then                                 you have weight which is the the                                 representation of that query for this                                 index reader for this at the top level                                 and then you have a scorer which is the                                 representation of that query for each                                 individual segment it's providing                                 iterative documents so when you say okay                                 I've got my segment I've got my score                                 over this segment I can just call this                                 iterator and it will return all the                                 documents that match pin turn it also                                 gives you access to scoring mechanism                                 hence why it's called the scorer so you                                 get the doc doc ID set iterator is                                 returned                                 you advance over your your iterator and                                 then you can call score Adult School for                                 each document you're sitting on and it                                 will give you the score for that                                 document generated by wait wait not                                 scorer and there is a shortcut here if                                 you know that this particular score                                 isn't going to match anything in this                                 particular segment it'll just return now                                 and you can sort of shortcut actually                                 have score is a it's a bit of a legacy                                 name because scoring isn't necessarily                                 the score as primary purpose you can                                 still use it to iterate over document                                 even when you're not actually interested                                 in the score if you're using as a filter                                 for example so let's tie it all together                                 we've got your query objects independent                                 of everything just a representation of                                 what you want to what you want to                                 retrieve from a particular index given                                 your index reader you have a query                                 generator in generator weight to maximum                                 documents the weight generates a score                                 for each segment and that score then                                 gives you a doc ID set iterator which                                 will iterate over all the matching                                 documents in that seven let's give you                                 two bits of pseudocode here so you can                                 create your weight you iterate over all                                 the leaves within your index you create                                 your scorer for each leaf you get your                                 iterator for that scorer and then you                                 iterate over the is rater so the                                 important thing is what do we do with                                 that duck ID once we have it so the next                                 class we look at is the collector and                                 this says okay I've got a list of all my                                 matching documents what do I want to do                                 with those documents as they've come in                                 again the collector has the same dual                                 structure it has the top-level collector                                 which is for the for the whole search                                 and then a leaf collector which is the                                 representation of that collector for                                 each segment and when you go through in                                 our the pseudocode ad earlier and as you                                 get each document you then call                                 collector collects                                 with that with the document ID that's                                 matching so it can give you that                                 pseudocode again                                 and you can say here in the middle here                                 we call in collection with a document ID                                 this is obviously heavily simplifies                                 there's a number of other things that                                 listen allows you to do you can search                                 leaves in parallel you've passed an                                 executor in some scores will actually                                 score a whole bunch of documents at the                                 same time this and they put a belt score                                 up which is good for performance in                                 certain circumstances this I'm not                                 talking like deleted documents here                                 which adds another complication either                                 if you need to be able to skip over                                 documents that you know they've actually                                 been deleted in the index and you can                                 also have something called a lien                                 termination so if you have for example                                 assorted assorted index and you can if                                 you're if you're trying to collect                                 things in sorted order and you know that                                 once you got to a certain duck I D                                 because the index itself is sorted you                                 can stop iterating over everything you                                 can jump out that but that's basically                                 the the nucleus of what happens when you                                 searching this see                                 so they seem to have a bunch of                                 collectors that come with it                                 the when you normal search when you                                 search for a query and say like oh I                                 want to get the top ten highest scoring                                 results from this query listing uses                                 something called a top score collector                                 when you're sorting by field it uses the                                 top field collector you can create your                                 own collector and pass that in there's a                                 public method on searcher excuse me                                 so all these top end collector classes                                 use a priority queue which means that                                 obviously for doing deep paging if you                                 want to say okay I want to get the                                                                                                      results if if you're using a priority                                 queue what it has to do is it then                                 allocates you know space for all those                                 top                                                                     most of them so what in that what loosin                                 allows you to do is actually exposes                                 this method called search after which                                 allows a live scene to to determine                                 whether you're going to put something in                                 the priority queue based on both the top                                 and the bottom values so this is when if                                 you look at solar or elasticsearch and                                 if you you know just page through doing                                 next next next next next in one of these                                 search applications you'll be tend to                                 run out of memory and things die but if                                 you use a scroll ghwarri or a cursor                                 mark under the scenes that's using the                                 search after method and so there's much                                 more memory efficient so in terms of how                                 you score things and what information                                 you have to hand when you're doing the                                 scoring collection is going down as you                                 iterate through things so they've done a                                 document at a time so your scoring                                 algorithm doesn't actually know anything                                 about how many document you've matched                                 what documents have matched before after                                 and this is actually information that                                 this can be quite useful for general                                 scoring algorithms so Lucine actually                                 provides another class called are--                                 scorer and this allows you to do a first                                 class search which is going to collect                                 all your documents you can score those                                 with your standards your standard                                 scoring mechanism you'll be m                                            you collect those top thousand Oxford's                                 or whatever and then you use a restore                                 to to go and do more complicated scoring                                 on those those collected results so                                 something like learning to rank which                                 Diego and so if you see I was talking                                 about earlier will use this method it's                                 you collect everything first and then                                 you run in underscore again so what                                 we'll do now is we'll a couple of                                 queries to see how does the score                                 iteration actually works how things are                                 implemented the simplest one is time                                 query so this just makes the post in                                 generation from your leaf readers so                                 which is just an iterator over the                                 postings list of a particular term so                                 you have where we have our ideas eraser                                 which basically just delegated directly                                 to be the postings enumeration which                                 will x-ray over your postings list if                                 the posting was an in racing there's no                                 that means there's no matching terms for                                 this particular token in that segment so                                 the whole thing returns now very simple                                 very straightforward very fast it                                 doesn't need to do anything clever it's                                 just literally reading bites off disk                                 boolean queries is the more complicated                                 one this is kind of the the most your                                 that the heavy duty Heather the work                                 hard worker of the loosing queries and                                 family and so you can have a billion                                 queries you can have muss clauses you                                 can have should clauses you can have                                 must not clauses you can have filter                                 clauses and different combinations of                                 all these things will actually result in                                 different scores being used under the                                 hood so if you only have mus clauses                                 it's a pure conjunction then we use                                 conjunction score oh if you only have                                  clauses it's a pure disjunction we                                 use a distinction in some scorer                                 we have rec opt scorer so required and                                 optional combinations of those and then                                 if you've got must not clauses on there                                 as well you have the rec Explorer so                                 required an excluded scorer these all                                 work in different ways                                 conjunction scorer is it's probably the                                 simplest one in that it just score has                                 been expose that thing called a cost                                 which is kind of a guess of how                                 complicated it's going to be to run this                                 to iterate over this particular score so                                 conjunction score sorts by by the costs                                 and said okay whatever the whatever the                                 lowest costs score is I've been used at                                 to drive my iteration it calls next off                                 on that and then it will advance                                 everything else to the same one to the                                 same duck ID if if we have a match there                                 then excellent everything's on the same                                 document we can match we return that ID                                 if it's not then we find out what the                                 maximum ID of all the stores is and then                                 advance the lead document to that one                                 again so we're using the the lowest cost                                 score to drive the iteration which means                                 you can you can skip over more                                 complicated scores the disjunction query                                 uses a priority queue and it's a it's a                                 heap implementation I think scores will                                 advance to the first matching documents                                 whatever the lowest yet score the lowest                                 doc ID you start off with that these                                 have to drive the iteration you call                                 next off on the whatever it is that's                                 got the lowest duck ID your update                                 update the heap and the current ID is                                 off ID at the bottom of half of the                                 queue I'm simplifying here obviously but                                 that's generally general speaking what                                 the algorithm is the point optional                                 scorer combines conjunction disjunction                                 and the important a nice thing to point                                 out here is if if we don't care about                                 scores if we only care about matches                                 then we can just ignore the destruction                                 part entirely because it's the                                 conjunction part which is driving                                 whether something matches on disjunction                                 just adds to the score                                 if scores are acquired then it advances                                 using the conjunction and then advances                                 the disjunction up to that same points                                 that's that same Dolph ID to get the                                 scores and then required exclusion                                 scorer contains yes so it takes any of                                 the other three scores to drive the                                 iteration and then we have our exclusion                                 score as well and it just adults at the                                 child score and checks against the other                                 one resolution Sakura to tell whether                                 something should match or not                                 phrase query so this is if you're you                                 want to find out it's basically a                                 conjunction I want this this document                                 akin to contain both these terms by also                                 want them to be next to each other so                                 and there are two different types of                                 score there's exact phrase which is okay                                 I these two need to be a fixed fixed                                 number of positions apart or the sloppy                                 phrase scorer which is a terrifying ball                                 of wax                                 kind of works we think there are some                                 tests which have been ignored for about                                                                                                      whether they're supposed to work or not                                 and yet so again it's a specialized                                 conjunction we you check to see the                                 documents got all the terms in there and                                 once you found doctrine which has all                                 the terms and it didn't go and check the                                 positions there are so there are ways of                                 speeding this up using something called                                 T phase iteration which Anton has been                                 talk about later on this afternoon                                 several patterns come to his talk so                                 there's I'm going talk about query                                 caching so we have all these these                                 different scores and what essentially                                 they produce at the end is they produced                                 a set it said iterator over a bit set                                 purse                                 which if you've got a very complex query                                 it's quite useful to be able to catch                                 that now if you want scores as well then                                 the problem with trying to catch that is                                 that it's not very compressible scores                                 come out as floats                                 if you end up matching something no over                                 a very large set of your rooms s it'd be                                 a very large proportion of the index                                 you're going to end up trying to store                                 lots and lots and lots of float values                                 which don't compress tool and also it's                                 specific for that index so that you                                 can't share it between different index                                 readers so but if you're we have stuff                                 that doesn't isn't scoring it's just                                 being used to filter stuff out then                                 that's nicely cashable and in the next                                 lecture has a query cache that will will                                 handle all this for you yes so rather                                 than calling quick query doctor weight                                 directly we go through the index search                                 instead indexes or create weight and                                 then the index searcher will say okay if                                 I don't need any scores and this will                                 wrap it wraps that weight with with                                 something called caching wrapper weight                                 which can then to tell them whether or                                 not it's connected well it can determine                                 whether or not you've already run that                                 query and if you have then it can                                 retrieve results from its cache if you                                 haven't it can run it in the background                                 and cache that result yeah so when you                                 what we call scorer caching right for                                 white those and C goes to C it's                                 actually already got the bit set                                 somewhere and the important thing is                                 because cash and prizes at the segment                                 level it's not the view over the whole                                 indexes the view over each individual                                 segments so if you're doing incremental                                 indexing you have a bunch of data that's                                 come in you open your search you open a                                 new in next reader and you open a new                                 searcher you might find actually quite a                                 lot of the data in there is exactly the                                 same as it was in your previous search                                 if you just added a new segment on the                                 end you still got a lot of the same data                                 structures there                                 and you can share your query cache                                 between these two index searches and                                 because it's got a view over the same                                 segments it can reuse all that you don't                                 have to regenerate where your caches you                                 don't invalidate everything just by                                 opening up a new segment this is a very                                 useful thing how can we tell that                                 scoring is not required whether two ways                                 a particular collector might not be                                 interested in scores so for example a                                 collector that sorts things by I field                                 values it doesn't really care about the                                 scores it can so it's available for                                 caching and also when you're when you                                 pass things as still ters to billion                                 query constants so she should be fully                                 inquiry occurred or filter that's                                 obviously you can say okay these                                 particular elements of the billion query                                 I don't care about the scores and these                                 and just using the filter stuff out in                                 which case we can use that they're                                 awaiting for caching yeah right I kind                                 of burnt through these they don't have                                 any questions standard two highlights                                 we still had a few minutes so if you                                 have any questions right now there is                                 time for that it's not there is a longer                                 break today to intimidate us to do it in                                 public okay so the way cuz you've                                 explained it everything seems to have                                 kind of an overall version and then a                                 leaf version yes I guess the way you've                                 explained it score is effectively a leaf                                 wait has anyone ever kind of considered                                 renaming it or is that just generically                                 that's about I think it's one of those                                 things that someone could come up with a                                 new name for it and then everyone else                                 would hate it so it's we covered naming                                 things and cache invalidation here so                                 yeah obviously it's all the hard                                 problems                                 but yet now that it's that that will be                                 a good description any more questions                                 bit of a vague one but if you wanted to                                 trade through a live Lusine query is                                 there any way to see all this company                                 well I was you can't separate using the                                 debugger there aren't hooks for that                                 kind of thing generally because this is                                 then this is the hot loop in the middle                                 of Lucy this is the very very fast stuff                                 so there aren't really any ways of                                 exposing that automatically one thing                                 you could do I suppose well you could                                 pass you can implement your own                                 collector which you know because in it                                 stack traces or whatever whenever it                                 hits a particular document and and you                                 can do wrapping collectors so you could                                 have a collector that you know delegates                                 to your top scorer or the top scorer top                                 Phil Kline sort heaven or what have you                                 but yes I mean I love most of this just                                 by stepping through the debugger so you                                 quickly talk about the Reece core                                 functionality when the initial path                                 happens where do the documents the                                 matching documents reside them to be                                 restored okay yes so I didn't I didn't                                 cover top dogs so when you when you use                                 a top score collector obviously the                                 collector just said it and it just just                                 pulled these documents in and then each                                 collector implementation will then have                                 another method on it says which you call                                 at the end say okay I now have some                                 results back these are the results I've                                 got these are the top ones I'm gonna                                 give you so rescore takes top Doc's                                 implementation and that just I mean                                 that's literally just an array of it's                                 basically just a wrapper of an array of                                 its yeah which is the dark IDs it                                 doesn't store anything else so if you're                                 then you know if you're in solar or                                 elasticsearch if you're then trying to                                 pretend on lots of field values what                                 what they have to do is then go through                                 for each of those doc IDs and go and                                 retrieve them separately from store                                 fields which is a bit                                 which is why you had you ever do that                                 first like the top ten you don't do it                                 we in the collector is everything                                 happening there but yes if a risk or the                                 you would you do your search you get                                 your top Doc's instance back at any past                                 actor to risk or a we use those Don IDs                                 you mentioned earlier that segments                                 could decide not to score in a sense                                 family terminate can you talk what                                 support there is for determination if                                 any I'll have to remember it is now so                                 in a way it basically works is the                                 throws an exception is early termination                                 exception there are a number of                                 different ways you can do that the so                                 they're the tool I can think of off the                                 top of my head of timed ones you can say                                 well I want to put a time limit on this                                 query if it's going to take longer than                                                                                                          out and so you can put that auditing                                 segment as it starts offing say okay                                 I've got a new score here have I run out                                 of time                                 but you can also do it as part of the                                 part of the iteration within the score                                 itself you can say okay when you call                                 next off word about it well okay we run                                 out of time now throw an early                                 terminating exception and then the other                                 way of doing that is if you've got a                                 sorted index so say you're interested in                                 returning things by date you can say                                 well I'm going to sort everything in                                 this index so that the the latest things                                 come at the beginning of the index and                                 new going so going back in time as you                                 iterate through and if you're only                                 interested in stuff has happened since                                 yesterday                                 as you get through as you're iterating                                 through you can see okay well I've got                                 to this document here has a field value                                 that's sort of beyond the where I'm                                 interested in so I know that everything                                 else I hit in the situation is not going                                 to be irrelevant for me so you can just                                 bail out then and throw your early                                 termination exception and that'll handle                                 at the top level in the index I've                                 searched after I had a question so when                                 you call create score on the way clear                                 you're going to make multiple scores in                                 a single index because an index has                                 multiple segments yeah practically                                 speaking when would you create multiple                                 waves from a soon                                 query at the top low like obviously a                                 boolean query we'll call create way                                 underneath the hood on all the sub                                 quarters but when would the boolean                                 queries create wavy called multiple                                 times I use say both times for different                                 queries or multiple times within the                                 same good within the same query now                                 generally speaking tend to be called                                 once okay there's never an instance                                 where it's not called where it's called                                 more than once because I also know that                                 from my experience it's only usually                                 called ones but I don't know yeah I                                 think it's only called once it can be                                 quite a heavy operations as well it will                                 be do certainly for things like term                                 queries it goes and it when you create                                 the weight it also goes and reads lots                                 of information from the index about know                                 where in each segment it needs to jump                                 to to get to read the terms index and                                 does a wrap up front and yet that can be                                 quite a said IO intensive operation that                                 can be quite a slow operation so you                                 want to make sure that that's not                                 happening more than once and as it is                                 under the other point yeah very much I                                 wanted to understand which parts of the                                 query year our most expensive and and                                 actually alluded to it and probably it                                 green is going to Tokyo right yes so                                 generally speaking any query that                                 involves positions is going to be slower                                 because it's not just you're not just                                 iterating through postings lists you're                                 also having to then go and check                                 positions and it's not one operation per                                 document you hate it's actually several                                 operations there are so there's various                                 payload queries which is a similar thing                                 it's reading three positions and then                                 reading payload bytes out a position and                                 doing comparisons on those so that's why                                 yeah that's what and that's why you have                                 this cost exposed so if you've got a                                 conjunction query that has one you know                                 very simple term query and one quite                                 complicated span query or something like                                 that then you want to drive it with that                                 term query and make sure that you're not                                 having to do all these complicated                                 operations on this more than this                                 heavier scorer that                                 actually we know we're not going to hear                                 that document anyway because injunction                                 of the turquoise doesn't match I think                                 we're good                                 thank you very much Alan thank you                                 [Applause]                                 [Music]
YouTube URL: https://www.youtube.com/watch?v=Z-yG-KvIuD8


