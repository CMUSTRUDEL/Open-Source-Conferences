Title: Berlin Buzzwords 2017: Simon Dollé - Feeding Word2vec with tens of billions of items, what could ...
Publication date: 2017-06-15
Playlist: Berlin Buzzwords 2017
Description: 
	Simon Dollé-Feeding talking about "Feeding Word2vec with tens of billions of items, what could possibly go wrong?".

Word2vec is an unsupervised algorithm which is able to represent words as vectors, the so-called word embeddings. It computes them so that words with close meanings are close together in the vectorial space. Originally developed for NLP applications, it has recently proven to be extremely relevant in numerous contexts such as biology, speech recognition, recommendation, etc.

At Criteo, an ad retargeting company, we use Word2vec to compute product embeddings. This allows us to compute similarities between products and consequently improve the relevance of the products we recommend to our users.

Spark MLlib provides a distributed implementation of Word2vec. However, using it daily on tens of billions of item as we do, raises challenges in terms of scalability and quality. In this talkI will share with you how we moved from POC implementation to A/B-Test. I will tell you the numerous things we broke during this journey and how we fixed them. Finally, I will expose how we plan to improve our product embeddings and use them in different ways.

Read more:
https://2017.berlinbuzzwords.de/17/session/feeding-word2vec-tens-billions-items-what-could-possibly-go-wrong

About Simon Dollé:
https://2017.berlinbuzzwords.de/users/simon-dolle

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              hi thank you                               yeah I'm Simon a software engineer at                               créteil and ad retargeting company and                               today I want to show you how expect our                               experience with using water web at a                               very large scale so first I would like                               to know who knows what water works here                               in the room and who actually used it in                               production okay what other people but                                I'll still do an introduction about what                                is water EXO water vac is an algorithm                                that was originally invented by Google                                researchers in an NLP context and the                                goal is to be able to compute                                similarities between words like                                introducing what they do it like they                                compute a mathematical representation of                                each word like your high dimensional                                vector and then we try to place it in                                this high dimensional vector here the                                dimension is                                                         but for the his of representation I'll                                keep this way and the goal then is to be                                able to group the the representation of                                words with similar meeting together so                                that we able to compute the distances                                over world so here we have fast speed                                instantaneous house building it all AZ                                like different cluster of of words and                                how does it work actually how do we                                learn where to put the words into this                                high dimensional space that's the                                question                                so accuse Jesus training data well let's                                use a very simple train data like a                                simple sent sentence the quick brown fox                                jumped over the lazy dog what it does so                                it first attribute a numbering so a high                                dimensional vector to each word and                                place them randomly in the space so here                                I have quick jump etc everything is                                random and then what it does is like it                                it's a word for the first word or the                                second word select it and then it masks                                the surrounding words so here the                                previews and um the next world                                it might SEM and it has the algorithm to                                predict this surrounding words so                                like hey I have what quick what do you                                think is more probable to to be before                                and then to answer this question the                                algorithm looks at the closest world to                                the quick actual quick word                                representation and then it figures out                                if it got it wrong or not if it got it                                right then change nothing if it got it                                wrong then it modified the position of                                the of the underlings so that the word                                to be predicted gets closer to the                                actual representation so here we are                                going to move the the embedding slightly                                closer and we do the same thing for                                Brown because which was the next word                                but what we actually also do is like we                                also move the quick quick word so that                                it gets closer to the other ones and                                finally jumped was really close to quick                                here and we don't want to be that close                                because in this context is not really                                relevant so we pulled this away so we                                have here or gradients like we know how                                to update our weights and then we move                                we move the the corresponding embeddings                                and then we have done what step of the                                algorithm and we move to the other step                                by selects the next word Brown and we do                                something like updating our weights and                                so on and so on and at the end we'll get                                groups cluster of words like here in                                this specific sentence that Fox is                                really rated too quick and brown dog is                                related to lazy jumped is somehow                                neutral and V is also neutral and I told                                you we do this over and over and the                                point is back to HUD the algorithm                                converge we reduce the size of the                                displacement over the time to be sure                                that it converts so this ought to work                                algorithm has proven to be extremely                                successful in the NLP context and many                                people started to use it in other                                contexts and we at créteil we do ad                                retargeting this is what I told you so                                what we do actually is like we display                                halves to the                                user and then we place products inside                                this hat and we are we bill our clients                                only if the user click on the product so                                we have a relief strong incentive to our                                 place relevant products inside our hats                                 and we thought hey could we use this                                 water back to improve our product                                 recommendation and by doing the simple                                 analogy we managed to do this so the                                 analogy is the following flag instead of                                 talking about words will consider                                 products and it's instead of considering                                 sentences will consider user stations so                                 for instance here we have a user                                 stations that we capture for our                                 trackers so we have one user and he has                                 been yes in an iphone and then a headset                                 and then trousers t-shirts etc and so                                 this is our new sentence and then we can                                 apply exactly the same algorithm try to                                 pick the headphone try to predict which                                 a word which product are going to be the                                 next one and the previous one and we                                 this way we can place the product inside                                 a high dimensional space and then figure                                 out that these are clauses and we are                                 these are close together and these                                 products are IT products and these are                                 close together the point is that we work                                 at a very large scale and to give you                                 another magnitude like we have to                                 process                                                                 billions events and to give you another                                 of magnitude like if you figure out a                                 Wikipedia then it's almost for the whole                                 Wikipedia in whole languages this is                                 four times less even is to give you an                                 example so in the original                                 implementation of what to Veck it was a                                 simple binary distributed well                                 distributed over the web where you could                                 only train it on a single desktop so it                                 obviously doesn't in here it will take                                 four                                 so we have to distribute this                                 computation luckily a spark Apache spark                                 and the as in this ml lib project an                                 implementation of the distributed water                                 back I'll explain you how it works                                 so the problem is the following we have                                 now many user sessions and we also have                                 many computers and we want Q compute                                 embeddings for these user sessions using                                 the cursor of machines that we have what                                 we do is actually what the algorithm do                                 is actually pretty simple well it split                                 the timelines and create some partitions                                 for corresponding to the number of nodes                                 that you have to process and then it                                 sends this data over one single computer                                 each time it computes unveilings so like                                 each computer is in slightly different                                 products in a different order so the                                 computation buildings are largely                                 different and then it merge them by                                 simply averaging them so by doing so                                 obviously you lose some quality because                                 each computer art had only pass your                                 knowledge of the full distribution but                                 somehow it works so it was all good like                                 we had our data we had the algorithm and                                 we had the distribution using a purchase                                 battle with with or take we run on it we                                 have also very large closer more than                                                                                                        at all will run it and should work and                                 obviously it crashed and and to figure                                 out well we had to figure out why I'm                                 the point is like okay we were able to                                 scale and had were able to do                                 computation over overall terms of events                                 etc but actually the limitation was the                                 number of unique products that we had so                                 we have around                                                           and if you consider the implementation                                 that I've described somehow here you                                 have to have all the embeddings                                 corresponding                                 to hold the different products in memory                                 because you want to be able to update                                 the weight pretty fast and if you put                                 them in a energy that it will take                                 forever again so you have to have                                 everything in memory and then for each                                 single note so as we add                                               it couldn't fit and we couldn't buy                                 machines to be able to have such around                                 that it would fit in it so we are the                                 program so how did we solve it so we had                                 a closer look at this timeline let's go                                 back to this one and we hold a here we                                 have a user                                 he has been to a tech website and then                                 es move to a fashion website actually                                 does it make sense to be able to compute                                 a similarity between this headphone and                                 these trousers with what for us this is                                 absolutely not redone first I don't know                                 exactly what it would mean plus in our                                 specific case we don't display head for                                 different clients like we decide to                                 design out for gap and then we pick                                 closes we don't mix headphones and                                 clothes so we say oh what if we split                                 the user sessions with the different                                 clients so we have now for one given                                 user we have one timeline for the for                                 the closes and one timeline for the for                                 the hi-tech we learn embeddings for the                                 closes and then we are able to detect                                 that t-shirt comes together and then the                                 trousers is slightly different and                                 something for the headset and by doing                                 so then we are we don't have that much                                 unique products to process for each note                                 leg here this is I don't know let's say                                 for fashion we have maximum                                        different products for each of the                                 clients and then we can simply fit them                                 in memory and process that so we were                                 able to process most of our clients                                 still there are some clients with a huge                                 number of of this tank product and for                                 those we for thought that couldn't find                                 any solution so for our city                                 we simply decided not to drop them and                                 not to compute any underlings for them                                 so it will it was a strong decision but                                 would say it's better to have something                                 that not to have any underlings for                                 anyone and we also used another trick to                                 reduce the number of different products                                 so actually we consider the very                                 unpopular products like in each                                 catalogue especially for the big                                 partners for the big website there are                                 really bestsellers and then you have the                                 rest of the catalogue web we have really                                 unpopular products and this popular                                 products have two interesting product                                 properties for us first there are many                                 of them if you look at the longtail then                                 they represent actually the longtail                                 they represent a lot of distinct product                                 secondly we don't think this these are                                 really relevant because if they are not                                 really popular we don't think it's worth                                 recommending them and they won't perform                                 well in how has so we decided to drop                                 them and so it drastically reduced the                                 number of events that we considered and                                 made the computation even faster so at                                 this point we were able to compute our                                 bedding for most of our clients because                                 we by removing the very large client we                                 remove only less than                                                   till we removed lots of the around                                     of the of the events and yeah so we were                                 able to compute the embedding for most                                 of their clients so we well we thought                                 we were done and then we realize that in                                 the implementation of what track then                                 what you have you have plenty of                                 parameters to tune you have to tune the                                 dimension of the of the embeddings you                                 have to attune the number of epochs you                                 use like the number of time you go                                 through your training data and we have                                 no idea on how to tune them like they                                 are default but obviously we use                                 different kind of data so the default                                 would must be a difference so there was                                 no metric actually in the implemented in                                 the in the water to the Xbox                                 implementation so we decided to                                 implement two of them the first one                                 it's called llh so ll                                             log-likelihood and actually it's modules                                 are where we predict the probabilities                                 of the surrounding world so exactly the                                 case that I described at the beginning                                 so we pick a word and when we ask the                                 algorithm to predict the probability of                                 the surrounding world and this is                                 exactly what's alleged Mercer and this                                 is pretty close to what we optimize in                                 the in the law during the learning step                                 this is actually the matrix actually                                 which we optimize and then on the other                                 side we have decided to use another                                 metric called recall at K here this is a                                 measure that somehow Mazur R where will                                 you rank the product so or does it work                                 we take a user timeline we pick a                                 product and then so we pick its                                 unveiling and then we consider the K                                 nearest neighbors in the embedding space                                 we have a set of products and then we                                 consider the actual next event in the                                 user station and then we try to figure                                 out if it is in the K closest products                                 or not so if it is indicated closest                                 product then we counter one because we                                 decided that somehow our ranking is good                                 and if not we count to zero and then we                                 average stuff so it really measures                                 through oh well our underlings are                                 useful to run the products and this is                                 actually much closer to the finite final                                 good that we have which is predicting                                 the next product to be able to have                                 relevant recommendations so given this                                 we have the matrix and then we were able                                 to bench some parameters like the number                                 of partitions that we use like it also                                 has impact on the quality because as as                                 I have told you when you may have the                                 data on just preach the data then                                 somehow you lose some quality the number                                 of epochs when we figure out that                                 actually one it works for our case was                                 enough so we don't have to waste much                                 computation power on this the learning                                 rate like how well how fast we decreased                                 the displacements over the training set                                 and also the embedding summation so we                                 were able to tune this but at one point                                 we Forte okay we have seen these data so                                 the                                 on the record we have improve them by X                                 percent but actually is it really what                                 we care about so actually as I told you                                 what we care is about generating                                 relevant recommendation and how do we                                 measure rate that recommendation is for                                 clicks or for sales so like we're                                 generating with our organization we                                 should invent with a user and we see                                 whether people click on the product and                                 then buy them together buy them and like                                 so far we had no idea of our an                                 improvement on the llh or the recoil at                                 K was related to the actual clicks or                                 the actual sales so what we did is we                                 took the opportunity of one of our ad                                 castle online a/b tests to actually                                 measure for each of our clients the                                 uplift in terms of click and in term of                                 sales and compare them to the quality of                                 the embed to the of the related                                 embeddings like the LH precision at k                                 and we plot the correlation and we saw                                 that very a coalition between them so                                 it's still kind of fuzzy we are in the                                 process of trying to refine them and                                 we'll refine the correlation over time                                 we'll see if they are really correct and                                 it's still too early to decide which one                                 of the true metric is the best for the                                 online test so we have decided to keep                                 both for right now but over the time we                                 are quite confident that we'll be able                                 to tune our offline matrix so that they                                 correct well with the online matrix and                                 then we can do much more offline work                                 and which costs less than an actual a B                                 test so so far so good we are able to                                 tune our parameters we had a nice                                 implementation then we add a nice                                 distributed algorithm plus a use cursor                                 so well we thought we were done and then                                 we add these embeddings and well                                 everything was fine and this is when                                 something kind of surprising Adams like                                 we are good algorithm good                                 infrastructure etc it's like the data                                 betrayed us like we                                 I was well everything was running fine                                 and I was working on the next iteration                                 of the embeddings and the idea there was                                 to try to improve them by combining them                                 with the description of the product so                                 there was our some of our researchers                                 are done some research like you can have                                 the user stations and you can also have                                 mix them with the description of the                                 products and try to improve the                                 relevance of the embedding so I was                                 trying to implement this unless I notice                                 that I couldn't reproduce the uplift                                 that our researchers add on their own                                 data sets so I tried to dig it too a                                 little bit and try to understand what                                 was happening and at what point I figure                                 out that for one specific client I                                 decided to plot the evolution of the lnh                                 with the number of samples that we use                                 in our training sets and this is what I                                 got so here this is the learning process                                 and here this is the llh so here we see                                 that it increases and then at some point                                 then it decreases and this is completely                                 unexpected so as we are trying to                                 optimize for the llh we would have                                 expected like trouble really smooth                                 curve going up that and then I think two                                 things or slightly increasing till then                                 so while this is was really surprising                                 and then I checked for over website and                                 I figure out that add many different                                 clients having the same behavior so when                                 I had a closer look at the data and                                 realize that actually the data that we                                 use visualisations is actually very                                 different from the NLP data that it's                                 usually used with well Trebek for                                 instance well you have much less variety                                 in each user station that you have in a                                 sentence like usually you have a someone                                 looking for I don't know                                 red red closes and then it looked for                                 red t-shirts and then for red trousers                                 but you only have two shirts and                                 trousers in these user sessions like                                 whereas for NLP you have much more                                 via tea like you have in each sentence                                 you have a word you have some known some                                 adverbs you clitoral and because of this                                 like imagine that we had a user fan of                                 Red Cloud so he has a user session and                                 then we pick this user session first so                                 we are trying to we are going to group                                 read items together and then we are                                 going to process the data over and over                                 and then a new new timeline and as I                                 have told you we have decreasing schemes                                 like over the time we decrease the size                                 of the update so it means that we have                                 we will have grouped the red items                                 together and then it will be somehow too                                 late and group them with the time                                 because of the the learnt representation                                 is actually really dependent on the                                 order of the timelines that we that we                                 had so we will somehow overfitting for                                 the first data and then for the data                                 afterward it wasn't the case so we                                 thought ok we think we have this issue                                 how can we settle this like the whole                                 program here is the order in which we                                 process the data so we thought hey are                                 we should shuffle the data somehow the                                 point is like the way the algorithm                                 worked like what the way I've described                                 them is like we split our sentences we                                 shall fall them across the network and                                 across the nodes when we process each                                 sentence and then we generate the pairs                                 like the tails of products like the the                                 the some short words and then with the                                 word for which we try to predict the the                                 one we try to predict and then we feed                                 work to back with us with we thought hey                                 we could actually have all the user                                 timelines generate the tails shut of                                 them and then feed what you like with it                                 and this way we'll be sure that the                                 pairs are completely run or shuffled and                                 then for the the pairs denoted by the                                 red closes fan then if it will be spread                                 over the whole data set and then we                                 won't have this overfitting scheme and                                 we did we did                                 and this is actually what we obtained so                                 this is a much better looking shape this                                 is much closer to what we are expected                                 is the while this is an actual curve and                                 it looks like a notebook curve actually                                 and so this is the quality increase and                                 when we checked it on different clients                                 and it was fixing everything so one for                                 a which we what we found the solution                                 the point like here is that it takes                                 almost six time or longer than for the                                 previous iteration and why so it's                                 because we generate the pairs before the                                 shuffling step and actually when you                                 generate the path length you add some                                 rather than see it generates much more                                 volumes and it means that you have much                                 more data to shuffle and so we couldn't                                 afford it so even if the quality was                                 better we had to find another solution                                 and then we thought okay we know what                                 the problem is we know that it comes                                 from the ordering of the pairs but we                                 know that we cannot generate the                                 bell--the upstream so I thought what                                 about mixing the two schemes actually                                 what we can do is we can split the data                                 like the way we did before or shuffle                                 them but here instead of processing all                                 the sentences one after the other in                                 inside one partition we could read a                                 batch of data a batch of user sessions                                 generate the pairs for them shuffle them                                 in memory and then feed water back with                                 this update our ways and then use                                 another batch of data etc etc this way                                 we could have reduced the correlation                                 between the between the consecutive                                 pairs so this is what we did and this is                                 what we obtained like because you see                                 much better looking so we have a huge                                 increase then we all we have an                                 asymptote and then which in the maximum                                 and if you look at look at the absolute                                 value then this is likely the quality                                 slightly lower for the previous case but                                 which is okay and what we had a small                                 decrease and here what's also                                 interesting is like you see these bumps                                 this actually corresponds to the time                                 when we change the batch so we we                                 generate a new batch and then we                                 decrease etc and then this is nice to to                                 series still the quality was not optimal                                 and we thought we could do better so                                 what we did is what we bench this                                 parameter about all the size of the                                 batches but we had to use and then we                                 thought that we could use what the                                 larger the batches the the motion of                                 thing you do but then you have a limit                                 about the memory that you use because                                 you cannot show fall all the other                                 partition so we thought a nice trade-off                                 and this is what we obtained like here                                 we have only one to two and cells                                 matches and this is really close to the                                 ideal to the ideal case that we have                                 seen before and if you look at the                                 quality this is we have a minimal drop                                 of quality or when you do this so yeah                                 so far we had solve this issue and we                                 checked on several clients and the                                 problem was fixed for most of them so                                 this is where we stand now so we have                                 this into production we have abt                                 students we are really promising with                                 also now and what I wanted to share with                                 you here is a few lessons that I                                 consider important in this experiment                                 the first lesson is that you don't have                                 you don't have to fear traders trade                                 offs actually you have to embrace them                                 like here in this situation there are                                 many in this use case there are many                                 situation where we had to do trade offs                                 like we had to drop some of our clients                                 to make the whole thing works we are to                                 trade the quality versus the the                                 learning time to have something                                 acceptable etc and this is okay when you                                 deal with such large amount of data then                                 it's somehow okay to remove that of it                                 or to skew slightly the data to get some                                 result is it's not ideal but something                                 is better than no                                 the second point that I would like to                                 share with you it like you have to                                 measure everything like you have to have                                 relevant metrics and these are these                                 metrics that allow you to do trade-offs                                 if you try to change the parameters                                 without adding any quality metrics or                                 performance metric then you don't do                                 trade off you're going blindly and                                 you're modifying your algorithm blindly                                 and one thing which is important to here                                 to notice also is that you also have to                                 challenge your metrics like you have to                                 consider different are different ones                                 and then to try to understand what we                                 capture well and that what we don't                                 capture well and then you also have to                                 ask yourself are they really correlated                                 or related to the final goal that I'm                                 trying to achieve so this was the second                                 lesson and the third lesson that I want                                 to share with you is like you have to                                 play with your data like even if you                                 have very good metrics and then you have                                 processed everything nice and sweet then                                 at one point where you are both a new                                 issue within your algorithm they are                                 always both and flow data etc and you if                                 you always use the same matrix and                                 you'll notable to find them so a nice                                 way to find them is like trying to                                 manipulate your data in different ways                                 try try to use them for different use                                 cases implement new algorithm and by                                 doing this you will have yourself new                                 questions and then you'll be able to                                 find flows into your algorithm and then                                 you'll be able to improve yourself on it                                 so what was the lesson the lesson that I                                 wanted to share with you and that's what                                 I asked for today so I want to mention                                 that well if you want to work with large                                 amounts of data a machine learning etc                                 then we I ring if you want to have some                                 questions well please ask me offline and                                 thank you for your attention if you have                                 questions them                                 thank you for the talk so we don't have                                 time for questions anyone one of the                                 things for the talk by the way first                                 thing it was really interesting one of                                 the things you mentioned and I'm I've                                 been wondering about is that you have to                                 train with this large amount of data and                                 I'm wondering have you tried sampling                                 not that I know well I was not the only                                 one doing some experiments but maybe we                                 did the implemented inception that we                                 take from our stuff that we do that                                 doing sampling usually reduce the                                 quality reduce the quality of the                                 mailings but I don't think we have done                                 it on our actual letter but from all the                                 stuff that we do predictive stuff that                                 we do we have learned that sampling is                                 often not that good and we didn't try it                                 on this to to the best of my knowledge                                 but could be an idea to speed up other                                 questions all the data stored so yeah so                                 you mentioned that you have the initial                                 set that you change the system on so                                 what happens if you want to add to the                                 set if you if I want sorry if you want                                 to add to the set so if you want to add                                 data to for the set to be to operate on                                 larger volume of initial data other can                                 you add that after the initial                                 unlearning is done well that's a good                                 question something I forgot to mention                                 it's like we do this training every day                                 so like to be able to be to a fresh data                                 and to have written ten bathing's                                 because like you know the correlation                                 between the products the change over                                 time sometimes two products get really                                 correlated for one reason or one over so                                 but something also that we learnt that                                 we have to refresh to refresh it over                                 the time so this is the easy way to add                                 vet to add some new data to tour that                                 asset still the question that is behind                                 I think well one interesting questions                                 like we retrain it and then the point is                                 like when we retain the data then the                                 initial position are purely random also                                 so the final configuration of the                                 embeddings from one day to another may                                 change drastically the assumption that                                 we've made and we have to check at one                                 point like we we we even if the                                 distribution of the embeddings in the                                 space changes we hope that the                                 similarities computed between them won't                                 change much so this could be an ID                                 something we want to ask for trying to                                 seed the training sets like trying to                                 inside our in instead of spread                                 embedding randomly at the very beginning                                 that use the accurate and Eddings and                                 try to update the weights this way so                                 these videos model averaging after all                                 the splitting and you said you have                                 different works vector kind of domain or                                 category of subcategory of your products                                 right                                 do you use model averaging for like a                                 user session so the new user session                                 comes in and we have like a manager                                 network that pcap the right word too                                 vague or you combine it afterwards                                 how does work well the combination is                                 shortly purely averaging because we have                                 not found anything better for now but if                                 you have any suggestions and you love                                 and welcome but yeah we know that we we                                 have a use of quality I haven't not                                 plotted here that at the end you have                                 the of the curves and I have floated                                 like you have the merging step and then                                 you see big drop and then well it's rich                                 with so we know that we lose some quite                                 here using this it's the best solution                                 that we have found so far yes maybe the                                 radical thank you for the call and the                                 talk is a very good person okay thank                                 you very talk can you tell us about the                                 time scale how much time is from one                                 experiment to the next one to the next                                 one is months years days are for our                                 experiments yeah well the whole projects                                 I think it started one year ago so I                                 think this is somehow a Content view on                                 what we did so for example for the                                 convergence issue like it took us                                 well I will say from one to two months                                 is to actually figure out what to do but                                 you got the trickiest one and then                                 usually we try to what to do this on the                                 spring basis like we use creme select                                 two weeks - oh for - so those are the                                 issues and this is assuming which this                                 works and this is helped by the fact                                 that we have this really are closer also                                 we are able to learn the embeddings oh                                 well in a matter of hours so we can                                 launch multiple experiments at the same                                 time to get the result the day after and                                 then compare the results so you said                                 it's like                                                           would you train on it correct or is it                                 we know it was our events yeah like us                                                                                                          your input data so for example your                                 friends so that would be excellent did                                 you remove it and clean it sorry when                                 you have accent like it's a different                                 word you would generate a different                                 embedding for for a word of accent or                                 boom load no actually what we process                                 here is like we process the product IDs                                 so we do like at the beginning I                                 describe route to that which actually                                 works with worth button here now we do                                 this analogy would say a product is a                                 word so we only have one ID okay so this                                 is option UNIX we don't have this                                 normalization issue sorry I missed it                                 but did you did you also try past text                                 from Facebook I know we didn't I have                                 heard about it there are times that's                                 pretty fast I trained it with a few                                 hundred thousand comments and on my                                 laptop in a few like half an hour of                                 your books okay so yeah no we I've got                                 civil time about this this is definitely                                 something we should try any other                                 question                                 oh that was it                                 Thank You salmon was a really good talk                                 [Applause]
YouTube URL: https://www.youtube.com/watch?v=qYpdW9cyEqY


