Title: Berlin Buzzwords 2017: Sean Braithwaite - Mechanics of Data Pipelines #bbuzz
Publication date: 2017-06-15
Playlist: Berlin Buzzwords 2017
Description: 
	This talk focused the topic on how to model data pipelines as retroactive, immutable data structures. It covers the topic of how do you build a data pipelines for a growing organization where different teams depend on each others data and need to be able to re-process data when errors occur upstream. 

I draw comparisons between the microservice architectures for both stream and batch processings and provide some guiding principals towards building resilient systems based on experience scaling out infrastructure at SoundCloud.

Read more:
https://2017.berlinbuzzwords.de/17/session/mechanics-data-pipelines

About Sean Braithwaite:
https://2017.berlinbuzzwords.de/users/sean-braithwaite

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              okay so thank you for the introduction                               my name is Shawn                               I'm originally from Montreal in Canada                               but actually for the past five years                               I've been living in Berlin so it's super                               nice to do a presentation here and get                               to know a little bit of the scene                               happening for the past eight years or so                               I've been working with data in different                                capacities from like the infrastructure                                perspective maintaining databases to                                doing more distributed systems cassandra                                type work and most recently as a data                                scientist sort of producing I guess                                models of how consumers listen to music                                these different experiences have really                                informs the way I think about data and                                the stuff that we build and I've tried                                to synthesize some of those learnings                                into this presentation so the name of                                this talk is the mechanics of data                                pipeline and it's effectively for people                                who have been working with data and                                don't want to babysit ETL so first a                                little bit of context I work at a                                company called SoundCloud and SoundCloud                                is this you know music streaming                                platform similar to Spotify but                                distinctively different we have hundreds                                of millions of people you know all                                around the world consuming music but you                                know it sort of have gone through this                                tremendous phase of growth when I                                started we're maybe                                             something and are now I think four                                hundred we were initially using you know                                Google Analytics like I think most web                                properties and then inevitably started                                to build more things in-house so during                                this transition the the role of data                                also changed that we started you know                                doing machine learning we had to fix                                problems like spam and we also started                                taking it more seriously and allowing it                                to sort of drive our decisions so the                                context here you know that it's framing                                some of these recommendations and these                                thoughts about architecture are you know                                a diverse organization were different                                different domain experts by trying to                                collaborate with data to do something                                quite complex which is understand                                consumers                                so what are we talking about                                specifically well                                data pipelines are the software our data                                pipelines are the software structures                                which emerge to process and disseminate                                information but let's maybe break that                                down a little bit so theater pipelines                                are distributed systems so we need to                                talk about the ways in which they are                                distributed and what structures they                                form when we put them together and I say                                here I use the word emerge for a                                particular reason I would like to                                suggest that data pipelines are not                                something that could be designed upfront                                but are somehow structured and grow and                                we want to talk about in this                                presentation how to sort of shape that                                growth so here's an idealistic view of                                what one of these software structures                                looks like you have a single source so                                data coming from SoundCloud and then                                being disseminated to multiple stages                                each block is a stage and is owned by a                                particular team these teams very or                                these stages they vary in terms of run                                time the technology they use and the                                storage where they put the data based on                                sort of like distinct requirements of                                the domain of the team that holds them                                so each of these teams has diverse                                domain expertise which they use the                                Trent to enrich the data and in some                                kind of meaningful way each stage can                                consume from one or more previous stages                                and also provide data to multiple                                subsequent stages so why do we build                                these things                                well because problem domains like music                                like platforms with hundreds of millions                                of people are really complex to face                                these challenges you might split it up                                into multiple subdomains each with                                distinct sets of experts one way to                                think about data pipelines is as the                                software structure which enables experts                                 from different domains to collaborate                                 so how do we build them you know                                 sometimes not so well                                 these software structures have a                                 tendency to become quite complex as they                                 become more complex they take more time                                 to manage and they turn people with                                 expertise or people with PhDs into                                 babysitter's or into operations people                                 in the worst case they become impossible                                 to extent there are several reasons why                                 these these data pipelines become quite                                 complex and they could be you know                                 perhaps we don't understand the domain                                 that we're trying to model so say if                                 we're trying to solve spam we may not                                 have the right expertise in spam just                                 yet we might misunderstand the nature of                                 the integration so how do we tie these                                 together we might do in an ad hoc way                                 which sort of proliferate complexity and                                 we might misunderstand coordination we                                 might not have a good idea of what are                                 the times in which people need to                                 collaborate and what are the times of                                 which you want to automate that                                 collaboration so given all this I hope                                 by the end of my talk I will have                                 convinced you of three things first that                                 data pipelines emerge and need to be                                 designed foundationally different than                                 static software second that the air                                 shapes by the organization and domains                                 which produce them and lastly that what                                 prevents data pipelines from growing are                                 these cases of incidental coordination                                 so to understand the architecture of                                 data pipelines I think we need to map                                 the conditions which produce them they                                 say that people are the products of                                 their environments but I also think this                                 applies to software so Conway's law                                 states that the software we build is                                 somehow a reflection of the                                 communication boundaries of the                                 organizations which produce it in                                 general you know I think this is this is                                 true but perhaps doesn't go far enough                                 I would I would argue that the way these                                 boundaries form matter as well to make                                 this argument I think we need to talk at                                 this higher level abstraction from this                                 level I think we get a better view of                                 the forces shaping data pipelines and                                 make better decisions about design so                                 these things can start quite small                                 typically with a single ETL so say you                                 have some kind of events being written                                 to HDFS every time a user plays a track                                 so this is one of the fundamental things                                 that soundcloud as a platform does so                                 HDFS I hope as we we all know is the                                 Hadoop file system which is really great                                 at storing petabyte scale data sets it                                 isn't so good however at querying data                                 to make the data easier to query you                                 might want to load this into a database                                 this way you can figure out for instance                                 how many plays you got yesterday and                                 compare that to how many plays you got                                 this day last year or something like                                 this this is a perfect case for an ETL                                 but what is it actually doing well it's                                 creating this sort of split plays as a                                 concept now exists in two places in HDFS                                 and also in this queryable database but                                 this split occurs on different levels so                                 on the level of the storage system on                                 the level of bounded context and also on                                 the level of organizations and teams                                 understanding these splits is important                                 as it drives the evolution of a data                                 pipeline and ends up shaping it so                                 founded context is this idea developed                                 from domain driven design how many                                 people are somewhat familiar with domain                                 driven design I think a large amount                                 that's nice but let's actually define                                 our terms here so specifically it refers                                 to groups of people or possibly software                                 which use a consistent language to                                 describe domain objects so in this case                                 play as a music platform SoundCloud                                 records data                                 and plays in particular we use this data                                 for all sorts of things actually to                                 train machine learning models to make                                 recommendations of what people should                                 listen to next but also to perform                                 complex calculations to payout artists                                 royalties based on how much they're                                 engaging the platform but before we do                                 either of these cases we do this attempt                                 to filter out spam so now imagine we                                 have two separate teams one which is                                 sort of an expert in fighting spam they                                 call them trust and safety and then                                 another which is producing reports will                                 call that team royalties and reporting                                 the data that the spam fighters work                                 with is completely different than the                                 data we use to produce the reports but                                 they use the same domain object right                                 plays it just means different things so                                 one is subject to spam and might have                                 additional fields and the other is not                                 so if you were to say take a list of the                                 top ten countries that get plays the                                 data set that has spam would look very                                 different than the data set without so                                 the this filtering of spam represents a                                 split in the bounding context and which                                 the play takes on these these two                                 different meetings so having events like                                 plays and HDFS is a great first step                                 I could always write a MapReduce job and                                 figure out how many plays I got each day                                 that's easy but what if I want to build                                 a web application to show these play                                 counts the users say I want to embed it                                 in every single widget on every single                                 platform that SoundCloud is available                                 well in this case you know HDFS is not a                                 suitable option so differences in access                                 patterns drive the introduction of these                                 new storage mechanisms so HDFS                                 relational databases column restores key                                 value stores are all good at different                                 things as use cases evolve we often need                                 to introduce new kinds of databases and                                 the data pipeline is then used to                                 connect these together                                 so teams grow to the size that they                                 could somehow manage to communicate and                                 collaborate efficiently there are at                                 least two reasons why team split first                                 when teams get so large that they                                 naturally partition into groups that                                 work more closely together than they do                                 with with it with the rest of the team                                 and the second is when a problem domain                                 becomes so complex that the team                                 naturally needs to split to have a                                 coherent image of what they were doing                                 you know one case of this would be when                                 most startups begin everyone could sit                                 at the same table and you know there's                                 just like the engineering team and then                                 eventually maybe someone is more working                                 on data or someone's more working on                                 front-end and they decide that they need                                 to form separate teams and we need to                                 sort of facilitate communication between                                 them so sometimes that these splits are                                 also represented by differences in                                 bounded context and the reason for that                                 is because sometimes these splits are                                 actually nested they are hierarchical in                                 a way teams contain one or many bounded                                 contexts which are represented on one or                                 many different storage systems you know                                 transgressions of this hierarchy incur a                                 high cost of coordination so for                                 instance if I were to share a database                                 with a different team and I wanted to                                 change maybe add a column remove a                                 column or even just change the semantics                                 of a column I could wreak havoc on every                                 query that was somehow dependent on that                                 schema so this is this case of incident                                 coordination and it's what we're trying                                 to avoid so let's look at reasonable                                 ways that we could aim to cross these                                 boundaries of storage systems teams and                                 bounded contexts so these splits that                                 were sort of outlined are important but                                 they need to be crossed to enable                                 collaboration so these splits are what                                 is connected by the data pipeline and                                 they require somehow a mapping on each                                 level of the split                                 the goal of designing a data pipeline as                                 sort of like an aggregate software                                 structure right is really to capture all                                 these interactions between the entities                                 at the different levels so in the case                                 in which we are loading data from HDFS                                 into a relational database we must admit                                 that these storage systems differ not                                 only in how we use them but how they                                 work so if you were here for Steve's                                 talk that preceded this one you get this                                 really good example about how s                                        describe the operation might have a                                 similar API to a POSIX system but                                 actually what's happening underneath the                                 hood is fundamentally different this                                 idea of being able to get a consistent                                 view of data is not available on every                                 storage system so when we build an ETL                                 which is somehow connecting maybe a                                 consistent storage system to an                                 inconsistent storage system we are                                 mapping fundamentally the mechanics                                 between the between how these these                                 systems work so if we follow this                                 domain-driven design model communication                                 between bounded contexts is done with                                 events events are these immutable                                 objects which represent state change and                                 working with events really structures                                 the way we think about state by sharing                                 different events different bounded                                 contexts are able to construct distinct                                 views of the data which are defined by                                 their own internal logic in this way                                 state is something which is computed                                 instead of shared in the case of spam                                 and plays a team with domain expertise                                 in filtering spam would provide a record                                 for every play event augmented with a                                 classification and possibly a confidence                                 rating people consuming that data set                                 would then both have you know know what                                 is spam and what is not but it'd be able                                 to modulate their tolerance for spam                                 based on their particular use case so if                                 all you're trying to do is display some                                 counts to a bunch of users then you                                 might have a higher high tolerance for                                 spam                                 in exchange for timeliness but if you                                 are paying money based on usage you                                 might have a very low tolerance for spam                                 and you might accept that things are                                 going to take longer to process if it                                 gives you more confidence in the                                 classification so back coming back to                                 this case of spam and royalties what                                 happens when you see a spike of plays on                                 a single day no is it because we were                                 subject to a new kind of spam attack                                 which we are unfamiliar with and were                                 unable to mitigate or did kanye west's                                 release a track you know this is one of                                 this is a clear violation of the                                 intended isolation provided by violent                                 contacts and the result is this case of                                 incidental coordination where case or                                 teams you know have to get together and                                 reconcile their view of the data and                                 sort of come up with a new definition of                                 what is right and wrong before they                                 split again and become independent or                                 continue to operate independently so how                                 do we form these contracts between                                 stages in a data pipeline actually so                                 every team involved needs the same thing                                 then it's quite simple for example data                                 recorded yesterday will be available the                                 following morning in HDFS but this                                 doesn't work because not everyone needs                                 needs the same storage system and also                                 as an international company there's no                                 unified concept of mourning or one                                 person's concept good morning might                                 certainly not work for someone else                                 so these contracts between stages also                                 in some way need to be compatible as                                 ETLs become composed sort of the                                 transitive nature of the interaction                                 means that if I produce data at                                      a.m. the soonest that I can make data                                 available would be at                                                    takes                                                                  build these coordination facilities into                                 our software we'll have two coronate at                                 this much slower rate you know as people                                 these are these cases of incidental                                 coordination that we're trying                                 to avoid and we're trying to design                                 around in our sort of conception of data                                 pipelines                                 oops events as the sole communication                                 method which can go between bounded                                 context are owned by a single team I                                 mean they alone can change the                                 definition of that data but it has to be                                 somehow coordinated with with consumers                                 so when a team decides to change their                                 definition how are these changes                                 propagated throughout the pipeline is                                 really the question are there mechanisms                                 for embedding these expectations and                                 contracts and what recourse the                                 dependencies have when these contracts                                 are violated either we provide a formal                                 mechanism to evolve definitions or again                                 we incur this cost of incidental                                 coordination so individually TLS can                                 also be difficult to operate when they                                 try to model an unbounded amount of data                                 you know a single bad record could stall                                 a pipeline but the question is should it                                 you know how many bad records would make                                 the result unusable this is only a                                 question that we could answer through                                 coordination between producer and                                 consumer                                 but once agreed upon you know it's the                                 job of the software we build to enforce                                 these contracts and to provide                                 mechanisms for remediation when a job                                 does fail you know the question is then                                 who is responsible and what recourse did                                 they have in the case of manual                                 intervention should every downstream                                 consumer be called upon when something                                 upstream goes wrong I don't think so                                 instead we need to automate this process                                 and provide a mechanism which not only                                 responds to failure but provides a                                 method of failure resolution when a fix                                 is applied to an upstream job the                                 pipeline should be able to continue to                                 run automatically okay so the setting                                 that we've used so far has been quite                                 abstract you know the purpose was to map                                 out the conditions which shape the                                 software we built in this mapping and                                 mapping this out we were able to outline                                 what                                 are about working with data pipelines                                 specifically designing systems which are                                 expanding in terms of technology domain                                 and people simultaneously and it's with                                 this context that we can set upon the                                 task of providing concrete                                 recommendations first we'll lay out the                                 anatomy of an ETL just to define our                                 terms and then we're decide will                                 describe some some sane ways of gluing                                 it all together this glue will serve as                                 a protocol which specifies how and when                                 we read and write data to minimize this                                 risk of incidental coordination so in a                                 single ETL is pretty straightforward it                                 has a source some kind of computation or                                 job what kind of use this job or stage                                 word interchangeably and then some kind                                 of destination the source and                                 destination are storage systems they                                 could vary so one could be a database                                 and one can be HDFS the ETL here can                                 also be seen as a job which executes                                 with some regularity in a streaming                                 world this would mean when a batch which                                 is a certain size and in a batch world                                 this would mean based on some schedule                                 but each ETL represents not only                                 computation but effectively a domain                                 model so when we filter spam plays we                                 are expressing a model which attempts to                                 capture you know what is and is not spam                                 this model then produces a new                                 representation of the input set                                 augmented with this internal logic                                 within this transformation is also a                                 domain-specific notion of correctness                                 and sometimes this can be difficult to                                 share so how do we know when a job has                                 succeeded actually but Big Data the                                 computations we design have to account                                 for this enormous amount of variety with                                 this variety become comes the risk of                                 partial failure a job may complete in                                 terms of process completion but the                                 output might not reflect our                                 expectations so think about a job                                 completing but                                 producing maybe                                                         the typical three gigabytes so in this                                 case of spam you know a machine learning                                 model tasks which has no ground truth                                 there is very little objective measure                                 of what is and is not spam and the                                 result of this it means you know                                 deploying changes is actually quite                                 terrifying                                 on one hand we might be protecting users                                 from the various agents on the other                                 hand we might be penalizing legitimate                                 engagement so this ambiguous notion of                                 correctness has the effect of making us                                 as software developers reluctant to                                 change even if we suspect improvements                                 can be made we need to ensure that our                                 changes are objectively better better                                 from a job perspective we can produce                                 this fingerprint so this is a new term                                 that I'm introducing and sometimes                                 somehow it tries to capture a collection                                 of summary statistics which model the                                 underlying domain these statistics                                 should aim to capture both the variants                                 and the invariants in the data set so                                 for instance the spent the amount of                                 spam that's produced each day might not                                 be constant you might have you know good                                 days and bad days but what if we could                                 produce you know a summary statistic                                 which is invariant what if we look at                                 for instance the distribution of IP                                 addresses by country or the amount of                                 user agents we can capture this notion                                 of variants using this measure of                                 entropy which is somehow the amount of                                 chaos or structure and a signal in                                 either case we need this notion of                                 correctness it's important that                                 definitions be bound to the right domain                                 using the operational domain so like the                                 exit code of the status or what Hadoop                                 gives you back you know is really                                 insufficient for coordinating                                 correctness between producers and                                 consumers instead you know correctness                                 should be modeled in the business domain                                 and reflect the knowledge of domain                                 experts even as this knowledge changes                                 a notion of correctness captured by                                 outputting summary statistics is                                 essential for collaboration as ETLs are                                 composed fingerprints can be consumed by                                 downstream jobs to assert their own                                 notion of correctness making the nature                                 of that collaboration much richer so                                 when we compose ETLs we allow this                                 collaboration between additional domains                                 the example we've used so far has been                                 mostly about spam and licensing but as a                                 larger business of course you have many                                 domains communication between stages is                                 done with these sets of immutable events                                 which flow to the pipeline as each stage                                 augments them with some domain expertise                                 you know by chaining etl together we are                                 also creating this operational                                 dependency so problems at you know one                                 stage of the pipeline have a direct                                 impact downstream the the transitive                                 nature of these dependency means that                                 it's often not clear who is responsible                                 and who should end more importantly who                                 should be involved in in fixing it if we                                 don't design our pipelines correctly                                 fixing broken jobs can require tedious                                 and expensive intervention by teams you                                 know these cases of internal                                 coordination have many side effects but                                 I think one of them is is particularly                                 worth calling out and that's you know                                 erosion of trust if the if the same job                                 keeps breaking despite the best efforts                                 of the person who owns it or the team                                 who owns it you know those downstream                                 consumers can become increasingly                                 frustrated these frustrations can often                                 lead people to effectively design around                                 failing jobs creating more complex                                 Aleutians which are dramatically more                                 complex than than need be you know this                                 case of erosion of trust and designing                                 around failure is something we really                                 need to avoid instead we want to design                                 for failure in an explicit way so                                 change which happens at one stage of the                                 pipeline has direct impact downstream we                                 know this now but what we want is a way                                 to respond to this change in an explicit                                 and automated way to do this what we                                 need is a dependency structure but this                                 dependency structure exists at two                                 distinct levels so first we try and                                 capture the dependency between jobs I                                 think that's the easy part but the                                 second captures dependency between data                                 so dependencies between jobs can really                                 be handled by a typical scheduling                                 system like air flow or Luigi these                                 systems ensure that different stages of                                 a pipeline can easily be chained                                 together these systems allow it the                                 parametrized execution and specify                                 things like you know the input folder on                                 HDFS versus the output folder and in                                 this way they have like a rough                                 understanding of the dependency between                                 data but maybe we need to be a little                                 bit more explicit than that and actually                                 we had a very nice talk last week by                                 Michael Houser from research gates                                 presenting momento which actually solves                                 this problem I would advise all of you                                 check out the slides when they're                                 released but what about the case in                                 which change doesn't follow a                                 traditional schedule so what happens                                 when data arrives late or report has                                 already been generated and then we                                 realized that it was missing some data                                 does it become the case first of                                 recognizing late arrival and then                                 contacting every team of every stage of                                 the pipeline to rerun the jobs you know                                 I hope not instead pipelines should be                                 able to use this dependency structure to                                 run automatically I'm introducing this                                 term here convergence as it captures the                                 idea that data is the conclusion of some                                 process this process should be                                 indifference to the number of steps it                                 takes actually it should simply converge                                 conversions you know it ensures that our                                 jobs update automatically when change                                 happens upstream it requires these                                 explicit links between jobs and the data                                 they generate with the benefit of                                 limiting manual intervention but some                                 kinds of intervention cannot be avoided                                 in particular when our notion of                                 correctness changes so when working with                                 data oftentimes our notion of                                 correctness is mutable updates the                                 domain model need to be reflected in the                                 data which represents them in the case                                 of spam                                 you know definitions of spent what is                                 and is not spam are changing all the                                 time as we get exposed to new kinds of                                 attack and just continue to learn but                                 one our definition of spam changes it                                 means that data we might have been                                 certain about in the past can no longer                                 be trusted and we need to redo old                                 computations so data pipelines which can                                 redo computations are called retroactive                                 this property allows domain experts to                                 effectively change their mind when they                                 do we can then ensure that this changes                                 are materialized in the data I know this                                 might sound extreme but I would say that                                 data pipelines which don't have this                                 property can not really be trusted there                                 would always be this risk that the data                                 these pipelines provide is a reflection                                 of bad assumptions on the contrary you                                 know data pipelines which are                                 retroactive can be trusted as they can                                 actually represent you know the best of                                 our knowledge you know advocating for                                 mutation especially in saw this highly                                 distributed world that we're in can seem                                 quite scary but maybe it doesn't have to                                 be we know that we have to change the                                 data to reflect changes in our domain                                 model but it's not always obvious how to                                 do this with big data systems given that                                 our data pipeline spans multiple storage                                 services there's no guarantee that we                                 could simply update records like we can                                 in a database                                 instead I would advocate for thinking                                 about data pipelines as immutable data                                 structures immutable data structures                                 never override data but instead produce                                 multiple versions these versions of data                                 can actually be tied to the software                                 which produce them since each version of                                 the data comes with a fingerprint                                 summarized in the data it contains or we                                 can quickly measure if we've actually                                 made improvements so say our fingerprint                                 includes like our classification rate                                 and then we could make updates to the                                 software to bring that into what we                                 consider to be a reasonable balance as                                 an individual owning a single job these                                 facilities provide me with a way to be                                 confident in making changes without the                                 risk of overwriting data or inciting a                                 case of incidental coordination you know                                 I could deploy my changes automatically                                 in combination with output fingerprints                                 I could put my software in a continuous                                 delivery pipeline and every time I make                                 a change into code I could actually make                                 a change to the data in this way data                                 pipelines as distributed software                                 structures can start utilizing a lot of                                 the development that's been happening in                                 other fields in particularly in micro                                 services so taking a step back these                                 properties I've defined are not simply                                 about individual jobs instead they                                 describe primarily the interaction                                 between jobs and in combination                                 hopefully the aggregate behavior of the                                 system making an individual stage                                 idempotent or capable of scheduling                                 itself when the underlying data changes                                 it's very useful it's something I would                                 certainly advocate that you do however                                 I'm not sure that goes far enough                                 expectations of operational behavior                                 shouldn't vary throughout the pipeline                                 this will create those preferences of                                 what people what data people are                                 comfortable depending on instead these                                 properties that I outline our objectives                                 for a collective common protocol for                                 collaborating with data                                 okay so in this talk I try to present                                 data pipelines in in a new way I took a                                 step back and defined them not as                                 incidental Frankenstein compositions of                                 software but structures these structures                                 emerge for a good reason to connect and                                 distribute domain expertise we looked at                                 the specific forces governing the                                 evolution of data pipelines over three                                 levels storage systems founded contexts                                 and teams this evolution came with a                                 need to map between separate entities                                 and acts which came with a consequence                                 right coordination we then tried to make                                 the distinction between incidental                                 coordination and the mechanisms we could                                 build into our software to avoid them                                 avoiding internal coordination came from                                 three suggestions making dependency                                 structures of jobs and data explicit                                 ensuring the output of jobs never                                 overrides data but simply provides a                                 newer version and finally by baking                                 baking in the assumption that                                 calculations we perform on data are                                 going to be wrong and that we most                                 certainly need the facilities to make                                 them right so finally you know data                                 pipelines connect and enable                                 collaboration between diverse                                 perspectives their growth is a                                 reflection of our need to form a richer                                 view of the world and we most certainly                                 need to build them better thank you                                 we have time for a few questions thank                                 you very much I thought it was extremely                                 insightful and interesting talk I have a                                 question if you could maybe say a few                                 words about two domains that are                                 connected not by an ETL type pipeline                                 but simply by data replication in which                                 of your thoughts are similar or                                 different on that kind of system data                                 replication so I imagine maybe your                                 ingestion and yeah and the spam                                 detection might actually share data that                                 aren't transformed in any way well I                                 think the ingestion phase right which is                                 somehow summarized in a single block in                                 the diagram of course that's actually                                 multiple stages so one of these things                                 would be encoding so we take data that                                 might arrive from JSON as sort of the                                 the lowest common denominator of formats                                 between different clients and then we                                 might encode it as a protobuf at                                 ingestion another thing that we do at                                 the ingestion stage is account for some                                 notion of deduplication that happens                                 from up for operational reasons instead                                 of semantic reasons so the a lot of the                                 tools we use like Kafka you know they                                 they get their delivery guarantees                                 through retries so we try and account                                 for that at the ingestion phase and                                 isolates sort of different domain                                 expertise from these operational                                 complexities does that sort of capture                                 the replication question yeah please any                                 other questions                                 I really like the fingerprinting idea                                 and it sounded I really understood how                                 you can use it for your everyday traffic                                 let's say but you mentioned that you                                 have to differentiate between a Kanye                                 West releasing a new track or is it a                                 spam attack so I imagine that the stats                                 there would change yeah so how do you                                 know what to do so this is this case of                                 incidental coordination actually sitting                                 in front of you is someone were working                                 with on this very problem so maybe you                                 could catch up with her afterwards but                                 truly every SoundCloud is a really                                 interesting problem domain because of                                 this variance of signal right it's                                 incredible the things that you could                                 arrive like we have to remind ourselves                                 every year of Ramadan to understand even                                 from an operational perspective we see                                 the the world change I think when when                                 someone releases a big attract like some                                 years ago was Miles mill had a has some                                 kind of battle with with Drake or                                 whatever and I think our ingestion                                 infrastructure like scaled by an order                                 of magnitude or something like that and                                 in this case you know it really is                                 everyone getting paged jumping on a                                 channel and speculation you know there's                                 very labor-intensive cost because people                                 are not doing the regular work but                                 actually debugging and I guess the the                                 frustrating part with these cases of                                 incident coordination is you don't know                                 if you're doing the right thing actually                                 like should I be looking is it is it you                                 like is it is it this person but the                                 reality it happens and I hope in this                                 talk I didn't really present mechanism I                                 didn't provide any recommendation on how                                 to detect it but instead how to                                 remediate it that once you if you say                                 that is a spam attack you know that data                                 is going to have to be reprocessed and                                 that part you can automate but figuring                                 out as a company you know that you might                                 be under                                 attack from a state actor or whatever                                 it's really expensive it's all hands on                                 deck and it's an amazing process to go                                 through actually I agree any other                                 questions                                 let's thank the speaker again                                 [Applause]
YouTube URL: https://www.youtube.com/watch?v=IHbh_MINeIM


