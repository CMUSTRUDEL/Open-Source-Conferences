Title: Berlin Buzzwords 2017: Marvin Justice - Migrating a Real-time News classification Engine to ...
Publication date: 2017-06-15
Playlist: Berlin Buzzwords 2017
Description: 
	As the world's leading provider of financial news, Bloomberg LP ingests on the order of 1 million news stories per day from over 100 thousand sources in over 40 languages. To facilitate users' ability to quickly retrieve news tailored to their specific interests, stories are run through a classification system containing hundreds of thousands of rules where they are tagged in real-time with a mean latency of under 50ms. 

In this talk I'll discuss the migration of the news classification engine from a legacy system to a solution based on Luwak/Lucene while retaining the query language of the existing corpus of rules.  

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              okay so yeah get in right now okay all                               right so welcome to the talk migrating a                               real time new specification engine to                               lieu a key seeing here's what I'm going                               to try to cover first I'll give an                               overview of a rules-based news                               classification at Bloomberg then I'll                               describe the legacy engine that's                               currently in use and the OTL query                                language then I'll give a real quick                                intro to Lou ACK                                for those of you that are unfamiliar                                with it and that leads to the main part                                of the talk which is how we built the                                Luwak application that understands this                                OTL query language and reproduces the                                behavior of the legacy system and then                                I'll say a few words about a custom                                version of solar we used to test                                everything and conclude with the current                                status of the effort and give you some                                performance numbers okay so Bloomberg is                                a large company with many products the                                principal product is the Bloomberg                                professional service also known as the                                Bloomberg terminal the terminal consists                                of a UI that connects to Bloomberg data                                centers from a client's PC or Mac and                                along with the custom keyboard the UI                                has a command line or a function line                                that accepts almost an unlimited number                                of individual functions among which the                                most some of the most widely used or                                several that are news related in the                                data centers we ingest over a million                                new stories per day from thousands of                                sources around the world in over                                   languages all for the Li the stories are                                indexed to search collections which were                                converted to solar in just the past few                                years since we're talking about news we                                don't necessarily have to keep every old                                story but any given time we maintain                                active collections containing                                approximately                                                    subscribers can enter up to                                           new searches per day                                they can do keyword searches but it's                                more useful for us to tag the stories                                say by a company stock ticker such as                                Apple or by topics such as gold or M&A                                and allow the users to search for the                                tags instead for good overview of new                                circuit Bloomberg you can check out this                                talk from last year's buzz words okay                                here's a picture of a Bloomberg keyboard                                this is one of the older models but you                                still see them around note the special                                yellow and green keys if you were to                                type the company stock tip or ticker for                                Apple in the UI and hit the yellow key                                label equity and then select news from                                the pop-up menu this is an example of                                what you might see the default view is a                                time sorted list of headlines along with                                an optional display of the companies the                                stories are tagged with the wire source                                of the story and its time of last update                                so for example at the bottom we have a                                story about companies competing over the                                world supply of cobalt so it gets tagged                                with the companies mentioned in the                                story                                it looks like Tesla got the highest                                score here probably because Tesla was                                mentioned in the headline but if you                                were to hover your mouse over that                                ticker column you'd see Apple and also a                                few other companies so also if you leave                                this screen open for a while what you                                see is new stories satisfying the search                                criteria with scrolling from the top                                pushing older stories off the bottom                                that capability is provided by the news                                alerting subsystem which nowadays also                                runs on top of a little act ok so what                                is rules-based news classification it's                                part of the news ingest pipeline which                                receives stories from various fees and                                an average of around                                               second but news can be very bursty so at                                any given time we might see                                Ickes of several many times that rate                                classification is also part of something                                called the primary build where we take                                all the active stories and Riaan jest                                them as rapidly as possible                                and we might want to do that if for                                example the classification rules are                                 updated and we want that to be reflected                                 in the historical stories in the in the                                 pipeline the stories are processed                                 through several stages before finally                                 being indexed solar in the                                 classification stage of the pipeline the                                 stories are tagged against hundreds of                                 thousands of rules the rules are written                                 in the OTL query language which I'll                                 describe in detail in the next few                                 slides most of the rules are for                                 companies and most of those are                                 so-called templated rules we take                                 information about a company from a                                 database and then Auto generate a rule                                 from a template but the larger and                                 better known companies have handwritten                                 rules and there are also thousands of                                 handwritten rules for topics such as                                 industry currency etc the rules are                                 written and maintained by a group inside                                 Bloomberg a group of domain experts                                 inside Bloomberg called the content                                 indexing team yeah so these stories pass                                 through the pipeline it gets tagged                                 against hundreds of thousands of rules                                 and since we have so many rules a brute                                 force approach isn't is it feasible so a                                 more sophisticated approaches needed for                                 the past ten plus years Bloomberg has                                 been using a classification engine                                 provided by Verity that was before my                                 time but I believe Bloomberg began using                                 Verity just a few months before it was                                 acquired by autonomy                                 and autonomy continue to support the                                 product until they were eventually                                 acquired by hewlett-packard and                                 hewlett-packard continued to support us                                 until just recently when they decided to                                 drop the product the very                                 declassification Inogen is provided as a                                                                                                     written in the OTL query language which                                 was invented by Verity as far as I know                                 and overall the Verity engine has been a                                 good workhorse for Bloomberg achieving                                 average latencies of around                                    milliseconds per story the maximum                                 latency is not always so good however                                 and that can be tens of seconds but the                                 really high latency stories are not what                                 we would consider urgent market-moving                                 news we get a small but steady stream of                                 stories that are really like a research                                 reports that can have lots of large                                 attachments and that can take a lot of                                 classified okay ok L stands for outline                                 topic language it is a complete IR query                                 language that provides term and phrase                                 leads but also has a phrase operator for                                 writing multiple related phrases in a                                 compact form which I'll illustrate on                                 the next slide it has support for basic                                 star and question mark wildcards plus a                                 support for a limited subset of regular                                 expressions it has the basic boolean                                 operators and or not for proximity                                 queries it has an ear but also a start                                 zone in OTL fields are called zones in                                 the start zone serves to limit a query                                 to the first end positions of the field                                 for frequency queries it has at least                                 slash n which takes a single child and                                 basically says match if this turn                                 rephrase appears at least in time                                 the count slashing operator takes                                 multiple children and says matches any                                 end of the children appear there                                 miscellaneous operators such as sound x                                  phonetic clinic phonetically sounds like                                 the Saurus for synonym which have never                                 been used at Bloomberg in which we                                 consequently decided not to implement in                                 the new system very important is that                                 oto provides for absolute scoring so                                 this is a good for a classification                                 problem which is just with a single                                 document at a time and for which loosens                                 rank based scoring doesn't entirely make                                 sense                                 there are several operators devoted                                 solely to modifying the scoring behavior                                 for example the any and many operators                                 are exactly like the or operator when it                                 comes to matching but have different                                 scoring behaviors most of the operators                                 have direct leucine analogues but not                                 all of them the consequence is that                                 we're going to have to we had to                                 implement some custom leucine queries on                                 our own for example leucine has a spans                                 first query which is similar to the                                 start zone our operator but at least as                                 of solar                                                                currently on saying first query doesn't                                 support multi valued fields which our                                 document is contained the count slash n                                 is basically the same as a boolean                                 minimum number said match in leucine                                 except leucine at least as of                                            a plot it doesn't support a spans                                 version of that query which we need a no                                 TL ok so here is an example of what OPL                                 looks like we have an N operator which                                 serves to limit the query to a specific                                 field headline in this case and OPL you                                 don't necessarily have to specify a                                 field in which case the query applies to                                 all fields                                 beneath the end we have a phrase                                 operator containing to any list                                 parent-child relationships between                                 operators are specified by indentation                                 with asterisks the phrase operator here                                 acts like an outer product forming all                                 possible ordered combinations of the any                                 list so in this example we're saying                                 match if any one of these                                           equals                                                                  so you might ask why not just write out                                 the six individual phrases and it turns                                 out that it's more efficient to specify                                 it can be more efficient to specify them                                 like this probably for six phrases it                                 wouldn't make much difference but we                                 have cases we have over a thousand                                 related phrases and specifying them with                                 a phrase and any's is noticeably faster                                 this is yeah like I said this is a very                                 simple example in our system the                                 shortest queries or over                                                with the longer ones clocking in at over                                                                                             classification engine with its own query                                 language which is being retired by its                                 vendor so we got to come up with a                                 replacement first requirement is that we                                 want to do this in such a way as to be a                                 minimal burden for the content indexing                                 teams there are literally tens of                                 thousands of handwritten rules and we                                 can't really ask these guys to rewrite                                 their queries in a new language so the                                 replacement must be able to interpret                                 OTL input also there are several                                 components downstream of classification                                 that rely on the existing scoring scheme                                 so we need to replicate that as well the                                 next requirement is that the performance                                 must be as good or better we are allowed                                 to spend a bit more on hardware as long                                 as it doesn't get out                                 and there are two aspects to performance                                 latency and load for latency we have the                                 legacy systems average of                                    milliseconds as our target for the load                                 the ultimate test is whether we can keep                                 up during primary build and finally we                                 need a way to for content indexers to                                 test new rules but Verity provided a                                 nice authoring environment for both                                 writing and testing rules we don't need                                 to replace the whole authoring                                 environment right away but we do need a                                 way to allow content indexers to                                 generate custom collections of stories                                 and run the rules against them ok so                                 where do we start well it turns out                                 there's a open source library called                                 Luwak that does exactly what we want and                                 the quote here is from the github readme                                 for Lu ACK simply put Lueck allows you                                 to define a set of search queries and                                 then monitor a stream of documents for                                 any that might match Lueck is the                                 brainchild of Alan Woodward who spoke                                 here earlier today and his colleagues at                                 flax in Cambridge England to really                                 describe Lueck and any debts would                                 require its own separate talk but                                 fortunately Alan woodwork gave just such                                 a talk at Berlin buzzwords                                            you can find online there briefly the                                 Lueck is based on uses a query index and                                 app researcher to do an inverted search                                 where documents are treated as queries                                 and queries are treated as documents of                                 the document it's essentially turned                                 into something like a giant boolean or                                 at its terms and run against the query                                 index in order to quickly select out                                 only those queries that have any chance                                 of matching and the queries that are                                 left over after pre selection can then                                 be run in brute-force parallel                                 fashion prior to the migration of                                 classification to Lueck bluemark had                                 already migrated the news alerting                                 subsystem to Lueck and Daniel Collins                                 gave a nice presentation on that which                                 you can find online their classification                                 is a little bit more challenging than                                 alerting because it uses the OTL query                                 language whereas alerting uses a pretty                                 basic query language                                 okay so we have Lueck and we need a                                 version that understands OTL and                                 replicates the existing scoring here are                                 the pieces the main pieces we're going                                 to need to do that which I'll elaborate                                 on the next slide next few slides first                                 we're going to need a Nokia parser                                 obviously we need analysis change which                                 seems straightforward but there are                                 complications in our case and since not                                 all OTL constructs can be represented by                                 existing lease inquiries we're going to                                 have to implement some custom queries                                 and figure out how to do the scoring and                                 once when you implement custom queries                                 then we're gonna have to do some                                 extensions to Lueck finally when we have                                 all that we need to integrate it into                                 our pipeline so where do we start                                 well the first thing we did was with the                                 higher flag                                 okay so Alan came on board as a                                 consultant and worked for a few months                                 to help get this project off the ground                                 in particular Alice Allen provided us                                 with nearly complete                                 okie L parser for which he chose the                                 atler for parser generator                                 he wrote a grammar file from which                                 lecturer parser listener components are                                 generated using antler tools and then                                 Allen wrote a query builder class to                                 take these generating components and                                 translate OTL into a loosie loosie query                                 object in a top-down walk of the parse                                 tree                                 okay analysis changed the issue here is                                 that the legacy system has its own                                 proprietary analysis change and those                                 are reflected in the way the rules are                                 written since in verity queries are                                 whitespace tokenized for example                                 consider Twitter handles okay so when                                 Twitter first began to become a big                                 thing for News the legacy systems                                 tokenizer config file was modified to                                 not treat at symbols it's punctuation                                 and specific Twitter handles were added                                 to classification rules in some cases                                 well that's a problem for our live scene                                 which wants to treat the @ symbol is                                 punctuation and just discard it                                 so the solution we chose was to fork                                 create a fork of leucine standard                                 tokenizer and modify the underlying                                 j-flex file so that at and a few other                                 symbols aren't treated as punctuation                                 another case was Korean where we had to                                 create a separate Fork of the standard                                 tokenizer                                 in order to handle mixed character words                                 where the legacy tokenizer                                 and which sees the scenes tokenizer have                                 different behavior for filtering we                                 found that the combination of lower case                                 filter and ASCII foley filter replicated                                 the legacy system quite well except for                                 a few cases where Lucy's ASCII fold and                                 filter pulls a few more characters in                                 the legacy system that did so to handle                                 that we simply added exclusion list to                                 the SD falling filter okay Chinese                                 Japanese that's where the real problems                                 occur so the legacy system used a                                 dictionary based tokenizer                                 from bassist act circa                                            initially we reached out to bassist AK                                 and asked if they could provide us a                                 solar pluggable version of that exact                                 tokenizer and they said they couldn't                                 really do that would we like to try                                 their latest version instead so we tried                                 the latest version but it was just too                                 different from the from the earlier                                 version so then we tried all the                                 available solar alternatives and found                                 that the one that comes closest to the                                 one we've been using is the ICU                                 tokenizer the ICU tokenizer is also a                                 dictionary based and empirically we can                                 tell that there's a least a                                     agreement between the ice cute                                 dictionary and the legacy dictionary but                                 that's exacerbated the problem is                                 decimated by the fact that in the legacy                                 system when an indexer wants to search                                 for a Chinese phrase for example the                                 very authoring tool will insert                                 whitespace where its dictionary says                                 says it should go so one thing we can do                                 to help for the new system is to abandon                                 whitespace tokenization for chinese and                                 japanese and instead to programmatically                                 reto kanai's the OTL according to the                                 new dictionary and that helps a lot that                                 brings the agreement up to above                                         it's still not good enough so one thing                                 we've been playing around with recently                                 is to customize the ICU dictionary                                 itself to try and bring it into a better                                 agreement with the old dictionary and                                 we've had partial success with that                                 approach still early and it's still an                                 open question as to whether it's going                                 to fix all our problems ultimately we                                 may have to bend our requirements Evette                                 and as the indexing team to adjust some                                 of the chinese-japanese rules okay now                                 on to custom queries and scoring as                                 mentioned not all OTL operators have                                 Lucene analogs so for those we had to                                 by custom queries for example for the at                                 least operator we wrote at least query                                 by Sicily copying an existing Luke                                 lucien query and then just modifying it                                 so that its internal iterator checks the                                 frequency of the sub score before                                 deciding whether there's a match we also                                 had to come up with a spans version of                                 that one and span min match query for                                 proximity queries containing count                                 operators                                 I'll cover the span starts query in                                 detail on the next slide we had to write                                 custom queries for all the scoring                                 operators but that wasn't really too                                 hard he just involved taking existing                                 Lucene queries and changing the scoring                                 methods for example it's just Junction a                                 crew craters and a crew operator in                                 ochio they're just judging a crew query                                 is very similar to the existing                                 disjunction max query except for the                                 scoring methods even for operators that                                 have direct leucine analogues we found                                 that small tweaks to lose scene were                                 required for example the scene has a                                 disjunction score or class that among                                 other things has a frequency method                                 calculates frequencies by incrementing                                 by one for each matching sub score well                                 I needn't give the correct answer for                                 OTL but by just modifying this this one                                 line of code we're able to reproduce the                                 legacy system pretty much exactly we had                                 to tweak the slot formulas for near                                 queries and I'll cover that in a further                                 slide ran into a lot of difficulties                                 with nested span queries which are very                                 heavily used in our system in fact                                 there's a open Lucene issue                                         called nested span queries or buggy                                 which pretty much sums up our experience                                 but we found that most of those issues                                 could be resolved by simple modification                                 the sort order                                 we'll see expand position queue okay so                                 in this slide I'll try to walk you                                 through our custom span starts query                                 recall that TL has a start zone operator                                 that serves to limit a query to the                                 first end positions of a field                                 well leucine has a span first query that                                 you know sounds like it might work can                                 we can we use that so okay for a test                                 let's try a query simple query that says                                 match is the term Lu act appears in the                                 five first five positions of the                                 attachment field okay here we have a                                 document with two attachments of a                                 thousand tokens a piece in the first                                 Luwak appears at position                                               no good but in a second                                 you know Lueck it's within five                                 positions of the start so yes this                                 document should definitely match the                                 span first query however uses a single                                 iterator and it can only check whether                                 the position of that iterator is within                                 five positions of the start so it sees                                 the second Luwak in position                                          says no this document doesn't match our                                 custom span starts query on the other                                 hand uses two iterators and relies on a                                 trick we picked up from Allen Woodward                                 so in the document analysis chain we                                 have a custom token filter that is                                 certain search to start anchor token at                                 the beginning of each value of a field                                 this token filter runs last in                                 particular after the lower case filter                                 so if we choose a start anchor with                                 containing capital letters there's no                                 chance that that token will ever be seen                                 by an end user so then we can have one                                 iterator iterate over the start tokens                                 while the other iterator eighths over                                 the Luwak tournament before and now if                                 we subtract the positions of the two                                 iterators we get we can get the correct                                 answer one thousand three you might                                      it's less than five so it's a match okay                                 so we had to modify the slot                                 calculations for both ordered and                                 unordered near queries and here I'll try                                 to walk you through the unordered near                                 case so we have a query fragment near                                 slash for of three terms in OTL there's                                 no such thing as nearest slash zero near                                 slash one is as close as you can get so                                 when we translate this to leucine it                                 becomes an order unordered span near                                 query with a slop of three so next                                 consider this text fragment does it                                 match well the three terms are all                                 present spanning positions from zero to                                 five what what's the slot if you look in                                 the near spans and ordered class the                                 slot is calculated as follows so you                                 have the max in position in cell dot in                                 position well that's six in this case                                 because the in position of a turn spans                                 is it start plus one minus the min                                 position cells start position which is                                 zero in this case minus the total span                                 length well what's the total span length                                 if you look where that's calculated it's                                 just the sum of the lengths of the                                 individual sub spans so that would be                                 three in this case so leucine would                                 calculate the slop at six minus zero                                 minus three equals three and say yes                                 this is a match in fact in the scene it                                 would be possible to have a near / one                                 say a hundred terms okay that's okay                                 that's that's one way to do things but                                 that's not how LTL works                                 and OTL a new year / four means                                 literally that there can be at most four                                 positions between the start and the the                                 first term and the last term as it                                 appears in the document so we had to                                 modify the slot formula in the near                                 spans an ordered class to calculate the                                 slots as follows and we get the fifth                                 it's not a match                                 and we're good to go okay so we've made                                 some changes to the scene do we have to                                 change it change any luat code the                                 answer is no we can run stock glue act                                 because Lueck is designed in such a way                                 that if you add custom queries and all                                 you need to do is extend some Louet                                 classes so recall that Louis Lueck                                 builds a query index and it does that by                                 using a query extractor critics eckers                                 so we had custom queries then all you                                 need to do is add custom extractors so                                 we also use a performance feature in                                 Lueck called query decomposition where                                 for example disjunction queries are                                 decomposed into their individual sub                                 clauses which can then can be selected                                 independently by the pre searcher and so                                 we just extended the query decomposer to                                 an opioid query decomposer which knows                                 how to decompose our custom queries                                 likewise for highlighting functionality                                 we extended the span rewriter class to                                 an ocl span rewriter class to handle the                                 custom queries okay so we have now a new                                 engine and we need to integrate it into                                 our pipeline how do we do that well the                                 pipeline consists of a set of discrete                                 Blumberg application services or bath                                 services so we're going to need a bath                                 service for our new engine                                 traditionally bass has been a native                                 code framework at Bloomberg and so with                                 the legacy engine all we needed to do                                 was link in the dot SL library provided                                 by the vendor                                 well Luwak and latina written in Java                                 and at the time we started this project                                 bass Java wasn't really available                                 although it's sort of available now so                                 we decided to do                                 us to integrate the new Luwak based                                 engine into a native bass using j'ni and                                 we call that new service Oryx there                                 would have been other alternatives for                                 example we could have embedded Lu AK                                 into a standalone web service and send                                 an HTTP request to it many people do it                                 that way                                 we chose the j'ni approach you know it's                                 worked well for us and so we're happy                                 with it                                 these auric services run on a farm of                                 Linux machines spread across multiple                                 data servers data centers the rules are                                 charted four ways which we found to be                                 sufficient so each each of the four                                 shards is replicated across each machine                                 of the farm and then the replicas are                                 fed by intelligent routers which track                                 the number of in-flight jobs and route                                 to the replicas with a leaf load we                                 currently have the parallel master                                 thread count of Lueck sent to to we we                                 tried sending it to four but found that                                 that didn't really work for new spikes                                 or primary build for those cases we                                 really need to be able to classify                                                                                                          so to avoid CPU starvation we had to set                                 the the matress thread count to two once                                 the replica has                                                     stories then any subsequent requests are                                 queued up but that's very rare in our                                 system okay so the last piece of                                 requirements was to provide a way for                                 content indexers to generate collections                                 and test their rules against them that's                                 just the classic search problem you have                                 one query you want to see which                                 documents and matches and why so what we                                 did was create a                                 custom branches of solar containing the                                 OTL parser the custom query the leucine                                 twigs just like everything except Lueck                                 the only additional change we made to                                 solar was to modify it so it could                                 handle so they had per language analysis                                 changed then we threw together a                                 Bloomberg UI and this is what that looks                                 like so this is a purely internal screen                                 a customer would never never be able to                                 access this so we have a you know in the                                 grid box we have several collections of                                 stories ranging in size from a million                                 all the way down one story there's a                                 button where you can create a new                                 collection if you want the yellow box                                 you can write a short query in either                                 OTL or plain so our format with the                                 other tabs you can upload pre-existing                                 OTL rules and then run them against the                                 collection the results are displayed                                 pretty much the same format as the what                                 I shows the very beginning of the talk                                 with the Apple search only if you click                                 on a headline in this UI you'll see the                                 query the result the story displayed                                 with a query hits highlighted ok so                                 that's it the current status is that                                 we're now running parallel streams in                                 the pipeline for both the old and new                                 engine so far we only keep the results                                 from the old engine except for a tie                                 we've recently had interest for Tai                                 language news and the legacy system does                                 it support Tai so we went we've gone                                 ahead and turned on Tai for the new                                 system for the other languages we're                                 doing the process of doing side-by-side                                 comparisons the the two engines and what                                 we found for for non Chinese Japanese                                 language is that the new system is able                                 to reproduce the results with a greater                                 than                                                                includes the scoring                                 for Chinese Japanese it's not quite so                                 good                                 slightly above                                                          rule dependent however you know some                                 some rule Chinese Japanese rules or you                                 know above the                                                    significantly worse and the problem                                 there goes back to tokenization which we                                 still have to figure out how about                                 performance well when we started we were                                 on solar                                                              then we officially upgraded to solar                                     and things got miraculously better so                                 today we're on solar                                               achieved an average latency of                                    milliseconds per story which is a nice                                 improvement over the the old system and                                 for large the larger stories we're way                                 ahead the maximum latency of a few                                 seconds as opposed to a few tens of                                 seconds the new system handles load as                                 well or better than the old system the                                 only caveat is that it does use twice as                                 much hardware and probably we could have                                 got by it with a little bit less than                                 that we put in the hard order order when                                 we were on in solar                                                   thought we were going to need a lot                                 that's a you know it's teardown so we're                                 going to use it                                 the only drawback to the Luwak system                                 and then it starts up much slower than                                 the old system barity had some magic                                 where you could create a binary version                                 of the query data offline so that when                                 the classifier starts up it's ready to                                 receive requests almost instantaneously                                 with Lueck we have to index the queries                                 every time we start a process and that                                 can take a few minutes in our case but                                 it's not you know it's not a big deal                                 because we have a high degree of                                 replication we can restart processes                                 when we need to without causing any                                 hiccups so that's it                                 [Applause]                                 cool thank you for the talk and now we                                 have time for two free questions okay we                                 have one here                                 hello you explained how did you how                                 ordering of spawnposition queue helps                                 you fix the okay so Nestor it's funny                                 you should read that is that that ticket                                 so actually we figured this I think it's                                 kind of obvious if you if you're like in                                 a debugger and see what's going on                                 we figured it out kind of on our own                                 before I was even aware of this ticket                                 but there's a comment by Alan actually                                 about this in the ticket although it's                                 not been submitted as a package for us I                                 I know so the the spam visit so you sort                                 by start position first the Louis start                                 position and this is just a tie the                                 existing code says lowest in position                                 well you don't really what you want you                                 really want to make it that big the                                 greatest in positions and if you do that                                 just you know changing a less-than side                                 to a greater than sign everything much                                 is overlapping you might actually okay                                 there's a problem with overlapping stuff                                 too and I said that you know this                                 resolved most of our issues but not all                                 so the other thing we do we kind of get                                 get around some of the overlapping stuff                                 but we don't we try to avoid in position                                 as much as possible and just use start                                 positions and that way we kind of but we                                 still have issues with the end the                                 overlapping stuff so there's something I                                 didn't show here but you can have a near                                 in OTL you can have a near of an all of                                 that basically conjunction of you know                                 so you can have all these terms you want                                 to near a certain term right and the                                 certain terms you want near they all can                                 be in the middle of your all and then                                 things get really strange we that's what                                 the legacy system the very system                                 totally cheated on that                                 so they decided to forget about how                                 things appear in the document and just                                 they use the order things appear in the                                 query right which is totally wrong but                                 but yeah we do something slightly                                 different that kind of gets that thing                                 half right half the time and not the                                 other half but we're still like hoping                                 some some progress happens on that                                 nested span ticket how actually deep                                 your span core is like the the resulting                                 span queries yeah well how large are                                 they how how large are they resulting                                 will be generated upon queries are like                                 I'll be there                                 howdy I mean I'm not sure what you're                                 asking so yeah so we have span queries                                 containing like you know not just like                                 terms but like whole query trees so                                 these content indexers you know they                                 know their domain but they don't                                 necessarily know how to write it                                 optimize query so they write some really                                 crazy looking stuff right and so we get                                 some very hairy looking span queries in                                 our system one more question and then we                                 have break thanks for your talk I was                                 just curious like this is a really a                                 tough problem like how would you rate                                 this one compared to like other                                 companies for example Flipboard they're                                 actually kind of clustering like this                                 real-time streaming of documents and how                                 would you say like any other methods                                 versus this kind of look like like index                                 so it's yeah okay that is the main thing                                 is this this is a rules-based system                                 right if you're going to do that yeah I                                 think you know Lueck is like you know                                 I'm not sure exactly how very that's                                 proprietary but I'm guessing that you                                 know under the hood it's doing something                                 very similar to what Lueck does so I                                 think this is like the way to go it's                                 kind of like the obvious thing to do now                                 in Bloomberg this is not the only                                 classification system so there's a lot                                 of work going around machine learning                                 techniques for classification you know                                 that's still kind of like not quite                                 right for us not mature enough to                                 completely use in the production so we                                 really rely heavily on rules-based but                                 yeah we're looking at other                                 other approaches inside Bloomberg cool                                 thank you for the talk again                                 [Applause]
YouTube URL: https://www.youtube.com/watch?v=5GLlzlDYqg8


