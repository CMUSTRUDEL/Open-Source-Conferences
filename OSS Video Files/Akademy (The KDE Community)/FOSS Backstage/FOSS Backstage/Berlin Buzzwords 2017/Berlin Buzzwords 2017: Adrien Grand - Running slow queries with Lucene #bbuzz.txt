Title: Berlin Buzzwords 2017: Adrien Grand - Running slow queries with Lucene #bbuzz
Publication date: 2017-06-15
Playlist: Berlin Buzzwords 2017
Description: 
	Search engines like Lucene have been designed to run full-text queries as fast as possible. You can search for combinations of keywords using boolean operators, and Lucene will give you results in milliseconds. This is possible thanks to the inverted index structure, which gives you a sorted list of ids for every term. 

Then boolean queries just have to compute the intersection or union of these sorted lists, which is a cheap operation. However in the real world, users often want to run more complicated queries like phrase queries, range queries or queries on scripts, which can't easily get you a sorted list of ids. 

In this session, we will dive into how Lucene executes queries and in particular recent improvements around execution of slow queries. No prior knowledge about Lucene is required, however users who have been exposed to Lucene, Solr or Elasticsearch in the past are more likely to enjoy this session.

Read more:
https://2017.berlinbuzzwords.de/17/session/running-slow-queries-lucene

About Adrien Grand:
https://2017.berlinbuzzwords.de/users/adrien-grand

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              okay let's start so we are going to                               spend together some time talking and                               exploring harrassing runs through                               queries so that they are not too slow                               so before we start some background on                               myself I've been working in surf for                               some time now and in particular I've                               been loosing commuters in                                             been working at electric since                                         quarry processing is part of the things                                I enjoy working on so that's something                                that's what I'm going to talk about                                right now so Cory processing tends to be                                an interesting topic of this world                                earlier today we had talked by Alan wood                                world about how Lusine uses quarries and                                collect all other framework works so                                that you can find matches in your                                inverting disease and also in the past                                if you look at the history of talks that                                we had at burgers world where we already                                had some talks about query processing                                and in particular how we can make Horace                                faster                                for instance in                                                    about using SM the instruction so simple                                instruction which per data include                                training earth speed up the decoding of                                postings which can be useful in order to                                make some queries in single combination                                of SAMCRO is using Google Chrome is                                faster and for instance in                                              lotta talk by Stephen Pohl about an                                algorithm which is called Mexico which                                can be used in order to speed up these                                junctions so queries that look like a or                                B or C or potentially more closest but                                today we are going to do something else                                something else which is talking about                                slow queries and Allison deals with them                                and I think that's especially                                interesting because that's something                                that recently started tackling about two                                years ago in spite of the fact that                                person has been have has been out for                                something like fifteen to twenty years                                now so first we need to explain what the                                slow query is so if you get queries the                                the main responsibility as far as                                matching is concerned let's forget                                scoring entirely for now is that the                                segment then it's produce an iterator of                                other cadiz and that story turn it to be                                indicate the order that property is very                                important because the fact that this                                stream                                Cady's is another Israel help realtor                                and efficient conjunctions so a and B                                and C and potentially la Cruces or the                                exemptions a or B or C and from the                                cetera does we expect that they have two                                properties in order to be fast first                                they need to be able to iterate over my                                cheese efficiently meaning that given a                                matched in the index they need to be                                able to very quickly find the next match                                to the next dog ad in the cage order                                that also matches degree and also they                                need to be able to efficiently skip                                overlap ranges of the cages which are                                not useful and that keep feature is                                produced especially useful for                                conjunctions so as you might have                                guessed now if your career is low it                                means that at least one of those two                                properties is not or not and actually                                that's where wants to do because we have                                two gangs of strawberries first we have                                decrees that concentrate efficiently and                                those queries will be quite like race                                queries and script queries the current                                for culture history is that they have a                                lot of work to do on the per document                                basis in order to check whether or not                                it matches the decree so indicator                                scripts you need to run the scripts if                                you don't run the script you can't know                                whether or not the document matches                                recurring and indicator phrases you have                                a lot of positions to read in order to                                check whether or not the two words that                                you're searching for can be found at                                consecutive positions in index                                okay so that's a lot of work to the                                document and then we have a second class                                of quarries which are praise that comes                                keep so when I said it comes keep I may                                be saying actually the iterator that did                                provide that The Scream provide it can't                                 keep but those queries are based on data                                 structures that make it very hard to                                 return                                 iterator that are in order they are very                                 good at finding matches but they are                                 going to produce decade is out of order                                 and so something like loosen needs to do                                 in order to make them see the framework                                 if you take all those matching de cádiz                                 then solve them so that can produce an                                 inorder iterator but the fact that it                                 needs to take all the matching de cádiz                                 and                                 for them means that dark IDs that you do                                 not care about because you're going to                                 skip over them in any way you also have                                 to pay the console in so it means you                                 can't benefit from skipping on those                                 queries and we see that for those two                                 can score is we have way that we can                                 make them perform better in the general                                 case you've already you can speed the                                 quiz up in your cases for instance if                                 you get right queries there is a lot we                                 alternate you need to do on the prowl                                 document basis and that's it if you want                                 to find all documents that match the                                 phrase then you need to iterate over                                 let's say you're searching for quick and                                 Fox                                 you need to sign all documents that                                 contain both we consult and then for all                                 those documents unity check deletions so                                 there is not really thing you can do                                 however in some case some cases we're                                 going to see that especially when they                                 are used in conjunctions we can make                                 things better and in order to explain                                 why I need to first explain to you how                                 conjunction aware so like I said loosen                                 is about producing                                                      producing salty traitors the segment                                 overall the matching blockages so in                                 that case we have two terms quick and                                 socks and we want to find all documents                                 that contain both terms so both quick                                 and Fox when you seen wants to run a                                 conjunction the first thing we're going                                 to do is to look at index tactics in a                                 lot figure out which one of the closest                                 has the least number of matches in that                                 case it it's quick and that particular                                 Clause is going to be used in order to                                 read each iteration                                 okay that's going to be helpful in order                                 to do as little work as possible and                                 basically the next step is to put a                                 litter                                                               will presented by the blue arrow that                                 you can see there and then iteratively                                 we are going to first we're going to                                 start iterating using the clue that                                 matches the least number of documents                                 and then we are going to use both are                                 closest as follow-up in order to check                                 whether or not the match the same                                 document so first we advanced the soft                                 close to the first matching document                                 request to and then we asked other                                 closes in that case we only have one                                 document that match box                                 to move to the next document that                                 greater than or equal to two that also                                 contains flux and we moved to                                        greater than                                                            a match in two so then we need to go                                 back to the first person list and ask it                                 what the third document you have said                                 greater than or equal to three and that                                 time we figured that we have a match                                 because or postings are positioned on                                 the same location we have a match and                                 then we keep doing that until we find                                 all the matches in that are contained by                                 both person list and the best object                                 that we use for conjunctions is very                                 efficient because in spite of the fact                                 that we had I counted that I don't                                 remember exactly but we have something                                 like                                                                  only needed to move the clubs or seven                                 times okay for those costumes so that's                                 what makes conjunctions efficient and                                 you might be wondering how can this be                                 efficient because in order to find the                                 spell document for instance that                                 president or equal to                                                  over a lot of documents but posting                                 lists the embed something that is called                                 a skip list which can be used not to                                 efficiently skip over documents that are                                 not needy so the operations efficient in                                 that algorithm which we just described                                 is called default so you might already                                 have read buta that we've seen                                 conjunctions and so if you if you read                                 about it progress that the recent                                 algorithm which I just described which                                 is result so that's hype graph in the                                 best case okay we are an in conjunction                                 over two fast queries which are                                 tempering thumb coils which directly                                 encode the result set in general index                                 using postings but sometimes we have                                 three iterators so like we said before                                 things like read queries and script                                 queries for instance let's imagine that                                 you want to find all documents that                                 contain two and B at Kentucky's position                                 that were is going to be multiplied                                 expensive due to the SAG that twin B                                   and we can write activist found in many                                 documents                                 so there are many many times you have to                                 read position not to check whether the                                 documents that contain both two and B                                 actually contain them at consecutive                                 positions which means that if you want                                 to prevent an iterator for that                                 particular grade is going to be very                                 expensive                                 because a mini document is going to have                                 to reposition in order to check whether                                 or not the document you're looking at                                 match it or not we said we are                                 interested in conjunctions so in that                                 particular case we are going I'm going                                 to take an example where the user wants                                 to find all documents that contain both                                 Shakespeare and twin B as a phrase                                 quarry meaning they should be found at                                 consecutive positions so we have                                 something like that where again we are                                 going to use Samuel algorithm the                                 leapfrog algorithm in odds I imagine                                 first so we have Shakespeare which is                                 which can be found in fewer documents so                                 we are going to use it as a leader first                                 we advance it we under control and then                                 we ask to be to find the sales document                                 that greater than or equal to one so                                 first weapons to two but vary which                                 contains both                                                      though it contains both terms those two                                 terms can be found at consecutive                                 positions so we move to the next                                 document that contain both terms and                                 again it doesn't match until we reach                                 five where the SU terms can be found at                                 consecutive positions you might be                                 thinking what pity that we checked                                 positions on documents that did not end                                 up matching but actually even for the                                 Qin side                                 it's a pity that we had to check                                 positions because document size does not                                 matter Shakespeare and it will be much                                 cheaper if we are able to check that                                 that particular document document side                                 doesn't contain Shakespeare before we                                 start taking positions which is                                 something which is with wise conclusion                                 and that the reason why in some                                        about two years ago we introduced                                 something which is called two-faced                                 duration and two phase iteration allows                                 the iterator over to be at the phrase to                                 be fit into two components one of them                                 is an approximation which matches a                                 superset of the documents that would                                 match the three phrase and that                                 approximation indicated fresco is that                                 also document                                 moved to Mb and then we have                                 confirmation step which is going to                                 reposition in order to confirm whether                                 or not a particular document contains                                 both terms at consecutive positions just                                 to be clear that the way that first                                 queries have always worked but the                                 things that we introduced in racine                                     is the ability through a new you through                                 a new API to execute the approximation                                 and the confirmation sobriety and that                                 new API can be very useful when you're                                 in conjunction because that means that                                 you can reach agreement between all                                 approximations before you start                                 executing the costly bits such as                                 breeding positions running a script it's                                 wrong any addition to that we have a                                 magical API so that you can run the chip                                 of its belt so let's think about an even                                 more complicated query where you want to                                 find all documents that both match a                                 script and contain the phrase first we                                 are going to assign all human that in                                 veins to terms of the phrase and then we                                 are going to look at the match cast API                                 of both the script and the phrase and                                 depending on which one is white                                 offensive we are going to run the least                                 expensive one cell so that's it folks                                 for the first iteration so just to give                                 a couple examples so we just talked                                 about right grace which split the                                 approximation in the confirmation into                                 in conjunction and an additional step                                 that checks position but that also                                 generalizes well with crypt queries in                                 the case of the pre pre the                                 approximation would be simply equate                                 that matches all documents and as a                                 confirmation we are going to executor                                 scripts to check whether or not the                                 cream matches the script and then we                                 have common queries like boolean queries                                 constants of curries that propagates a                                 trade off so that let's imagine that you                                 have a very complex query at the deepest                                 level you have a price query and at the                                 top level you have a future which is a                                 very fast desire                                 if you don't return the fact that will                                 increase in cost and conquer is                                 propagated iterators means that you're                                 only going to check whether the phrase                                 matches on document that also match the                                 double feature okay so that's                                 optimization does not only apply to a                                 single                                 junction eater to propagate a let's face                                 it traitor add to the top of the tree                                 the criteria so that's it for the first                                 iteration and now we are going to talk                                 about the second-class Astro queries                                 which are Korea that can't keep and I'm                                 talking about that one I at the second                                 step because that one actually                                 increments on the improvements that we                                 we made it with to face duration so that                                 I need to explain why the queries are                                 flew so if you get at three that's                                 something that's very similar to because                                 III a single dimension we carry three                                 which is a data structure that we are                                 using with point in electron range                                 queries on the right field so basically                                 you have a tree where underneath you                                 have pair that consists of the dark ID                                 and an adage identify of the document as                                 well as the value that associated with                                 the document and then the tree is going                                 to be organized by value and on every                                 inner node of the tree we are going to                                 record the range of values that can be                                 found on leaves and the other part our                                 node so that one stems at the root level                                 you can know that all values contained                                 is at three are all within                                              the left and the                                                                                                                                 can see how this data structure is very                                 efficient at finding all documents at                                 natural range because as you work down                                 the tree you can quickly eliminate path                                 of the trees that are never going to                                 match because there is no intersection                                 with the range of your correct okay so                                 that the data structure that were using                                 you know to exceed range queries we                                 receive but because it's organized by                                 value in the back of the Heidi let's                                 imagine that you still have the data                                 structure and you want to check whether                                 or not document size for instance                                 messages are occurring there is no way                                 that you can know which branch of the                                 tree you should follow in a design                                 document size which means that Jesus a                                 structure is very good at signing all                                 documents the                                 metrand which is very bad at verifying                                 whether a very small set of documents                                 matter range which means that if you are                                 on a query that looks like that so you                                 have a very restricted query such as                                 occur in an ID field okay on the one                                 hand and on the other hand you have a                                 numeric range query on a very larger                                 range which is likely to match many                                 documents in spite of the fact that the                                 left side of the equation matches only                                 one document you're going to pay the                                 price for collecting all the matches                                 okay using the trick collect ultimate                                 cheese for the wrench GUI before you can                                 start running intersection between the                                 stories and that's what makes it slow                                 but there's another way that we can run                                 range queries with the scene and in                                 particular we have another data                                 structure that we can encode in the                                 index which is called dog values and if                                 you are not familiar with the values                                 that the sense name that just means that                                 we have a way to store a columnar                                 representation of your data in the index                                 and that columnar presentation can be                                 used to run your engine queries using to                                 face iteration which we just talked                                 about so other approximation we are                                 going to use a query that matches                                 everything a natural just like we did                                 saw script and as a confirmation we are                                 going to use our values in order to read                                 the value so I'll touch the document and                                 check whether or not it's in the range                                 just to be clear using this kind of                                 query based on the values to run ranges                                 all the time is axis in a majority of                                 cases bad ID because if you want to find                                 all documents that match arranged you                                 have no choice but to do a linear scan                                 so we need every possible document in                                 your index and check whether or not it's                                 in the range okay which is something we                                 need to avoid the previous data                                 structure is a tree we just saw is much                                 better at signing all the matches                                 however things are will be different                                 when you use a range in a conjunction in                                 particular if use points so let's say                                 that you have a very simple query that                                 looks like a thumb query which is                                 intersected to temporary and arranged                                 goron what's going to be the cost of                                 running that whole crane depending on                                 how the range                                 is implemented if you are using points                                 basically the cost of that grade is                                 going to be driven by the cost of                                 signing all the matches or the range so                                 basically the cost is going to get in                                 there with the total number of documents                                 that match drain the range on the other                                 hand different if you use that values in                                 order to implement the range what person                                 is going to do with the iteration is                                 that it's going to iterate over all the                                 matches of the dum curry and check                                 whether or not they match the range with                                 the values so that time the cost is a                                 number of check document which is a                                 number of documents of zelda crews of                                 the conjunction and as you can imagine                                 there are cases when using points is                                 going to be cheaper and there are cases                                 when using duct values givenchy to be                                 cheaper and so how can we decide which                                 one to use an innocent side six point                                 five story which has been released and                                 earlier this year we added a new query                                 which is used index order value screen                                 oops                                 there's a typo forego that which is                                 going to look at indexed artistics                                 in a lot of free art which strategy to                                 introduce in order to executive range                                 glory and essentially is going to use at                                 the point gory which to which we added a                                 new api that it can estimate how many                                 documents are going to match just by                                 looking at the tree which can be done                                 cheaply and then it looks at all the                                 components of the conjunction and                                 basically if the range is going to be                                 the clothes that has the least number of                                 documents which means it's going to                                 drive iteration then we are going to                                 this point which are much better better                                 data structure in oxide or the matches                                 for modular quarry but on the other hand                                 if the range GUI is not going to drive                                 the direction and only going to be used                                 to check whether the documents from                                 other classes match the entire boolean                                 cream then we should use the values so                                 that works well theory which is still                                 important to run a benchmark just to                                 make sure it works as expected and so                                 when we open that you we we run                                 benchmark over a deal                                             subsets                                 of Wikipedia that hide in Pascal body                                 field where you have the content of the                                 article and they'd feel that the last                                    caption date of the article and then                                 I've been running crazy to correct that                                 so a term query on the body intersected                                 with arranged away on the last by                                 decision date and I've been putting the                                 latency of degree so the entire gwee                                 enquiry hunkering end range query based                                 on the total number of documents that                                 the range matches so we're looking at                                 next slide                                 beware that it's using a logarithmic                                 scale both on the x and y axis and                                 here's what gave so that actually the                                 first test which I ran and as you can                                 see so here                                 so let's start with a simple one to W                                 line which is the Green Line so it's a                                 bit hard to read but the real line is                                 this one ok so we can see that the cost                                 is always the same no matter how many                                 documents match the range for you and                                 that actually expected because like we                                 said before we look back with our values                                 the way that the rent go were with                                 horizontal up we work with that we are                                 going to iterate over all the matches                                 for the jumper E and check the values                                 range so the number of document that                                 match the range doesn't actually matter                                 which is confirmed by this metric much                                 mark now if you get points which are the                                 proper line so which is here then we                                 said that the cost of the green curry is                                 mostly dependent on the total number of                                 document that matched the range and                                 again                                 expect here which I'm going to talk a                                 bit about later we have indeed a                                 relationship between the number of                                 documents that match the range which is                                 here expressed as a percentage of the                                 total number of documents in the index                                 and enter latency which is on the y-axis                                 and we are perfect in that case that the                                 new index of the values query that we                                 worked on which is a blue line so it                                 again how to read but that would be that                                 line actually makes the perfect decision                                 in loquacious and before that threshold                                 which is the number of documents that                                 match the term query is going to use                                 points which are more efficient when it                                 comes driving iteration of the                                 conjunction                                 but afterwards is going to use the                                 values which that we saw are more                                 efficient when it comes to only checking                                 whether document that matter term for                                 ultimate range and again it it brings                                 some interesting speed ups so it's hard                                 to to realize due to the regarding scale                                 but here what we observe is actually a                                 thirty times                                 speeder which is significant and okay so                                 that thing we see here is actually due                                 to an optimization we have with points                                 because when we match both the modern                                 houses index we have an optimization so                                 that we are going to compute all                                 document I do not match another enjoy                                 any of that and the optimization that                                 index or W Square performs is not aware                                 of that optimization but unfortunately                                 things are not always at that good and                                 this is a same benchmark stem data set                                 the only thing which I changed here is                                 that I replace the term from the                                 temporary with another term which is                                 previously it matched Jo point one                                 percent of the index and this time is                                 matching one percent of documents in the                                 index and this time you can see that                                 where are ways making the right decision                                 okay so because we are switching at one                                 percent between points in purple and the                                 values here in spite of the fact that                                 point after more efficient then the                                 values up to something like eight                                 percent of index and similarly due to                                 the optimization we have for in jizan                                 points points again become faster than                                 the values when you have more than                                 something like eighty percent matches so                                 other confusional this optimization                                 tends to be good on average but                                 unfortunately it can make the wrong                                 decision sometimes and that's something                                 that I would personally like to see the                                 it's hard okay we're working near sticks                                 and we are trying to compare costs of                                 different quarries even though they do                                 very different operations which makes it                                 very challenging                                 I've been describing these optimizations                                 orangeberries electorate we are exactly                                 the same way for do queries in Tusculum                                 bounding box queries and your distant                                 queries which work exactly the same way                                 as a range queries on one dimension                                 they are just generalization from                                 whisper dimension and there are still                                 things to do so like I said the                                 heuristic still needs improving because                                 sometimes it make the wrong decision but                                 she is a welcome if you have ideas how                                 to speak that and there are some queries                                 that we'd also like to fix so for                                 instance prefix way card and regression                                 based queries have the same issue that                                 they can't keep efficiently they need to                                 collect all the matches before you can                                 start doing anywhere on the rkd set                                 because wind sorting and we also don't                                 have rent codes or range filled the                                 ranks I'm not sure if you're familiar                                 with the wrench fields that that's                                 something that we introduced in loosing                                                                                                         range and then to sell for over a pure                                 overlapping ranges so let's say that                                 windex range which consists of                                           you serve for                                                           to intersect so there is a match if you                                 sell for                                                                 so you're not going to have a match and                                 the main reason why we haven't done that                                 suffice that we do not have the greatest                                 report for range fear so far which is                                 again something that we could think                                 about adding that's everything I wanted                                 to say so if you have questions we'll go                                 ahead                                 [Applause]                                 so again the last point thanks for the                                 talking right and again the range                                 queries and the range field so how do I                                 thought that's the whole concept behind                                 having range fields so if you don't if                                 you cannot apply range queries on it so                                 how do you use it so you can play range                                 choice on them but we don't have the                                 optimization to your dog values                                 automatically when it's supposedly more                                 efficient but at the same time range                                 fields tend to be more selective query                                 range queries and range fields tend to                                 be most active queries so the                                 optimization is not as much needed                                 otherwise I would say but still it would                                 be good to have very seen optimization                                 thank you so just to finish on that one                                 we do support growing range field with                                 rent craze actually we support multiple                                 kind of variations we've got intersects                                 within contains and is joint so there is                                 really evaluative query that you can run                                 around clear then it works we just don't                                 have the optimization yes ah does it                                 ever make sense to run two strategies in                                 parallel and whichever one finishes                                 first you're done or is you know if the                                 choice is too hard between I guess so                                 that's a good question the other                                 trade-off they depend whether what you                                 want to improve this latency or                                 throughput because that kind strategy                                 would bring probably a better latency                                 because you would get whichever returns                                 first but at the same time at the same                                 time you would have twice the cost so                                 you could have well throughput that's                                 something that you can certainly do on I                                 mean it doesn't have to be implemented                                 in you think that's something that can                                 do by using lean API you can run so you                                 can build two queries one we don't value                                 not run the range and the other one with                                 points in order to run the range and                                 then bounce to thread which are going to                                 run the coin parallel and get whichever                                 result comes first that's something you                                 could do with the API as it is today                                 I think we good thank you my children                                 there we go                                 [Applause]
YouTube URL: https://www.youtube.com/watch?v=p51vIDWHWqk


