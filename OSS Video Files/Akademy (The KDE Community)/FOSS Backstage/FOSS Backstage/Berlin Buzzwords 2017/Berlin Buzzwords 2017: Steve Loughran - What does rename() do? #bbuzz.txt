Title: Berlin Buzzwords 2017: Steve Loughran - What does rename() do? #bbuzz
Publication date: 2017-06-15
Playlist: Berlin Buzzwords 2017
Description: 
	We software developers take for granted the notion of  "a filesystem", with its paths, directories, files and operations. Yet when it comes to distributed filesystems, those notions built from years of using desktop systems actually constraining us to a metaphor which is no longer sustainable

This talk looks at our foundational preconceptions from the perspective of trying to define a single operation in Hadoop HDFS, rename(), what it takes to implement it in a distributed filesystem —and what has to be done to mimic that behaviour when working with an object store.

Preconceptions about rename()'s semantics are deeply embedded in large scale applications such as Apache MapReduce, Apache Hive, Apache Spark and the like, being the operation used to atomically commit work —and so do not work the way we think they do on Object Stores like Amazon S3.

We have to rethink our strategies for committing distributed work, with Hadoop's new "S3Guard committer" being the example of the world we have to move to. The time of renaming files is over.

Read more:
https://2017.berlinbuzzwords.de/17/session/what-does-rename-do

About Steve Loughran:
https://2017.berlinbuzzwords.de/users/steve-loughran

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              five ones                               also turning up even if it's because                               you've only accidentally turned up                               instead                               your thoughts next and I may talk about                               rename which sounds an odd topic but                               actually I'm talking about the whole                               problem of how do we persist in recover                               state in why do we persistent recover                                state an answer is we do that so we can                                get what our data back later this is how                                we used to save state we used to have a                                little dog boxes we used to there was an                                API designed for a single process nice                                and simple there Nick's nodding in                                familiarity because they still have                                these at home this is a                                                by the way having moved all the hard                                disks finally we had gigabytes worth of                                data we had multiple users but multiple                                processes this is where POSIX came from                                POSIX is the one through API for storage                                we have files we have directories we                                have renamed and we can open files but a                                guarantee that they're not there before                                this is what most programs are built on                                the assumption are in a hard disk hands                                up who's got a hard disk that's really                                good hands up who's got a multiple                                exabyte data storage facility somewhere                                in a desert near a hydroelectric plant                                everybody using cloud storage has one of                                these hands up who's using s                                         storage photos on facebook yeah that's                                exactly it so thing is we've moved on                                from floppy disks to buildings you have                                to drive around with a golf cart and yet                                we're still we still rely on things like                                rename to actually get our work done                                because we've evolved models and API in                                our brains and in our code to deal with                                data the most fun enough one of the core                                models is actually relational algebra or                                is it's known sequel it you use that                                it's somebody else's problem to deal                                with how things get stored you have to                                go back transactions in their isolation                                but generally it's somebody else's                                problem take a step down give into the                                world where I live in which is actually                                the layer underneath things like sequel                                and other stuff where we're we're still                                trying to maintain that metaphor now                                we've got a POSIX API similar ish our                                file systems we still have files and                                directories but now now we're working in                                those giant data centers the file                                systems are distributed                                would wear behind that single API the                                interface all the main form Hadoop stack                                applications work underneath we wrote it                                for HDFS but now we glue in all the                                object stores handler in the audience                                who is using any of those applications                                at the top keep your hands up if you're                                working with any cloud object stores                                from them okay two or three people up                                there now a core part of this in your                                code if you're using as a destination                                for this work is we absolutely rely on                                renamed as the way of committing our                                work you're doing any query in edit                                MapReduce spark whatever we copy the                                data but mr. slide actually we rename                                stuff we you write your data into a temp                                directory in your final destination you                                can have hundreds of thousands of tasks                                running you can have tasks speculating                                working on the same data simultaneously                                the first one that finishes sends a                                message to the job manager and says I'm                                done job manager if it's happy with it                                says ok you're ready to commit and we                                can use we can commit that and single                                transaction this works because rely on                                the fact that data we store in Hadoop                                HDFS all the data the terabytes where                                the data lives scattered across the                                cluster and data notes but we have a                                single metadata store the name node                                which actually stores our directories                                and trees the metadata so the commit                                operation only takes place in that                                single server we can lock a bit of the                                filesystem                                we can do a transaction we can save it                                it's essentially a database with                                database operations so nice and simple                                we do a rename in contrast Amazon s                                  we've put a lot of effort into making                                 the object store look just like a file                                 system to keep all those people writing                                 the code above happy it looks like a                                 file system except it's just one of                                 those metaphors that if you push hard it                                 will absolutely collapse on you here's a                                 problem there is no matter data there is                                 no name no there is just a model in your                                 head of what you think is one                                 on the API alone in reality your data                                 gets stored over different shards and                                 the location of your data the specific                                 shard you use is determined by a hash on                                 the file name alone that means you can't                                 rename a file there is no file to rename                                 their only blob and if you want to give                                 it a different name you actually have to                                 copy it from one machine to another                                 which takes about six megabytes a second                                 so to do a rename we pretend we're doing                                 a rename we do a list we do an explicit                                 copy of every single block of data and                                 then finally we delete the old craft                                 people using s                                                       work in the MapReduce and especially                                 SPARC these days they talk to us and                                 they say hey your transactions are                                 taking really low why is at the end of                                 my process I've done all the work and my                                 machine just sits there not doing                                 anything for about                                                      really well in development and the                                 answer is is because that coffee                                 operations is Primus portion to your                                 data size but the thing you have to                                 worry about is not the time it takes to                                 commit if they don't lift up above                                 because the list to work we have to have                                 a consistent way of listing all the data                                 in the object store and Amazon s                                       not list data reliably so you may                                 actually miss the output of a single                                 task one or two files we do the copy we                                 delete everything and your code can keep                                 going happily not realizing that                                 actually you've generated corrupt data                                 and it can take a while to propagate the                                 worst thing is everything appears to                                 work really well during development only                                 when you go into production it goes                                 wrong mostly when there's only                                 intermittent misbehaviors on s                                          really becomes visible so that's a                                 problem another problem in fixing and                                 the way to do it is we throw out all our                                 assumptions about data about storage                                 about files and say this is an object                                 store let's embrace that fact                                 let's stop hiding from it and pretending                                 that it's actually a filesystem let's go                                 with what Amazon office                                 and the secret is here actually is they                                 use something called multi-part uploads                                 where we can write data to a single                                 address we can start extend an operation                                 which is effectively transacted to say I                                 want to write my data to the object                                 store you get a URL back and then can do                                 repeated post to that file but it                                 doesn't come into existence until issue                                 the final post saying create the object                                 here is the order D tag list of main                                 division uploads we've been using this                                 for a long time in the output of when                                 you're writing stuff to s                                                F                                                                      been doing on the same process so you                                 write your stream you close it using a                                 POSIX API nice and always well but it                                 turns out we can be more devious about                                 that we can upload the individual blocks                                 for mumping individual process as we go                                 along but we can commit the data on a                                 different machine or even abort it so we                                 change the transaction operation not                                 from a rename to commit but more the job                                 manager decides whether to actually post                                 the completion operation of those rights                                 so put the stuff up there once we're                                 actually happy with it the task can                                 communicate and we can decide there and                                 then whether to upgrade this stuff or                                 not so we got the two pending uploads we                                 do a list that requires consistency                                 separate problem I'll ignore that one                                 and we just do a post so that we're                                 using the transactions built into the                                 object store so we can get away with all                                 this stuff working friends actively I'm                                 actually going to give a demo of this                                 I'm get it working actually this is my                                 smart unit test not particularly                                 exciting but we can actually simulate                                 the two different commits operations and                                 we have a special mode now in the s                                  client will return inconsistency on we                                 can add a lag on deletion this wouldn't                                 take a while because we're over slow                                 Network I'll go back to talking so if                                 you actually turn the inconsistency on                                 then everything dramatically breaks                                 every time and we can show that the fall                                 out commits us the normal one is both                                 slow and in the presence of                                 inconsistency leads to corrupt data so                                 we have to get rid of it and that's what                                 I've done basically this code down                                 exists you can                                 download it we're going to feed it back                                 into the dupes read before long for                                 coming out later in the year and then                                 you'll be able to use it from a dig Map                                 Reduce and spark this is code finish yep                                 no still going on this one so step one                                 we change how we commit data we're                                 affecting the first big corruption from                                 but there's a lot more in that POSIX                                 world that we have to look at and say is                                 it dead is it obsolete how would we do                                 it differently given me assuming an                                 object's daughter and the key one is                                 this whole notion of I'm going to open a                                 stream of data and read it by seeking                                 sporadically it's probably the one we                                 got to look at the same dead for things                                 like HTTP request especially looking                                 HTTP - coming along we actually want to                                 issue over leaves no bulk requests of                                 byte arrays saying here are three here                                 are three different offsets within this                                 very large multi-gigabyte object let me                                 do get from them overnight because right                                 now you read an input string your                                 process goes okay give me the data from                                 offset                                                               that read then immediately the client                                 says oh I want this extra data here and                                 when so the filesystem we have no idea                                 what's actually going to be needed next                                 your program does especially for                                 actually using a column data format like                                 or campaka in those exactly in advance                                 what it's going to be skipping so can we                                 actually move the api's on to doing this                                 world or turns even look at it what ap                                 is can we implement in the file systems                                 and the object stores that application                                 users are actually going to take up on                                 because I could write they put the                                 effort in write this stuff and if it                                 doesn't get used it's just kind of                                 wasted so anybody work in this layer get                                 in touch and we'll think about what to                                 do now did that finish yeah I've got two                                 tests doing a commit first the new one                                 nice and happy takes a while give the                                 Diagnostics times everything dealing on                                 big long random pause and it's sales                                 because there was no data there and that                                 this is this is the default state right                                 now you'll get you're working in this                                 really fear failure                                 because your code won't have assertions                                 about the number of files I get                                 generated you'll just say run the query                                 query returns orders well except there                                 is no data there later on or you're                                 missing you're missing a couple of parts                                 of your data and that's why right now I                                 would not recommend anyone using s                                       destination of work to from a duplex                                 does anyone do that in the room from if                                 you keep your hands down now so it's                                 really dangerous it looks nice but it's                                 really dangerous so stop it we will have                                 a fix right now coming out late in a                                 year if you really in a rush to do it                                 now a big chunk of the code I'm using on                                 is actually based on something from                                 Netflix okay so you go to Netflix code                                 repository and you can pick up what                                 we've been using so we need to rethink                                 when you look at POSIX and say is POSIX                                 dead for the new fastest answer is maybe                                 that the API can survive but the other                                 thing is there is one more storage model                                 coming and it's actually high speed                                 non-volatile memory attached to the                                 mainboard of your service of your                                 laptops if there's less people like                                 Intel working on you're getting FS B's                                 that will go on cards right now you can                                 slot in their generation of stuff                                 they're promising is going to be even                                 faster it's not going to be as fast as                                 DRAM maybe                                                              will fit straight into main memory                                 you'll be able to address it for your                                 program and when your program goes down                                 and comes back up or your state will be                                 there in a memory so you can even bypass                                 the loading and saving face you can just                                 map it straight in now again there is a                                 POSIX API here I'm using C at this point                                 as it's the language you can get at it                                 where you can basically here's a file                                 I'm going to map it straight into memory                                 and then you can do pointer operations                                 on it so I can define a C struct map it                                 into memory work with it and then save                                 it back that is what we have right now                                 and it's there does anyone in the room                                 you are usually doing this okay it does                                 work today with POSIX but it works                                 because we're in charge of when things                                 get synchronized we really are writing                                 it                                 to a file system so I can do stuff like                                 I can take a field in a record I can                                 just add                                                               increment operation I could then copy                                 that field                                                             multi-threaded process I might need to                                 put a lock on it so I can get away with                                 it because I know we don't actually                                 synchronize to the end if we're looking                                 at non-volatile memory that's gone now                                 each write operation to the file system                                 to your memory is potentially write to                                 the store so I increment that first                                 operator final an current field                                      your process clashes at that point your                                 your struct may not be consistent I do a                                 field copy still one field - yeah I've                                 done it now my struct is now consistent                                 my data is now correct but what happens                                 if the CPU has only got that data in                                 level one cache it's not been written                                 back to the non-volatile memory yet so                                 your code Gary's on thinking hey I've                                 written my data structure aren't I happy                                 but in fact it's not persisted your                                 system crashes you come back up again                                 you're in an unknown state so                                 effectively we've moved the transaction                                 problem away from the file system the                                 opposite direction from the object store                                 and now it's how we access data in                                 memory I'm not working lining this there                                 are people doing it an academic space                                 already there's lots of papers you can                                 read on it if you want the key point is                                 nobody quite knows yet what is the best                                 API a model to work with here it                                 actually looks very similar the research                                 stuff is about using things like log                                 structured format since stuff log                                 structure itself where I write new                                 records I don't ever do overwrite fields                                 in place I create new new obstruct new                                 records then update references to it own                                 by the way I can't use absolute pointers                                 anymore because you've got to use offset                                 references or something like that                                 because of course memory addresses                                 change so it's not going to be free in                                 this well we have great opportunities                                 for speed and performance but it's going                                 to require us to rethink what our                                 applications do we're going to rethink                                 how we do transactions and maybe we're                                 going to have to go back to see                                 okay because it's the language of                                 pointers and other languages on top I'm                                 pointing at Java in particular are going                                 to have to rethink how we model memory                                 if you want to take advantage of this                                 world so key point nouns storage moving                                 in two ways we started off on a sloppy                                 in the hard-disk                                 one direction we're in two things the                                 size of Facebook and POSIX I think we're                                 hacking it the API is to make them look                                 like POSIX out there but the metaphor is                                 really starting to fall apart and we're                                 going to need to do something there                                 but the other direction against the hard                                 disk has become an SSD the SSD is moving                                 on to the mainboard the chip                                 manufacturers are saying we're going to                                 go from faster again POSIX                                 POSIX doesn't fit into this world you                                 can make it look like it does we've had                                 RAM disk with a drama fest for a long                                 time but to really really get to the                                 benefits of this new world we're going                                 to have to go back to pointer references                                 and somehow get into the languages and                                 the API is a model of transact goodness                                 into that dress and finally sequel will                                 survive or variants thereof okay if you                                 live in that space it becomes somebody                                 else's problem you'd better make sure                                 they get it right you better understand                                 how your database does any form of                                 transaction but otherwise you're pushing                                 it down to people and it's not you so                                 we're very short time for questions so                                 are there any questions okay scoot                                 around real fast                                 [Applause]                                 hi hi so I'm doing everything I                                 shouldn't do with all that reading files                                 from history writing to them honestly I                                 didn't really run into any problems of                                 genomic because that is small but what                                 should I do if I do end problems you're                                 running on a strike yes                                 stop it much where to next question                                 you're not even going to know you've got                                 a problem maybe you could leave chat but                                 otherwise you won't it's only notice                                 you've got corruptions okay what intro                                 so my stuff you can put it on to                                 something like HDFS and then copy it                                 over at the end of your work okay you                                 get better performance there                                 if using Microsoft Azure storage it's                                 actually fast and consistent so it all                                 works nicely okay but s                                                the enemy is normally list consistency                                 so even if you get everything a unique                                 name you don't going to worry about                                 updating consistency but the listings it                                 can be slow to discover data it can be                                 slow to find deleted data                                 everyone's code assumes everything is                                 consistent okay I don't think that's                                 those api's inside little mental model                                 on the head we're all stuck in floppy                                 disk land                                 thanks all right two other questions                                 I Steve go talk so that you didn't                                 mention e/m or FF which Amazon have                                 produced and Netflix as you know also                                 have center and Spotify had done                                 something similar okay we all right                                 what's your opinion on them okay we are                                 implementing something in Hadoop which                                 actually all these things that use                                 DynamoDB is a consistent metadata store                                 we actually have something in Hadoop                                 coming up that way - in fact I rerun my                                 test with a - D                                 I think it's local Dynamo I can probably                                 actually get it working where we use                                 Dynamo for this stuff as well what it                                 actually delivers is two things you get                                 the consistency to rename work but                                 you're still using your completing of                                 transactions and operation it takes lots                                 of time okay so that's the promise all                                 taking time to commit which is why I've                                 been collaborated with Netflix on this                                 faster committer because they have that                                 commit lei problem                                 and because DynamoDB runs up large bills                                 so amazon they're committed to it the                                 thing we're doing seagard yes it's the                                 place from Game of Thrones is going to                                 deliver it - what                                 they also deliver that IMDB it's a lot                                 lot faster - list we do a get in HDFS so                                 actually see the file exists to list                                 something                                 it takes about                                                          pauses in the logs while we list                                 directories no one and that really kills                                 performance at the beginning of any                                 query hi very smart query where it                                 enumerates the directory or worse it                                 does a directory tree walk to say what                                 data do I have and that's a real                                 performance killer dynamodb you can                                 actually get it really fast the problem                                 is you do have to pay upfront for your I                                 ops you get billed more for it so it's a                                 trade of cost versus time if you want to                                 learn more about storage Jim you can                                 come and ask me afterwards you know                                 maybe start a project on it                                 okay we're out of time if you have more                                 questions when we later I'll see you                                 later let's thank him again                                 you                                 [Music]
YouTube URL: https://www.youtube.com/watch?v=UOE2m_XUr3U


