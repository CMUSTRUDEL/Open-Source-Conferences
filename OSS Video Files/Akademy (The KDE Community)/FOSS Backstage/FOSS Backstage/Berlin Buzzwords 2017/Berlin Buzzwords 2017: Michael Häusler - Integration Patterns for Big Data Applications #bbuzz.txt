Title: Berlin Buzzwords 2017: Michael HÃ¤usler - Integration Patterns for Big Data Applications #bbuzz
Publication date: 2017-06-15
Playlist: Berlin Buzzwords 2017
Description: 
	Big Data technologies like distributed databases, queues, batch processors, and stream processors are fun and exciting to play with. Making them play nicely together can be challenging. Keeping it fun for engineers to continuously improve and operate them is hard. At ResearchGate, we run thousands of YARN applications every day to gain insights and to power user facing features. Of course, there are numerous integration challenges on the way:

- integrating batch and stream processors with operational systems
- ingesting data and playing back results while controlling performance crosstalk
- rolling out new versions of synchronous, stream, and batch applications and their respective data schemas
- controlling the amount of glue and adapter code between different technologies
- modeling cross-flow dependencies while handling failures gracefully and limiting their repercussions

In this talk we will discuss how ResearchGate has tackled those problems. We describe our ongoing journey in identifying patterns and principles to make our big data stack integrate well. Technologies to be covered will include MongoDB, Kafka, Hadoop (YARN), Hive (TEZ), Flink Batch, and Flink Streaming.

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              thank you so much                               for the big audience here I'm super                               excited and to tell you a little bit                               about the big data integration patterns                               that we came up at ResearchGate                               ResearchGate                               is a professional network for scientists                               and we we research gate is built for                               scientists and give them the tools to                                connect to collaborate and to discover                                relevant research we have our mission is                                to connect the world of science and                                really make research open to all make                                all the research artifacts accessible                                and so far we have                                                      who are registered users for such gate                                and our database contains more than                                    million publications and so far we have                                discovered more than                                            citations that connect those                                publications so a big part of what we                                are doing is finding those connections                                with connections between publications                                between users between research topics                                and that that's what we are doing and                                that means big data processes in data                                processing is a large part of it                                analyzing that scientific content and                                then in the end making good                                recommendations to our users for that we                                started pretty early with doing Hadoop                                and other big data processing and we run                                two hadoop clusters in production one                                which we call the last cluster the                                near-real-time cluster which is                                optimized for latency and mainly ran a                                running HBase and flink streaming and we                                have the batch cluster so-called                                analytics cluster which mainly runs                                MapReduce hive and flink batch and both                                of these clusters are in the petabyte                                range so when we are talking about those                                applications so what we are doing we are                                running over                                                           a normal day roughly                                                scheduled um applications that run every                                day and roughly                                               interactive ad-hoc analysis                                and we do ingest more than                                              data sources every day and we're                                   engineers and that is everyone from                                front end to system operations and this                                means a lot of jobs quite a few                                engineers but this is still an amazing                                amount of work so what you think a lot                                about is develop a productivity so we                                want it to be fun to develop new jobs                                you should get your results quickly but                                it should also be fun to upgrade the job                                later when you want to build a new                                feature for it so ease of maintenance is                                very important and use of operations                                because no one wants to get woken up and                                wants to get cold at night if one of                                those                                                                  we come up with some integration                                patterns and principles how we build our                                workflows and they are all about                                productivity about easy collaboration                                between teams basically coming up with                                building blocks that make it really easy                                and fun to build bigger systems out of                                it so speaking of reuse and developers                                should be able to reuse results from                                everywhere from other teams be it                                software or be it data yeah these                                patterns though of course they need to                                be strategic but they should really be                                driven by real-world use cases and they                                should really be driven by real-world                                pain points this is what you want to                                solve with them because that is what                                then developer productivity is about and                                they should not be dictated by a single                                technology so we try to focus on by with                                these principles on more the abstract                                level and really think about what's the                                idea behind it and not go too much into                                like how do we do with this with this                                specific framework and why is it so big                                data is still a fast-moving space like                                the data batch processing today is quite                                different to compared to five years ago                                and five years ago                                                      it was my first time to attend buzzwords                                as a guest and I still can wizardly                                recall the discussions that                                we're having service hey should we use                                high it or should we use pig for example                                 or should we use HBase coprocessors or                                 it was hey what is the best client                                 library to talk to zookeeper with and                                 that's not questions that keep us awake                                 at night today so this whole space has                                 evolved quickly so many new frameworks                                 have happened and we are in a quite                                 different world today than it was five                                 years ago                                 and while the batch processing world is                                 slowly maturing and the speed of                                 innovation goes a little bit down with                                 the stream processing world it's we're                                 still in the middle of it this is what's                                 happening right now this is really                                 exciting and what's happening today with                                 streams will probably influence for                                 example SQL on streams will influence                                 what we will run on top of stream                                 processors instead of batch processors                                 for the upcoming years so this is                                 evolving and that means the big data                                 architecture also must evolve over time                                 so as I said we wanted it to be use case                                 driven and that's why I want to very                                 quickly tell you about our first big                                 data use case that we had early                                         implemented author analysis on her group                                 back in the day that was Java MapReduce                                 because few options available and since                                 then early                                                              production the problem is easily                                 described you have two publications and                                 imagine that both publications have a                                 common author in this case for example                                 yet modish and you want to know is this                                 the same person or are these two                                 different people who just could                                 instantly share the same name so this is                                 kind of a clustering problem a                                 classification problem also named                                 disambiguation problem all rolled up                                 into one and it is a super high product                                 impact is a user facing feature because                                 if you find this out correctly then we                                 can build beautiful publication detail                                 pages where we connect the scientific                                 content directly to the users who                                 created it so one thing that's maybe a                                 bit different from other first use cases                                 with Hadoop is that we are talking about                                 in                                 reaching user-generated content so the                                 users are creating those publications                                 and we want to enrich them with some                                 information so they're both working on                                 the same data set and that's a little                                 bit different from an analytical use                                 case if you would have analytical use                                 case you would start with users                                 producing some data in your life                                 database you would ingest that to your                                 Hadoop cluster then you would run some                                 analytics on it and that would in the                                 result go for example to your bi tool or                                 if you're for example building a                                 recommendation model then again the                                 users would click on your site you would                                 record interesting signals on the live                                 database and you would ingest these data                                 ingest the signals and features build a                                 recommendations model in Hadoop for                                 example ship that model off again to                                 production but it's not going to the                                 same live database it's sitting next to                                 it right that was a little bit of a                                 special thing that we have to do there                                 very quickly about the data model so                                 this is just a small part of the data                                 model at ResearchGate and what they're                                 talking about basically is the                                 connection here between publications                                 authors and accounts and these live in                                 and are owned by different micro                                 services which means our first                                 implementation looked a little bit like                                 this you have data owned by different                                 like services and we are ingesting that                                 and we are doing offer analysis I'm                                 cheating a little bit here this is an                                 oversimplified representation what it                                 really looked like was more like this                                 but for the sake of this talk let's keep                                 it at the simple level so what's going                                 on here well you have the data sources                                 and you ingest them which gives you some                                 input data in your analytical system in                                 our case HDFS and you running some data                                 processing on top of it which gives you                                 some intermediate results and then you                                 might for example run at this one to see                                 which of these results are really                                 relevant for production and then your                                 final results and you're exporting those                                 results and it was fine for the very                                 first use case but you can already see                                 that some things are not optimal and                                 that we quickly try to tackle and                                 six and that was you should always be                                 cup of data ingestion so what I mean by                                 that you have here this flow and it kind                                 of makes sense it was the first up flow                                 that we filled there was no data in the                                 cluster so the team who built the flow                                 also built the data ingestion and just                                 made it one right but that's not optimal                                 let's say for example that one service                                 is using false positive as a database                                 and the other is using MongoDB as a                                 database then that should be really an                                 implementation secret of that service                                 your flow should not be concerned about                                 that and you should not have to                                 implement connectors to all the                                 different databases all over again and                                 worst case be restricted in the choice                                 of your processing framework because one                                 processing framework might have a                                 connector for your favorite database and                                 the other might not so for another thing                                 these imports are usually quite                                 expensive so you want to reuse them as                                 much as possible and one thing that                                 surprised us a little bit and was it is                                 really hard to the back if you don't                                 separate that and this is what I want to                                 explain a little bit more so imagine you                                 have written a job and you did all the                                 things in the right way so this means                                 you have perfect unit tests and                                 integration tests but still suddenly                                 your job breaks on production and then                                 how are you tackling this how are you                                 finding the root cause and there are so                                 many options is it a new constellation                                 in your input data that you just never                                 save seen before or it is a change on                                 the cluster did someone roll out a                                 security fix or change the configuration                                 or is it a race condition that always                                 was there but this day is just the very                                 first time that you've observed that                                 race condition and if you want to be                                 back that root cause you need to have                                 some crucial operational capabilities so                                 it should be super easy to do a clock                                 analysis of all the involves data you if                                 you try to find out what the doc is you                                 want to be able to run statistics on                                 your input data on your intermediate                                 results and on your final results                                 even more important if the flow broke                                 today but it worked very well yesterday                                 what you want to be able to do is rerun                                 with the current configuration of your                                 cluster with the current state of the                                 world rerun the same flow on yesterday's                                 data and because with that you can rule                                 out that it's a data issue and if you                                 finally found and fix the bug you and                                 you have come up with a hotfix you want                                 to rerun it on today's data but you want                                 to run it on exactly the same data that                                 the first iteration run on because that                                 might have triggered this might have                                 less constellation you want to know for                                 sure that this is fixed and you can                                 sleep well                                 during the next night so how do we                                 decouple I mean we already said we don't                                 want the flow to own the data ingestion                                 some could it now argue well if the                                 database is an implementation secret of                                 the service the service should be                                 responsible for pushing that information                                 and that's not what we decided for so we                                 decided for a dedicated data ingestion                                 component and so the advantage here is                                 that that flows don't have to have                                 connectors for all the different                                 database technologies but also that the                                 services do not need to know about the                                 different Big Data technology which is                                 currently on work and fast changing and                                 also it allows you on a one central                                 place to implement features like hey                                 let's for everything that we ingest                                 let's always mount it to hive because                                 that will allow the kind of ad hoc                                 analytics and it also allows then that                                 data to be reused by many flows okay                                 so the important thing though is if you                                 have such a dedicated component it needs                                 to be generic so every team who hasn't                                 need to ingest a new data source should                                 be able to get it ingested so you don't                                 want every request for data ingestion to                                 be handled handed over to a central data                                 engineering team and sit there in the in                                 the gyro board for a few weeks until                                 that is done you want every team to                                 contribute to this central platform data                                 import and every ingested data should                                 immediately be available to other                                 consumers as well so that once you have                                 done the work of integrating it it                                 should be available to any use case with                                 any framework and including analytics                                 and as we said before if you have this                                 dedicated component you can easily have                                 feature parity for all data sources for                                 example mounting everything in hive so                                 the next thing that we quickly realized                                 is the importance of speaking a common                                 format and what I mean with that is have                                 at least one copy of all data in a                                 comment format it's okay to have                                 multiple formats but you should always                                 have this common ground and formats                                 ResearchGate                                 developed very very quickly when we                                 started first we use texts because that                                 was what all the hello world and hello                                 world in that case was word count                                 examples did but it has low productivity                                 low performance and it gave us a lot of                                 bugs so we very quickly explained in                                 exchange that the sequence files which                                 gave us better performance and fewer                                 bugs but also developer productivity and                                 then we very very quickly moved over to                                 a burrow and that was really helpful and                                 it has an amazing set of features if you                                 really look into it which you can build                                 other integrational patterns on top of                                 it so the schema evolution is really                                 nice the fact that it's self describing                                 is amazing because it's trivial to mount                                 another data set in Drive for example                                 and have full schema feed allergy                                 it has a reflect date on reader which is                                 really really underestimated in my                                 opinion and it's flexible because of its                                 robust nature you can use it for both                                 batch and streaming so but we didn't                                 want to stop there we wanted more we                                 wanted better performance better                                 compression that's why we added OSC as                                 an additional format but we didn't drop                                 the other one so you you can add                                 additional formats that have specific                                 features that are great for certain use                                 cases pH for certain frameworks but                                 still keep this common ground format so                                 we think OSE is great for batch and we                                 would love every framework to support it                                 but that does not mean that we will drop                                 Avro because that is still this one                                 thing that everyone understands and it's                                 flexible for both batch and streaming so                                 have at least one copy of our data in a                                 common format which means your choice of                                 processing framework should not be                                 limited by the format of existing data                                 so if one team says hey we want to use                                 this and for that we are importing it in                                 parquet that's totally fine but still                                 get the other one so that you can reuse                                 the data in other use cases and which                                 means then every ingested sauce will be                                 available for all consumers and when                                 optimizing for a framework you consider                                 a copy the next one is a bit related                                 that is speak a common language and what                                 I mean by that is continuously propagate                                 schema changes and if we go back to our                                 previous example the question is you                                 have some databases which have an                                 inherent schema and some databases which                                 are schema-less do we now have                                 structured or unstructured data the                                 thing is just because the data is in                                 MongoDB does not mean it's unstructured                                 for almost all of our use cases the                                 service and you knows very well about                                 the structure and knows exactly what                                 their data looks like and so this is                                 something we are really interested in                                 and it goes a little bit to the data                                 warehouse versus data Lake argument so                                 the classic data warehouse would mean                                 you enforce a schema at ingestion time                                 to do this schema on right but this                                 means you have to maintain all this ETL                                 stuff all those transformations and the                                 data lake in the classic sense assumes                                 no schema at all and basically defers                                 this task of discovering the schema to                                 the consumer now we didn't really like                                 both approaches because for the data                                 warehouse it's way too expensive to                                 Maine all these transformation processes                                 and transformation does mean a potential                                 loss of information of data so we didn't                                 want to spend time on on these                                 transformations we wanted to spend time                                 on building great features but just as                                 well we didn't like the data like the                                 pure classic data Lake approach because                                 losing schema information which is                                 already known in a service is really bad                                 and different consumers have to                                 rediscover the schema over and over and                                 over again and we don't have time for                                 that because we want to build great                                 features in that time instead so the                                 question is can we have both and with                                 both I mean the best properties of both                                 words not the worst of course so can we                                 preserve the schema is information that                                 is already present sometimes the                                 database level but in many times in our                                 use case at the application level while                                 at the same time always preserve the                                 full data never lose data be truthful to                                 our data source and by that meaning can                                 be always propagate schema changes that                                 you have in your source of truth                                 database and the question is can we have                                 something like a data lake house maybe                                 and it turns out become so what we did                                 is we went for a code first approach so                                 the service knows what the data looks                                 like and the service usually has                                 entities which describe how that data                                 looks like and we want those entities to                                 define the schema and whenever a                                 developer makes a change to those                                 entities then we know if we can get the                                 information from there we are always up                                 to date so we want to automatically                                 convert convert the schema which is                                 inherent in the entities to another                                 schema and that's possible and actually                                 everyone is doing that already today so                                 almost everyone is having some                                 annotations in the entities of their                                 services for Jackson for example to say                                 how this is converted to a JSON format                                 or you have entities from your object                                 document meta which tell you how are you                                 going to put this into the database and                                 here for example you have a field and in                                 your favorite programming language                                 unfortunately abstract is a reserved                                 word so you have to call it differently                                 but in different systems you want to                                 have it named properly and so what we                                 did we implemented a few other mm and                                 extra other annotations then make this                                 really easy that's already a while ago I                                 think it went into upper                                             available to everyone so we have our                                 name other ignore our alias a barometer                                 and with that we can automatically                                 generate another schema which has all                                 that information so we want to                                 continuously propagate schema changes                                 which means we want to make our data                                 ingestion process generic and driven by                                 those other schemas which means changes                                 in the other schemas are continuously                                 propagated to the data ingestion process                                 whenever you deploy an updated version                                 of the entities in production to your                                 service also update the other schema and                                 the data ingestion process which means                                 the which means the data ingestion                                 process now can be completely generic                                 and the way how to fetch it from the                                 database is all encoded in in this other                                 schema in metadata it also then allows                                 Averell schema evolution is a is a                                 really nice feature turned out because                                 you don't have to upgrade all consumers                                 straightaway you can still have cons you                                 much out there flows out there which                                 have the old schema deployed and our                                 schema evolution will usually do a very                                 good job of making your consumers read                                 that data caveat is of course if you                                 have a breaking change that is not                                 covered by other schema resolution you                                 still have to deal with that with a                                 change management process but the                                 important thing that we get out of this                                 is that everyone speaks the same                                 language and that was really surprising                                 how much that affected the organization                                 because the engineer who's building the                                 service and providing for example the                                 json api is talking about exactly the                                 same fields as the engineers who are                                 implementing the clients of these                                 services so the engineers who are                                 working with javascript for example                                 they're also talking about exactly the                                 same fields and engineers who are                                 building the big data flows are talking                                 about exactly the same fields and our                                 product analysts which are using for                                 example high F or ipython to do these                                 analysis also talk about the same fields                                 and this is of communications between                                 the company just reduces so many                                 potential friction and misunderstanding                                 so one of the biggest advantages of that                                 approach it turned out to be was the                                 improved communication within the                                 organization there was a small extra                                 benefit that we did not immediately                                 expect it's also coming out of this                                 combination of using Averell reflex                                 reader and using entities because if you                                 have exactly the same schema in your                                 life database as you have in your                                 analytic system then you can actually                                 share code and share a business logic                                 when would you do this imagine you want                                 to make a stricter validation rule on                                 your service then you of course can                                 deploy it there to production but you                                 want what you want to find out is how                                 many records will be affected by that                                 and you can take this stricter                                 validation rules just put it into a                                 batch process run it over all the                                 datasets that you have in your heart                                 cluster and then you can get statistics                                 what this change will do                                 what this will mean for for your data                                 and you can get this up front so that                                 was speaking a common language one thing                                 which which turned out to be really a                                 tricky one was modeling data                                 dependencies explicitly so we have split                                 it up now we have this explicit data                                 ingestion process but we're still not                                 happy and we are still not happy because                                 of that edge that is going from the from                                 the red box to the orange box because                                 this is still too tightly covered what                                 are the potential issues with that is                                 for example you want a flow to start as                                 early as possible but you don't know how                                 long the import will take how we solve                                 this is with introducing memento which                                 is a system which can be used to publish                                 datasets so what happens the ingestion                                 process provides all the data and puts                                 it into HDFS and only after it's done it                                 notifies and signals memento that this                                 data is really there and the consumer                                 can call for memento and memento as soon                                 as this data is published will notify it                                 and then you the consumer can work with                                 the data and read it so I'm showing here                                 the memento version to API this is still                                 work in progress right now at                                 ResearchGate                                 so you would publish metadata in the                                 data ingestion process for example in                                 this case we are talking about the                                 office collection in our refined                                 database and you would say he'll you the                                 type is HDFS and it's well it's from                                 basically yesterday so it's from                                                                                                            morning early on at all                                               publishing that and your consumer can                                 then pull with a certain waiting time                                 and it can describe with a language the                                 quality of the data that                                 to get so you say hi I want this office                                 collection and I require data from today                                 and I want it in the HDFS format I want                                 just the files with their files not the                                 highest one and what you get back is you                                 get from a mentor as soon as this is                                 published the information where to find                                 this so you don't have to know about                                 these HDFS parts in theory this could                                 mean we can even switch completely                                 storage like a fetch list from a                                 different cluster fetch list from s                                  instead and one important thing is                                 you're getting a unique artifact ID                                 that's really important because you can                                 then always refer back to that specific                                 version of the data that you requested                                 so what this means are more flexible                                 scheduling running you can run flows as                                 early as possible you can have multiple                                 ingestion or processing attempts while                                 at the very same time retain immutable                                 data so imagine something goes wrong                                 with your data ingestion pipeline and                                 you want to rerun it then you have to                                 make a business decision what do you do                                 with all the flows then have already                                 started do you want to keep them running                                 with the old data and only the new flows                                 pick up on the newer version of the data                                 if that is the case if that is a                                 decision                                 with the system you can do it you just                                 load another attempt a better version of                                 the data and the old flows can still                                 refer to the old data basically they                                 have repeatable reach and one very nice                                 thing is it allows analysis of the                                 dependency graph so you can find out                                 which datasets are used by what slow                                 again that helps a lot for the                                 communication within the organization so                                 we're still not happy because we learned                                 to talk about decoupling export of                                 results so we have the system in place                                 we are talking to momenta for the                                 ingestion but we are now not happy about                                 that edge about that edge here which is                                 about playing back the results                                 so first of all you want to split your                                 flows that it is very very easy to skip                                 this export thing you should model your                                 flows in a way that you can run them                                 side-effect free why is it so imagine                                 someone needs to install a security                                 patch on the cluster then they might                                 want to rerun the most critical flows to                                 prove that the system is still working                                 but we have already exported our data                                 for today so we don't want to export it                                 same thing you have a new development                                 version of your flow and of course you                                 test it with unit tests of course you                                 test it with integration tests but if                                 we're talking about big data you have to                                 also do this test run on real production                                 data because they might always be a data                                 constellation that you have not                                 anticipated and then you want to run a                                 development version on production data                                 of course you don't want to export that                                 and what's important every flow should                                 do this in the same way so that there is                                 no surprises so if you have this kind of                                 a dry run mode make sure that you do it                                 in a consistent fashion so we split this                                 up but the other thing is we don't want                                 this job to write to the database                                 directly this database should be owned                                 by the service only the service should                                 write to it and we have this small                                 special requirement that I talked about                                 in the beginning and that requirement is                                 we are writing back to the same database                                 that we have written from so this means                                 we are that we are computing but while                                 we are computing the world is still                                 turning it might be that the result that                                 the kind up with is no longer compatible                                 with the state of the world as it is                                 right now as it is a time of export so                                 instead we want to throw those results                                 back at the service and what we                                 experimented with and then turned out to                                 be very successful is just make HTTP                                 calls all the time so pushing results                                 via HTTP back to the service                                 what this means is that this export of                                 results is just becoming a client to the                                 service like any other clients so this                                 means it's it's much more it feels much                                 more natural for the service developer                                 also the service does not have to be                                 aware of how do I read this data from                                 HDFS how do I read this data from Kafka                                 it gets it pushed just by HTTP and the                                 service can validate each individual                                 result and check hey is this still                                 plausible is this still do I want to                                 ingest this back into my life system at                                 the current state of the world and you                                 can do this with plausibility checks                                 with business rules or with optimistic                                 locking and last but not least it makes                                 testing so much easier because it's so                                 easy to just spin up a service and then                                 on your developer notebook and then                                 throw some HTTP calls against it and see                                 how that behaves then to always have to                                 put a file for example on your Hadoop                                 cluster and then make it ingest from                                 there so that's really helpful for                                 testing as well so we came up with a                                 component which turns Avro files into                                 HTTP calls and we want that to be part                                 of the flow but we want to it to be a                                 standardized component so that we have                                 this small building block where we can                                 compose big resolutions out of it and                                 this component treats every single                                 record in the other file as in                                 individual and if there's one million                                 entries in the other file it will make                                   million HTTP calls out of it                                 it handles tracking of the progress it                                 basically treats this input files as a                                 queue and this also means you can stop                                 this process at any point in time so you                                 can stop it and do some maintenance work                                 some operational work and then resume at                                 any time it's also because it's a                                 standardized component it allows us to                                 have some standard features like for                                 every with every HTTP request send the                                 batch flow which from which this is                                 originating and what we can also do then                                 is the service can see oh this is not a                                 call which is originating from                                 human user this is a call which is                                 coming from this batch flow I'm                                 currently at my scaling limit I don't                                 want to auto scale anymore and I just                                 send a back pressure signal and this                                 component I'll go to a chippy will then                                 handle the back pressure signals from                                 the service and this is my last one for                                 today model slow orchestration                                 explicitly and then make that a big bit                                 faster so please consider using an                                 execution system and take the system of                                 your choice whether this is Azkaban or                                 luigi or FL airflow or any of the others                                 pick the system of your choice but don't                                 stop there this is the one point where                                 you can most easily create a maintenance                                 nightmare to come for your system so you                                 want to have extra standards here in                                 place and something like inject paths                                 always from the outside don't construct                                 them in your flow inject calculation                                 dates always from the outside never call                                 now in any of your batch flows consider                                 it making it functionally pure inject                                 configure is a configuration settings                                 never hard code something like hey this                                 mapper needs full gigabyte of memory and                                 foresee that you need different settings                                 for different environments because your                                 developer notebook will matter have less                                 RAM and less data than your destined and                                 the depth and environment will have less                                 RAM and less data than the production                                 environment and you don't want to waste                                 resources on your developer notebook and                                 this is                                                            settings so always think about ease of                                 operations tuning of settings and ease                                 of upgrades now I talked a lot about                                 batch processing and now you're probably                                 thinking hey batch processing is so                                      what about stream processing and so                                 let's look quickly at what kind of                                 sources of streaming data we have at                                 ResearchGate                                 so there are some services which have                                 natively which natively produce                                 time-series data and we put this as Avro                                 Andrew Kafka and then there are some                                 other services whose native format is                                 not so much                                 time-series data and we're the most                                 natural representation of your data                                 would for example be a graph and of                                 course you can turn everything into time                                 series data I mean look at the Journal                                 of your data base and you have every                                 change in time series data but it's not                                 the most natural way and so what I just                                 said is exactly what we did so we build                                 entity conveyor which is a system which                                 looks at the data based replication and                                 looks at every change that's happening                                 there and can put that into a Kafka Q so                                 what we can then do we can have stream                                 processors which join that information                                 which subscribe to both native time                                 series data and the time series data                                 that we provide from any other source                                 and what we do is we write the results                                 of that stream processor into a Casca                                 queue again and as this was working out                                 so well with the HTTP calls what we                                 decided to do is we decided to implement                                 a component that we call m to come which                                 deprives to capital topics and contain                                 convert those kafka topics to HTTP calls                                 and put them back to the service and                                 when you're thinking about it like this                                 we are going through the same patterns                                 we are going through the same principles                                 again we D Kappa data ingestion we speak                                 a common format we want all the data to                                 be available in Kafka so we want all                                 data to be available in Averell files                                 for all batch stops you want all data to                                 be available in Kafka topics as well for                                 streaming jobs and inside there we want                                 those records to be Avram then we want                                 to speak a common language so we are                                 listening directly to the database we                                 want to retain this full information so                                 when we are changing the entities of our                                 service we are also letting entity                                 conveyor know about that schema change                                 and we are decoupling the export of                                 results and throwing these back wire                                 HTTP to the services so you can have a                                 test version of the stream processor                                 which just writes to a different topic                                 and no                                 and is listening to that but you can                                 then do some analysis on it afterwards                                 so then there's also a lot of execution                                 specific things and those differ so I                                 fear we're running out of time I'm not                                 going into details with that one but                                 what you're seeing here is that almost                                 all of the rules that we came up with                                 that systems also apply for our                                 streaming systems and if you look                                 carefully you see one is missing so what                                 about number form model data                                 dependencies explicitly that's something                                 what we have not yet completely figured                                 out for the streaming world but we think                                 about it and potentially put Kafka                                 topics into memento storing off sets of                                 interest as decided by the producer                                 there as well and we hope that this will                                 facilitate switching between                                 incompatible versions of stream                                 processors so what I want to close this                                 talk with here is you have this big data                                 architecture and it's involving and of                                 course a stream processor is a very                                 different thing than a batch processor                                 so they have special requirements but at                                 the same time there's a lot of reuse if                                 you think about about these building                                 blocks and about these patterns early on                                 and what I like to encourage you to                                 think about is that there is not this                                 batch camp and that there is this                                 streaming camp and then there is no                                 connection between them actually we                                 think that they are going very very well                                 together and you should be able to just                                 do both as needed by your use case so                                 that's about it thank you so much for                                 your attention                                 do you have any questions                                 this one here so what were the factors                                 that made you create momentum stead of                                 using the existing workflow managers so                                 what we have in the very beginning we                                 tried to model the data dependency                                 between two different flows as an                                 azkaban edge which leads to a horrid                                 horrible yeah horrific and huge graph                                 that no one could look at anymore and it                                 entangled this to one unit of deployment                                 what you want is if you're running                                 hundreds of flows with thousands of jobs                                 you want that one team can safely deploy                                 just one flow yeah and know that it                                 affects only them if you try to model                                 that with this wave for example in                                 Azkaban edge then you always have this                                 monolithic flow which describes all the                                 data dependencies in your company and                                 that was just too entangled into a                                 restricted habit I say the companies                                 used to Luigi and our flow to do what                                 you describe could you so measure what                                 you were not satisfied with those tools                                 so we rebuilt this before Luigi and the                                 airflow was out so that's a short answer                                 so we started building this in                                          you help me curl in                                              basically it was development I'm sure                                 that other people have come up with the                                 same problem and come up with their load                                 this their solution for that what I'm                                 saying here is not that you should all                                 now use memento from now on what I'm                                 saying is model your data dependencies                                 explicit and really think about that how                                 you model them and think about units of                                 deployments you don't want one huge                                 monolithic configuration which describes                                 it all right well let's take what many                                 questions offline let's think the                                 speaker thank you so much                                 [Music]                                 [Applause]
YouTube URL: https://www.youtube.com/watch?v=Gz5enlaEjRk


