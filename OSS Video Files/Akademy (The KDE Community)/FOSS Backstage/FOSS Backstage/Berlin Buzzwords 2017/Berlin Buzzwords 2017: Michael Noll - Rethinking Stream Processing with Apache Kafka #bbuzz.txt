Title: Berlin Buzzwords 2017: Michael Noll - Rethinking Stream Processing with Apache Kafka #bbuzz
Publication date: 2017-06-15
Playlist: Berlin Buzzwords 2017
Description: 
	Michael Noll talking about "Rethinking Stream Processing with Apache Kafka: Applications vs. Clusters, Streams vs. Databases".

Modern businesses have data at their core, and this data is changing continuously. How can we harness this torrent of information in real-time? The answer is stream processing, and the technology that has since become the core platform for streaming data is Apache Kafka. Among the thousands of companies that use Kafka to transform and reshape their industries are the likes of Netflix, Uber, PayPal, and AirBnB, but also established players such as Goldman Sachs, Cisco, and Oracle.

Unfortunately, today’s common architectures for real-time data processing at scale suffer from complexity: there are many technologies that need to be stitched and operated together, and each individual technology is often complex by itself. This has led to a strong discrepancy between how we, as engineers, would like to work vs. how we actually end up working in practice.

In this session we talk about how Apache Kafka helps you to radically simplify your data architectures. We cover how you can now build normal applications to serve your real-time processing needs — rather than building clusters or similar special-purpose infrastructure — and still benefit from properties such as high scalability, distributed computing, and fault-tolerance, which are typically associated exclusively with cluster technologies. 

This talk is presented by our Gold partner Confluent.

Read more:
https://2017.berlinbuzzwords.de/17/session/rethinking-stream-processing-apache-kafka-applications-vs-clusters-streams-vs-databases

About Michael Noll:
https://2017.berlinbuzzwords.de/users/michael-noll

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              I haven't even started yet you know you                               don't know what you're in for right yeah                               thanks for the introduction                               a few words about myself I'm an engineer                               turned product manager working at                               confluent which is the company founded                               by the creators of Apache Kafka we're                               based in Palo Alto on the u.s. west                               coast and as I said I'm not doing                                product management work which is bit                                different to what I did before I am a                                contributor to Apache Kafka obviously                                I'm also committed to projects like                                Apache storm and a few others but                                nowadays most of my work is is really                                focused on Kafka so before we start let                                me ask a question though who of you here                                in the audience is familiar with Apache                                Kafka all right well let me take a                                picture okay maybe that was one person                                that is not familiar with Apache Kafka                                so for that person let me briefly show                                this slide here and what I want to show                                here is that Kafka started out as a                                messaging system a pop subsystem                                whatever you want to call it a few years                                ago but nowadays it's like a                                full-fledged stream platform which you                                can also use to process the data in                                casco for example or integrate cupca                                with you know technology such as you                                know Hadoop Cassandra and so on                                and so in this talk I will focus on the                                part that edit stream processing                                capabilities to Apache Kafka the so                                called Kafka streams API so another                                motivational example imagine we were to                                build a hotel business we would need to                                implement a few roles such as a security                                guard and on the technology side we                                could do this with let's say a single                                application or micro service that runs                                on one machine of course relatives a bit                                more complex so eventually we need to                                add further roles like receptionist room                                service and so on again you know more                                applications running on you know more of                                these machines this is already                                complicated but it gets even further                                complicated on the technology side as                                soon as we need to scale out so if you                                reach the point where one security guard                                is not enough but we need a team of                                security guards suddenly we need to run                                distributed applications across many                                machines VMs or containers and at that                                point it becomes to believe very                                challenging for us                                because then you're solving a                                distributed systems problem and what we                                realized when we set out to design the                                kafka streams API for Apache Tosca was                                that we wanted to give something to                                developers that is part of the solution                                and not part of the problem because                                oftentimes this with your yak shaving                                you have your actual problem and you                                start digging into these technologies                                and it gets even more complicated the                                more you add to your like portfolio of                                technologies and I think a key part that                                we realized is that as developers we                                want to build applications we don't want                                an infrastructure or clusters so we want                                our applications still to be a bit like                                these new technologies we want them to                                be elastic scalable for taller and you                                know and if you know of these things but                                at the same time we wanted to question                                know the commonly held belief do we need                                separate shared processing clusters do                                we need separate databases if you want                                to do stateful processing on streams in                                Casca do we need something like reddit                                or Cassandra just in order to do some                                accounting for example so essentially                                what we want to do is something like                                this analogy we want to enjoy the taste                                of beer but we don't want to have the                                alcohol because we want to still you                                know try it home safely and this is what                                led to the creation of the kafka streams                                API and I will talk a little bit mod now                                there's the androids that we did the                                things that you can do with it and so on                                but the point here is that it is a tool                                so that you can go to recent application                                support your company's core business if                                this thing isn't working your company                                 isn't working and that meant for example                                 that we build fault tolerance built-in                                 it's not something that you need to                                 enable in an obscure way you explicitly                                 need to disable it to get rid of it                                 say for elasticity in zone so we talked                                 about it in a second                                 so you might be familiar with this                                 concept of enriched my library say if                                 you're using Scala and you're turning                                 this upside down we're giving you a                                 library with the casco streams API that                                 enriches your applications and what does                                 that mean it means you get features such                                 as a cluster to go I have an example in                                 the next slide the kind of did a                                 database to go you can run it everywhere                                 you can use it for very small use cases                                 all the way to very large use case and                                 ually go in between those you get                                 exactly ones processing even under                                 failure you can do in your event Time                                 Protocol joins aggregation and so on and                                 a whole lot of more so as an example                                 here's an application that uses the cast                                 of streams API and I just want to                                 highlight this is everything that                                 doesn't look to run in the koshkas                                 servers it doesn't run in the Kafka                                 progress but at the pyramid of it in a                                 Vienna container or something like that                                 and if we want to scale out this                                 application we just spawn additional                                 instances of the application so I'll                                 talk about that later on but the point                                 here is that this should be super easy                                 you shouldn't install yarn just to run                                 it on two machines for example so                                 motivating example here                                 imagine your task to create a real-time                                 analytics paper like this one my common                                 use case would be no cybersecurity you                                 want to monitor whether any of your                                 services is currently under attack by                                 some adversary how would you implement                                 this use case so typically what people                                 have done and I've done that didn't pass                                 myself you would collect data in real                                 time in order to additional analytics                                 and so on then you would stand up a                                 separate processing cluster you know                                 like spark or storm then you would                                 submit a job to that cluster and some of                                 the cluster would write the results back                                 to a database and then you would have a                                 front-end application that would create                                 the database in order to show some                                 results like that roadmap now what's the                                 problem with that approach when the                                 problem is and I think this is one of                                 the key problems that are not necessary                                 technological technique technical                                 problems but people problems here we                                 what you can see is like the parts in                                 clue this is your application it spread                                 all over the place it spread across                                 systems and typically it's spread across                                 organizational boundaries like there is                                 a cluster team that runs the cluster a                                 database team that runs the database and                                 then you you are the lonely soul that                                 tries to stitch everything together just                                 in order to make this simple dashboard                                 so with Kafka streams you can collapse                                 all of that into single application set                                 if you want to you can docker eyes and                                 then deploying kubernetes you can use it                                 locally on your laptop in order to test                                 it and so on I have few more examples                                 like that so one point here is that you                                 can simplify your architecture a lot but                                 it also has radical implications how we                                 actually developed                                 deploy your applications in the first                                 place and that is what I want to focus                                 on but before I talk about that how do                                 you actually write an application in the                                 Calico streams API well you have two                                 options if you have a DSL which is what                                 is shown on the left this is more in a                                 functional programming style way to                                 write your application bit like new                                 Scala collections for example and then                                 you have the processor API on the right                                 which is often used for more complex                                 event processing the cool thing is you                                 can actually combine the two so you can                                 have like the normal DSL flow on the                                 left but there are some certain steps in                                 there that you need to optimize it with                                 something very specific to your company                                 and you can just plug those in so let's                                 take the no canonical word count example                                 simple thing we get inputs as text lines                                 and then we want to split the text lines                                 into verse and counter we've seen word                                 now how does that problem translate to                                 how you would work with Kafka strange                                 first if you want to develop this                                 application you can do it on linux mac                                 and windows I think Linux and Mac we can                                 take for granted nowadays but oftentimes                                 people are stuck on Windows if they're                                 working for a bank or an insurance                                 company in these other known tightly                                 regulated industries so you can use all                                 of these no common operating systems to                                 do that here's the key code that you                                 would write first line is the one that                                 reads from capita in the scale away the                                 second statement is the actual word                                 count and the third line at the bottom                                 is writing back to Kafka in the scale or                                 where this is not super interesting to                                 talk about but there is something I want                                 to highlight hard to see but we have                                 something called a case to indicate                                 table what is that so the thing that you                                 realize when you start to get motivated                                 to look into Kafka is that often times                                 you realize my business is actually                                 real-time screens of information from                                 customers users and so on so like an                                 ongoing conversation break and walk                                 between your customer so screens are                                 everywhere people here are familiar with                                 Kafka so I assume that you kind of                                 understand the idea of streams of data                                 and do this in real time but the other                                 thing that is important to understand is                                 that tables and databases are everywhere                                 - there's a reason why we've been using                                 databases for                                 kids in order to build applications                                 products and so on so the key point here                                 is to realize that in practice you need                                 both streams and tables all the time                                 whatever you're doing whether your                                 logistics and finance in fraud whatever                                 you need both strings in tables to order                                 to solve your problem so if you pick a                                 stream processing technology this would                                 be first last concept in the API for                                 example even for word count                                 you need streams and tables and                                 something important to realize here is                                 that there is a close relationship                                 between the two if you look in the left                                 column there is a table that is                                 undergoing mutations if you capture                                 those mutations mutations into a stream                                 a change structuring of the table you                                 can reconstruct the table as where and                                 we're exploiting that functionality in                                 numerous ways behind the scenes in                                 Apache Casca and we also give you the                                 option to exploit it in your own                                 application logic so for example in                                 capita streams you have a case tree and                                 a cake table you have operations that                                 take you back and forth between those                                 two or those that retain the shape of it                                 where count is a trivial example where                                 you already need this so for example                                 sometimes you have a stream and you want                                 a table like you want to get customer                                 information in real time and you want to                                 build a continuously updating profile of                                 every customer from a variety of sources                                 this is taking a stream and outputting a                                 table sometimes you have a table and you                                 want to get back a stream for examle in                                 order to make real-time alerts work                                 there are use cases where you have a                                 stream in the table you started with the                                 table get a changelog stream transform                                 that stream and create a new table like                                 a new materialized view of the original                                 table or you have a stream of data                                 coming in and you want to enrich this                                 incoming stream with additional context                                 information like customer information                                 about information about the customer                                 that just paid something on you know                                 Apple in the Apple store if there's a                                 fraudulent transaction or not if this                                 customer is never paid out of I don't                                 know Australia and so far only paid out                                 of Europe then most probably the                                 Australian transaction is fraudulent                                 so going back to the work out an example                                 typically you know other talks stop here                                 show                                 these few lines and say that it's just                                 one line or two lines but let's do out                                 of this a bit what do we need to do in                                 order to actually deploy this in                                 production in casco screens you only                                 need a little bit to add first we need                                 to configure for example where to find                                 Casca and at the bottom we just tell it                                 to start doing the work that's it we                                 have now an application that you can                                 deploy locally on your laptop and you                                 can run at large scale to process                                 millions of messages per second                                 literally like the code that I've just                                 shown you so that means whether you have                                 a small use case it's a good fit for                                 cascading at extremes if you have a very                                 large use case it too is a very good fit                                 for castle and casco streams and                                 everything in between in the company I                                 worked before I would say that maybe                                     of the use cases are in large scale will                                 still send our very small scale but                                 still important data on maybe medium                                 scale but still important data but we                                 were kind of forced to use completely                                 different technologies depending on                                 whether we reach that threshold of                                 having to push it across let's say from                                                                                                        discuss the screens use the same                                 technology all the way through from                                 local prototyping a manually installed                                 proof-of-concept in production all the                                 way to large-scale automatic deployments                                 but before we deploy to production right                                 we want to test stuff with custard                                 streams what you're doing is you're                                 deploying a normal Java Scala closure                                 application so you can use any kind of                                 unit testing tool and methodology that                                 you like you can also integration                                 testing with it and system testing as                                 well so since unit testing is a de                                 bourree to talk about let's talk about                                 integration testing so one thing you can                                 do for example is if you have an                                 application at work on that reach on                                 Kafka does stuff right back to Kafka for                                 integration tests you can spin up in                                 memory processes of Kafka and you know                                 services like conference give me a                                 registry if you working with ever for                                 example it's your schemas for your data                                 and any other thing that your company                                 already has and to the integration test                                 against that yes you can also buckeyes                                 that and you know run it locally run it                                 on Jenkins which also means that                                 whatever your company is already using                                 in order to do testing in our continuous                                 development continues in equations you                                 can continue to use that just because                                 you're working on some medium to large                                 scale data doesn't necessarily mean you                                 have to completely change your tool                                 chain                                 a completely custom get approval for                                 that from your intersecting whatever                                 now exhuming we tested everything we                                 want to deploy it what would we need to                                 deploy well you can pick whatever you                                 want it's a normal java application                                 anything that can run or deploy a JVM                                 application works which means basically                                 everything on the planet so you can                                 using those stuff like a puppet and a                                 little you can use enough a current you                                 can use cloud services you can use                                 acrimony these missiles you can yarn if                                 you really want to so all of that works                                 and why because there's nothing special                                 to it                                 you're just deploying a normal                                 application as it should be you're not                                 building infrastructure you're building                                 applications so for example with docker                                 how would that work well you have this                                 code that I just showed you you can                                 compile it into a jar and create a                                 docker image for it and then you just                                 launch one or more containers as many as                                 you need and you can do this your life                                 operations without disrupting you know                                 the data flow and talk about that in a                                 second so super simple it's so simple                                 that people typically don't believe that                                 it's actually working before they try it                                 out so but let's stop talking about work                                 count now how does that work so this is                                 a                                                                       a glimpse of it like a trailer but I try                                 to at least give you some pointers to                                 start with so something super important                                 for screen processing is fault tolerance                                 and particularly for tolerant state                                 which I'll talk about in a second let's                                 start with fault tolerance                                 imagine you have an application that                                 does stuff like word count it remembers                                 things that is seen in the past and then                                 it needs to do something if this machine                                 fails we want to make sure that we can                                 migrate this this work to a running                                 machine how do we do that well the easy                                 part is transferring the code logic like                                 you know your dollar file that is super                                 easy the tricky part is because is how                                 to transfer the state in a way that you                                 don't lose data and what we're doing                                 here behind the scenes is exploding this                                 relationship of the table is a stream as                                 a table is a stream and so on so we're                                 changed logging the state of the                                 application behind the scenes                                 continuously into changelog streams or                                 change our table using Kafka and when we                                 need it                                 you reconstruct it from there and all                                 this without data loss so this is super                                 cool for failure scenarios but this is                                 actually even cooler for normal                                 operations imagine you have an                                 application and you just run it on a                                 single container or single VM or single                                 their metal machine and this application                                 does some stateful processing now you                                 realize oh this one machine or this one                                 container is not sufficient I need more                                 power to process the data the only thing                                 that you need to do is to spawn                                 additional containers spawn additional                                 instances of your application and Casca                                 streams will automatically distribute                                 you know and like split and partition                                 the state and make it available to the                                 various instances and they start                                 collaborating on the work this works to                                 rely for per agency you don't need to                                 take down your core business just                                 because you need to you know add one                                 more machine to the mix I mentioned that                                 at the very beginning we really wanted                                 to design something that is thoughtful                                 and out-of-the-box that is fitting an                                 environment where you need to be running                                                                                                     during live operations and likewise if                                 you want to save money you don't no                                 longer need like three of these                                 containers you just stop some of them                                 and the remaining ones basically get all                                 the data all the state and resume the                                 work and continue working to                                 superelastic same for scalability                                 imagine you're having a large incoming                                 stream of data and you need to do                                 lookups against the database you know                                 like in Florida Texas one exam that I                                 mentioned earlier one of the drawbacks                                 of such an architecture under data paths                                 myself is that for every incoming                                 request you have to hit a remote                                 database across the network so one of                                 the downsides is you have a very hyper                                 record latency and for some use case                                 that is actually quite prohibitive you                                 really need to have like a few                                 milliseconds and not like seconds of                                 time but also you're kind of coupling                                 the elevated availability of your                                 application with the availability of all                                 these external systems so what do you do                                 if the database is down do you wait do                                 you retry is it acceptable in your case                                 are you violating a delay if you wait                                 for more than five seconds so and for                                 that what we allow you to do with                                 calculus changes again unifying the                                 the ideas of streams and databases and                                 tables and kind of lift the database                                 information into your application so                                 you're doing local lookups you can                                 exploit data locality which is super                                 fast and also you know coupling your                                 application with external systems and                                 dependencies beyond the scalability                                 improvements this also means you can                                 containerize your application there is                                 nothing that you need to like                                 orchestrate across different teams in                                 your company in order to make your                                 application work and of course                                 elasticity works in that case as well to                                 start more or fewer instances as you see                                 fit                                 I actually have a demo to show that but                                 since it's just a twenty minutes talk I                                 can't do that so so the last thing I                                 wanna mention here is what we call                                 interactive queries so what we allow you                                 to do is if your application just some                                 state will work you know like the read                                 database icon in here we allow you to                                 directly expose that information the                                 latest processing result of your                                 application to other applications                                 forward countries could be like what is                                 the latest count of the word hello this                                 gives you a lot of flexibility because                                 you can now use two different approaches                                 and it can combine the two approaches as                                 well if you need to exchange data                                 between your application and                                 microservice with another application or                                 micro service you can either do this                                 indirectly through Kafka as the                                 communication channel for all this                                 information                                 or you can expose it directly to this                                 other application and you can also you                                 know mixing and combine these two                                 approaches as you see fit for some use                                 case it makes sense to decouple for some                                 it makes sense to a couple and for some                                 it makes sense to a couple at the                                 beginning but because later on and just                                 to highlight here like these other                                 applications when using directive                                 Curie's that doesn't need to be a Java                                 or scale application but can be like any                                 programming language that you want the                                 JavaScript Python in PHP if you're into                                 that or COBOL and remember this earlier                                 example that I showed with this roadmap                                 they're continuously updated therefore                                 the things that allowed us to get rid of                                 the database in this use case is this                                 interactive curious feature because you                                 no longer need to have a database just                                 as a handover point between your three                                 processing part and                                 disturbing part might be the user-facing                                 service serving part and you can expose                                 this for example for REST API you can                                 use trick for that you can use a custom                                 interest protocol that you have whatever                                 your favor you can implement that so                                 here this is an example of REST API that                                 we built for a music demo that we called                                 cascade music we are generating charts                                 in real time based on incoming events                                 how which song has been played by which                                 user and you can create that live                                 through REST API and of course to sum                                 this up securities are supported as well                                 so if you want to encrypt it and                                 translate when you're talking to me                                 application to your Kafka cluster when                                 indicate or authorize I oughta write                                 your applications or only some of your                                 application they get the sensitive data                                 in your cluster that is supported as                                 well so to wrap this up as application                                 developers we really want to build                                 applications and not infrastructure                                 because there's too complex to tag them                                 sealing and too painful and with casco                                 streams what we wanted to do in the                                 Apache Kafka community is give you a                                 tool to build these cool applications                                 that power your core business without                                 having to say you know shoot yourself in                                 the foot and if you want to learn more                                 about this I give another talk at a cop                                 comida later today so you can drop by if                                 you have the time and you can also you                                 know correctly at our booth we have one                                 like in the the main conference area                                 where I have time for questions more                                 than I might have here enough thank you                                 you
YouTube URL: https://www.youtube.com/watch?v=ACwnrnVJXuE


