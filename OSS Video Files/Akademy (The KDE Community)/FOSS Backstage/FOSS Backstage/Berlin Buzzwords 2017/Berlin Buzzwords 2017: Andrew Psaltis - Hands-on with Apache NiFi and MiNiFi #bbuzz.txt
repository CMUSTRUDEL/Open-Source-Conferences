Title: Berlin Buzzwords 2017: Andrew Psaltis - Hands-on with Apache NiFi and MiNiFi #bbuzz
Publication date: 2017-06-15
Playlist: Berlin Buzzwords 2017
Description: 
	This talk will provide a hands on introduction to Apache NiFi and MiNiFi. Focusing on how to use them to securely, efficiently, and reliably collect data from the edge, curate and deliver it to other tiers in a streaming pipeline. This will be a very interactive session to ensure all participants understand the how and why of Apache NiFi and MiNiFi. We will cover the following topics:

Apache NiFi Overview
- Describe Apache NiFi and its use cases.
- Describe NiFi Architecture

Core Concepts
- Understand Nifi Features and Characteristics.
- Understand NiFi user interface in depth.
- Understand Processors
- Understand Connections
- Understand Processor Groups
- Understand Remote Processor Groups
- Explain Data Provenance in NiFi
- Understand Concepts of NiFi Cluster

Designing Data Flows
- Understand how to build a DataFlow using NiFi
- Learn how to optimize a DataFlow
- Learn how to use NiFi Expression language and its use.
- Learn about Attributes and Templates in NiFi

MiNiFi
- Overview
- Use Cases
- Integrating with NiFi

Read more:
https://2017.berlinbuzzwords.de/17/session/hands-apache-nifi-and-minifi

About Andrew Psaltis:
https://2017.berlinbuzzwords.de/users/andrew-psaltis

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              thank you thanks for coming after lunch                               so we're going to talk about attaching                               knife identify sounds pretty loud so the                               first thing I want to talk about it's                               going to set the stage for just data                               flow and the associated problems                               minifying knife I aim to solve so think                               simplistically we kind of think about                               data flow this way that people just                                think that I'm ingesting some data and                                move it along and I store it somewhere                                reality is the world looks more like                                this alright everyone's got data all                                over the place some people you know                                smiling or laughing is this is kind of                                the world we live in right that it's all                                over the place                                we're doing things with it and then try                                and store it it's really this mess of                                data moving around that considered data                                flow that knife I minify are designed to                                solve it really gets summed up pretty                                well in this X SCD cartoon right that                                you end up with like these                                             standards get people that sit back and                                decide that they need to come up with                                one general standard you know                                          good six months later you end up with                                   competing standards and we end up with                                the mess that we have of constantly                                different standards for moving data for                                storing data for encoding data for all                                sorts of things so if you then look at                                Apache knife I and it's Genesis and kind                                of where it's going                                it started out trying to solve that                                problem probably about                                                the NSA and these are some of the key                                features that were decided upon and                                still stay with it today as it continues                                to grow so some of the things as far as                                this to guarantee data delivery from the                                point of ingestion to the point of                                delivering data it's guaranteed of no                                loss you know to have data buffering                                it's not always is a consumer able to                                ingest data at the same rate so how do                                you buffer that how do you apply back                                pressure in the flow and how to relieve                                back pressure during the flow having                                prioritized queuing is so you see this                                whole data flow not all data is created                                the same not all data has the same                                importance you have some day that make                                a system that's an alarm that you need                                to act on faster and some data that                                could take slower so you could                                prioritize as a data move to the flow                                which is more important you have flow                                specific quality of service sometimes                                you prefer latency sometimes you prefer                                throughput and you can figure this as                                you go a variety other things too                                probably one that's most important is                                data provenance and we'll touch on a bit                                and kind of walk through and see how                                that works an inflow templating and                                clustering so knife eye is a web-based                                UI on top of this framework that allows                                you to drag and drop and interactively                                build these data flows ok so it's you                                could think about it as molding clay and                                we'll see as we kind of walk through and                                I'll build a couple of flows later on a                                demo that you get to interact with this                                data as it's moving and be able to                                change your mind be able to side things                                and move through so some of the things                                that need to think about it some the                                terminology that comes we start talking                                about                                                                get understanding as to the common                                things you'll hear the first is we'll                                hear a lot to talk about flow files                                anytime you look at anything to do with                                nya file you're always hear this concept                                of flow files you can really think about                                them as a unit of data that moves                                through                                                                an HTTP response where an HTTP you'd                                have the data right so you have the data                                and you have the headers in minify or                                   you would have the headers which would                                be called attributes then the data which                                is just the payload or the content so                                very similar type of concept to keep in                                your mind as we start looking at it that                                there's two different pieces the                                attributes and then the data itself a                                 processor we saw that screenshot before                                 you have these processors process is                                 really just anything that you could use                                 to ingress manipulate or egress data all                                 of them are called the processor so                                 anything that you do that takes action                                 is a processor you have a connection                                 between the processors this is where                                 you'll set things like                                 organization different types of back                                 pressure I think you have a process                                 group so this allows you to visually                                 simplify things and group them together                                 otherwise you end up with like this                                 horse blanket of the typical ard of                                 everything just all over the place using                                 a processor group allows you to simplify                                 that and make it clean as well as think                                 about it as a component so if we that if                                 you're familiar with the flow based                                 programming model knife I get a lot of                                 its ideas and a lot of it based on flow                                 based programming so if you come across                                 web based programming or you're familiar                                 with it this is how it maps in the flow                                 based programming world is what's called                                 an information packet in knife is called                                 a flow file you see things like a black                                 box in flow-based programming in ninth i                                 that's a flow file processor they can                                 see the other items that are on here so                                 if you have thoughts about where do they                                 come from or different you know want to                                 understand more about the background an                                 eye-fi with you read in the flow based                                 programming world you'll see a lot of                                 similarities and this gives knife eyes                                 based upon all those principles so if we                                 put together some of these terminologies                                 of a processor connection processor                                 group here you have a generate flow file                                 processor there's a connection between                                 that's a success there's a router an                                 attribute processor with an incoming                                 that's going out of there                                 and that goes says process data process                                 group okay so each of these will be                                 considered a this is processor rather an                                 attribute of processor process data a                                 process group and then where you see                                 this cue will be the connection between                                 us so I want to take a moment to talk a                                 little bit more about some of the key                                 features so the need for data provenance                                 the often times people think about it as                                 just its lineage and just kind of get an                                 idea as to where things were and kind of                                 the path through something but it really                                 provides a lot more than that so you get                                 this idea of having not just the                                 traceability and lineage for an operator                                 of understanding what happened but also                                 from a compliance as well so you get a                                 complete chain of                                 and a complete understanding of what                                 happened and every change to a piece of                                 data so if you have situations where                                 you're moving data across countries and                                 certain data is not a lead not allowed                                 to leave the boundaries of a country                                 then you have Providence to ensure and                                 provide audit that that actually never                                 happened                                 here we see financial customers that                                 users understand what changed to a piece                                 of data                                 what computations were done and you can                                 see that changed all the way through as                                 a data flows this shows an example of it                                 really just not lineage so this would be                                 a way that you can look at the                                 provenance this happens to be coming                                 from a processor that's putting data                                 into solar and you can look at this                                 lineage tree as two that it was received                                 there was a send event there's a drop                                 event you see more details about this                                 provenance event how long it was in                                 lineage the UUID for the flow file of                                 the size what it was you see the                                 attributes to see the content and all                                 this information you could also get out                                 of nine file and send to another system                                 so if you did want to use it for                                 auditing and understanding everything                                 done to data as it flowed through you                                 could get this data sent it to solar                                 scene people that take it to solar build                                 dashboards on it to understand the flow                                 of data people that work in a financial                                 space that load it into HBase or                                 something else they could then run                                 audits on it we do a variety of things                                 with this information if you look at                                 security oftentimes people moving data                                 that's sensitive but the security around                                 it and it's really just not about having                                 encrypted communication knife I supports                                 encrypted communication whether it's                                 from minified in ninth I or between                                 knife I nodes or on the wire anywhere                                 supports encryption also supports this                                 notion of not all users have the same                                 access to data so you could control what                                 users have access to what parts of a                                 flow what they could do to that flow                                 what they get seeing the flow back                                 pressure you can figure on this                                 connection so if you right-click you'd                                 see a dialog that comes up where it                                 shows back pressure that object                                 hold in that little picture is part of a                                 screenshot from looking at that this                                 allows you to do things to configure                                 back pressure for the next processor so                                 it's at every connection in between                                 processors you can configure the back                                 pressure based upon the number of flow                                 files or the size which whichever one                                 comes first so you can decide that in                                 this case this log attribute has a                                 default of it's ten thousand flow files                                 or one gig in size if that connection                                 builds up to have the ten thousand flow                                 files or one gig the upstream processor                                 knife I will stop scheduling it to run                                 so will automatically stop it from                                 running until that other processor keeps                                 up in this case that processor is                                 stopped so it's not going to do anything                                 but you'll see as we kind of walk                                 through a flow that you could stop a                                 processor make changes that you need to                                 change perhaps it's the regular                                 expression you're using or you're                                 evolving a JSON path you're doing                                 something with it you get a chance to                                 stop it make whatever changes you need                                 start again and then the data start                                 flowing and you get to do this at every                                 processor all the way through get to                                 make lots of different decisions and                                 decide what you want to do it mentioned                                 a little bit ago about prioritization                                 you again sometimes not all the data is                                 the same right if you're getting data                                 that's coming off of a sensor and                                 perhaps it has to be coming off of a                                 heart rate monitor and you're trying to                                 understand the health of somebody                                 certain patterns of measurements coming                                 off are going to be more important and                                 perhaps signify that the person is not                                 well than just normal activity or                                 they're sleeping so you get to                                 prioritize based upon either first-in                                 first-out so size or the data or an                                 attribute on that price on that piece of                                 data that's important to you so you can                                 decide that if you're looking at a value                                 that is heart rate that that is more                                 important than any other data in that                                 packet and if the value is above a                                 threshold and that has priority and if                                 one of these prioritizes doesn't meet                                 your needs and knife is completely                                 extensible and you can add their own                                 the latency first throughput again this                                 is on a per processor basis and it                                 allows you to make decisions and really                                 give knife I a suggestion if you will as                                 to whether that processor whether you                                 prefer to have lower latency in which                                 case every time there's a flow file in a                                 connection it's going to get a chance to                                 execute against that or if you prefer                                 higher throughput in which case it's                                 going to treat it more as like a batch                                 and if you move that slider close to say                                 the                                                                   guarantee it's going to happen at                                   second to second and it's really going                                 to be the number of flow files that are                                 in that connection                                 at about that amount of time and it's                                 going to schedule that processor to run                                 so in that case that processor is going                                 to get a batch of flow files to operate                                 on whereas if you skewed it towards the                                 lower latency then it's going to run                                 every single time there's a flow file in                                 the connection so you get to make these                                 choices at every processor level and you                                 get to and how that goes and see other                                 things too as far as like a scheduling                                 strategy the concurrent task you can                                 think about those really as if you will                                 the threads that knife I will run in                                 parallel for a given processor when you                                 look at the extension integration pretty                                 much everything that you see so the flow                                 files you can build your own there's a                                 maven archetype to get people started to                                 build your own processor there's a                                 notion of this reporting task when I                                 mentioned getting the provenance                                 information out of                                                      it's called a site-to-site report                                 provenance reporting task which just                                 runs and will get data to another system                                 soft in time to use reporting tasks for                                 that there's a controller service notion                                 of that really is much like a JDBC                                 connection or any sort of connection                                 pool or a connection pool to hive or to                                 other expensive resources so you see the                                 connections serve as a controller                                 service being something that you're                                 going to reuse across processors and the                                 REST API everything that you saw in the                                 UI and the stuff that we'll see in the                                 UI and the demo it everything that the                                 UI is doing is doing through a REST API                                 that knife that exposes so anything that                                 could be done in the UI and you could do                                 via a REST API so often                                 times people want to automate our tu-                                   so you could use the REST API to do that                                 so something to keep in mind that is                                 really just how                                                     often times as humans we want to put                                 things into a box based upon our frame                                 of reference and where we're coming from                                 ix is no different oftentimes depending                                 upon people's background and kind of                                 what they work on in their environment                                   side gets usually put into one of these                                 four bubbles almost always some of it is                                 definitely only confused by some of the                                 terminology that's use and some of the                                 processors are built that kind of                                 continue to help muddy the water but it                                 really kind of gets put into these it's                                 not any one of those it really has bits                                 and pieces of each of them does this                                 take a moment to kind of walk through                                 each of these and just kind of give you                                 some ideas of how to think of nya Phi's                                 it relates to those technologies so if                                 you look at like the processing                                 frameworks right you for a lot of stuff                                 talk about flink you know the last two                                 days about kafka streams hear people                                 talk about SPARC those systems are                                 fantastic for analyzing complex                                 relationships doing complex event                                 processing looking at data over windows                                 at time joining streams the updated                                 together and doing a lot of complex work                                 knife-like doesn't do that it doesn't                                 understand how to work across windows a                                 time doesn't understand how to join                                 streams of data together it's really                                 working towards that flow so if you                                 looked at it in an environment where                                 you're using spark streaming or kafka                                 streams or flink you look at knife eye                                 as being that tool that you could use to                                 get the data from a source to clean the                                 data massaged it transform it really get                                 it ready for something like link or                                 spark or kafka streams and so instead of                                 doing any sort of transformation logic                                 and data cleansing if you will in a tool                                 that's designed for analysis knife I                                 really excels in that space                                 knife I compared to like message buses                                 and things like Kafka the Kafka in the                                 related tools your fantastic job saying                                 give me as much data as you can as fast                                 as you can I'll hold onto it and give it                                 back when you ask for it but that is all                                 a black box right you produce a byte                                 array into Kafka and you get back out                                 this payload now if I could think of us                                 when it comes in its what you want to do                                 with that data once it arrives until you                                 deliver it so it's really everything                                 that's inside that's happening and it's                                 how you handle this data flow when you                                 look at ingestion frameworks like spring                                 or flume camel a lot of those work great                                 at orchestrating a workflow they're very                                 developer oriented tools you have to                                 build everything else around it ix is                                 not available is just a jar that you                                 could include in an application you add                                 things to it you use it to manage a data                                 flow and it provides all that tooling                                 around it and when you look at it the                                 first audience                                                        flow manager and it's allowing someone                                 in the enterprise that understands the                                 flow of data at an infrastructure level                                 and just the overall flow for them to                                 manage it and to manage the flow of data                                 without you having to write code or                                 change code because something changed a                                 different database is no longer                                 available because it's there's                                 maintenance going on how do you switch                                 things in or out how do you stop the                                 flow of data I do understand what                                 happened to the data knife I really                                 works in that space whereas in spring or                                 flume or any of the tools like that                                 you're left having to do all that work                                 when you look at it compared to the ETL                                 tools again we end up using the term                                 like transforming data with knife I only                                 leads to more problems and now the                                 latest version                                                       poor man CDC if you own some other ETL                                 stuff but it's really thinking about                                 knife I as it's great at dealing with                                 events and dealing with rows of data                                 there's new processors that allow you to                                 deal with many rows of data and to                                 sequel queries over them but it unlike                                 an ETL tool that's used to a warehouse                                 or any data system is connecting to that                                 has a schema ingesting all that data now                                 if I could connect to databases that a                                 problem and ingest it but you're not                                 going to be able to say select all the                                 rows from a customer table all the rows                                 from an order table and join them or do                                 some table like operations on them in                                 masse now if I doesn't have that type of                                 ability it's not designed in that ETL                                 world you'll see some processors again                                 that kind of somewhat start to bleed and                                 blur that line more but it will never be                                 something that you would use as a                                 complete replacement for informatica if                                 you're doing really heavy                                 ETL working informatica knife is not                                 that tool in the same breath though                                 there's folks that use knife I to do                                 image recognition and facial recognition                                 images and ingest image data and then                                 run like OpenCV against those images in                                 a flow and that same flow you could then                                 analyze Twitter messages you can connect                                 to a database you connect to s                                          to all these different sources of data                                 and have them all go through one flow so                                 you could operate on different types of                                 data which is quite different than what                                 you'll see in all the ETL tools so if we                                 look at it minify and how this plays                                 into things minify is a sub-project to                                 knife i and really have this goal of zen                                 let me get the key parts of knife i and                                 move them as close to the edge as                                 possible so knife is very comfortable to                                 run in the data center and assumes that                                 a machine is running on that it has full                                 use of a CPU of the RAM of the i/o of                                 everything that's there knife minify is                                 designed much more as I'm an agent and a                                 guest on an OS and how can you then take                                 these same types of capabilities and                                 move them all the way out how do you put                                 them on a car how do you put them on an                                 oil rig how do you embed them in a cash                                 register how do you push this type of                                 the                                 Providence secured in everything all the                                 way to the edge when you start to think                                 through that and think through how do                                 you move this code that's used to                                 running in a data center and what the                                 realities are of running out of the data                                 center there's you know a couple of key                                 things that come out you know obviously                                 there's limited compute capabilities                                 depending upon the hardware you're                                 running on restricted software does                                 often no UI scalability concerns                                 security becomes a concern as well so                                 you have a lot of these same things a                                 lot of the other features you'd see but                                 you got a lot of realities of what                                 computing at the edge is like when you                                 look at the features that minify has                                 they're almost identical to knife I so                                 guaranteed delivery same thing from the                                 point of ingesting data of whether it's                                 on a device somewhere to it being                                 delivered is guaranteed the idea of data                                 provenance so now you can understand                                 what's happening to that piece of data                                 from the time it's being generated all                                 the way back being able to have back                                 pressure and the pressure release the                                 prioritization of data are these things                                 things the key differences that you'll                                 see with minify is that unlike knife I                                 where you're pretty much molding clay                                 and it's this command and control                                 environment with minify it's very much a                                 design and deploy so you would design                                 the flow in knife i and then deploy it                                 as a yamo file to minify with that                                 there's also this notion of having warm                                 readable three deploys so as you change                                 the configuration and you push it to                                 minify then you can have it kind of do a                                 warm redeploy of that similar to                                 producing a war file that runs in a day                                                                                                        just warm restarts similar type of                                 concept and minify there are some things                                 that take precedent from                                                 see it minify the notion of site to site                                 so psyched the site's a protocol that                                 was developed for knife I it's available                                 as a Java client library it's used in                                 between nine and nine find                                 census is used between minified and IFI                                 can be used by anything that runs on the                                 JVM and also now in C++ you'll see                                 implementations and work that's been                                 done to use site-to-site to integrate                                 say knife I with flank or knife I would                                 spark or storm or other systems usually                                 not the best idea from a production                                 standpoint you usually want something in                                 the middle as a buffer not tie those two                                 systems together but you could do it so                                 if you had an idea of something that you                                 want to build to communicate with                                     whether to push data in or get data out                                 then you could use a site-to-site                                 library the providence information that                                 again is the same type of idea that you                                 see in knife I and you get fine-grain                                 event level access so when something is                                 created when attributes are modified                                 when it's viewed when it's downloaded                                 what's done to it everything is recorded                                 all the Associated attributes and                                 metadata about the event recorded as                                 well so now you can end up with a map of                                 this flow files journey from a device                                 where it was the data was originated all                                 the way back to knife I in a data center                                 so you get to see a whole lifecycle of                                 what happened to this some other stuff                                 that just shows how you would look at                                 that provenance event and where it                                 starts to differ is trying to get the                                 fit right so knife is built in Java runs                                 on the JVM minify has to try and work in                                 that same way so you'll see things that                                 are a little bit smaller there's no UI                                 so it's all just generated off of logs                                 that the configuration is declarative                                 using llamó and then a reduced set of                                 bundle components so the scoping of it                                 there's minified Java which is much                                 smaller has a reduced set of components                                 all the components that work with knife                                 I will work with vinify Java you                                 probably wouldn't want to though just                                 because of the size and then there's                                 minify C++ that's again smaller and                                 this idea of making it smaller and                                 writing it for particular processors in                                 particular libraries and languages like                                 iOS or Android so if we put the whole                                 thing together of where it would fit so                                 you have like all the way the edge is                                 constrained high latency localized                                 content environment you got stuff in the                                 middle so you have these sources of data                                 got to cut this regional infrastructure                                 then you've got this core oftentimes you                                 see like nine file in the core and                                 either nine fire minify at the edge and                                 it really that line gets blurred as far                                 as where it makes sense in some cases                                 this constrained device and only at the                                 edge people still put minify or put a                                 knife I and then in other cases it just                                 doesn't make sense                                 and minify makes more sense and you can                                 kind of bleed back and forth as to where                                 that goes all right so let's take a                                 moment and kind of look at it running so                                 let me just change my display here                                 okay still visible okay so this I have                                 one flow if we just looked at this                                 quickly you know so this across board is                                 the processors notice of input/output                                 ports a process a group a remote                                 processor group which that would be                                 another instance of knife I that's                                 running site-to-site or an application                                 that you may have this idea of funneling                                 templates and you can record stuff it                                 just take notes so if you were to                                 drag-and-drop you literally just drag                                 and drop and pick a processor so this is                                                                                                       released the other day I don't know if                                 there's more processors at it but                                 there's somewhere around you know                                                                                                             things you want to do alright so                                 everything from my sequel Kafka MQTT                                 pop                                                                    with Jason o RC so you can just keep                                 cruising through of all these different                                 things you want to do there's stuff to                                 execute script as interesting if you                                 were wanting to use that it supports                                 Python groovy Lua JavaScript sometimes                                 people will use them to have an idea as                                 to how they want to handle data before                                 investing in writing a processor                                 sometimes writing it in Python or in                                 JavaScript is good enough and you don't                                 need another processor so this flow                                 right here connects to the meetup API                                 streaming API so if I were to run this                                 you can see this is connecting over                                 WebSockets to go and get data if I stop                                 this here and so you know I don't just                                 you know want to see what this looks                                 like make it stop and you'll see if this                                 kind of builds up it starts the buffer I                                 could go here and I could list this and                                 see what's there so now I can see here's                                 what the data looks like here's the                                 details about it here's the attributes                                 that came in and these attributes                                 continue to build up as you go through                                 the flow if I then went and viewed it                                 I can then view this content so this is                                 a JSON I could view what it is and so I                                 could go through and just start working                                 through this and figure out what I want                                 to do in this case this flow it's going                                 to evaluate this JSON path I can look                                 and see what this is and it's going to                                 go through and from that JSON it's going                                 to look at dollar being the route get                                 event that event name and it's going to                                 start plucking this out right and start                                 filling these attributes with the data                                 that defines here so if we look at the                                 data provenance you can see where this                                 has run and the data that's flowed                                 through here of what's happened so now                                 you can see that okay there were                                 attributes modified here so now I could                                 see there's a details there's the                                 attributes so this is no value set when                                 the data was coming in there was no                                 value we set this attribute in this                                 processor so you can see everything that                                 went on you can then go and look at the                                 content if you got something wrong in                                 that mapping you can look at the input                                 claim what the flow file look like                                 coming in I think you look at the output                                 as to what you did to it going out so                                 you can see the before and after of                                 everything that happened you could then                                 replay this data if there's something                                 that you fixed and you want to change it                                 so you can see all this that's going on                                 again all this data you could get out of                                 nine five to another system as well so                                 let me just show you another part of a                                 flow so this one here I have minified                                 running on this Raspberry Pi that's                                 connected to my laptop and then I have                                 this Texas Instruments sensor tag which                                 has like a dozen different sensors on                                 this thing you get them for like fifteen                                 dollars right so I have minified running                                 on its Raspberry Pi I got the sensor                                 here if we start this thing running you                                 see that I'll start moving data across                                 you go over here let me see if this is                                 my Raspberry Pi let me go ahead and                                 start minify                                 okay so that started up                                 guys little no jsf oak of course helps                                 up in the right place right that this is                                 going to connect to this over Bluetooth                                 Low Energy start getting data if you                                 have this on something and someone                                 starts to knock the package around or                                 whatever it is                                 drops it you can see this is getting                                 stuff from drop it and make it fail you                                 see it's getting stuff coming off of a                                 gyroscope that's in here if you then                                 went and looked at minify it looked at                                 nine five this data is getting sent you                                 know in this case is just logging it so                                 if we stop this here stop one of these                                 let's see if i could get it to lets them                                 know we got data this coming through and                                 this ends up being like x one sets you                                 get the roll pitch and the yaw of what's                                 coming off of the gyroscope and you'd                                 see data that's flowing through but you                                 could run it in all different things you                                 could do a bunch of stuff with it so                                 this does not have minify on it but it'd                                 be an idea of how can you do it and                                 oftentimes something like this doesn't                                 have a full network stack so oftentimes                                 like an IOT setup you see people that                                 have something could be a pie or                                 something else                                 that would run as an IOT gateway that's                                 connecting to sensors getting data                                 gathering data on that sensor and from                                 that IOT gateway and then sending it to                                 minify or deny phi so in this case                                 minified running on the pi that data                                 provenance is from the point that it's                                 ingesting this on the PI all the way                                 back so then if I were to deliver that                                 data into Kafka you'd have guaranteed                                 delivery from the PI all the way to when                                 the data was put into Kafka and you'd                                 understand the whole path that data took                                 from a Raspberry Pi that could be                                 deployed in a manufacturing plant                                 somewhere all the way back as it moved                                 okay so if you want to learn of course                                 more about it there's different                                 resources join the minified Community                                 knife eye community participate it's                                 pretty active group there's a lot that                                 goes on and it's pretty fun once you                                 start writing it all the different                                 things that you could do they've seen                                 people that have done stuff on raspberry                                 pies and putting like a collar on their                                 dog that has a chip and determining when                                 the dog comes in and out of the house to                                 turn on or off the sprinkler system you                                 know do different things you'll see                                 stuff that Horton which is done with                                 connected car there's videos out there                                 of people controlling a sunroof based                                 upon what the GPS says in the car                                 whether you're it should be sunny where                                 you are or raining of controlling the                                 sunroof in a car so you could do a                                 variety of things pretty easy to get                                 going and everything that you see there                                 is extensible either with Java or a REST                                 API so that thank you for your time and                                 guess that's the buzzer take questions                                 if you have them Thank You Andre do you                                 have any questions                                 I think sitar mmm yeah                                 what I'm puzzled about about a knife I                                 it has a very nice user interface but                                 for what kind of volumes is it suitable                                 because um what I don't understand is                                 that you know you're talking about very                                 low latency and at the same time you're                                 talking about slow files and it sounds                                 to me like what what is it is it microbe                                 etching what no engine does use to                                 actually run the data can I run it at                                 really large scale on a Hadoop                                 environment or something like that right                                 so the question was what types of                                 volumes can knife I handle and how does                                 it do scheduling and can you run it in a                                 Hadoop environment did I capture that                                 correct so I'll answer in Reverse or                                 running on a Hadoop environment today                                 knife I doesn't run on yarn and mezzos                                 or anything in the future possibly will                                 I think there is ideas for it to run on                                 that so today does its own scheduling                                 and really the basis for that is when it                                 started ten years ago those other tools                                 were not even a twinkle in someone's eye                                 so it does all its own scheduling from a                                 scale standpoint work with                                 telecommunications companies that are                                 running                                                                 through knife I so it runs at very high                                 scale you know credit card company that                                 you know if I ask you what's in your                                 wallet you probably idea who they are                                 they use it to capture all sorts of                                 fraud information and cyber security                                 they move somewhere around like                                    thousand events a second through knife I                                 going from on creme to off creme to the                                 cloud so you could run it at very high                                 speeds or and low latency or you can run                                 it for bats and some people use it to                                 ingest files it really depends the idea                                 of a flow files and the reason for                                 mentioning of keeping in mind that                                 there's the attributes and there's the                                 content the attributes that I showed in                                 the UI so when you look at when you look                                 at this and if we even just go to this                                 other flow here and you look at any of                                 these when you look at this view here                                 right you get to click on this and look                                 at this flow file these attributes are                                 what move from processor to processor                                 the con                                 if at all possible stay still on disk so                                 once knife I ingest it whether in this                                 case is just a meet-up JSON object or                                 whether it's a                                                     something coming from you changing your                                 channel on a set-top box the content is                                 stored once when it comes in and as it                                 moves as long as you're not doing                                 something to cause knife I to have to                                 rewrite it like compressing the content                                 the content stays still the flow files                                 have a pointer to where that content                                 lives and just the little bits of data                                 that's the attributes move from                                 processor to processor so designed to                                 move things at pretty high speed I got a                                 question                                 and so we're using nine five and one one                                 of the problems that we that we face is                                 ingesting from rabbit and cube why                                 because if you want to ingest fast from                                 rabbit you have to at least get get                                 batches of information so multiple                                 messages and we are not being able to do                                 that with with my file because it's more                                 oriented on message per message space is                                 there any any way to tackle that or it's                                 supposed to be like that message per                                 message so which version knife I are                                 using I can tell it okay so that would                                 be if you can using like the consume JMS                                 processor you want to see if that                                 supports batching and it may be that                                 processor that either it cannot ingest                                 more than a message at a time if it                                 supports batching it should be able to                                 run and ingest more messages it be it                                 the processor of what it can do so if                                 AMQP not okay so but a good knife I                                 support Sebastian right correct supports                                 batching and if you're able to consume a                                 batch of messages of supporting batching                                 from its sources can depend on the                                 processor and if that processor was                                 written to support batching so it's                                 going to be so if it's AMQP is can                                 depend if whoever wrote that processor                                 if it supports ingesting more in a                                 message at a time                                 so there's cases words like query                                 database table and others that you could                                 define the batch of records you want and                                 the size of batches and things like that                                 so it kind of depends I know like a PCP                                 that's a similar if you set the batching                                 then it'll send out flow files based                                 upon the number of messages are received                                 AMQP you want have to look and see with                                 that processor thank you that questions                                 you mentioned that you are right that                                 they're writing to the local disk is                                 there any redundancy on it so it's a                                 server dies and what is if your data set                                 is bigger than the disks on on each                                 server or threa                                 do I have to have PP servers for that                                 all right so the question was at what                                 about data redundancy since knife is                                 writing locally so today there isn't H a                                 from the software standpoint of                                       sending bits of data all around a                                 cluster it'll run the cluster there's a                                 flow file repository that these flow                                 files are being written to a Content                                 repository and then a problems                                 repository those are all local so today                                 the best thing you could do is to have a                                 raid configuration that gives you that                                 redundancy AB there's work going on to                                 have h a at the data level from an                                 application of                                                        similar to what you'd see Kafka is how                                 it does replication and moves messages                                 around you possibly see that there's a                                 design for it on the Apache wiki for it                                 that doesn't exist today                                 so the best bet is at a hardware level                                 of having raid oftentimes people are not                                 using knife I as a data store the data                                 is in transit so sometimes losing disk                                 isn't that bad but you could configure                                 it and then as far as running out of                                 disk the amount of data stored for each                                 of those repositories is configurable so                                 you can configure it to meet your needs                                 for your unnamed credit card company                                 that you mentioned before what is the                                 hardware stack for a cluster look like                                 how big is it                                 so for knife I it's typical that you see                                 in this is I think it was on the slide                                 about comparing knife I like storm spa                                 or spark or flink where you see like                                 thousands of nodes in the knife i case                                 you see clusters that are you know tens                                 sometimes less so in that case they have                                 three nodes internally that run as a                                 cluster and then six in AWS to kind of                                 have the accommodation of matching their                                 hardware to what's in the VMS and AWS                                 rammer just                                                              network bound and then IO bound as it's                                 writing                                 and then memory and CPUs can depend on                                 what you're doing so but always is going                                 to be network first of how fast can't                                 move data okay thank you Alice Claudia                                 silk and your answer yep I'll be around                                 if anyone has further questions
YouTube URL: https://www.youtube.com/watch?v=U998Z_i7heQ


