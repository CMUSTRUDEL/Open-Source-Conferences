Title: Berlin Buzzwords 2017: Ted Dunning - Update on the t-digest: Finding Faults in Real Data #bbuzz
Publication date: 2017-06-15
Playlist: Berlin Buzzwords 2017
Description: 
	Is your system working? Really? Average response times and throughputs donâ€™t tell the whole story. To really understand what is happening, you probably need measurements like the 99.9%-ile response time. A growing number of systems are using the t-digest to do this. 

I will explain the algorithm with practical examples, talk about how it is much simpler and faster than before, talk about integration in systems like Elastic, Solar and streamlib, tell some real-world deployment stories and show some pretty pictures.

Read more:
https://2017.berlinbuzzwords.de/17/session/update-t-digest-finding-faults-real-data

About Ted Dunning:
https://2017.berlinbuzzwords.de/users/ted-dunning

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              I don't suppose I wind up famous for it                               but I guess this is the modern way of                               saying HP an avid Berliner right so what                               I want to talk about today is tea digest                               but I should hmm there it is okay so I'm                               Ted dying I work for map are I also do a                               lot of work with Apache lately on the                               board of directors previously incubator                               previously mentor to lots of projects                                and you'll see many of the projects that                                I was involved with actually here                                because this community is a big part of                                the stuff that I've been working on for                                quite some time                                vampire makes a data platform and Apache                                I assume that everybody here knows this                                is a slide I lifted from a more                                commercial audience who didn't really                                know what Apache was today I want to                                talk about tea digests but I want to                                talk more generally about why we should                                measure distributions not just single                                data points why we should be able to                                combine them and visualize them I'm                                going to talk about some of the basic                                ideas for how to digest is built and how                                some alternative algorithms work as well                                I'm going to talk about the recent                                results for the the upcoming release of                                tea digests and then show some practical                                applications so this starts of course                                like all good software with a CO an the                                student comes to the master you can                                pronounce student as user and you can                                pronounce you know a master as                                administrator or something like that or                                or ops person and the the student says                                it's broken                                this is a time-honored statement of a                                student or a user it's broken and of                                course the response to that is always                                what did you do I mean what what changed                                of course you've blamed the user that's                                a tradition as well and what has changed                                is this key really important question                                did you make a change did somebody else                                make a change                                the universe changed somehow that's the                                key to finding what went wrong and of                                course in our wildest dreams by saying                                what has changed we imagine the user is                                enlightened and they go off and figure                                out what the problem is                                doesn't usually work that way but it's                                just a story so finding that change is                                absolutely the key but it's hard to find                                that change and more and more we work in                                things that are real-time ish and so the                                things that we need to find the change                                in our values that change quickly and so                                it isn't a single value that we have to                                see we have to see somehow a                                distribution of values here's some data                                the stuff on the right these are the                                ping times back to Google from my hotel                                room last night                                I often wind up doing this trying threw                                out what just happened but look at it                                it's like                                                       sometimes a bit faster sometimes a                                little bit slower looks pretty decent                                yeah that's okay so it should be just                                fine                                now if we look at the mean like data                                science that we are it's about                                    milliseconds look at the standard                                deviation that's it so that's the end of                                the story right that's that's a lot the                                way an awful lot of people think about                                it but if we actually plotted the ping x                                over many seconds it's not that simple                                real life isn't that simple there's                                there something that goes on                                occasionally in these spurts when it                                goes from                                                             longer so a single value a single mean                                mean with a de standard deviation it                                doesn't tell us what's going on it                                doesn't tell us the true picture of the                                data and we can't have all of the data                                this is only a thousand samples in real                                life of course we don't have a thousand                                samples we have a billion                                and we have a billion from many                                different sources and so on and what's                                worse is that this kind of data is                                long-tailed it has this big peak that                                fools the mean and standard deviation                                then it has this very long tail of weird                                  and we need to understand that you                                 have to know the distribution of it now                                 knowing the distribution here's the                                 distribution now that isn't a simple                                 either because you can't really see all                                 the data out here and you don't have                                 really very good accuracy back here so                                 we can see already that a single number                                 is absolutely not enough to understand                                 the quality of what we're doing even                                 just with ping x from a hotel room and                                 of course we want to deal with much more                                 complex situations so as I see it what                                 we need what we really really really                                 need in time series databases and                                 monitoring systems and just OLAP                                 databases in general is I want to be                                 able to compute the distribution of some                                 value over any time period for any                                 restriction that I might want I'd like                                 to be able to take elasticsearch in and                                 find out the distribution of some value                                 for any particular subset of                                 measurements that I'd like and I don't                                 want it to take up the same amount of                                 space as the original data I wanted to                                 take up a tiny tiny fraction of that and                                 I don't want it to take as much time as                                 it would take to grovel through all the                                 original data I want to take a tiny tiny                                 fraction of that time I want to do any                                 query kind of along this idea give me                                 the distribution from this data source                                 where I have some filter give me all the                                 latencies that Tom had yesterday with                                 his Frodo application                                 well what about last month                                 that sort of thing so that's the goal so                                 idea number one would be to do this kind                                 of histogram these are fixed width bins                                 that are essentially predefined so let's                                 assume we have the bins we can now just                                 decide which bin it's in and then we can                                 count whenever a sample comes in we                                 decide which bin it goes to increment                                 account and we just have this big long                                 thing of integers it's useful I produced                                 a graph things like that but it's just                                 not very general because we have to pre                                 define the bins which bins whose data                                 what range                                 what accuracy and furthermore we have                                 very difficult dynamic range if we if we                                 just talk about Layton sees we might                                 talk about from                                                     milliseconds to                                                          have bins to handle all that we've got                                 very very very large number of bins and                                 that's almost comparable to the amount                                 of data we had it's not good enough it's                                 not what we want so let's try another                                 example                                 these are exponentially distributed bins                                 so certainly with certain kinds of data                                 say latent fees the data we just saw the                                 accuracy that we want is relative to the                                 measurement itself and that implies that                                 we could have bin widths that are                                 relative to the size of the measurement                                 one point one millisecond isn't that big                                 a difference from one millisecond and                                                                                                         from a thousand milliseconds it's the                                 same relative accuracy and so if that's                                 the kind of accuracy we want we could                                 make a bin which is roughly say ten                                 percent increase in size and now we can                                 cheat a little bit it's not exactly that                                 perfect ten percent in this cheating but                                 it's very very close and the trick is                                 that we can use peas                                 is just a few bits out of the                                 floating-point number that represents                                 the measurement itself and that will                                 give us the bin number so it's almost as                                 fast as the first example just take the                                 range divide the measurement by the                                 minimum take those bits out of the                                 floating-point number that's our bin and                                 we count typically we have eight bins                                 per octave                                 every time the measurement doubles we                                 have eight little bins it doubles eight                                 more bins but the bins are twice as                                 large and it doubles again and we have                                 eight more bins but the bins are now                                 four times as large as the ones over                                 here simple it's fast it's OK for                                 positive data it's OK for data that has                                 resolution that's small very tight near                                 zero and large far from zero but that's                                 not general it's really not general it                                 is a nice property have good relative                                 error in the measurement space but it                                 isn't a general property that applies to                                 all of these things here's a picture of                                 it and so here is simulated long tailed                                 data and you can see the accuracy the                                 bin width here is half the measurement                                 but out here it's a tiny fraction of the                                 measurement and here it is with a float                                 histogram you can see that we have big                                 guns out here and the bins get bigger                                 and bigger and bigger until they're kind                                 of the size of the number there and you                                 can see they get smaller and smaller and                                 smaller so that the relative accuracy                                 stays about                                                              way down so these are better                                 these are definitely better than the                                 fixed size bins for a lot of                                 applications but they're not everything                                 so let's talk about how we could have                                 completely adaptive bin sizes that adapt                                 to the data that we have and let us do                                 all of those OLAP type of queries that                                 we're talking about and the first                                 intuition is that we may not really want                                 accuracy relative to the measurement                                 we may want to have an accuracy relative                                 to the distribution and so if you think                                 about it if you want the median you                                 might want like                                                         minus                                                                 nine two fifty point one but if you                                 wanted ninety-nine point nine nine                                 percent I'll that point one percent in                                 quantile space is completely                                 unacceptable                                 it makes the ninety-nine point nine nine                                 percent I'll completely meaningless so                                 let's put the ACT you see in terms of                                 percentile and in particular let's make                                 the accuracy very very fine on the edges                                 and much more coarse in the middle so we                                 need to have bins with very small counts                                 in the edge and large amount of counts                                 in the middle and that has to be                                 adaptive to the data here's a picture of                                 why that is this is a synthetic                                 distribution it's it's a skewed                                 distribution and this is the first one                                 percent of the data on the Left we've                                 taken equal sized bins in terms of                                 samples and we take a hundred bins each                                 one has a hundred samples and you could                                 imagine they're trying to estimate the                                 quantile in this range is quite                                 difficult from that one bin from a bin                                 alone all we can do is in linear                                 interpolation so we're going to have                                 really crummy results here but if we                                 were to have adaptive size bins say with                                 two samples here eight samples there                                 nineteen samples there we can follow the                                 curve much more precisely and get very                                 very much more accuracy and the variable                                 size bins even though they have much                                 more accuracy there's only a few more                                 bins that are required so we kind of get                                 something for nothing practically it                                 turns out the original t digest                                 algorithms this is the beginning of the                                 actual update the original t digest                                 algorithm was kind of complex and the                                 idea was that we had bins and cinders of                                 bins that we remembered and for each new                                 sample we would find the nearest bin and                                 we would add it to that bin or not                                 depending on whether or not the bin                                 matched the size constraints that we                                 have to implement that required trees                                 fact adriennes right there I think he's                                 the author of the the well current                                 champion algorithm which is a balance                                 tree and such but it turns out that                                 there's a much simpler way to phrase                                 these algorithms and that's one of the                                 big updates you can take a bunch of data                                 you can sort it now if you just walk                                 through that sorted data I can group it                                 into bins of the right size really very                                 easily and I can then remember the                                 centroid and the amount of data in each                                 one of those bins and that's a tea                                 digest                                 now we don't can't do this for all the                                 data and so to update what we do is we                                 start with the tea digest from the                                 previous slide and a bunch of new data                                 we can sort those together and do the                                 same clumping operation now some of the                                 previous centroids are going to be big                                 enough that we can't add data to them                                 without making them too big fine we let                                 them stand sometimes the actual data                                 points themselves will need to clump                                 together to make things that are big and                                 as big as they can be without being too                                 big that's fine - we clump those and                                 sometimes the data new data will fit                                 next to an old centroid and it will fit                                 within the size constraints so it's                                 really a simple algorithm we buffer data                                 we add the centroids into that and sort                                 it and then we clump it and that's our                                 news tea digests so we can update this                                 fairly fast                                 and in fact we can update it in ways                                 that amortize many of the costs relative                                 to the final size of the tea digests not                                 the input size that's cool because the                                 finites                                 the output size                                 is now bounded and fixed that also gives                                 us an advantage in that all of the data                                 structures in this new algorithm are                                 completely static there's no heap                                 allocation at all during operation we                                 can also now merge these things very                                 very efficiently the merge algorithm                                 just takes a bunch of tea digests takes                                 all of the centroids all of the bins in                                 there sorts them together and then goes                                 through and clumps them as possible as                                 necessary so the new algorithms are very                                 very simple they give us the full                                 adaptive non-linear bins that we want                                 and they give us grouping and regrouping                                 the grouping and regrouping and the                                 merging is key because that lets this                                 all be parallelized lets us build                                 large-scale OLAP databases on this and                                 the speed is very good the previous                                 champion AVL tree digests had speed that                                 grew slightly with size whereas the new                                 one is just dead flat that's how fast                                 how long it takes because the                                 amortization is adaptive relative to the                                 compression size and the speeds good                                 this is a hundred nanoseconds that's ten                                 million data points per core if you have                                 a hundred cores doing this we're talking                                 about a billion data points per second                                 that's not bad the accuracy is as good                                 or better than it was we see point per                                 million sorts of accuracies at the tails                                 that's in in quantile space and larger                                 errors in the middle near the median but                                 River that a thousand parts per million                                 is still only                                                            the accuracy is still good so the status                                 then with a three point ax release is we                                 have a stable system there are some                                 known bugs in corner cases for data                                 distribution and the AVL tree digest is                                 still the best in the upcoming release                                 where we have some changes in meaning                                 for the API the accuracy is fixed                                 pathological cases this speed is                                 improved                                 there's no dynamic allocation with the                                 primary algorithm and for the float                                 histogram which is now added into the                                 package the speed is even better about                                 five nanoseconds per data point it's                                 coming real soon now if I had more                                 evenings and longer nights it would come                                 sooner                                 so probably winter will accelerate it                                 summer is a hard time here's some some                                 ideas about a real cuffs company these                                 guys have a very large cloud service                                 that starts with a I can't say any more                                 letters than that because that still                                 leaves some ambiguity and they want to                                 do metrics across all those machines                                 across all the services both internal                                 and external and they want to be able to                                 query any combination of metrics and                                 find out what the distribution is and                                 display it in roughly a second million                                 machines                                                     measurements sometimes thousands per                                 second and they want to be able to                                 aggregate that on any period from                                 seconds up to months and get a clean                                 distribution with high accuracy and they                                 can do that they can store a very high                                 resolution or                                                     histograms at high compression level so                                 they're all very small they can then                                 combine them and merge them millions per                                 second in order to get the aggregates                                 over there very large time frame and                                 they want to be able to say what has                                 changed and the way that actually comes                                 out what was the distribution of some                                 key performance indicator like launch                                 times or Layton sees our query times                                 yesterday what was it last month what                                 was it on these machines                                 what was it in North America what was it                                 in Asia they can now do those queries                                 using this system we also have customers                                 who have remote data centers and they                                 have stuff I mean                                 doesn't really matter what they stick it                                 into a stream they can stick entire                                 distributions which are only like a few                                 hundred bytes into a stream they can                                 mirror that off into analytics land into                                 Galactic headquarters and because the                                 stream app replication doesn't really                                 care it's all multi master they can                                 build a very very simple architecture at                                 least as far as they're concerned what                                 they do is add to a histogram until the                                 time period has passed and send the                                 histogram into a stream on the analytic                                 side they just collect the histograms                                 that they find of interest and put them                                 together and this is the kind of                                 simplicity you want for the application                                 side of a metrics monitoring system now                                 do you remember that graph earlier that                                 was the distribution the trouble is you                                 can't see these so I'm going to talk a                                 little bit about how do you visualize                                 these sorts of systems remember that                                 data down there                                 we have no idea what that is because you                                 just can't see it anything less than a                                 pixel really doesn't matter                                 here's good results here's simulated                                 query times or something like that for                                 good system and here it is for a bad                                 system good bad and can you even tell if                                 I sum if I didn't have the title there I                                 wouldn't be able to tell one percent of                                 the measurements imagine we have a                                 cluster of a hundred machines and one of                                 them is three times slower than it used                                 to be that means one percent or less                                 because it's slower it may even get less                                 traffic one percent of those query times                                 are out here now and we can't see it                                 because it's only one percent of the                                 data we want to be able to see that data                                 we want to see the difference well one                                 of the things you can do is you can log                                 the vertical axis you have to be careful                                 because there's a lot of zeros and then                                 you can see red is bad black is good the                                 vertical axis this is what is it two                                 orders of magnitudes so that's like one                                 percent and                                     one percent and you could see we're kind                                 of in the one-percent range where bad                                 stuff happens now if we think about it                                 we started with uniform bins here's what                                 non-uniform bins do one of the cool                                 things about the non-uniform bins is                                 that they are big out here and so where                                 we had really spiky up and down and up                                 and down stuff neighboring unit counts                                 here get merged together so that when we                                 log that vertical axis we have something                                 very nice here and this is a uniform                                 characteristic of all these nonlinear                                 binning is that they give you data that                                 average as well and so it visualizes                                 well and so now we can say here's                                 yesterday here's today something is                                 wrong we can now combine this with other                                 techniques for finding changes things                                 like LLR and and so on for finding                                 differences in counts and that will let                                 us say what went wrong remember the ping                                 data here it is on the log scale and we                                 can now see that there's a long tail                                                                                                           point eight percent of the time it's                                 good but about                                                           so we have ability to do these counts                                 wrong summary the pictures are right so                                 we have the ability to do the summaries                                 build histograms very high speed merge                                 them in parallel store them concisely                                 build query systems on that and now the                                 ability to see what the differences are                                 so that we can actually answer the                                 questions that we had at the beginning                                 which is why does Netflix doesn't not                                 work or why is the system broken what                                 has changed and I wanted to spend more                                 time here than just me talking we have a                                 lot of time now and not have you guys                                 asked questions I wanted to ask you guys                                 questions about where this ought to go                                 what's the applicability of this sort of                                 thing so who here has systems to monitor                                 who here is not a real theoretician okay                                 we I think the the colloquial term there                                 is everybody everybody has stuff that                                 they'll get in trouble for if it doesn't                                 work tomorrow or tonight at                                             morning                                 who here has Layton sees that they                                 measure well this is a new world isn't                                 it you know six years ago at buzzwords                                 it was all MapReduce sort of thing and                                 nobody cared about Layton sees now                                 essentially everybody does who has other                                 stuff measurements that users give you                                 that you have no idea really what it is                                 so if you know it's Layton sees you know                                 how to analyze it but how many have data                                 that users give you this just stuff                                 that's user stuff yeah so fewer say like                                 a quarter or so of the people I have a                                 lot of data like that here's the data                                 Ted deal with it                                 so these things could make a difference                                 than it sounds like who here has to                                 query that data and retain it retention                                 is a big deal right anybody have a                                 system like that they like to describe                                 and tell me how this wouldn't work or                                 how it would work okay I'm going to                                 assign questions Adrian does this look                                 like it it'd help you yeah the you've                                 got to digest now in your system but                                 this is faster and and more static                                 easier to combine it but it helps for                                 the reasons you mentioned that it had a                                 pathetic to the value that you have all                                 right so may take advantage of this                                 question to do future requests please so                                 often our users so currently with today                                 dress you try the centered' and account                                 white bucket and our users would like                                 try to forget to have Barbara gates                                 about some other fields that they have                                 in double comments and tea digests will                                 would allow to do that but the API does                                 not allow okay today so                                 you know it is not a feature in                                          me explain a little bit what the                                 question means the idea here is that                                 from one measurement one machine one                                 place there's some characteristics of                                 that measurement                                 what machine did it come from what is                                 the metric what is the application what                                 is the user what is the table what is                                 the file so it has a bunch of                                 characteristics that apply to the entire                                 histogram but then when we combine                                 histograms with different                                 characteristics some of these samples                                 like the red ones obviously come from                                 the red machines and the black ones come                                 from the black machines when I combine                                 these two I will get a histogram that                                 looks a lot like the red distribution                                 but I will forget that all of the black                                 measurements are over here and some of                                 the red measurements are over here and                                 so I think what Adrian's asking for is                                 it would be nice if those columns if                                 those buckets remembered the tags that                                 were on the measurements he's nodding                                 thank goodness and so then what you                                 could do is you could ideally draw a                                 circle around this part of the histogram                                 and say what is different why are these                                 data different from all others and does                                 that sound good to everybody else yeah                                 okay                                 well then I'd better do it huh barking                                 follow to the window yeah or I'll work                                 together with you on it the trick is to                                 do that economic like Ellen's got a hand                                 up and she picked on Adrian and said you                                 knew he was using tea digest could he                                 just say what his system is or his                                 company for I can say it for him he                                 works for elasticsearch and the reason I                                 pick on him is because he contributed a                                 much better implementation than my first                                 implement                                 and so I have to be mean to him now in                                 retribution for for showing me up a                                 little bit yeah he made it three or four                                 times fast or maybe five times faster in                                 the original implementation so that he                                 could put it into elasticsearch and so                                 with this change that he suggesting                                 users would like is that feasible for                                 you and do you see it in the near future                                 I think it is feasible the the questions                                 really come down to space time dynamism                                 it isn't necessarily clear right away                                 how will you make that as efficient as                                 we would like it might be that we use a                                 digest for the the tags or it may be                                 good enough just to have an extensible                                 list associated with each thing because                                 we don't necessarily need to know how                                 often the tags occurred we could have a                                 vote right here so with that                                 implementation we could remember the                                 counts for all the tags or not we could                                 instead remember just the tags for each                                 bucket anybody have thoughts on that                                 we'll take it to the mailing list so                                 yeah here's somebody over here yes we                                 run a fairly large open T is DB cluster                                 we have like                                                         second now or something I think and it                                 becomes very painful with yeah yeah you                                 know it does where we are you know we've                                 built a bit of a low latency solution to                                 get that second thing that we wanted a                                 second solution thing so we also have a                                 like a lower latency in-memory version                                 that is not open these DB that we built                                 ourselves but the question is is anybody                                 working on getting digests into                                 something like open DSTV or some more                                 you know where we don't have to build                                 everything ourselves so I've seen I've                                 seen some work on that and basically                                 what people do is they say I'll take a                                 float histogram with really coarse                                 buckets so like two buckets per octave                                 that means I only have                                               something for a decent dynamic range and                                 then I just make                                                      which just makes the port open DSD be                                 even worse because now instead of                                                                                                     those I want distributions but now they                                 each have                                                         doubled tripled quadrupled load on it so                                 it just makes things worse I think a                                 much more promising point of view is                                 something like the streaming                                 architecture that I talked about here I                                 don't have a map R hat and it hits red                                 with white letters on it so I can't wear                                 it anyway but the fact is that these                                 streams are very very fast and they can                                 handle very very large cardinality of                                 topics and so we could easily build a                                 system on that where each topic is the                                 metric and the stream then holds                                 whatever kind of data we want that could                                 be a measurement or it could easily be a                                 distribution of measurements and it                                 would be pretty easy to put something a                                 lot like the open T's to be API over the                                 top of that it's just a REST API and you                                 could query a stream by thing                                 give me these offsets well which offsets                                 I can have a side stream which is an                                 index into time so I can figure out                                 which offsets I want and I just get the                                 raw data then and I can combine them any                                 way I'd like so that's another                                 alternative and have an intern                                 experimenting with that basic idea and                                 the the difference in performance should                                 be stunning because even just one node                                 of these kind of streams can do                                    million inserts per second instead of a                                 hundred thousand three hundred thousand                                 is very very good on open TS DB we have                                 some customers who get it over a couple                                 million but it's scary you know at any                                 moment the wheels could come off yeah I                                 mean it's also I think for King at least                                 a bit of the fact that HBase has been an                                 awkward thing for                                 us I mean we do run a Hadoop cluster but                                 HBase needs a completely different set                                 of settings for the Hadoop cluster and                                 like it becomes a bit of an awkward                                 thing yeah eh basis is not a good                                 neighbor it is the one that you don't                                 want in the apartment house you want it                                 in its own house and to plug again you                                 can do better with map our DB because                                 it's more of a friendly neighbor but the                                 fact of the textual storage the fact                                 that open T is to be is so oriented                                 around one number per measurement per                                 time really really limits the whole idea                                 and putting it on a more general                                 streaming platform does a couple of                                 database is all designed around mutation                                 and this data once it's done it's static                                 so that it's crazy to pay the cost of                                 died neck dynamic content when you have                                 no dynamic content a stream is optimized                                 all around static content and is much                                 more natural for time series what's the                                 normal size of the T dices that depends                                 on your your your parameter right so if                                 you wanted say ninety-nine point nine                                 then I was set at around forty or fifty                                 the total size of it is twice the                                 compression rate so if fifty there could                                 be a hundred centroids typically there's                                                                                                         were typically the                                                  floating point for the centroid one                                 floating point typically for the count                                 so that we can go above two billion and                                 a little bit of other data so you can                                 you can guess that's about                                             uncompressed and then these compress                                 pretty well because the counts very                                 often are small so                                 some good ideas here yeah a patient                                 ma'am hi thank you thank you for the                                 talk um what for what I understood the                                 only aggregation happening in time-based                                 periods is the counting is the histogram                                 making right                                 I had only low-level aggregation yet                                 happening yeah at that point you lose                                 the shape of the signals you lose the                                 time shape yes you lose a lot of the                                 time shape so if you're doing it say                                 every                                                                    variation in that oh I was considering                                 doing only like minute or five-minute                                 intervals that's why I thought we would                                 be losing more of the time shape or even                                 devolve okay                                 have you considered or would it be                                 possible to do like Fourier analysis                                 frequency analysis and store the                                 frequency coefficients the decomposition                                 on frequency well my own feeling is that                                 if you really want the time evolution                                 store the friggin data so you know the                                 reason for doing something that makes                                 the data smaller in my mind because I                                 sell things that store data of course                                 but the reason that I think that's                                 making data smaller is good is so that                                 you could query it faster but if you're                                 sitting there saying what actually                                 happened in this three-second interval I                                 think what you should be doing is                                 storing the actual data because you                                 cannot reasonably summarize that data                                 and still see those dynamics                                 and and so you know in all of these                                 systems the point of it is compression                                 and sketching we can compress by by                                 saying what is the distribution and we                                 forget exactly when that happened or we                                 could say here's the time dynamics and                                 it's like a physical resonance system so                                 that phooey a analysis would compress                                 the data but in all of these systems                                 compression is in by                                 implication a lossy compression we're                                 aggregating data and and by nature that                                 is lossy if you don't want the loss if                                 you really want to know that data it's                                 going to be very hard to have a lossy                                 thing that gives you exactly the                                 original data unless you know a lot                                 about the signal so I would I would                                 think the right answer just store the                                 real data look think I'm worried the                                 other stuff and then Ellen has a hand                                 I hope she's got somebody over there                                 we're almost out of time I think one                                 more question yeah just about two more                                 minutes is there one more question back                                 there                                 somebody's wiggling it's like an auction                                 if you wiggle too much you bought it so                                 you better better have a question ready                                 if you're going to wiggle here we go                                 Adrian again have you thought about                                 moving to digest to an organization such                                 as yep it Apache Software Foundation oh                                 let me put this hat back on this was my                                 Apache hat yeah so we should always                                 consider what is the right course for a                                 project and my guess is the tea digest                                 is going to be a very stable project                                 there won't be big changes over time                                 adding the tokens and the histories that                                 way would be a significant change adding                                 a whole new class of algorithm to make                                 it five times faster that's a                                 significant change I don't expect that                                 to happen very often anymore as such                                 it's going to be boring the mailing list                                 will be very quiet and so on and also                                 now I've been a limiting factor so far                                 because the releases are not as often as                                 I would like if we had a bigger                                 community that might happen more often                                 but the community is very very small we                                 have five to ten contributors so far and                                 some have come and some have gone so                                 it'd be very very hard to build a real                                 community around this and so the point                                 of Apache is to build communities that                                 will outlive people and I think that's                                 totally a good idea if the project is                                 the kind that would act                                 I have a community around it I don't                                 think that tea digest is going to build                                 a big community so I don't think Apache                                 is appropriate for it                                 I do think that open source is                                 appropriate but for now I think it's                                 kind of a appropriate out of one-person                                 show level of effort if I ever get tired                                 of it then we need to figure out                                 something and maybe Commons mass at                                 Apache might be a sort of thing but even                                 there you have risk of orphan code that                                 nobody currently in the Commons project                                 understands or updates and so I think                                 it'll become static I think that it                                 won't matter that it's static and so I                                 don't think that another structure                                 unless you have an idea but so far I've                                 been very open about it I believe very                                 strongly an open community if it is open                                 source open source close community I                                 think is a bad contradiction but right                                 now I think it's appropriate having gone                                 through an incubation of projects that                                 just couldn't build a community I think                                 it's better as a non Apache low overhead                                 low friction amorphous thing                                 unfortunately we're out of time but                                 sounds like a lot of great discussions                                 you can find Ted afterwards let's give                                 him a round of applause for great talk
YouTube URL: https://www.youtube.com/watch?v=Y0eF7SuMQjk


