Title: Berlin Buzzwords 2017: Lars Albertsson - Protecting Privacy in Practice #bbuzz
Publication date: 2017-06-15
Playlist: Berlin Buzzwords 2017
Description: 
	With the rise of data-intensive applications, privacy and personal integrity has become a focus topic. Although companies may have incentive to collect all available data forever, privacy regulations act counter balance. Regulations limit the data that may be stored, for how long it may be stored, how access is given, and give users rights to have their data deleted and get information about the data stored by companies. 

The regulations put constraints on technical solutions, and makes it challenging to architect and implement systems that allow engineers to efficiently make beneficial use of sensitive data. Unfortunately, failures to properly protect privacy can be very expensive, since the work required to rework core data models and wash tainted data can be massive.

This talk provides an engineering perspective on privacy protection. The intended audience is architects, developers, data scientists, and engineering managers that build applications handling user data. We highlight topics that require attention at an early design stage, and go through pitfalls and potentially expensive architectural mistakes. 

We describe a number of technical patterns for complying with privacy regulations without sacrificing the ability to use data for product features. The content of the talk is based on real world experience from handling privacy protection in large scale data processing environments.  

Read more:
https://2017.berlinbuzzwords.de/17/session/protecting-privacy-practice

About Lars Albertsson:
https://2017.berlinbuzzwords.de/users/lars-albertsson

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              all right thank you                               you hear me okay waiting from technology                               anyway thank you for coming and for                               climbing all of the stairs finding your                               way here and for coming here in spite of                               Ted Dunning talking downstairs he's an                               excellent speaker so make sure to watch                               his video later we're good to go okay                               hear me well blended                                you already heard my name as most                                speakers here I've done a bunch of                                things and working with data has been                                one of the themes of my career from like                                twenty years ago when I did my master's                                thesis at the supercomputing Center in                                Stockholm working with scary things like                                MPI and Cray vector machines there will                                be data at the time and many of the                                things that I have learned during these                                years of picked up working at Spotify                                with data infrastructure and data                                modelling in their Hadoop based                                environment and their Hadoop cluster                                actually turns                                                      scary thought that was like                                bleeding-edge not long ago and nowadays                                I work as an independent consultant                                helping companies get value out of data                                and I'm interested in privacy sort of a                                passion of mine and my clients are also                                interested in producing because they                                need to comply with gdpr and other                                requirements so I went scouted for                                resources on the web and the resources                                that I could find were like falling into                                two categories either a very problem                                oriented the legal aspect here's                                everything that you might go wrong and                                you might get a large fine if you don't                                get it right or like someone selling a                                product to me and I put all of your data                                into our system and everything will be                                much better what I failed to find was                                like engineering oriented resources how                                to actually go about protecting users                                privacy without sacrificing all of the                                goodies that we have so I decided to try                                to give that talk myself but we you get                                only bits and pieces so please if you                                have things to contribute in this area                                you know see this as an inspirational                                of telling the world there are a lot of                                angles that you have to cover and if you                                want to protect users privacy and that                                you had to work on and I will focus on                                the bottom ones here architecture and                                culture most of the talk will be                                technical there will also be some like                                bottom-up culture awareness for                                engineers I will only give you tools                                patterns not a whole system that will                                solve everything for you so you have to                                take the pieces and assemble them as you                                see fit it's kind of like then an IKEA                                piece of Ikea furniture you have to do                                the last work yourself and I will assume                                that you have other ways to solve the                                other aspects for example that you have                                security primitives in place to like                                limit access and so forth so I will not                                touch the subject and I believe that the                                reason I'm here and nobody else is that                                I'm independent if somebody comes from a                                company speak about previously the first                                question they get is so what is like a                                jewel company and nobody wants that                                question so but the IV not enough places                                to cuss like reasonable doubt so what                                I'm saying today does not describe                                practices at any specific company there                                are bits and pieces that I picked up                                during my junior journey both from                                companies solving things the good way                                and company making companies making                                mistakes so from an engineer architect                                or developer perspective there are some                                things that you need to be prepared for                                and I'm going to spend a lot most of the                                time on the top one the ability to                                delete users and forget the information                                because that's a tricky one but there                                are a number of other things that you                                also need to be care about like limiting                                the access that employees up to date and                                limiting the access that any hackers                                that intruder a system might have to                                data and so forth and you there are also                                upcoming requirement of                                being able to export all of the users                                 data so forth I'll touch a bit of those                                 but most of the time will be spent on                                 the right to be forgotten so to set some                                 context here if you go back in time to                                 our first like web applications things                                 simple                                 there was a monolith and all the                                 database and all of the state of the                                 users were generally stored in that                                 database so if you get a request from a                                 user and say can you please delete all                                 the information that you have on me and                                 remove me and or what data we got to me                                 that was a fairly simple thing to comply                                 with and if you even if today you read                                 like it takes written by people that do                                 not understand how data processing                                 systems works today this seems to be                                 sometimes it to be an assumption so this                                 is like a cultural bridge that you may                                 have to get today we don't work this way                                 since the comment of the doop we now                                 have the power both economic and                                 technical to store everything that has                                 ever happened and we do so because it                                 gives us the power to go back and the                                 new pipelines to get more data and we                                 organize things in a in pipelines that                                 refine this raw data and data is copied                                 in these pythons and so forth we do this                                 for good reasons this in comparison to                                 the sort of database centric well that                                 we used to live in enables us to do much                                 more complex things it enables us to her                                 be tolerant to human error we can it's                                 easy to fix bugs on and the data                                 processing side by reprocessing and so                                 forth so we are not going to give this                                 up and go back to a sort of mutable                                 database world because this is one of                                 the factors that sort of separate the                                 wheat companies from the chaff companies                                 so we have to find a way in such an                                 environment to to be able to delete                                 users anyway if you scale this up this                                 is roughly what a data processing system                                 today looks like in the companies that I                                 work with                                 you collect either events on the left                                 side or dump databases and you pour them                                 all into something that goes by the bus                                 word data Lake this is the bus first                                 conference I'm happy to use buzzwords                                 without shame you can also call it cold                                 store roll store that rule a cool store                                 you never change typically unless                                 privacy comes along but for technical                                 reasons you preferred not to changes and                                 then you have pipelines refining that                                 data combining it and then produce like                                 refined artifacts on the right side for                                 example recommendation in this list and                                 so forth now I need your help here how                                 many people recognize this as a fair                                 description of the kind of systems they                                 work with or would like to work with                                 okay good most of the owners because the                                 what I'm saying is relevant for this                                 context but not for other context one                                 key component here which we'll get back                                 to a few times is workflow manager and                                 that is the sort of glue that holds your                                 pipelines together and knows about the                                 dependencies and helps you like backfill                                 when something goes wrong and helps you                                 build a stable system out of fragile                                 components and a lot of companies today                                 tend to use the Luigi or airflow way                                 have a python-based DSL for expressing                                 the DAGs and you have a fairly rich way                                 to express your your DAGs now how many                                 in here use either Luigi or outflow or                                 something similar just a few okay then                                 that will be my first advice of the day                                 if you're using something that is more                                 primitive like Lucy or bash script or                                 whatever you will have a difficult time                                 solving the or following the                                 recommendations that I give up here so                                 that the I really would like to                                 recommend you to go with a more                                 competent workflow manager you might                                 have completely different solutions that                                 are not allowed in that case I would                                 like to love to hear about them                                 afterwards                                 so the there are some patterns that will                                 fall into in order to make these fairly                                 complex complex systems work and be                                 stable in production we tend to favor                                 image mutability when we create datasets                                 we tried not to touch the mavs arrestor                                 and we have systems that are with at                                 least one semantics and redundancy and                                 do things more times rather than too few                                 times and so forth and we architect                                 these dependency graphs by making copies                                 of the data everywhere in order to have                                 reproducible systems it's it's actually                                 fairly common to have thousands of                                 copies for example of your entire user                                 database which of course makes it more                                 difficult to delete your users so in a                                 system where these like philosophies are                                 applied the quit the request please                                 delete everything that you have on me                                 you please enumerate everything that you                                 have on me can be really difficult to to                                 answer so I have a couple of solution                                 ideas to present you they some are some                                 are purely technical making it                                 technically at feasible and sort of                                 economically feasible to actually comply                                 with requests for deletion and some are                                 purely on the cultural side making                                 developers aware and then there are some                                 in between whether you can architect the                                 technology to to sort of push developers                                 to do the right thing                                 we'll start on the cultural side I                                 recommend that the the one of the first                                 things that you do is to classify the                                 data that you have and I the GDP are                                 another sophistication speed VAR                                 classifications this is not an example                                 of a classification this is an example                                 of an classification that you might                                 decide to use in your company each                                 company would have a different                                 classification and you need to make your                                 own decisions                                 it is common to classify in three tiers                                 on the bottom tier things that are not                                 attached to any people at all for                                 convenience I will give these color                                 names red yellow green and the middle                                 classification is information that is                                 not in particularly sensitive but might                                 be attached to a person like names or IP                                 addresses and so forth whereas most                                 companies also handle some sensitive                                 data you might think that you don't                                 because you don't handle financial stuff                                 for medical records but it's actually                                 easy to end up with some real sensitive                                 data anyway if you have like messages in                                 your application or if you have GPS                                 coordinates then there will be some                                 stuff that falls into some kind of very                                 song that's difficult to classify and                                 when you process this data there's some                                 kind of like approximate arithmetic that                                 apply if you take something sensitive                                 and concatenate it with something long                                 sensitive line yet still have something                                 sensitive for example if you put the                                 user some food in a mail message if you                                 aggregate things for example by counting                                 them none sir I'm saying usually is that                                 you always apply but you have something                                 non sensitive enough of it you can                                 actually draw conclusions from from that                                 data so if you manage to have enough                                 data on user that can be identified to                                 users you might actually be able to draw                                 some sensitive conclusions about their                                 but the like behavior and so forth so                                 these rules are like not clear and if                                 you make machine learning models out of                                 sensitive or semi sensitive data you in                                 theory you would hope that the machine                                 learning models generalize so that you                                 end up with something that's non                                 sensitive but because of overfitting it                                 turns out that if you probe these models                                 you can actually identify identify users                                 again so that's something to be careful                                 with                                 also on the cultural side I would                                 recommend that you use these                                 classifications and make them throw them                                 in the face of the developers make them                                 visible at all times                                 for example in names of datasets I                                 usually recommend to put them like early                                 in the path of your datasets or field                                 names or in wherever names are                                 appropriate because these makes the                                 developers aware of the privacy issues                                 exist on earth at their camping with                                 with personal data it also if you're                                 consistent it also enables you to write                                 them some simple cooling one of the                                 reasons that I recommend putting the PII                                 levels first in the prefixes is that                                 usually access rights are given on a                                 prefix basis at least in an Amazon s                                     there if you if you have for example                                 have green first then you know that you                                 can give full access rights to anybody                                 in the company on all the green stuff                                 and it simplifies your access rights                                 management so one other cultural thing                                 that we did at the company was to wrap                                 all of this sort of supported data                                 functionality into a single gateway tool                                 this this tool is similar to like the                                 AWS command-line tool or the g-cloud                                 command line                                                             company and then we push that all of the                                 data sciences and people that work with                                 data to use this tool it's this tool had                                 no business logic whatsoever                                 it only was it was only thin wrapper to                                 other programs and it had hardwired                                 configuration so I knew about the                                 production clusters and so forth and                                 this gave us a central point of like                                 governance where we could do audits and                                 to some degree verifications and also                                 enforce like policies that we want to                                 judge so for example if somebody made a                                 temporary cluster to the sample sample                                 with some temporary data that we could                                 enforce the dis cluster with the day                                 that was deleted after a while and so                                 forth                                 this may seem like like enforcing                                 bureaucracy but it turned out that the                                 data scientists were really happy with                                 this because it provided them with a                                 directory of all the things you can do                                 and it also shielded them from the                                 operations like the details of what                                 names or IP addresses and clusters worth                                 so they felt really enabled and and it                                 also pushed people to also doing the                                 right thing so one thing you don't want                                 people to do is download PII data on                                 their laptops because then you lose                                 control of it so there was no command                                 for downloading to laptop there was                                 there were commands for creating                                 temporary resources in the cloud instead                                 now the data scientists felt so enabled                                 by this that they started spinning up                                 clusters back and forth and then the                                 manager said I want these numbers by                                 tomorrow so after a few weeks we had                                 somebody who were now in in charge of                                 cost control which I see as a good sign                                 all right now tools the technical stuff                                 and forgetting about things if you have                                 a something like an adult cluster and                                 then a bunch of pipe lines and everybody                                 writes a new pipe lines                                 if you drop in a piece of my personal                                 data somewhere if you don't focus this                                 will spread like weed all over your all                                 over your cluster and out into the to                                 the online databases that you have for                                 serving users and so forth so you have                                 to have some kind of plan in order to                                 prevent this from from happening you                                 need to like pricing governance like                                 bureaucracy on the cluster or to                                 encapsulate data or to drop data the                                 best thing from a probe is a perspective                                 of course to drop data so you start by                                 dropping the data that you don't need so                                 you can either make your data completely                                 anonymous for example if you have                                 pageviews or somebody played a video                                 whatever you just drop the the user ID                                 and                                 it's for each record there is no way to                                 get back to the user ID when the data is                                 anonymous now unfortunately with                                 anonymous data is not luck you can do                                 you can count things but not more than                                 that you can still make the data                                 anonymous and type some information if                                 you look up the user and join in with                                 for example course demographical                                 information what age country gender and                                 so forth users when the data at the end                                 of the pipeline becomes more useful for                                 example for business insights but you                                 still have completely anonymous records                                 but if you add enough data so that some                                 data is actually rare for example they                                 are not in some set codes only very few                                 people live then you actually no longer                                 have known anonymize data and you can                                 connect it back to users if you have                                 really anonymous data the GDP are states                                 that the these rules no longer apply so                                 you don't have to fulfill all the rules                                 but I'm not a lawyer                                 don't take my advices as legal this is                                 just a link for your general information                                 you will need to go back to your company                                 and solve all of the legal things and                                 get approvals and so forth so you can                                 halfway anonymize things for example by                                 replacing the user ID with a consistent                                 hash of the user ID it's the upper                                 example here then the all of the records                                 are still linked back so you can see                                 that the same user was doing all of                                 these records they don't know which user                                 if you just look at one data set this is                                 called pseudonym ization there's a                                 misconception here that if you do this                                 it's called anonymization it's not it's                                 called pseudonym ization so don't mix                                 these up because these terms appear in                                 legal texts and so forth so be careful                                 with it                                 and this is still pretty sensitive                                 information because you can trace things                                 back to the user perhaps not by looking                                 at one data set they combined by                                 combining with other data sets so all of                                 the rules still apply so forth                                 like if you make it consistent - then                                 you if you have the user ID later you                                 can hash that and figure out which users                                 belong to which records so you can in a                                 student muxu donna mais pipeline like                                 build recommendation indexes and then in                                 your online service look up which users                                 is which if you you can also go like                                 half way by adding a salt into the into                                 your hashing so it's no longer                                 reproducible who wants me to explain the                                 concept of salting okay good then it                                 cannot be used for for things like                                 recommendations but you still get a feel                                 for the extent still get an information                                 about the user journey so you can use it                                 for like product insights and for                                 looking at user sessions and the travels                                 of users inside your website and so                                 forth so if you don't if you're not                                 discarding data or in complement to                                 discarding data you want to forget users                                 and then you will have your your online                                 user database you remove the the user                                 from the database and in your blue                                 cluster or SC or whatever the dump of                                 the debate database no longer contains                                 that user but that uses information has                                 been copied throughout all of the                                 downstream jobs so you need to do                                 something about that this sort of                                 simplest brute force solution is to                                 rerun all of your downstream jobs and in                                 this case this is the these are the                                 cases where you might benefit from a                                 fairly sophisticated workflow manager                                 because that's tool to help you do                                 reruns of us unfortunately they don't go                                 all the way for example there there are                                 no built-in support for like versioning                                 and revisions of data set so you have to                                 do a bunch of work on your own in order                                 to get this to work well it's also                                 really computational expensive but you                                 can start doing this today without                                 changing your data models so there are                                 better patterns if you tweak with the                                 data model                                 for example you can encapsulate your PII                                 data by not putting it and copying it                                 around and all of you get your data set                                 instead you put a like a hash or you                                 UUID in your data sets and that refers                                 to some kind of table where the PII                                 information is stored and then once you                                 remove things from the table                                 nothing can no one can access that                                 information anymore so that now you are                                 going to have one place we need to                                 remove things in order to drop useless                                 data and the drawback is that you need                                 to join with this table all the time but                                 from a previous respect perspective this                                 is a lot better now I'm mixing up a                                 couple of concepts here I'm saying                                 tables which is something that belongs                                 to a database world and I'm speaking                                 about data sets and so forth you can                                 have things like that like the PII table                                 in a day in a database usually it's a an                                 anti-pattern                                 to query live databases from from your                                 pipeline code but in this case it might                                 be a good exception you can also have                                 data sets that represent this this                                 lookup table which is what I would                                 recommend and in a usually in a                                 dependency graph or pipeline if you're                                 computing and data set for a particular                                 date say in this case in this example                                 all the purchases formed a you would                                 constructed your dependency graph to                                 point on the source data sets for the                                 same day so the uses for the same day                                 the orders for the same song and then                                 you make an exception for this dii table                                 so that you always depend on the latest                                 one so that you know that whenever you                                 remove something that it's going to move                                 normally we would want to have a                                 reproducibility but not in this case you                                 explicitly want things not to be                                 reproducible because you want to be able                                 to remove users and this is easily                                 accessible this this syntax here is                                 example from Luigi where this is fairly                                 easy to do I'm sure you can find                                 examples out there                                 so this is a variation of the the PII                                 table where you don't put the actual                                 data in a separate table instead you put                                 in a decryption keys so you encrypt all                                 of your PII data and copy it around all                                 of the data sets but in order to decrypt                                 the data for your pipeline processing                                 you need to go and look up in this PII                                 key table and then when you want to                                 remove a user you just throw the                                 decryption key out of the table you                                 still have still have all of the                                 encrypted data out there but since you                                 like a key you cannot use it and                                 therefore the user is protected                                 some companies legal departments might                                 not agree with this you need to get your                                 approval I can't vouch for that but this                                 is actually a super practical pattern so                                 if you don't know where to start in this                                 in this journey this this might be the                                 slide you want to go back to you can                                 also use this decryption decryption key                                 to decrypt multiple fields so that                                 simplifies life for you so you can                                 typically you would have a record with                                 lots lots of PII data and names email                                 addresses and so forth and you encrypt                                 them them all with the same record then                                 you only have one table where to remove                                 the keys and if you want to make this                                 more granular you can have different                                 keys depending on four different types                                 of data this gives users the ability to                                 ask for a partial oblivion please forget                                 some of my information but not others so                                 for example users might want you to                                 forget all of their information about                                 their whereabouts their GPS tracking                                 which you might be tracking for some                                 reason but they still maybe want the                                 personalization to work well more                                 accumulation indexes which are not                                 dependent on on the GPS tracking                                 there's another variation where you can                                 discard the ability to link to join                                 between datasets so in this case you                                 have two datasets for example like                                 activities and users and you have a key                                 field in these datasets which you use to                                 join the datasets now if you encrypt                                 that key on both sides and store the                                 encryption key here in the in the key                                 table once you throw away that key you                                 can no longer join these two together                                 which means that if one of these                                 datasets holds your activities if you                                 throw away the key                                 you can still benefit from the                                 activities for example to feed your                                 recommendation engines but you can no                                 longer figure out which user it was that                                 actually made these actions in all of                                 the key table account patterns that I've                                 shown here you can also apply a salt                                 thing if you want some cases if you                                 don't so you get consistent you get                                 consistent encryptions so that you if if                                 you don't solve in your encryption with                                 the key in your data set or cross                                 datasets one clear text thing will be                                 encrypted that's the same thing so you                                 can connect things together which you                                 may or may not want whereas if you use                                 salting you can still do the decryption                                 and get all of the information back but                                 if you only have the encrypted variant                                 you can no longer connect things                                 together now in some cases you have                                 conflicting requirements for example you                                 may have legal requirements to never                                 forget anything for for like financial                                 reasons or for for crime prevention                                 reasons and this is in conflict with                                 forgetting users but there is a trick                                 here that you use the same key table                                 pattern but when you discover the key                                 you don't drop it completely you just                                 drop it familiar all of your records                                 then you give it to somebody else                                 this somebody else might be user and you                                 can tell the user here's the key if you                                 want to make any claims towards us you                                 need to provide this key or it can be                                 some kind of external entity that will                                 only give back the key for example in                                 with a code Lord                                 so this enough about oblivion this is a                                 slide with some mistakes but I've seen                                 made that are very costly very very                                 costly one of them is to use personal                                 data as keys as now I mean not                                 encryption keys but as primary keys and                                 joining keys in your data models because                                 then they're really difficult to throw                                 away and reworking all of your data from                                 such a state is very expensive the other                                 mistake the C made is to publish things                                 that actually contain PII data and you                                 might do this by accident for example if                                 you if you your users might publish be                                 able to publish some kind of list with                                 compilations or document or whatever and                                 that URI contains the username or                                 something it that that's an easy mistake                                 to make but once you publish things you                                 cannot forget them and the third mistake                                 is to publish pseudonymous datasets                                 under the impression that they're                                 somehow anonymous and there are plenty                                 of examples where companies have moved                                 up here the AOL search dataset is the                                 most famous the you might remember the                                 Det Netflix like recommendation                                 competition apparently somebody                                 managed to dicen or pseudonymous that by                                 correlating activities with activities                                 on IMDB and they could figure out who                                 the users were so there's absolutely no                                 way to be sure if you publish the Donna                                 my data so juices just don't so another                                 trick the previous trickster spoke about                                 they they work best in the pipeline                                 domain where you have a                                 move scenario when you compute things in                                 pipelines you can go out and pull this                                 data set in and pull the table in in                                 your online services they might not do                                 this pool they might be online all the                                 time so it's useful to publish a sort of                                 data set or a stream of all of the users                                 that want to be forgotten                                 so that online services can consume this                                 and do whatever they need to do in that                                 serving database is not to remove users                                 where you should from a previously                                 perspective it's better to strive                                 towards the pool pattern so if you can                                 change your online services to like                                 rebuilder in the six indices on a                                 regular basis or something then you're                                 safer in terms of PII the push pattern                                 is more fragile because it's easy to to                                 miss you have PII leaks now I'm saying                                 the leak like that's a really simple                                 thing to do it turns out that it's not                                 because all of the our components and                                 all of our abstraction layers they also                                 have this like immutability and then                                 redundancy and so forth and just take an                                 example I guess a whole bunch of people                                 here know how roughly our Cassander                                 works you have a bunch of nodes and if                                 you actually want to delete some data in                                 there you don't go up to the nodes and                                 delete it you publish the or Cassandra                                 under the hood publishes a tombstone                                 record saying that this data is should                                 no longer be there and then spreads to                                 all of the nodes and everybody's in                                 agreement except that the date actual                                 data is kept for quite a long time until                                 you run something called major                                 compaction where you clean things up but                                 for some configurations of Cassandra you                                 cannot actually do major compaction and                                 it's common to see installations where                                 they are never done so data never gets                                 deleted                                 likewise if nodes are connected for a                                 while or disconnected for a while they                                 are not aware of this deletion so the                                 bottom line here that's for every                                 component you use you need to have                                 component specific expertise on how to                                 really delete things and this can be                                 quite tricky                                 likewise for every storage                                 storage layer they use be it virtual                                 disks or storage as a service or                                 whatever you need to figure out how to                                 actually delete things so I have a                                 couple of pieces of advice here keep the                                 number of components down in order to                                 minimize this comtesse that you need                                 keep the number of storage layers down                                 so that there are fewer things that you                                 need to know if you use cloud storage                                 layers if I understand the legal aspect                                 size you need to go out and get                                 agreements from from the providers and                                 saying that we they will comply if you                                 delete things and so on so that there's                                 a non-technical aspect to it as well and                                 try to come up with simple strategies so                                 if you for example if you're you're in                                 the cloud and use Cassandra you can run                                 your major compactions and then actually                                 cycle the machine so that no machine                                 lives for more than                                                    or whatever and as you cycle the                                 machines and remove the block storage                                 layer that's probably going to work out                                 well is to get an agreement with with                                 your cloud provider so find some kind of                                 simple strategy like this I can also                                 observe that the there's a cost to                                 heterogeneity in general I mean each                                 component that you bring in this is not                                 so high in a in a like a micro service                                 world where you have autonomous teams                                 and so forth because the teams can                                 actually fairly autonomous in the data                                 processing world the cost of                                 heterogeneity heterogeneity is much                                 higher because the data is naturally                                 more coupled you pass data all through                                 your organization through many teams                                 these the privacy regulations may                                 increase the cost of heterogeneity even                                 more                                 you not only need to comply with                                 forgetting users who want to be                                 forgotten you also need to limit the                                 amount of time that you that you store                                 data and I would advise you to try to                                 solve this in your workflow manager                                 because that's where data creation is                                 controlled that's where data destruction                                 also should happen make the default a                                 short retention and do an exceptional                                 whitelist of the exceptions rather than                                 the other way around because if you do                                 blacklist thing you will miss something                                 from Peru now fortunately retention                                 ideal or retention requirement is in                                 conflict with the technical ideal that                                 you'd like to keep all of the data                                 around or forever in raw form so that                                 you can go back in case you make                                 mistakes and so forth so what you can do                                 here is if you have PII a PII data set                                 that you need to remove because of                                 retention rules then we can take it to                                 take the first downstream data sets and                                 sort of promote them to your cold                                 storage to your data like and say that                                 we are now regarding this as the raw                                 data we will no longer be able to                                 reproduce it this has the advantage that                                 the all of your workflow management bags                                 still work as expected you know it's                                 just a sort of a social convention that                                 you no longer are able to discard this                                 data and if you make this first step                                 kind of washing and cleaning where you                                 remove the PII data or separate the PII                                 data then you are sort of maximizing                                 your ability to to rerun things later                                 there is a concept of lineage which                                 isn't talked a lot about but it will be                                 needed for the requirements to export                                 all of the data to users and to                                 enumerate all of the data we have what                                 you have                                 it's basically tracking data flow as it                                 goes through the through your pipeline                                 tags you can either these do this on a                                 granite large of data sets and then your                                 workflow manager is your best friend                                 there are no ready-made tools for it but                                 it's a good place to implement it                                 or on the granularity of fields there                                 are some tools how to do this I am to                                 work with them they do is typically by                                 instrumenting like spark or similar                                 tools today this is done mostly for for                                 like foreign generators pipeline change                                 management and so forth but it's also                                 useful concept to do when for example                                 when tracking that personal data is                                 leaking through datasets where it's                                 actually not needed and I have a                                 challenge to throw out out there I don't                                 believe it instrumenting the frameworks                                 is a good idea or a good place to solve                                 these things I think they might be much                                 more useful if you sold them inside a                                 type system so if there are any Scala                                 geeks in the audience this one's for you                                 you could for example wrap your data                                 type standard data types with an                                 ornament them with with light pii level                                 information and or history information                                 to trace where the actual data comes                                 from and build tooling around that I                                 haven't seen any such tool so this is                                 like a solicitation provocation to any                                 people out there that might want to hack                                 on this contact me if you want that's it                                 I couldn't touch down this myself so                                 there are a bunch of people who deserve                                 credit for helping me with the contents                                 of this presentation and in case you                                 want more context on all things like                                 building pipelines and the types of                                 structures that I'm assuming or workflow                                 managers or so forth there are these                                 links up there the first one is a                                 presentation I made last year and the                                 second one is a likely curated déja                                 engineering reading list contains a lot                                 of good information                                 the other two links are the the only                                 material that I could find there from                                 from like government authorities that                                 contain some kind of practical                                 information to help you going with this                                 I haven't dug too far but I like the                                 material that I saw it was solution more                                 solution and problem oriented                                 all right questions                                 [Applause]                                 we have time for one quick question                                 thank you for the great talk I never saw                                 any of these buttons applied in practice                                 but I have a question on one of those                                 you presented where you delete the day                                 encryption key                                 what if an user once there is their data                                 to be recorded again but then it's ID is                                 present associated to all data with the                                 old encryption key and the new                                 encryption key how do you end of that                                 the closest answer is this where you                                 might you might opt to give the user the                                 decryption key in case he would like he                                 or she would like to bring the data out                                 again if the user has to be forgotten                                 and you don't do this then the parent                                 data is permanently gone there's no way                                 to get it back does that ask you a                                 question                                 one day I'll deliver some sugar for the                                 user and a new key that was generated                                 and still associated to the same user                                 maybe its knife problem at that's why                                 you don't understand but are you                                 referring to this scenario where you                                 have multiple keys for the user even                                 with these for example if I drop the key                                 for just one set of TF and they want to                                 start encrypting again with a new key                                 because the previous one is lost how can                                 i dissociate between the old data with                                 the old encryption key and the new data                                 for the same user with the new                                 encryption key I mean for some the the                                 corruption will fail is that enough of a                                 criteria two outside areas okay the                                 decryption fails maybe there was an                                 elegant way to do so but okay these are                                 so lucky I don't have one for you sir                                 unfortunately we're out of time but                                 thank you Lars                                 [Applause]
YouTube URL: https://www.youtube.com/watch?v=QR3347Qr58A


