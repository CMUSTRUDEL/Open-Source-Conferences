Title: Berlin Buzzwords 2017: Erik Onnen - Optimization of Public Cloud for Efficiency and Scale #bbuz
Publication date: 2017-06-15
Playlist: Berlin Buzzwords 2017
Description: 
	Twelve months ago, Cloudability's operations team set out on an ambitious project designed to control AWS spending while simultaneously adopting automation best practices and improving overall efficiency. This talk will highlight lessons learned in optimization of AWS operations resulting in a spending reduction of over 50% while simultaneously doubling the improvement of resource utilization and growing top line company revenue.

Specific topics will include Cloudability's use of automation tools including Puppet, Docker and Packer in addition to AWS-specific tools like Lambda functions, Elastic Container Service, the EC2 Spot Market, Auto Scaling Groups and AWS Aurora. 

This talk will cover specific elements of operating a SaaS infrastructure for cloud efficiency but should be applicable to all users of cloud computing.

This talk is presented by Cloudability.

Read more:
https://2017.berlinbuzzwords.de/17/session/optimization-public-cloud-efficiency-and-scale

About Erik Onnen:
https://2017.berlinbuzzwords.de/users/erik-onnen-0

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              I'll apologize up front I'm less than                                  hours in Germany from the west coast so                               I'm a bit jet-lagged I also picked up a                               cold along the way so I'm probably going                               to drink both these waters as I as I                               going through the talk real quickly                               before I get started                               show hands in the audience number of                               people who are running production                                workloads in either AWS or GC PE roger                                half the audience or so ok so as                                mentioned I'm Aragon and I'm the                                vice-president engineering at the                                company called credibility I predominate                                have a distributed systems and                                operations background the majority of                                this talk is going to be sort of                                operations focused in terms of how we                                scale my team and cloud ability we'll go                                into a little bit about what cloud                                ability does is a product I'm not going                                to try and dwell on that in this talk                                this is more focused on how we've                                actually changed some our own internal                                operational policies so a real brief                                history about cloud ability we were                                funded in                                                        Portland if you're not familiar is                                between Seattle and the Bay Area on the                                west coast we our mission in life is to                                help our customers make the most                                effective use possible of public cloud                                and I'll go into a little bit of detail                                about what I mean by public cloud in a                                moment we are a pure SAS offering which                                means we don't operate a data center                                everything that we do is completely                                based in AWS for the most part we run a                                little bit of arms infrastructure in GCP                                Google compute but for the most part                                every bit of our offering is operating                                in Amazon Web Services and that'll be                                the focus of this discussion                                that's that most of what I'll talk about                                today is fairly applicable to the other                                major cloud computing platforms there's                                not very much AWS centric in this talk                                it just happens to be what we use to                                operate our infrastructure one thing I                                did want to emphasize is that as in                                startup we followed what I would                                characterize is a pretty typical startup                                path meaning in the early days we grew                                very quickly and if we encountered                                certain problems usually we tried to                                just spend our way out of those problems                                we didn't really focus on engineering                                first principles per se we're trying to                                ship features very quickly and that was                                what we favored we also did what was                                comfortable meaning what did the team at                                the time understand how to operate what                                were they familiar with use those                                technologies ship features as rapidly as                                possible get them market test them and                                move on and over time that becomes                                something that                                that sort of neglect for how we're using                                cloud resources is something that                                ultimately will catch up with you and                                that's going to be a big part of the                                focus of this talk so if I could sum up                                this talk in terms of memes it would be                                two things I will be playing the the                                role of the old man yelling of the cloud                                or grandpa Simpson but effectively this                                is a journey of how we got good at                                operating AWS cloud and so when I joined                                the company in                                                         needed to provide value to our customers                                but we weren't really keeping our own                                house in order and we over time had to                                learn how to properly operate cloud so                                that we could likely try and do for our                                customers extract the maximum value out                                of how we were consuming cloud resources                                so very quickly I'm going to cover some                                terminology that I'll throw around a lot                                of acronyms in this talk so that I can                                not articulate them every time I'll do a                                quick history of our use of cloud                                computing and then I'm going to go                                through a high-level overview of what we                                change in terms of how we operate our                                own internal teams that will then play                                into sort of our journey of cloud                                efficiency and some specific changes                                that we made and then we'll dig into                                some specific case studies around areas                                 where we have pointed improvements where                                 we want to improve how we were using                                 cloud and I think about a little bit of                                 time for Q&A at the end so some                                 high-level technical terminology when I                                 say public cloud I may use that                                 interchangeably with the term called                                 paths that's an acronym for                                 platform-as-a-service                                 I basically mean the same thing for both                                 of those and in every case you can look                                 at that as either AWS Amazon Web                                 Services Microsoft Azure or GCP Google                                 compute platform that's at least in the                                 context of this talk what I mean when                                 I'm talking about pass or public cloud                                 oftentimes there's a negative                                 connotation around public cloud people                                 think oh it's insecure and it's not                                 well-managed that's a bit of a misnomer                                 in every case these days modern best                                 practices would have some sort of                                 Software Defined Networking like PCs and                                 firewalls and various platform services                                 in front of things just because it's                                 public cloud doesn't imply an inherent                                 level of insecurity                                 this talk is also about efficiency so                                 efficiency in this regard means if I                                 spend a dollar with a cloud provider I                                 spend a euro with the cloud provider and                                 I getting the maximum value for that                                 dollar my operating my infrastructure                                 well and then there's another component                                 of this talk which is scale and scale                                 we'll have two meetings in this talk the                                 first meeting is sort of the                                 additional meaning of as our                                 infrastructure grows as we're operating                                 more resources within public cloud are                                 we able to do that without making                                 significant changes in our engineering                                 infrastructure but it'll also refer to                                 scale in terms of how my team operates                                 meaning can we do more with fewer people                                 or can we continue to grow the business                                 as a meaningful platform without needing                                 to throw a bunch of headcount at the                                 problem and hire a bunch of people can                                 we continue evolving our business                                 without significant pain efficiency and                                 scale are definitely not always going to                                 co-occur especially in an early-stage                                 startup so while we may be able to scale                                 easily by throwing money at a problem                                 that doesn't necessarily mean that we're                                 efficient in terms of how you use cloud                                 resources likewise you can be highly                                 efficient and be completely unscalable                                 so you can be super efficient it's been                                 very little money in clouds but not be                                 able to scale your business or your                                 infrastructure and not be able to bring                                 on new customers easily and so evolving                                 both of those at the same time is really                                 where a lot of the secret sauce from                                 this talk comes in and then I'll often                                 use the term compute resources that just                                 basically means there's a thing that                                 we're paying for in our case in AWS so                                 most commonly people think of that think                                 of resources as an ec                                           resources could also be something like                                 an auto scaling group or it could also                                 be an ECL cluster pretty much anything                                 that Amazon is going to charge you for                                 would be categorized as a resource and                                 the terms at this time so if you could                                 put yourself into a time machine and                                 rewind about                                                        largely what my life looked like I                                 literally spend lots of time in data                                 centers this is not me this is actually                                 a picture of the Oregon Google Data                                 Center that they run with GCP cloud but                                 effectively I spent a lot of times in                                 racks and data centers spending many                                 years sort of honing the craft of how do                                 we operate data center infrastructure                                 this when I started engaging in the                                 industry there weren't things like                                 puppet and chef config management was                                 really just sort of getting off the                                 ground and over time we learned okay                                 well how do we operate data center as                                 well it was an unknown thing we                                 eventually got better at it we thought                                 we had to discipline mastered and and we                                 sort of plotted on then along starting                                 in about                                                                 seeing AWS really emerge                                 and and that sort of changed a lot of                                 things in terms of what we thought we                                 knew about how to operate our                                 infrastructure so when I drink loud                                 ability one of the things that I                                 initially did was step back look at our                                 infrastructure look at how we were                                 operating things and see where I thought                                 we needed to pay some attention and                                 effectively at the time in                                              operating our data center like a cloud                                 so what does that mean                                 means the number of things as a handful                                 examples that I can point to we we never                                 really trued up decisions or reconciled                                 value meaning as a team spun up a                                 handful of ec                                                           over time and look at those ec                                          where was it a good decision to use the                                 instance size that we use was a good                                 decision to use the family that we chose                                 were they well utilized over time or did                                 somebody just spin them up and walk away                                 from them if it was a test environment                                 was anybody even paying any attention to                                 that over time we didn't really have any                                 sort of science around capacity planning                                 so if we needed something we click the                                 button we turned it on we provisioned                                 the resource but we didn't step back and                                 say ok well what do we think we're                                 trending over time what do we expect                                 where do you expect to be in terms of                                 our consumption cloud resources say                                    or                                                                   also point out that our platform in                                 terms of the service we're offering was                                 not being very adaptive to changes in                                 AWS ecosystem so if you don't follow                                 what Amazon is doing with AWS they're                                 innovating at a breakneck speed they're                                 constantly shipping net new products                                 over the last                                                           thousand major new releases that's not                                 necessarily linked in that new product                                 about what counts as sort of a minor                                 product iteration release or taking a                                 net new product market and we've quite                                 frankly weren't being adaptive and                                 taking advantage of services that the                                 platform was bringing to market another                                 concrete example of this is we were                                 running one virtual machine Pro workload                                 so for example we had an asynchronous                                 consumer that was reading a message off                                 the queue we would have deployment unit                                 that would deploy to is one or more                                 virtual machines that would consume that                                 queue but that was kind of the extent of                                 it right we wouldn't have say multiple                                 workloads running on virtual machines                                 and oftentimes if something was working                                 we would step back and not touch it and                                 then move on to the next task at hand                                 this is effectively not all that                                 uncommon in terms of how you would                                 operate a data center but for all of                                 these things I would contend that these                                 are not the right way that you should be                                 operating                                 efficiently in public club and then                                 there were handful of other things that                                 we needed to change at the time we sort                                 of lacked accountability or ownership                                 over resources so it was not uncommon to                                 go find a server running somewhere and                                 we knew how I was configured because it                                 was in our puppet infrastructure but we                                 weren't quite sure who the actual owner                                 of that was so if something was running                                 a trip in the red and it was at a                                 hundred percent CPU utilization wasn't                                 necessarily easy to go track down who's                                 responsible for that service we had very                                 little financial leverage of our                                 infrastructure and I'll get into what                                 that means in a bit more details would                                 go to the talk and quite frankly our                                 growth in cloud spend was not                                 sustainable so we were spending                                 significantly more and more each month                                 with Amazon as we grew our business and                                 we needed to step back and assess why is                                 this the case this is something we can                                 actually get under control or is this                                 something that's just going to continue                                 to spiral forever and we need to figure                                 out what to do and lastly we wanted to                                 be able to tell enough an incredible                                 story so you at the time in                                          running into                                                     platform for our customers and we were                                 helping them manage their cloud a fish                                 their cloud sort of resource utilization                                 efficiently but we weren't actually                                 applying those same best practices to                                 our own internal use of cloud and so                                 that's what the majority has taught                                 focuses on is how do we actually change                                 our operational policies to make better                                 use of cloud so there are number of                                 changes we enacted in particular we                                 changed some team structure we changed                                 our some fundamental pieces of our                                 architecture we changed our basic                                 approach to how we do operations in the                                 cloud and then we changed our approach                                 to cloud usage in general so dig in a                                 bit more there our team structure we                                 effectively really reoriented our teams                                 to be much more functionally aligned                                 around pieces of our architecture so one                                 of the offerings that we have for our                                 customers is analytics around how you're                                 consuming cloud resources and as any                                 good analytics companies want to do we                                 have a data ingestion pipeline that                                 pulls data down moves it into sort of a                                 data warehouse type infrastructure I'll                                 go over some details of what that                                 pipeline does but at the time we started                                 this sort of team reorientation we                                 couldn't actually have a clear owner of                                 the pipeline function in or did we have                                 a clear owner of say the analytics                                 function we have what would                                 traditionally be called full stack                                 developers and everybody kind of dabbled                                 in every piece                                 the infrastructure and so we set out to                                 sort of to specialize in certain areas                                 of the architecture and to have better                                 accountability around those areas of the                                 architecture we implemented what is                                 commonly called an SR remodel site                                 reliability engineers the acronym from                                 Google and what this means in short is                                 that if you're writing a piece of code                                 you are going to be the one that deploys                                 it to the production environment you are                                 going to be the one that is accountable                                 for when it alerts if it breaks in the                                 middle of the night you're the one                                 that's going to get paged not an                                 Operations team and we push that                                 accountability down into individual                                 development teams as part of that each                                 of these functional teams that own the                                 part of our architecture was responsible                                 for KPIs around efficiency and we'll go                                 into some examples of what those look                                 like but KPI being and key performance                                 indicator and you could look this as an                                 example this would be our my utilization                                 of my ec                                                               am I just wasteful e spending up new vc                                  servers when I don't need them am i                                 letting them linger and not turning them                                 off when I don't need them like being                                 elastic with my infrastructure those are                                 some examples of KPIs that we                                 implemented for individual functional                                 teams and then last thing we also had                                 for each team a notion of a unit                                 economic that they were responsible for                                 providing to the business and I'll                                 provide some concrete examples of that a                                 bit later on the architectural upfront                                 we made a conscious decision to                                 specifically embrace needed cloud                                 platform and tools so there's going to                                 be a lot of concrete examples of that in                                 the talk but effectively we wanted to                                 stop ignoring what AWS was doing around                                 us and to leverage God as best as we                                 possibly could while also focusing on                                 things that we were good at and adding                                 value on top of the platform but not                                 just operating infrastructure for the                                 sake of operating additional                                 infrastructure and in certain cases we                                 wanted to identify opportunities where                                 we could actually simplify some of our                                 architecture and maybe relax the needs                                 as they were related to the business and                                 they've an example of that a bit later                                 in the talk in regards to the operations                                 team we completely revamped that                                 function within our infrastructure so we                                 wanted our operations team to rather                                 than being a barrier or somebody who                                 received a piece of code from an                                 engineering team we wanted our                                 operations team to actually be an                                 enabler so we set out to make sure that                                 they would become the experts in                                 platform capabilities meaning they're                                 responsible for keeping tabs on what M                                 is                                 is doing they then take that they digest                                 that given their knowledge of our                                 internal systems and they actually                                 become a leverage function for our                                 engineering teams so they may see a                                 change in an auto scaling group for                                 example and in the way Amazon manages                                 cloud formation templates relative to                                 auto scaling groups and they'll turn                                 around and produce an example of that                                 back to the engineering team offer that                                 up as hey here's something that you                                 could leverage that we think would help                                 this particular service that you're                                 maintaining their job is no longer to                                 operate code for people it was to more                                 impactful II enable the engineers                                 themselves that were actually deploying                                 code the obstacle in our environment                                 have responsibility for shared services                                 so things like our elk stack or log                                 aggregation our metrics monitoring and                                 alerting infrastructure they sustain                                 that in that infrastructure but it's a                                 tool that is used by individual                                 engineering teams themselves and then                                 they kept some traditional duties as                                 part of this in terms of month being                                 security resources for the teams but the                                 key takeaway here is that we really                                 changed them from being somebody who's                                 on the other side of the fence somebody                                 who is in sort of an adversarial                                 contentious relationship to being an                                 enabler for the engineering teams and a                                 partner to the engineering teams so that                                 lets the oh sorry I glossed over one                                 last thing originally that last bullet                                 point was eat our own dog food which is                                 sort of an industry saying my marketing                                 team didn't like that so we called this                                 drink our own champagne but this                                 basically means take our own product                                 apply it to how we operate our                                 infrastructure and feed that back into                                 our product to make it a better stronger                                 product based on how we're actually                                 doing the thing that our product is                                 meant to do so the remainder of the talk                                 is going to go through what I call the                                 the journey of cloud efficiency and it's                                 broken down into four stages I'm going                                 to frame this relative to our specific                                 experience with AWS but I can tell you                                 from what we see from our customers this                                 is a very common sort of evolution in                                 terms of how you adopt cloud                                 infrastructure in the the stages                                 themselves aren't really measured by any                                 metric so that sort of up into the right                                 gradient in the area underneath that                                 doesn't really qualify anything it's                                 just meant to convey a level of                                 sophistication and a level of effort                                 that you need to put in to continue                                 doing cloud well effectively so the                                 first year visibility is a pretty                                 straightforward thing but you'd be                                 surprised how many people don't get it                                 right                                 and it really starts with just simply                                 tagging all the resources that you're                                 using inside of an infrastructure and                                 that probably sounds like a pretty                                 obvious thing to this group but I've                                 lost track of the number of customer                                 deployments that I've seen of our                                 software that had little to no tagging                                 policies it's to a point where now best                                 practice in the industry is basically to                                 terminate an ec                                                         adhere to a tagging policy when it's                                 launched and to do that as quickly as                                 possible so for example one of our                                 customers you literally cannot launch                                 infrastructure in their environment                                 unless it meets the specific tagging                                 policy around that ec                                                    you adhere to this this then has a lot                                 of side benefits and side effects that                                 you can build on as you move through                                 this journey of cloud efficiency but it                                 really does start with tagging                                 unfortunately we can't tag everything in                                 AWS today there are just some resource                                 types that cannot be tagged but Amazon                                 usually fills those gaps pretty quickly                                 and I would also argue that this is a                                 net new thing in terms of the ability to                                 tag resources in a precision that we                                 have around resource visibility is not                                 something we had back in that data                                 center world so it's a bit foreign if                                 you're coming from operating a data                                 center at scale in terms of how you                                 operate your cloud infrastructure this                                 notion of adding decorative metadata                                 around resources and things that let you                                 put resources into meaningful groups is                                 a relatively new and learned skill that                                 that we've had to adopt once we have                                 tagging then we can start to put                                 resource consumption into groups along                                 with those functional teams so we can                                 actually start to get a better sense of                                 how an individual team is consuming                                 resources on the cloud you can contrast                                 this with previously everything was just                                 kind of thrown into a production                                 environment account or a staging                                 environment account or a dev environment                                 account but you would have to go track                                 down individuals and say are you                                 operating this server is so yes okay                                 great now I understand that this dollar                                 amount goes to this team but where does                                 everything else go with proper tagging                                 we can actually start to put cost and                                 resource consumption utilization into                                 buckets that belong to a team and then                                 have more meaningful conversations with                                 those teams about what they're using                                 what they're spending where they think                                 they're going with their infrastructure                                 one of the nice side effects of this is                                 that now that we can quantify how an                                 individual team is behaving so for                                 example to contrast our pipeline team                                 with our analytics engine team                                 for our recommendation team when one                                 team has a key breakthrough in terms of                                 how they're operating infrastructure                                 they have a quantifiable thing that they                                 can look at and they can say for example                                 we started using spa and by using spot                                 instances we were able to reduce our                                 cost by                                                                 becomes a bit competitive with other                                 teams they see that benefit they see                                 that measured impact and they step back                                 and say okay how could we implement                                 something similar on our side and                                 overall everybody benefits once you've                                 got sort of measured consumption of                                 resources in infrastructure so stage                                   is what we typically call in our little                                 corner of the industry financial                                 leverage and I won't spend too much time                                 digging into this but effectively                                 financial leverage in terms of public                                 cloud amounts to if you make a                                 commitment to spend a certain amount of                                 dollars you can get discounted pricing                                 and in the Amazon world this is known as                                 our eyes or reserved instances that's a                                 bit of a misnomer these days the                                 industry is quickly converging away from                                 the notion of I want to have this kind                                 of pricing for a single host and what's                                 actually happening now is that people                                 are adding up base compute units so you                                 could look at this for example and say                                 of all the ec                                                           a thousand CPU cores that are part of                                 that infrastructure and of that thousand                                 CPU cores if I take a probabilistic                                 model around how our resource                                 consumption is changing over time I can                                 actually project that I think I'm going                                 to end up keeping say                                             thousand so I want to cover                                              discounted pricing by making a                                 commitment in our case we were able to                                 leverage our own tool drink our own                                 champagne effectively to attain about                                    percent RI coverage for specific                                 workloads and for each of those                                 workloads immediately out of the game we                                 get a                                                                 effort we're driving our price down by                                                                                                     think we're going to continue to use                                 over time but the nice thing is recently                                 we've been able to step away from is                                 this server going to step around and                                 look at in aggregate what are the                                 compute units we're using how many of                                 those compute units do we think we're                                 going to keep overtime and divorce stops                                 the idea of specific hosts to other                                 things all throughout there real quickly                                 sorry what I'm saying I'll throw out                                 there real quickly something a lot of                                 people overlook in terms of Finance                                 leverage is the secondary market it's                                 one of my favorite things about me to be                                 West this notion that I can go buy a                                 reserved instance from AWS without that                                 extended duration commitment so to put                                 an example on this this morning I ended                                 up buying about                                                       with just a                                               for one of my teams                                                   small and I'm comfortable making that                                 commitment it's a lot different than                                 making a                                                            reserved instances and I was able to                                 save them I think was $                                                  involvement on my time so about five                                 minutes of my time saved that particular                                 team $                                                              extent of the journey the first two                                 stages there's a deliberate dividing                                 line between the first two stages and                                 the reason for that is we can attain the                                 first two stages in terms of visibility                                 and financial leverage without really                                 involving the engineering teams we don't                                 need an engineering team to come in and                                 write code we don't need to change our                                 operational policies per se tagging is                                 quite straightforward once you get it                                 based into like a chef infrastructure or                                 into some basic AWS configurations but                                 to really continue to do cloud well                                 effectively to obtain more cloud                                 efficiency now we have to start looking                                 at changing the ways that we actually                                 operate the infrastructure and getting                                 the engineering team involved and that's                                 about stage                                                            be about so as we dig into stage                                       we changed our operational practices                                 we're going to look at a handful of                                 specific initiatives that we kicked off                                 that allowed us to continue to extract                                 more value out of clouds continue to                                 have more efficiency excuse me so                                 initiative number one we effectively                                 when we kicked off this broader effort                                 we step back and we looked at different                                 workloads across our environment and                                 decided where we wanted to target some                                 specific places that we weren't happy                                 with whether that was how much those                                 areas are costing us keep in mind now we                                 have good visibility into which pieces                                 of the architecture are costing us money                                 and we can put pointed efforts behind                                 reducing spend in certain areas or if we                                 had certain areas where we weren't happy                                 with the architecture we could step back                                 and say how do we make this more cloud                                 native how do we use best practices of                                 cloud as opposed to what we've been                                 doing for the last four or five years                                 and so the first one of those was                                 effectively re architecting our data                                 pipeline so two years ago we had                                 something that looked similar                                 to this we had a data ingestion set of                                 servers that would run some jobs on a                                 periodic basis they would integrate with                                 AWS they would pull down cloud watch                                 metrics they would pull those in they                                 would write them to a react times                                 basically a time series schema on top of                                 a react cluster at its peak that was                                 upwards of                                                            running some pretty significant                                 infrastructure at the end of the at the                                 end of its life is running on larger i                                  infrastructure                                 once the time series dated and written                                 to react we put a message into rescue we                                 didn't have a different set of hosts                                 that would pick up that message out of                                 rescue then go read the data back out of                                 react start to process it and ultimately                                 write it into a my sequel data store                                 there are a number of problems with this                                 architecture for us first one being that                                 a lot of the data ingestion side of                                 things and a lot of the rescued                                 consumers were frequently idle they                                 would be they would wake up they would                                 do some work for maybe                                                then they would basically go back to                                 sleep and nothing would happen for                                     of the time on these hosts so we had                                 sort of burst scale that we had to deal                                 with but the way that we were operating                                 at the time we weren't scaling up or                                 down we just had a fleet of machines                                 that would wake up grab some data into a                                 database and then go back to sleep                                 and so what we ended up changing that                                 architecture to was to be significantly                                 more cloud native so now the way this                                 looks today is we have lambda functions                                 that run on a cron schedule so there's                                 no sort of self scheduling that's done                                 as an independent function those lambda                                 functions have a tiny bit of state that                                 they need to maintain which they write                                 at dynamodb just so we know when certain                                 fetches are happening etc they now write                                 messages into SNS topics or SNS queues                                 as opposed to rescue which is sort of                                 another facility that AWS provides for                                 us and then the consumers of that are                                 effectively operated on top of                                 auto-scaling groups those auto-scaling                                 groups are backed by spot instances as                                 opposed to long-running durable ec                                  hosts that we have the reason about are                                 we're going to keep them around forever                                 in this case this workload spins up in                                 result to a mess or in response to a                                 message being in EQ processes and data                                 and then effectively those hosts go away                                 because we're doing this on spot we save                                 we end up spending about                                                 what the on-demand rate is for a similar                                 workload likewise we got rid of the time                                 series database altogether                                 so the                                 consumers that are reading from cloud                                 water effectively buffering data they'll                                 then write that into s                                                   nice facilities of s                                                   finish writing a file you can have AWS                                 subsequently emit an SNS notification so                                 different message goes out into a                                 different SMS queue and then we have a                                 second set of consumers that are running                                 on ECS in the form of docker containers                                 they will read that message out of queue                                 process the file write a different file                                 to an s                                                              being a park a file format which we then                                 push further downstream so a couple of                                 benefits of this one it scales                                 elastically so as there's more work load                                 things scale up they do the work and                                 then the host literally go it they're                                 not left standing around - we eliminate                                 an entire database so we were able to                                 get rid of about                                                       our infrastructure which ended up                                 netting out looking similar to this so                                 we were able to remove a significant                                 chunk of our infrastructure that                                 basically was a large not invented here                                 syndrome relic that being our react                                 cluster was something weren't good at                                 operating and something we didn't really                                 know very well and we just picked off                                 the shelf because we felt a little bit                                 comfortable with it but it wasn't really                                 our core competency our monthly spend                                 were able to recover over ten thousand                                 euro just by making this single                                 architectural change and then there's a                                 significant reduction in operational                                 complexity so we're no longer operating                                 a database we're no longer operating                                    different nodes to implement this react                                 cluster we fell back to sort of first                                 principles of moving around files simple                                 events processing those files                                 elastically and throwing away of                                 instruction when we were done with it                                 you can see that's a little bit hard to                                 read but this is kind of a monthly spend                                 for this particular team again remember                                 we have visibility into what these                                 individual teams are doing because we've                                 taken the time to tag things and the                                 monthly spent for this individual team                                 was on a significant downward trajectory                                 the slight uptick at the end of it was                                 when we adjusted data for one of AWS is                                 largest customers we brought them on                                 board and so there's a slight uptick but                                 we were able to scale that dynamically                                 without changing any infrastructure our                                 react lustre was no longer on fire and                                 we were able to take that into stride as                                 some perspective this particular                                 infrastructure right now for us on                                 average process is about                                                of data per second although that's a                                 very first you workload it's not                                 constantly                                 being at that so it's more like there'll                                 be a million pieces of data the                                 commander it'll process that throwaways                                 from the construction of                                            pieces of data that will come in or                                 process it will throw a single                                 construction                                 how does fits into the overall                                 perspective for this individual team                                 this is a breakdown of many people think                                 that when you run in ec                                              single line item of cost that it's one                                 particular piece of data that's not                                 actually the case there's a lot of                                 nuance that goes into what it means to                                 operate in piece of ec                                                 and in this particular case that the                                 purple zoom in is us no longer operating                                 significant EBS volumes that were                                 previously attached to that react                                 cluster so as some context there we had                                 effectively had to use large EBS volumes                                 though to get the throughput that we                                 needed out of that react cluster and                                 that was a significant portion of our c                                  span that we were able to just basically                                 take off the table by simply using s                                  when we didn't need more complicated                                 data store so the the second sort of use                                 case in terms of our operational                                 efficiency here was a migration of a                                 seven terabyte seven node my sequel data                                 warehouse cluster this was effectively                                 mandated by the fact that we were                                 running RDS for my sequel and RDS my                                 sequel RDS has a hard limit of about                                 seven terabytes so we were basically                                 running out of the ability to add more                                 data to this particular database and the                                 natural migration for this in our case                                 was to move to Amazon Aurora this is a                                 good example in our environment of using                                 platform native technologies as opposed                                 to trying to operate something                                 themselves and so the high level                                 takeaway of what we gain by this was our                                                                                                    queries on RDS my sequel was around                                     seconds keep in mind these numbers are                                 not stunning in terms of the                                      percentile latency but this was a data                                 warehouse right so these were all flying                                 batch jobs were effectively being run to                                 be normalized data that would then be                                 written out to a more efficient store to                                 hand off to customers by virtue of                                 moving just to Aurora so this was almost                                 no code change I'll say like maybe a                                 couple lines of code and maybe some                                 configuration changes by virtually                                 moving to Aurora we were able to get                                 United                                                                 to                                     any seconds there's an ocean in terms of                                 RDS my sequel of provision i--once or TI                                 offs we're able to go from having to pay                                 AWS for                                                                we're doesn't have an ocean of PIOs                                 and we didn't need that for a capacity                                 with aurora fun fact the side note AWS                                 will allow you to purchase                                             for RDS y sequel and in reality my                                 sequel engine can only use                                           those so you can basically buy more than                                 you even theoretically use with my                                 sequel engine on RDS and then an                                 unintended side effect that was the                                 results of doing his work was that our                                 provision time for an individual replica                                 went from                                                                seven minutes with aurora this is if                                 you're not familiar with Aurora                                 effectively the secret sauce behind it                                 is a distributed file system that is                                 eventually consistent based on a log                                 structure merge approach and the aurora                                 engines themselves are just cpu and                                 memory on top of that distributed file                                 system so spinning up a new replica is                                 really just copying around cache for the                                 new nodes in terms of the read replicas                                 what this meant for us is that we could                                 actually take this data warehouse and be                                 elastic with it which is something we                                 didn't anticipate out of the gate but it                                 was a really cool side effect of                                 adopting this platform native technology                                 so an example that would be if we had a                                 large batch job that had to hammer the                                 data warehouse that normally would have                                 taken seven hours on my sequel RDS we                                 could spin up an Aurora node we could                                 run through the large batch job that                                 would take about                                                        we could throw that node away for four                                 or five hours because we could stand up                                 a replicas so quick our DML or our data                                 manipulation replication time went from                                 the worst case there's our                                      percentile replication lag went from                                 seven minutes to                                                       is very efficient in terms of right                                 replication and our failure mode                                 operations got significantly better                                 so Aurora does a lot of management of                                 partition detection of election of                                 master replicas failover things like                                 that that we had to do manually in a                                 RDS my sequel universe and if you zoom                                 out a little bit you look at what does                                 it actually cost us at the end of the                                 day we ended up spending about                                       about                                                                Aurora cluster in terms of dollar for                                 dollar compared                                 as compared to RDS my sequel the next                                 use case here is a migration of our                                 HBase cluster from I to to i                                           families and I called this this                                 particular story for us platform                                 adaptation and really what I mean by                                 that is because we're paying attention                                 to the changes that Amazon is making to                                 their infrastructure we're looking at                                 the innovation they're doing and we're                                 taking advantage of that in a dynamic                                 adaptive way we're able to migrate a                                 specific workload from I - family - I                                 three family one of the reasons that was                                 compelling so it's a bit hard to see but                                 this is effectively a bonny output for                                 the local SSDs on the ITU's in the i                                    and there's two things that I'll call                                 out the first is that the random                                 sequence II improved by an order of                                 magnitude between I - and the i                                         likewise then the the raw numbers                                 improved from especially in the read                                 side we got more than a                                                from                                                                megabits per second this is simply by                                 migrating a workload or right sizing a                                 workload from an i - to an i                                             at a higher level the hardware is fairly                                 equivalent so same number of V CPUs same                                 amount of RAM the local stores you                                 actually get more storage but more                                 importantly the network interconnect on                                 the i                                                               gigabit per second interconnect by                                 moving from i                                                           us is that we can move away from a thing                                 called placement groups in terms of ec                                  which meant that everything was                                 operating in a single AC and we can move                                 to having a multiple AV deployment                                 because now the                                                         removed network bandwidth as a                                 constraint for us and then really the so                                 there's a couple other good side effects                                 here I'll skip over the rest of them but                                 one of the cooler things is that we were                                 able to do this but also reduce cost at                                 the same time so however they managed to                                 do it Amazon has been able to engineer                                 differences between the i                                             Hardware family and reducing price by                                 over $                                                                pretty quickly because we're running                                 short on time but we also undertook a                                 significant containerization effort and                                 effectively everything that's playing                                 the containerization side a couple of                                 highlights for us really the reason we                                 set out with this particular container                                  is a shoe initiative is because we have                                  poor utilization on a lot of our PC to                                  hosts so if you go back to that notion                                  of we had one workload per VM that would                                  wake up do a little bit and then go back                                  to sleep that was largely idle for                                      of the time by virtue of adopting                                  containers were able to move to a bin                                  packing strategy where we would layer                                  workloads on top of each other get                                  effectively upwards of                                                  of those CPUs for example on the ec                                   hosts that we're offering the containers                                  which on the whole gave us a utilization                                  improvement of over                                                     the hosts that were backing those                                  containers and by virtue of turning off                                  the things that we're periodically doing                                  work but not up the full time that we're                                  not fully utilized right we were                                  actually able to save                                                  the ec                                                           particular workload this also got us                                  some cool side effects in terms of being                                  able to do things like blue green                                  deploys through our Jenkins deployment                                  right now for a new deployment                                  well ship a new container out we'll                                  switch back and forth between blue green                                  deploys and our jenkins infrastructure                                  is actually capable of changing an al                                  beer and application load balancer from                                  the blue deployment to the green                                  deployment we can do things like                                  automated smoke tests between them we                                  have a very clear rollback path again                                  all leveraging platform native                                  capabilities we made significant use of                                  ec                                                                      workloads so things like time                                  insensitive Map Reduce drops or EMR jobs                                  I say time insensitive EMR drops because                                  if you're going to run an EMR job on                                  spot you have to be prepared that it's                                  going to go away that's kind of the                                  caveat of spot and if you don't engineer                                  accordingly or schedule your workloads                                  accordingly then you'll get bitten by                                  spots but has a ton of cost-saving                                  potential but at the end of the day if                                  you don't take certain concessions into                                  account it's ultimately not going to be                                  more efficient for you you're not going                                  to get things done on it that you need                                  to get done the net takeaway for us is                                  that by moving certain workloads to spot                                  we have able to save over                                              per month in terms of what we would be                                  spending it on demand but again this                                  requires engineering effort this isn't                                  something we just get for free by                                  flipping a switch and saying yes you                                  spot in this case spots a lot more                                  complex than that                                  there's                                  many many nuances to doing spot well and                                  I won't have time to go through all of                                  them here but effectively we ended up                                  taking a machine learning approach to                                  how we create predictive models around                                  spot and then trying to figure out when                                  does it make sense to use spot given                                  those machine learning models and where                                  we think the spot market is going what                                  we think the probability of the                                  likelihood of termination of a spot                                  instances and this is something that we                                  effectively had to build in-house                                  because we weren't happy with the AWS                                  tools we didn't get everything we needed                                  out of spot lock and spot sweep for                                  example and then one thing a quickly                                  point out most people overlook the                                  fundamental basics of spot in the sense                                  that there's a specific case where you                                  shouldn't use spot which is if you have                                  unused our eyes so if you pay for a                                  reserved instance and you can create a                                  probabilistic model around the fact that                                  you're not going to use that reserved                                  instance then spending money on spot is                                  actually going to cost you more in the                                  long run because you should have used                                  the unused reserved instance meaning an                                  RI is it's not cost are going to pay for                                  it you should use that before you should                                  use spot but modeling that is a                                  complicated thing it's not always                                  trivial to figure out should I use and                                  you see to instance to take advantage of                                  an unused RI or not and then lastly                                  there's an important notion when you                                  operate a SAS company so I live it to                                  this at the beginning of the talk of                                  what we call gross margin so as we sell                                  more dollars of software to our                                  customers there's a nonzero costs to                                  operating our infrastructure and the                                  Delta between that cost or cogs cost of                                  goods sold and top-line revenue is what                                  we call gross margin so if we build up                                  everything we've talked to so far in                                  terms of visibility into the spend in                                  terms of financial leverage in terms of                                  optimizing our infrastructure that has                                  some really cool side effects for us we                                  can now start to do things like look at                                  what it cost us to ship an individual                                  feature look at what it cost us to                                  operate a specific customer or operate                                  our infrastructure for a specific                                  customer and this is what I mean when I                                  say we have visibility into unit                                  economics meaning I can go to our                                  product team and say this is what it                                  costs to operate our infrastructure for                                  this customer so what should we be                                  charging that customer what's the margin                                  that we want to make on this particular                                  feature set this lets us make better                                  engineering decisions because we can                                  focus on core competencies and value add                                  as opposed to operating infrastructure                                  we can figure out where we should focus                                  optimization efforts with                                  in our infrastructure we wouldn't be                                  able to do this if we weren't                                  experienced in doing it already                                  and if we didn't have good visualization                                  and good financial leverage and how we                                  operate our infrastructure this is                                  really sort of the culmination of all                                  the three previous steps there's a thing                                  in the states that not all not                                  everything was roses meaning not                                  everything went great there were a ton                                  of lessons learned doing this like I                                  said the rate of change in AWS is                                  staggering definitely need to be                                  cautious with spending commitments you                                  can certainly over by our eyes if we're                                  going to make significant changes to                                  your infrastructure that's a things that                                  you have to be careful with the spot is                                  definitely its own special snowflake and                                  if you do it wrong it can end up costing                                  me more money at the end of the day um a                                  couple other cool examples that I won't                                  have time to go into and then lastly                                  I'll mention there is one downside                                  theoretically to leaning into these                                  platforms heavily to making more use of                                  the native teachers which is vendor                                  lock-in all right so if I make use of                                  Aurora and there's not an Aurora analog                                  on one of the other platforms what does                                  that mean if I decide I want to leave                                  AWS this isn't something that concerned                                  us but it is something that's probably a                                  concern for several people so if I can                                  leave it with one thought it would be                                  that while we got good at operating data                                  centers and there were certain best                                  practices there that is not what it                                  looks like to effectively do cloud well                                  there's a lot of things we have to                                  change as cloud evolved as the platform                                  offers new features to us and if we lean                                  into those if we do those well and                                  become good effective operators of                                  public cloud then we can save a                                  significant amount of money in our case                                  we're able to cut our infrastructure                                  cost by over                                                       million dollars a year I love that                                  because then I can go back to my boss                                  and make in case I get to hire more                                  engineers and this is a good feedback                                  cycle in terms of our product and and                                  how we set our own experiences back into                                  our product so thanks                                  [Music]                                  [Applause]                                  thank you very much Eric                                  we actually are already on time and as                                  the barbecue starting to downstairs and                                  you want to collect your voters for food                                  because there are vultures but before we                                  go to barbecue take your just downstairs                                  as well so thank you very much Erica but                                  let's take the questions down there and                                  you'll be more efficient with the beauty                                  always better right thank you much                                  [Applause]
YouTube URL: https://www.youtube.com/watch?v=xsu2axSQXgg


