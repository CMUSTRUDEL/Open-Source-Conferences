Title: Berlin Buzzwords 2017: Stephan Ewen - Experiences running Flink at Very Large Scale #bbuzz
Publication date: 2017-06-15
Playlist: Berlin Buzzwords 2017
Description: 
	This talk shares experiences from deploying and tuning Flink steam processing applications for very large scale. We share lessons learned from users, contributors, and our own experiments about running demanding streaming jobs at scale. 

The talk will explain what aspects currently render a job as particularly demanding, show how to configure and tune a large scale Flink job, and outline what the Flink community is working on to make the out-of-the-box for experience as smooth as possible. We will, for example, dive into - analyzing and tuning checkpointing - selecting and configuring state backends - understanding common bottlenecks - understanding and configuring network parameters.

Read more:
https://2017.berlinbuzzwords.de/17/session/experiences-running-flink-very-large-scale

About Stephan Ewen:
https://2017.berlinbuzzwords.de/users/stephan-ewen

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              one is kind of how they started viewing                               Apache chief link and the second one                               like what what were the things they                               figured out are important to pay                               attention to when you operate it at                               large scale so I'm some of the use cases                               that that these lessons learned are                               taking from here here are just a few                               that that I mentioned there they're more                                for which we've gathered that but here                                just a few from from which explicitly                                some of these lessons learned are taking                                so first one is we're working after a                                while now with with Netflix to establish                                infrastructure for for various use cases                                amongst that streaming streaming                                ingestion and modeling of user                                interaction sessions the interesting                                takeaway for us from that is both both                                in the in the way we've learned about                                what artifacts are what problems occur                                at scale especially because they're                                running everything on Amazon and the                                peculiarities of like the Amazon                                container engines and the and the                                behavior of s                                                    because of the the types of jobs they're                                running which are an interesting mix of                                stateless jobs jobs with small States                                and jobs with very very large estate I'm                                just to give you an idea of what okay                                this looks it looks much better                                originally this is I think gets lost                                somewhere in the video cable like the                                the scale that this is that this is                                running is is quite quite significant                                it's running like link is running in in                                over                                                                with                                                            hundreds of streams another of the use                                case is that that some of these lessons                                learned here are are taking from is is                                our collaboration with Alibaba Alibaba                                has a system called blink which is based                                on flink it's if you wish an adoption of                                link with with which is like more                                integrated with their with the way their                                yarn cluster is set up and so on and the                                way they were                                you know the integration with their you                                know operating metric system and all of                                that it's it's been when it started out                                a fairly significant difference from                                flink but now it's a very small                                difference from language sort of works                                very hard together with them to kind of                                merge merge most of the two systems so                                they're they're very similar these state                                today                                and yeah the the scale that they're                                running this on is probably even even                                even larger so they're they're single                                fling jobs running on more than                                thousands of notes with tens of                                terabytes of like in in process state                                and one of the really cool things that                                this thing thing does is when when                                there's this crazy shopping holiday in                                China called the singles day which is                                roughly like the Chinese equivalent of                                the Black Friday but in the US and the                                the real time like the real time search                                optimization to figure out which which                                products should be you know ranked up                                and down and so on depending on on                                trends and so on sexually running on                                this on this stream computation system                                which is which is pretty significant and                                and one other use case ad that we've                                taken a few interesting lessons learned                                from especially for the first section                                like how do we use path link these days                                arm is from um from that use case it's a                                social network called Drive trap but the                                other guys that used to do top gear now                                do the Grand Tour and they their team                                has obviously not the three themselves                                but they're like the Technic technical                                team has actually implemented the social                                network if you wish almost completely on                                top of stream processing so everything                                comes enters every user interaction                                comes is locked into in this case Kafka                                and then you have a stream processor                                that consumes this lock of actions and                                computes the view of the world as it's a                                to be presented to all the users on the                                website and then it's mirrored out to                                elasticsearch and readies to be actually                                 served by the by the read layer so this                                 this doesn't run it quite the scale as                                 the others do but it's kind of it's an                                 interesting complexity in itself because                                 it's almost the entire application of                                 that social network that runs in the                                 stream processor not just you know like                                 individual parts that support it and                                 like I'd like to start with the first                                 part like what have we learned about how                                 users actually view flink that was there                                 was kind of an interesting process                                 process for us because you know we                                 always try to explain what what flink is                                 to users and once in a while we actually                                 try to listen and okay if you explain                                 flank to us now how would you actually                                 describe it it was very interesting                                 maybe some of them describe it                                 completely different than we would                                 describe it and here's something we we                                 learned that's actually interesting                                 so many of us still does okay we we've                                 you've linked these days actually as a                                 as a system for you know pretty generic                                 stateful event-driven processing meaning                                 you have a system that that that you                                 take is you take as a building block for                                 implementing stateful services stateful                                 micro services it reacts to events to                                 calls it has it has application state                                 that it maintains and it triggers other                                 actions and it's it's responsible for                                 maintaining the consistency and                                 durability in the persistence of all                                 that that application state incidentally                                 without a database that's that would be                                 the classical way to do it                                 so it's kind of a framework that kind of                                 you goes more to the left side a                                 framework for event-driven applications                                 to build them based on on patterns like                                 event sourcing or arm command query                                 responsibility segregation then there's                                 of course users that say yeah so it's a                                 stream processing framework you know you                                 have your streams of events and you                                 compute over the streams of events you                                 do windows session windows tumbling                                 windows and all of that and of course a                                 lot of users still feud as a batch                                 processing framework so we've kind of as                                 kind of organized this like this view                                 that we learned about it as this it's                                 kind of different different crowds I                                 would say that the users come from that                                 describe it in these different angles                                 the crowd that describes it as                                 event-driven applications is usually                                 exactly armed teams that that build                                 applications the you know the front                                 front and back current of the life of                                 the live services of teams versus the                                 other the other teams are often like the                                 data teams and the companies that say                                 you know for us it's a stream processor                                 or a batch processor but the kind of                                 common substrate is that it's a system                                 that's built actually on stateful event                                 processing and the the most interesting                                 description I've heard how how users                                 view flink is actually it's a system for                                 actually realizing the the need memory                                 image model for replications and                                 persistence so what does that mean um if                                 you want to if you want to build a                                 distributed stateful application                                 um this is a very interesting pattern to                                 do it um based on based on the idea of                                 event sourcing every time you do it you                                 want to do an interaction with that with                                 that application you create an event                                 that describes the application you put                                 it into a log that where it is                                 persistent and then you let actually the                                 application are process it and yep and                                 then the application becomes very very                                 simple it's just it's just a process                                 that consumes an event and update some                                 arbitrary memory data structures you                                 know like it would be a standalone                                 process nothing nothing more there's no                                 there's no abstraction that makes                                 actually these data structures in                                 hindsight go against you know like a                                 database fire and or EMM framework or                                 whatever it's really just like you would                                 have a new standalone java application                                 and then periodically blink actually                                 takes your snapshot of that of that                                 thing that's like the the memory image                                 and uses that for persistence and and                                 when a failure happens what it does is                                 it restores you the memory image and                                 replace you all the events that have                                 happened since the since net failure                                 just just viewed like that there's                                 actually no there's nothing that's                                 particular to stream processing here                                 it's just a mechanism to actually have                                 processes with state that run and you                                 make them you make them recoverable or                                 you make them you make them for tolerant                                 in a very efficient way because while                                 you're actually processing with and                                 interacting with these local data                                 structures you worry about nothing right                                 there's there's no additional cost that                                 you pay for any persistence and so on                                 this just like once in a while of Ekron                                 process that does you a snapshot                                 persistence of that memory image and and                                 then blink would actually be if you wish                                 a distributed instantiation of that                                 system so you have events coming in and                                 you have a lot of individual processes                                 that just consume the events and                                 manipulate their internal state and do                                 something and I'm flinging the system                                 that kind of keeps this whole thing for                                 tolerant and and consistent in case of                                 in case of failures so there were some                                 there is a very interesting interesting                                 lesson for me that is how how we found                                 quite a few users actually describing                                 back the system to us and these all                                 terms like event sourcing and and memory                                 image actually terms that have been have                                 been coined before by um you know by by                                 by people that work in the in the space                                 of application engineering the                                 come up with design patterns Darren's on                                 and I just turned out this fits the                                 whole idea of links so well so in the oh                                 this looks horrible                                 this should be this should be gloriously                                 yellow glowing bubbles over there just                                 just picture them as such so the the                                 whole way that then fling operates in a                                 distributed fashion is actually you know                                 you have this do you have these                                 connected pieces of distributed                                 communication attached with some                                 embedded state that for all means to the                                 application programmer actually reacts                                 as as local state at memory speed and                                 you just have have events that flow                                 through that that manipulate this state                                 and someone to the wires link scans over                                 the whole thing and take city snapshot                                 and make sure everything is everything                                 is consistent and because there's such                                 this in flink is designed to have kind                                 of a loose coupling between those                                 between those things you can actually um                                 you can restore or you can restore um                                 the state of these computations from the                                 snapshot or you can actually alter the                                 computation or the the whole whole                                 structure of how how computation depends                                 on each other and still restores because                                 there's like a loose coupling between um                                 this state and the computation and it's                                 every time you restore it um it's kind                                 of it's a matching procedure that                                 matches the state into the computation                                 so it's kind of a an interesting                                 application building framework in some                                 sense based on yeah based on the idea of                                 it has so many names event sourcing                                 reactive programming but but in essence                                 together with local state for                                 computation and and snapshots so another                                 way to view this would be actually to                                 say if the if the classical way of                                 building building applications is you                                 know you have a you have a layer of                                 compute and you have a layer of a                                 database for persistence what's what's                                 link really gives you is an architecture                                 to to change this into you do you do                                 require persistent storage for streams                                 something like like a log something like                                 Kafka so but then it gives you a very                                 good building block to actually say                                 transform this to an architecture where                                 application state is                                 that is completely completely local and                                 it just interacts with our with with                                 storage in a like snap shot persistence                                 mechanism alright so what this will                                 actually this may actually now create                                 create some some some confusion in in                                 the mind of trying to okay piece that                                 together okay if it's that building                                 block you know it's probably not not                                 very hard to imagine how such a building                                 block is a really nice building block                                 for screen processing because stream                                 processing is really exactly there to                                 take events and you you try to put them                                 into perspective of each other with the                                 help of some local state and this local                                 state you know the simplest case is just                                 a counter per window right or it's a                                 it's the state of a session or anything                                 right so this actually gives you arm                                 gives you the interesting building                                 blocks just from the runtime there's an                                 there's an aspect in here that I haven't                                 actually talked about much and I don't                                 have the time today to talk about that                                 but this is the the handling of time                                 that we built into flink which is kind                                 of the second ingredient that that then                                 helps it to make make it a really really                                 good match for stream processing                                 primitives to track progress in event                                 time completeness of data and so on so                                 because it it seems like this is a very                                 fairly primitive set of or a fairly                                 basic and general set of building                                 primitives that is very powerful for                                 these different use cases things                                 actually come now come up with a                                 different a different set of layers so                                 there's also something we interestingly                                 learned from from users how they if they                                 if they look at filling today how they                                 how they want to approach it and um the                                 the latest set of of users actually have                                 this interesting transition how they go                                 from from coarse-grained to ever more                                 fine-grained levels of abstraction so                                 the the most the most basic primitives                                 is on the on the bottom layer it really                                 gives you just this events times and                                 snapshots and so on slightly more more                                 high-level is the data streams API which                                 gives you the DSL for stream processing                                 and batch processing and then on top of                                 this um primitives for our API is for um                                 for streaming sequel the table API is                                 kind of a an embedded DSL that is that                                 is roughly the same as as SQL and you                                 can you can kind of navigate this this                                 layers of abstraction                                 on what you want to do if you want to if                                 you want to just build an application                                 that generates some insights from this                                 from a stream of events you may start on                                 that level if you then want something                                 slightly more complex that you know that                                 has a very custom way of doing of                                 deriving deriving you know statistics                                 over over windows of state over time                                 then you go a level below and if you                                 actually want to build really just a                                 custom application that that reacts to                                 that reacts to events then then you go                                 even one level below and the interesting                                 thing is you can mix all these levels in                                 the same application and and we do                                 actually see that as as progress as                                 projects go on and go on and go on and                                 they try to that they come to the point                                 that they say okay we've actually built                                 this now can we actually even reflect                                 that characteristic in the real in the                                 real time program can we reflect that                                 more and can we reflect that other                                 characteristic they start actually                                 moving down that stack bit by bit by bit                                 so we've actually seen a lot of folks                                 that that actually moved from the data                                 stream API from from how you define                                 Windows they're just to the to the layer                                 one below where they say okay just give                                 me are betrayed state give me some                                 timers make sure it's consistent and                                 I'll just I just wired together whatever                                 I want um don't don't dictate me any API                                 also just give me computing state and                                 I'll do the rest                                 all right so that that is kind of a how                                 power of your flink can evolved over                                 time with with what we what we saw how                                 how our users are using it                                 there's also another part of it that                                 where we learned kind of how it behaves                                 in an operational context that are that                                 I'd like to share and I'd like to start                                 actually with a with a very neat arc                                 inside and that is that the event stream                                 pipeline in general is a it's something                                 that works so if you just piece together                                 the streaming and your compute and so on                                 this is this kind of it's a very robust                                 thing there's not a lot of like very                                 complicated memory management or here                                 you're there are parameters that you                                 need to chew in here and they're                                 involved that ports just works there are                                 of course things that that that that                                 come up again and again and here they're                                 if you like more more general insights                                 and then there's one that I would like                                 to actually spend some                                 I'm on iterating so their view obvious                                 things like okay dependency conflicts                                 are probably the thing that I have the                                 feeling that users spent most time on                                 their way to production solving                                 dependency conflicts these days or rough                                 edges around the deployment ecosystem                                 because it's really a it's really a                                 crazy space everything behaves and                                 evolves kind of in a different way yarn                                 masons doctor all the security                                 infrastructure there's this fancy idea                                 of overlay networks in container engines                                 which is conceptually really nice thing                                 but messes up everything for the systems                                 that actually run inside them and have                                 to do service discovery now all of a                                 sudden very different so these are these                                 are things that are like obviously on                                 the in the space around around flink                                 that um that have to be paid attention                                 on and not another thing that we kind of                                 realized and where we where we plan to                                 invest some work in is that we we've                                 seen that the dependency on any external                                 system eventually causes avoidable                                 downtime if you wish so the snapshots                                 that that flink takes at the moment have                                 to go somewhere in most cases it's HDFS                                 s                                                                     would actually think that most companies                                 have two HDFS cluster well under control                                 or s                                                                available but apparently that's not                                 always the case so the I don't know if                                 you remember this a few weeks ago when                                 s                                                                      stopped working that's also when we got                                 this flood of emails like okay all our                                 fling clusters kind of on Amazon you                                 know they fail to checkpoint obviously                                 because there's s                                                      there's very interested this there's a                                 few things that we can actually do so                                 we've kind of changed I think the                                 mindset since then a little bit ends and                                 under thought okay let's actually try                                 and really make it as independent as                                 possible in the future of all of these                                 dependencies while they you know exploit                                 them while they work but don't don't be                                 blocked by them for the times that they                                 don't work um perhaps your realization                                 sounded like something that we would                                 almost consider as a soft problem when                                 we're still doing that same processing                                 in the early days or more even before                                 that batch processing but it in                                 streaming the whole the who is you                                 lies a can of worms has actually been                                 opened again and and come back as a as a                                 very different beast first of all                                 streaming has different patterns with                                 which it interacts with data structures                                 which make industrialization some of the                                 tricks that that we added to flink to                                 make serialization cheap and batch not                                 quite as efficient but even even more                                 important thing is that all of a sudden                                 remember this figure from few slides                                 back where it said that flink actually                                 start owning start holding the state the                                 compute end the stay together and not                                 the database anymore which actually                                 means that you would you want to start                                 doing the same things with the                                 statements linked as you used to do with                                 the database and that means you would                                 actually want to like version it evolve                                 it over time that means evolving the                                 implementation of your sterilizers or                                 fling serializers but also you know just                                 regular schema evolution you have you                                 started out storing you know session                                 with a certain amount of information                                 about users you want to drop some and                                 add some other it changes the shape of                                 the state it changes if you wish the um                                 it changes the classes that you use in                                 your in your variables in the memory in                                 the memory image right so all of a                                 sudden um what we need there is a way to                                 allow users to kind of in hindsight fix                                 that and correct that because most of                                 them just start out programming and                                 saying you know I'm such a nice easy                                 thing I'll just program and then a few                                 months later they figure out okay damn I                                 actually overlooked something so now I                                 have to kind of evolve everything in                                 this in this memory image and put it                                 into a newer form so this is something                                 where we started working on and putting                                 a lot more effort and kind of you can                                 think of it as a schema evolution of                                 application internal state um so much                                 for the for the kind of more more                                 high-level other the lessons learned                                 that I want to touch only lightly upon                                 the one that we want to go into a little                                 more detail is actually the fact that                                 robustly checkpointing is if you wish                                 then the most important part of running                                 a large-scale a large-scale flink                                 application and i'd like to kind of go                                 into this and and if if you give a few                                 pointers like how do you do get some                                 insights whether whether these things                                 are all going you know Petrey and easy                                 or if something if something causes                                 checkpoints to take way longer than you                                 think or cost way more than you would                                 would imagine that would cost how do you                                 actually get at the bottom of that what                                 is usually the course and what are kind                                 of ongoing ongoing trends in order to                                 like to keep continually improving on                                 that so again um so the the basic                                 mechanism of checkpoint against link is                                 is the following assuming we have a lock                                 of input events at some point in time                                 the system triggers a checkpoint and                                 that that is marked by the source tasks                                 of the streaming computation injecting a                                 checkpoint area and that checkpoint                                 barrier flows through and whenever it                                 reaches a stateful operation it marks                                 that it marks the alignment point to the                                 updates to the state data structures at                                 which point that particular snapshot has                                 to be taken and where you have to                                 actually set the metadata to so you know                                 that you know you can recover from                                 exactly at that point so it's injecting                                 these barriers then flowing through the                                 stream triggering the snapshots so that                                 sounds that sounds quite easy there's                                 one little nifty detail in all of that                                 and that is in order to get proper                                 exactly one semantics there's there's a                                 step that that I think many users are                                 not not really too too aware of just                                 because you know you rarely see it                                 happening when you actually run it and                                 that is the that is the alignment phase                                 so think of it as as the following when                                 when you have different pieces of                                 computation running around and at some                                 point you want to establish a                                 synchronous point between them you can                                 you can think of that as a point where                                 you have to do a little bit of you know                                 like bookkeeping and making sure that I                                 say in the database space these                                 transactions would have to still be                                 accounted for here while these shouldn't                                 be accounted for you know you have to                                 kind of establish that level and say now                                 I have a now I have a consistent view                                 over all the different states and of the                                 computation and I can I can use this as                                 a as a as a consistent snapshot across                                 everything and that is kind of the                                 equivalent to that if you if you log get                                 distributed um distributed transactions                                 would be the alignment phase and sling                                 snap                                 right so there's this checkpoint                                 barriers coming from various streams and                                 the operator actually has to say okay my                                 snapshot has to reflect everything                                 before the barriers are nothing behind                                 the barriers so what it will do it will                                 actually once it receives barriers from                                 one of the streams start either holding                                 back or buffering up a little bit of                                 data from that stream until it's it's in                                 the other barriers and once it's                                 actually seen that then it will actually                                 emit a variant say okay here's the                                 here's the the point from my downstream                                 that marks at that point of alignment                                 okay I think this is very hard to read                                 but okay let me let me try and explain                                 it a bit so this is so dude usually that                                 just that just works works very easy                                 like this alignments take take                                 milliseconds or so around the check                                 point and um in cases when you see                                 checkpoints not you know not going                                 through as fast as the USS you used to                                 you know you're triggering checkpoints                                 every few seconds they go through fast                                 and then once in a while you see okay                                 here's here's one that that takes a                                 little longer there there are a few                                 tools that you can actually use to                                 exactly try and figure out whether this                                 is actually a problem or not one of the                                 most useful ones is the is actually the                                 fling flink web UI that you can use to                                 draw it draw some inserts from that so                                 it gives you some numbers like what was                                 the end-to-end duration of a checkpoint                                 what was the size of a checkpoint and                                 what was the time they were spending                                 alignment and if you if you actually                                 then look at the subtasks you see even                                 more details like how much time did the                                 checkpoint spent on on materializing the                                 on taking this this image snapshot in a                                 synchronous way in the astronauts way                                 how long did in alignment to take how                                 many bytes did it actually have to                                 buffer and how long did the whole thing                                 take end-to-end so if you if you'd then                                 dissect the the checkpoints you'll                                 actually see that these like these                                 numbers all kind of refer to different                                 different characteristics of behaviors                                 of the distributed systems so the the                                 duration that the alignment takes the                                 amount of data buffered means how long                                 how long does it actually take to                                 establish like a point where everything                                 is everything kind of lined lines up for                                 a snapshot obviously you want this to be                                 as fast as possible how long                                 snap shock take synchronous versus                                 asynchronous part synchronous port means                                 how long is the pipeline actually                                 interrupted for processing between                                 around a snapshot and cannot actually                                 take the next event and continue                                 processing it and I synchronous means                                 how long does it take in the background                                 - let's say persist this to HDFS to s                                  or whatever and then there's the entry                                 and duration and this there's actually                                 an interesting hidden metric in there if                                 you wish which is if you take the                                 end-to-end delay at the end-to-end                                 duration and subject the synchronous and                                 the asynchronous part and if you wish                                 even the alignment duration then you can                                 figure out how long did it actually take                                 from when the system trigger to snapshot                                 for everything to flow through and the                                 barriers to actually reach the operator                                 to take it snapshot so you can actually                                 see how far is it delayed is the late                                 other later operators kind of delayed                                 rather than are compared to the earlier                                 operators so if these if these numbers                                 actually don't look as if these numbers                                 look like this then then this is easy on                                 the other hand this was running you know                                 like on a for virtual processes on my                                 laptop so this isn't actually you know                                 this isn't a large-scale screenshot from                                 a large-scale implementation but if you                                 want to if you actually see these                                 numbers and and some of them are are too                                 high here's is what this usually means                                 if you if you're working in your system                                 so if you have a very long delay until a                                 checkpoint is triggered that that                                 typically means that you're operating                                 under a constant back pressure which                                 means that at least for what the system                                 is doing at that particular point in                                 time it's actually under provision so                                 you try to do more than the machines can                                 get through in that particular point in                                 time and then you know Flinx back                                 pressure mechanism kicks in and just                                 makes the whole pipeline adjust to the                                 slowest part because otherwise it would                                 it without just overflow if the snapshot                                 to take just very long then it it can                                 usually it usually means that you know                                 it could either be that the bandwidth to                                 your storage is really crappy this is                                 actually has actually happened it could                                 also mean that you're just keeping a lot                                 of state on the machine and it it down                                 it actually means that per checkpoint                                 you have to do too much work blink                                       release incremental checkpoint so that                                 checkpoints actually transfer the diffs                                 over over previous checkpoints which has                                 actually made this problem go                                 for many users away and then we have we                                 have the alignment duration what I was                                 talking about earlier so this is in some                                 sense I think the most important                                 robustness metric because even if the                                 others are high yeah if the others are                                 high you know if you're in the back                                 pressure situation you may still be                                 working fairly well if you're taking                                 long to checkpoint large state this may                                 still be well but if your alignments get                                 kind of thrown off and this is something                                 you may want to look into                                 so what could Edward are actually                                 typical situations where these                                 alignments start costing costing a lot                                 and again I'm going back to the                                 comparison I try to make a few a few our                                 slides back that was that these                                 alignments in in flanker and stream                                 processing they kind of correspond to if                                 you're running like if you're running                                 distributed databases and to try to                                 establish like kind of consistent points                                 these are the points we have to take                                 certain you have to you have to take                                 certain um give to establish a a common                                 denominator way say ok this transaction                                 has actually completed in all my                                 parallel charts versus this has not and                                 so on right and what can happen in in                                 such a system is for example that you                                 know you're computing a very large                                 aggregate or you're computing very large                                 windows and streaming and you're                                 emitting it right at the point in time                                 when you do a checkpoint and it just so                                 happens that this huge data is queue so                                 it only affects one node and all the                                 others are doing very well this think of                                 it as you know the transaction would                                 have effect one short very heavily and                                 do a lot on that and all of the others                                 it would be good in that case the others                                 could still not commit the transaction                                 before the other shard is done so this                                 is kind of the equivalent to that you                                 know that this it's just very secured                                 work on one node or the                                 the other equivalent of that would be I                                 know it is just like temporarily stored                                 by something like garbage collection and                                 you know because link doesn't really                                 work like a transactional system but                                 like a streaming system a-- tries to                                 make progress but some of the work will                                 actually back up if certain notes say ok                                 i'm actually as part of this alignment i                                 can I cannot make progress here so                                 there's there's going to be a little bit                                 of back pressure built up along these                                 cases and in some sense the most                                 important thing that we've learned in                                 order to keep the whole thing running                                 well is make sure that these builds are                                 build ups of like pressure around                                 alignment they they are resolved very                                 fast                                 so and that that led to two things that                                 we actually introduced into flink first                                 was um a setting where you say give the                                 system a minimum time between check                                 points and I would actually say that if                                 you run this large today this is almost                                 the most important thing to set in                                 checkpointing you don't I would actually                                 almost argue that we drop the check                                 point interval setting and actually                                 purely work with with that or at least                                 with something that takes this into                                 account as as a policy think of it as                                 the following way you could just tell                                 the system you know to check points as                                 fast as you can but always make sure                                 that you have two seconds of pure                                 progress between check points where you                                 know we're not sharing bandwidth with                                 storing the the snapshots in in whatever                                 system has three HDFS and so on but just                                 make pure progress on your computation                                 don't don't do any resources on                                 something else and then given that you                                 fulfill this policy to check points as                                 fast as possible this seems to be                                 something that that has has worked                                 really well for um for users that said                                 it just said the Check Point interval to                                 a very low value and then set a minimum                                 time between check points the other                                 thing that we can observed is that the                                 more we the more asynchronous work a                                 check point can do that the more these                                 problems just seem to go away so that                                 the cheaper the alignments get because                                 they're they're smaller interruptions                                 that cause because the build up of back                                 pressure and that is that is what we've                                 been actually working in like in                                 response to these observations over the                                 last of the last months a lot like                                 making sure more and more and more stuff                                 actually gets asynchronous so if you                                 look at flink                                                            on the snapshots for the rocks to be set                                 back end and in one to one there was                                 kind of a hidden feature to do it on the                                 on the estate back-end that he actually                                 keeps it in as plain Java objects on the                                 heap and flink one three this actually                                 became the the default option and                                 operative state also became um became                                 asynchronous and fling one four we now                                 have an open pull request to also make                                 timers and everything and synchronously                                 are persisted so moving that moving that                                 along another thing that we kind of                                 realized is that we always we're always                                 arguing between should                                 what should we actually make the default                                 choice for a state back end state back                                 end in flink defines both the data                                 structure that keeps in the end your                                 your in memory variables especially if                                 they're organized by key and you can in                                 theory swap them in out of memory                                 depending of computation for a key is                                 actually hot or not there they're kind                                 of two prominent ones one that just                                 keeps it really as as pure pure memory                                 of pure Java data structures and memory                                 verses that serialize it into Roxie be                                 back and forth and this is kind of the                                 flowchart we we sort of came up with to                                 to help to help users make the decision                                 what what should they take if you have                                 stayed that is larger than memory and                                 obviously you need to pick Roxie be if                                 you're comfortably within actually the                                 memory of a machine then it's actually                                 very interesting to trade to kind of                                 look at how expensive is it to move                                 these objects between as you realized                                 form of heat and on he began whenever                                 you want to compute on them you                                 obviously need them on heap whenever you                                 want to purchase them you would need                                 them off heap how does your data rate                                 effect that you know the cost of                                 civilization and if yeah so this loader                                 has kind of been a good help to do that                                 it's a bit simplified but it it kind of                                 gets the soul visceral yeah                                 decision decision process roughly                                 roughly map down I think we're running                                 out of time                                 and with that that's actually perfect                                 because I'm also I'm also pretty much                                 done done with the presentation there's                                 just two things I would like to mention                                 if you if you're actually interesting in                                 the interest in the stream processing                                 space or if you're working with flink                                 and you like to arm you'd like to share                                 some of your your personal lessons learn                                 to use cases or just you know just                                 here's the thought of what would be                                 really cool in stream processing and you                                 want to share it with a with the                                 community around around that um at part                                 there's a conference called fling                                 forward that is taking place right here                                 exactly where brilliant possibilities in                                 a few months and the call for submission                                 is still open so both of your if you're                                 interested to be a speaker or you're                                 just interested to learn more about                                 stream processing very happy to yeah to                                 meet you there and if you actually if                                 you actually like the Searle work of                                 stream processing apache fling cuts on                                 the whole like real-time application                                 echo system and you want to you'd like                                 to work work on this full-time then come                                 talk to us and we also have a booth and                                 in the palais so feel free to stop by                                 thanks for faced tougher                                 [Applause]                                 [Music]
YouTube URL: https://www.youtube.com/watch?v=ct9cHlegI-U


