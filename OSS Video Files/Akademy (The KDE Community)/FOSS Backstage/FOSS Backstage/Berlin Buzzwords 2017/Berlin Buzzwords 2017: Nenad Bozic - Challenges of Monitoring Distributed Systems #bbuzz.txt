Title: Berlin Buzzwords 2017: Nenad Bozic - Challenges of Monitoring Distributed Systems #bbuzz
Publication date: 2017-06-15
Playlist: Berlin Buzzwords 2017
Description: 
	Back in the days, you had a single machine and you could scroll down the single log file to figure out what is going on. In this Big Data world you need to combine a lot of logs together to figure out what is going on. Data is coming in huge volumes, with high speed so choosing important information and getting rid of noise becomes real challenge. There is a need for a centralized monitoring platform which will aid the engineers operating the systems, and serve the right information at the right time.

This talk will try to help you understand all the challenges and you will get an idea which tools and technology stacks are good fit to successfully monitor Big Data systems. The focus will be on open source and free solutions. The problem can be separated in two domains which both are the subject of this talk: metrics stack to gather simple metrics on central place and log stack to aggregate logs from different machines to central place. We will finish up with a combined stack and ideas how it can be improved even further with alerting and automated failover scenarios.

Read more:
https://2017.berlinbuzzwords.de/17/session/challenges-monitoring-distributed-systems

About Nenad Bozic:
https://2017.berlinbuzzwords.de/users/nenad-bozic

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              hi and welcome to my presentation today                               we'll be talking about the challenges of                               monitoring this                               systems my name is Nana borage and I'm                               coming from Serbia from company's market                               where I'm I co-founded that company and                               I'm working as a technical consultant we                               are dealing with we are doing big data                               engineering consulting in the data                                science and in our day to day business                                clients call us when problem arises and                                we need the good and powerful insights                                from monitoring existence in order to                                pinpoint the problem and fix stuff it is                                sort of similar like when your car is                                broken first thing that you will do is                                go to mechanic modern cars these days                                have powerful board computers and you                                will hook in laptop mechanic wheel who                                can laptop check out certain graphs and                                figure out what part of the car is                                broken and get on with fixing this part                                of your engine we need the same things                                in IT systems and we're we are building                                many many companies these days are                                building and trying to get the best                                monitoring solution out there it's                                really hot topic how many of you are                                working with distributed systems these                                days                                can you raise you can so the through the                                system starts to be an new normal and                                how many of you are operating those                                systems so maintaining monitoring yeah                                feel your pain monitoring distributed                                systems is really hard when you have                                single instance basically if that the                                instance dies I think it's easy to                                figure out why when you have distributed                                systems failure on one one not the                                propagates to the other node and so on                                so things get complicated when you try                                to correlate the failures from different                                machines so here is the agenda we will                                first run and set some ground rules in                                monitoring the main and we will explain                                important terms then we will run through                                metrics data stream and tools which may                                help you visualize metrics then we will                                run through log data stream and tools                                which can help you collect log events                                after that we will show you how I will                                show you how you can combine both                                metrics and logs because looking at the                                metrics alone or logs alone is not                                enough to explain all the failures I                                think it's much better to combine those                                two and be in full control and last but                                not least we'll cover alerting and we                                will explain how you can step away from                                dashboards and continue on building your                                business logic while your let machine                                alert when certain failures happens I'm                                a big fan of domain driven design and                                first thing that you will do is set some                                ground rules and create the main                                language so let's do the same thing with                                monitoring how we like to look at                                monitoring is it consists of three parts                                you have metrics logs and alerts metrics                                are showing you that your system is                                performing within thresholds or outside                                of thresholds and then it's time to                                investigate log data string is providing                                context so usually when you have certain                                peaks and metrics you want to go to log                                and figure out what is going on what                                caused this peak to rise alerts as I                                said it is important not look when you                                don't have money for IT department                                looking at that for                                                     machine you need to figure out certain                                rules let machine a large events problem                                is happening and then go and browse and                                a rest of the time spent developing new                                features which are driving your business                                all these three parts doing all these                                three parts right is hard you need both                                technical skill set and you need to                                understand business domain at this                                picture I am going back to our example                                with mechanic he is looking at their                                board on computer and this Airport to us                                doesn't explain much I would say he he                                knows the mechanic domain and he knows                                certain parameters and what is normal                                what is outside of normal his expert in                                his field so he is both technical                                experts and probably understand the main                                 so he will ask you whether you're                                 driving frequently whether your drive                                 in Serbia where you see a bunch of bumps                                 or you're driving in Germany where                                 everything is flat so he needs this                                 domain knowledge and technical knowledge                                 to reason about those insights that                                 monitoring solutions are giving giving                                 us                                 so let's first touch metric data stream                                 as I said they sell metrics data stream                                 is indicator that everything is working                                 within expected boundaries in my                                 experience it is easily forgotten                                 feature while you're developing your                                 application metrics are pushed aside and                                 we will do this when we are in                                 production let's build feature X feature                                 Y and stuff and then you go to                                 production without any metrics and                                 failure starts happening and then it is                                 already too late                                 having good dashboards having good                                 visualization it's hard probably you                                 will first start graphing everything                                 then you will see that there is a lot of                                 noise and you cannot look at that                                 anymore                                 then you will delete stuff then failure                                 will happen and you will figure out that                                 you're missing important information so                                 it's iterative process and try to I                                 don't know try to cut retrospect this                                 talk with your team and try to try to                                 add stuff which is missing and delete                                 stuff which is creating noise in                                 distributed systems it is even harder                                 because you have many graphs to watch                                 and it is easy to fall into information                                 overload trap having bunch of machines                                 sending bunch of metrics we can add                                 bunch of graphs so you really need to                                 have these retrospectives and tighten up                                 your monitoring stack when you're                                 thinking about metrics data stream                                 you're faced with decisions which stack                                 to choose where should you vote with                                 self-managed solution or software is a                                 service solution also there is a                                 discretion whether to pay for something                                 or use something which is free                                 open-source those decisions are                                 important and mostly they are made based                                 on your use case so you know you need to                                 know use case I can give you context how                                 we how we reasoned about it                                 we needed to provide the guarantees the                                 latest guarantees in                                                                                                                            be under certain threshold every went                                 with the data dog anyone from data dog                                 in audience I had this thought three                                 weeks ago and on a patch gone and the                                 guy from data dog was in audience so I                                 need to I needed to pay attention what                                 I'm talking about                                 data dog has                                                          graph so well if you look at                                         period it will show you averages not raw                                 data and in order to provide the SLA on                                 this high                                                             day you need really raw data and data                                 dog didn't work for us so we decided to                                 go with the self managed solution it is                                 trade-off for self managed solution you                                 will spend most of your time maintaining                                 one more system but then again you will                                 have better level of control and you can                                 put monitoring stack inside your V PC                                 and you can secure your data it is not                                 always an option to send metrics to some                                 third-party cloud-based provider so you                                 need to know your use case and make a                                 decision based on use case so let's run                                 through stack that we choose the                                 building this self managed solution                                 which is the ratio chosen Rieman and                                 sync server Ramon is the processing                                 framework which is acting as a server on                                 matrix machine and you're using                                 different clients on your applications                                 Raymond Cal Java client Python client                                 and all bunch of clients and you're                                 basically sending selection of your                                 metrics to Raymond server and Raymond                                 server is processing and storing those                                 events it has integration with influx DB                                 and we choose a name for the baby                                 because it has a measurement in its core                                 info DB is a time series database which                                 is storing measurements with timestamp                                 value fields and tags so it's perfect                                 for time series                                 it's built for that this is its sole                                 purpose and it has good integration with                                 gray fauna which we are using for                                 visualization your fauna is a                                 visualization open-source visualization                                 tool with many visual aids you can plot                                 graphs time series tables gouges                                 whatever and you can plug data sources                                 and in fact DB is one of those data                                 sources and this is how one of                                 dashboards that we build looks like this                                 is Cassandra cluster and on top you can                                 see easily that                                                          basically a shell check and on the left                                 and right this is a total request rate                                 on our cluster and below that we plotted                                 the request rate per host because                                 dealing with this kind of hassle a and                                 high nines we needed to figure out                                 whether we have bad hosts in in our                                 infrastructure so we needed really pair                                 host request rate and you can see a peak                                 in the beginning of the graph on single                                 host there was increased request rate                                 this is probably a good place to                                 investigate further you can see that                                 based on this therefore you cannot                                 explain why this peak happened so you                                 will either go to I don't know memory or                                 disk or network statistics or your you                                 will start start browsing locks when                                 matrix Peaks metrics Peaks are                                 indicators that something bad happened                                 and log log log events our next thing                                 that you will look look at log log will                                 logs will provide context they will                                 explain from technical standpoint what                                 happened working caused across this peak                                 and usually you're looking at metrics                                 you're glancing it to into metrics but                                 when problem arises you want to check                                 check logs it has same challenges that                                 with met with as with modern metrics you                                 need not to have too much information                                 you need not have too few information so                                 it's again iterative process you will                                 first start logging everything with info                                 level then you will                                 realize that you have a lot of noise and                                 you will after you go to production you                                 probably will narrow down log events                                 only two important things in distributed                                 systems it is again much harder because                                 you have many terminals open and it's                                 getting easy to fall into information                                 overall threat this is how our initial                                 approach looked like so with the help of                                 CSS HX Mac tool we opened up                                    terminals for                                                      tailing logs and no one could figure out                                 what is going on because it's impossible                                 to look at logs and distributed systems                                 like this so we needed the better                                 solution then again we were faced with                                 decisions we needed to decide again                                 whether to go with the self managed                                 solution or software as a service                                 solution and again we needed to figure                                 out whether it it is good to pay for                                 some solution or to use free solution                                 decision based on our experience with                                 data dog we decided to go with self                                 managed solution but it boils down to                                 the same things as with matrix tech                                 technical team skill set the level of                                 control that you want to have and                                 security of your data                                 I emphasize here security of your data                                 matrix are just arbitrary values with                                 timestamps so they are not providing                                 many insight from your application with                                 log story is different I seen many                                 things in logs people are logging email                                 addresses and stuff and it's not always                                 an option to go with the cloud based                                 provider and push many insights about                                 your business to some arbitrary                                 cloud-based the tool so we decided to go                                 with anyone not familiar with LK here so                                 yeah okay I guess became pretty much the                                 standard when when you want to collect                                 log events from a bunch of machines and                                 visualize them on single place and the                                 distributed systems it's a must you saw                                 this                                                      so it consists of elasticsearch location                                 cabana file bit is placed on each of                                 machines and it is listening to log                                 events when new event arise it triggers                                 it gets trigger and shifts it to log                                 stash log stash is there to process your                                 log events to split a log event into a                                 bunch of parts or to do something about                                 log event and then you really index your                                 log event to elasticsearch which is good                                 for for freeform free text searches                                 which is usually what you do in with log                                 event messages and on top of that you                                 have cabanas visualization tool which                                 will which is look which looks something                                 like this so we on top you have a time                                 span time span so you can you you can                                 configure what to look here it is                                    minutes below that there is this white                                 leucine query freeform edit box so you                                 can type any loose in query and in the                                 middle you have log events sorted by                                 time from different machines this way                                 you can figure out at certain point in                                 time what happened on each of your                                 machines and how failure probably on one                                 machine did reflect on other machines in                                 your system                                 Kuban also has visual visualization                                 similar to what we showed with Griffin                                 Oz so this is a application errors                                 through time on our stack and you can                                 see these hi graphs near the end of the                                 time span and this is probably a good                                 place to research a bit further and go                                 into row logs but now we were faced with                                 the - - visualization tools so in                                 Griffin of you're looking metrics then                                 we were jumping to cabana to look at                                 logs and having to look at the two tools                                 is not convenient we wanted better                                 solutions we wanted to combine                                 everything under one umbrella so                                 again I want to return to real world                                 example why we did all of this to give                                 you a more context we needed to we                                 needed to provide reliable guarantees                                 that we'll hear latest you'll                                            requests during                                                        milliseconds and whole infrastructure                                 was deployed to AWS and we had a bunch                                 of machines                                                     applications and it was really complex a                                 really complex infrastructure so we                                 needed the really good insight from our                                 monitoring system in order to figure out                                 because single peak can mess up our                                    hour wrestle a girl fauna is a good tool                                 for combination it has pluggable data                                 sources and both in flux DB and                                 elasticsearch can be used as data                                 sources in your fauna and this is what                                 triggered us we all reduce we are                                 already using in flex DBS data source                                 for our metrics and we wanted to bring                                 our logs close to our metrics so we can                                 visualize everything on a single screen                                 and reason about it looking at single                                 dashboard this is our stack I already                                 explained the pieces of it so I will                                 just run through it fast                                 you remain clients on on machines fear                                 these days switching to Telegraph as                                 well and testing boat in parallel there                                 are telegraphy the process open source                                 processing tool by influx data it has a                                 bunch of input plugins and bunch of                                 output plugins and you can collect                                 metrics I guess more easily than with                                 than with Rieman because with dreamand                                 you need to build application and the                                 telegrapher example has out-of-the-box                                 OS stack net starts these starts                                 collecting and a lot of good things and                                 we're storing metrics in influx DB and                                 oranges are log data stream we are                                 having file based sitting on machines                                 shipping to log log stash log stash is                                 processing stuff storing in LS research                                 and we are still having                                 to look at our roll logs because                                 sometimes we need to jump in to roll                                 logs and figure out what is going on we                                 are using your phone on top of both in                                 Flex DB and elasticsearch and we are                                 creating dashboards like this on top we                                 chef                                 it looks a bit foggy but on top it's                                 slow queries to extend or not basically                                 we are storing each and every query                                 about                                                                 that around                                                            dots which are around                                                    not good so and below below that we are                                 story we're showing drop messages on                                 Cassandra Cassandra is good for for                                 performance and it's full power tolerant                                 and it is usually dropping messages when                                 it has high load so it can serve more                                 messages so we wanted to check how                                 slowness on machine infla looks at                                 through in Cassandra logs you can see                                 that at the exact same time when we have                                                                                                   messages on Cassandra and below that you                                 can see what actual details from logs                                 what kind of messages were drop mutation                                 read messages and how many of them so                                 looking at this soul dashboard you can                                 figure out that latest single machine                                 cause the cause the Cassandra to drop                                 messages but looking at this dashboard                                 you cannot figure out what was the root                                 cause of this latency I deliberately                                 didn't put our latest dashboard here we                                 added the disk disk IO wait times on top                                 because we sat down we look at this                                 dashboard we realized that we cannot                                 figure out what was the cause and we                                 need we figured out that we needed to                                 improve this dashboard so we added disk                                 IO stats on top and then you save the                                 full picture disk there was display to                                 see on AWS EBS volume which caused the                                 latency and center not responded with                                 drop messages and this is whole journey                                 and it's easy to reason about                                 whole problem looking at single single                                 debt world and last but not least                                 because now we have combined metrics and                                 logs we wanna as I said step away from                                 building this man monitoring stack                                 playing with that but no one wants to                                 pay pay for us to visualize stuff and do                                 these cool things people you need to                                 develop features and leave machine to do                                 alerting and let you know when it is                                 your time to investigate it is similar                                 like encar encar example usual your                                 dashboard is dark and when you have                                 certain failures or the problem it with                                 the car certain lights will pop up and                                 you know that it is time to call                                 mechanic it is same Indian IT systems                                 you will define your alerts and you will                                 have your support team react on on those                                 alerts a letter in alerting as I said is                                 giving you freedom to step away from                                 dashboards this is also important                                 someone else place the domain knowledge                                 about alert when designing alerts so                                 this makes it possible not to have only                                 experts looking at certain death words                                 knowing which are thresholds of good                                 healthy system and which are thresholds                                 of faulty system so somebody else                                 created is alert and you can just react                                 and bug this guy hey investigate this                                 timestamp same story with as with logs                                 and metrics alerting is must be known                                 must not be too frequent because you                                 will tend start ignoring alerts so this                                 is really important to keep in mind keep                                 your alerts just just triggering on                                 important events it is similar like all                                 of you know the story about the boy who                                 cried wolf it was boring he needed to                                 guard the sheep's and he was crying wolf                                 two or three times                                 people from villa form nearby village                                 game every time less and less people                                 came from from village and after three                                 times actual wolf came and he again                                 cried wolf but no one no one came from                                 village they were people from village or                                 getting used to false alerts and he used                                 all kind of false alerts and when                                 failure happens when wolf came no one                                 reacted this is scary scary situation                                 and you don't want to be in that                                 situation to get a failure of your                                 database in production and no one                                 noticing because they have a bunch of                                 emails inside certain certain folder                                 which are getting ignored we were we                                 realized that we wanted some better                                 solution here as well so we started                                 building Sentinel smart alerting                                 solution we wanted to have machine                                 learning algorithm crunch these metrics                                 and figure out doing anomaly detection                                 and we I think real problem with alert                                 is having humans make assumptions about                                 the systems we are designing we're                                 designing alerts and we can say that the                                 PC CPU on                                                              were fire alert but then again who knows                                 if we have trend of our instances using                                 more and more CPUs so this needs to move                                 through time and we needed a better                                 solution so we decided to hook spark                                 streaming on our stack that I explained                                 so we have this system which we are we                                 are monitoring we are storing our                                 metrics into database and we are                                 visualizing metrics using your fauna but                                 we also we are also pushing metrics to                                 spark and sharing data models crunching                                 those metrics and checking out when                                 certain values are outside of boundaries                                 after that happens we want email to be                                 fired to maintenance guys providing full                                 I've shot of the systems this email                                 looks like this it has all the important                                 numbers for someone to reason about it                                 says in rip colored in red the stat that                                 triggered the trigger alert in this case                                 disk evade disk IO was the main cause of                                 this alert and we wanted the machine to                                 have died and diagnostics message so in                                 the last sentence it is said that disk                                 IO was higher than usual during that                                 time and there is a link to brief on a                                 dashboard with specific disk statistics                                 so it is good for someone who don't want                                 to browse through all the graphs and                                 stuff this this is a summary and it                                 provides one click to dashboard which                                 which probably has the source of the                                 problem this is still working in                                 progress we are planning to this is                                 alert so some something already happened                                 what we want to do is to push this                                 matrix through some neural network or                                 something like that                                 and try to predict based on trends when                                 certain to do predictive maintenance                                 well through certain trends to figure                                 out when problem will happend and then                                 there is time to react                                 then then you can do something about it                                 ok and to conclude I want to emphasize a                                 couple of important things that I                                 touched upon this presentation I                                 emphasize this in matrix stack in log                                 stack and in alerting stack you need to                                 have right amount of information you                                 know it's scary to have too much                                 information because this will be noise                                 and you will ignore things it's scary to                                 have too few information because you                                 could you will not be able to explain                                 problematic situation you need to have a                                 good selection of metrics on your                                 dashboards you need to have good                                 selection of logs and this is iterative                                 process I mentioned that we in company                                 have                                 matrix monitoring retrospective meetings                                 we sit down all of us in team and they                                 speak which - do you look which locks                                 are polluting your locks and which are                                 providing informational important                                 information is there something that is                                 missing that you would like to add and                                 we are constantly changing monitoring                                 stack I think it's important to have                                 this up-to-date because it's your main                                 source of truth when failure happens do                                 not end up fixing monitoring machine                                 instead of fixing application business                                 logic this is also important I spoke                                 today a lot about open source stack that                                 we use self managed stack but this is                                 the and you can think that it is silver                                 bullet but we ended up working on                                 monitoring stack more than on our                                 application stack its distributed                                 systems system monitoring stack is one                                 more system under your hand and your IT                                 department will have to scale this                                 system to fix stuff - even worse we had                                 the situation that failure on our                                 monitoring stack propagated to failure                                 on our instances because logs were                                 piling up and we took all of our disk                                 space luckily it was in staging                                 environment but these things can happen                                 when while on software is a service                                 solution somebody else takes care of                                 that be proactive not reactive this is                                 something that we are trying to do with                                 central so we are trying to push our                                 metrics to mail to neural network and do                                 predictive and predictive maintenance                                 and telematics by our needs every use                                 case is different if you have specific                                 use cases we did the build stuff we                                 build the diagnostic solution for                                 Cassandra because solutions out there                                 couldn't provide the latencies for all                                 the queries usually they were working                                 through percentiles and highest                                 percentile is                                      sent requests and we needed two more                                 nines so we needed to look at really all                                 all queries and we decided to go and                                 build build build a tool which will aid                                 us in the in this links I blogged about                                 this topic a lot so I covered the three                                 parts of this presentation in more                                 details if you if you want you can check                                 it out                                 explain this monitoring stack which we                                 are using I explained distributed                                 logging and matrix stream here is my                                 Twitter channel maybe I could say the                                 head this emphasize this on the                                 beginning of presentation I posted the                                 this presentational SlideShare so I                                 could save you a couple of pictures you                                 can check me on Twitter and you can get                                 this presentation and last link is our                                 github project where we automated this                                 monitoring machine there is ansible                                 project which and you can spin up this                                 monitoring machine and play around with                                 it it has Riemann influx graph on i lk                                 on it and you can check it out and you                                 can play locally with it and check out                                 how this how everything works and that's                                 basically the I ran through it a bit                                 faster than I expected but it's a                                 lunchtime so yes it's a good thing I'm                                 open for any questions                                 yeah questions I want to give you the                                 microbe thanks for the talk what I'm                                 about to ask you is more of a thought                                 experiment than anything else so some of                                 the logging that I concern myself with                                 have to do with machine learning stuff                                 so recommend recommendation engines and                                 you name it and some of the stuff you                                 can measure like the click-through rate                                 how well are we doing that's something                                 you can plot over time and that works                                 there's also other internal stuff                                 numerix are things converging kind of                                 nicely and you name it and these plus at                                 some point become kind of complicated to                                 the extent that you need more than just                                 a simple line chart I've been playing in                                 my head with a thought of you know if                                 all the logs goes to some nice central                                 place I could have an eyepin notebook                                 like one of those Jupiter notebooks with                                 the plotting and stuff and put that in                                 the chrome thing that would just run                                 every day where I get all the other                                 stuff I might be interested in and is                                 this something you guess could be                                 something you could consider because it                                 it feels like it is a little bit hacky                                 just wondering what your opinion on                                 something like that might be if you're                                 doing the same thing with metrics we                                 currently concentrated on metrics and                                 build the spark stream where we are                                 doing machine learning on metrics so but                                 you have this math you will have logs in                                 elasticsearch so I guess you can use                                 elasticsearch api's and start doing some                                 clever stuff from logs and return either                                 metrics to influx VB roll-ups or I don't                                 know how many times certain X appears or                                 something like that so make sense I                                 don't know if elasticsearch is not the                                 best tool for the job locks can be                                 stored in some other place but                                 elasticsearch is good because it                                 provides full-text search so you can                                 work with these strings also you can                                 push logs in parallel to elastic search                                 and to something else which you are                                 using for machine learning so it makes                                 sense it makes perfect sense                                 next question was here somewhere                                 hi do you have any insights on file bit                                 semantics or guarantees I mean you don't                                 want duplicate log and                                 in case of network failure or if five                                 classes we do miss log entries from your                                 log yeah that's that's a good question                                 running to monitoring your monitoring                                 stack so file bit you can configure                                 amount things with file bit and it has                                 back pressure it says it is saving                                 events if your elasticsearch for example                                 log stash is not responsive it will                                 start start collecting events and just                                 push them when there is a network                                 connection in between but yet this is                                 the important thing I guess you need to                                 monitor your monitoring stack you it                                 would be wise to have certain insights                                 even on file bit on each machine to to                                 figure out we managed to fill up space                                 with file bit events while elasticsearch                                 was not responsive this is the situation                                 which I mentioned on staging environment                                 so file bit logs which were waiting for                                 network connection started piling up and                                 this caused a failure on instance where                                 your databases which is scary which is                                 really scary so it makes Rieman for                                 example comes with its own monitor is in                                 its all its own metrics and you can                                 visualize metrics about Rieman client                                 and server so you always know how your                                 remand server performs which is part of                                 your monitoring stack but it is also                                 emitting metrics about its performance                                 any questions left for minute then                                 enough thank you very much for this                                 interesting talk thank you                                 [Applause]
YouTube URL: https://www.youtube.com/watch?v=a_E4irrqSMY


