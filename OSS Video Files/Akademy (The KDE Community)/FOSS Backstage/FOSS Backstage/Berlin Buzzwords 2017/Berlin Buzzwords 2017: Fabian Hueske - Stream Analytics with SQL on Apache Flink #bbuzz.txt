Title: Berlin Buzzwords 2017: Fabian Hueske - Stream Analytics with SQL on Apache Flink #bbuzz
Publication date: 2017-06-15
Playlist: Berlin Buzzwords 2017
Description: 
	SQL is undoubtedly the most widely used language for data analytics. It is declarative, many database systems and query processors feature advanced query optimizers and highly efficient execution engines, and last but not least it is the standard that everybody knows and uses. 

With stream processing technology becoming mainstream a question arises: “Why isn’t SQL widely supported by open source stream processors?”. One answer is that SQL’s semantics and syntax have not been designed with the characteristics of streaming data in mind. Consequently, systems that want to provide support for SQL on data streams have to overcome a conceptual gap.

Apache Flink is a distributed stream processing system. Due to its support for event-time processing, exactly-once state semantics, and its high throughput capabilities, Flink is very well suited for streaming analytics. Since about a year, the Flink community is working on two relational APIs for unified stream and batch processing, the Table API and SQL. The Table API is a language-integrated relational API and the SQL interface is compliant with standard SQL. 

Both APIs are semantically compatible and share the same optimization and execution path based on Apache Calcite. A core principle of both APIs is to provide the same semantics for batch and streaming data sources, meaning that a query should compute the same result regardless whether it was executed on a static data set, such as a file, or on a data stream, like a Kafka topic.

Read more:
https://2017.berlinbuzzwords.de/17/session/stream-analytics-sql-apache-flink

About Fabian Hueske:
https://2017.berlinbuzzwords.de/users/fabian-hueske

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              yeah thanks for the introduction                               welcome everybody yeah I'm going to talk                               about stream processing stream                               processing with a sequel on a page we've                               linked a Patrick link I guess you                               already heard about it a little bit it's                               basically a platform for distributed                               scheduled stream processing so the main                               characteristic is our it's faster you                                got low latency and high throughput at                                the same time so formally you have                                basically G choose whether to go with                                low latency or high throughput with a                                flink basically choose both of that it's                                accurate in the sense that it offers                                event time processing                                so you're processing data based on                                timestamps in your data and not based on                                the time when data arrives on your                                machines and it also provides exactly                                once guarantees for the stage that                                you're holding in your in your                                operations it's reliable so you can have                                a highly highly available or setup of a                                fling cluster and with you with the                                feature of snapshots you can basically                                take a snapshot of your application                                consistent snapshot and then arm start                                another application or the same                                application from that snapshot at any                                point in time later so these are like                                the key features of length fling is used                                in quite a few or by quite a few                                companies across very different                                industries these days so it's used by                                online retailers it's used by telcos in                                finance social media and also arm mobile                                gaming and many of these users run flink                                on very large scale they're processing                                billions of events per day and have                                state that grows to tear up exercise so                                from for many of these users this is                                like a very good set up the data stream                                API off link is basically the probably                                most well known API to to implements                                tree processing applications                                it's very expressive so basically all                                the logic that you implement goes                                to user different functions and you got                                you have this API of of many concepts                                that are very to the core of stream                                processing such as windows triggers you                                have some ways to deal with time and                                state you can do asynchronous codes so                                all of this is built in this API so it                                gives you a lot of control of about how                                you're processing streams and how I deal                                with state and time basically all those                                concepts which are pretty much to the                                core of stream processing on the other                                hand many applications follow follow                                similar patterns and don't really don't                                really need this expressiveness of the                                data stream API and these these                                applications can be can be implemented                                are more easily and more concisely with                                a domain-specific language and what's                                the most and most popular DSL for for                                data processing of course its sequel so                                that's why flink or the flink project of                                thing community is working on relational                                api's form for apache flink right now                                there are two different api's there is                                standard sequel and so called a table                                API which is a language integrated API                                for Java and Scala we'll see later how                                query looks like in this table API and                                both of these api's are integrated                                unified API is for beds and stream                                processing which basically means that if                                you specify a career in the the                                semantics of the query do not depend on                                whether the input is a stream or a batch                                input so it doesn't matter whether you                                read the data as a stream or reading a                                file the result that is computed by this                                query will be the same so this is a very                                important very important point that I'd                                like to stress it's all about the                                semantics here that you have the same                                API for fetching stream processing the                                to API is the sequel end table API                                have common translation layers so                                there's a lot of a lot of the internals                                also the random code is the same and all                                of this is based on a picture calcite                                which is a yeah clearly optimized a                                clear policy proposal and app educator                                is basically used to pass sequel and                                 also to translate the table API cards                                 into a common representation which is a                                 logical theory plan then this logical                                 query plan is optimized by the website                                 optimizer with a custom routes depending                                 on whether the input is it's streaming                                 or badge and then the this physical plan                                 gets translated either in a blink data                                 set program if the input is batch input                                 or I did a stream program if the input                                 is stream streaming input so we have                                 different translation part for batch and                                 stream but the result is I said is it's                                 the same alright so um how do curious                                 look like in these in these api's so                                 first of all on a table a query                                 so you see we're starting here from so                                 called table environment this pmf at the                                 top on this we can call a method called                                 scan we give a name here which is the                                 name of a table we do some filtering on                                 that so with the the table is a table of                                 clickstream data so which has like a in                                 common feeds they would expect so there                                 some UL included in there and we aren't                                 interested in anywhere else let's start                                 with these dub-dub-dub xyz.com after                                 that we do a group i we group by the                                 user that did the click and after that                                 we basically want to count how many how                                 many links this user clicked that came                                 came from this domain so if you do the                                 same query in in sequel it has it looks                                 like this so there's a little bit of                                 code around but the secret viewer here                                 basically looks like                                 as we would expect it to look so we have                                 a select which basically selects what                                 you what put the result should contain                                 the user and the kind of clicks in the                                 from clause you define the table that                                 you want to read if we're closed for the                                 filter and the group by then tells how                                 the record should be group for the                                 aggregation so and it doesn't really                                 matter whether clicks is a file or                                 database table or stream as I said um                                 this is this is a unified API that can                                 be used both on beds input and on stream                                 input so how does it look like if clicks                                 is a file arm so we have this file here                                 on the on the top left which gets                                 translated in some kind of table                                 representation then we call this query                                 on that and the result is here so on on                                 the on the right hand side this is                                 basically what you would expect security                                 do and what do we do if we get more                                 click data so click click debtors you                                 like produce enter stream what do we get                                 it if we get more click data well we                                 have to query run the query again so                                 this is how you would do it basically in                                 a bachelor right either you collect the                                 data like an invention for each day and                                 then your every day you run the query                                 and yeah so this is how it would be done                                 in a in a batch word but what if clicks                                 is represented the stream so you don't                                 write it to a file but we want to ingest                                 I'm basically a directly the stream and                                 compute the the results based on the                                 stream we of course want to have the                                 same result if we directly ingest the                                 stream into into the query as if we                                 would first write it to a batch file but                                 then the question is how well does                                 actually like sequel work on on streams                                 so                                 sequel was not really designed for                                 screams so if you look at the concept                                 you see relations are like bounded sets                                 screams of infinite sequences the                                 database management system is assumes                                 that it can access all data off the                                 table that it that it wants to process                                 whereas stream data raised over time                                 right so you don't know what what data                                 you have to process in five minutes and                                 a secret fury honor on a table or on a                                 database table or file returns the                                 result and it's done so it's completed                                 whereas a streaming theory computes the                                 result continuously based on the data                                 that arrives and never really complete                                 nonetheless database systems are doing                                 something like that already so or more                                 advanced database systems there's a                                 feature called materialized views which                                 are basically very similar to regular                                 views so you basically define a query                                 that on some some some tables and then                                 you have like a virtual table of that                                 but the difference between virtual views                                 and materialized views is that the                                 result of these clear is actually                                 persisted as a natural database as an                                 actual table inside the database and                                 this feature these materialized views                                 are usually used to speed up analytic                                 appears and but this also means that the                                 database system has to make sure that                                 this materialized view is updated or as                                 kept consistent with the base levels so                                 if I would define a materialized view                                 and I update the base tables then the                                 database system is make sure that these                                 updates are basically used to also                                 update the materialized view and this                                 this whole maintenance of materialized                                 views is very similar to actually                                 processing sibilant streams so if you                                 think about it the base tables the base                                 table updates like the insert delete or                                 update statements they go to against the                                 base tables at some kind of a stream if                                 they                                 about it and then the database system                                 has to basically use or process to view                                 these update statements and based on                                 these update statements has to update                                 the materialized view and this is                                 actually pretty much what simulant                                 streams is about in flink we recently                                 added this feature of continuous queries                                 and the concept of this also called                                 dynamic tables dynamic tables are                                 changing over time so you have a heavy                                 table and as more data arrives or from                                 from a stream these tables are changing                                 and you can very dynamic tables and such                                 a query on the dominating table produces                                 another dynamic table and this dynamic                                 table is updated based on the changes of                                 the input data these curious                                 discontinuous gears and dynamic table do                                 not terminate they're basically just                                 listening to the to the updates of the                                 input tables and then keep the result                                 table consistent with the changes on the                                 with it with the changes on the on the                                 input tables                                 yeah so other queries that we rerun                                 conceptually run on these dynamic tables                                 however dynamic table is just a just a                                 logical concept here                                 so in fact linked internally doesn't                                 really produce this dynamic tables but                                 it's it's a nice mental model to think                                 about how how these continuous queries                                 are processed in order to integrate that                                 with a stream processing we needn't need                                 a way to turn a stream into a dynamic                                 table and later turn the dynamic table                                 back into a stream so it basically looks                                 like this where we have a stream first                                 it's logically converted into a dynamic                                 table then we define or rendered                                 continuous query which produces a new                                 dynamic table and this result table is                                 later converted back into a stream                                 how can we convert a stream into a                                 dynamic table well there are a couple of                                 ways to do that one is the so called                                 append mode we basically have a stream                                 of events arriving and each of these                                 events is simply appended to this                                 dynamic table so this dynamic table is                                 then continuously growing it grows as                                 more data arise from the stream and we                                 simply each of the of the events that                                 arrives on the stream is just appended                                 to the end of the table                                 another mode to turn a stream into a                                 dynamic table is the absurd absurd mode                                 so here the input data which has a                                 certain certain schemer has a composite                                 key or has a key or composite key some                                 some key attributes and all records that                                 arrived from the stream either inserted                                 if we've never seen that key before or                                 we update the existing record with the                                 same key so this is C here we have this                                 this streaming of six events                                 the first one is has the user one with a                                 name Mary and later this this user with                                 with ID you want is overwritten by by                                 later later events that arrived on the                                 stream so once we have this this dynamic                                 table defined the question is how can we                                 evaluate a query based on that that is a                                 clear it against such a dynamic table                                 and this basically looks like this so if                                 we take this even a little bit                                 simplified example here of a simple                                 group account theory again this is the                                 the schema of a click stream here and if                                 now the data arrives in the in the                                 clicks table on the left hand side                                 this query will then update the result                                 table on the right hand side so if we                                 get the first record on the input clicks                                 table we'll get immediately see that the                                 result table is updated so as more data                                 arrives we update the result table so                                 with a                                                                  uu                                                                      a count of                                                            another record arrives with you                                      update the count here                                             basically what what the query internally                                 does it has the the result table                                 internally it keeps that estate in flink                                 and it's more data arises if more change                                 data arrives it takes the the current                                 state of the of the result table and                                 uses the update on the base server to                                 also update the result and if we now add                                 more data the counts of this dynamic                                 result table evolve one thing that is                                 important to note here is that the rows                                 of the result table are updated so this                                 table is really dynamically changing                                 it's not just appended it's really the                                 rows are of the result table are updated                                 and this is something that we later have                                 to take into take into account so this                                 is a simple query now you might ask                                 yourself well some processing is all                                 about windows right so here we don't see                                 any windows so how how do windows do                                 winners relate to this and flink sequel                                 and also the table API also supports                                 different types of windows this is one                                 example where we do a tumbling window of                                 one hour on the click stream and here                                 with basics don't want to count how many                                 links user visited since we started the                                 query which is basically what the other                                 query did but here                                 want to count per for each hour how many                                 links is user visited so we say again                                 scan these links this clicks table then                                 we define a window and the window is                                 defined here is a tumbling window over                                 one hour on the time column which is C                                 time here and the terminal window is                                 basically just a just the window which                                 is evaluated every hour so we have a                                 window from from                                                     from                                                                     I'm here on W which is an alias that we                                 assigned to the window that we defined                                 before we want to group I in addition on                                 the user field and then we say ok the                                 result that we want to compute from this                                 is we want to have the user we want to                                 have the end timestamp of the window and                                 we want to to count how many links a                                 user visited within this window if you                                 look at the secret fear it basically                                 looks very similar in the last release                                 of carrot side care that edit these                                 window functions and which we can add or                                 group window functions that can add to                                 the group by clause so here we see a                                 tumble we give the time attribute see                                 time and then we define how long the                                 window should be and there is another                                 function called tumble end which is                                 function that returns the end timestamp                                 of the window here and apart from that                                 the query is very similar to to what we                                 seen before so we say group by tumble                                 end user from clicks and again we want                                 to have the user end times of the window                                 and also the count and the count of the                                 links that are the user visited so if we                                 run such a theory it looks like this we                                 have here a couple of clicks in the                                 range from                                                            all of these are aggregated by the query                                 into these two records                                 you see user u                                                           or the green green fields                                 user YouTube just visited one and the                                 next hour from one to two user to again                                 visited just one link and is a three                                 visited two and so on so here we can                                 basically compute these aggregates as                                 they go and an important thing to note                                 here is that the rows are appended to                                 the results table so here once an entry                                 has been added to the to the result                                 table it's not updated anymore so this                                 is where this query differs from the                                 occur before we had where we emitted a                                 row which was later than updated okay so                                 if you now want to turn a dynamic table                                 into a stream again what do we have to                                 do so here the important point is that                                 we somehow need to deal with the updates                                 on the dynamic table right so if we have                                 a table which updates its rows we cannot                                 simply we somehow need to encode these                                 updates in the stream that we send out                                 to the don't downstream system if we                                 have a table which simply append as in                                 the end the window at example here then                                 we can scan simply if we know we will                                 never update any record that we have                                 computed we can simply omit this to a                                 stream and omit the records of the                                 stream and we are we're good                                 however if you know that we somehow are                                 that everything that we computed might                                 change in the future we someone need to                                 need a mechanism to encode that and the                                 way that flink does this is kind of                                 inspired by databases or by by the                                 logging mechanisms of database systems                                 and database systems use these locks                                 basically to to be able to restore                                 restore databases and tables in case of                                 a failure to have the data consistent                                 and there are two techniques basically                                 one is the redo lock which stores all                                 or new records that should be added to                                 the database arm in order to be able to                                 redo changes in case of a failure in                                 case these these changes have not been                                 materialized to the disk yet when the                                 failure happens and the other one is a                                 so-called undo log which as to us the                                 old records that were changed in the                                 transaction in order to be able to undo                                 changes in case will change the database                                 has written to the table but not                                 committed the transaction yet when a                                 failure happened so we kind of like use                                 the same terminology here for for                                 storing old and new records yeah so one                                 technique to convert the dynamic table                                 into a stream is the what we call a redo                                 undo conversion and in this in this                                 example here we have again this simple                                 simple query which is the non windowed                                 group ikon theory and we have the input                                 data here on the left-hand side we say                                 use ID                                                                 we see that the first entry here we                                 basically omit two types of messages                                 into to the resulting stream and these                                 matches messages are insertion or                                 deletion messages insertion is marked                                 here with a plus sign and deletion is                                 minus sign so if you get the first                                 record here it goes into the query which                                 produces conceptually this resolves                                 dynamic Taylor and if we convert this                                 result dynamic to every packet or stream                                 we omit this insertion of user ID                                       count equal                                                             we don't do any updates we again insert                                 something in this and this dynamic cover                                 but then if we get the threat record                                 here with user ID                                                    have to update a record that we                                 previously computed and this is then                                 done by invalidating the record that we                                 previously admitted so we have mine                                 you won one which invalidates the first                                 record that we emitted and then we add a                                 new record you a d                                                       this is how we encode the updates into                                 this stream and then the downstream                                 system would meet would need to                                 basically be able to interpret all these                                 messages and accordingly update it on                                 its own website this is one way to                                 include updates in a stream but the                                 drawback here is that we get quite a few                                 records right so whenever we update                                 something or whenever we change                                 something we have to admit omit to two                                 records or a deletion at an insertion in                                 many cases it's also possible to to not                                 do that and just omit one one record and                                 that's the case when the when the result                                 is our resulting table has a unique key                                 and in this case the user ID is a unique                                 key because we only have one single row                                 for each user ID because we grew up on                                 that and in this case we encode the                                 outgoing outgoing records in by three                                 different kinds of messages that we made                                 one is again an insertion which is very                                 similar as before but then we have an                                 update and delete message and these are                                 always refer to their to the to the key                                 attribute so when we get the first entry                                 here with you one we again doesn't do an                                 insertion the same for you too but then                                 if we get the the this third record was                                 just a second for u                                                   update a record and then we say okay                                 here for this key for key u                                             update this record to the record where                                 the count is two and this is how we can                                 then encode these updates in an outgoing                                 stream if we know that there is a unique                                 key on there on the data results table                                 so um can we run any crew on a dynamic                                 table uncertain not so there are two                                 types of constraints that we have to                                 consider when we translate a secret                                 clear on the stream so you cannot just                                 simply                                 run any arbitrary query on a on a stream                                 and these constraints are in space and                                 computational effort basically so the                                 query that I've shown before the run                                 example that we've that we've used all                                 the time here it's basically a query                                 that you cannot simply run in a                                 distributed way unless you know that the                                 that you're the amount of users you have                                 does not does not significantly grow so                                 the problem here is that as I said                                 before when the cure is computed you                                 have to check the current current state                                 of the result table and update it with a                                 result which means that the results                                 table has to be kept in memory by the                                 better streaming theorem and so this                                 this state might grow over time if for                                 instance we would not group by here on                                 the user ID but we would group on                                 something like a session ID which                                 session ID which is unique then we would                                 most likely run into a problem at some                                 point in time because we get a new                                 session IDs and new session T's we                                 accumulate more and more state our                                 internal state grows and grows and grows                                 as most session IDs arrive and at some                                 point in time we might run out of might                                 run out of disk space or memory                                 depending on what kind of step back and                                 you choose so we have to make sure that                                 that the state of a query that the Cure                                 internally uses to to process its result                                 does not does not grow to large and the                                 other input we had the other constraint                                 that we have to consider is as I said                                 this computational effort if you look at                                 this theory here where we have some kind                                 of a users table and this users table                                 records the last login of a user and we                                 constantly want to know okay we want to                                 rank our users by the last log and we                                 also want to have always want to have                                 like the user that locked in most                                 recently to be on top of the of the                                 results table or have a rank that is                                 rank one then this would not work well                                 if you would evaluate that in a stream                                 because whenever we get a new record                                 with a new login we have to change all                                 other records so the amount of work that                                 we have to do for a single update is                                 very is very very big and this is                                 nothing that would work very well in                                 practice so where we can do something                                 about the first problem with the state                                 the the second the second problem of the                                 computation effort is a lot more yeah                                 it's basically based on the semantic                                 system of the query and you cannot do a                                 lot about it so what can we do to burn                                 the state of a query so one thing that                                 we can do is to arm play a little bit                                 with the semantics of the query so we                                 could for instance at a predicate here                                 into the where clause we say for                                 instance last click time interval one                                 day which basically is something that or                                 a function that I just made up but this                                 could indicate that I'm only interested                                 in the in the data of the last day so                                 we're basically when I compute my query                                 I would always compute the query on the                                 last                                                                   the stream evolves I always keep Basel                                 II only the last last                                                   would then also automatically limit the                                 size of the state that I have to have to                                 worry about when I evaluate the query                                 another option to do this is to                                 basically trade accuracy of the result                                 for for the size of the start for the                                 size of the state what I mean by that is                                 actually if you want to compute a result                                 of this query without the way across                                 here absolutely correctly there is no                                 way around to just keep the state for                                 for each user arrives at any point in                                 time they might arrive a record that                                 accesses                                 any user and then you would need to                                 update this then you would need to                                 update the corresponding record in the                                 end your results table however if you                                 know that over time some of the records                                 become or some of the grouping Keys                                 become become stale or inactive as for                                 instance when I would group on on a                                 session ID instead of a user then you                                 can basically say okay I want to remove                                 records which have been inactive for                                 let's say                                                               bound perky and whenever a key is                                 updated I reduce the timer say okay if                                 this is not updated within                                               I can just remove the state if then it's                                 at some point I receive another record                                 for this for the same key that that I                                 just removed that can like have like                                 because in that case this would the                                 system would basically see this as if                                 the just received record would be the                                 first one ever observed for the scheme                                 but depending on the use cases set if                                 you something like a unique session ID                                 as a key this works very well to bound                                 the state so um what is the current                                 state of all of this the this relation                                 ideas are rapidly evolving we've got                                 many contributors contributing features                                 and features and improvements to these                                 API so we get more more built-in                                 functions we get also core functionality                                 being added and there are so many many                                 people asking for these features on the                                 on the mailing lists the API is actually                                 used in production very large scale at                                 Alibaba and Alibaba is also one of the                                 contributors pushing us for these                                 features and collaborating with us um                                 recently linked                                                         added a couple of exciting features                                 there among those are window records for                                 group I so basically this comber keyword                                 that have just shown it's no                                 available in                                                         popping windows or our certain windows                                 so you can say I want a session with a                                 gap of                                                                 the the tablet here will compute                                 aggregate for all records that are not                                 more than the gap apart from each other                                 we also have these non-winner aggregate                                 with update changes in flick                                         user-defined aggregation functions so                                 what kind of applications can a bit with                                 this actually so our very calm use case                                 is something like the continues ETL why                                 where the query continuously ingests the                                 data you play at transformations and and                                 winner gets on it and then you write the                                 data out to to do some kind of files to                                 another Kafka topic that is later                                 consumed by some some other processes                                 for the data or you write into a                                 database or HBase or whatever so this is                                 basically kind some kind of like a yeah                                 ETL ingestion transformation types of                                 workloads that can be very easily                                 specify with a sequel or the table API                                 so you don't need to hand folders up for                                 that it's much easier to use these api's                                 further for these kinds of                                 transformations another thing that you                                 can't go through this is basically send                                 it around this feature of updating                                 updating tables and very very common use                                 cases here are dashboards or reporting                                 or but also on these types of event                                 driven and event-driven different                                 applications that always want to have                                 low latency access to some kind of state                                 which is a continuous computed from from                                 arriving data so you have the query                                 which maintains this kind of                                 materialized view and this view can                                 metric can be materialized to Cassandra                                 or another key value store or relational                                 database or compact                                 Casca topic and then your applications                                 basically always go against these this                                 key value store or this data store and                                 have the results which are updated as                                 the stream is consumed where they buy                                 they buy the continuous query and later                                 also in a later version of link all                                 these results will also be able to be                                 maintained in curable state so in this                                 case you don't even have to write use                                 out into Cassandra in external data                                 store you can just keep it inside of                                 link inside of links gerber state and                                 then the applications can go directly                                 against this state and what this                                 basically means is that Flinx somewhat                                 becomes kind of like a database because                                 it get exchanges as a stream ingested                                 into in the flink link update compute                                 the result keeps the updated state                                 inside of inside of the curable state                                 and then applications can go directly                                 against that yeah so um table a pn                                      support many use cases it's a high-level                                 specification and declarative so it's                                 fairly easy to use everybody knows how                                 to use evil it's optimized it's                                 efficiently executed and you can                                 actually do quite a bit of quite a few                                 of things because the table API and c                                 will also support user-defined functions                                 for scalar functions table functions                                 aggregation functions so you can also                                 encode a lot of functionality inside of                                 these UDF and the the possibility to all                                 have tables which which emit updates for                                 for computed results and dematerialized                                 ed to an external key value store this                                 is enables a lots of lots of very very                                 very exciting applications and yeah if                                 you have some kind uses that might fit                                 into these these things I encourage you                                 to just try out in case you haven't                                 heard this so data Adams is organizing a                                 swing conference in September this year                                 the core for paper ends soon and in                                 about a week or so so if you have a                                 topic that you'd like to talk about                                 please submit and submit a talk there                                 here's another famous plug arm I'm a                                 quarter of this book stream processing                                 an effective link it's available on all                                 rightie early release so you can get it                                 there and we yeah it should be if                                 everything goes goes according to plan                                 should be available sometime early next                                 year completely but with the early                                 release you have you can you can read                                 what we have right now alright and                                 finally we also hiring so in case you                                 find any of this interesting or you'd                                 like to work directly on fling                                 come and talk to me thank you                                 [Applause]                                 [Music]                                 [Applause]
YouTube URL: https://www.youtube.com/watch?v=c8J1O4qZRLA


