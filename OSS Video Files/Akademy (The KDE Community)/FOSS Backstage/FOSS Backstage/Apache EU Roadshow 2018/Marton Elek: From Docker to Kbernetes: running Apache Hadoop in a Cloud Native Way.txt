Title: Marton Elek: From Docker to Kbernetes: running Apache Hadoop in a Cloud Native Way
Publication date: 2018-12-05
Playlist: Apache EU Roadshow 2018
Description: 
	Kubernetes slowly become one of the most popular container runtime environment while Hadoop has already been widely used open source bigdata platform since a long time. The questions is here: how can we use the new cloud-native toolset to administer and manage Hadoop based clusters? Is there any benefit to run Hadoop and other bigdata application on top of Kubernetes? In this presentation I will show that Hadoop is not a legacy application but it could be run very easy in cloud-native environment thanks to the generic and distributed by design.
Captions: 
	                              yeah welcome everybody I'm Martin Alec                               and I would like to speak about the                               connection between head to Ozone and                               kubernetes and oh yeah I can do it I am                               an Apache committer interactive project                               which is incubator project and this is a                               ruffed protocol implementation which                               could be embedded to somewhere for                               example it's embedded in in a new                                subject of Hadoop this is the Hadoop                                ozone or a Hadoop HDDs I also have some                                playground project where I'm trying to                                continue as the all word I started it                                with Apache Big Data projects and I'm                                running them in multiple environment                                with a she called Nomad kubernetes                                Dockers form docker compose and in a                                different way                                ok what I would like to speak about the                                kubernetes and hadoop and the connection                                between them and this is not just a                                one-way connection so it not just about                                running Hadoop in the kubernetes but                                what I believe that opposite direction                                it's also very useful Hadoop is a                                storage system right and at least the                                HDFS part is a storage and I think it's                                a storage is a very valuable component                                in the in the kubernetes it ecosystem so                                this is what I would like to speak about                                the but first of all we need to clarify                                why do we need to check kubernetes at                                all so why could be it interesting we                                know that kubernetes is the new unicorn                                and it's in the top of the hive so why                                we would like to start head up there                                well we need to check the kubernetes and                                what kind of components are run usually                                in kubernetes and I think we can agree                                that one of the reason of the popularity                                of the kubernetes is that it's very                                powerful way to run micro service based                                application and the micro service based                                application are run very well so the                                next question what are the micro                                services so what is this architecture                                well it's it's very good but there is no                                exact definition but I just                                got one from Martin fuller and according                                to this definition the my cursor is                                something like where we have small                                services own processes and lightwei                                communication so small service on                                process pipette communication small                                service on processed lightweight                                communication this is a Hadoop cluster                                with all of the components and what can                                we see on process is lightweight                                communication and more or less small                                services is different from the distance                                very very very check it from so it seems                                that the head open at the kubernetes and                                the micro services it's a very good pair                                and the Hadoop and the micro service is                                almost it's at least it's immoral                                so the Ori so my guess that it should be                                useful to check them because my                                expectation is that also could be useful                                for for Hadoop if it's very similar to                                the micro services so we need at least                                to check it and this is why we are here                                to check how the Hadoop could be run in                                a in kubernetes but first of all what is                                Apache Hadoop do you use Hadoop or do                                you know how very good I have a complete                                head of training just in                                              what is Hadoop this is the exactly the                                same way as as I explained Hadoop to my                                grandparents that what I'm working on                                every day so it's it's a complete                                training actually so here it is the big                                data big data to set actually so next                                question from my governor that what is                                Big Data yeah this is exactly the same                                as the smaller that just bigger right so                                what is the small data yeah that's an                                easy question the small data is the                                excel sheet right I'm pretty sure that                                small data equals an excel sheet so big                                data is an extra exertion which can't                                fit on my computer right that's in the                                picture so how can I do calculation with                                this well first I need a lot of machines                                and I need to split or cut the excel                                sheet to smaller parts and one of the                                one of the Hadoop sub-project the HDFS                                is just splitting the data and replicate                                them and handling all of that this is                                one problem                                 the other problem is                                 no the calculation it's a little bit                                 more difficult for example if you would                                 like to calculate the maximum number in                                 this excel sheet or in this data set you                                 need to calculate the maximum at each                                 split and after that you need to                                 calculate the maximum of maximum so this                                 calculation is handled by arm which is a                                 scheduler and it could just execute all                                 of the calculation in the in the right                                 place and we also have the MapReduce                                 which is a library to write this                                 calculation this is not the only one                                 which could be used although other                                 frameworks or products such as spark or                                 fling could be used Oh what is                                           or                                                                     component in in Hadoop and ozone and                                 actually two new components ozone and                                 HDDs which are merged just two or three                                 months ago so the next question watch is                                 what what is Apache Hadoop ozone or HDDs                                 well for understand that one we need to                                 understand first what is the HDFS okay                                 so for Hadoop HDD HDFS as we discussed                                 we have big data and we just splitting                                 them and trying to distribute them on                                 the data nodes we have one master node                                 or two master know the name node and we                                 have worker nodes so there are two main                                 responsibilities of the day of the name                                 node one is just a map between an ID and                                 the location of the data so this is an                                 idea of a big binary data so a byte                                 array or something like this and we have                                 a map and if you say that this is the                                 first block then it should be replicated                                 to somewhere ideally on multiple machine                                 and there is an other map where we met a                                 file to multiple book IDs one or                                 multiple books this depends the size of                                 the file usually we're just flipping the                                 file and and replicating the block so                                 this is the name node this is in the                                 name node currently and I think it's                                 very well known that there are some                                 limitation here because both of the maps                                 are handled handled in the same company                                 so the idea is just we need to cut them                                 the separate them to two separated                                 component and the bottom one is the                                 Hadoop hdds this is one of the new sub                                 project and their idea is that we need                                 some very low level service just to                                 replicate binary data it doesn't matter                                 if it's a file data or an object or                                 something just the binary data in a                                 block should be replicated on top of it                                 it's very easy to implement additional                                 services like a file file services as in                                 the HDFS but it's easier to start with                                 some even a more simple service like                                 object store it's very similar to the s                                  object store we have buckets and no                                 files just keys so this is the Hadoop or                                 zone this is an other sub new                                 sub-project and also on uses the hdds                                 lower-level layer in fact it's not just                                 a separation but there are some other                                 optimization because until now we had a                                 problem with the many small files in                                 HDFS I don't know if you if you know it                                 but the problem is that if there is a if                                 there are many small files all of the                                 need tunnels are reporting just the                                 block IDs at all I have it all I have it                                 so the idea is that we can report it in                                 bigger groups that oh I have this group                                 of blocks so we can use this super block                                 mapping and we can use this super block                                 here okay so we can impact implement                                 very easily a object store like s                                      this is not the only one what clanky                                 what we can implement because we have                                 the lower layers so everything is                                 possible we have the object store we can                                 implement an other mapping to map                                 something from file path to a binary so                                 this is the HDFS it's also could be                                 refactored                                 we can provide more role                                 roll devices this is the quadricep                                 project it's still on a on a feature                                 branch and long-term I'm not very                                 familiar with hive and age weights but                                 long-term I think they also could could                                 be improved to use just the lower layer                                 instead of the fiber because usually                                 hive                                 don't don't need doesn't need any any                                 fire just a binary which is replicated                                 okay this is this is the Apache ozone                                 and and hdds so let's go forward to the                                 rocker so next question of how can we                                 dr. eyes or how can how can we continue                                 eyes it well it's easy I don't know is                                 it familiar for you this creating                                 dockerfile so this is just the base                                 image I am putting the Hadoop and done                                 my that my original title for this                                 presentation was how to dr. I love to                                 contain eyes Hadoop but idea is just it                                 one slide so done nothing yes so that's                                 not the not the tricky part so what is                                 the tricky part                                 yesterday I had another presentation                                 about the tricky parts and my statement                                 was that you know this this is every                                 household appliance is tagged in the                                 European Union and just to make it                                 easier to compare and yesterday I said                                 that we need something similar for all                                 of the containers environments as well                                 just to make it easier to compare if                                 kubernetes is good enough or not so I                                 have some label for kubernetes and all                                 of that and so the tricky part is not                                 the container is Asian the tricky part                                 is solved a configuration management the                                 provisioning to to provide some kind of                                 networks between the containers the                                 multi-tenancy                                 so there are a lot of problems it should                                 be sold with containers but today we                                 won't check all of the technical details                                 instead instead of that we will check                                 from the user point of view so we have a                                 Hadoop issue where there are some design                                 documents and we have two main use cases                                 one for the developers one for the users                                 are there any developers here okay users                                 okay so what is more interesting for you                                 I have loved both or we can just try it                                 out and after that hi Candace                                 okay so let's start with it maybe maybe                                 with a user part so that's a Hadoop                                 documentation I don't know if you know                                 there is a new feature that route are                                 based Federation in Hadoop                                               actually just one part of the                                 documentation so if you are interested                                 about the Rooter bass Federation it's                                 easy very easy you need just read                                    pages of the documentation check all of                                 the properties and try to find the right                                 properties and it's especially complex                                 because Tudor bass Federation works                                 between                                                                  thing what you need is just create one                                 cluster and other cluster both of them                                 have to name knows data nodes you run                                 all through that so it seems to be                                 complex right what about if somebody                                 just interesting about what is this                                 Reuter based Federation and my vision is                                 that we we need to provide docker                                 compose files and actually this is oh I                                 can can I oh ok so this is in my project                                 but some of the stable features are                                 already sent to the Apache so my vision                                 is that we need to provide some kind of                                 broker compose file to with every new                                 features for example this is the editor                                 coding and by default it's easy to start                                 doc our cluster so it you don't need to                                 understand that you can see that it's                                 almost the same thing just three pending                                 for all the components we have a image                                 we have some startup script so and we                                 have all of the configuration as                                 environment variables here with some                                 naming conventions to save it so but my                                 idea is that it shouldn't be understood                                 you can just use it as a docker compose                                 definition                                 you can start a local cluster and you                                 can play it and look you can try it out                                 with just dr. Campos up and you have a                                 local cluster even with the Oh with this                                 claw oh yeah                                 this cluster could could be where is my                                 cluster                                 oh so this class that could be started                                 in my machine so it's                                              components and it's very very hard to do                                 where you cluster but with this come                                 doctor is a doctor eyes approach it's                                 very easy to do start ok let's go back                                 to the full screen ok so that was the                                 the user based approach but the same is                                 true for the development baby are not                                 involved on the development even if I                                 strongly recommend that it's it's very                                 good part but we have similar definition                                 of the clusters with docker and we can                                 just try it out very fast the every new                                 patch and after that it's yeah it's                                 faster to to apply the patches and I                                 have another vision that you know that                                 there are some there are votes for the                                 before the head releases and usually the                                 committers are just checking out and                                 doing some smoke tests so all of the new                                 head release is its are very stable and                                 all of the new hadoop versions could                                 very stable calculate the word cons                                 usually and the terasort because these                                 are the features which usually are                                 tested and my idea is that we can with                                 this docker compose based approach we                                 don't need many machines but we can just                                 test it locally and and start a cluster                                 and the wall head loop could be more                                 stable okay so that just technical                                 details that the to approach one is the                                 development we don't need anything in                                 the docker container we can just met the                                 latest build version and yeah acceptance                                 test and check the version and the other                                 one is just provide easy to use                                 environment for the end users ok so                                 that's the power of the county                                 and I think so even just locally it                                 could be very useful let's back to here                                 so what's next                                 it's very good but there is one problem                                 with that that it could work only my                                 machine so the idea is that I would like                                 to use the power of the containerization                                 in a cluster so what can I do                                 yeah kubernetes is the new unicorn so I                                 would like to try it out we already                                 discussed why it could be a good idea so                                 next question do you know kubernetes                                 okay more or less I have a                                            complete kubernetes training so never                                 mind if you if you don't know but sorry                                 if I just over simplify something so the                                 problem was that I have multiple nodes                                 right I couldn't use the docker kompis                                 based approach when I can just start                                 more and more continents on my local                                 machine so I need something and the                                 kubernetes it first of all it's a                                 scheduler so I can just say that oh I                                 need a container tree started somewhere                                 and I'm not interested where the                                 containers will be started they will be                                 sorted by the kubernetes or another two                                 containers another application or a                                 third one and the big benefit of the                                 kubernetes comparing for example to the                                 normal scheduler that it's not just a                                 scheduler and it could handle together                                 containers as applications so it could                                 provide additional features like network                                 storage for example the same config                                 files could be magically mounted to the                                 containers without any other thing or                                 there will be a virtual network between                                 the containers okay and all of the                                 complexity of the kubernetes so the                                 resource is the resource definition are                                 just most of them are just definitions                                 or rules that whole the company                                 containers should be executed for                                 example the demons that is a rule to                                 start a start a container instance on                                 each node or the replica set it just                                 rule to start a given amount of                                 containers somewhere doesn't matter                                 where oh so it seems to be very powerful                                 so what's wrong with the kubernetes well                                 it's not nothing is wrong but the                                 problem is that way how the kubernetes a                                 thinking it's a little bit different                                 than how the hadoop things imagine a                                 back-end application so this is a                                 back-end application and the kubernetes                                 could scale it up so it could start                                 multiple back-end and I have a front-end                                 application and the front an application                                 it's it's very dummy it would like to                                 just connect to the backend but no I                                 have multiple back-end so kubernetes                                 provides a load balancer and it passes                                 all of the all of the incoming rack                                 requests to one of the backends well it                                 seems to be okay there is only one                                 problem that those containers have no                                 network identity no dns nothing the d                                 end only the kubernetes service the load                                 balancer itself has network identity and                                 what do we need for hadoop and typically                                 for any other mmm stateful distributed                                 application we need network identity                                 because we have already solved the same                                 distribution or application problem and                                 for the replication we need to                                 communicate between data node Eneida                                 name node or between the client and the                                 data and also we need stable network                                 identity stable dns which is not                                 available except for the Tait stateful                                 set in kubernetes so the stateful set is                                 the most common way to run Hadoop and                                 similarly stable to the application and                                 yeah it could be used and it's very                                 useful much for example state was set                                 you can't do easily the same what you                                 can do with the demos that for example                                 to start a worker node at each physical                                 node okay so it seems to be working but                                 the benefits to run Hadoop where we can                                 say a lot of things about about                                 kubernetes and then but there are my two                                 favorite ones the ecosystem and the                                 flexibility so let's check a example                                 okay like to monitor Hadoop with bromate                                 house do you use primate house okay it's                                 a it's a monitoring to some something                                 like this so monitoring monitoring tool                                 you have a lot of data and it just can                                 display it and you can calculate a lot                                 of things so it's very easy by default                                 there is a pro metal server and it pulls                                 the other components and there should be                                 an HTTP endpoint here which prints out                                 all of the metrics so that's what we                                 need okay yeah but what we have is just                                 a good old hadoop application there is                                 no HTTP endpoint and the other problem                                 is that pro Mattel's doesn't know where                                 are the HTTP endpoints so what was the                                 two benefits the flexibility and                                 ecosystem so the ecosystem part is that                                 no of this I think nobody can introduce                                 a new development tool without good                                 support right because the it is just so                                 I like used and I think sooner or later                                 rather sooner that will be the same                                 situation for kubernetes so just newer                                 and newer components are supporting                                 kubernetes pro metals is also under the                                 cloud native fundage foundation so it's                                 very easy but pro metals supports                                 kubernetes and it's very easy so it can                                 just check the kubernetes api and                                 collect all of the available HTTP                                 endpoint and this is a kubernetes                                 resource different definition you don't                                 need to understand you need just to                                 check the difference so where there is                                 the one difference yeah so the older                                 what we need is just add to annotations                                 because kubernetes could understand and                                 annotations and based on these                                 annotations they can pull the containers                                 okay still we have a problem so the pro                                 Mattel's knows that there should be an                                 HTTP endpoint but there is no HTTP                                 endpoint so the next is the flexibility                                 so until though I said that we are just                                 starting containers in the kubernetes                                 but in fact we are starting ports and                                 not containers the pod is the kubernetes                                 unit for the container and usually it's                                 one container but it also contains                                 something like for example volumes so                                 one container and I would like to mount                                 a configuration file to the container                                 and in some strange cases it could be                                 multiple container in the pod this is                                 the so-called                                 side core citecar pattern pattern so                                 imagine that this is my head up                                 container and you can add any additional                                 container and a strength of this                                 approach that both of the containers can                                 see the same network interface can see                                 the same volumes and after kubernetes                                                                                                         very easy to create something which just                                 connects to the good old java process                                 and publish all of the metrics in an                                 HTTP form to promote house and this is                                 what can we do and a it's just a few                                 lines of batch code but we can add in a                                 different container and only thing what                                 we need is just at these two lines for                                 the kubernetes resource definition so                                 that's the flexibility that without                                 modification I just added a monitoring                                 tool and the same is true for example                                 local action so you don't need to create                                 an own local action because                                             local actions are already integrated                                 with kubernetes                                 okay so we have kubernetes next next                                 question that                                 yeah it's Hadoop Club native or not yeah                                 I think we so that it's very easy to                                 start the Hadoop and it's very powerful                                 but there are some problems but minor                                 problems because Hadoop is designed to                                 be a distributed application actually so                                 it's very easy to run in kubernetes but                                 there are some areas which can be                                 improved for example this DNS handling                                 it could be turned off                                 but it's very hard to turn off                                 everywhere and there is other things                                 that the configuration loading could be                                 improved to make it more dynamic or yeah                                 the UI is just as an example this is the                                 UI of the ozone and it could work over a                                 reverse proxy so it's very typical that                                 in a cloud native environment you can                                 see the UI's over a reverse proxy and                                 currently the good old Hadoop UI the                                 name node UI for example doesn't work                                 yet but hopefully it will be fixed soon                                 ok so that was the one direction so                                 Hadoop could be started in kubernetes                                 and it's very useful or very powerful to                                 run in kubernetes but there is an other                                 direction so what about kubernetes is it                                 worth or maybe we need just kubernetes                                 and we don't need hadoop anymore right                                 well the Hadoop is multi-tool                                 sub-project so you're in HDFS and I                                 think the storage part could be very                                 useful in in in the kubernetes part so                                 that was the picture of the kubernetes                                 and we discussed that there are some                                 additional services for example the                                 sword storage in fact the storage layer                                 it's pluggable ok so you can use                                 multiple storage provider and after that                                 you can see the same storage from each                                 container ok and we also discuss that                                 with Hadoop ozone we have this low level                                 block layer the data layer where we have                                 just bytes which are replicated and we                                 can create additional layers for example                                 this Quadra which could publish the data                                 over any kind of interface for example                                 as a x                                                             multiple storage NFS I scuzzy local                                 storage a lot of a type of storage what                                 Quadra supports is the ice casting                                 interface so over ice cozy we can                                 publish the available capacity to                                 kubernetes as well ok how does it look                                 like this is again kubernetes resource                                 different definition you don't need to                                 understand all of the details that what                                 something like this is a container                                 definition here and we are just running                                 a cat command to to wait for for                                 attaching volumes and yeah that's that's                                 the important part                                 so I can say that I need some more um--                                 from a persistent volume cream so the                                 persistent volume claim is the important                                 part so I I can say that oh it would be                                 great if somebody would have an one gig                                 storage and the add there should be a                                 add the connection between the claim and                                 pod it's the name but the important                                 thing is that we need a persistent                                 volume with the same size and the same                                 type so this is the exact persistent                                 volume and this is volume specific so                                 because it's a nice cozy volume we need                                 all of the ice cards the data which is                                 depends from or honk or Quadro cluster                                 okay but let's simplify it a little bit                                 so let's just say that we have a                                 resource for definition we have this                                 persistent volume claim and the claim is                                 born to the persistent volume okay there                                 are two type of provisioning of the                                 volumes one is the steady state ik                                 provisioning where we have                                 voyons because they are created by the                                 system administrator and when a new pod                                 request a new persistent volume it will                                 be just bound to the persistent volume                                 claim the other one is the dynamic                                 provisioning so there is a new resource                                 and we need one gig storage but there is                                 no storage but we have a component which                                 could be the Hadoop which can create the                                 volume dynamically and the volume                                 finally will be bound to the resource so                                 that's what can be done with with the                                 Hadoop at least with this Quadra so this                                 is the good old Hadoop this is the HDFS                                 part we have a master component and we                                 have the slave components and every save                                 component we can activate in this new                                 HDDs layer which is just a plug in the                                 data node so this is the master                                 component of the new HDD layer this is                                 this is about binary replication and                                 this is the object store which is which                                 is similar to the s                                                      mapping between the key name and binary                                 and this is the other system which                                 provides the raw x                                                  CAHSEE for the nodes and this is the                                 dynamic provisioner which could could                                 check the kubernetes api if somebody                                 needs new storage and can just create                                 new blob store here and just make it                                 available for older nodes so this is how                                 it works but maybe it's more simple just                                 to show oh okay maybe it's easier to ah                                 I can see it because it's here okay                                 let's just do so this is a kubernetes UI                                 and yeah there are a lot of details here                                 and maybe we don't need to understand                                 all of the things just the feeling the                                 whole how does it work so                                 we have a lot of components we have all                                 of the name node data node all of the                                 servers running in in separated                                 containers and the c block is the old                                 name of the Quadra this is the ice                                 Kassie part and and the storage part                                 okay we have registered its storage                                 class this is required where oh I have                                 where is the where is it                                 oh just a moment I have I would like to                                 turn off the subtitles subtract ah okay                                 what has been happened in the meantime                                 I lost it ah okay so we have the                                 components we have the which registered                                 Hadoop as a storage class so Hadoop will                                 provide storage okay what can we do ah                                 we can create a new resource it's very                                 similar what we so that we have                                 persistent volume definition oh it would                                 be great if food if somebody had one gig                                 storage and we have the deployment and                                 there is the storage yeah and there is                                 the volume definition and the claim name                                 is the same so I need a claim and the                                 claim defines that I I would like to use                                 one gig storage okay let's upload it and                                 hopefully we'll have some storage                                 okay and now Berenstain volume claims we                                 can see that oh there is the claim and                                 its bond so we have a volume but the                                 volume is magically created by the                                 Hadoop component this is the persistent                                 volume oh okay we have the persistent                                 you can see that is this random number                                 so I'm pretty sure data it's not created                                 by me because I can't type so long                                 random numbers okay and these are the                                 raw data to access actually the Hadoop                                 storage okay                                 but it's it's takes some time to to be                                 started big                                 we need to format one gig over the                                 network so in the meantime we can just                                 go to the console and go inside the                                 newly created container and check if the                                 c-block is started so this is inside the                                 management service and we can just see                                 that all really they are created                                 magically and without any intervention                                 ok                                 so it's created maybe we can just make                                 it more faster and go to chemically                                 still formatting for marketing for one                                 thing ok let's go to here Oh finally it                                 started                                 so we have the persist we have the                                 container and it's running oh yeah we                                 had some problem because it was too slow                                 to form up the wall volume but because                                 kubernetes Martinov just try it again                                 and again after I don't know                                           we have the new volume so we can go to                                 the newly created container which is not                                 part of the Hadoop cluster it could be                                 any kind of container and we can just                                 check if it's mounted yeah mounted but                                 not ah yeah that could be the good path                                 yeah there is a new empty storage so I                                 can just create a new file I was here ok                                 the new file is there so it's somewhere                                 in the Hadoop cluster but I can see it                                 over the x                                             moans yeah there should be amount there                                 is the mount x                                                        Milo kubernetes itself and what if we                                 can do is just delete the pod because as                                 I mentioned most of the resource                                 definition resource definitions in                                 kubernetes just rule that how the pods                                 should be created or a container should                                 be created if I delete the container it                                 says it will be recreated automatically                                 and actually it will be recreate its                                 recreated on an other machine so this is                                 one machine one hostname this is another                                 hostname                                 so it's recreated on an other machine                                 but even if it's recreated on the other                                 machine after a while because again it                                 should be on                                 seeing everything so after a while                                 mmm bonk reload ah so now it works and                                 this is this this is a container which                                 the same settings but started on an                                 other host and on the other host oh I                                 can go to the container this is a new                                 container actually with the same                                 settings and the same storage it just                                 mounted so all my file is there I'm                                 happy okay so that's what we can achieve                                 with with the new head of sub projects                                 okay stop                                 so let's go back to the presentation ma                                 that's almost everything what I can say                                 so that that's the two parts right the                                 Hadoop could be run in kubernetes in a                                 very powerful way we will have                                 out-of-the-box monitoring logging                                 everything and the kubernetes could use                                 the storage from the Hadoop HDFS cluster                                 or more precisely this HDDs cluster okay                                 so just a summary the containerization                                 it's it's powerful even if we use it                                 just from the from local docker compose                                 but Hadoop can be started relatively                                 easy in the cloud in any cloud native                                 environment but especially in kubernetes                                 and it could work yeah we can improve                                 the head up with this dns and it could                                 be even better and the other side that                                 with a Hadoop cluster we will have not                                 just an HDFS storage or Hadoop                                 compatible storage but we will have an                                 object store for object store like s                                  and we will have some role storage which                                 could be used from any other container                                 that's the vision that Hadoop is all in                                 one storage or a Hadoop HDFS together                                 with all of the new sub components could                                 be just all in one                                 all-in-one storage system okay that's                                 pretty much all I have some source here                                 where I'm just continuing the Hadoop and                                 other big data including ozone is there                                 any question                                 oh yeah it's it's just because it's not                                 not yet finished so sooner or later we                                 will have a separated page to to                                 introduce us on and put all of the                                 existing documentation you you mentioned                                 efficiency how did you compare those two                                 approaches which which approaches in                                 Cuba needs and and natively in a node I                                 think it's almost the same so it's very                                 low overhead to run something in the in                                 the container it's another question the                                 storage so it's that the efficiency of                                 the storage so yeah it's not not faster                                 than the local SSD right so it's not for                                 that but it could be there are some                                 caches and it's under development                                 actually so the first also release ozone                                 will be released                                 midsummer hopefully so yeah it's another                                 question that the speed of the storage                                 and yeah there are some use cases when                                 when the local storage it's more faster                                 and more useful but it won't be                                 replicated so a few not replicated data                                 it may be a little bit slower but it                                 will be replicated so that's the deal                                 yeah but just to put container it's it's                                 no problem so yeah even just in this                                 laptop I started                                                  something like this and                                                  something - okay any other questions                                 alright then thank you thank you very                                 much                                 you
YouTube URL: https://www.youtube.com/watch?v=ySDjSLeWzNw


