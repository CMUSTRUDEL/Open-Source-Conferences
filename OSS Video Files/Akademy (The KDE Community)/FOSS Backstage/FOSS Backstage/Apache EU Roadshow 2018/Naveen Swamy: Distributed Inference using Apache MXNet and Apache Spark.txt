Title: Naveen Swamy: Distributed Inference using Apache MXNet and Apache Spark
Publication date: 2018-12-05
Playlist: Apache EU Roadshow 2018
Description: 
	Deep Learning has become ubiquitous with abundance of data, commoditization of compute and storage. Pre-trained models are readily available for many use-cases. Distributed Inference has many applications such as pre-computing results offline, backfilling historic data with predictions from state-of-the-art models, etc.,. Inference on large scale datasets comes with many challenges prevalent in distributed data processing. Attendees will learn how to efficiently run deep learning prediction on large data sets, leveraging Apache Spark and Apache MXNet (incubating)
Captions: 
	                              my name is Naveen Swamy I'm a software                               developer at Amazon I work on deep                               learning frameworks and tools                               specifically Apache I'm excited today I                               want to talk about a little bit about                               deep learning then the Apache I'm a deep                               learning framework finally at the topic                               of today distributed inference of the                               challenges in it how to use MX net and                                spark together to solve those problems                                how many of you know what deep learning                                is here don't waste your time if you                                already know I can skip through and okay                                let me just quickly go through them so                                like you all know machine learning is                                about using algorithms to process data                                and make decisions but in traditional                                laminal before you could use an                                algorithm you had to go do or do feature                                extraction where you told the algorithm                                what are the salient features in your                                data to look at before you could use                                this had some drawbacks specifically it                                needed domain experts if you're doing                                recommendation systems on you on retail                                products you need to you need a domain                                expert who understands what are the                                features in it and then these were                                handcrafted features so they were                                error-prone and most importantly this                                was not this doesn't work on new                                problems you need to start over all                                again deep learning or neural networks                                solves this problem differently                                the term deep learning and neural                                networks is used interchangeably so the                                area of neural network design originally                                was inspired by how learning works in                                our brain but today it's become more of                                an engineering and algorithmic challenge                                to solve various machine learning tasks                                in this system it learns the most                                important features from experience                                itself by building hierarchy of concepts                                learning each concept at a time take                                this example which is a neural network                                which has many layers and here we are                                trying to                                feed an image and classify or categorize                                into three different categories car                                person and dog so the first layer is                                called the input layer where you feed                                the raw pixels to the network and then                                you have a hidden layer the first hidden                                layer which is trying to make sense of                                edges in the image that are the layers                                in between input and output are called                                hidden layers because the values are not                                given by the data the system is learning                                to represent the relationship between                                the input and output and then you have a                                second hidden layer that's trying to                                make sense of corners and contours you                                have a third layer which identifying                                object parts in the image finally you                                have an output layer giving you                                probabilities across these three                                categories                                once we train it well if you feed an                                image of a dog you should get a high                                probability on dog the category dog one                                thing to note are deep learning hear the                                word deep doesn't mean it's gaining                                deeper understanding it just means that                                the number of her layers is really                                really large so deep learning dates back                                to nineteen forties a few things happen                                that change the landscape in the last                                                                                                       of data with digitization of everyday                                life and commoditization of IT resources                                by cloud providers like AWS it's never                                been easy to collect and analyze data                                right so with lots and lots of data we                                are able to create deeper and deeper                                networks that's learning a lot more                                concepts in the data and then you have                                high performance compute here I mean                                GPUs a general-purpose CPUs have a few                                cores that can support only a fewer                                number of software threads                                well GPUs have hundreds of cores that                                supports thousands of course thousands                                of threads this is especially important                                for deep learning because if you look                                deep down in the neural networks these                                are all linear algebraic operations that                                can be heavily paralyzed                                so with GPUs we can get faster combustor                                 experiments and faster learning in                                      researchers started using GPUs and                                 suddenly they were able to create models                                 that were                                                            which were and learn much faster another                                 thing is that this field is rapidly                                 changing                                 you see papers coming out every day                                 algorithmic advances so that definitely                                 helps to a faster learning and get                                 better accuracy and in that respect in                                                                      Alex krevske from Jeffrey intense lab he                                 is a pioneer in the area of AI and works                                 out of Canada he presented a model                                 Beyond based on deep learning deep                                 neural networks for image classification                                 where the task is to identify the say                                 you know the most significant image                                 object in the image which was like                                    percent more accurate than the best                                 known algorithms to that date this was                                 in                                                                   happened after that all of this gives us                                 bigger and better models with battery                                 and giving us better AI products that's                                 permeating into our everyday life like                                 in personal assistants in machine                                 translation apps you know autonomous                                 vehicles this might not be very useful                                 here but with such great public                                 transport back in the bar I come from                                 San Francisco Bay Area                                 it's horrible my commute is                                             way and I would love to see a affordable                                 self-driving car I'm really excited and                                 waiting for it and then there are                                 companies that are taking into a deep                                 learning into healthcare field this one                                 in particular is trying to help                                 physicians diagnose heart diseases and                                 avoid unnecessary angiograms and then                                 there is alphago zero this system                                 learned the world's greatest                                 intellectual game go just by playing                                 against itself without needing any                                 expert data                                 just to give you a perspective on the                                 complexity the number of configurations                                 on a go board is more than the number of                                 atoms in the universe that's                                           power                                                                 against the world's go champion KJ just                                 by playing itself against itself one                                 thing to note without any panic is that                                 the system still had to go play millions                                 of times before it could master while                                 players themselves played much fewer                                 number of times and they start playing                                 instinctively so we are still relevant                                 machines have not taken over us so let's                                 see where deep learning falls in this                                 broad category there to play the machine                                 has to play millions of times but how                                 fast they had to play like in a shorter                                 time than any human now it took a couple                                 of days or weeks I don't know the exact                                 number but it's it's not really yeah                                 you're right that you can power up                                 computer and feed it but to solve                                 problems you need much more than just                                 being able to play it so many times                                 right yeah so let's see where deep                                 learning falls into this broad category                                 of artificial intelligence which was                                 meant to actually focus on developing                                 cognitive capabilities of humans to                                 reason and think or many years many                                 machine learning and non machine                                 learning models methods have been                                 developed that and tried to achieve AI                                 in particular I non machine learning                                 methods there was rule based system                                 which where they believed that they                                 could achieve true AI just by hand                                 crafting rules one popular one was                                 called expert systems in                                                machine learning it's an algorithmic                                 field that blends ideas from computer                                 science statistics and many other                                 disciplines to solve problems that are                                 harder for humans but easier for                                 machines and then which could be                                 expressed as                                 formal mathematical rules deep learning                                 is a subset of machine learning which                                 tries to solve problems that are easier                                 for humans to solve intuitively but                                 harder to express problems in terms of                                 mathematical rules whether deep learning                                 is are trying to achieve true AI is                                 really debatable and I was reading an                                 article by Michael Jordan on AI                                 revolution he is a computer science                                 professor and statistician in UC                                 Berkeley he had a very interesting take                                 on it he says he goes on to say that                                 what we have developed and focusing are                                 actually to develop intelligence                                 augmentation systems I a not AI to help                                 human intelligence and creativity that                                 was interesting despite all the progress                                 there there are some limitations to deep                                 learning one of them is that it requires                                 lots and lots of data                                 correspondingly lots of compute power                                 though we have lots of data to get                                 expert data it's very expensive and                                 unreliable sometimes just not available                                 and then it cannot detect inherent bias                                 in data imagine deploying these                                 decision-making systems in at large                                 scale if it was trained on data that has                                 bias that's that's a disaster awaiting                                 right so and another thing is that                                 sometimes you cannot explain why some                                 models work and some don't                                 often the application scientists say                                 that there is a black art of hyper                                 parameter tuning in in order and deep                                 learning to understand or to make it                                 work and then you can only hypothesize                                 after the fact hopefully you're                                 convinced that deep learning is a big                                 deal let's see how it works it has two                                 phases one is training where you develop                                 a model and feed lots of data and then                                 you at the end you check if it met your                                 output objective if not we send the                                 error back let's take this very dummy                                 Network where                                 has an input layer with two inputs and a                                 hidden layer and an output first we pass                                 samples from a data set this is also                                 called forward propagation the layer the                                 layers are connected in between with                                 weights which are initialized randomly                                 to start with and then we have we define                                 our objective function here for                                 illustration it's very simple y equal to                                 $                                                                        we take a loss function and propagate                                 the error Brak updating the weights to                                 by taking gradients of each of the                                 weights this is where the learning                                 happens we do this many many times until                                 the objective is met or the desired                                 accuracy is met at which point in time                                 it gives us a model also called a pre                                 trained model next we making use of this                                 pre trained model in your real life                                 applications it's what our deep learning                                 inferences we take the model and feed                                 new application data to get predictions                                 this is just running half of the                                 training / training where we run only                                 the forward pass our deep learning                                 inference can be done in two modes one                                 is a real time where you need immediate                                 feedback for example in fraud detection                                 and then you can do it in batch mode                                 when you have lots and lots of data this                                 is especially helpful when pre                                 computations are necessary for services                                 like recommender systems which has to go                                 sort and rank against hundreds of user                                 products another place where it's useful                                 is to let's say you have already a model                                 that's been used to run inference on and                                 now you develop a state-of-the-art model                                 you want to go back fill all of those                                 inferences so we can use the we can use                                 batch mode there and another one is if                                 you develop new models before you can                                 deploy it to production you want to test                                 it on your existing data to verify if it                                 is giving you a better result                                 let's take a look at the types of                                 learning in play today in deep learning                                 now the first one is supervised learning                                 which is where a lot of advances have                                 been made this is using this is telling                                 the system what semantic content is                                 present in your data and what it has to                                 match that and learn the weights to                                 actually match the output example is if                                 you feed for image classification an                                 image of a dog you tell the system                                 hey the output I'm expecting here is a                                 dog alright this is used in speech                                 recognition machine translation and like                                 I said image classification and then                                 another type of learning is unsupervised                                 learning like I said there's lots of                                 data but we want to find patterns in                                 that data we use so unsupervised                                 learning to do                                 for example clustering and Association                                 discovery then there is another new                                 exciting type of learning which is in                                 between supervised and unsupervised                                 where it tries to identify the patterns                                 but if when it cannot understand what's                                 in the para what's in the data it                                 queries a human where is a user to get                                 the labels and continue from there on                                 and there is also reinforcement learning                                 that is learning in its current                                 environment from the experiences using                                 rewards and feedback for example                                 robotics is one that makes use of deep                                 reinforcement learning so far we covered                                 a few things about deep learning so                                 let's see apache MX net framework which                                 is a incubating project at apache the                                 first question is why do you want y MX                                 net and how is it so different I have                                 many reasons here one is it offers                                 programming API is in many different                                 languages so you don't have to learn                                 another language to use this it also                                 offers simple syntax and it supports                                 both imperative and declarative style so                                 as we speak there is a contributor who                                 is working on a closure API if there are                                 any                                 interested closure skilled audience here                                 you can use that and next is that you                                 can create highly optimized models that                                 you can deploy to mobile and IOT it's                                 also a very high performance in training                                 we have seen that it can achieve near                                 linear scaling across hundreds of GPUs                                 and again open-source incubating it's                                 truly open source because no one entity                                 can control the project right so anybody                                 is welcome to contribute to this cause                                 then there is onyx support how many of                                 you know what on exists there are many                                 awesome deep learning frameworks but all                                 once you start using them all your                                 intellectual property is embedded in the                                 model that it creates now you have stuck                                 to the framework if you want to move to                                 another framework there's there were no                                 options so this is an open source                                 initiative by Amazon Microsoft and                                 Facebook to come up with a                                 interchangeable or model format so you                                 can take do training on one one one deep                                 learning framework like white arch and                                 bring it on to MX net and run them here                                 and then there's glue on support glue on                                 is another open source initiative to                                 simplify the simplify and standardize                                 the deep learning API is it's a it's led                                 by Amazon and Microsoft this offers                                 imperative api's that is easy to                                 understand easy to use MX and not only                                 implements this API is it also on the                                 backend transforms into a highly                                 efficient symbolic graph so now you not                                 only can go quickly prototype your model                                 you can also deploy it to production                                 without any without losing any                                 performance let's look at a few                                 constructs in MX net the first one is in                                 D array it's nothing but a                                 multi-dimensional array so this supports                                 imperative tensor operator                                 that work on CPU and GPU tensor is used                                 interchangeably with multi-dimensional                                 array but if mathematicians hear me say                                 that they get very upset tensors are                                 much more than multi-dimensional array                                 which represents relationship at a                                 relationships of data at a higher                                 dimension anyway there's them a symbolic                                 AP X it is similar to nd array                                 but adopts declarative programming for                                 optimization reasons where you create                                 the graph first and then give it to the                                 system to optimize it in this example                                 here we have two operations simple                                 beasts a C equal to B star a and C equal                                 to C D equal to C star                                                   D array the numerical computation would                                 happen as it sees the statements or the                                 operations in symbolic world you would                                 first create first create a graph and                                 then compile at which point in time I am                                 excellent back and goes and optimizes                                 that decomposes the operators and                                 optimizes removing all the redundant                                 memory references and also parallelizing                                 independent operations in this example                                 you can see that C is not being used so                                 it it optimizes to remove that redundant                                 reference now we have lesser memory                                 consumption there is another set of API                                 is called module which is designed to be                                 a high-level api's to work with symbol                                 module represents computational                                 components that execute forward and                                 backward pass on the network and also it                                 operates the parameters which are the                                 weights with these ApS you first create                                 a graph of the network in this example                                 we have a we have two fully connected                                 networks fully connected is something                                 that passed straight connections and                                 there are other types of network or                                 convolutional RNN so without going into                                 the details this creates a network and                                 then we feed it to the module                                 and the next step we will bind this so                                 that the back end will allocate memory                                 for it will bind with the input data                                 shapes finally we will pass data to the                                 module and call either the fit routine                                 which is to train the network on your                                 data or call the product which will if                                 you have a pre trained model will give                                 you predictions on new application data                                 that you have let's look at a small demo                                 here here I'm showing MX net is                                 available as a PI Python pip packet so                                 you can easily install it and I'm                                 showing you a resonate                                                   what resonate                                                            oh and here there are two files symbol                                 and parameters symbol represents the                                 network and parameters are the weights                                 in the system if you look at the symbol                                 you see that it's made of many many                                 layers with convolution pooling and                                 activation layers and to the start of it                                 you will we will pass the data there you                                 go                                 over there and if we end in the                                 parameters you'll see these are                                 floating-point values or doubles                                 representing the weights between the                                 layers                                 so let's there is another file called                                 sign set which represents the                                 relationship between categories and the                                 labels let's look at a small example and                                 see how to run prediction here we are                                 see I'm using the image of a ship to                                 predict it's a three dimensional array                                 like I said this is this means that it                                 has three channels RGB you can also                                 check by running the shape operation on                                 it you see that it has three dimensions                                 there and the size is                                                  we run predict on it you can see that it                                 gives you as a container ship with about                                                                                                         want more than that obviously                                 so so far I've covered a few concepts in                                 deep learning and talked about MX net                                 now let's see the challenges in                                 distributed inference when you have a                                 large data set to run inference on                                 you want to run them on multiple                                 machines right there are a few                                 challenges to efficiently running this                                 the first one is you need a                                 high-performance deep learning framework                                 just to give you an idea of the                                 complexity involved to run forward pass                                 on one image it takes about on a model                                 like ResNet which I'll talk next it                                 takes billions of floating-point                                 operations so it your framework needs to                                 be really efficient to make sure that                                 your resources are well utilized and                                 since you have a lot of data you need a                                 distributive cluster that's able to do                                 resource management very well and then                                 schedule jobs for your track them and if                                 it fails retry them you need to also                                 efficiently partition your data across                                 the cluster so that all the resources                                 are very utilized you need a deep                                 learning setup as well sometimes it can                                 be very hard to make it efficiently work                                 because of the dependence is involved                                 you need the right GPU drivers you need                                 right libraries so for this particular                                 problem                                 Amazon deep learning army where you can                                 launch a ec                                                          makes it a breeze to use all of the many                                 of these look very familiar and similar                                 to large-scale data processing systems                                 right apache spark solves many of these                                 problems and you can use multiple                                 cluster managers it also works very well                                 with MX net and it integrates with                                 Hadoop and big data tools if you have                                 data on Hadoop clusters now you can gain                                 new insights using deep learning so                                 forth I have a demo for this I will be                                 using image net trained resonating                                 classifier                                 it's a image net is a project that runs                                 annual competitions for visual                                 recognition tasks and researchers                                 worldwide                                 and they have a database of a million                                 images across thousand different                                 categories in                                                            presented a model that was able to                                 surpass human capability in this narrow                                 subset of visual recognition tasks                                 originally it had                                            it's called residual neural networks                                 here I'm using resonate                                                 layer version even this needs about                                 three billion floating-point operations                                 and for demo I will use C for                                         data set with                                                            for research so I couldn't use it here                                 I'm using a publicly available data set                                 I will use the Python version of spark                                 on Amazon EMR if you're a scholar                                 developer MX net is also available in                                 Scala so you can use that and also it's                                 not necessary to use on an EMR you can                                 have your own spark cluster and have em                                 accept installed also the demo is based                                 on CPUs but if you have lots of data                                 then you can easily extend this to GPOs                                 so let's see how the pipeline itself                                 looks like all of my images is away I                                 put them in s                                                          any highly available distributed file                                 system or even EFS for that matter so I                                 will download all the SDKs on driver                                 there is no need to distribute or move                                 images across driver to executors how                                 many of you know spark okay yeah it has                                 a driver and there are workers driver                                 submits the job to the workers and                                 manages the scheduling so what I was                                 saying is you don't need to move data                                 between these two components all you can                                 do is have the keys create the key just                                 download the keys onto the driver and in                                 spark there is a term called RDD you can                                 create a DD and then partition across                                 the cluster already is resilient                                 distributed data set                                 so it's a very efficient way to                                 distribute data and on the executor we                                 will fetch a batch of images like I said                                 these are linear algebraic operations a                                 bad-size of more than one is just                                 another dimension in tensor so we will                                 try to parallelize that by Craig                                 fetching a batch of images and then                                 we'll decode this two numbers another                                 library Python library we will convert                                 that to num by format and run                                 predictions using the MX net library all                                 of these happens inside this park map                                 partitions method spark has map it's                                 based on Map Reduce but it calls                                 transformations and actions and in the                                 transformation you can either act on one                                 data item at a time or a chunk of data                                 at a time so what I did here is to take                                 a chunk of data in map partitions and                                 then convert it into a batch also we                                 don't want to initialize this model over                                 and over for every map partition so we                                 want to make sure that adds to latency                                 so we will initialize that only once                                 I'll show you how and finally we will                                 collect the predictions at the driver if                                 you look at just looking at the code                                 here I'm setting the number of executors                                 course by default spark goes and creates                                 one task per core but MX never uses all                                 the CPUs very efficiently so I'm asking                                 spark here to create only one task for                                 executors and not compete with them                                 except so it just adds a context                                 switching and like I said I'm using map                                 partitions the number of partitions is                                 based on the batch size and the number                                 of images you have batch size is                                 something that I determined by running                                 experiments based on the CPU type I have                                 and how much memory I have like I said                                 before we need to more load the model                                 only once for whatever reasons Park                                 doesn't give you any protein on the                                 executor instead we use                                 singleton and a static method to load                                 the model on to the process only once                                 and we have another thing is in MA                                 we don't need to actually we have to                                 only import the libraries at the map                                 partition method in instead of top of                                 the module because at that point the PI                                 spark just tries to serialize these                                 libraries from driver to the worker and                                 most often fail saying this has pointer                                 references which it cannot understand so                                 we'll we'll import it at the map                                 partition method and then make the                                 libraries available on all the nodes so                                 it doesn't have to serialize the                                 libraries have already available let's                                 just quickly see a small how to use it                                 it on EMR it makes it very easy it's a                                 click of a button for you to install MX                                 net and spark together here we don't                                 need all those I just need spark and we                                 have MX net                                                           just create a cluster I will use a V PC                                 and let me show you that for the core or                                 the driver core node where the driver                                 lives and here we have GPU instances                                 available if you want to make use of                                 them but here I will just use a m                                        which is enough for a data set that I                                 have and fork or I will use c                                          large for such nodes so yeah and then                                 you click a button and you will just be                                 able to create a cluster let's take a                                 look at how to use this cluster here I                                 take the master instance and then we                                 login to that yeah                                 the code sample is available on github                                 so I will try to clone that I will clone                                 that here and I use that to submit a                                 spark job oops                                 all right                                 okay that exists already so let's just                                 submit the job okay I have the command                                 here on a blog that I wrote I'll share                                 it with you here I'm showing that we                                 have                                                            executors and passing the model URLs and                                 with a bad size of                                    you can also optionally pass s                                         and is the key to write the output into                                 here that's the test bucket that I have                                 demo bucket if you want to test drive                                 this you can use it and we will give it                                 a key to write the output okay submit                                 yeah once it submits to a driver we can                                 go to look at it on a resource manager                                 URL that that's available with Hadoop                                 and spark                                 it's submitting that job yet alright it                                 started running that's great let's take                                 a look at the URL here it shows you the                                 job execution flow here you can go to                                 the application master here you see that                                 it's already started let's see what's                                 going on it made about yeah let's see                                 okay it made about                                                because it was                                                          let's look at some logs to see what it's                                 doing you can see that it's it's loading                                 the downloading the model and created                                 its created the symbol and then you can                                 see it ran a prediction on a batch of                                 image that's great                                 let's quickly look at the job that's                                 already run                                 okay this is a job I had previously                                 submitted and you can see that it was                                 able to process                                                        four minutes that that was pretty                                 impressive                                 to summarize I talked about a few deep                                 learning concepts and why deep learning                                 is a big deal these days again it's only                                 a very tip of the iceberg there's lots                                 more next I covered MX net nd array and                                 symbol and module finally we saw how to                                 use a magnet and spark for distributed                                 inference so the for the project we just                                 released                                                            inference API is a simplified set of API                                 is for Scala and it's available on may                                   s immune package we are also working on                                 Java EPS we have heard that research                                 scientists while they prefer Python to                                 model but engineers want a type safe                                 language because they are putting it                                 into production in a large system and                                 they're used to the ecosystem which has                                 built around Java so we are working on a                                 simplified set of API is in Java we are                                 also considering support of date spark                                 data frames so that the communication                                 becomes really smooth finally this is a                                 our community is fast evolving and I                                 want to ask you to join hands or to                                 democratize AI and bits of Apache                                 project the code is all available on                                 github and deep learning is a very                                 exciting field go check out join us and                                 join our mailing list and see what's                                 going on bring out of your use cases so                                 that we can prioritize them there are                                 some resources that I put together here                                 Apache slash incubator MX net on github                                 Azure repo I wrote a blog about it it's                                 available in the link then few things                                 and if you're new to deep learning MX                                 that blue one has a excellent set of                                 tutorials taking you from all the way                                 from                                 all the way from linear algebra and many                                 basic concepts all the way to                                 implementing papers and the latest                                 greatest papers so check that out and                                 thank you so much for your patience                                 [Applause]
YouTube URL: https://www.youtube.com/watch?v=WJ7wpXadFao


