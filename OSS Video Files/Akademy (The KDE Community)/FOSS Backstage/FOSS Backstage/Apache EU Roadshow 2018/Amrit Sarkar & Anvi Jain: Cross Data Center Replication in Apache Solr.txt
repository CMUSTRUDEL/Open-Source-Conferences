Title: Amrit Sarkar & Anvi Jain: Cross Data Center Replication in Apache Solr
Publication date: 2018-12-05
Playlist: Apache EU Roadshow 2018
Description: 
	High availability of data across geographic regions for search and analytical applications is a challenging task. Mission critical applications need effective failover strategies across data centers. Apache Solr offers Cross Data Center Replication (CDCR) as a feature from 6.0 and has added more features in subsequent releases.
Captions: 
	                              we're here to talk about cross data                               center replication in Apache Solr I'm on                               regen working as a software engineer at                               progress software progress offer is                               based out of Bedford and we have offices                               all around the world                               we develop tools and platforms that                               enables businesses to deliver new                               technologies that are adaptive and                                cognitive and we provide solutions for                                enterprise integration data                                interoperability and application                                development I would like to invite amra                                to introduce himself                                hi I'm little Cole working as a senior                                research engineer at lucid works so                                lucid wes is a search enterprise company                                based out of San Francisco and I have                                offices all over the world we have a                                product lucid works fusion built on top                                of a part a solar which drives search                                applications and we also provide                                consulting and support organizations who                                are using Apache Solr                                as their search or analytics technology                                yeah so moving forward the agenda for                                today's talk will begin with an                                introduction to solar then we will talk                                about the motivation behind building                                this feature that is CDCR                                and then we will give you an insight                                about CDCR the components it present in                                it the approaches that we've used and                                we'll end with a short demonstration so                                Apache Solr as we all know is an open                                source search engine that is built on                                top of the Lucene library it is highly                                scalable reliable fault tolerant and has                                various features like auto completion                                highlighting deduplication learning to                                rank among others solar is used as a                                search engine and navigation in the top                                websites of the world the solar cloud                                mode basically is the powers the                                distributed searching and it has the                                following major components the zookeeper                                that acts as the manager for the whole                                solar cloud and stores its                                configurations and the cluster state                                information that is which node is alive                                which is node and the which is not and                                on which node the leader resides etc the                                collection as you can see here is a                                logical index in a solar Clow                                a collection can be further divided into                                logical chunks of data logical slices                                every logical chunk is called as a char                                and every char may have multiple copies                                or which are called as replicas so out                                of these replicas for every Shire a                                leader is elected by the zookeeper and                                the zookeeper holds information about                                other configuration informations about                                the leader so what was the motivation                                behind building this feature so as you                                all know no data center is completely                                disaster-proof these days a disaster can                                be man-made or natural or can be due to                                any malicious activities leading to a                                loss of huge amount of data due to which                                companies can incur huge financial                                losses as well if you see at the graph                                over here the cost of one day of                                downtime for an organization can be huge                                and every company may not have the means                                to to set up a proper disaster recovery                                strategy in place and even if they do so                                the strategy should be reliable should                                be revisited regularly and should be                                maintained from time to time there are a                                couple of disaster recovery strategies                                in place and some of them can be                                incorporated in solar as well what we                                can do is we can index data into                                multiple data centers simultaneously for                                that purpose we can use a messaging                                service like Apache Kafka now the so                                what we can do is we could redirect our                                data to a patch of Kafka and Kafka will                                handle the redirection to multiple data                                center simultaneously the overheads in                                this approaches you need to configure                                Kafka at your own level you need to know                                the proper configurations plus the data                                at all the data centers may not be                                consistent because you do any failure at                                any one of the data centers you might                                have an inconsistency of data and if                                there is such a behavior or such an                                 instance that have                                 we'll have to reindex data to that                                 particular data center to maintain                                 consistency everywhere another approach                                 that can be used as disk mirroring that                                 is copying the entire hard disk from one                                 location to another or secondary storage                                 from one location to another now as you                                 all know the data in today's world is                                 huge right gigabytes to petabytes and                                 disk mirroring may take a significant                                 amount of time to copy the data also                                 while copying what if the data gets                                 corrupted in between due to any error                                 State or so the whole the whole copy                                 that we have maintained will become                                 useless will be of no use then another                                 thing that can be done is taking regular                                 backups so this approach basically                                 involves taking backups incrementally                                 also this approach is more useful when                                 the amount of data is less and the                                 backups should be maintained outside the                                 server because there is no point in                                 maintaining the secondary backup in the                                 same server where where data can be lost                                 due to any disaster any error state so                                 this the data should be maintained                                 outside the server and it should be                                 retrieved quickly whenever needed and                                 here also if the backup is incomplete                                 due to any errors the whole data will be                                 useless then there is an internal API                                 present in solar itself which is the                                 backup API and it is available at the                                 collection level this API takes backup                                 of the entire collection and its                                 configurations and this API is also                                 again prone to failure due to at live                                 indexing or due to any error State say                                 out of memory exceptions or power                                 failures or anything so as you've seen                                 there are a lot of strategies we have in                                 place right now as well but all of them                                 are not very reliable not robust and                                 they require a lot of human intervention                                 as well from time to time to avoid all                                 these things and to remove all these                                 shortcomings CDCR or cross datacenter                                 replication was introduced in Apache                                 Solr                                 and I would like to invite I'm ready to                                 give you an inside                                 thank you so in the next few slides we                                 will see how cause data center                                 replication actually works and how it is                                 better than the other stated strategies                                 so this particular feature was added                                 around about                                                                                                                                  one single definition it would be                                 forwarding updates or actions to                                 secondary cluster or clusters monitor                                 their application and make sure that all                                 the clusters involved in the CDCR                                 operations are up and healthy it                                 supports both unidirectional data is                                 transferring data from one data center                                 to other data center and also by                                 directional that is all the data centers                                 will be in sync automatically this                                 particular feature is implemented at the                                 collection level and only data is                                 configured configurations are not so                                 when is so a data is replicated                                 configurations are not it is a                                 responsibility of the designer and the                                 developer to make sure that all the                                 entire topology and all the                                 configurations in the respective data                                 centers are consistent we have multiple                                 API is available to start stop and get                                 the status of the CDCR process like                                 action start stop and status now this is                                 how our unidirectional approach looks                                 like we have the source data center and                                 the target data center now we will as we                                 have mentioned that the CDs here is                                 implemented at the collection level so                                 we will just call it source collection                                 and target collection each collection                                 have two shots and two replicas if you                                 look closely the data is getting                                 transferred from the leader nodes at the                                 source to the leader nodes at the target                                 that is only the leader nodes are                                 allowed to talk to each other                                 communicate each other this particular                                 strategy is active passive approach that                                 is you can do the live indexing in one                                 of the primary collections and all the                                 all the respective other collections                                 will be kind of like a hot standby this                                 is how a sample configuration of the                                 unidirectional approach looks like we                                 will discuss all these configurations in                                 bit detail in the                                 slide so what we really like you to                                 focus is upon the ZK host ring and the                                 target parameter value so at your source                                 you can define your target zookeeper                                 connect string and you will define the                                 target collection value you can have                                 multiple sets of these kinds of                                 configurations for your X number of                                 targets at your source so now to discuss                                 the underlying concept of the CDCR of                                 what exactly do so as I mentioned                                 updates are forwarded from the leader                                 nodes at the source to the leader nodes                                 at the target each update operation in a                                 solar cloud is embedded with a unique                                 version that particular version is based                                 on time it looks like something like one                                 six zero one seven and something like                                 that and this particular unique version                                 is maintained as a checkpoint as a                                 persistent or a permanent node at the                                 target and a temporary or an ephemeral                                 node at the source these particular                                 checkpoints maintained at both the                                 sentence should be synchronized at all                                 the points to make sure that there is no                                 loss of data also this particular                                 strategy does not require adding                                 additional network traffic and the                                 natural update workflow of the solar                                 cloud that is if an update operation has                                 been received by a collection it will be                                 first be sent to the leader node of the                                 particular collection the leader node                                 has a responsibility to forward these                                 particular updates to its own followers                                 this particular strategy makes sure that                                 all the data gets replicated to all the                                 nodes at both source and target so these                                 are the core components of the CDCR of                                 it is build upon so first for the first                                 type synchronization that is all the                                 data should be replicated from the                                 source to target for the first time                                 buffer and bootstrap gets into action                                 while after the initial sync is in place                                 and you want the smooth forwarding of                                 updates replicator and update log                                 synchronizer make sure that happens                                 starting with initial sync so let us                                 suppose the target is not available                                 because it's not being                                 created or being down for a significant                                 amount of time and a huge chunk of data                                 has been already being indexed at your                                 source there are a number of documents                                 already there you would like to transfer                                 the data from source to target in the                                 best possible manner in timely fashion                                 and with minimal resources buffer and                                 boots that makes sure that it happens                                 starting with buffer we would like you                                 to focus on the left-hand side and the                                 CDC request handler that's a Java class                                 available in the solar source code which                                 is responsible to process all the CDC                                 are specific api's this particular                                 configuration is put into the solar                                 configured XML in a solar collection and                                 it is mapped to the endpoint CDC are now                                 buffer in this example is enabled so                                 whenever buffer is enabled all the                                 references to the updates has been sent                                 to the source will be kept in memory                                 indefinitely until they are transferred                                 to the target this particular strategy                                 works very well when the number of                                 updates are limited or in check but if                                 it gets unbounded you will bound to face                                 issues with threads or with heat map any                                 heap memory so it is best to disable the                                 buffering during normal operation or                                 when the normal application is in place                                 API is are available with action equal                                 to disable buffer to disable it and                                 action equal to enable buffer to enable                                 the buffer next we have bootstrap to                                 mitigate this limitation of not being                                 able to handle a huge amount of updates                                 bootstrap was introduced in                                           the first time when target gets up the                                 katatak collection is up its checkpoint                                 is negative it's minus                                                  sees that that checkpoint is minus                                      will each trigger a bootstrap operation                                 bootstrap essentially is copying data                                 from one place to another in this                                 particular case the data is replicated                                 from the leader node of the source to                                 the data directory of the leader node at                                 the target once this copy finishes at                                 the target all the non leader nodes will                                 issue a collection API called request                                 recovery it says that copy the index                                 from my own leader so it's absolutely                                 simple                                 a                                                                  leaders at the target pulled from their                                 own leader this particular bootstrap                                 operation is also available to an                                 implicit API which can be fired on the                                 target like this it's not really                                 mentioned in the documentation and                                 nowhere in the any any dog blog post and                                 anything                                 the reason being this particular would                                 stop leverages the Apache Lu scenes                                 Index fetch operation which is a single                                 threaded process and if the index is too                                 much vague or it's too huge this                                 particular operation can take a                                 significant amount of time so you want                                 to make sure that you can leave this                                 particular operation to the solar itself                                 of when to trigger and when to not and                                 not do something like this manually next                                 we have some supporting components too                                 which make sure that everything and all                                 the components are in sync with each                                 other so we have state managers which                                 gives a tab on certain zookeeper nodes                                 and whenever some changes happens they                                 will trigger some action so process                                 state manager keeps a tab on CDC estate                                 on the left top corner you can see                                 process started buffer disabled if an                                 API is issues to stop the process or                                 enable the buffer it will inform all the                                 respective components that some kind of                                 operate change has happen                                 similarly with zero state manager which                                 keeps a watch on top of state dot JSON                                 which looks something like this so we                                 have the collection citibike it will                                 contain the information of which how                                 many shards are there which particular                                 nodes contains which particular replicas                                 which replicas is a leader etc so                                 whenever a leader changes in that the                                 source level or at the target level the                                 leader state manager will inform all the                                 respective core components that some                                 kind of changes happen so we have                                 discussed how the initial sync should be                                 done our initial sync is done so once                                 the source and the target are at least                                 in sync for the first time now new                                 updates are being sent to the source and                                 they need to be propagated to the target                                 in a timely fashion                                 a replicator makes sure that happens                                 it forwards the updates in batches and                                 at regular intervals these batch size                                 and the interval are specified in solar                                 configured XML so let's suppose you have                                 very heavy indexing going out at the                                 source or you have very heavy query load                                 you would really not like the other                                 components in solar to overwhelm that                                 particular process you would like the                                 performance to not be affected by                                 something CDCR which is which is used to                                 have a hot standby so this particular                                 strategy that is sending the updates in                                 small batches and an interval make sure                                 that performance is not affected so in                                 this particular example we have a                                 bad-sized                                                            thousand milliseconds that is one second                                 now let us suppose thousand updates has                                 been sent across to the source which                                 needs to propagate it to the target it                                 will take around about seven to eight                                 seconds to reach to the target there                                 will be a slight delay but that is                                 something we have to cope up with after                                 each successful operation checkpoint is                                 updated at the target and propagate it                                 back to the source so that they can be                                 synchronized and if a leader goes down                                 the newly elected leader will come up                                 and we'll start the replication from                                 itself and if a target is not available                                 or being done for a significant amount                                 of time replicator will try to forward                                 the same batch again and again                                 indefinitely until the connection is                                 restored now at this particular point of                                 juncture what you really need to do is                                 stop the CDCR process and do a basic                                 diagnosis at your respective targets                                 next we have the update log synchronizer                                 so as I said that all the leader nodes                                 are only allowed to communicate with                                 each other but what happens when a                                 leader node that the source goes down                                 now I understand that we are using                                 leader non-literal terms a lot in these                                 particular slides but it is important to                                 understand it so what happens when a                                 newly elected leader comes up after one                                 goes down from what point it has to                                 start the replication that's being                                 maintained by update log synchronizer so                                 each non leader at the source only will                                 ask its leader that what                                 the checkpoint at the corresponding                                 target in irregular intervals in this                                 particular example we have that interval                                 set as                                                              seconds one minute                                 so whenever a current so the newly                                 elected editor comes up heat will start                                 the replication from the last checkpoint                                 it already has it may he play some                                 updates but it will make sure that no                                 updates are lost in the process the last                                 component we have is CDCR update                                 processor factory so as I mentioned in                                 the underlying concept Solon has a                                 tendency to add a version to every                                 update operation no matter where it is                                 coming from so when an update is                                 forwarded from source to target it                                 already has a version and you would                                 really like to use the same versions to                                 have the synchronization but solar tends                                 to override it                                 CDs are present factory makes sure that                                 it does not override that this is how                                 you can differ and define the CDC update                                 versus a factory yeah you will define it                                 and add update request processor chain                                 which will be a default chain of the                                 update endpoint if we really like to                                 understand what is request handlers what                                 is update request processor chain or CDR                                 update which is a fractal check out the                                 official documentation to understand                                 them better                                 the so we are done with discussing the                                 components of CDCR and knowledge cus the                                 monitoring API so we have so we have                                 three api's available as of now the                                 first is queues which provides us                                 information of how many update our                                 target is behind the source and what was                                 the last time stamp a successful                                 operation was executed at the target                                 right it will contain some more                                 intrinsic information regarding the                                 transition log sizes and numbers at the                                 source we really did not discuss about                                 the role of transition log in this                                 particular talk and we have it abstract                                 check out the documentation or the                                 reference links at the end of the slides                                 to understand them better                                 it also contains information about                                 update log synchronize or whether it is                                 running right now or dignity or being                                 stopped then we have the Opera                                 API which provides information about the                                 average number of operations being                                 executed at a particular target per                                 second and those are broken down into                                 numbered average number of deletes and                                 average number of ads we have the last                                 one errors API which will provide                                 information about the consecutive errors                                 or and the cumulative errors occurring                                 at a particular target it will also                                 provide information about if there are                                 any bad requests or being internal                                 errors happening at a target so that you                                 can stop the CDC process and do a basic                                 diagnosis on that respective target                                 cluster now I will invite back on we who                                 will take us to the bi-directional                                 approach and the rest of the slides yeah                                 so so far we have discussed about the                                 unidirectional approach with which was                                 an active passive approach where the                                 indexing can be done only at the source                                 and for the targets can be used only for                                 the queering purposes so to remove this                                 limitation the bi-directional approach                                 was introduced in version                                              which is a passive passive approach and                                 there is no concept of a source and a                                 target so if you will look at the                                 pictorial representation over here we                                 could see that all the data that was                                 index into cluster                                                   with cluster                                                            indexed at cluster                                                   with cluster                                                   simultaneously and there are no source                                 and target every cluster has its own                                 every cluster is just the same over here                                 and since that is why it is an active                                 active approach so if you will look at                                 the configurations in the cluster one                                 collection configuration the zookeeper                                 is pointing to cluster                                              cluster                                                                  pointing to cluster                                                   attribute for cluster                                                  cluster                                                                cluster                                                            similarly so this is how basically the                                 CDC are in the UN unidirectional                                 approach was enhanced or updated into                                 the bi-directional approach if you will                                 look at the unidirectional                                 approach whenever an update comes to the                                 source it indexes the data into itself                                 and just pushes off forwards the data to                                 the target and the target indexes into                                 itself in case of a bi-directional                                 approach with the bait a new flag was                                 added is CDCR                                 if the value of the flag is CDCR is set                                 as false that means it is a normal                                 update and if the value is set as true                                 that means it is a CDCR update or it has                                 come through up through another cluster                                 so whenever an updates come two clusters                                 a cluster one here the flag is set as                                 false                                 what cluster one does is it indexes the                                 data into itself sees it's a normal                                 update that is a CDC RS false marks the                                 flag is true and forwards the update                                 further now that update now say suppose                                 comes to cluster to cluster two sees                                 that the flag is CDCR is Magda's true it                                 just indexes the data into itself and                                 does nothing so the bi-directional                                 approach makes it possible to have a                                 cluster of solar clusters if you will                                 see here every cluster is in constant                                 conversation with each other and there                                 is no concept of a source and a target                                 and there is no center of this entire                                 cluster thus making it possible to have                                 decentralized multi cluster setup so the                                 gist of the whole bio direction approach                                 is that there is no source no target                                 it's a decentralized cluster there is no                                 single point of failure anymore and                                 since in the bi-directional approach                                 indexing can be done to any of the                                 clusters hence in case of a failure the                                 indexing can be moved from failed                                 cluster to a working cluster with                                 minimal human intervention and they this                                 adds an advantage of dividing the whole                                 traffic geographically so say for                                 example I have one data center in New                                 York and another a dissenter place here                                 in Berlin                                 what I can do is I can divert whole of                                 my us-based traffic to the New York                                 place data center and one of my Europe's                                 our eastern part of the world's base                                 traffic to the Berlin data center this                                 would reduce network delay and provide                                 much better search experience so we will                                 be giving a short demonstration on the                                 approaches the unidirectional approach                                 the bi-directional approach of CDCR for                                 that purpose                                 we have created an application and                                 analytics application on Apache Zeppelin                                 [Music]                                 yeah so the data we have used is of                                 bikers the distance traveled by them                                 from one place to another the time taken                                 by them their gender classifications and                                 all and we have created some graphs and                                 configuration graphs around it our                                 Zeppelin application is currently                                 pointing to cluster                                                     the configurations for cluster                                         if you can see the zookeeper is pointing                                 to cluster                                                             cluster                                                               pointing to cluster                                                   index the name of the collection is same                                 we have kept them same for both the                                 clusters which is citibike over here we                                 will reload solar cluster                                                see that there are no documents over                                 here and we'll reload solar for cluster                                                                                                         here and similarly if you'll reload the                                 CDCR for both cluster                                                    could see that it's stopped right now so                                 what we will do is we will index some                                 data into cluster                                                 Zeppelin application is pointing to                                 cluster                                                                graphs we've plotted around the                                 application around the data sorry so we                                 could see that there are around                                    thousand records now present in cluster                                                                                                       all I have come up                                 and if you'll reload solar for cluster                                  there are                                                similarly for cluster                                                 zero records in CDCR is not started and                                 it's just a single cluster as of now now                                 we'll start CDCR on cluster                                            disable buffer so what this should do                                 this is kind of a unidirectional                                 approach where we are using cluster one                                 just for the indexing purpose and                                 cluster                                                                purpose as well so in class this is                                 cluster                                                           cluster                                                                  thousand records because CDCR was                                 enabled all the records that were pushed                                 to cluster                                                        cluster                                                            cluster                                                              onto cluster                                                          copied to cluster                                                     run the application we could see around                                                                                                       here and if we'll reload cluster                                      first solar we could see around                                     thousand records here again now we will                                 enable CDCR on cluster                                                 become a bi-directional approach since                                 series here is enable on both the                                 clusters now we can index data as well                                 onto the cluster                                                        clusters now can be used for indexing                                 and querying purpose yeah now say for                                 example our cluster                                                    any reason some failure out of memory                                 exceptions or anything so for that                                 purpose we kill cluster                                                demonstrate that and we'll reload the                                 application we've written down these                                 shell scripts and we've shared them on                                 the github handle that we mentioned in                                 the references page you can go and have                                 a look at them this might take some time                                 yes so since our application is failing                                 now                                 since we have a backup at cluster two                                 what we can do is very smoothly we can                                 just point our application to cluster                                 two and within no time we can see that                                 the data is back again our application                                 is up and running again it's taking some                                 time                                 yeah so we load the application a data                                 is we are again able to see the                                     thousand records now since our cluster                                 two can be used for indexing as well                                 what we will do is we will index some                                 data yeah so cluster one is down now                                 yeah right now will index some data on                                 to cluster two now and if we'll reload                                 the solar cluster we could see around                                 two twenty thousand records over here                                 and the Zeppelin application should also                                 have the same number of records now say                                 we found out the reason why a cluster                                 one went down and will restart the                                 cluster now what this should do is since                                 CDCR is enabled on both the clusters                                 whatever data that was indexed into                                 cluster two after cluster one went down                                 should be transferred back to cluster                                 one that's how smoothly it works so we                                 are just starting the solar cluster yes                                 and if we'll reload solar cluster one                                 it might take some time yeah so we have                                 the same number of records that we index                                 to cluster to in cluster                                               all about the demonstration for CDC ah                                 the unidirectional bi-directional                                 approach and how smoothly things work I                                 mean with minimal intervention you can                                 just find your applications to any                                 available clusters that's present so                                 we've discussed a lot about CDC are but                                 like any other feature it has some                                 limitations to the first thing is the                                 CDC are specific properties must be                                 specified in solar configured examine at                                 the time of the collection creation                                 itself if you want to enable CDC are on                                 an already created collection or on a                                 non CDC our collection that is not                                 possible as of now another thing is that                                 whenever a CDC our specific property is                                 changed in solar config the whole                                 collection requires a reload right now                                 and since book I already mentioned                                 bootstrap is a single-threaded process                                 and if the amount of data is huge which                                 needs to be copied from one cluster to                                 another bootstrap may take a significant                                 amount of time and CDCR does not support                                 transaction and pull type replicas as of                                 now solar has three type of replicas nrt                                 near real-time transaction and pull type                                 out of which only nrt replicas are                                 supported so there will be some                                 improvements that will be coming around                                 CDCR and solar the first thing will be                                 moving the configurations from solar                                 config dot XML to zookeeper what this                                 will do is it will remove the limitation                                 of reloading the entire collection                                 whenever a CDCR specific property is                                 changed then some new api's will be                                 added the reload CDCR modify CDCR and                                 support for transaction and hole type                                 replicas will be added in the future so                                 this is the references page you can go                                 and have a look at them they'll give you                                 more                                 knowledge about the CDCR feature and                                 given insight about thing we have also                                 mentioned the github handle where you                                 can find the scripts we ran while the                                 demonstration that's it from our a thank                                 you very much we would like to welcome                                 any questions you people have yeah you                                 have finished a bit early in case of                                 this by the record bi-directional                                 synchronisation what happens if there                                 are two conflicting updates no two                                 conflicting updates the contact and                                 updates yeah that's a great question                                 actually                                 now blockchain please yeah yeah so if a                                 same update has been sent to the Buddha                                 I'm like the both the centers at the                                 same time with the same timestamp they                                 will be indexed into itself forwarded to                                 each other and one of them will be                                 rejected at the other end so what really                                 happens is the version of the update                                 should be greater than the current one                                 to be index itself to get accepted if                                 the index is not unlike a bigger than                                 the already one there it will be                                 rejected anyway so when you are doing                                 the simultaneous indexing to both data                                 centers you have to have to make sure                                 that you don't set the same update at                                 the same time otherwise obviously at the                                 yeah so otherwise let us suppose a data                                 center at the berlin has indexed the                                 same update and at someone in the new                                 york has updated the same update one of                                 them will not be able to see his own                                 update because that will be yeah                                 overwritten by either one of them so so                                 one so they will be synchronized yeah                                 that would be more like not lost it's                                 more like depending on the version it                                 has been assigned on the basis of the                                 machines timeline right because these                                 versions are calculating to the servers                                 machines so if these machines are in                                 sync let us suppose they're in CST                                 timings or maybe in est time into                                 something like that then we will ensure                                 that at least                                 one of the updates will be there all the                                 time of whatever has been received after                                 but yeah I'm like this is one thing we                                 even in the documentation solar official                                 documentation                                 it is absolutely mentioned that always                                 index at one datacenter at one time even                                 if you have the bi-directional in place                                 okay thank you
YouTube URL: https://www.youtube.com/watch?v=NKR6B9bWz-s


