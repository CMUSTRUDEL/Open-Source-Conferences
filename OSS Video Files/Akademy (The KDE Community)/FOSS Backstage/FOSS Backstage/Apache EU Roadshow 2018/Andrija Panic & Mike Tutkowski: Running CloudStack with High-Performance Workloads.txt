Title: Andrija Panic & Mike Tutkowski: Running CloudStack with High-Performance Workloads
Publication date: 2018-12-05
Playlist: Apache EU Roadshow 2018
Description: 
	This presentation will focus on my real-world experiences transitioning from a smaller CloudStack IaaS provider with light workloads to a more sophisticated configuration required to run enterprise workloads. We will cover specific differences in running KVM-based CloudStack on shared storage. In particular, we will focus on discussing light workloads vs. those having more serious CPU and IO demands and how to solve different stability, performance and usability challenges by leveraging Managed Storage in CloudStack
Captions: 
	                              my name is Andrea punish I'm senior                               cloud system engineer with in here data                               and over here we have Mike who probably                               know                               yeah I'm Mike tech Kowski been in the                               cloud tech community for about five and                               a half years now I'm a PMC for about                               four years and I'm the current vp of                               Apache cloud stack I'm actually have a                                very minor role in this particular                                presentation is Andreas has pretty much                                collected all of this information on his                                own I have a small part later so I'm                                gonna disappear for a while and and come                                up around in the middle of the                                presentation for                                                       so basically this is going to be a short                                story of let's say or kind of sharing                                experience from our side this we grow up                                from well at the beginning of obviously                                startup mode company what kind of issues                                we experienced with the cloud stack with                                storage with our infrastructure and so                                on so basically there are growing up                                issues you can say and then how we                                basically sold it and how we now                                transition to using the managed storage                                or the SolidFire solution so let's move                                on just a bit of storage history but                                previously I would just mention we are                                running purely KBM advanced networking                                so VPC stuff no basic zones nothing it's                                purely let's say VPC we are we ended up                                using at the moment for the Tate release                                with a lot of let's say changes internal                                fixes and so on improvements so I'll try                                to be a bit quick because we lost some                                unexpected time originally we basically                                when we were in seriously startup mode                                let's say we had certain amount of                                hardware and we said what kind of                                storage can we do that can provide us                                some nice features obviously we went to                                itself a bit later we went with NFS                                expecting a bit more performance a bit                                less let's say integration problems                                eventually we went with a let's say full                                SSD based NFS solution this is as you                                can read do it yourself solution so this                                is not net up or anything similar                                they're here some drawbacks obviously                                but it's also pretty good you know the                                integration to cloud stack is probably                                the best one among different primary                                storage solutions but still there are                                some shortcomings so in the next few                                slides basically I will speak about a                                different set of challenges and issues                                which we experienced during this period                                of using Cepheid NFS and then how we                                transition to moving managed storage so                                the next few slides don't observe them                                as some kind of complaints that this is                                just honest honestly sharing some of the                                issues which we had and later how we                                sold and so will basically speak about                                stability performance and usability and                                management challenges or issues I need                                to mention that most of the stability                                issues are fixed especially some of the                                things are actually very well addressed                                in the                                                           previously explained we will speak about                                that later as part of this performance                                we may obviously vary depending on your                                hardware and what you use in your setup                                and usability management wise some                                features stays some features are being                                improved and that's that's pretty much                                it so briefly on stability we originally                                faced an issue when any kind of                                exception in the Lib RBT which is                                basically our PD client library for for                                talking to itself from the liver it                                would basically crash any kind of                                exception would cause agent to crash and                                this is what Paul mentioned previously                                that you cannot know if your VM is                                really running now or not and since we                                enable at the moment VM high                                availability cloud stack will go and                                start the VM on another host while                                actually the VM is still running so you                                would get the storage corruption for the                                volume of the VM very nice and                                convenient so all these issues are                                basically resolved at the meantime or at                                least most of them so if you go together                                itself at least on stability and issue                                 wise you are in much much better shape                                 that needs to be said several balancing                                 deep scrubs will mention it as well on                                 the performance let's say section but                                 basically any kind of data rebalancing                                 which is moving data internally between                                 safe nodes for any reasons repairing                                 Delta score you know rebalancing data                                 for any reason and the deep scrubs which                                 are based                                 equivalent to low file system file                                 checks on the hard drives or SSD drives                                 it use you know it safe plaster can                                 impact client i/o originally due to our                                 own let's say we can say mistake or at                                 least less than optimal sizing of the                                 safe cluster and combined with sir with                                 that deep scrubs we ended up even in                                 some situations that will be happened                                 only once but was very funny that client                                 I or wasn't served for                                                 you can imagine that client VMs will go                                 to read-only and you say crap it was                                 really fun this was very beginning of                                 our let's say storage issues then also                                 flapping monitors in droids these in                                 self do two different network issues can                                 also cause slow request same same let's                                 say stability stability penalty flapping                                 is the official terminology for going up                                 and down basically and monitors and wise                                 these are these are object storage                                 daemon in itself let's say hard drives                                 and monitors which are monitoring the                                 whole infrastructure can go up and down                                 in safe can become really crazy if you                                 have issues with network devices cable                                 switch ports defective firmware like in                                 our case and so on so just be aware that                                 safe is very sensitive to network make                                 sure it's stable volume snapshotting can                                 then also take long time to complete                                 simply not really the snap shooting                                 process but the process of coping over                                 the snap to the secondary storage and                                 this is now a very nice improvement that                                 Paul mentioned previously there being                                 decouple so when you execute a PR for                                 creating snapshot it will almost be                                 instant it won't last for two hours                                 three hours because you imagine one                                 terabyte volume somebody creates a                                 snapshot it's being copied over to                                 secondary storage we help customers like                                 that and then simply because this is                                 executed in sequence you can read more                                 at the last slide I provided a set of                                 links where you can read the description                                 of the original issues basically this                                 will block the job queue for the other                                 task is in the agent queue for on the                                 same KVM cost so for example you might                                 end up in situation when you cannot                                 start with your router because the                                 snapshot is running it's kind of silly                                 but this has been obvious                                 the address at least the specific issue                                 and there are some improvements being                                 done on the snap shooting sites as well                                 in general on seven KVM or actually in                                 kayvyun in anyway I then also I                                 mentioned we use the NFS do-it-yourself                                 solution so not Enterprise it was based                                 on the DFS and we once went to a kernel                                 panic which is basically my last name so                                 basically all agents rebooted themselves                                 at some point in time because agents                                 usually assume that you have our good                                 quality and when two one two one two one                                 two                                 a and SMO hey hey that's not me hey one                                 two                                 [Music]                                 dude you did good didn't it turn yeah                                 good yeah I guess I can it's okay for                                 your can I continue in the meantime or                                 let them play Macarena or something so                                 at least I know I know that one anyway                                 back to the topic so back in the days                                 this was again                                 first let's say first year of our cloud                                 operation due to the caving carpet                                 script will basically rights to the                                 primary storage we're talking actual                                 about NFS only for now implementation                                 agents assume that you have a stable                                 highly available storage and if there                                 are issues writing to the storage agents                                 assume that they are the ones that are                                 in problem so agent let's say okay I                                 have the stupid guy let me reboot myself                                 and because it was actually a storage                                 issue all agents rebooted himself very                                 funny experience to be honest but still                                 we didn't have too much minds back then                                 so it's okay then if you moved forward                                 to the performance I already mentioned                                 briefly rebalance dip scrubs and                                 flapping ways these are monitors Kennish                                 you know cause serious performance                                 issues and basically what you can do in                                 to solve this is you can throttle the                                 rebounds process which means that it                                 takes literally days to finish but it's                                 much less impact on the client i/o so                                 you don't have you don't care let's say                                 penalties in that sense disable dip                                 scrubs                                 believe it or not we had to do this                                 because simply when you do our low file                                 system check across drives we still end                                 up with having a blocked I requests                                 replace wise these if you know the drive                                 is about to fail or network devices we                                 recently after some maintenance had                                 issue with defective cables defective                                 switch port                                 and maybe something else and basically                                 Steph was going just up and down up and                                 down and it was causing hell of a lot of                                 block tile requesters actually not                                 recently but during the last maintenance                                 that I can remember so make sure that                                 your metal is okay then this is                                 performance not necessarily penalty but                                 no we couldn't use improvements that are                                 in general possible when you are using                                 client libraries or client side to talk                                 to that self cluster or to have write to                                 the safe cluster basically you can                                 define it you can say to that self i                                 want to strap my data across multiple                                 objects so it's really imagine rate zero                                 across five ten ten devices in red zero                                 it's not really redundant obviously but                                 here you can basically defy the stripe                                 size imagine you're writing I don't know                                 forty megabytes of data you can write it                                 across theoretically silly numbers but                                 you can write it across ten objects of                                 four megabytes and these eventually and                                 in the best possible scenario will be                                 written to                                                             or nine depending on the placement                                 groups and everything but in Lee                                 brothers there was no yet support at the                                 moment when we were using it so we could                                 not to make let's say we could not make                                 a better performance so we use default                                 let's say so to speak then also one huge                                 issue at the beginning is that there was                                 no proper self snapshot removal process                                 so basically you create a snapshot it's                                 being copied over from set to secondary                                 storage when you won't want to delete a                                 snapshot CloudStack will delete it in                                 database it will delete it on secondary                                 storage but there will be left self                                 snapshot on primary storage so imagine                                 hourly snapshots which is                                              per day times a couple of days you end                                 up having like                                                         primary storage and I will mention this                                 later as well this causes huge huge huge                                 performance penalty what we temporarily                                 did we just hit and the hourly snapshot                                 button and kindly ask our clients to                                 stop doing all right snapshots until the                                 things like fixed and then we eventually                                 implemented proper safe snapchat removal                                 cause this is our humble contribution to                                 community we didn't contribute maybe too                                 much but at least this useful part is                                 there starting from                                                     you couldn't continue with performance                                 whatever kind of evolves shared                                 environment you use network storage                                 doesn't matter if you don't have proper                                 quality of service you will eventually                                 end up with the noisy neighborhood                                 scenario                                 so basically when we are not speaking                                 purely about kayvyun so i'm referencing                                 KVM as well actually only gave him here                                 when you create your disks on shared                                 storage there is actually a quality of                                 type set to hypervisor                                 which will basically generate just a                                 limit now let's say to read and write                                 rates in either bytes per second or iOS                                 per second whichever limit is reached                                 first these are not                                        these are any size iOS I will show you a                                 nice demo hopefully after after this                                 let's say text part of the presentation                                 this is for some reason not applied to                                 the hot plug volumes so you know imagine                                 client who wants to benchmark your                                 storage up touch ten volumes do raid                                 zero and then kill the storage it was                                 very funny                                 back in the days for me personally this                                 is not really true quality of service                                 it's more like damage of control of a                                 kind but it's still much better                                 obviously than not having anything so                                 this is still good enough to avoid at                                 least most of the noisiest neighborhood                                 scenarios now a bit of my wrote it here                                 like to be funny in theory versus                                 practices exercise imagine here                                     client VMs which have optimized arrival                                 for let's say you are doing I don't know                                 some sequential writes or if you're                                 using self you do RBD caching on the k                                 vm side so you have                                            sequential streams that you send to the                                 storage safe NFS doesn't matter                                 so in general any kind of storage can                                 very well handle sequentially you agree                                 you know hard drive we do on almost                                     single hard drive                                                   second but you know the four key random                                 i/o it can do barely four hundred or or                                 or six hundred kilobytes so it's it's                                 really but the problem that happens is                                 basically that                                 in the backend on the storage you give                                 too much streams interleaving and this                                 becomes absolutely                                                     this is the reason basically at any kind                                 of hardware solutions with no matter                                 what kind of magic location we actually                                 use become absolutely useless and that's                                 the time when you actually need to                                 employ SSDs and use the random basically                                 i/o power that it provides this there is                                 actually very nice you can okay my laser                                 is dead                                 there is very nice actually white papers                                 from Intel how to reach                                             haves with the Intel and some software                                 solution that will actually backup this                                 exactly this theory so moving forward on                                 usability and management what I'm                                 briefly mentioned previously as well                                 there was no there was actually snapshot                                 lifecycle issues when talking with self                                 and then due to the certain RBD layering                                 issues we end up having serious                                 performance degradation across cold safe                                 cluster not adjust the volumes which get                                 these snapshots we even implemented some                                 custom snapshot deletion script which                                 will basically clean up any unreferenced                                 nap shot from safe and the later                                 implemented code which I already                                 mentioned now to here to make things                                 even more fun for us at the same time                                 there was a certain issue variable size                                 or something similar in rather Java                                     where if you have more than                                             it will basically crash when you want to                                 delete the volume which is on self                                 basically the rather Java try to protect                                 all the snapshots and need to try list                                 me all the snapshots and you give more                                 than                                                                  handle more than                                                 basically this is the time where your                                 version was implemented by Widow some of                                 you who are on the community lives                                 actually knocking video then commander                                 and big things to Widow for fixing this                                 because this is this was basically                                 combined with this above issue and it                                 was very pronounced for us you know                                 whenever customer deletes the volume we                                 have age and this connection we have iam                                 downtime so a lot of um downtime with                                 self but not necessarily because of                                 death it's also combined with with let's                                 say our own maybe mistakes as the                                 process of learning its snapshots                                 already mentioned imagine we actually                                 did here a customer who                                 was trying to snapshot two terabyte                                 volumes but they're being okapi door to                                 secondary storage imagine a scheduler on                                 a huge volume that means that you have                                 all the time terabytes and terabytes                                 moving from primary storage to secondary                                 storage if one of the snapshots because                                 of the timeouts becomes broken it's                                 stuck in an allocated state or some                                 intermittent intermediate state it's not                                 fully finished then it will actually                                 break the snapshot scheduler in general                                 and you need to go to database dig clean                                 up things and so on so you need to                                 basically educate customers these are                                 not really snapshots in real reality                                 these are more like volume backups which                                 we will touch a bit later                                 continuing with snapshot when talking                                 about NFS and self but there is no                                 actually possibility to revert back                                 these resources to a snapshot at state                                 so there is no revert back there is only                                 not only that's good enough when there                                 is no better solution you can provide                                 provision new resources or so VM                                 template from the root volume snapshot                                 or data new data disk from the data disk                                 snapshot and this actually takes time                                 obviously and also a bit of billing                                 issues for the customer some minor                                 things is like improper file extension                                 when you download volumes and improper                                 file type in database which later causes                                 different issues which this was                                 basically all locally fixed and I also                                 believe it's fixed in the meantime in                                 the upstream so again this is all like                                 three years all the issues but I'm just                                 describing you my or actually our fun                                 not - right then a very important topic                                 is a volume equation between different                                 primary storage it's if you're using Zen                                 or VMware you have this nice emotions                                 and emotions own things that you can                                 basically live migrate your volumes from                                 one storage cluster to another storage                                 while the VM is running so no down time                                 during moving volumes from one storage                                 to another which is very nice thing to                                 do for KVM there was only so far                                 possibility to do                                 offs wine storage migration basically                                 need to stop the VN migrate all the                                 volumes which can take hours in some                                 cases and then start them and then                                 customers are not really happy because                                 of the downtime and now this                                 this is my one of my favorites and we                                 will have a demo combine them actually a                                 bit later there is nothing special about                                 this case because it's so common or at                                 least should be very nice and with no                                 problems - viim live among Croatians                                 which is basically let's say ROM memory                                 migration has nothing to do with storage                                 at this point in time but later you will                                 actually see how and why it has                                 something to do with it if you have a                                 busy VM which basically means the VM                                 which gets a lot of a high ram memory                                 change rate so busy sequel server for                                 example which we experience with                                 microsoft sequel and so on for some high                                 workloads if there is a high ranch ram                                 change rate basically the camera                                 migration of thread will never look and                                 it cannot catch up with the amount of                                 changes that this VM is dying doing -                                 its RAM memory and basically due to this                                 you cannot actually put your hosts into                                 maintenance mode which is kind of really                                 silly you will agree so you know you                                 need to do some kernel kemo operates                                 whatever you cannot put the cost into                                 maintenance mode you can below read I                                 will not spend some time right now but                                 you can below read I extremely simple                                 way how to artificially obviously                                 reproduce a busy VM and just watch it                                 being migrated forever                                 but how we actually solve this I speak I                                 will speak later in more details we're                                 basing employing the dynamic auto                                 convergence from camel which basically                                 dynamically throttles the CPU more and                                 more and more and more as enough                                 basically that the busy VM is not so                                 much busy anymore and then it's Ram                                 change rate gets slower or low enough at                                 least and then kmo can actually finish                                 the migration this is just a funny rate                                 of changes per second this is gigabytes                                 not Giga beats gigabytes you will see                                 this you know demo as well and it works                                 now we will do it later so this part of                                 the presentation where I'm supposed to                                 be funny because I don't care nice                                 graphics squirrels penguins nothing                                 since this is open-source it's a bit of                                 boring presentation so failed attempt to                                 be funny how can we solve all these                                 challenges we can basically stop                                 expecting higher performance or stop                                 using us who needs snapshots                                 migration anyone or we can basically                                 become let's say serious and and and see                                 how to actually really solve these                                 issues you can install this in a certain                                 ways by obviously even with with our                                 current storages but what we do we                                 basically some how it happened that we                                 combine solution to many different                                 problems together with going to with                                 SolidFire at a time where we transition                                 to moving SolidFire so Mike this is five                                 minutes for you yeah Michael basically                                 tell you a bit about in general about                                 SolidFire and then I'll take or for the                                 rest of the presentation I can't                                 remember is the next slide yeah all                                 right one so yeah I work at SolidFire                                 it's been two years now since it was                                 acquired by NetApp the main thing that                                 I've done in the cloud stack community                                 in the five or so years that I've been                                 involved with cloud stack is bring                                 quality of service to you know the cloud                                 stack cloud environments so as Andrea's                                 was mentioning cloud stack was never                                 really built with the concept of quality                                 of service in mind it was always                                 designed around either NFS or maybe                                 really large I scuzzy volumes that you                                 might put a clustered file system on and                                 then have your virtual disks share that                                 clustered file system but there was                                 really no planning around quality of                                 service which is a critical component in                                 any kind of a cloud you don't know what                                 other people who are sharing those                                 storage resources are up to from day to                                 day so if you have mission-critical                                 applications in some cloud it could turn                                 out that that application is running                                 great one day and then the next day the                                 performance is tanked because other                                 people are utilizing the cloud more                                 heavily and just from day to day you                                 really don't know what to expect from                                 your storage performance without having                                 some kind of guaranteed quality of                                 service so that's where where SolidFire                                 comes into play and what I've been                                 working on in the cloud stack community                                 over the years will see in some level of                                 detail through demos in a moment how                                 this actually looks in practice but from                                 just a high-level point of view the idea                                 here is to be able to deliver guaranteed                                 storage quality of service to the                                 virtual disks of each and every VM in                                 your cloud so that you don't have to                                 worry about what other people maybe                                 people you don't even know they don't                                 work for the same company as you you                                 don't know who these peers are who are                                 sharing your cloud environment well it                                 doesn't matter if you have a robust                                 quality of service system backing that                                 cloud you have guaranteed performance on                                 each and every one of your virtual disks                                 and you don't have to worry about what                                 everybody else is doing in the cloud at                                 all so I will leave it at there and let                                 you continue on with the rest of your                                 presentation sure so you can basically                                 read a bit of details here but these                                 presentations will be also online so you                                 can maybe read it more you know in a                                 more piece let's say I would like to                                 move forward if that's ok but basically                                 what you have the option to guarantee we                                 are the minimum I ops property of a                                 volume we will see this in the demo you                                 have the option to make                                                 performance in normal and in the most                                 vaulting conditions including                                 maintenance cluster expansion replacing                                 that these cases the hell checks which                                 are may be equivalent to safe deep                                 scrubs and so on basically because part                                 of the ops is let's say sold or offered                                 for the end the user customer is part of                                 the SSDI ops is reserved for internal                                 purposes so it's very transparent during                                 any kind of maintenance your performance                                 is not degraded so I yeah last but not                                 least no special external library                                 because at the end                                 SolidFire volumes are basically just I                                 scuzzy                                 lunes connected to the KVM costs and                                 just passed through all basically like a                                 local disks to the vm so no way to                                 actually hear the liberty issues like we                                 did give or originally back in the days                                 with the lib RBD which is required to                                 communicate with the self cluster from                                 liberty so basically there are a list of                                 some of the improvements of the manor                                 storage which was also mentioned briefly                                 by Paul a lot of these are basically                                 come from the here and you can also see                                 this is not necessarily meant to be a                                 really you know commercial for soil fire                                 so you know some of the things are also                                 missing over there some of the I don't                                 want to say basic things but you know                                 some of the things that we could do with                                 NFS and self now when we move to                                 SolidFire we could not do some of these                                 things so these were all these features                                 which were basically missing are now                                 improved and/or implemented actually by                                 Mike and one of the more important ones                                 was offline storage emigration and                                 eventually the online storage migration                                 which is let's say equivalent to the V                                 motion or or Zen motion for KVM inside                                 the cloud stack this is actually                                 absolutely new thing inside cloud stack                                 at Cambrian world proper out conversions                                 and so on we will basically speak about                                 that but basically from the operator                                 cloud operators are signing what we                                 let's say offer or sell to our clients                                 we feel basically want one ratio like we                                 are not losing any feature which we                                 previously did offer with EF an NFS or                                 non managed storage so I will skim                                 through these slides really quickly                                 because you already talked about them we                                 are running public clouds so it's not                                 private its public stuff you cannot                                 control what customers do they rely                                 heavily on snapshots but we already                                 mentioned take ours and these are not                                 really truly snapshots because their                                 volume copies we even internally rename                                 the features to volume back ups not                                 snapshots and basically so on so forth                                 we mentioned all this how we basically                                 solve this with we SolidFire basically                                 snapshots on SolidFire stay on solid                                 fire it's true as well for NFS and safe                                 but clouds stapled and copy over                                 the volumes but with SolidFire there is                                 a difference these networks are not                                 copied over to secondary storage this is                                 huge difference they're all reference                                 and they you work directly with snapshot                                 soon on my own primary storage which is                                 I believe something similar or possible                                 with Zen with a non managed storage so                                 your reference at everything                                 the recommended primary storage if I'm                                 not mistaken but I'm not a Zen guys so                                 if I'm wrong sorry for that for these                                 are obviously real snapshots you it                                 takes little three to four seconds to                                 create and restore from very easily they                                 don't consume almost no space not                                 literally but almost no space and you                                 don't need to move and write hundreds of                                 gigabytes basically to the secondary                                 storage there are three four seconds so                                 you can start to reach your router there                                 so in the next part of the presentation                                 we will we have five demos but I'm not                                 going to do them obviously due to time                                 constraints this is very short one but                                 basically I kindly ask you to view it at                                 your own pace the presentations you can                                 download later and there we also links                                 at the last slide so I'm not sure how it                                 works here on the Mac mic but oh you                                 play the embedded you'd YouTube                                 [Music]                                 okay this is the metal video tube that                                 you reload directly from from internet                                 for some reason it's not displaying but                                 I I have the links yeah yeah but this                                 one we are going to skip anyway but you                                 can actually in this previous you can                                 see how you create a snapshot with some                                 important data you artificially produce                                 data damage you lose some date and then                                 you revert it's literally you know three                                 minutes to reproduce the whole thing                                 it's very simple it's very fast it's                                 very efficient it's nice very short                                 actual and nice demo so please take a                                 look at it later                                 now we are going to do a demo as well on                                 the quality of store of storage we                                 already mentioned all these let's say                                 issues or or limits of the hypervisor                                 level quality of service on seven NFS                                 volumes and basically how we solve them                                 we leverage what is actually the                                 foundation of the SolidFire they're all                                 built around the quality of service we                                 guarantee performance per volume and                                 that's the minimum my UPS but basically                                 client will usually get more always get                                 the maximum my ups if the cluster case                                 in our free resources and there is also                                 one a very big critical difference here                                 is in a non managed storage or let's say                                 an NFS server there is no easy way to                                 actual monitor or cluster performance                                 how do you know if your storage cluster                                 becomes too busy that it degrades the                                 performance it's impossible to know but                                 we saw it fire you actually have a                                 basically the perfect point monitoring                                 environment where you can actually see                                 if you need to add more space or more I                                 ops for that matter so this is a quick                                 demo now which we are going to do I'm                                 going to probably actually need your                                 help Michael please I also have the                                 links to the last slide as you like                                 the presentation was made on Windows and                                 for some reason it's not displaying them                                 on Mac yeah yeah this is the second one                                 please you can                                 yeah I'm going to mute yeah sorry for                                 that so basically here I'm going                                 what happened                                 it's not this playing                                 first screen second screen alright it                                 was there but it takes time more right                                 going to the left rather it                                 ah something-something happening yeah                                 yep                                 you can move the mouse and watch were                                 there if you can just rewind the video                                 please yeah sorry some technical issues                                 too much of them so we have a demo VM                                 basically I'm going to show differences                                 between hi provides the level quality of                                 service how its implemented and and                                 SolidFire so we have a root volume which                                 is on an NFS freshly deployed VM if you                                 do the video XML you can read the i/o                                 Tunes section which says read bytes                                 fried bytes that's the part where you                                 actually generate part of the XML please                                 do it at your own pace later but yeah                                 that's when you consume hypervisor level                                 quality of hypervisor type of service                                 basically when you define certain limit                                 reads and writes a rate that is                                 basically sent to the XML file that's                                 generated inside livered now if we                                 briefly attach a few data volumes which                                 I just pre created previously to the                                 same VM we will see the differences                                 between SolidFire and the other data                                 volumes pretty quickly                                 yeah so you can basically check the VM                                 just to make sure that all volumes are                                 attached by the way these videos are                                 with sound so you can view them you know                                 normally on youtube so back to the                                 hypervisor we can check inside the VM we                                 see now we have total total four drives                                 got three of them got plugged first NFS                                 set and then SolidFire data what if you                                 do the video dumping XML we still have                                 the item section for route volume but                                 it's missing for two data volumes NFS                                 and self and it's missing by design on                                 SolidFire volume this is the last volume                                 which we attached so basically were just                                 going to briefly stop the VM and start                                 it so it picks up the proper parameters                                 from classic database and generate                                 proper XML while the VM is stopping I                                 will basically show you very briefly                                 this is SolidFire GUI we have our                                 quality-of-service volume which we                                 created the minimum miops maximum burst                                 so you set I use pipes certain types                                 limits per volume and this is a                                 difference that there is nothing set on                                 the hypervisor side not throttling                                 nothing we do that on storage side VM is                                 being started and basically to now pick                                 up the the different the different                                 parameters yeah it will pick up the                                 different parameters basically let's                                 just copy the command and yeah you can                                 see over here now I will repeat the same                                 there's a big similar command route for                                 him                                 data data all here propriety on sections                                 so far is missing the one Mike would                                 need your probably or help again to                                 switch back to the so the presentation                                 in the rest of the video you can see how                                 you define hypervisor versus storage                                 quality of service but I'm I am in                                 Fordham a bit short on time so I want to                                 do a more interesting demo this is one                                 is kind of basic                                 okay so we did this one now important                                 topic is basically migration from non                                 managed storage and efforts f                                     SolidFire you basically here so far                                 already new cluster new volumes are                                 being provisioned there in whatever way                                 you implement this via disk offerings                                 but then how do you migrate the volume                                 so you can do offline storage migration                                 which is logistically not really                                 possible due to downtime you need to                                 organize downtime or you can actually do                                 the online storage migration which is                                 something new that might be implemented                                 for                                                                      for                                                                      of this with certain version of livered                                 and VM volumes are basically being                                 migrated or actually mirrored in                                 parallel on a block level and at some                                 point in time which we will see in the                                 next video the VM is being eventually                                 also live migrated so but we have what I                                 mentioned previously the busy VM issue                                 when when this part is basically done                                 the volumes are live migrated online                                 storage migration key is done then you                                 hit the VM live migration to another                                 cost but if the VM is busy it will never                                 complete so Mike did a great thing by                                 implementing this but it rendered it                                 useless if you have a busy VM so we                                 implement the Cameo making use actually                                 of came out or convergence through the                                 cloud stack with a code and global                                 parameter it requires a newer version of                                 Khemu because it implements dynamic auto                                 Congress intelligently throttles more                                 and more which came with all the                                 versions of cammo the raziel regular                                 auto congress convergence is not really                                 useful at all so this is purely demo                                 which I'm going to skip just out of                                 coverage convergence versus busy VM                                 we're going to do super demo later then                                 we have online storage migration which                                 is analog to V motion or or Zen motion                                 stuff consumed from cloud stack this is                                 not new in leaverton KVM but there was                                 no support so far on cloud stack so                                 basically this is just a bare                                 functionality demo which we're going to                                 skip also and now I'm going to do our                                 next demo which is super them of these                                 two but with a very busy VM in a very                                 busy storage                                 explanations you can read but I will                                 tell them during the demo so Mike jump                                 in and I'm done after this demo we had a                                 bit of                                 yep                                 good thanks Mike so basically focus you                                 log into your environment and the                                 original demo is four to six minutes but                                 I I will explain it in in six or seven                                 minutes total so we have a demo VM let                                 me just put into fullscreen okay so you                                 can observe this is a                                              observe as a host on which it's running                                 we can observe volumes of this VM we                                 have a root volume which is not relevant                                 it's on SolidFire we don't care for the                                 purpose of this demo but we do have two                                 additional data volumes which are two                                 NFS volumes of                                                          do rates also raid zero across two                                     gigs and right with                                                   second to produce a busy storage and                                 then we are going to do a similar with                                 with with a CPU and RAM so this is just                                 volume overview you can see both of them                                 are on NFS and then if we go to the                                 hypervisor level this is a source host                                 we have a VM running over here we also                                 have a destination host because when you                                 do emigration storage is moved but also                                 the VM is live migrated on the                                 destination because there is no VM                                 running on the source host during the                                 migration which is not yet start we will                                 observe the progress of the live                                 migration ram memory live migration we                                 are also going to observe in another tab                                 also on the source host the block level                                 jobs which is basically done in storage                                 migration progress these are two NFS                                 volumes which are being mirrored which                                 will be mirrored or migrated actually to                                 SolidFire inside the VM we are using the                                 stress test utility from Google writing                                                                                                          busy absolutely for                                                    storage side we see we have                                             of                                                                    raid                                                                    a total size of let's say                                              we are going to use the fee or file fee                                 or whatever you spell that utility which                                 probably you are familiar with that's                                 benchmarking                                 but I am rating this or capping this to                                                                                                     purely right to the storage because this                                 is dev environment so we are not really                                 unlimited performance-wise zero CPU                                 usage at the moment zero with iost at                                 zero disk usage across the software ate                                 everything zero so basically will now                                 start actually we now produce a busy VM                                 this is stress top utility that makes a                                 b b vm busy instance of ram changes it                                 is just coping over ram memory pages                                 really with a crazy speed we are                                 starting fear we are writing with                                     megabytes you can observe over here per                                 second and it takes some time to build                                 up this but basically we will now                                 observe all                                                         percent full and basically we can                                 observe that with a very stop utility we                                 can observe physical host CPU                                 utilization                                 this is                                                                  in a separate them we will see how this                                 percentage goes down as the vm is being                                 throttled by dynamic out of convergence                                 you will see it in much more detail so                                 please make sure you you view it it's                                 very interesting one we are using the                                 cloud monkey command for now only in                                 storage migration is available only via                                 API call this is a new migrated to                                 machine with volume we are referencing                                 virtual machine ID host ID to which the                                 VM will be migrated and then migrate                                   pool migrated                                                           pool migrated one storage so telling                                 them these two volumes sorry tall                                 volumes not storage volumes migrate them                                 to this same pool in these cases                                 theoretically it could be two different                                 SolidFire clusters we started the                                 migration job we wait for a few seconds                                 so it really kicks in on the hypervisor                                 level it's now still let's say building                                 up so the block level copy progress or                                 the volume mirroring from NFS to self or                                 online storage migration if you want is                                 starting obviously from                                               being progressing if you check the                                 destination course we have a destination                                 VM which is in power state it has the                                 same excel file except the difference                                 that it's not reference NFS volume it's                                 referencing soil fire volumes and the                                 ramp quantities being copied or this is                                 SolidFire GUI I'm basically here these                                 two new volumes which have been                                 automatically provisioned as up as you                                 know parallel to them to NFS volumes I'm                                 just increasing the maximum me ups for                                 these volumes to speed up the migration                                 process and for both volumes let me skip                                 for five seconds to save some time and                                 then we can observe in the reporting tab                                 you can observe volume performance here                                  you can see very granular per volume                                  actual current performance it's still                                  been building up it refreshes every                                     second but basically I now configured a                                  split view the video is paused and                                  continued to be recorded a few times so                                  if I scroll down you will see different                                  percentage going up but all the time                                  while the block level jobs running                                  basically the copying of the NFS to                                  SolidFire you will also see certain a                                  throughput on the SolidFire storage                                  being written to as a part of the                                  migration obviously and you can observe                                  this until you get bored to tears later                                  I switched up to this a different view                                  still it takes some time to finish a                                  block level migration when it reaches                                     point something percentage then the vm                                  live migration will kick in so now I'm                                  going to rewind to                                                      this number in your in our presentation                                  so you can revive yourself as well and                                  after that the lime aggression will kick                                  in and will be very soon done with the                                  presentation so it rewinds faster on                                  windows just to tell you the truth but                                  it doesn't work so when the volumes are                                  basically                                                              will decide at some point to kick in                                  with the live my creation of the vm                                  which is run a more reliable creation                                  and here you can observe memory                                  bandwidth which is coping rate and so on                                  so forth you can observe number of                                  iterations this will climb to                                     iterations because first iteration it                                  will fail then Kemah will throttle the                                  CPU then the second iteration will also                                  failed to throttle more and more and                                  more and more out basically until you                                  reach                                  some                                                                     go to                                                                 being throttled more and more and more                                  and then throttle actually enough that                                  these                                                                  can it's not busy anymore that much and                                  it will it will basically finish                                  eventually this took actually one point                                  half hours without any video recording                                  but that's that's pretty much it at some                                  moment the VM will finish the migration                                  and it will it will disappear from this                                  source cost which is basically now I'm                                  done in leading literally one minute so                                  sorry for that okay                                                     you can find some links direct the demo                                  links which are also part of the                                  presentation embedded some other links                                  from Mike what to expect with Seth and                                  so on and there is a small bonus script                                  from here which we use internally to say                                  you have this VM character migrated to                                  SolidFire it will find all the volumes                                  which are not on SolidFire and migrate                                  them over sorry for all the technical                                  issues it's beyond or I just wanted to                                  point out that his videos that we some                                  of them that we did see and some that we                                  didn't see he has them all narrated                                  nicely so if you if you do watch them                                  online it's not just a silent video                                  everything is explained nicely so I                                  really do encourage you to take a look                                  at those and yeah he put a lot of great                                  work into making those videos and                                  providing great explanations so go ahead                                  and check them out if you can Thanks                                  ok thanks guys yeah unfortunately had                                  some technical issues if you have some                                  questions just ask the guys during the                                  break thanks a lot guys                                  [Applause]
YouTube URL: https://www.youtube.com/watch?v=qWGK4z33Mzo


