Title: Lightning Talk: Nate Day - Dense Vectors in Elasticsearch
Publication date: 2020-07-02
Playlist: Haystack - Joint Virtual Event 2020
Description: 
	Lightning talk from Berlin Buzzwords | MICES | Haystack â€“ Joint Event 2020
Captions: 
	                              sweet thank you Charlie can you guys                               hear me okay yeah                               can you see me okay yep cool all right                               well that's good for me bad for y'all so                               I wanted to just say thanks to everybody                               coming to this talk I'm kind of very                               very green in the search rope I joined                               osc back at the end of                                             just started to dip my toes into NOP                                search so this is the little kind of                                real life story about that so motivation                                for this talk is everywhere I turn                                around when I'm reading blogs and stuff                                trying to get up to speed everyone's                                talking about dense factors right all                                these fancy schmancy BIRT based encoding                                machines that are really kind of just                                tearing apart benchmarks left and right                                so what I wanted to do was try and apply                                some of that so this is kind of like the                                background requirements for what I'm                                talking about                                let's talk kind of the things that made                                what I'm doing possible is this kind of                                idea of since transformers right that's                                not really new that that's a link I                                dropped a link in slack too if you guys                                want to click along I tried to link out                                all of the resources that I'm using so                                you can follow along I'll touch on data                                a little bit I can't really share an                                example dataset but I'll tell you why in                                a slide so the background really is that                                there's wonderful pre-trained models out                                there designed to get at this idea of                                sentence similarity or or meaning                                inference and elastic has made some big                                releases to kind of allow this kind this                                kind of data this new kind of feature to                                come in to elastic recently so the dense                                vector released in                                                    what made it super useful for people is                                that you can actually now they                                introduced this cosine similarity                                function and so cosine similarity really                                is just an idea that you're gonna find                                out what's most similar in terms of                                angle and distance and angle and a                                vector space so things that should be                                kind of on the same mark and the idea is                                that that would surface ideally like                                like intention door                                like like documents and so the the                                specific application context here for me                                is the trek news track my background                                before coming to OSC was working in                                pharma biotech and when I found out kind                                of about this conference Trek as a way                                to get started in IR I was able to kind                                of talk a bunch of my OSD colleagues                                into chaperoning Mia if we did an entry                                into it and so we've decided to go ahead                                and enter Trek the trek news track for                                                                                                      every year by the NIS T up in                                Gaithersburg Maryland right around                                Washington DC it's free to enter they                                give you a lot of nice data that's                                already got some human judgments to go                                along with it but when you do enter you                                sign some kind of agreements about                                releasing corpuses and releasing                                judgments and material because they want                                to keep the competition you know open it                                are just like durable going forward so                                that's why I can't really share all the                                data that's involved with this before                                but hopefully it gets a flavor for it                                and you can hook you can apply this kind                                of stuff to any any data that you can                                parse into sentence tokens and I know                                probably everyone listening is a better                                data engineer than I am so that probably                                won't be that won't be the sticking                                point for getting NLP embedded into your                                search but the goal here really in terms                                of the specific track which is just kind                                of like um a sub competition at track is                                is given a document so imagine we're                                reading we're reading The Washington                                Post side ourselves given a document                                what other links should I read to find                                out more about that document say oh I                                want to be able to go a little deeper                                I'm going to understand the background                                context a little bit that that's really                                the goal of this kind of feature if you                                can imagine what it would look like                                downstream when it's actually deployed                                 into their production system so the the                                 corpus is made up of roughly                                         articles that when I pulled it into an                                 index not the most efficient index ever                                 right we're tuning for relevancy here                                 but                                 it was two gigabytes and then when I                                 added dense vectors it blew up to                                    gigabytes so I just wanted to kind of                                 call that out and in the sense of how                                 big some of these dense vector things                                 can get right dense vectors they're                                 dense for a reason they're heavy there's                                 a lot of numbers floating in there so                                 there better be some bang for your buck                                 in terms of performance which are                                 getting out so like any good experiment                                 I wanted to set a reasonable baseline                                 and the baseline I decided to try and go                                 against for performances the more like                                 this query in elasticsearch this is                                 really designed for this kind of                                 specific task almost it doesn't                                 necessarily take into account the                                 background something that happened in                                 the past right that would be relevant to                                 an article but we could accomplish that                                 with another filter but it does a really                                 good job of honing in on articles that                                 share a lot of tokens and hopefully                                 that's a lot of kind of meaning                                 similarity to and like it does really                                 good it's based on BM                                                good BM                                                                in and see effect if there's can kind of                                 how they compare against this so for                                 doing so if you're doing embeddings let                                 me see I have a select message                                 fullscreen okay yep                                 oh I think you want to do like this so                                 so for embeddings I'm only using the                                 title field only let me just bounce over                                 and show you guys an example document                                 real quick so we can just kind of see                                 what what one document will look like so                                 this this is a document in this room                                 there's kind of a title field kicker is                                 the section contents is the first                                 paragraph contents two second paragraph                                 contents all all the paragraphs but I'm                                 only doing the title and the idea there                                 is that I was just kind of going after                                 this the similarity there on one                                 sentence I'm using this distill burp                                 model apologies for the early switch                                 this disturber model out of this ukp lab                                 which is a fantastic github repo if you                                 guys want to learn more about this I                                 highly recommend starting there they                                 have lots of great examples and I want                                 to call out that I'm not doing                                 fine tuning on this model right I'm just                                 getting started with NLP haven't                                 exported that would look like definitely                                 haven't sat down to label the data in                                 terms of this this model the model they                                 trained is actually designed around                                 inference similarity between sentences                                 so whether they agree or contradict were                                 the labels that they built out to to                                 train this model so I'd have to do                                 something similar to what they did to                                 keep going in that soda I haven't sat                                 down to do it this is just what you get                                 right off of the download this is the                                 collab research notebook that I've                                 shared with everyone so you guys can see                                 it's over here all things told it's like                                                                                                         short from importing everything to                                 fitting the model to pickling things out                                 so I can use them back in my index so                                 the code is not intimidating like I                                 think the concepts and the meaning                                 behind it are still are still                                 intimidating for me though so let's talk                                 a little bit about performance I wanted                                 to kind of jump back into Kabana now so                                 we can go through this but we start off                                 with kind of just a more like this query                                 against title and so if this is the                                 document right here this Lake Braddock's                                 which is a high school in Northern                                 Virginia Erin Hollins who is a football                                 player if we're trying to find documents                                 most similar to these high school                                 football contexts let's see more like                                 this does a pretty good job for us off                                 the bat right this is already getting                                 late late Braddock's Natalie Butler it's                                 another high school athlete for like                                 Braddock Bronco Mendenhall is actually                                 he happens to be a running back so this                                 phrase has come out of nowhere before is                                 kind of maybe it's a similar descriptor                                 for running backs again we're heading                                 Lake Braddock right and remember this is                                 just kind of here it's a little crazy                                 where we get into Bernie Sanders has                                 emerged so as the Donald Trump of the                                 left so this is just running on those                                 tokens at that point ensure doing a more                                 like this search in a title field                                 probably doesn't matter isn't isn't the                                 best doesn't matter is the wrong thing                                 to say isn't the best way to go about it                                 you'd like to expand it to maybe be the                                 all document but just to keep things                                 fair that's what I'm doing here so if we                                 if we just do the way the way elastic is                                 implemented the cosine similarity stuff                                 it's really kind of just as a script                                 score so in order to really run it over                                 the entire document I'm just dropping                                 this match all in and the idea here is                                 that I've already gone and kind of hand                                 encoded the title from this document and                                 stored it here in the params it's hidden                                 because it's huge                                 but the idea being that we just used                                 that cosine similarity plus                                           all ranking scores have to be greater                                 than                                   but if we do that we can see the kind of                                 the titles we get back and this takes a                                 long time right it's running over all of                                 those documents and you can see it                                 matches itself right which is good that                                 means the embeddings are doing the same                                 thing each time they go into the model                                 surprise surprise and the next the next                                 article trump is openly telling us he's                                 above the law                                 here's his next move kind of weird to                                 see that slip in there Michael Sam is at                                 least another football player which is                                 reasonable at the Navy Yard there was an                                 accident or there was a mass shooting                                 actually as what that's about                                 unfortunately and then can immigration                                 reform and so you see we're getting                                 pretty off-topic how to clean outdoor                                 furniture right there's a lot of things                                 that don't really make a whole lot of                                 sense in terms of the high school                                 football context so what I wanted to do                                 next is just kind of run embeddings and                                 blend the scores a little bit and so                                 this is me kind of just tuning by hand                                 nothing crazy and figuring out a blend                                 there and so this is this is something                                 kind of similar but we're getting back                                 towards Lake Braddock and we are giving                                 a little more diversity than our first                                 just BM                                                                 because you see some themes kind of come                                 up in here like an individual athlete                                 has emerged is one of the sports top                                 people that comes up there with Jay                                 Beagle he's an NHL player for the                                 Washington Capitals this also this also                                 comes up with a                                 in a gel er Jordan Binnington and this                                 idea of coming out of nowhere and                                 emerging you can see is you know they're                                 not exactly the same words but the model                                 is already kind of getting the idea that                                 those are similar English constructs or                                 contexts and so I just wanted to kind of                                 wrap up there in terms of talking about                                 because there is some potential right                                 for using it to capture the idea but                                 there is some stuff that still doesn't                                 really make too much sense and just to                                 give an idea of how good the more like                                 this could be right we're not using any                                 cosine similarity in this last search                                 just more like this across the whole                                 document like I alluded to thinning you                                 see some pretty pretty solid results                                 coming back Reed this is a football                                 score against their rivals actually the                                 guy were the running back we're looking                                 for is in that game                                 Centreville which is another school you                                 know ramps up Natalie Butler was a big                                 star for like Braddock so she comes up                                 again                                 and then this is a basketball from the                                 same school so you get kind of into                                 these standout stars and at late                                 Braddock and so it's a pretty good                                 search if you want to learn more about                                 sports at Lake Braddock right and that's                                 kind of one of the goals in this thing                                 so just to recap MLT query on titles                                 okay you know it starts out pretty good                                 only tidal embedding is a little wild                                 you get some different themes in there                                 there are really different different                                 nouns or subjects right we bounce from                                 Trump to Bernie Sanders to NFL players                                 but and then blending is actually pretty                                 good you know I've seen a couple talks                                 already here the one that stands out was                                 the vespa one kind of talking about                                 hybrid approaches so that works out                                 pretty well and actually brings some                                 diversity in there I'm not sure it's                                 really the background task I was looking                                 for but and then really just kind of                                 taking advantage of the BM                                              doing more like this on a broader piece                                 of the document works works the best and                                 so that's my talk sorry you think well                                 thanks for listening so I ran a little                                 bit over for Doug but if you guys want                                 to reach out to me at all please you can                                 get me on Twitter you can send me an                                 email and I'm also on the relevancy                                 slacks so feel free to reach out if you                                 want                                 talk about title embedding for any kind                                 of embeddings because this is something                                 that I think is super fascinating it's                                 obviously gonna be here for a while in                                 search and I think it's probably                                 something when we look back and a couple                                 of years will say oh yeah well we all                                 had to kind of pick that up a little bit                                 just like you know data science and                                 stuff was over the last five years you                                 know so thanks so much for listening and                                 I'm ready for any questions you guys                                 might have what's that say thank you                                 night we've got one question from slack                                 our time is asking isn't background I II                                 tried to get more background in the                                 story more about diversifying the                                 results than more like this are you                                 getting more missing parts of the story                                 yeah so that's a great question and the                                 way that the topics phrased I think it's                                 kind of you could interpret it just like                                 you said which i think is totally fair                                 based on what we've seen kind of in the                                 relevancy judgements on the past years                                 of Trek and what they've given us to                                 kind of look at as a baseline it seems                                 like they want something so if it was us                                 it was about Aaron it might be about his                                 freshman year at school would be an                                 ideal story in their in their mindset so                                 it'd be like imagine a ruling before the                                 Supreme Court and they want to kind of                                 watch the progression of a case to how                                 it got there                                 and having that time since is something                                 I didn't cover but that has to be that's                                 a big part of what they're defining is                                 relevant to so fantastic well we'll see                                 how it goes at Trek looking forward                                 right right I'll keep you guys up to                                 date and hopefully I'm gonna port all                                 this to a blog post too that's a little                                 more durable with an example datasets of                                 people you don't know tracts been around                                 a very long time and you can go back in                                 time and see some of all of the                                 innovations in IR and search you know                                 rate you know appearing in trade                                 competitions going back decades so I                                 would encourage you to check it out it's                                 very much in the academic side of IR but                                 it's is where a lot of a lot of search                                 algorithms have cut their teeth rather                                 you
YouTube URL: https://www.youtube.com/watch?v=LOOuzBtR9bg


