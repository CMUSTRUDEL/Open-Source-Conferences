Title: #Haystack: Trey Grainger - Thought Vectors, Knowledge Graphs, and Curious Death(?) of Keyword Search
Publication date: 2020-07-02
Playlist: Haystack - Joint Virtual Event 2020
Description: 
	More: https://berlinbuzzwords.de/session/thought-vectors-knowledge-graphs-and-curious-death-keyword-search

The world of information retrieval is changing. BERT, Elmo, and the Sesame Street gang are moving in, shouting the gospel of "thought vectors" as a replacement for traditional keyword search. Meanwhile many search teams are now automatically extracting graph representations of the world, trying their best to also provide more structured answers in the search experience. Poor old keyword search seems so outdated by comparison - is it dead, dying, or simply misunderstood? Contrary to popular belief, embeddings and thought vectors only solve a small subset of search problems, and each of these three tools (keyword search, thought vectors, and knowledge graphs) actually serves a critical role in building the next generation of search experiences. In this talk, we'll define and highlight the strengths and weaknesses of each of these search methodologies, discuss the role each should play in a modern search solution, and demonstrate where each fails to get the job done and also how they can best complement each other to optimize relevance. We'll walk through interactive, open source demos showing each of these three types of search in action, demonstrating how to balance the strengths, weaknesses and tradeoffs between them for different user intents and query types.
Captions: 
	                              hi everybody really glad to be here                               thanks for the great introduction                               Charlie I'm going to talk today about                               thought vectors knowledge graphs and the                               curious death of keyword search                               so as Charlie mentioned I am the founder                               of search kernel a company helping a                               consultancy helping other companies                               build next-generation search formerly                                chief evidence officer lucid works                                author of a I powered search which is a                                work in progress and an advisor to pre                                search the decentralized search engine                                very briefly search kernel we just                                mentioned it you know if you're                                interested in help with some of the                                techniques that I'm going to talk about                                today would be would love to chat with                                you pre search meant for many of you who                                are unfamiliar is a decentralized search                                engine think of it as ultimately a                                decentralized open source version of                                Google that's a very audacious goal but                                where you know in fairly early stages of                                building out something that I think is                                going to be really important to the                                world so stay tuned for more on that and                                of course a ad powered search is my                                newest book that I am currently working                                on if you're interested in getting a                                copy there's a discount code here at the                                bottom they add powered                                          manning's website so the agenda for                                today is pretty simple it's the title of                                the talk we're going to talk about                                thought vectors we're going to talk                                about knowledge graphs and then we're                                going to talk about keyword search and                                how thought vectors and knowledge graphs                                replace or augment keyword search so to                                get started you know before we started                                talking about thought vectors and what                                they really are which I'll get into in                                just a minute we had keyword search so                                this as many of you will be familiar                                with is an example of an inverted index                                where I have documents the documents                                have keywords in them so for in this                                example the word Apple the word donut                                and the word juice are all terms that                                appear within these documents and then                                we pre index a mapping of every possible                                key word into a postings list of                                documents that that keyword appears                                within so that when somebody searches                                for the you know the search apple juice                                we look up apple find all the documents                                that's in lookup juice find all the                                documents that contain juice and then do                                a set intersection and ultimately find                                all of the documents that contain the                                words Apple and juice from there you                                know this is the standard be m                                          which I won't get into but we                                essentially take every document and we                                score it relative to those terms Apple                                and juice to try to figure out which                                documents are the most related to the                                grade that comes in this is traditional                                keyword search most folks watching this                                presentation are probably very familiar                                with with this technique of course you                                know in recent years there's been a                                massive effort to improve search and                                bert√© which many of you have probably                                heard about is Google's attempt that                                they've actually rolled out into their                                search engine to do so and so this is                                the article from when Burt was first                                released in Google search results and                                just to give you a sense the article                                says Google is making one of the biggest                                changes to its ranking algorithm as it                                gets artificial intelligence a deepening                                role in the world's most popular search                                engine the change which Google described                                as its most significant revision and at                                least five years uses a new form of                                language analysis to understand users                                queries better it is set to until now                                Google's algorithm has tried to single                                out the most important words in any                                search ignoring smaller or common words                                that seem less significant this enables                                that to zero in on the main subject but                                often results in it misunderstanding its                                precise request the new technique known                                as Bert relies on language model built                                up from analysis of vast amounts of text                                 online rather than reading a string of                                 searched words sequentially it analyzes                                 them all at the same time including                                 smaller words that would have been                                 ignored before one example Google gave                                 of the types of questions that could now                                 handle was how old was Taylor Swift when                                 Kanye jumped on the stage this points to                                 more complex queries that have been                                 beyond its reach before the update marks                                 the first application of the piece of                                 research on natural language processing                                 from last year that has drawn                                 considerable attention in AI circles and                                 so I keep mentioning thought vectors                                 what our thought vectors                                 you've probably heard the word                                 embeddings before but there's many kinds                                 of embeddings so thought vectors and                                 compass all of those kinds so for                                 example you can have a word or phrase                                 embedding that essentially means that we                                 take the words or phrases that are you                                 know within documents and we map them                                 into numerical vectors representing the                                 meaning of those words and phrases so we                                 can have word or phrase embeddings but                                 you can also do things like sentence                                 embeddings where you take an entire                                 sentence and try to map it into a vector                                 representing the meaning of the sentence                                 you can do the same thing with                                 paragraphs to create paragraph                                 embeddings                                 a vector that you know maps to the                                 meaning of an entire paragraph when                                 taken as a whole and of course you can                                 take an entire document and break up all                                 of the pieces within the document and                                 try to map it to a vector that                                 represents the overall meaning of the                                 document if you think of this in terms                                 of the way we think of an inverted index                                 historically you know where as you might                                 have a query that comes in so for                                 example on the left here these are query                                 keywords you know Apple cheese juice etc                                 and then you have an inverted index like                                 we looked at before which has you know a                                 bunch of individual words mapped to                                 whether they exist within documents or                                 not in this case you you could actually                                 represent a query for example the query                                 for Apple as a vector where the vector                                 has a                                                               exists and a                                                           and the length of this vector                                 essentially has one element for every                                 possible word that exists in any of your                                 documents now this is known as one hot                                 encoding what you can then do if you                                 represent each query as a vector so                                 Apple has you know one in the position                                 for apple juice has a one in the                                 position for juice for that feature then                                 you can ultimately add those two vectors                                 together to get the vector for apple                                 juice or apple and juice both appear                                 this is similar to the set intersection                                 we saw a few minutes ago with the                                 inverted index you can also do multi                                 term searches using that technique so                                 now Apple and juice has a                                              these positions we can you know think of                                 this vector as the query for apple juice                                 I mean so far these look                                 almost identical right we've got our                                 inverted index here mapping terms to                                 documents and then over here we've got                                 you know queries and the same thing I                                 happen to be up with that which is query                                 and over here we've got queries and                                 we're mapping those two vectors that                                 represent you know where they exist but                                 then the magic comes into play with                                 thought vectors with these embeddings                                 which is the idea of dimensionality                                 reduction so whereas before we had one                                 feature or you know dimension and this                                 vector for every possible word what if                                 we don't want to have that many                                 dimensions we can apply dimensionality                                 reduction so that in this case instead                                 of having each query mapped to the                                 specific words that it contains instead                                 we have it mapped to a reduced set of                                 features which describe in some way the                                 things we're searching for so in this                                 case I've created eight different                                 categories or features one of them being                                 whether the query over here represents                                 the term over here represents food                                 whether it represents you know something                                 you can drink whether it contains dairy                                 bread caffeine whether it's sweet                                 whether it has calories or whether it's                                 considered healthy so you can see in                                 this case for example a doughnut is food                                 it's not a drink contains a little bit                                 of dairy contains a lot of bread it is                                 very sweet                                 it contains the the highest number of                                 calories of everything on here and it's                                 not healthy at all and you could you                                 know see the rest map to a similar space                                 in terms of how they map to these                                 features so once we've done that and                                 we've essentially represented all of our                                 terms or phrases we create these vectors                                 that we just saw on the last screen that                                 represent each of those terms or phrases                                 these are known as our embeddings or our                                 thought vectors representing words and                                 phrases in this case once we've done                                 that we can take a similarity between                                 any two of these vectors you know using                                 something like the dot product or a                                 cosine or you know Euclidean distance or                                 any number of similarity measures and                                 ultimately we generate similarity scores                                 for each of these so for example if I                                 search for cheese pizza and I                                 grab the vector for cheese pizza and                                 then I do a cosine similarity between                                 that vector and all these other vectors                                 I can get a ranked set of results                                 showing me that                                 for example cheese breadsticks are the                                 most similar thing to cheese pizza                                 cinnamon breadsticks are the most                                 similar thing to or the second with                                 similar thing to cheese pizza and so on                                 with water being the least similar if I                                 take green tea you'll see that water and                                 cappuccino and latte and then apple                                 juice are the most similar these all                                 have to do with being drinks going from                                 most healthy to least healthy and then                                 donut is the thing that is the farthest                                 away in terms of similarity similarity                                 to green tea and so implementing this                                 kind of vector search those were simple                                 examples we'll do something more                                 complicated in a minute requires dense                                 vectors support in your search engine                                 and so you know all of the open source                                 or at least partially open source search                                 engines have solar elastic search                                 festival etc all contain some level of                                 vector dense vector support I'm                                 typically using solar so I'm going to                                 show you some examples of how to do this                                 with solar today and what one of the                                 first ways is through solar streaming                                 expressions capabilities as a neguin                                 expressions I can pass in documents                                 where you know these are the exact                                 documents I was just looking at and I                                 can pass in a vector I'm using a                                 multivalued float field where I just                                 pass in each of the dimensions as the                                 the multiple values going in once I've                                 done that I can use a streaming                                 expression like this where I'm selecting                                 the top apologies                                 somebody just rang my doorbell I can                                 search for the top from this query where                                 I'm searching for my food collection and                                 trying to do a cosine similarity with                                 this vector you know this being a query                                 for actually the donut vector and do a                                 cosine similarity and what I see is that                                 the donut document is the most similar                                 to the donut vector followed by                                 in breadsticks and cheese pizza I'm and                                 so on another thing that I could do is I                                 can search for oh sorry                                 another technique I can use if I want to                                 use a vector search in my main line                                 query not as a streaming expression is                                 to use solar one four three nine seven                                 which is the vector fields and functions                                 in solar patch that I'm personally                                 currently working on and I'll show you                                 how to use that so same thing here if                                 you you know just want to pull the code                                 a build solar and start solar you create                                 a collection once you've created a                                 collection you add your you know dense                                 vector field type to your schema and                                 then you create an actual vector field                                 so in this case I'm naming my field                                 vectors underscore V making it a dense                                 vector and going from there once I've                                 done that then I can add documents just                                 like before where I've got you know                                 doughnut apple juice                                 cappuccino cheese pizza green tea etc                                 here here's all my documents the one                                 interesting thing to note and the solar                                 implementation here is that I can                                 actually pass in multiple vectors per                                 field within the document so in this                                 case you know when I do my vector                                 similarity scoring instead of being                                 limited to one vector per document which                                 would be a document thought vector our                                 document embedding I can actually you                                 know potentially do you know a vector                                 per paragraph or a vector per sentence                                 or even a vector per word once I've done                                 this I can now come in and you know run                                 my search so in this case I want to sort                                 by and return the vector similarity so                                 I've got my in this case my query is                                 going to be for this vector for that                                 represents a doughnut my cosine                                 similarity function is here this vector                                 cosine function I'm going to return that                                 cosine score and I'm going to sort my                                 results based upon that cosine                                 similarity calculation so in this case                                 when I search for the doughnut vector                                 what I get back is you know the toppers                                 with a perfect score is the donut                                 document because this first vector in                                 that document got a perfect score                                 cinnamon bread                                                         and and so on and so forth so this is an                                 example of using the cosine similarity                                 to score documents based upon the                                 similarity of the vectors in this next                                 example I'm doing something similar but                                 instead of only relying on the vector                                 similarity calculation I'm actually                                 combining the vector similarity with                                 traditional keyboard search so in this                                 case actually search for the term bread                                 and I'm gonna match that like a normal                                 keyword search and rank based upon the                                 the BM                                                                 also going to boost the ranking of my                                 search results based upon the cosine                                 similarity between the vectors so in                                 this case you have same thing here's my                                 doughnut vector here's my sicko sine                                 similarity calculation still returning                                 the fields or in this case I'm doing a                                 boost based upon the cosine similarity                                 and a normal query based upon bread and                                 so this case what you see is that                                 because I matched on the term bread and                                 my query that I only get the two                                 documents that contain the word bread                                 and them so cinnamon breadsticks and                                 cheese breadsticks but I'm actually                                 boosting because I search on the vector                                 for donut I'm boosting on the one that                                 is the most related to donut in this                                 case cinnamon breadsticks is more                                 related to donut than cheese breadsticks                                 and you can see in the explain down here                                 for the calculation that's indeed it                                 matched indented BM                                                  term bread and then boosted based upon                                 the cosine similarity for the vectors                                 this next example allows me to                                 essentially use the vector cosine                                 similarity as the score and to actually                                 cut off the results beyond a certain                                 score so unlike BM                                                       search relevancy which has all of the                                 the scores relative to each other where                                 there's no sort of absolute score                                 indicating                                 the quality of the match when you're                                 doing something like a vector similarity                                 and cosine in this case the scores are                                 actually more absolute where a                                       perfect score and a zero is you know no                                 match at all and so in this case what                                 I'm doing is I'm running my query my                                 query itself is the cosine similarity                                 value that is calculated everything else                                 and here's exactly the same but now I'm                                 applying a filter which limits the rate                                 the documents coming back to only                                 documents that have greater than                                      cosine similarity so in this case the                                 results I get back are ordered by                                 relevance in this case the cosine                                 similarity score and they're cut off so                                 that no documents with a score less than                                                                                                       get my top-ranked documents donuts                                 cinnamon bread six cheese pizza and                                 cheese breadsticks                                 so that should give you a general sense                                 of how to implement you know once you've                                 got these thought vectors how to                                 actually implement them in Apache Solr                                 but of course those are all fairly                                 contrived examples using you know food                                 items and attributes of food that's not                                 a real-world application and I also came                                 up with those categories by hand so the                                 question then becomes how do i encode my                                 documents and queries into vectors so                                 that's where vector encoders come in so                                 vector encoders take queries documents                                 as documents instance paragraphs etc and                                 transformed them into thought vectors or                                 embeddings usually they leverage deep                                 learning which can discover rich                                 language usage rules and semantics and                                 the map them to combinations of features                                 in the vector and there's lots of                                 different libraries that do this and to                                 give you a sense of those libraries I                                 think everybody is pretty familiar with                                 word Tyvek came out in                                      sort of took the NLP world by storm                                 glove came out about a year later and                                 improved upon what word to Becca do fast                                 text a few years later and then in                                      the this paper about all you need is                                 attention came out they introduced                                 transformers which ultimately took us a                                 generation beyond what words a Beck and                                 the others did where                                 whereas word Tyvek came up with a static                                 representation of a vector that                                 represented a you know a word                                 what transformers did is they took the                                 idea of context into consideration so                                 that words and phrases could have                                 different meaning within the context in                                 which they appeared and so shortly                                 thereafter we got Elmo                                 we got the universal Simmons encoder                                 which didn't necessarily you follow the                                 transformer motto but then almost                                 everything you saw from that point on                                 leverage the transformer model a uln fit                                 really made transfer learning which is                                 the ability to start with a base                                 language model and then retrain the only                                 a portion of it to based upon the the                                 content your domain that's where it                                 really became popular here in these                                 models and then of course Bert was a the                                 next major breakthrough you know which                                 Google has leveraged very heavily and                                 then you know you can see from that                                 point on there's just been an explosion                                 of different models that have come out                                 from all of these you know not even                                 including anything that came into being                                 in                                      lots of Sesame Street characters that                                 the theming is fun there but but that's                                 sort of what we what we see now and I                                 haven't even included any of the                                       stuff other than I did want to mention                                 GPT                                                                      beginning of June this month which we'll                                 talk about in just a little bit and you                                 can actually see this is the leaderboard                                 for the Glu benchmark which is one of                                 the benchmarks that measures the success                                 rates of each of these models and half                                 of these I don't even recognize the the                                 field is just growing by leaps and                                 bounds so quickly and so we mentioned                                 transformers which most of the modern                                 models follow you know what is a                                 transformer a transformer ultimately                                 allows you to take incoming texts so in                                 this case I've got the she went for a                                 walk outdoors to encode it into a vector                                 representation or an embedding and then                                 once I've done that I can take that                                 representation that in theory represents                                 the meaning of the the input and then                                 decode it back into something that's                                 more you know human understandable so in                                 this case I might get something out like                                 the woman went outside to walk if my                                 input was she went for a walk out door                                 the meaning was sort of communicated                                 here and then we translate it back out                                 you could also build a different decoder                                 so maybe I encoded this way but then I                                 had a decoder built for the German                                 language and I can automatically get a                                 translation out that that is pretty good                                 because the meaning has been preserved                                 in the embedding the when we're doing                                 search implementations and trying to do                                 similarity for search typically we don't                                 use the decoder part of the transformer                                 and we focus on just on the encoder and                                 so in this case documents will go into                                 the encoder and for each document I will                                 get you know a document vector or maybe                                 a you know paragraph vector sentence and                                 vectors etc those all get indexed into                                 the search engine and then whenever a                                 query comes in I you know similarly get                                 the embedding for it and then I do the                                 similarity calculation that I showed you                                 a few few slides back that allows you to                                 just score each query vector relevant to                                 all of the vectors represented in your                                 documents there's some performance                                 considerations however whenever we're                                 doing this for real-time search so for                                 example the vector scoring is slow it's                                 a whereas with the traditional inverted                                 index you look up the documents that you                                 need to score and then only score those                                 with vector similarities you have to                                 actually score every single document                                 that you're considering to figure out if                                 it's a match or not and so this is                                 equivalent to doing a table scan in a                                 database so it can be very very                                 expensive and very slow so the solution                                 set to that is this notion of quantized                                 vectors or Proximus nearest neighbors                                 approximate nearest neighbors the idea                                 here is that you map the vectors into a                                 set of discrete values ultimately                                 creating tokens that you can put into                                 your inverted index to match on to                                 reduce the number of documents that you                                 have to actually do the full vector                                 scoring on and that enables you to                                 really combine vector scoring with                                 traditional search while not being                                 painfully slow so the recommended                                 approach when you're implementing this                                 in a search engine is typically to                                 do approximate nearest neighbor search                                 leveraging one of these quantization                                 techniques and then after me you found a                                 potential candidate set of documents                                 that should be good matches you then                                 rear ank those documents leveraging the                                 vector similarity score and so you know                                 similarly in solar if you want to                                 implement this you know same general                                 setup as before except for in this case                                 I'm saying that instead of scoring all                                 of the documents I only want to bring                                 back the top I only want to bring back                                 the top five documents that are returned                                 and of those five documents I'm then                                 going to run my vector cosine similarity                                 on those so you can see my example here                                 document one these first                                 I guess sorry I probably should've been                                 four but these top documents have a                                 higher score because they've been                                 rewrapped and from my donut search you                                 see that donut cheese pizza apple juice                                 and cappuccino where the top ranked out                                 of those first five that came back and                                 then you can see that now I'm thinking                                 so typo should be four but and then you                                 can see that for the rest of the                                 documents they just appear and you know                                 document order because they didn't                                 actually get scored and so in practice                                 you had just a wild-card search here to                                 show the technique but in practice if                                 you've got a mechanism to either do an                                 initial keyword search essentially as                                 your approximate nearest neighbor query                                 or to build some approximate nearest                                 neighbors or clustering capability that                                 you can then tag on the documents so                                 that you can then filter on them before                                 the scoring you would just you know pass                                 in the a n n filter as either your query                                 if there's some relevance associated                                 with it or just as a filter if you just                                 want to filter down to a candidate set                                 of documents and then rewrite them so                                 there's lots of different approximate                                 nearest neighbors approaches this gives                                 you an example of you know some of the                                 benchmarks there's several that are                                 currently in the process of being                                 implemented in Apache Lucene so I'm                                 actually having vector fields and Lucene                                 that internally can implement                                 approximate nearest neighbors indexing                                 to make search faster I'm not going to                                 go over those but definitely check them                                 out and then of course the                                 you know some limitations for these                                 embedding models so this article you                                 know we talked looked at earlier about                                 Burt one of the things I didn't mention                                 before is that it actually only is being                                 used by Google and one of one out of ten                                 searches so it's not a solution to solve                                 every problem but in                                             searches it is you know so                                        searches are still using other                                 techniques additionally you know the                                 search companies Google said the bird                                 the bird technique would return more                                 useful responses to many queries that                                 are also said that in some instances the                                 new algorithm had produced worse results                                 than before so it doesn't necessarily                                 always improve things further GPT                                   which just came out at the beginning of                                 June it is a major breakthrough there's                                 something like a hundred and seventy                                 five billion parameters that use this to                                 train but in the paper they actually                                 identify some general issues with the                                 use of these kind of language models so                                 for this article in this article says                                 despite the strong quantitative and                                 qualitative improvements of GPT                                      still has notable weaknesses those                                 weaknesses include an inability to                                 achieve significant accuracy on                                 adversarial nli which is you know where                                 programs need to determine the                                 relationship between two sentences GPT                                   does little better than chance on things                                 like adversarial in Li the authors write                                 worse having amped up the processing                                 power                                                                 weren't exactly sure why it was coming                                 up short additionally and even more                                 startling is the next observation the                                 whole practice of trying to predict                                 what's going to happen with language may                                 be the wrong approach and you know these                                 may be the wrong types of models for                                 this and so if you look at traditional                                 keyword search versus vector search we                                 can actually note if you throw out a few                                 examples where where each one is likely                                 to do well or to fail so for example for                                 obscured keyword combinations like this                                 boolean query keyword search does really                                 well and vector search completely fails                                 for specific identifiers and attributes                                 for example if somebody searched them                                 for a specific product or attribute                                 a keyword search will succeed because                                 it's just filtering down unknown                                 attributes where this vector search will                                 likely fail depending upon how it was                                 trained and and whether there's                                 overfitting or not for natural language                                 queries like can my spouse drive on my                                 insurance keyword search might get lucky                                 but is probably gonna fail pretty                                 miserably because it doesn't understand                                 the words whereas vector search will                                 succeed                                 fuzzy language queries like famous                                 French tower are things that keyword                                 search will mismatch because it again is                                 just looking in individual keywords but                                 where vector search will will succeed                                 but then you've got this issue of                                 structured relationship queries so                                 things like a popular barbecue near near                                 haystack where you actually have meaning                                 associated with each of these and you're                                 looking at the interrelationship between                                 them in this case keyword search will                                 fail and vector search will fail because                                 vector search won't necessarily know                                 that haystack is a is an event at a                                 particular location and I'm looking for                                 a geospatial search for trying to do all                                 these relationships just vector search                                 just doesn't quite pull it off and so                                 you know that's where we get into                                 knowledge graphs and I'm gonna skip over                                 this sort of what is a knowledge graph                                 piece of it but there's one particular                                 kind of knowledge graph that I've worked                                 on called a semantic knowledge graph                                 that essentially allows us to take the                                 keywords that we have in our documents                                 and relate them statistically to try to                                 figure out other related keywords now                                 presented on this in the past so I'm not                                 going to spend much time on it today but                                 I can essentially take a term look                                 through documents to find other related                                 terms and then you know sort of traverse                                 through the graph that way and so one                                 way I've used this and since this is a                                 remote conference I didn't have a                                 location so I used a previous conference                                 that I spoken out which is activate but                                 what I can actually do is query the                                 semantic knowledge graph or query my                                 inverted index and ask it questions like                                 hey what does barbecue mean and what it                                 can tell me is that hey based upon the                                 content in my inverted index barbecue is                                 most related to the category of                                 documented restaurants and within that                                 category you know it means things like                                 barbecue brisket and ribs and pork and                                 the second-most common categories                                 to equipment and within that category                                 barbecue is related to things like grill                                 charcoal and propane I can then combine                                 that with a structured knowledge graph                                 so in this case my Acree for barbecuing                                 or activate I can do things like say you                                 know let me search for let me find the                                 word near see the decimal in two                                 location distance traverse from there to                                 you know activate which is the                                 conference figure out that activate is                                 was when it was last held at the what                                 was held at the Washington Hilton and                                 then you know the the Washington Hilton                                 I can look up a location for that and so                                 this is an example I'm not going to go                                 through the details but an example of me                                 taking the keyword activate and mapping                                 it using a knowledge graph traversal on                                 solar to the location that activate was                                 held at and then you know this example                                 again the slides will be available                                 afterwards allows me to take that                                 keyword of barbecue look that up and the                                 semantic knowledge graph we did just a                                 minute ago                                 take the keyword of activate do one                                 query to solar to traverse the knowledge                                 graph and ultimately bring back a search                                 build a search query that actually goes                                 to solar that looks like barbecue ribs                                 brisket dot type of restaurant and then                                 filtering to the location that activate                                 was held at and all of that can be done                                 with one graph you know look up in the                                 knowledge graph and then a subsequent                                 query to actually rank the results                                 similarly you know it's great for top                                 barbecue in Berlin realizes that top                                 actually means a you know a boost on                                 popularity of documents and then                                 everything else here is very similar to                                 before and we talked about the                                 transformer models before where context                                 really matters I'm in this case you know                                 you can see that when I search for BBQ                                 near activate that it understands that                                 I'm looking for a restaurant but if I                                 search for barbecue grill it understands                                 a different meaning of the words and                                 also that grill is a sort of outdoor                                 appliance and the the category of grill                                 and so the same kind of nuance that we                                 get with thought vectors and these                                 transformer models we can actually get                                 through our knowledge graphs as well if                                 we can shuck them appropriately and so                                 with the knowledge graph approach                                 answering a query like four-star hotels                                 near movie theater                                 open after midnight in Berlin is                                 something we can handle pretty easily                                 whereas with either keyword search or                                 thought vectors that would be                                 challenging and so you know just a                                 couple of thoughts                                 here's we wrap up on semantic vector                                 spaces so thought vectors are embeddings                                 and knowledge graphs and keyword search                                 ultimately all resolve to the same                                 overlapping semantic vector spaces we're                                 still dealing with the same content and                                 the same relationships within the                                 meaning of the things in our domain it's                                 just these are different representations                                 or techniques we can use to get at that                                 understanding at the semantics each of                                 these are really just different ways of                                 looking at the same relationships that                                 exist within our content likewise it's                                 also possible to model user behavior                                 signals into either numerical vectors                                 tokens on documents or as nodes and                                 edges in a graph again each of these                                 techniques can be used both for content                                 and for user behavior and then finally                                 certain types of attributes are much                                 easier to represent as numerical vectors                                 such as image features and you know                                 things like that                                 this makes multimodal learning or joint                                 learning much easier to accomplish then                                 in the past with just keywords and                                 knowledge graphs and so if you sort of                                 think of this as different approaches if                                 I take a query like this one here                                 machine learning research and                                 development Portland Oregon software                                 engineer and new to Java traditional                                 keyword search would represent it this                                 way a knowledge graph based query might                                 expand that based upon the relationships                                 that it understands and a thought vector                                 would ultimately try to map the entire                                 query into one vector of meaning and try                                 to search on that to probabilistically                                 find the results I like to think of                                 things in terms of though when we're                                 solving search problems in terms of                                 they're our ultimate goal being to                                 understand user intent which ultimately                                 means understanding our content                                 understanding our users and                                 understanding our domain knowledge                                 graphs and keyword search are key ways                                 that we we do that and I like to think                                 of you know dense vector search not as a                                 new dimension but essentially as a                                 technique so we can use the inverted                                 index to match on tokens we can use                                 dense vector search to                                 score and then match based upon high                                 scores or we can use a hybrid approach                                 which I showed earlier when you're                                 trying to combine both together and so a                                 couple of takeaways first thought                                 vectors are embeddings work very well                                 for natural language queries and                                 questions or for modeling non textual                                 data like image features and user                                 signals dense vectors also make it                                 easier to enable joint learning modeling                                 text content user behavioral signals                                 images and so on to a shared vector                                 space the keep in mind that even Google                                 only uses the technique currently for                                    percent of their queries knowledge                                 graphs on the other hand work best for                                 relational queries including reasoning                                 about known entities dealing with                                 hierarchies and multi level inference a                                 keyword search is not dead it works best                                 for longtail keywords specific item or                                 attribute search like names and ID's                                 obscure keywords boolean type query                                 needs and is the safest catch-all or                                 fallback                                 for most queries and then finally it                                 should be possible to actually enable                                 explainable AI on trained inspector                                 representations by mapping similar                                 documents back to overlapping semantic                                 vector spaces at the keyword and                                 attribute level such as through the                                 semantic knowledge graph I showed                                 earlier essentially if we understand                                 that each of this these techniques is                                 ultimately trying to get at the same                                 semantics in the same semantic space                                 vector space then we should in theory be                                 able to overlap these techniques on top                                 of each other and use one to explain the                                 other and so that's an area that I                                 personally plan to spend some time                                 researching in the near future so I                                 think at the end of the day the best                                 systems will likely use a combination of                                 all these approaches blended together                                 and hopefully more to come soon on all                                 of that so that's it for me thank you                                 Charlie if you wanna know if we have any                                 questions if we've got a couple of                                 minutes maybe take a few of those but                                 yeah thank you everybody                                 Charlie you're muted I think thank you                                 Trey fascinating as always                                 unsurprisingly we do have a few                                 questions I not sure we'll get to all of                                 them but there is Hannah there's a                                 breakout room and I'm sure Trey would be                                 happy to answer some questions in this                                 lack as well so learn I'm gonna start at                                 the top and see what we can work through                                 the next few minutes so a couple of                                 combined questions here mateo asks are                                 vectors always composed in by non-                                 components and peter as to that about                                 using dot product and cosine distances                                 and search so in lucy nate- schools are                                 prohibited but dot product and cosine                                 distance return return negatives how do                                 we work around this yeah so usually                                 because of the limitations that we've                                 seen they're usually if you need to                                 include negatives you can essentially                                 just cut off at zero and then move                                 everything that was negative to the                                 positive side so that's that's the                                 technique you would typically use it's                                 you know the hacker workaround but it                                 can definitely be done just you at the                                 end of the day you can just lose half                                 your precision from from the from the                                 negative side but so that's how you                                 would do that in terms of the other                                 question was dot product and cosine what                                 was the actual question well when you do                                 do dot products or cosine calculations                                 you often end up with negative numbers                                 so how do you how do you then use them                                 so if you if you then end up with                                 everything on the positive side and                                 you're doing your your cosine there then                                 you basically focus on the the range                                 between                                                                  on our next question aya asks how is the                                 similarity score calculated if there are                                 multiple vectors for a document field do                                 you take a max score yeah great question                                 so in the implementation that I was                                 showing that I'm working on essentially                                 you know I won't pull the screen back up                                 but essentially a one of the parameters                                 you pass into the vector cosine function                                 is a selector so it can either be max                                 min first                                 last or average so if there's you know                                 ten vectors in the document you can make                                 the store coming out of the function the                                 average of the similarity of each                                 between each of those vectors or                                 alternatively take the max score the min                                 score or if you for efficiency reasons                                 want to just take the first vector                                 maybe I've indexed a bunch but I only                                 want to use the first you can say first                                 and then it will ignore all the                                 additional vectors and up or you can do                                 last but that's probably less less                                 common but yeah there's just a selector                                 that you pass in okay next question from                                 Charles if you have suggestions or case                                 studies on how to tune the relative                                 weighting of I are scoring versus the                                 thought vector cosine distance scoring                                 so even just rules of thumb be useful so                                 so I had a basically if you're doing a                                 keyword search and using like being                                    weighting that versus the the the vector                                 score                                 that's the question huh yeah um so I                                 don't and I don't know that there's                                 there's not gonna be a fixed answer I'm                                 just gonna depend upon your domain and                                 and what your users are doing and the                                 kinds of queries are sending in what I                                 would say is it's it's not gonna be                                 based upon a you know always using let's                                 say                                                                  similarity score I think it's really                                 gonna be based upon the kind of query                                 that comes in so you might have queries                                 that look like questions you know who is                                 the president of so-and-so University                                 that is probably going to be best                                 answered by the vector similarity score                                 versus the keyword score and so in that                                 case you would want to weight the vector                                 similarity very high and the keyword                                 similarity either non-existent or very                                 low in another case somebody might type                                  in a query that is just a product ID or                                  the name of a product in that case you                                  would want the keyword match to be very                                  high and the vector score to be low so I                                  think ultimately the the best techniques                                  are going to involve getting a sort of                                  let's call it a query intent classifier                                  up front where the first step of this is                                  going to be taking the query and mapping                                  it into a decision tree about how to                                  actually choose the                                  relative weights I didn't cover that in                                  this talk but I think ultimately that                                  decision is very important and it's                                  gonna be very nuanced and changed on a                                  per query basis okay great                                  I think I can probably squeeze in one                                  more question so I'm asked it on behalf                                  of Senate who asks how does the vector                                  proach fare against learning to rank                                  it's a great question so there so                                  ultimately they're sort of solving two                                  problems so when you implement learning                                  to rank you're building a ranking                                  classifier that is learning attributes                                  and then ranking based upon those                                  attributes typically it also follows                                  this rear rank approach like I showed as                                  a suggestion for performance for from                                  the dense vector search in the case of                                  dense vector search the way I would                                  think of it with these transformer                                  language models they are they're                                  essentially learning two things they're                                  learning and understanding of language                                  itself so the way that words and phrases                                  interact how questions are asked and                                  answered things like that which is                                  generally transferable to most domains                                  that you know exist in that language for                                  example English or German or                                  what-have-you and then separately                                  they're learning the domain-specific                                  terminology that is used in the data set                                  that they're trained upon and so for in                                  the former case the understanding of                                  language itself is something that                                  learning to rank typically is not going                                  to get you learning to rank stip eclis                                  more focused on engineered features that                                  it can match on but to the degree that                                  you get into domain-specific                                  understanding there's going to be some                                  overlap if you use any textual based                                  features in your learn to rank algorithm                                  they're probably both going to be sort                                  of optimizing the weights in a very                                  similar way but what learning to rank                                  will get you beyond that is any other                                  engineered features for example a Geo                                  distance boosts or a category boost or                                  maybe there's other features that you                                  wouldn't necessarily learn from your                                  language model then that matter in your                                  domain may be a you know recency score                                  or popularity boosts or something like                                  that which isn't related to your text                                  learning sharing is going to be able to                                  optimize those non semantic features                                  and give you better scoring windows                                  better fantastic well thank you very                                  much tre we're going to leave it there                                  but I'm sure you'll be around to answer                                  questions in the slack and there's also                                  the breakout room we're going to take a                                  quick break now and we'll be back in a                                  few minutes with Tim Alison                                  you
YouTube URL: https://www.youtube.com/watch?v=1EVRWQLufNE


