Title: Berlin Buzzwords 2016: Julien Le Dem - Efficient Data formats for Analytics with Parquet and Arrow
Publication date: 2016-06-12
Playlist: Berlin Buzzwords 2016 #bbuzz
Description: 
	Hadoop makes it relatively easy to store petabytes of data. However, storing data is not enough; columnar layouts for storage and in-memory execution allow the analysis of large amounts of data very quickly and efficiently. It provides the ability for multiple applications to share a common data representation and perform operations at full CPU throughput using SIMD and Vectorization. 

For interoperability, row based encodings - CSV, Thrift, Avro - combined with general purpose compression algorithms - GZip, LZO, Snappy - are common but inefficient. As discussed extensively in the database literature, a columnar layout with statistics and sorting provides vertical and horizontal partitioning, thus keeping IO to a minimum. Additionally a number of key big data technologies have or will soon have in-memory columnar capabilities. This includes Kudu, Ibis and Drill. Sharing a common in-memory columnar representation allows interoperability without the usual cost of serialization.

Understanding modern CPU architecture is critical to maximizing processing throughput. We’ll discuss the advantages of columnar layouts in Parquet and Arrow for in-memory processing and data encodings used for storage - dictionary, bit-packing, prefix coding. We’ll dissect and explain the design choices that enable us to achieve all three goals of interoperability, space and query efficiency. In addition, we’ll provide an overview of what’s coming in Parquet and Arrow in the next year.

Read more:
https://2016.berlinbuzzwords.de/session/efficient-data-formats-analytics-parquet-and-arrow

About Julien Le Dem:
https://2016.berlinbuzzwords.de/users/julien-le-dem

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              hi so I'm going to talk about parking a                               robust columnar formats one in                               particular is on disk optimized for on                               disk and the other one for in memory so                               before we start just read a bit about                               myself if you're wondering where my                               accent comes from French and I live in                               California so I guess that would be some                               mix but I'm guess it's more French                                accent than anything else so i'm working                                at dreamy oh and we work on big data                                analytics solution before that I worked                                at Twitter on data platform where I                                started working making parque and I'm an                                Apache member and currently being                                involved with parquet and arrow the                                equator formerly walking on pig so today                                I'm going to talk about a few things so                                first I'm going to talk about the                                benefits of columnar formats in                                particular in data processing data                                analysis query execution and then I'm                                going to talk about another very                                important thing is driving standards in                                the community in the open source so that                                we can actually build on ecosystems with                                all those systems because you've been                                hearing about spark and Hadoop and                                Impala and all those things and really                                there are a lot of things you can choose                                from and they can work together but they                                can work together only if we have                                standards data formats in between so                                first I'm going to talk about benefits                                of columnar formats you know with a                                kitten slides so that looks good for the                                internet so columnar layout if you think                                of like any type of table so here have a                                simple example of a logical table we                                have three columns a B and C and we are                                when we were presented it in a computer                                whether it's on disk or in memory you                                need a linear representation when you                                have all the data one bit after the                                other                                so if you choose a row layout you're                                going to put each row after the order so                                you will have a                                                        values for each column interleaves and                                each column would be a different type so                                you have data from different types into                                your leave like that when you have a                                columnar layout you choose to put all                                the values for one column together first                                and then all the values for the second                                column and then all the values for the                                third column which means that you're                                going to put together values that are                                all of the same type and now all much                                more ammo genius and that will have                                several advantage advantages and first                                when you're doing analysis and you're                                doing a sequel query on some data set                                typically the data set has many columns                                and maybe dozens maybe hundreds                                depending but when you're doing a query                                in you analyzing something you're                                accessing only a few of those columns so                                the columnar layout will make it easy to                                access only those columns because                                instead of scanning the entire data and                                doing a lot of small cigs on the data                                set you're just going to be able to read                                entire big chunks of the columns you                                need and then do big jumps a few big                                jumps is much more efficient than a lot                                of tiny ones and another aspect is when                                compressing data because we put together                                all values from the same type instead of                                having a string and int string an int we                                have a bunch of string so a bunch of                                integers we can compress all them all                                together much more efficiently for                                example let's say you have an integer                                value and you know the maximum value for                                the entire column then you can use only                                the number of bits that are required for                                this maximum value and you can beat pack                                them together to use a lot less memory                                than four bytes per value and make it                                much more compact and similarly if                                you're compressing strings with regular                                brute force algorithm being it the you                                know either the zip LW family type of                                algorithm more snappy or regular ones                                the algorithm will be much more                                efficient because you're putting                                together things are more homogeneous                                instead of mixing a lot of different                                 things together                                 so I've been saying that parquet is one                                 disk arrow is for in memory but really I                                 started by building park a                                           storage and really you could ask why                                 don't you put just park in memory and                                 the main reason is because there are                                 different trade-offs that we want to                                 make and optimize the format for                                 different access patterns so when we                                 talk about disk storage the same you're                                 going to write the data once and it's                                 going to be accessed by many different                                 queries that may access different                                 columns so the access pattern on which                                 column it's accessing its its it has to                                 be written once and it's going to be                                 reused many times and when we read the                                 data it's mostly streaming right we're                                 going to read all the values one after                                 the other possibly we're going to skip                                 world chunks and I'm going to talk about                                 that but we mostly access them in order                                 and and so there's a priority to I your                                 rejection even though we still want good                                 cpu throughput but the priority is I                                 your addiction in this case for any                                 in-memory data structure it's more                                 transient so it's being built on the fly                                 for the purpose of the execution of a                                 query of doing some processing on it                                 like for example if you're doing a                                 sequel query is going to build only                                 those columns that you're crying and the                                 access could be easier we are looking at                                 all the values are in other cases you                                 may want to have random access to                                 individual values for example you are                                 doing a joint and is doing a joining                                 memory it may have want to access                                 different values are referring to values                                 for indexing in data structures so                                 you're doing some indexing and your                                 reference index of the value so in                                 Parque we don't necessarily optimized                                 for constant access to a value in the                                 list in arrow it's optimized for                                 constant access time of the value and so                                 in memory we really want to prioritize                                 for CPU stupid so we still need good I                                 oh and representation but it's really                                 the CPU throughput that's most important                                 so now I'm going to talk a little bit                                 both first about parque and in more                                 details on how it works and then i'll                                 talk about error so first they both                                 support nested data structures so i                                 listed it for both format because real                                 life data structures are nested and we                                 have lists of nested objects and                                 structures and several layers deep and                                 second it's a compact format so one                                 thing it provides its better compression                                 than regular are oriented with brute                                 force compression algorithm on top and                                 that's because of the type of where                                 encodings if we have a bunch of integers                                 we know the max value we can make them                                 more compact or if we know it's a bunch                                 of strings we can also do dictionary                                 encoding which means build a dictionary                                 and then code only integers and that's                                 much more compact another thing like                                 that and we get better compression just                                 by the columnar layout and putting                                 together things that looks the same as                                 opposed to interleaving values that look                                 different and so in can do up to messiah                                 by the projection push down which is                                 reading only the columns you need and                                 filter push downs which is taking                                 advantage of statistics that are part of                                 the file to skip entire chunks of the                                 file so one of the main advantages of                                 columnar storage on disk for queries is                                 first you access only the column you                                 need because you can do a scan on only                                 those columns you need to load and based                                 on statistics in the files you can skip                                 whole chunks of the data let's say you                                 filter by a certain range if you know                                 the minimum value in maximum value in                                 each chunk you can end those minimum                                 value don't match the range you know you                                 can skip the entire block you can skip                                 entire chunks of the data set so that                                 with both vertical and horizontal                                 partitioning you limit the i/o to the                                 minimum and and so this slide is about                                 nesting because all my example on slide                                 are simple and flat schemas but really                                 parka supports nisti data structure and                                 this is just giving an idea of how                                 nested structures are turning to a flat                                 representation which we capture all the                                 nice things with repetition and all this                                 and if you want more details there's a                                 link on the page with a blog post that                                 goes in the details on how that work so                                 now I'm going to talk a little bit about                                 arrow so like I said it's also supports                                 nested data structures and it's                                 optimized for CPU store boot on modern                                 CPUs and I'll go a little bit into how                                 modern CPUs work and why it's important                                 to understand how they work to present                                 the data in a way that is going to be                                 processed efficiently so things to think                                 about our pipelining seemly instruction                                 are single instruction multiple data                                 those are extending instructions on                                 processors that say do the same                                 operation on those four values at a time                                 and cash locality and because processors                                 have cash on them that is much faster to                                 access than main memory and the other                                 aspect of arrow is because the in-memory                                 representation is is the format you work                                 on it can we can use scatter gather iOS                                 there's no serialization when you send                                 it over the network there's no extra CPU                                 work to transform the in-memory                                 representation to the wire                                 representation we just send the                                 in-memory representation on the wire and                                 read it back on the other end and that's                                 all you need so typically in systems                                 that send it over the wire if you use                                 thrift the proto buff or whatever you                                 have your in memory representation and                                 then you need to serialize it into your                                 wire representation which takes a lot of                                 CPU so in this mode the goal is to make                                 everything one representation and to                                 remove all that overhead so this slide                                 is about CPU pipelining so if you think                                 of modern CPUs you know they don't                                 execute one instruction after the other                                 anymore so that was very the first sip                                 user were made now CPU instructions are                                 decomposed into like                                          and what the CPU is trying to do because                                 not all the area of the CPU is used at                                 once is trying to prepare start                                 processing the next instruction before                                 the previous one is finished which means                                 we have staggered execution of                                 instruction which means that you need to                                 be able to know what the next                                 instruction is going to be before the                                 previous instruction is done and so to                                 do that the CPU is trying to guess                                 there's a branch predictor what's called                                 a branch predictor and the CPU is going                                 because sometime if you have a if in                                 your code if the value was this do that                                 if the value was something I'll do this                                 then that's a branch and that's where                                 the CPU is going to do one thing or the                                 other that means the different execution                                 instruction is going to be calm and                                 that's called a data dependency right so                                 like the next instructions depend on the                                 result of the current one which means we                                 need to wait which is what you see here                                 so here you have for Imaginary                                 instruction ABCD and you can imagine                                 that's like a tetris break falling to                                 the bottom with time so we are moving                                 downwards as time goes on and so in a                                 perfect world we start and I have a                                 pipeline of four steps like imaginary                                 pipeline we start preparing instruction                                 a and it goes through the CPU pipeline                                 and after four cycles of the CPU it's                                 done and we do all the temp thing for                                 the instruction we assume they all                                 independent we can all process them                                 staggered and we do you know foreign                                 strip on average we do for instruction                                 we do one instruction per cycle on                                 average but if we were wrong and when we                                 else finish realized well wait a minute                                 we're not having the right instruction                                 we need to start over and start another                                 instruction right we need to wait until                                 this one is finished then we have what                                 you call a bubble and you're losing the                                 entire number of CPUs of the lens the                                 depth of your pipeline which means you                                 can lose up to                                                         huge difference between Ryan code that                                 will we have an independent instruction                                 that can go in power almost in parallel                                 versus something that we introduce                                 bubbles with the CPU always has to wait                                 before starting strong in executing the                                 next instruction and so that's where                                 columnar is very important                                 because you're going to do the exact                                 same thing on all the values over and                                 over and because you're writing your                                 code in that way you can have a tight                                 loop that says hey do this on all the                                 values so increment index do the                                 processing on the value right there                                 result increment the index do the                                 processing which means is always doing                                 the same thing in the tight loop instead                                 of you know looking at all the values of                                 your top pole and doing different things                                 and doing much more efficient and taking                                 much more adventure advantage of the                                 process of pipeline so that's the reason                                 and you know you can refer to the monet                                 DB paper which is like the inventor of                                 vectorized execution in databases and                                 that's basically the main advantage is                                 we use the most of the CPU it doesn't                                 have to wait to know what could you do                                 next and the part about the cash                                 locality like you know you have the main                                 memory of your computer there's the bus                                 in between and there's a CPU and the                                 cash is right on the CPU so accessing                                 the cash is much faster so the CPU will                                 say whenever it needs to access memory                                 if we go to the RAM size send me that                                 data we go over and then process is it                                 and what it will try to do is try to                                 keep working on the same amount of                                 memory and right on it locally because                                 it can do go very fast in reading and                                 writing in the cache and only when we                                 need to fetch more data we sent over                                 data that we are done working with then                                 we will have this latency so and here                                 every time we go through the bus the                                 processors to wait to get the next thing                                 so big and in culinary execution because                                 we work on one column at a time and we                                 do the same thing we get better cash                                 locality then if we get all the rows you                                 remember when we in a wrong until                                 representation in memory we get all the                                 different values so we have to execute                                 all the instructions for all the values                                 like evaluate expression for one or at a                                 time so data is more at your genius and                                 we do more thing and so there is more                                 back and forth between ram and cpu and                                 if we do columnar there is more locality                                 because we we can keep it tighter to the                                 we love working and so all those points                                 about CPU efficiency so basically I've                                 talked about those things and like it's                                 just a representation that show you                                 different types and now the interleave                                 or they're the same and the one thing                                 that you don't talk about was Cindy                                 single instruction multiple data and in                                 such cases the CPU is operation                                 instruction that you say hey process                                 that same thing on for value in parallel                                 so when you use those instruction it's                                 playing for X throughput right because                                 it's just a processor belief or for                                 instruction in parallel totally and so                                 that's thanks to column narrow                                 presentation that's very easy to say hey                                 we're going to do four times the same                                 instruction again and again so to give                                 you a little idea of how the arrow                                 nested or presentation works if we look                                 at name is a variable length value right                                 it's a string so in value the first                                 value is                                                               is                                                                   array that delimits points to the                                 beginning of each value age is a fixed                                 lens value so you have a plane array of                                 all the values one after the other                                 because a fixed length and for phones                                 with choose a list of strings you can                                 see that you can compose those things                                 right so we have a variable lens values                                 which are strings and I'm you know                                 truncating it here so that it fits in                                 the slide where's the offset that                                 delimits the strings and then we have                                 another upset vector that points to the                                 beginning of each string so the first                                 list has two elements the second list at                                 one element so you can see that the lens                                 are presented by the first second offset                                 we have two elements                                                     both point to the beginning of the                                 values in there                                 so another thing for the in-memory                                 processing is arrow comes with a memory                                 allocator based on Nettie and a notion                                 of tree tree of alligators so that he                                 can deal with quotas and providing                                 memory because when you have a query                                 execution you want to understand which                                 operator in the query execution is doing                                 what and how much memory it's using and                                 limit how much its operator is using for                                 example you may have a hash join and                                 also another apparent that applies a                                 function to the values and they both                                 will use memory and you want to be able                                 to allocate different code as so that                                 the join may want to spill data to disk                                 and things like that so now the the                                 other very important part of this is to                                 build that as a community-driven                                 standard because it's not just about the                                 technology the thing is there's a common                                 need like a lot of project and I'm sure                                 some quotes are looking into columnar                                 execution because since the monet DB                                 paper that's a natural evolution that's                                 how we are going to make all those                                 sequel execution or query much faster                                 like in Impala in sparks equal in drill                                 in all those systems and so all those                                 project are looking at those things and                                 so the goal is to make that we may as                                 well all do it the same way and at least                                 for the in-memory representation and                                 that will have a lot of benefits for the                                 ecosystem and any bling inter                                 communication between all those system                                 the other benefits of course is to share                                 the effort and building that thing                                 together instead of reinventing the same                                 thing ten times but truly sharing the                                 effort is something that that's one                                 driving force but that would not be                                 enough right like it's not because it's                                 the right thing to do to implement                                 something once that people have                                 different opinion but the other driving                                 vector to agreeing on how we want we're                                 going to do this is to be able to be                                 interoperable laughter the way and so                                 bail on success stuff park a park is                                 started as like just Clara and the                                 Impala team                                 and me at twitter trying to build a                                 standard and after being very open to                                 accept the drill community is a spark                                 community and then it started becoming a                                 standard kind of grass root thing and it                                 worked really well but now this                                 community is built right those people                                 already have interacted and negotiated                                 how's this thing is supposed to work                                 based on that now it's easier to build                                 aro and use the same thing for the                                 in-memory processing so it's really a                                 start up from the start and so I had a                                 few quotes by the impala team but the                                 spark team talking about the drill team                                 about you know how vectorized execution                                 and column nine memory is important and                                 I'm sure you've heard it in other tops                                 so some of the arrow goals are to have a                                 well-documented and cross-language                                 compatible spec and you know I talked                                 about how it's designed to take                                 advantage of modern CPUs and it's                                 unbeatable in those all those execution                                 and jeans and the goal is not to replace                                 a huge part of everything the goal is to                                 agree on the format and be inter parable                                 have a few libraries in common and let                                 every engine innovate in what's is                                 specific to them whether it's flink                                 order it sparks equal whether it's                                 Impala whether it's Apache drill all                                 those things provide their own value and                                 you know i mean this common library                                 doesn't prevent them from doing their                                 own thing but it's interoperable the                                 arrow project started so it's officially                                 was created at the apache foundation and                                 in February and the member of the PMC                                 come from all those all those projects                                 so you are going to recognize a lot of                                 Apache project are the things and all                                 those people are getting together and                                 the goal is to make sure it's really                                 something that we all agree on so that                                 it really becomes a success in it's the                                 beginning of the ecosystem so things                                 like you recognized in this both grey                                 engines like drill Impala and pandas                                 spark storm or                                 and also storage layers like parquet                                 could you Hadoop Cassandra and those                                 things will have to interact very                                 closely I will talk a little bit about                                 the angle to build this ecosystem and                                 why it's important so today when you                                 want to integrate things you cannot mean                                 one-to-one integration in all those                                 systems like for example parque victor                                 is code there some that lives in spark                                 some of that leaves in drill some that                                 lives in impala and they all like their                                 own slightly different implementation of                                 it and they all have their own in-memory                                 representation that is easier columnar                                 or not some are oriented in memory some                                 are culinary ented and its really it's a                                 mess like every project needs to                                 integrate with the other one they need                                 to find a format that will be standing                                 in between the two there's probably                                 going to be serialization disser ization                                 to convert from one format to the other                                 it's inefficient and there's a lot of                                 work duplication and typically when you                                 hear about talks about people who do                                 profiling they realize that a lot of CPU                                 is wasted on serialization dissertation                                 but it's something that you're stuck                                 with if all the system didn't agree on                                 what format they were going to talk up                                 from so with error noticed like complex                                 simple your arrow becomes like a                                 standard that helps make things easy to                                 communicate because if everyone agree on                                 the in memory format like we're doing                                 then they can integrate once and then                                 everything works together because there                                 is a common columnar format that is                                 efficient for processing and also is                                 standard so we can just send it across                                 the wire there is no average for                                 cross-system communication so we get a                                 lot of CPU saving not by optimizing                                 stuff but just by agreeing on how we're                                 going to transfer it and there's                                 opportunities for sharing of                                 functionality for example you can make                                 something that converts parka to a row                                 in memory sure make it really good                                 shares it once and then everybody's                                 using it so now you know instead of                                 having variable performance querying                                 parquet depending on the system you're                                 using like                                 currently for example hives some time is                                 less good than spark or drill that are                                 really committed to use park it very                                 well instead like everybody will have                                 like very good benefits and we don't                                 like spend the effort in each project to                                 make that very good so language bindings                                 parka started with I did all the Java                                 binding so they were like Java libraries                                 that were used in sparks equal in drill                                 in many other places and there was C++                                 code but that was like tightly coupled                                 with the Impala code now there is an                                 effort by West McKinney works at                                 Cloudera has been doing a very good CPP                                 c++ library that can be reused for the                                 c++ code in particular is interested in                                 integrating with python and pandas which                                 is like big data library like for people                                 who use are they like to use pandas in                                 python as well and on arrow there's a                                 java implementation the c++                                 implementation same usual suspects is                                 being done by Wes and once you have a                                 c++ implementation you can integrate                                 with all those native code based code                                 JVM languages like Java or Skala can use                                 the Java implementation and the primary                                 initial focus we say here it's a raid                                 right manage memory because there's well                                 RPC layers that could be shared and                                 right now it's not a common library but                                 the main thing is once you agree on the                                 format you don't need to put all the                                 tools in one library but it's valuable                                 to share the effort on some of those pcs                                 so I'm going to talk a little bit about                                 the RPC remote procedure call and IPC                                 inter-process communication which                                 basically either sending the data to                                 some other machine on the cluster are                                 sharing the data in between two                                 processes on the same node so a little                                 bit on how you look at the data so we                                 say the representation is columnar but                                 we don't really load the interior                                 cassette in memory write the data set                                 will be logically split in record                                 batches likes in here and really when                                 you're doing a core execution our                                 machine learning or whatever it is on                                 this data in memory you're going to work                                 on the subset of data at a time it's                                 just world map reduce thing right you                                 load a bunch of data you do some                                 processing you send it over you know the                                 next thing and so upfront you need to                                 define the schema for what vectors                                 you're going to find in your data                                 optionally you can have a dictionary                                 batch for you know instead of keeping                                 valuable lens values you can build a                                 dictionary and replace them by IDs which                                 is fixed lens integers and there are a                                 lot of values for query execution of                                 having fixed lens values instead of                                 variable lens right if you're doing an                                 aggregation instead of keeping a hash                                 table of the values to keep incrementing                                 the counts you just keep a plane array                                 and instead of looking up a hash you                                 directly look up the index of the values                                 right because when you build a                                 dictionary like IDs for that dictionary                                 will be between                                                       packed and you can just build an array                                 and do a much faster aggregation                                 evaluation like that so you have schema                                 optionally dictionary and then a bunch                                 of record batch is representing the data                                 and if we drill down in that so in a                                 single record batch you have a data                                 header that describes some offsets and                                 then you will have each vectors                                 representing the data so if you remember                                 that example I add on the other side you                                 have the vectors for the name so the                                 bitmap is for is it not now we just keep                                                                                                          remember name is a variable length value                                 so we say where the value start and then                                 the actual data which is all the values                                 one after the other edge is fixed with                                 so we have a bitmap to see if it's not                                 all not and then we have all the values                                 one after another and so on                                 so each vector is contiguous in memory                                 it doesn't have to be contiguous all the                                 vectors don't have to be contiguous one                                 after the other in memory because when                                 you're building them each one is a                                 different stream right we add values to                                 them but once we send them other the                                 wire because they're going to be written                                 like we're going to tell the network                                 layer to say a write this to network or                                 existing network and so on and so forth                                 on the other end is going to be a                                 tightly packed data structures which all                                 local and after one another so when you                                 receive it it's an ice pack pack data                                 structure so the main thing about moving                                 data between systems so in RPC we want                                 to avoid service agent Dessler ization                                 so we just we can send the data but                                 avoiding transformation because the wire                                 format is different from the in memory                                 format and we make I've written here                                 layer DVD because we haven't shared yet                                 as a standard library how to do their                                 raid rights and like each system does                                 its thing but it's going to be a shell                                 library to how to send those packets but                                 truly very it's very simple right we                                 have this record bachelor presentation                                 just send it over the wire with a proper                                 header and for IPC we have a prototype                                 using a memory mapped files and for                                 integration with between drill and                                 Python and you can imagine you have your                                 soul Apache drill is then it's written                                 in Java it is a sequel engine written in                                 Java so currently write your                                 user-defined function in Java but with                                 arrow because drill is using arrow in                                 its in memory or presentation and                                 actually I'm going to show them that                                 slide about this sorry it's truncated                                 but if you have your sequel engine                                 producing arrow you can pass a pointer                                 to it two different process which is                                 Python which is not training on the JVM                                 that can read this data                                 and I know the previews took talked                                 about it's bad to have shared memory but                                 in there you have to remember that this                                 is an immutable piece of data right we                                 write a record batch we finalize it now                                 it's immutable it's not going to change                                 anymore and we share a pointer to it in                                 read only when it's going to be read                                 only by this process and now this                                 process can produce a new record batch                                 which is the output of what this                                 user-defined function is doing and this                                 can be passed back to the sequel up to                                 the next sequel operator to the quarry                                 execution engine and what that means is                                 that there's zero overhead to do cross                                 process communication inter-process                                 communication and because we can just                                 have shared pointer to an immutable data                                 structure producing new one and share                                 pointer to the result with a new                                 immutable data structure and that means                                 that now you can use Python to do your                                 favorite gdf instead of having to do                                 whatever each system is implementing as                                 a user-defined function framework or                                 interface and that means once you wrote                                 it for Python in Python for let's say                                 drill is going to work for spark it's                                 going to work for in power because they                                 all did now the interface between the                                 two is Error so first we removed all                                 costs of you know it's not going to be                                 jni with the overhead of calling an atty                                 from Java and it's not going to have to                                 convert the representation to another                                 one it's all standard so there's no                                 overhead of inter-process communication                                 and its standard which means those                                 user-defined function that we work for                                 each query engine and if I go back so                                 that was for inter process communication                                 and for our pc i have an example of what                                 a query execution physical plan looks                                 like and so this example is select some                                 of a from t group by B and so you start                                 by scanning parka files and really like                                 we did before we're going to read only                                 two columns and turn them into our own                                 columns in memory and then here we have                                 three lines because there are three                                 machines in my cluster so that's machine                                 one machine to mantri machine                                           and each machine we do partial                                 aggregation and prepare blocks for doing                                 a shuffle and send you their machine                                 right so we take the key space and using                                 consistent hashing or whatever mechanism                                 we're going to decide which machine will                                 be responsible for the total aggregation                                 view of that particular key and so based                                 on that you can prepare if we have three                                 nodes in our cluster each node will                                 prepare three outputs with partial                                 aggregation result fudge set of keys and                                 then this is going to be sent over the                                 wire so here we stand the same machine                                 so there's no copy but if we go on                                 another machine we just send this about                                 the wire there's no extra sterilization                                 decision logic is just copied right over                                 and then the final aggregation which is                                 combining those three views together is                                 done and then when sending the result                                 set to the client is just same thing                                 against the in-memory representation is                                 just sent over the wire directly to the                                 client and so really that's it right                                 because the immemorial presentation is                                 the same as the wire representation we                                 remove all this overhead are converting                                 things and also each step you know you                                 can imagine loading from kudu kudu is                                 columnar representation on disk and                                 currently when you use a kudu client                                 it's going to convert it into assemble                                 those columns to present it to the                                 client in a row oriented form and and                                 then if it's if you're reading that with                                 apache drill for example is going to                                 turn in back into column now our                                 presentation so kind of you go back and                                 forth which is really inefficient when                                 once they're both integrated with arrow                                 is going to go directly from the                                 columnar representation of kudu to the                                 in-memory corner                                 entation and you don't go back and forth                                 between columnar and royalty and on that                                 you know almost done so the next step is                                 to do the shared park at euro conversion                                 I've add those shared library so there's                                 really a common format that all those                                 project have agreed on now we need is                                 some shared libraries and do the                                 integration in some of them and there's                                 also interest in the Intel persistent                                 memory library called Apache mnemonic                                 he's looking into that and so persistent                                 memory is not really interesting because                                 it persistent or at least not yet it's                                 interesting because it's a layer in                                 between SSD like disk and memory which                                 is slightly higher latency that Ram but                                 it's much cheaper you can have a lot                                 more of it for the price so it's kind of                                 having a good compromise in between so                                 if you want to join the conversation                                 like it more interested in this you can                                 lo look at the dev mailing list i'll                                 follow the twitter accounts and i'll be                                 open to questions are we done thank you                                 for the presentation                                 so I see there is a question here I was                                 just wondering um what if you need to                                 transport the data from a little endian                                 tuba canyon system and vice versa how                                 would you handle that so so the Indian X                                 is fixed so most system sorry I'm always                                 forget which one is which but for                                 example the Java default representation                                 is the wrong one so can someone remind                                 me which one is the most common that we                                 use all the time it's little Indian                                 right so every hardware system right now                                 it's easier little-endian or supports                                 both right like I think that's a point                                 so it's going to be a little engine and                                 Java is big Indian by default right                                 that's incorrect right because I always                                 got confused so parka is a little engine                                 and because we were we were walking with                                 the Impala team and their system is in                                 C++ so they want to be able to do the                                 trick when you know the cast an array of                                 integers into an array of bytes and it's                                 going to do the right thing so it's all                                 little endian by default so the same                                 here there's not going to be any                                 conversion because the format is defined                                 as it's all little Indian and that's it                                 so you don't need to know what the                                 system is underneath because I believe                                 unless someone knows or something else                                 little-endian is the best cheerful                                 choice for current hardware at the time                                 it's kind of big engine is kind of                                 outdated so from practical standpoint                                 this is not a problem no it's not a                                 hardware isn't there yeah it's not the                                 choice you get to make on Indiana's it                                 is the format as one in DNS and that the                                 way it is and there's no need to know                                 what the system does because part of the                                 strander is you have to do little engine                                 I'm not going to take sides over the                                 religious issue of ending this I'm just                                 going to serve this is actually a                                 complete repetition of the religious war                                 between Apollo and son in about nineteen                                 eighty nine about the wire format of                                 Apollo RPC versus NFS and of course NFS                                 is what their policy with recipient                                 makes good basically saying well never                                 stable wire format we're going to                                 produce what we know I'm confused you                                 basically hard-coded the exact opposite                                 of what son got through with NFS on you                                 so you basically said tell Intel is the                                 part that wins there's nothing wrong                                 with that I'm just you know server-side                                 it actually makes sense I thank you for                                 a top I'm curse about spilling so when                                 the memory is too low to hold a narrow                                 file in memory mm-hmm so like all these                                 systems that do in-memory processing the                                 one thing you want to look at is how                                 does this system behave when the data                                 set isn't fit in memory so arrow doesn't                                 define that right arrow defined how you                                 represent a given record batch which is                                 a subset of your data set so it defined                                 that you're going to be able to split                                 your data set in record batches and have                                 each chunk in memory at a time and then                                 it's kind of up to each system how they                                 do it so typically like for example if                                 you do a join of your doing aggregations                                 you're going to spill to disk right so                                 let's say I'm doing aggregation and I                                 keep the keys and the current values and                                 then at some point the key set doesn't                                 fit in memory anymore so you're going to                                 spill something and if I go back to this                                 for example in the aggregation that's a                                 good example because here it's                                 simplified you know and there's only one                                 record batch and then there's only one                                 record batch for each of those things                                 but it doesn't need to be right so here                                 what you can do is we're building those                                 partial aggregation in memory so I'm                                 doing the sum from the point of view of                                 that node and then when I want the                                 global son but I sum all those partial                                 sums together and I get the global some                                 which mean then here if I run out of                                 memory                                 point against pill to disk or you know                                 send it to the next one directly and say                                 hey I'm partially done there are too                                 many distinct keys I cannot keep it in                                 memory knee anymore I can spill it to                                 disk and do you know a disk-based merge                                 on this side or send it over and start a                                 new one and start aggregated for the                                 other keys right and that's still                                 correct because some is an associative                                 operation right so you can do this part                                 of the sum and then a doesn't fit in                                 memory because I have too many distinct                                 keys flash it to disk start over and the                                 contract is but just send them all                                 someday mole on the other end and you                                 can do a disk best some maybe I was                                 sorting the keys on purpose so that this                                 is more efficient when I write it to                                 disk or something so that's one example                                 so basically the system that uses arrow                                 will have to implement some strategies                                 right so you can start aggregating it                                 then it doesn't fit anymore you save it                                 you write it to disk you keep going in                                 memory as much as you can and then you                                 combine the results and of course if you                                 spill to disk then you take a hit is                                 going to be much faster if it's old fits                                 in memory butts like it's a typical                                 query execution problems and what are                                 the trade-offs so that's where also that                                 this format needs to be as compact as                                 possible for in memory because you want                                 to do as much as you can directly in                                 memory right to avoid overhead of                                 pointers and thing so columnar is good                                 for that as well because you know if you                                 have an array of objects that pones to                                 other values you add the overhead of a                                 lot like in Java typically a lot of                                 references we cease trucks that would                                 not be a problem but let's say if you do                                 the typical like how a lot of the system                                 started they are the list of Java like                                 if you look at how big works back in the                                 day you have a you would have a list of                                 top pole object and topple object is a                                 map of string name field name to the                                 actual value which is very inefficient                                 uses a lot of memory but the column now                                 representation like that is more compact                                 so the game is as much as possible do                                 everything in memory                                 it doesn't work anymore you need to fall                                 back to disk do something send partial                                 results and they're going to be more                                 work involved but have some kind of                                 trade off oh by the way yeah we have one                                 last question but you have to keep it                                 really short okay so you can take it                                 offline so it's going to be a yes or no                                 answer not just kidding thanks for the                                 talk you said avril packages or blocks                                 they are mutable other plans to                                 introduce versioning updates or do you                                 always think you update the on disk                                 format and then you per query load in                                 memory arrow packages and you ever want                                 to reuse them in the future so I'm not                                 sure if I'm going to answer your                                 question but so I've said this is like                                 transient in memory but you can reuse                                 the same format for spilling to disk or                                  like you are on this cash right you can                                  temporarily serialize it to disk or do a                                  lot of stuff or keep it keep an                                  in-memory cache as well sorry can you                                  repeat maybe we can take it offline I                                  said which part was important to you so                                  I think the initial assumption is it's                                  you created once per query yeah so sorry                                  I missed that part so yeah you're free                                  to modify the content it's not it's not                                  just up and only you're free to go                                  editor value maybe you're doing                                  aggregations right and you keep updating                                  that some in the Earl you have in memory                                  and you at some point you're spawning to                                  it and that's fine it's just when you're                                  done writing to this record batch and                                  you finalize it basically you're saying                                  I'm not modifying it anymore and at that                                  point it becomes immutable it doesn't                                  have to be mutable from the start or                                  happen only you can mutate it as much as                                  you want but you're a single rider to it                                  and then you finalize it and you say                                  it's immutable now other people can read                                  it and then can read it in parallel and                                  it's fine because nobody else is                                  mutating it and that's that the theory                                  behind that and also one thing I wanted                                  to precise is this is typically not                                  being going to be visible from the user                                  code instead unless you're doing some                                  special integration between two of those                                  systems so that you're actually                                  contributing to making arrow work                                  everywhere                                  but basically usually it's hidden in the                                  system right so when Impala uses it or                                  drill uses it its internal or                                  presentation it's not something that you                                  see or they're going to be a client that                                  abstracts it at for you and so I'm like                                  thank you and we can you know can have                                  drinks and talk more about that stuff                                  afterwards thank you so much Julian
YouTube URL: https://www.youtube.com/watch?v=43O5BCabBcU


