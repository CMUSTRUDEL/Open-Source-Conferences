Title: Berlin Buzzwords 2016: David Whiting - Big Data, Small Code ... #bbuzz
Publication date: 2016-06-12
Playlist: Berlin Buzzwords 2016 #bbuzz
Description: 
	David Whiting talking about "Big Data, Small Code: Using Java 8 and Apache Crunch to quickly develop concise, efficient, readable and testable data pipelines for Hadoop MapReduce and Spark".

New execution platforms may be popping up all the time with the intention of being the "hot new thing" in Big Data, but all the while most of the heavy lifting in data organisations is still done with Hadoop MapReduce; and it continues to be a sensible choice for whole classes of ETL and aggregation problems. 

Apache Crunch is a simple framework on top of MapReduce - with support for running on Spark as well - which applies simple, typesafe, functional programming idioms to batch data processing pipelines to maximise developer productivity. With the addition of Java 8 and the upcoming crunch-lambda module, it is now simpler than ever to express your intent and get code working on your cluster quicker. 

This session will introduce the concepts behind Crunch, introduce the API, and provide practical examples of how it can be used to simplify your codebase and increase your productivity.

Read more:
https://2016.berlinbuzzwords.de/session/big-data-small-code-using-java-8-and-apache-crunch-quickly-develop-concise-efficient

About David Whiting:
https://2016.berlinbuzzwords.de/users/david-whiting-0

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              hello welcome everybody this is a                               session called big data in small code                               and it's all about using Java rate and                               apache crunch to quickly develop concise                               efficient and testable data pipelines I                               got to tell you a little bit about                               apache crunch and going to tell you a                               bit about the new crunch lambda module                               i'm going to give you some real-world                                code examples from how we're using it at                                soundcloud and the impact that's had on                                the teams and then talk a little bit                                about the future at the end but before                                we get on to all of that let me just                                quickly introduce myself my name is                                david whiting that's me there i've been                                working with Hadoop and big data for                                about five years now first at last.fm                                then it's Spotify now at soundcloud in                                berlin so you might be able to see a                                theme in the kind of companies i've been                                working for as well as my normal                                day-to-day jobs I've got especially                                interested in improving the developer                                experience for big data like to make it                                as easy as possible to pragmatically                                deliver the data that people need I'm a                                committer on the Apache crunch project                                so that's my full disclosure there and                                I'm also a music producer in artist so                                you can go to there if you want to hear                                stuff if you want to get in contact with                                me Deb w at Apache dog or at da whiting                                in tweet abuse or comments and criticism                                afterwards so let's look at the state of                                the big data world in                                                  a lot of these conferences you hear a                                lot of hype about new technologies new                                execution platforms new ways of doing                                things new api's you hear a lot about                                Sparky hear about machine learning you                                hear about real-time stream processing                                hear about these weird advanced                                visualization techniques and especially                                now I think the trend at this conference                                is probably gonna be about event                                sourcing and doing everything from an                                event pace base point of view if you                                actually talk to the people who are                                working in organizations which are doing                                this kind of thing doing big data you'll                                find a lot of them are actually just                                using MapReduce to run etl aggregation                                and analytics jobs as they always have                                been there's a there's a huge wealth of                                code and legacy and infrastructure that                                was all built for Hadoop MapReduce and                                people aren't going to throw that away                                in a second when the new technology                                comes along or at least it's not very                                sensible to throw that away when the new                                techno                                she comes along so in short and                                apologies if in the audience is a person                                whose talk is called this but I wrote                                the slide before I knew this there's                                another tool called MapReduce is not                                dead tomorrow but yeah this is my my                                point here is that MapReduce is not dead                                it's still being used in all kinds of                                organizations it still works so let's                                try and make it better so in in my in my                                role in data infrastructure at                                soundcloud i'm working with what you                                might call the boring end of the data                                pipeline so we're not doing fancy                                analytics we're not doing a fancy                                machine learning we're preparing the                                data for all those teams to be able to                                do what they need to do so we're                                filtering out any debt bad data that                                might have come from the clients we're                                transforming data that might be in                                different formats so we have some                                clients that might emit some jason in                                some old schema because that client we                                can't update it anymore it's out in the                                world we also might have some new client                                or some server emitting the new proto                                buff data and we need to consolidate all                                that into the same format so we can                                process it downstream we also need to                                split the data take all the data of all                                the different event types and the time                                and it happens and split it into buckets                                but idea the type of event it is and                                time it happen so that means when people                                are processing the data later they can                                just read a directory on HDFS and not                                 have to pre filter everything read too                                 much and throw most of it away we have                                 some correlation jobs so we can                                 correlate related events so for example                                 if someone on soundcloud starts                                 listening to a track and then they seek                                 to another part of the track and then                                 rewind and then carry on playing we want                                 to see all those events as one view so                                 we have some some jobs to stack up                                 events on top of each other because we                                 want to post them as one pro system is                                 one unit we also have some monitoring                                 workloads monitoring the Council of                                 events and the types of events we're                                 getting because that's a really good                                 sign that something might have gone                                 wrong if we see big spikes or dips in                                 the counts and then some simple pre                                 aggregation jobs which feed into                                 analytics or other databases like                                 Cassandra                                 that kind of thing but we have a problem                                 all of this stuff is written in                                 MapReduce or at least it was six months                                 ago but the MapReduce API hasn't dated                                 very well it hasn't aged very well since                                 about ten years or something it's been                                 around maybe more the rest of the way we                                 interact with data has moved on a lot                                 since then and if you're still writing                                 code against the original MapReduce API                                 you're probably getting a bit frustrated                                 with how much you have to do which is                                 where Apache crunch comes in Apache                                 crunch is an attempt to make that                                 developer experience for MapReduce a                                 whole lot better and more modern so I'm                                 going to take a quote from the Apache                                 crunch web page because it explains it                                 better than I can and that's that the                                 Apache crunch Java library provides a                                 framework for writing testing and                                 running MapReduce pipelines its goal is                                 to make pipelines which are composed of                                 many user-defined functions simple to                                 write easy to test and efficient to run                                 so in practice that means it's                                 functional you express everything as a                                 series of functional transformations                                 it's typesafe rather than fields based                                 as many other products in this space are                                 so that means is particularly                                 well-suited for the boring end of the                                 pipeline when you're dealing with big                                 structured records that you have to                                 propagate intact and you can't deal with                                 any mismatch and that data and you have                                 a strict schema to adhere to everything                                 runs as MapReduce there's no extra                                 software you have to install on your                                 Hadoop cluster you just build it against                                 crunch and that gives you a jar you can                                 run with Hadoop jar or yarn jar on your                                 cluster there is there is a zero                                 boilerplate requirement unless you're                                 doing anything particularly fancy none                                 of that setup and configuration and that                                 whole ceremony about writing that                                 produced job supplies it also has a                                 switchable implementation which ones in                                 memory so that makes it really easy to                                 run unit tests of your pipelines and                                 also the way you construct your                                 pipelines is comprised of standard Java                                 functions so if you want to do low-level                                 unit testing you're just testing normal                                 functions and it's used at a despite not                                 being the coolest or the most trendy                                 thing it's actually used at a surprising                                 number of major                                 Jose shins we're using it soundcloud                                 before i used it at spotify it's used                                 its owner it's used a whole host of                                 companies that don't really speak                                 publicly about the stuff they're doing                                 so i can't tell you what they are but                                 i've spoken to people who work there who                                 use crunch so it's pretty stable it's                                 pretty usable so the basic approach                                 behind crunch should be familiar to                                 anyone who's seen any of the more modern                                 api's for developing with data and                                 that's that each data set as you read it                                 as you in every stage of processing is                                 an immutable lazily evaluated collection                                 of of records so you take a collection                                 and then you apply a transformation you                                 get another collection you apply a                                 transformation you get another                                 collection and at some point when you're                                 done you write it to HDFS and that's                                 when the whole thing gets evaluated a                                 query planner comes along and turns it                                 into MapReduce jobs and then writes out                                 runs the jobs writes everything out to                                 HDFS so if you're familiar with spark                                 and hardy DS a spark idd is sort of like                                 a crunch p collection and the API I'm                                 not sure why Britain gr                                                  not quite relevant to this bit it's                                 relevant to the next day nevermind so                                 still                                                                   to see this example it's mostly a tool                                 for counting words so in crunch land it                                 looks a little bit like this so from the                                 top this is the whole code by the way if                                 you put this in a public static void                                 main then put it in a jar it'll run as a                                 MapReduce job on the cluster so we're                                 reading from a text file in HDFS then we                                 have this parallel do function which                                 takes each line of text we split it by                                 space iterate through the words in the                                 line omit the word as the record to                                 output then this built-in count function                                 counts the number of instances of each                                 word then we have another parallel do                                 thing which takes the pair of strings                                 along so that's the word and the count                                 and formats it for output by just here                                 taking the word colon count then we                                 write to the text file and then when                                 right crunch done that means it's going                                 to do all the query planning and the                                 execution and                                 run your jobs right everything to HDFS                                 now this is already a big improvement on                                 working with the normal MapReduce api we                                 haven't had to do any mappers and                                 reducers and implementing classes and                                 passing class names to everything but                                 there's still a lot of noise here                                 there's still a lot of redundant                                 information if you can see when we're                                 doing these do FM's and these map FM's                                 we're expressing the type information of                                 the inputs and the outputs at least                                 three times in each of those and that's                                 too many and also it hinders readability                                 if you're trying to pass all of this and                                 you've got all this extra noise around                                 the anonymous inner classes in the way                                 because actually what we're really                                 interested in is this which is a whole                                 lot less information and a whole lot                                 easier to read now luckily for us java                                   is out and now means that we don't have                                 to do this whole ceremony of anonymous                                 inner classes if we want to pass around                                 functions that we've defined at the at                                 the cool side so in about late last year                                 about October November time we at                                 soundcloud upgraded our Hadoop cluster                                 to the next version along and in the                                 process we also upgraded to Java                                       that gives us the ability to run Java                                   stuff on our Hadoop cluster I don't                                 think it's officially supported by                                 Hadoop yet but we've been running it on                                 cloud errors cdh                                                       it seems to be pretty stable for us but                                 the trouble was there was no compatible                                 Java                                                                 because crunch is still very much based                                 on abstract classes rather than                                 interfaces it doesn't it there is no                                 drop-in replacement for him so in                                 January of this year I wrote a new a new                                 API for crunch based on Java                                          crunch lambda and the idea behind it was                                 to take advantage of all Java eights                                 features in method references and lambda                                 expressions and the streams API and wrap                                 crunch in that so you can interact with                                 it in a way that feels as native to Java                                                                           so that means we work with streams                                 instead of iterators and iterate balls                                 and all those logic that we saw                                 expressed as these do FM's and map                                 offense are now just lambda expressions                                 or method references so as of last month                                 that is now in the main crunch release                                 oh Fortino and ready for use and it                                 works pretty well we're using it a                                 soundcloud so if you take the example I                                 just gave you and put that into crunch                                 lambda work world it looks a little bit                                 like this now the first thing you can                                 notice about this is there's a lot less                                 code but the second thing like I'm not                                 one to cut down code volume arbitrarily                                 if it's not saving anything but the most                                 important thing about this as I think                                 it's more a lot more intuitive and easy                                 to read you can follow the structure of                                 the program a lot more easily see                                 reading from the text line we're taking                                 each line we're splitting it by space                                 but instead of doing this in a pair of                                 input and omit we're doing a flat map                                 which is usually a bit more intuitive                                 for people to think about as the                                 transformation from one record                                       more records we do a count operation and                                 then we're mapping them to the same                                 formats before writing out of the text                                 file and the only thing that's different                                 about this in terms of using the API as                                 we've wrapped the original collection we                                 read from the data in this lambda trap                                 so that's crunch lambda it's fairly                                 simple it's a really lightweight wrapper                                 so it uses all the crunch execution                                 underneath in order to understand a bit                                 more about crunch it helps to look a                                 little bit at the data model behind it                                 so in in the world of crunch these three                                 types of collection there's AP                                 collection a p table and a P group table                                 in a P collection you're just looking at                                 individual records of data so that could                                 be a string if it was lines of text in a                                 fire or it could be numbers if you've                                 got some numeric data set to process in                                 our case it's most offered in structured                                 records so we have all of our event data                                 stored as                                 buff's in sequence files on HDFS so when                                 we have a collection of things it's                                 usually a collection of some event type                                 but the important distinction between                                 that and other types is that according                                 to crunch it doesn't know about the                                 structure in the data readjust knows                                 that you have one event so in their                                 table representation you can go from in                                 the table representation crunch knows                                 about a separate key part and a value                                 part of each record so here you might                                 take your structured record and extract                                 a key from it to create a key part of                                 the record as well or you might be                                 reading data from HDFS in for example a                                 sequence file which has a key in a value                                 part already so that's naturally                                 represented as a table so in this                                 example you can see that some of the                                 value rose share the same key and some                                 of them have different keys so when we                                 when we take the table when we group by                                 key we go to a group table and that's                                 where we collect together all the values                                 for each unique key so if you do a                                 transformation from a table to a group                                 table and then back to a collection or a                                 table that pretty much in all cases gets                                 run as a MapReduce cycle so when you go                                 from the table to the group's table                                 that's when you're going from the map                                 side to the reduced side if you if it                                 helps you to think about in terms of                                 MapReduce originally so with those three                                 types i'm going to show you another                                 slide that everyone tells me i should                                 remove from this presentation because                                 it's confusing of these arrows                                 everywhere but we're actually missing                                 the left part of it as well but this is                                 just showing you the different                                 transformations between the collections                                 and how you get from one to the other so                                 we can read in data and get a collection                                 or a table and when we have a table with                                 a key in a value we can group by key to                                 go to a group table and then in group                                 table you see all these operations for                                 aggregating values by keys so we have                                 ways to combine them or providing a                                 reduced function or collect them into a                                 collection or do some other kind of                                 custom processing                                 so that's all well and good and seem                                 sensible but what if we're not running a                                 word count company it's quite often in                                 these kind of talks to give a couple of                                 twigs amples and then say here you go                                 off you go run with it without any                                 examples from the real world so I'm                                 going to take you through a couple of                                 examples on how we're using crunch right                                 now in soundcloud to do some one simple                                 workload and one more complex workload                                 so the first example I'm going to talk                                 to you about is all about monitoring                                 event counts so as I mentioned before                                 it's really useful to monitor the                                 Council of events as they come through                                 to look for anomalies that might be                                 happening in the clients or anomalies                                 that might be happening in our transport                                 our event transport system a quite                                 common case is that someone pushes out a                                 new client and they don't tell us about                                 it and they've slightly changed the                                 schema so they're emitting a string when                                 they're in they used to be emitting an                                 int or something and then the count for                                 that massively drops because we can't                                 pass it in our system so it's really                                 useful to monitor counts over time and                                 one thing we do is just count events by                                 the type of events that might be sound                                 played or user click to like button                                 something like that and applications                                 that would be like iOS Android web                                 something like that so we do that by                                 using a metric system called prometheus                                 if you don't know about it it's a really                                 good way of tracking metrics and doing                                 alerts for your services and things                                 there's another open source project that                                 came out of soundcloud so if you wanna                                 know more about prometheus just as an                                 aside go to Prometheus taio it's growing                                 way beyond soundcloud now and it's there                                 it's a big thing but here we're going to                                 use the push gateway which is a way of                                 pushing metrics that Prometheus where                                 prometheus is normally a pull system and                                 so you push it to the push gateway it                                 hangs around on there for the Prometheus                                 scraper to come around and scrape the                                 metrics so we create the crunch pipeline                                 using the MapReduce implementation                                 create a push gateway client with which                                 is just an HTTP m point that we push the                                 metrics at this gauge thing is a thing                                 from prometheus so we're creating a                                 gauge with event count                                 as the metric and two labels which is                                 the event type in the application so                                 sound played and iOS for example we                                 create that gage and then we read the                                 events into a table this read events                                 function is a little bit more complex                                 than just read from text file or                                 something so i factored that into a                                 function i'm not showing that here but                                 when we've done that we have a                                 collection of key value pairs in the                                 table which is a long time stamp and a                                 structured protobuf event type but we're                                 not interested in the timestamp because                                 in the read process we are already                                 bucketing it by time so we throw away                                 the key part which is what this values                                 thing does and then we map it through                                 just a plain old java function which is                                 creating the stats key object from the                                 event and that stats key is just                                 comprised of the event type and                                 application in the real world is                                 actually a few more fields in there                                 which is why I justify having a separate                                 type for it but I've slightly simplified                                 it for this then again just like the                                 word count example we're counting the                                 number of events for each stats key then                                 we use this materialized function and                                 the materialized function is really                                 powerful feature of the crunch                                 abstraction while materialized does is                                 write out the results at that point to a                                 file so it will run the MapReduce job                                 right to a file on HDFS and then open                                 the file for reading and give you a                                 pointer into the data locally from where                                 you're running the job so all of this                                 the map and the count has been run on                                 the to cluster and then we when we use                                 materialized we're bringing that data                                 locally and running some more code with                                 it so it gives us a Java                                             those stats key and count                                            then when we have all that data locally                                 we can just iterate over it in this in                                 this application itself so that's the                                 stream style for each here we take the                                 statue of stats record apply the labels                                 to the gauge and set the count then                                 finally at the end of it we can push all                                 the metrics to the Prometheus push                                 gateway and then on the Prometheus push                                 gateway we set up graphs and we can set                                 up a loading rules so if any of these                                 keys is much higher or much lower than                                 it was yesterday or last week                                 then we can fire off alerts wake someone                                 up in the middle of the night or                                 something like that so that's a really                                 simple use case for how we're using it                                 in our event transport and delivery                                 pipeline as a just kind of last check                                 very simple very simple check but it                                 also helps us notice a lot of failures                                 earlier than we ordinarily would so the                                 second example I'm going to take you                                 through is about a user facing product                                 and that's the the SoundCloud stats                                 product if you're a creator on                                 soundcloud enough is anyone here a                                 creator on soundcloud okay so got one at                                 the back so if you're an artist on                                 soundcloud sound creator then you have                                 access to a stats product which gives                                 you feedback about how people are                                 listening to your sounds so this is mine                                 for one of my artists pages so you can                                 see all kinds of data about when people                                 have listened to me how many plays how                                 many likes how many comments but in the                                 bottom half of this you see some top                                 lists explaining different dimensions so                                 we have the most played tracks by this                                 artist we have the countries in which                                 the tracks by this artist are played the                                 most we have which users are listening                                 to the artists most and we have which                                 websites they get to your page from so                                 it's all about giving sound creators a                                 bit of insight into how people are                                 listening and maybe like if you see one                                 country spiking than you might think ok                                 now I should try and engage listeners in                                 that country or if you see one track                                 that you're not expecting to be popular                                 is more popular than all the rest of                                 your tracks maybe it's time to                                 investigate why and see if you can do                                 something with them so it's really                                 powerful data for the creators to use                                 and the the backend for this is                                 implemented again as a crunch pie                                 pumpkin chai pipeline so this is the the                                 main part of the code for this before                                 this there's a little bit that reads the                                 events and at the end of this there's a                                 part of it which                                 takes all the output data and formats                                 into records which get loaded into                                 Cassandra we actually create the SS                                 tables and push them from the MapReduce                                 job itself but I've left that bit out                                 because I want to talk about the                                 processing steps so this is a little bit                                 more complex example it's going to                                 manifest as two separate MapReduce type                                 cycles but let's go through it                                 step-by-step so we have an event stream                                 of events that happen somewhere in                                 soundcloud so here we have an example                                 sound sound played so the sound has been                                 played by David that's me in Germany on                                 the web client now there's a function                                 called create facts from event that                                 generates some one or more facts from                                 that event so in this case there's three                                 facts that come from that event one is                                 that that the sound has been played in                                 Germany and next fact is that the sound                                 has been played on a web client and the                                 next fact is that the sound has been                                 played by the user David and we emit a                                 what's called a bucket is the key part                                 of this and a value of one for each one                                 of those so we're driving like more than                                 one piece of information from each                                 factor from each event that comes in and                                 then we have other event types like                                 sound favorited or reposted or something                                 like that then we group those by key and                                 we sum up all the counts so we can see                                 that this track was played in Germany                                 twice for example with that input data                                 of two records for the Germany bucket                                 and one record for the UK bucket and                                 then after we've summed up those counts                                 so when we do this reduced values that's                                 when it will happen on the reduced side                                 of the MapReduce we also need to filter                                 the values because some events can                                 generate negative facts so if if someone                                 unfollow someone or unfavorites                                 something it can create a negative count                                 and it turns out if you present the data                                 to users with negative favorites or                                 negative reposts then they're not so                                 happy about it so we filter out the                                 negative counts because the user don't                                 like seeing those and then we move on to                                 the next step and the next step is about                                 we have to shuffle shuffle the key in                                 the value parts of the record so what                                 when it used to be a bucket play country                                 Germany                                 to what we're now interested in is some                                 aggregating things by the by the column                                 in that topless view that I showed you                                 earlier that will be the the heading for                                 the top list so here this is the top                                 country this top countries that this                                 track is played in and then the value is                                 a topless pair of Germany and two                                 because the idea is you want to collect                                 all those values together and keep the                                 top k of those so once we've shuffled                                 the key of value part which is just this                                 remap dimension keys just takes one                                 thing out of one record and builds this                                 new pair of these two parts of the                                 record then again we do a group by key                                 and it gives us a group table and then                                 we can operate on the value part of this                                 and this map values function will just                                 give you a pair of the key and a stream                                 of the values and excuse me give you a                                 pair of the key and a stream of the                                 values and then you can return some kind                                 of record so here we're using a plain                                 old java streaming top-k algorithm which                                 is this fine top k so we just create a                                 sorted set with a comparator the count                                 reversed so we want the the biggest ones                                 of the foot at the top and then we                                 compare also by the dimension value                                 which is like the country name because                                 we want to keep this deterministic if we                                 run it again we want to have the same                                 output and then for every input we add                                 it to the set evict stuff if it gets                                 bigger than k because k in this case is                                 something like                                                         sensible way of doing it so since we've                                 been changing things around to use                                 crunch at South out we had a when I                                 turned up at soundcloud about eight or                                 nine months ago we had a lot of old                                 MapReduce code using with the original                                 MapReduce API and some custom in-house                                 abstraction over it which didn't really                                 help very much then we had quite an                                 impact at soundcloud so we see a lot of                                 these complex MapReduce job crafts job                                 graphs replaced with simple pipe lines                                 which are easy to read they're easy to                                 understand another developer can come                                 and pick it up read it understand what                                 it does make my                                 locations to it so just in my team as                                 well as those two we have the entire                                 event cleanup and splitting and that                                 part of the pipeline is all in crunch                                 now and we have the job that takes                                 related events and stacks them up                                 related events related to sounds being                                 played and stacks them up so we can get                                 an accurate idea of the duration that                                 someone has been listening to a sound                                 for that also happens in crunch now we                                 have four different teams using it we                                 have our team in the data infrastructure                                 we have the insights team the royalties                                 and reporting team delivering their                                 their reports to the record labels and                                 also the trust security and safety team                                 are doing some of their analysis using                                 it and all of them are saying that                                 they're they're much happier using this                                 than MapReduce we've even had some                                 people that were writing stuff in spark                                 changed to using crunch because the                                 execution is a little bit more reliable                                 and it doesn't use so much memory on the                                 cluster so for especially if you have a                                 really big data set spark can sometimes                                 be a bit if you with that when you're                                 spinning to disk so people who were                                 using sparker now he's in crunch and                                 finally get more reliable and finally                                 because the way it's much easier to test                                 some one like this then using the                                 MapReduce API we find the jobs they                                 didn't have any tests at all now have                                 tests which is a great improvement we                                 actually have some confidence both in                                 our existing old logic and when people                                 are a new stuff that it might work so                                 for the future I mean crunch is a bit of                                 a strange case here because it's a                                 relatively new API in the last couple of                                 years or so to work on a relatively old                                 technology that most people consider                                 towards the end of life from our                                 experience despite its zero dot version                                 number the version numbers of cruncher                                 extremely conservative i will consider                                 it's been really stable for four or five                                 versions at least now and it's feature                                 complete it's stable it works                                 we're using it really happily as a as a                                 pragmatic way to keep writing MapReduce                                 code that does the things that we need                                 to do every day we're really excited                                 about the Apache beem beem project I'm                                 hoping to hear more about that I think                                 there's a couple of talks about it at                                 the rest of this conference which is                                 actually based on the same paper that                                 the crunch API was originally based on                                 so the crunch API was based on this                                 flume Java paper that came out of Google                                 and apache beam is the open-source                                 version of the google data flow api                                 which is also an evolution of the flume                                 java paper so it seems that all of these                                 technologies are kind of converging on                                 the same same API and certainly the same                                 approach to programming so we're with                                 beam in the pipeline where we're not                                 worried at all about writing crunch code                                 we don't think this is a waste of time                                 to do because the the path from crunch                                 to beam or spark or any of the other                                 technologies should be relatively simple                                 the programming model is the same the                                 way you have to decompose your programs                                 is the same beam is getting a lot of                                 traction we're excited about that of                                 course but we won't be able to use it                                 probably until this time next year and                                 I've even considered I'm not sure I'll                                 do this or not but you I was discussing                                 last night at the speaker's dinner maybe                                 we could write a beam runner for crunch                                 for MapReduce so you can continue to run                                 your your your beam applications on                                 MapReduce by crunch but it sounds a                                 little bit like pipeline inception so                                 I'm not sure if that's a good idea or                                 not it might happen oh I made a slide                                 about that okay yeah so in summary                                 MapReduce isn't going away soon much as                                 we'd like it to and much as the hype                                 would tell us it is there's a lot of                                 investment and everything that's gone                                 into that and it's still quite a good                                 fit for a lot of problems so crunch is a                                 really pragmatic way to keep working                                 with it and not be so frustrated java                                   has been a really great improvement for                                 functional programming it stops everyone                                 from having to jump at scala immediately                                 if they have some kind of vaguely                                 functional need                                 we're finding it actually scholar is                                 such a big and complex language that if                                 we can keep people on Java H rather than                                 reaching for scholar then we'll save                                 ourselves a lot of complexity in the                                 long term so we're really happy to be on                                 Java rate and using crunch chart crunch                                 lambda and Java rate it ties all the                                 concepts together and makes us more                                 productive data engineers if you want to                                 learn more about crunch a good place to                                 start is crunched at Apache dorg the                                 user guide for crunches is really                                 complete I haven't yet got around to                                 writing the user guide for crunch lambda                                 but the javadocs are really complete so                                 if you go to the javadocs for crunch                                 look up the lambda class and it's got                                 lots of examples and information there                                 if you're if you're interested in the                                 use cases I presented from SoundCloud                                 I've written a long floor long formed                                 blog post on the SoundCloud developer                                 blog about those two examples so if                                 you're a developer soundcloud.com / blog                                 you can read about them if you want to                                 read more about crunch lambda and also                                 general topics that I'm writing about                                 Java and modern Java styles that's my                                 blog at radical Java calm and if you                                 want to read these slides again because                                 you missed something there up on                                 tinyurl.com / be buzzed crunch and                                 that's it I've got a new album out last                                 month so check it out if you're                                 interested I got time for a little bit                                 of QA great thanks tanks are done David                                 for the intriguing                                 if any one of you have any questions                                 please feel free to raise your hand I'll                                 just quickly run across any questions                                 any thoughts to add we are good all                                 right can't that be off the hook like                                 that we got one of the back oh cool                                 please introduce yourself hi mattias                                 andersson I was just wondering are you                                 happy working with it with your Java                                   lambdas to your compiler no not your                                 compiler but use eclipse with which has                                 type notations that are used in or do                                 you have any problems with piping for it                                 nice dreams I'm using IntelliJ most of                                 the time and I don't really have any                                 problems it's a it's a bit confusing                                 because sometimes like for the crunch                                 things you put the type information like                                 you have your lambda expression then you                                 have an indicator of the type and                                 sometimes I find myself I have to fill                                 in the type thing first because then                                 I'll get the autocomplete on the lambda                                 expression so there's a couple of little                                 in the idiosyncrasies like that because                                 it can't figure out that no matter what                                 I put in the second part it's still                                 going to be the same input but apart                                 from that it's I have already had any                                 problems with IntelliJ any more                                 questions alright so thank you David                                 again feel free to come and ask me                                 afterwards if you have any more                                 questions                                 you
YouTube URL: https://www.youtube.com/watch?v=r6GUlo-QK5Q


