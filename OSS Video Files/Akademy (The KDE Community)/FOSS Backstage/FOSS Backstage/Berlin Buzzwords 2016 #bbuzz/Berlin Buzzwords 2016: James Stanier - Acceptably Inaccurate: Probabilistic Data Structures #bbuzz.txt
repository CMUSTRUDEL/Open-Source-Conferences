Title: Berlin Buzzwords 2016: James Stanier - Acceptably Inaccurate: Probabilistic Data Structures #bbuzz
Publication date: 2016-06-12
Playlist: Berlin Buzzwords 2016 #bbuzz
Description: 
	Writing software for the Internet puts engineers face to face with traffic that makes doing even simple things difficult. Seemingly straightforward operations such as counting, set membership and set cardinality become either extremely slow or prohibitively expensive to do using traditional methods.

Help is at hand, however: probabilistic techniques are both fascinating and extremely useful. By sacrificing a predictable amount of accuracy, we can perform operations at scales we never thought possible, and fast! We'll introduce approaches such as Bloom filters for set membership, count-min sketch for frequency in streams, and HyperLogLog for cardinality. No maths PhD is required.

We'll look at the before and after effects of using these techniques in real-world scenarios, and present the libraries that you can go away and play with right now.

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              and over the last couple of years we've                               built some new products that use these                               in the back end and I thought I'd share                               it with you so what are we doing today                               for things first thing is some                               motivation so if you don't know what                               these data structures are or have any                               idea at all then I'll give you some                               motivation as to why you should care                                about them and might why you might find                                it interesting and then we're going to                                be looking at three probabilistic data                                structures and I think that these are in                                order of complexity we're going to start                                off with bloom filters which I think                                quite simple to understand then we're                                going to go into count min sketch which                                is a little bit more tricky to                                understand and then we're going to try                                our best to explain hyper log look which                                is quite complicated but first off why                                should you care about probabilistic data                                structures so I've tried to prepare some                                motivation for you here to see why you                                might be interested here are some                                storage technologies and we have a line                                underneath these storage technologies                                now i'm going to put an arrow head on                                this line and try and show you some                                things so if we put an arrow head on                                here I'd say that the speed of these                                things in terms of the speed of reading                                the speed of writing goes up as we go in                                that direction on the slide so I think                                we will agree that memory is like the                                fastest thing to read and write to yeah                                some nodding good some people are awake                                SSD excuse me SSD is excuse me well too                                much smoking SSD is quite fast compared                                to hard drive but hard drive is spinning                                disk so it's slower and tape is very                                very very slow in terms of cost is in                                actual dollars pounds euros and /                                  gigabyte of any of these things you'll                                find that a gigabyte of memory costs you                                a lot more than a gigabyte of SSD which                                costs you a lot more than a gigabyte of                                hard drive and so on so cost is an                                important factor I'd also say that the                                ease of use to you as a developer is                                definitely in this direction as well                                because if you want to use memory in                                order to store data it's actually really                                easy you just initialize an array or you                                initialize a set or something in your                                code use one line however if you're                                going to be using an SSD or a hard drive                                in order to                                be doing stuff for your data you might                                need to use an open source framework in                                order to do it so here's a very naive                                example of perhaps I could declare a set                                in memory of strings but if that's too                                big for memory then maybe I need to                                store some documents in elastic search                                or maybe solar and if we're going                                further down the chain and we're using                                hard drives we may need to use Hadoop or                                HDFS or something like that the idea                                with this is that if we keep things in                                memory it's so much easier for you when                                you're writing your programs to do stuff                                that error goes in the other direction                                when we're talking about storage per                                node so this is either on a physical                                server in your data center or AWS                                instance or something you'll find that                                usually you have more memory and sorry                                less memory available that you do space                                on an SSD and if you have some massive                                raid configuration of hard drives you                                can end up having a lot of storage                                available and hard drives so the                                question is how do we do more stuff here                                because not only is it faster it's also                                easier to use in your code and you have                                to spend less money on storing stuff and                                you don't need to maintain any external                                storage systems so that's the motivation                                for these data structures so                                probabilistic data structures allow you                                to do things in memory at a scale that                                you previously didn't think was possible                                they often use a really tiny memory                                footprint they also have really cool                                names so when you talk to your                                colleagues about them and you show them                                your code you submit your pull request                                you look really clever which is awesome                                 and I think they're really fascinating                                 things and as I said at the beginning of                                 the talk sometimes the barrier to entry                                 is a little bit high so what we're going                                 to do today is we're not going to blind                                 you with loads of mathematics we're                                 going to look at some Java code that                                 does things in a naive way we're going                                 to look at the effect that has on the                                 heat and how much memory we're using                                 then we're going to walk through the                                 algorithm of each of these data                                 structures and show you how they work                                 visually and then we're going to rewrite                                 that code using the proper ballistic                                 data structure in the library that you                                 can go away in use today and then we'll                                 look at the heap again so hopefully that                                 gives you some sort of concrete examples                                 of how you'd use them but one thing that                                 you have to remember when you're using                                 these data structures is a very                                 important thing that as a developer you                                 are able to accept a predictable level                                 of in accuracy so inaccurate in the so                                 much that when you use this data                                 structure it                                 be wrong and also it might be wrong by                                 some amount but when you're using them                                 you accept the amount that you accept it                                 to be wrong and if you're ok with that                                 then you can use them but another                                 important thing to think about when you                                 use these things not just in a back-end                                 system but when you use them in                                 something that's user facing is that the                                 numbers that come out of these things                                 might not be completely correct so that                                 what does that mean to the business                                 analysts who are using your software                                 what does it mean to the users if things                                 are slightly wrong just something to                                 think about without further ado we're                                 going to do three data structures today                                 the first one bloom filters so let's get                                 started on this they don't have anything                                 to do with flowers at all they're named                                 after a mathematician called Burton                                 Howard bloom and what they're used for                                 is set membership tests so you can test                                 whether an element is in a set and to                                 begin with the easiest way to explain                                 why you would use and is to show a                                 really naive example using Java so we're                                 going to use a java.util.set here and                                 this isn't how you would code visitors                                 to a site because we're using strings                                 for eye peas and all that kind of thing                                 but it's just an example what we're                                 doing at the top is we're making a new                                 set of strings and hopefully you've all                                 seen a set before in Java util then                                 we're going to add three elements to                                 this set which are three different IP                                 addresses and then at the bottom there                                 we can call the contains method to see                                 whether one of these elements is in that                                 set and the first element is because we                                 put it in there and the second element                                 isn't because we didn't put it in there                                 pretty straightforward but the problem                                 is if you're using a set in order to do                                 this kind of comparison of whether                                 something's there or not the set starts                                 to get big because you're having to                                 actually store the strings in the set                                 when you're doing it so I wrote a little                                 program I created random you you IDs the                                                                                                         and then I use visual vm to have a look                                 at heap and as we are moving up towards                                 a million uuid is in our set we're                                 looking at a fair amount of he pits                                     meg at that point and if we were for                                 example very popular website and we're                                 receiving millions and millions maybe                                 tens of millions of hits a day if we                                 were trying to keep this all in memory                                 it would suddenly get very very large so                                 this is where bloom filters coming so                                 yes there's a paper at the bottom which                                 is called space-time                                 trade-offs in hash coding with allowable                                 errors and you'll see that it was                                 published a very very long time ago when                                 computers had significantly less memory                                 than they do today so it comes from                                      I wasn't even alive that paper is quite                                 old I wouldn't recommend reading it                                 there's better summaries and hopefully                                 this is one of them and the best way to                                 show you how it works is just by example                                 so we're going to initialize a new bloom                                 filter there we go isn't that cool it's                                 just an array of bits and it's of some                                 size and we'll talk about how you                                 determine the size of it later on but                                 it's an array of bit they're all set to                                 zero that's the first bit the second                                 part is that we have some number of hash                                 functions and all of these hash                                 functions have to be different but                                 there's I'm going to talk about how many                                 you need later on and then what we're                                 going to do is we're going to add an                                 element to our bloom filter so like in                                 that example the first element comes                                 along it goes through the first hash                                 function and it gets mapped to one of                                 the bits in its array and then that bit                                 get set to one and it also goes through                                 the second hash function which maps to a                                 different bit and that gets set to one                                 does that make sense so far nodding good                                 what we're going to do next is we're                                 going to see if something that we didn't                                 put in there is in the bloom filter so                                 this element from the earlier example                                 goes through the first hash function and                                 it goes to one of the bits which is set                                 to one and then it goes through the                                 second hash function which doesn't map                                 to any of the bits that have been set                                 and no matter how many hash functions                                 you have if any of the bits are not set                                 it's definitely not in there when we                                 check to see whether an element that we                                 have put in there is there it goes                                 through those two hash functions like                                 before and it maps to the bits that have                                 been set and in this situation the bloom                                 filter tells you maybe so this is the                                 probabilistic part in that there's a                                 percentage of error that may be                                 occurring here before we look into what                                 the percentage of error is and how it                                 affects the bloom filter let's just show                                 you how you can whack one of these in                                 your code straight away I would                                 recommend using the guava library which                                 is Google's sort of bag of Java tools                                 that has loads of things in it and that                                 was the latest version as of a couple of                                 weeks ago                                 and then I've rewritten that naive                                 example with a set but using one of                                 their bling filters so you see at the                                 top there we've replaced our set with a                                 bloom filter which takes strings and it                                 has a static create method which takes                                 two parameters the first one is this                                 curious thing called a funnel all the                                 funnel is is you're telling the balloon                                 filter what object you are putting in                                 there and then gravel will go and do                                 some hashing for you there are built-in                                 funnels for all of the primitive types                                 like integers and strings and so on if                                 you're going to be putting some of your                                 own objects in there then you can define                                 your own funnel and you say hash on this                                 field in this field in this field and                                 the second parameter there is the number                                 of elements that I'm expecting to put                                 into the bloom filter so it's                                           this particular situation I mean as you                                 can see that's actually wrong because                                 I'm only putting free in so the bloom                                 filter would be too big but just as an                                 example and then liking the other set                                 example we just put those elements in                                 the bloom filter and then instead of a                                 contains method like you get on a set                                 you get a may contain and in this case                                 the first one returns true because we                                 put it in there and the second one                                 returns false because we didn't                                 understandable excellent so what does                                 this do to our measurements well as the                                 number of view IDs went up with a set we                                 obviously had to store every single                                 string in the set so we were getting on                                 for                                                                     bloom filter you can see that the amount                                 of megabyte of heaps being used for this                                 is very very small so even when I was                                 expecting a million elements we're only                                 looking at Norton point nine megabytes                                 in order to do this equality test so                                 that saves a significant amount of space                                 if you just want to do set membership so                                 you're able to suggest the size that the                                 bling filter should be in the parameters                                 and we had                                                            this changes in the bloom filter is the                                 number of bits that you have in that                                 array so the number of bits goes up as                                 the number of things that you want to                                 store goes up so if i wanted to store                                    things it initialized with                                              it was given a million as a suggested                                 input it was                                                         bloom filter which is quite a lot but                                 bits aren't that expensive compared to                                 store in the strings by default in guava                                 you get a three percent false positive                                 probability rate and I abbreviate this                                 to f PP in some of the tables if that                                 and it's a little easier to put on the                                 slides and what that means is that when                                 it tells you that something is there                                 there's a three percent chance that it                                 actually isn't and you have to think                                 about whether that's okay but with the                                 guava implementation what's quite nice                                 is it can take an additional third                                 parameter and you can specify the error                                 rate that you're happy with so if you                                 think that three percent is too                                 inaccurate you can give it one so this                                 case it's a naught point five percent                                 instead of three percent and what this                                 does when you change that is that as you                                 change the false probability rate the                                 number of hash functions that the bloom                                 filter is using increases so with my                                 very accurate bloom filter at the bottom                                 which is                                                            there are                                                            time I insert and read something is                                 being hashed                                                        those are the two design parameters the                                 number of bits for the size and the                                 number of hash functions for the                                 probability and there you go you                                 understand bloom filters wonderful so                                 what are they used in generally caching                                 stuff is a really useful use case                                 there's a paper that I've linked to the                                 bottom there which is from a coma which                                 talked about using bloom filters in                                 order to on high traffic websites the                                 first time that you visit if you're like                                 a what they call a one-hit-wonder serve                                 up a cached version of the page if you                                 are continuing to browse or maybe you're                                 an actual human interacting with the                                 website then using that bloom filter you                                 can check whether they've been there                                 before and you can serve up some more                                 compute heavy content the second one                                 there is quite interesting in HBase and                                 Cassandra bloom filters used to encode                                 the locations of data so that when you                                 scan across the cluster it can skip out                                 places where it knows that there is no                                 data and one thing that we used it for                                 and was in real time matching so people                                 were able to search to specify a whole                                 other people on Twitter who they were                                 interested in getting alerts for and                                 often these lists could be huge like                                 millions of Twitter authors and then we                                 see realize that into a bloom filter                                 into the database and then we could poke                                 it into our storm topology as a bloom                                 filter that we read out so it's quite a                                 nice technique as well if you don't need                                 to know who those people are it's a                                 great one down two to go                                 next we're going to look at countenance                                 sketch and this is a slightly more                                 recent data structure it says not as old                                 as                                                                    name suggests is for tracking the counts                                 so if you're for example interested in                                 reading a stream of data from Twitter                                 and then storing the number of times                                 that you've seen each hashtag that kind                                 of use case then count men sketch is                                 what you want to use let's look at the                                 really naive example first using Java so                                 what I've used here just to keep it                                 concisely as a multiset a multiset                                 really is just a hashmap in the                                 background it's a guava thing so the key                                 of the hash map is the thing that we're                                 tracking so for example the hashtag and                                 then the value is the counter of how                                 many times we've seen it so every time                                 the hashtag would come along it would                                 get added and then incremented the                                 counter certainly sense cool so in this                                 example we use the IP addresses again                                 just just to show we create a multiset                                 at the top which is of string generic                                 type and then we add some things into it                                 the first two elements get added once                                 and then the last element NER gets added                                 twice and then there's a count method                                 that you can call to show you how many                                 times you've seen it that elements at                                 the top there we've seen twice because                                 we added it twice and in the last                                 element                                                                  really simple stuff but like the set                                 example before you'll also see that it                                 starts to use a lot of heap when you're                                 traveling lots of things and the thing                                 that's also annoying about the counting                                 problem is that often if you were                                 looking at streams of data like tweets                                 the tale of these things is huge so the                                 amount of things that have only been                                 tweeted once is often very very big so I                                 did the same thing as before so I                                 generated random new UID and the heap                                 similar kind of profile so as we got                                 towards a million elements in our count                                                                                                         Meg being stored which is quite a lot so                                 if you're looking at for example the                                 Twitter Decker hose which is like                                    million tweets a day and trying to keep                                 this in memory it starts to get really                                 big prohibitively big and then you might                                 want to spill it to a data store but                                 then you've got the time that it takes                                 to do things with that and so on can you                                 keep it in memory you can with kampmann                                 sketch so here's a paper which is quite                                 it's not the original paper the original                                 papers from                                                             summary paper that the original authors                                 wrote which is only a few pages long and                                 it describes it really concisely and                                 it's called can't mean sketch because it                                 does counting excellent it uses the word                                 min because you will see that it uses a                                 minimum function in order to find out                                 what the count is and it's a sketch                                 because it's kind of an approximation so                                 what does it look like and I'll leave                                 this here for just a few seconds it                                 looks like this so remember that the                                 bloom filter wasn't an array of bits                                 which we were using the count mean                                 sketch is an array of counters this time                                 so we want to be able to count things so                                 imagine that they're integers but we                                 don't only just have one row of counters                                 we have multiple rows and when we talk                                 about account mean sketch we say that                                 the width of the sketch is the number of                                 counters that we have on every row and                                 then the depth of the sketch is the                                 number of rows that we have each of                                 these rows if you look on the right hand                                 side has a hash function that's                                 associated with it now it's important                                 that every row has a different hash                                 function that's really important and                                 you'll see why in a minute and it's best                                 to show how it works by example I think                                 so here's what we're going to do we're                                 going to add an element in and I'm also                                 going to shrink the sketch down so it                                 fits on the slide let's add our first                                 element the element comes in and then we                                 go to the first row of counters which                                 has the first hash function associated                                 with it and we hash it and it maps to                                 one of those counters and it is                                 incremented from                                                        second row we also run it through that                                 different hash function and it maps to                                 one of the other counters and it                                 increments from                                                          again on the third row it comes in it                                 maps to the counter over there from the                                 third hash function can it increments                                 from                                                                     let's add the same element again for the                                 second time each of these hash functions                                 will map to exactly the same places and                                 it finds a                                                               ones get incremented to two cool let's                                 add a different element in this time so                                 here's a new element on the first row it                                 maps to that first counter which was                                 zero and it increments it to one and                                 then on the second row                                 I've done here as I've simulated a hash                                 collision taking place so this element                                 on the second row maps to exactly the                                 same counter this time it can't Lee                                 increment from                                                                                                                               wrong but that's ok as you will see in a                                 minute on the third row it maps to a new                                 counter and increments from                                           now we want a query count mean sketch to                                 say how many times have I seen a certain                                 thing for the first thing that we see                                 which is the first element that we added                                 we run through the hash functions again                                 and then we get the values that are at                                 those places first has shrunk from fines                                 to the second hash function is where the                                 collision took place at it finds three                                 the third hash function Maps                                             here's the min part of counting sketches                                 it takes the minimum of those values in                                 order to give you a result so the                                 minimum of two three and two is two                                 which is correct in this case and then                                 if we try and get the value out of the                                 second element that we added on the                                 first row it maps to a one the second                                 row is where it collided and it maps to                                 a three and then on the third row it                                 maps to a one and then we take the                                 minimum of those three values and we get                                 a one so you can see that by the hash                                 functions being different it means you                                 will get collisions but because you have                                 lots of rows hopefully when you go over                                 all of them you'll get one of them that                                 is accurate enough so the thing that's                                 different about the bling filter with                                 counting sketch is that for a human                                 being with the bloomfield so i think                                 about how many things i'm going to put                                 in there and then i'm going to specify                                 that amount and then that's how it works                                 the count being sketch is kind of                                 different because it initializes to a                                 fixed size at the beginning and it never                                 changes and the parameters that you use                                 in order to size it are a bit more                                 awkward to get your head around and                                 here's what they are two parameters the                                 first one is called epsilon which is                                 util and and this is the accepted amount                                 of error that you are ok with being                                 added to each item so how wrong are you                                 ok with accounts being the second one is                                 called delta which is also really                                 helpful and this is the probability that                                 when you receive a result that it's                                 outside of that epsilon value so it's                                 wronger than you are okay with and those                                 are your two parameters                                 and this code is taken from the library                                 that I'm just about to show you if you                                 plug epsilon and Delta into these two                                 equations then that calculates the whip                                 from the depth of the sketch and we'll                                 have a look at how that changes with the                                 different values in a second so you kind                                 of understand it which is good let's use                                 it in some code so i would recommend                                 using the clear spring analytic string                                 library this was written by the folks                                 that add this which I think do buttons                                 and websites where you share stuff and                                 then it counts how many times it's been                                 shared so I assume that they use this                                 stuff quite a lot and I've rewritten the                                 example that we had earlier so replace                                 the multiset with a count mean sketch                                 and it's very easy you're declaring a                                 count min sketch at the top and it's                                 taking three parameters in the                                 constructor here the first one is that                                 epsilon value which is how accurate how                                 acceptable accurate should my counts be                                 so in this particular case I want them                                 to be                                                                   second parameter which is naught point                                                                                                         they be within the bounds that I                                 specified I know that's difficult to get                                 your head around but it makes sense                                 after a while and the third parameter                                 there which is I wish there just                                 overridden it is a random seed I've just                                 put one and then you can add things into                                 the sketch added those first two items                                 once and then I've added the third item                                 twice and then i can call estimate count                                 instead of count and it tells me to for                                 the first one and zero for the second                                 one so with direct comparison of the                                 bloom filter in the heat it's kind of                                 difficult to do because it's it doesn't                                 scale with the number of things that go                                 in there but just to remind ourselves                                 when the multiset had a million elements                                 in it it was                                                             have a look at what tuning the                                 parameters does at the top we've got a                                 counting sketch that's fairly inaccurate                                 I'm accepting ten percent difference in                                 the counts that come out but still                                    percent of the time it will be within                                 that and you'll see that as you tune it                                 to be more and more accurate with the                                 epsilon the Delta increasing the number                                 of counters the width on every row goes                                 up as you'd expect but actually not up                                 by that much it goes from seven to                                    counters per row but where it gets quite                                 big is the depth so in the very accurate                                 count mean sketch of the bottom there                                 are                                    thousand rows of counters which                                 obviously means                                                          but the nice thing is is that the heat                                 usage is very small so you can keep the                                 counts of millions of million                                 developments in about                                                  there if you want something fairly                                 accurate or if you want something that's                                 fairly inaccurate you can get away with                                 barely any heap at all which is much                                 nicer than using the multiset and use                                 cases this any kind of frequency                                 tracking counting stuff is the obvious                                 thing one really nice use case is NLP so                                 I read an example where they took a                                    gigabyte corpus of text and then they                                 funneled it all into account min sketch                                 to do the word frequencies and that fit                                 into about                                                               which then means you can keep it in                                 memory and do stuff with it which is                                 really cool there's two extensions I'm                                 not going to talk about today because I                                 will run out of time but there's a an                                 extension called heavy hitters which                                 uses a heap next to the counting sketch                                 in order to keep the top x of things                                 that you've seen so the obvious use case                                 there is what are the top hashtags right                                 now in my area and there's also an                                 extension called range query which will                                 probably have heard of in solar only                                 seen and I'll leave that as an exercise                                 for you to read about it so good we're                                 two things down is everyone still awake                                 and alive good the last one the more                                 complicated one and I'll apologize                                 upfront for having skimmed over some of                                 the maths in this one but I hope that                                 you'll thank me for it in Susa the one                                 with the silliest name hyper log log                                 who's heard of hyper log log most of the                                 room so hopefully I'll get this right                                 the easiest way to understand it what                                 you tend to do is you go and download                                 the paper and then you open up the paper                                 and then you look at all that crazy                                 maths and then you get very scared and                                 then you put it away and you're going to                                 do something else and so what I'm going                                 to show you this sort of the journey                                 that the authors take towards getting                                 towards high / low blog so what is it                                 forward to begin with it's for                                 cardinality estimation so given some                                 huge list of items what's the                                 cardinality so how many unique items are                                 their we're going to start off as per                                 the other ones with our naive java                                 example and for this I'm just going to                                 reuse the set because the mathematical                                 property of a set is that each item is                                 unique so what going to do is create a                                 new hash set I'm going to stick                                 something                                 in there there's three unique elements                                 there because we add one thing three                                 times we call size which is effectively                                 the cardinality and it says three you                                 wonder how sets work hopefully that                                 makes sense but just to remind you as                                 before when we're using large amounts of                                 things in that set when we get to a                                 million elements we're looking at                                 hundreds of megabytes of heat being used                                 and if you all for example computing the                                 daily visitors of a top website like                                 LinkedIn or Google or something you can                                 see that doing this cardinalis and                                 stuffing memory is actually quite a                                 challenge because you have to hang on to                                 all of it while you're doing it so we're                                 going to do now is a very gentle walk                                 into hyper log log so it's an iteration                                 of an iteration of an iteration of                                 different techniques and if you see the                                 progression it's I think easier to                                 getting into intuition as to how it                                 works the actual implementation is still                                 quite complicated but I'm hoping that it                                 will have some kind of impact by going                                 for it today we're going to start off                                 with something called linear counting so                                 this is from a paper in                                               called a linear time probabilistic                                 counting algorithm for database                                 applications the application they're                                 talking about here is if I have a                                 massive database table how do I know how                                 big it is without having to actually                                 scan everything they use this technique                                 that looks a little bit like a bloom                                 filter so here we have an array of bits                                 we had an array of bits in our bloom                                 filter it's of some particular size                                 we're going to call it size n then what                                 we're going to do is have one hash                                 function just one and we're going to                                 insert an element we run it through our                                 hash function and it maps to one of the                                 bits and it sets it to one and then                                 we're going to add another element and                                 it's going to nap to one of the bits and                                 we're going to sell it to one and then                                 what we're going to do is an equation                                 and I'll explain what this equation is                                 so the variables in this equation M is                                 the width of our linear counter that                                 means how many bits do we have and then                                 W is the weight of our mask which is                                 just the number of ones that we have and                                 then L n is the natural logarithm so if                                 you plug in this you get minus ten times                                 the natural logarithm of ten minus two                                 divided by                                                       estimation of the cardinality the                                 cardinality is too so it's not far off                                 however the very                                 it's on this is really really big and it                                 gets quite unwieldy at large amounts but                                 there's an intuition there that you can                                 record something and then do an equation                                 in order to estimate so next we're going                                 to move on to log log and this is much                                 more powerful but it's much more                                 complicated and we're going to look at                                 the multiple steps that we used to get                                 there first off we're going to have an                                 intuition of flipping coins so                                 everyone's flips a coin before fifty                                 percent chance that you get ahead fifty                                 percent chance that you get a tail now                                 if I got five heads in a row how many                                 times do you think I flipped the coin                                 maybe twenty something like that if I                                 got a hundred heads in a row how many                                 times would I've been flipping coins for                                 thousands if I got ten thousand heads in                                 a row I've probably been flipping the                                 coin for a very very very long time the                                 idea is the more things you get in a row                                 the less likely is that it would have                                 happened and the longer I would have                                 been doing that particular thing so with                                 that intuition in mind of rows of heads                                 let's use hashing so let's take an                                 element let's hash it and then get the                                 binary representation and then let's                                 count the number of zeros that it begins                                 with this particular binary                                 representation starts with the one so we                                 have no leading zeros whatsoever so we                                 record a zero second one let's hash it                                 same hash function and then record the                                 number of leading zeros which is in this                                 case one because it starts with                                        then another element and so on and                                 record the number of leading zeros now                                 what if we could use that intuition that                                 it's more rare to get lots of leading                                 zeros in a row and plug that into an                                 equation we could do something really                                 really naive like                                                       maximum number of leading zeros that we                                 had to estimate so                                                   maximum number of leading zeros which is                                                                                                         the estimate is too but that's wrong                                 because it's three but he's just another                                 step towards getting to where they're                                 going so the author's then show how you                                 can take that intuition and you can                                 improve it and one way that you can                                 improve it is to do lots and lots and                                 lots of different hashes i'm talking                                 like many many many different hashes and                                 then you can take the average of all the                                 according that you've seen in order to                                 give you something that's slightly more                                 accurate but the problem is as you throw                                 all these hashes at it it starts to get                                 really expensive in a bit clunky so is                                 there something more clever that you can                                 do and the authors show that there is                                 and there's this thing called stochastic                                 averaging which I'm not going to go into                                 the maps but I'll give you an intuition                                 what you can do is you can get away with                                 just using one hash function on the                                 right hand side you declare a number of                                 buckets and this is very similar to                                 count mean sketch they're just an array                                 of counters and those counters are where                                 you're going to store the number of                                 leading zeros that you've seen and what                                 you do is when you hash it once you slip                                 off the prefix of the hash and I've just                                 used for as an example it's different in                                 the real thing though and that prefix                                 maps to one of the buckets and then the                                 remaining part of the hash you count the                                 leading zeros and then in that bucket if                                 the number of leavings areas that you                                 have is greater than or equal to the one                                 that's already in there you record it I                                 hope that makes sense but you take the                                 first bit nuts to a bucket record the                                 leading zeros of the last bit and then                                 you have a fixed number of counters that                                 you are using and then the authors do                                 something even more magic which is they                                 show that given that you two and you                                 raise it to the power of the sum of the                                 maximum zeros that you seem divided by                                 the number of buckets that you have                                 which is fixed multiplied by the number                                 of buckets x magic number and if you                                 ever see an implementation of this you                                 will see a massive array of magic                                 numbers and what these are a statistical                                 analysis that they do to show that a                                 different sizes of estimates if you                                 multiply it by a magic number it nudges                                 the estimate in the right direction                                 wonderful but i'm not going to show you                                 the magic numbers but from all of this                                 stuff they show that they can get an                                 average error in the cardinality                                 estimate of                                                           root of the number of buckets and they                                 also say that with                                                       enough counters to do estimates of                                 billions of elements which means you get                                 a four percent cardinality in accuracy                                 but the nice thing is with a                                      buckets you'll need five bits per bucket                                 in order to do this which means that you                                 can do the hypha log log in about                                      which is absolutely nothing but actually                                 that's not the whole thing so there's a                                 hyper Lord paper which improves it even                                 further and what the authors do now and                                 it's called near-optimal in this case is                                 that when they get the counts of the                                 leading zeros they throw away the                                 largest thirty percent of recordings                                 that they have and they leave the                                 remaining seventy percent and just by                                 doing that because the sketcher always                                 tends to                                                              get there are down to this now which is                                                                                                    buckets so if we plug in a thousand and                                 twenty four buckets we get                                 three-point-two percent in accuracy and                                 then they can't even stop there they go                                 one step further and they say instead of                                 doing a regular average in the power of                                 two they do a harmonic mean instead                                 which is a slightly different way of                                 calculating an average and it goes down                                 to three percent so that's an intuition                                 of how it works it's quite magic but                                 it's also very cool and if you're really                                 good at math you'll really enjoy the                                 paper if you want to use it in your own                                 code you don't have to do any of that                                 stuff there is a same library that we                                 used before for Kampmann sketch has                                 hyper log log in it clearspring analyst                                 extreme I've rewritten the example from                                 earlier we create a new hyper log log at                                 the top we give it the percentage of                                 accuracy that we're interested in we add                                 some elements in and then we call                                 cardinality and there you go heap size                                 brilliant so if we're doing that                                 cardinality estimate with a set hundreds                                 of Meg's of heap if we're using hyper                                 log log nothing so if you can imagine if                                 you have lots of these in a storm's                                 apology you're not going to be putting                                 any pressure on your workers at all use                                 cases for that constant time cardinality                                 estimate for example unique site                                 visitors estimates of massive tables                                 because you could either do two things                                 you can either have a running hyper log                                 log counter that you keep all the time                                 and you keep adding to it or you could                                 just use it when you say do a big batch                                 job and you stream out loads of data and                                 you just count on the fly and there you                                 go that is it so we've gone through                                 three probabilistic data structures I                                 hope that made sense and we also had a                                 look at some code before and after to                                 show you that there's libraries out                                 there that make it really easy to use                                 this stuff I almost choked at the                                 beginning which is excellent and thank                                 you very much for listening just to                                 recap bloom filters are for set                                 membership good for caching Kampmann                                 sketch                                 good for counting how many times you've                                 seen something and hyper Lord Lord is an                                 interesting story and its really good                                 for cardinality estimates and that is it                                 thank you very much for listening okay                                 thank you thank you James can we unmute                                 this mic tell me what                                                 thank you James you started to meet                                 ahead of time and finish two minutes                                 ahead of time to give two more minutes                                 for Q&A any questions in the room this                                 is where I really scared regarding the                                 bloom filters have you compared to them                                 performance and memory consumption these                                 are cuckoo filters for example because                                 they have they own advantages sorry can                                 you say once more coo coo filters yes                                 because the main problem of the bloom                                 filters you could not remove the data                                 from the filters are oh wait that's the                                 really tricky thing said the particular                                 use case that we used it for it was                                 absolutely fine not to have that data in                                 memory because we didn't need to iterate                                 for it later but obviously if you do                                 need to iterate iterate for it later                                 then that's the trade-off that you have                                 so i don't know maybe for example you                                 could have a bloom filter but then store                                 the data in a solar elastic search index                                 for a look up when you need to but yeah                                 it's use case specific i hope that                                 answers your question maybe it didn't                                 yeah okay basically a zero-sum tasks                                 when you need to remove the data from                                 there are fish for example yes clear now                                 that's a really really important point                                 so not only can you not get the data out                                 as in if you had a set you could remove                                 the data again when you put stuff into a                                 bloom filter it's very very difficult to                                 remove it if not impossible so there are                                 extensions to bloom filters that can do                                 these things but having looked at some                                 of the papers it all just gets really                                 complicated so you do have to think if                                 you are keeping a load of bloom filters                                 in memory                                 particular data sets what happens when                                 that data set changes to say if you have                                 a cache that invalidates then you have                                 to reload the ho bling filter again and                                 that's definitely some it's bear in mind                                 thank you for bringing up but I haven't                                 got any good advice there ok ok next                                 sticky question it's not so much a                                 question as that I just want it in the                                 intervention that might be clear to                                 everyone but has always helped me                                  bowhunters I think are different from a                                  lot of probabilistic algorithms in that                                  one of the trick questions the no                                  question is always right so you can add                                  or use bone filters in a lot of                                  situations where where if you get an                                  answer yes you will look up anyway but                                  if you get an answer no then you save a                                  lot of work and you can do it without                                  any probabilistic trade-offs yeah that's                                  a really really good point and I should                                  have said that in the talk so and if you                                  were using a balloon filter as part of a                                  loading cash or something and then                                  that's really good because you have a                                  hundred percent guaranteed correct No so                                  yes and if that filters out most of your                                  requests and then when you do get a yes                                  you do something else then that's that's                                  a really good use of it thanks for                                  bringing up I think I have a question                                  related to a hyper lock lock in order in                                  the MapReduce environment is there any                                  chance to do calculations in each member                                  or each reducer and combine them                                  afterwards so especially for unique use                                  our calculations where don't want to                                  salt first by user that's a really good                                  question and I don't know the answer I                                  know that for example with bloom filters                                  you can do operations to kind of combine                                  them and but I'm not sure if the same is                                  true with hyper log look maybe somebody                                  else may know the answer I guess if you                                  do lots of separate hyper load logs and                                  then combine them it just gets more and                                  more and more inaccurate but I don't                                  like sorry any more questions or                                  comments just please raise your hand                                  one I wanted to answer the previous                                  question the Clear Springs have an                                  article on how to combine hyper law                                  clerks with different even if they have                                  different accuracy okay great co-op look                                  at I thank you anyone else going once at                                  the front twice so you can increase                                  accuracy of the idle resins by adding                                  more hash functions of the does this                                  have a notable mmm an impact on the run                                  time or does it not matter so the hash                                  functions don't need to be really                                  complicated they don't need to be like                                  cryptographic level hashes or anything                                  they use a technique which i think is                                  called pairwise hashing where you                                  actually just have one hash function but                                  is parametrized so for example in the                                  count win sketch some of the inputs to                                  the hash function would be the row that                                  you're on and then it can do some like                                  modulo arithmetic so as far as I know as                                  long as they're all constant time it                                  should be okay and but I can't give you                                  any good proof ok thanks ok any other                                  hand I don't see I think that's it ok                                  thanks James chance everyone
YouTube URL: https://www.youtube.com/watch?v=NLXjsMS7uBM


