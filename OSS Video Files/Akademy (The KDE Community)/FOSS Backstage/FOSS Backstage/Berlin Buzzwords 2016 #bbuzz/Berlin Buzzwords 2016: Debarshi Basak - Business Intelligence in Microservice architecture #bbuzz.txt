Title: Berlin Buzzwords 2016: Debarshi Basak - Business Intelligence in Microservice architecture #bbuzz
Publication date: 2016-06-11
Playlist: Berlin Buzzwords 2016 #bbuzz
Description: 
	Bol.com is the largest online retailer in the Netherlands and Belgium and is still growing at a staggering rate. Bol.com is a fact based decision making company hence business intelligence plays a key role in the organization. For gathering business intelligence (BI) insights we have a team of twelve dedicated BI engineers. 

However, it is not just the scale of data, but also the complexity of these services that can makes it difficult to gather all the business insights. The growth of data, users and complexity have forced our team of to rethink our traditional relational data warehouse structure. Currently we are moving towards a more hybrid BI solution based on Microservices at big data scale, using Hadoop, Hbase, relational databases, etc.  

In this presentation, we will discuss key concepts that govern our datawarehousing, the unique challenges we have faced and are still facing with petabytes of data along with design decisions, tooling landscape and architectural choices. We have created and explored various toolings and concepts for solving our problems.  

We discuss concepts pertaining to Data quality at big data scale, large scale batch scheduling, job monitoring, ETL processing, reporting, continuous deployment, shift in mindset of traditional datawarehouse developers and team autonomy for BI at scale. Essentially, all the hacks to get our systems towards awesomeness. 

The presentation will guide audience through transformation of legacy (vintage) BI towards more scalable BI setup as well as mental and techincal block that came along the way. We believe it is important to educate impact of microservices on datawarehouse and we have learned how we could do that at petabyte scale.

Read more:
https://2016.berlinbuzzwords.de/session/business-intelligence-scale-microservice-architecture

About Debarshi Basak:
https://2016.berlinbuzzwords.de/users/debarshi-basak

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              awesome so I hope you guys can hear me                               yeah so my name is Dabashi know and I'm                               gonna talk about business intelligence                               and Microsoft's architecture I mean so                               uh boon comm is the company I work for                               and it's it's actually going to a                               transition right now and we are moving                               towards micro services so I thought we                               we are kind of doing some cool stuff we                                think it's cool but i thought we I would                                share with you guys and let you guys                                decide so what can you expect from this                                presentation well I'll give you a brief                                introduction about me and my company                                well and it run you through the history                                of business intelligence that was that                                was happening in Volcom since the                                inception of the company and and we have                                in my opinion they're like two eras have                                a good that's going to in in Bulacan one                                is the map we'd use face and now it's                                more streaming and and how we do bi with                                that kind of stuff it's fairly new                                fairly untested but I thought I would                                share some insights and some operational                                stuff about how we manage our                                infrastructure so but me                                so I'm Dabashi I'm a software engineer                                Volcom so I have twin responsibility at                                goal outcomes I'm part of a big data                                platform team that's I'm responsible                                it's a DevOps team that builds tools and                                and other kind of infrastructure stuff                                for Hadoop and and second iam also part                                of us CA CR Co team which makes Google                                Google related optimizations and stuff                                like that                                yeah so uh and about Bolcom to give you                                a context so it's it's an so so it's                                it's an e-commerce platform in                                Netherlands fairly popular well so the                                the companies or a company is one of the                                few first implementations of scrum in                                Netherlands and we have over a thousand                                employees actually over                                           employees in IT and over                                               and it's fairly young the average age is                                around                                                               and we have a motor that we built you                                build you run it and you love it what                                you build so so and to give you a idea                                how big the size of the data is so these                                are some metrics about the actual                                functional data but I mean in terms of                                the plaster we have a fairly small                                cluster of                                                              we have around                                                       you know monthly Gurgaon                                         I got unique jobs running in the cluster                                so it's about nodes RVD now has really                                high capacity so - totally it's around                                                                                                      in our cluster so that's pretty much                                about what what's happening so but                                today's focus is about micro services                                and how you can do bi in it it's it's so                                before we good at the topic so what is                                micro service I mean if you see this                                thing this this this is a called the                                dead star architecture it's like yeah I                                hope this I'm assuming that's all war                                fans yeah so I yeah so I mean in the in                                the cool of this dead star is actually                                something called dependency hell so it's                                actually a thing so this is the IP is                                really badly for the shop but I mean I                                picked it up from from a presentation                                it's an actual thing so what happens is                                in micro service often you if you pick                                up any presentation view of often have                                Amman                                where you which is probably Java or will                                be up and then you have a database and                                there's some magic happening and you get                                some services out of it which I like so                                there's actually a science behind it so                                you I'm not really sure but you find you                                have dependencies and you try to figure                                out cohesion between the dependencies                                and then you decouple them together but                                you basically build services out of your                                existing will take on probably new                                services or something so that's that's                                pretty much what micro service is I well                                just for the context and the second part                                of the presentation is about business                                intelligence um for those who are fairly                                new in this field so business                                intelligence is about analyzing data                                 sets and collecting actionable items for                                 your stakeholders yeah basically you you                                 want some KPIs or for your stakeholders                                 to understand how your business is doing                                 as essentially so some of the                                 requirements that often an organization                                 has is that all this data should be                                 collected continuously and in an                                 automatic fashion and often have                                 different kind of data sources like flat                                 files and xml and Excel she's internal                                 databases FTP HTTP the biggest kind of                                 sources so and once you collected them                                 and you have transformed them the                                 analytics that you can do on this data                                 set should be flexible so these are                                 often the requirements of an                                 organization - so it's the business                                 intelligence it's not really a new field                                 to be honest we already have techniques                                 like ETL which extracts data from know                                 which basically means extracting data                                 from different sources we have                                 transformation lag and you have these                                 architectures exist and then you'd load                                 it to a target data model and target                                 data model have something like kimbos                                 our data modeling techniques like a star                                 schema or snowflake schema and more                                 advanced will be OLAP cubes which are                                 like derivative of of star schema to                                 give you an idea about that                                 so on my so                                 on my left left are you this is more of                                 a star schema so the granularity of your                                 data which is the thing in the middle is                                 called fact which is the lowest lower                                 most granular data which has the                                 references to all this dimensional data                                 of facts are basically mutations and you                                 have dimensional data which is the                                 actual when you combine them together                                 you get the actual context of your data                                 and more advanced is basically the cubes                                 which is basically you aggregate your                                 data for different dimensions so you can                                 actually drill down and drill in easily                                 and in if you have standard to length                                 not sunk tooling but yeah if you but if                                 you have if you buy to so it has a                                 different way of querying so you use MDX                                 to query will up cubes I'm not the best                                 person to give you idea about this but I                                 mean it's it's a fairly open data you                                 can find it's been there for a long time                                 so it's kind of the idea behind it so in                                 in a monolithic days back in I think                                 early around                                                           to have this so we had online we Cibola                                 come as a webshop so we had online                                 systems and if a marketing wants to make                                 a campaign the marketing person would                                 actually query the online systems make a                                 report or out of it doesn't matter you                                 know and then do stuff but at one point                                 you know it doesn't really work it                                 affects the actual webshop and at that                                 point they started thinking of actually                                 decoupling your bi system and well at                                 that point they hired a lot of                                 consultants and architects and they came                                 up with the architecture of called the                                 data hub architecture hub hub and spoke                                 architecture and bi so you actually have                                 a data as actually replicated to a data                                 hub so you have a materialized view with                                 a remote link to your online systems and                                 you would request a refresh your                                 materialized view every                                               maybe daily                                 no it depends upon how often you want                                 the data and from the data                                 that's that step is called application                                 and from our deed hub to the data                                 warehouse you would have under the                                 materialized view that would sink your                                 with the data hub every                                                  depends or how often you want the data                                 that step is called replication do but                                 and then you would so once the data is                                 available in the data warehouse you                                 apply your ETL kind of logic you know                                 and you would get your target data model                                 but your end users would be put on so it                                 implementation was fairly easy you would                                 buy in some existing prepared                                 proprietary stuff from some user support                                 database provider complexities are                                 abstracted you don't have to care about                                 refreshing data really I mean and but                                 then there are data overheads and of                                 course there's latency you know because                                 of the refreshing happening across the                                 systems so so some of the business guys                                 said I want data I want it now so so we                                 had we still had the online systems so                                 but then we had the message queues in                                 place so the online systems would                                 publish messages in the in the broker                                 and we would have in the data west side                                 we would have listeners which were                                 written in stored procedures back in                                 those days                                 mm yeah so they would schedule this                                 listeners and this would pull in data                                 from from message brokers you know so                                 the problem is you know if well in our                                 case loss of messages because of the                                 tooling that we used was not was                                 messages by guaranteed but it it the                                 thing is you can have in this                                 architecture you can have lots of                                 messages if databases are not really                                 kind of made for this directly and                                 implementation is complicated in fact in                                 certain places they actually had absurd                                 for every message coming in on the                                 target table so that's an implementing                                 it if you don't understand what you are                                 doing it you can end up making really                                 bad software                                 in and then you have a nightmare for                                 operations no because if message broker                                 goes down you have a lot of different                                 things happening so so that's it so that                                 was okay no but when we move towards                                 micro-services architecture the things                                 just magnified all the problems                                 magnified and what we saw was there were                                 too many just too many sources to                                 collect data form within the                                 organization all the internal sources                                 exploded it affects the stability of                                 reports because you have too many                                 dependencies to cadena collect the data                                 form and now what we had was we had a                                 team of seven or eight people now who                                 they had to do collect data from three                                 hundred services and just the BI team                                 doesn't scale as much and then you have                                 to on top with that you have to apply                                 all these ETL log logic for most of the                                 data and and then you have service                                 concatenation that you have to do and it                                 just is too much for a team of few                                 people so at that point we decided to                                 move to Hadoop because as the services                                 were scaling the data was growing and we                                 had to do something about it                                 so you at Budokan we have we finally                                 have a good experience with Hadoop we                                 have been running it in production for                                                                                                          things can go wrong and we know how to                                 fix it so some of the jobs like a                                 commander was working it's really bad in                                 terms of the amount of data processes so                                 we know what kind of failures we can                                 expect and also in that point we also                                 started defining how our service looks                                 like so at this point we made a decision                                 of a conscious decision of how we can                                 make bi even easier even though if we                                 scale out from services things so we                                 defined how a service should look like                                 so a common thing in a service or micro                                 service-oriented architecture is is                                 having RPC over HTTP like a guess                                 service or something and                                 message queues but what we introduced                                 was the idea of bulk interfaces so it is                                 it is the philosophy behind it is that                                 you should be able to transfer huge                                 loads of data across services so so                                 think of a scenario where you have to do                                 an initial load on some service no you                                 you don't really want to do that on                                 we're using HTTP no you what should be                                 able to do that directly to another way                                 so that's what we decided to build so                                 bulk interfaces are in our                                 infrastructure it's basic it's a                                 philosophy but in our infrastructure is                                 basically an HBase table it's a                                 key/value think so think about a think                                 of it as a key it's a key value entity                                 but with the key composition consists of                                 a time component in it Center tower and                                 the and functional key component in it                                 so the time is prepended yeah time is                                 prepended to the function key and when                                 you replay all the events because it has                                 time you know and you can scan certain                                 you can have a sliding window in your                                 data set and if you replay all the                                 events you get the latest state of your                                 of your data essentially so in this case                                 if you replay the time at times and you                                 can actually get the state of different                                 way dodged a latest state of different                                 wear assets it's also it's is fairly                                 inspired by event sourcing pattern it's                                 a design pattern essentially which does                                 something like this and the key design                                 is inspired by open TS DB open BSD B has                                 a similar key design but it has a metric                                 name in front of it something like that                                 so we we used this kind of principle the                                 idea and the second part is that the                                 data here is kind of immutable so you                                 just prepend                                 data so in this way you yeah you can                                 collect if you have seen a presentation                                 about turning databases inside out it                                 talks about it basically looks like a we                                 do login                                 databases are my sequel bin in the my                                 sequel databases if so so that's the                                 principle behind bulk interfaces so so                                 once we have this the source systems                                 this is the way services I cannot share                                 data with us so we started thinking of                                 how we can now we think about bi in in                                 in Hadoop so so these are the essential                                 steps so let's let's assume we have                                 three services what because we have the                                 data in this fashion we can do Delta                                 processing easily so it's a it's a key                                 concept and an ETL so you don't want to                                 take the complete data set every time                                 and and and processor so you want to                                 have a sliding window where you go from                                 time but do you want to t T X and you                                 just want to only that slice of the data                                 so we so the way we get this data is why                                 accuse into the HBase tables and then so                                 the services and Hassim has there is a                                 mapping layer called tooling custom                                 tooling within our infrastructure called                                 Eddy                                 so it takes so it so all the services                                 actually send an XML data or adjacent                                 data it can be any data format and Eddie                                 translates it into an insert statement                                 in HBase and puts it so teams do not                                 have to deal with or with HPS directly                                 so you can if you are actually doing it                                 you can also use the yes service from                                 from MySpace if you don't know do not                                 want deal with the the intervals of                                 HBase as a team so we insert the data                                 into all the service and on all the                                 service we have Delta processing                                 happening and so what we use as we use                                 we have big scripts written which which                                 do Delta processing on on these Deek on                                 these HBase tables and we have big this                                 is fair and then we have a scoop job                                 which inserts data into our data                                 warehouse when into our models all the                                 transformations are happening there and                                 actually to be honest we do not do it                                 any transformations                                 so the whole idea of transformations of                                 question                                 back to the services because we are only                                 stalking the states so the only so we                                 don't we all we do is we do Delta                                 processing on the data collect them                                 aggregate them and then push it back to                                 data warehouse so this is fairly the                                 Commons architecture if you ask your                                 people who have been doing data                                 warehouse in Hadoop but there is a                                 problem here right I mean no the problem                                 is at the scoop and that's a bottleneck                                 if you want to move towards more                                 self-service bi it's it's it's fairly                                 difficult to do that and kind of this                                 kind of scenario so yeah so we have a                                 tooling so we have one DAC for                                 scheduling these jobs we have Chronicle                                 for the database side yeah so uh so                                 before I so first of all there are two                                 problems here there's one that's                                 basically the bottleneck that can happen                                 at the scoop and because of its going to                                 a single Amanat data data warehouse and                                 the second part is that you have to                                 guide big scripts for all these all                                 these services that you have to collect                                 data from so first we try to solve that                                 basically what we did was we try to we                                 invent a bi integration unit so the                                 thing is once you push the                                 transformation to the services all you                                 have to do is do a very dumb thing where                                 you have to take the events and we play                                 all the events and just push the data so                                 I mean this is a very common pattern                                 across all the all the services so which                                 we thought we can actually automate this                                 thing now we shouldn't be writing                                 pictures for all the all the sources so                                 what we did was we thought what are the                                 use case what are the things that we                                 need one is we need an equation job for                                 every service and we need a way to                                 concatenate all the services so on one                                 of many keys so we got we because I                                 purely did not have the dev ops                                 background but we took an inspiration                                 from puppet so this is basically a                                 puppet ice-t-- way of add                                 service into the whole infrastructure so                                 we we have an H base table we map it to                                 an Oracle table and what this script                                 essentially does we have a tooling which                                 actually generates a big script all the                                 monitoring I'll on that and and and                                 scoop script and it it pushes data to                                 the the warehouse so you basically also                                 have types from from that's required so                                 if something fails it gets notified and                                 things like that so in this way it's                                 pretty tightly chained with the whole                                 infrastructure it's also get scheduled                                 also you can also specify the schedule                                 that you want and and so that's that's                                 one thing no we didn't do not want to                                 collect data from every service but big                                 fiction so we started generating them so                                 you would just plug your server sent to                                 the data warehouse like this but then we                                 still have now we have two problems one                                 is the bottleneck yes the second one is                                 that everything is batch so there is                                 still latency you know there is still a                                 certain amount of latency but because                                 MapReduce for example is optimized for                                 high throughput so every time you got                                 the job                                 it's it takes a lot of it takes certain                                 amount of containers in in your cluster                                 and and it depends upon the how much                                 data you have but still you it's a it's                                 optimized for high throughput you don't                                 want to do that every every                                            or every                                                               how small can you make your batch no so                                 that's that's the problem it's bad thing                                 and then you have all these pipelines it                                 takes time to get the data into the we                                 put and since we are in e-commerce                                 platform we we really want to be on top                                 of the of the curve we really want to be                                 half the data and analysis right away so                                 we said we realized that actually data                                 is a synchronous every data is as as                                 almost every sources that we had in our                                 system is a synchronous plates are                                 synchronous orders                                 are synchronous offers are synchronous                                 so in fact a batch is is actually a                                 stream know so its stream missus                                 essentially you have a point star point                                 and you have your endpoint is infinity                                 and batch is more of a bounded stream so                                 so we decided to move to the streaming a                                 Ghana with flank and with flank what we                                 had was the entry of bagger is low your                                 code is smaller and and you have really                                 nice java functional api's to do that                                 and we kind of have operational                                 experience now because we have been                                 trying it out a lot so we decided to                                 actually experiment with our things but                                 one thing that we realized is that you                                 don't really need cues for swimming or                                 you don't always need accuse cues for                                 streaming you can use actually you can                                 use edge base tables for streaming to                                 know its stream you can make streams out                                 of files too so what we started thinking                                 was so let's say we have a given because                                 we have this immense source pattern so                                 let's say we have a start time and then                                 you can with HBase you can actually ask                                 for give me next X records no it can be                                                                                                          and with what would by to give me X                                 records you keep getting a small batches                                 of it costs back and your in this way                                 you can build a stream out of it the                                 other way to do that is basically you                                 say you give start to an end or to your                                 so you know in your sourcing component                                 and you can ask for give me a start time                                 and time give me all the great cost from                                 this thing so in this way you can                                 actually stream your edge base tables                                 all of a sudden so so let's take an                                 example for example we have offers and                                 we have product catalog so you want to                                 see let's say for product category X how                                 many offers are there or what is the                                 best offer for this product with some                                 description so what you would do is for                                 given product ID                                 you would start streaming them so                                 because the Buddhist teams are                                 independent of each other also offers is                                 calculating best offer for this Product                                 ID while credit cat years is kind of a                                 slow because it's a lot of data so it it                                 eventually enters and on the on the we                                 have a sink where we have the table has                                 Kia's Product ID and everything else so                                 it looks like a star schema so you can                                 add more things with product IDs key so                                 it kind of becomes like a star schema                                 and and it's an H base table and you                                 apply platform which is a new tooling                                 that we are using for self-service bi on                                 on HBase so it kind of you can actually                                 start using this stock on platform right                                 away so we have been thinking how we can                                 automate this and what we ended up doing                                 is a wrapper around                                 flink so you can actually say from this                                 table on certain key you can and you can                                 look up while you are streaming and you                                 sink to certain multiple tables so                                 that's that's one of the things that we                                 did and finally we have been because                                 flink is a streaming application we do                                 not deploy it like Hadoop so what we do                                 is we you know a maven                                 we have built a maven project where                                 which it builds into a docker image and                                 we push it to a docker registry and                                 while deploying it pulls in the docker                                 the latest or the version of docker                                 image we tag it with the the version                                 number and it pulls it and pushes to                                 mesos and starts the flink client on on                                 Messer's and submits the job and in                                 hadoop yarn and yeah so one thing that                                 we actually learned I was basically you                                 do need a dedicated team for doing                                 innovations of these sorts because it it                                 takes a four                                 come team is too much to do these things                                 and the one thing you have to think                                 about this is not tools but how to solve                                 problems it's it's kind of a hipster way                                 of solving bi problem but and the other                                 thing that we saw was Flint can be                                 flinky because we you have to figure out                                 a lot of things yourself and and there                                 are a lot of issues well if you have                                 attended nice presentation there's an                                 issue with Kerberos ticket and stuff and                                 there's a lot of frameworks out there                                 and when we started doing this bi thing                                 on Hadoop well we there was there was no                                 khylin back in those days around                                                                                                                  which which i think is if you can start                                 with that you should definitely look at                                 it and and yeah and also like a lot of                                 VI and hadoop developers don't think                                 about infrastructure they take it for                                 guarantee I mean I would say you expect                                 the infrastructure for ado and yeah and                                 that's pretty much it yeah thank you                                 thank you very much okay we do have time                                 for questions                                 does anybody have any burning question                                 could you give me a sense of like the so                                 you sat back in the beginning the                                 problem is that you have batches and the                                 batches can be very large right so there                                 is a big delay proportionally so just                                 just like orders of magnitude you                                 managed to like improve this delay like                                 how much quick because the pipeline                                 seems a lot more complicated yeah but is                                 it is it like at which point is it worth                                 it that's amazing which dealer you're                                 talking about I mean well you said in                                 the beginning that like from your from                                 your offer or from your application to                                 the data warehouse they were like two                                 batches yes yes yeah which take a long                                 time but now you kind of removed them so                                 like how much quicker is the data flow                                 so for example like in previously                                 previously before I joined teamed there                                 with certain jobs that did self join                                 with themselves and to calculate the                                 best offers and they were really bad no                                 some of the times if the offers would do                                 an initial load they would take a day to                                 process and and with these it's it's it                                 would definitely finish within the Nog                                 or something no I would like to ask how                                 did you manage the dependency while                                 moving through microservices what                                 dependency with other so are these yes I                                 mean we haven't moved completely and and                                 that's always a discussion it's actually                                 an organizational problem then the thing                                 so I mean the way they they do it as you                                 look at the common dependencies together                                 and you try to make a service out of it                                 so that it becomes an independent                                 deployable unit on on your                                 infrastructure but I mean it's it's                                 always a discussion I mean what is the                                 how do you take that out of that so that                                 I don't know if that answered your                                 question so so that's that's how we do                                 it now we try to figure out how we can                                 independently deploy certain part of                                 application for example you have to                                 check out a fulfillment or something so                                 what are the services required for                                 for certain things and you deploy it                                 that way so that you can continuously                                 deploy the application so are you using                                 it in production for historical data or                                 just for for kind of very recent data                                 like one month yeah we have historical                                 data okay so if you are using it for                                 historical data how do you want though                                 later ieaving facts and later arrives or                                 emissions so it eventually becomes                                 consistent and if you look at the HBS                                 kind of with the plink architecture it                                 eventually becomes consistent so it's so                                 that's that's how we solve it I don't                                 know if that answered your question                                 yes it does so it means that at the                                 point you have to fully represents the                                 entire data yeah anybody else you're                                 using missiles in one of the last slides                                 on which scale for some services or                                 generally so I'm not sure if I can give                                 you a complete idea but I mean I think                                 it's around is we are going to                                 production with meso soon so and the                                 scale is around it's a small scale at                                 Lake around five to ten machines but I                                 mean we have it's going to be part of                                 our of our core infrastructure yeah okay                                 thank you very much let's thank the                                 speaker again
YouTube URL: https://www.youtube.com/watch?v=0FT8EB9gQoA


