Title: Berlin Buzzwords 2016: Robert Metzger - A Data Streaming Architecture with Apache Flink #bbuzz
Publication date: 2016-06-11
Playlist: Berlin Buzzwords 2016 #bbuzz
Description: 
	Data Streaming is emerging as a new and increasingly popular architectural pattern for the data infrastructure. Data streaming architectures embrace the fact that data in practice never has the form of static data sets, but is continuously produced as streams of events over time. Moving away from centralized "state of the world" databases and warehouses, the applications work directly on the streams of events and on application-specific local state that is an aggregate of the history of events.

Among the many disruptive promises of streaming architectures are:
- decreased latency from signal to decision
- a unified way of handling real-time and historic data processing
- time travel queries
- simple versioning of applications and their state (think git update/rollback)
- simplification of data processing stack.

This talk introduces the data streaming architecture paradigm, and shows how to build an exemplary set of simple but representative applications using the open source systems Apache Flink and Apache Kafka. Delivered by the creators of the Apache Flink framework, the talk explains the building blocks of data streaming applications, including:
- event stream logs
- transformations and windows
- working with time
- application state and consistency

Read more:
https://2016.berlinbuzzwords.de/session/data-streaming-architecture-apache-flink

About Robert Metzger:
https://2016.berlinbuzzwords.de/users/robert-metzger

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              thanks a lot for the nice introduction                               um hoga so I'm going to talk about                               apache fling today and my name is Robert                               Metzker I am a committee mpm CMM of the                               Pesci fling project and I'm also working                               at data artisans here in Berlin a                               company that has been founded by some of                               the creators of Apache fling and and                               today's talk is going to introduce you                                to a pet chief link but i would like to                                start first with my take on the stream                                processing space so why i think that                                streaming is a really big thing so if                                you look at the program of this                                conference you also saw that a lot of                                talks are about streaming and i'm giving                                my personal view where I think that                                streaming is such a big thing and then I                                will do a little introduction to fling                                and and then I will take one very common                                batch use case and review how we can                                transform this batch use case into a                                streaming use case to show you how you                                can run existing batch workloads with a                                streaming engine and also what new use                                cases or new business values you can get                                out of a streaming system and after that                                I will do a little demo where I showcase                                this streaming etl job and and how to                                get started and use flink and soap a                                chief link is an open-source stream                                processing framework while building the                                engine we are focusing on low latency                                and high throughput and we are also                                focusing on state food stream processing                                which means that unlike first generation                                stream processors the system is aware of                                what you're doing within your coat so                                when you for example have a variable                                that is counting a number of events or                                you have a hashmap and we're all your                                data is contained you can tell fling                                look this is my data please take care of                                it make backups and so on and this gives                                you a lot of advantages over other                                systems and of course it's a distributed                                system similar to all the other big data                                frame works it runs on come on                                the hardware and and s name suggests                                it's developed in the purchase of a                                foundation in March we released the                                    version and and the community is                                currently discussing and working towards                                the                                                                     presented for example query with sade                                which might be a new feature that is in                                the                                                                     in production i think some of our users                                present here at passwords and you can                                find blog posts and so on describing                                what they are doing with link um so what                                about this whole streaming pass that                                everybody is talking about and in my                                opinion streaming is the biggest change                                in the data infrastructure since Hadoop                                for multiple reasons so number one and                                it radically simplifies your                                infrastructure so you need fewer systems                                you don't need to build a fancy lambda                                architecture to cover your patch and                                your real time streaming workloads you                                can do everything in one system you can                                do your own with you can do more with                                your data and you can do it faster so by                                having this low latency engine that can                                do batch processing and stream                                processing at the same time you can                                cover your existing use cases and you                                can add new ones as well and yeah you                                can confirm quickly subsume bet you can                                get rid of the whole of a lot of fetch                                pipelines and you might wonder now house                                is possible am I not a bit over                                promising here but the reason is why I                                am why I can confidently say that I                                really believe that streaming systems                                are a really big thing is that um if you                                look at the way data is produced in the                                real world you will see that data is                                actually produced in a continuous                                fashion so if you have for example a web                                server running somewhere and in the                                batch world you basically have two web                                server and then you have local lock                                fight so every every times every time                                somebody clicks somewhere an entry is                                written                                to the apache server web block and then                                you have like mapreduce shop also or                                 flume which is pulling the data from the                                 web servers into HDFS and then you have                                 another job which is pulling the data                                 from HDFS into Cassandra whatever and                                 there's a traditional way of processing                                 data with a batch system however the                                 data is actually produced in a real-time                                 fashion so every time somebody is                                 clicking somewhere a new event is                                 generated and new systems like blink and                                 Kafka embrace this nature so instead of                                 storing the fire first on the web server                                 then in HDFS and then to the Cassandra                                 or whatever database and you can just                                 stream the events from the web server                                 immediately to Kafka and then the stream                                 processor can pick up the data from                                 Kafka and process it from there so real                                 world data is produced continuously and                                 we just need to commute continuously                                 processes it process it as well and to                                 do that I am suggesting to use a pet                                 chief link and this is the Apache fling                                 stack so when you're downloading flink                                 from our website you're getting this                                 entire stack and and the core of the                                 stack is this streaming data flow                                 runtime it's an engine that allows you                                 to em execute and data flow graphs in a                                 distributed system and for starting this                                 engine there are different options                                 there's the yarn deployment option                                 there's the standalone cluster mode and                                 that some of our users are also using                                 for deploying flink on docker and                                 there's the local mode the local mode                                 allows you to start fleeing from within                                 your IDE and or on your laptop and which                                 allows you to set breakpoints the park                                 and try out the code that will run on                                 the cluster later also locally and and                                 the interesting observation about this                                 engine is that it doesn't really know                                 it's not really aware of the the source                                 of the application that it's executing                                 so whether the program has been                                 implemented in the data stream API or                                 the data set API for patch doesn't                                 really matter for the engine                                 so there is like an abstraction between                                 these api's and the engine and and for                                 the data set for the best programs                                 there's an optimizer which is generating                                 a graph for the engine but since it's a                                 streaming talk I will only focus on the                                 streaming side of our api's so the data                                 stream api is very nice easy to use API                                 that feels very natural for Java                                 programmers and and it's the most                                 commonly used API for for writing                                 streaming programs it's very similar to                                 high level API is known from the bash                                 world so it's really easy to get started                                 with fling and and on top we've built                                 some use case specific API sand                                 abstractions so for example we have a CP                                 library for complex event processing and                                 which allows you to detect event                                 patterns in your stream so if you're                                 doing some I don't know fraud analysis                                 or behavior detection or so you can use                                 this EP and engine and which can do                                 pattern matching um an upcoming feature                                 in                                                                       to define sequel statements on streams                                 so this will allow users or even more                                 users to enter the streaming space and                                 then the Apache beam another Apache                                 project currently undergoing incubation                                 it has been contributed to apache from                                 google and theme is basically um one API                                 for different engines in the OP source                                 world and also on Google's cloud engine                                 and and flink as one of the most                                 sophisticated runners for this beam API                                 so currently spark and flame are                                 supported and then the SS compatibility                                 matrix on their website where you can                                 see which features are supported by                                 which engine but the cool thing about                                 themes you can implement your stuff in                                 one a p.i and run it with different                                 engines and there are other layers like                                 the storm ap added allows you to run                                 existing storm drops on fling and                                 there's patches are more which allows                                 you to to do machine learning on streams                                 um maybe a different way of looking at                                 fling is looking at its features so I've                                 created four categories and number one                                 is true streaming so it's a real                                 streaming engine that is focusing on                                 high throughput and low latency so by                                 having this engine that can deploy this                                 data flow graphs which are continuously                                 running and immediately pushing data                                 forward as it enters the system and we                                 can process the data really quick and                                 fast and also this engine has very well                                 behaved behavior when it comes to flow                                 control so the system naturally behaves                                 under under load situations and it can                                 slow down em upstream operators if                                 operators are not fast enough for                                 processing the data and then the engine                                 has support for event time which allows                                 you which basically makes a system aware                                 of the time when the event happened in                                 reality and this way we can reproach                                 reprocess historic data and we are not                                 the system is not suffering from network                                 failures or out of clock out of order                                 clocks I will explain later how exactly                                 the event time is working but it's                                 really crucial feature for for streaming                                 engine em the api's and libraries as I                                 said a very rich for example you can do                                 the complex event processing and we have                                 very flexible window API that allows you                                 to define windows on time on the number                                 of elements you can also build session                                 windows that allow you to em detect                                 basically user sessions or for example a                                 user session when somebody is clicking                                 on web servers so if link allows you to                                 detect that some events belong to the                                 same user and then you can do analysis                                 on that session m and flink allows you                                 to do state food stream processing so                                 the system can be made aware of the                                 state and the variables that you're                                 using within your application code and                                 with this we can                                 and provide exactly once guarantees for                                 m                                                                 failing we can restore your state and                                 the internal windows of link are also                                 using the the registered state of link                                 so we are also taking care of the window                                 content so they are not lost in case of                                 a failure and we have these safe points                                 that allow you to create globally                                 consistent checkpoints and of your state                                 this way you can take a snapshot of your                                 state store it in HDFS and then for                                 example deploy a new version of your job                                 or upgrade your flink version or to some                                 plaster maintenance and then you can                                 restore your job from that specific safe                                 point so let's move an existing and data                                 analysis job into the streaming world                                 and one very common use case is etl                                 extract transform load and I've looked a                                 bit for definition for etl and I found                                 many that's why I came up with my own so                                 Adam won't edit one more and my                                 definition is just move data from A to B                                 and transform it on the way and so the                                 old approach for atl m works like this                                 you have some data sources for example                                 server logs or machine data mobile or                                 IOT some sensors and so on and and then                                 you are ingesting this data into one                                 common raw data store some Hadoop                                 vendors call this the data lake and we                                 just put your raw data and you'll figure                                 out later on what to do with the data                                 and then you have some periodic jobs and                                 which read the data from this data lake                                 normalize it clean it and then put it                                 into some system like elasticsearch or                                 into a or RC columnar file format                                 for later analysis for example with                                 Impala or hive and so on and then the                                 last step is to do aggregations on the                                 data for example if you're doing a                                 website or so and you want to see the                                 the user behavior                                 again you're using periodic batch jobs                                 and ingest the data and into these data                                 stores like Cassandra Redis or my sequel                                 um so how can we move such a batch                                 architecture into the streaming world so                                 instead of using HDFS we're using a                                 Patrick Kafka for the for storing the                                 raw data so all these data sources are                                 just sending their stuff into Kafka and                                 we figure out later on what exactly to                                 do with it then we are using a stream                                 processor and apache flink has for                                 example a connector for patrick africa                                 which is also exactly once for the                                 consumer so in case of a failure and                                 flink we can just restore and nothing is                                 lost on everything is in sync then                                 within fling and I've put like a generic                                 data cleansing task here and which is                                 removing invalid records and so on and                                 then some of the data goes to a                                 transformation and alert generation and                                 the output of this data could for                                 example go to elasticsearch again and to                                 rolling file sync so the rolling file                                 sync is for example creating new                                 directory every minute where the system                                 is putting data into em yeah and the the                                 other part of the clean data is going                                 into time windows to do some real time                                 applications so we have for example jdbc                                 sync or a cassandra sink um that allows                                 you to send data from fling and into                                 these other systems so you see that the                                 architecture is already a bit simplified                                 because you have this one system here in                                 the middle that is taking care of the                                 clean data and the aggregated data and                                 that is also taking care of consistency                                 so as I said the Cassandra the kafka                                 connector is taking part of links                                 snapshotting so it's exactly once and                                 then for example our Cassandra sink is                                 also providing em exactly one semantics                                 for idempotent updates                                 so the data is always in sync and                                 another advantage of this architecture                                 is that you're reducing the latency and                                 drastically so the events are processed                                 immediately as they arrive as they enter                                 into this architecture into the system                                 in the previous architecture you saw                                 that I had these two loading jobs                                 between the stages that we are loading                                 the data from the web servers IOT data                                 into an HFS and then this other job                                 which is creating the aggregated data in                                 this case data is processed as it enters                                 the system it can immediately walk into                                 an elastic search or a file system so                                 the latency goes down drastically and so                                 if you if you're comparing the different                                 approaches that exists in space and this                                 periodic petshop approach is in the                                 range of hours maybe some users if                                 they're using fancy hardware they can                                 get down to minutes with a batch system                                 but it's pretty expensive and and                                 tedious to to run such an infrastructure                                 because you have to keep all your bed                                 shops running all the time it's                                 complicated then for example apache                                 spark has this batch processor with                                 micro purchase built in so basically                                 it's just a logical and development from                                 this periodic batch shop that is                                 triggered by a workflow manager and to                                 this triggering into the street into the                                 best processor itself so spark itself or                                 other systems that are using micro                                 patches and they just trigger batches                                 continuously and from the outside it                                 looks like a stream processor with that                                 approach you can get down to seconds and                                 but only with the stream processor you                                 can really get into this milliseconds m                                 latency area and you see this little                                 stars here so your mileage may vary and                                 don't believe my numbers believe your                                 own numbers and I can they say this was                                 confidence because Flinx really easy to                                 use so in half a day you should be able                                 to do little proof of concept and try it                                 out yourself on your own                                 hardware with your own requirements and                                 so on and then you can do your own                                 measurements and see what the latency is                                 that you get for you use case um so                                 please don't trust numbers try it out                                 yourself another advantage of advantage                                 is this event time awareness so the                                 events that we are processing and they                                 might arrive out of order in the system                                 so if you're looking in this at this                                 architecture again you have these three                                 different sources and these three                                 different sources might have different                                 clocks um so imagine for example we have                                 this mobile phone and somebody with a                                 mobile phone is walking into a factory                                 so the factory Wi-Fi recognizes okay                                 somebody entered the factory then this                                 Wi-Fi recognizes okay he's in a factory                                 a nap on the way on the mobile is                                 sending a message to the web server so                                 these blue events here are all related                                 to the same real world event however the                                 clock of the mobile is off so the Wi-Fi                                 clock says at                                                            factory the app set says close to                                       I got this event so the clocks are out                                 of sync another issue is network still a                                 network delay so maybe the network of                                 the factory is delayed or not working                                 all the time so it might happen that                                 events arrived much later in kafka then                                 events from the web server or from the                                 mobile phone another issue is machine                                 failures it could happen that for                                 example machines at fling or in Kafka or                                 somewhere else are failing and this way                                 we are not processing data for for short                                 period of time with stream processors                                 event I'm aware we can basically tell                                 the system to use the time when the                                 event happened in reality and not the                                 time when the event arrived in the                                 system and this way you can do stuff                                 like reprocessing and you always get                                 correct results also in case of failures                                 so let's turn this into reality and so                                 I've prepared a little demo that is also                                 doing this streaming et al so I've                                 created two drops one very small job so                                 this small job M is so this says if what                                 does it say flink Twitter sauce so                                 inflicted as a Twitter search sauce                                 which is reading data from Twitter and                                 intro Kafka and then there's a second                                 chop the streaming et al chop and which                                 is reading the data from Kafka into this                                 job so let's have a look at what the job                                 is doing so this is the topology here's                                 the kafka sauce which is reading the                                 data from Kafka here's a theater                                 operation which is featuring all the                                 records that are system data so in from                                 this M Twitter data source you're                                 getting tweets and you're also getting                                 some system data from from Twitter and                                 with this further I'm just removing all                                 the system data to get the tweets only                                 and then it's splitting up into three                                 different flows the first flow is a                                 rolling file sync here I'm filtering for                                 all the tweets which are English so I'm                                 figuring on the field length language                                 equals English and then i'm writing this                                 to a rolling file sync so in a class                                 that you would use HDFS or MS and s                                     or you can also use a local file system                                 like I do in my demo the second flow is                                 doing a window aggregation so I'm                                 counting the language of the tweets in                                                                                                   collecting tweets for                                                 I'm looking at their language and I'm                                 counting each of it each of the                                 languages and then I'm sending the                                 result after                                                 elasticsearch at the tumbling window                                 which is always collecting for                                    seconds then pushing the result and then                                 it's collecting for another                                              and the last job here or the last part                                 of this job is doing a streaming word                                 count so I'm extracting only the text                                 field from the tweet and to get just the                                 string not just full Jason with all the                                 user data location and so on from                                 Twitter then I'm tokenizing the word so                                 I'm taking this whole text string and                                 splitting it through to into the                                 individual words and into a tableau to                                 so much comma                                                           word count you know from every juice so                                 this is basically the mappers and this                                 is the reducer in this case since we're                                 on a stream we cannot really reduce                                 because we would basically collect data                                 forever and that's why I'm using a                                 one-minute time window for collecting                                 the frequencies of the word so there's a                                 window standing open for one minute and                                 collecting em all the words and deck                                 hounds at every                                                evaluating this window so it's basically                                 a sliding window every                                                 sliding forward but it's always of size                                                                                                  emitting every                                                    frequencies of the words in the tweets                                 of the last minute then there's a                                 top-end window which is just sorting the                                 frequency and so that I can get like the                                 top                                                                  then I'm writing this to Kafka so that's                                 the job that I'm going to present you                                 now um you can see my screen and so the                                 first thing that I'm going to do is I'm                                 going to connect to the consume tweet                                 word count topic and Kafka so as I said                                 I'm writing the data to Kafka and this                                 topic is reading the events from Kafka                                 am reading these top end events from                                 Kafka alright the next step is to start                                 fling so when you download flink you get                                 basically can you see the screen it's                                 probably a bit too small M yeah                                 I try to increase the font size a bit                                 okay so I'm doing bin so now I've                                 started fling and once you started flink                                 on localhost                                                          interface the web dashboard of link and                                 so right now you see that there's one                                 task manager connected test one                                 processing slots and they are all                                 available and and yeah one more thing so                                 as I said you can do patch and stream                                 processing with the same engine and what                                 I'm going to do now is I'm basically                                 going to do batch processing only with a                                 streaming engine because i have a kafka                                 topic as I've showed you on the slides                                 with the tweets and the scaf car topic                                 contains I think                                                      tweets from a few days ago so I'm now                                 just reading all the oat we old tweets                                 from Kafka em and the interesting thing                                 is that due to this event time I will                                 still get counts on the top end window                                 because i'm using event time it would                                 still for the historic data create                                 correct windows em with accounts so um                                 let me start the streaming etl job um so                                 now you see here that the job is running                                 so you see this topology that I showed                                 you earlier you see that here the                                 operators are M processing data so it's                                 processed                                                             and counting and here you're seeing ah                                 it's not very well visible so here                                 you're seeing all the top end counts for                                 the tweets so you see here the the time                                 so it's currently a tune first now sat                                 jun                                                                      m verts from the word count and written                                 into Kafka into this M word count topic                                 in Kafka                                 um maybe I can increase the font size                                 even further so that you can see it so                                 can you now see it so so now we are at                                 June                                                                 last time when i try it out this demo                                 and you see that the most frequent word                                 is retweet and then it's probably new                                 line a the and so on so I didn't do any                                 stop word filtering or so I just took                                 the raw data ok so now it's stopped                                 doing anything because there's no new                                 data being written into Kafka so what I                                 will do now is I start a little job                                 called Twitter into Kafka so it's                                 connecting to Twitter I'm out let me                                 open em another terminal so this is                                 called consume tweet it's just the                                 listening on the kafka topic where i'm                                 going to write the tweets so i'm now                                 starting this job i'm just starting it                                 from the IDE and now you should see ya                                 so now all the tweets are showing up in                                 this so this is just the tweets that I'm                                 getting from Twitter that I'm riding                                 into Kafka and um I think in one minute                                 we will start seeing em the word count                                 for the top end of these tweets that I'm                                 currently collecting em from Twitter the                                 reason why there is a delay is because                                 i'm using water marks so if link has                                 this built-in bottom arc system for am                                 handling late arrivals so as i explained                                 we have these events that arrived out of                                 order and so the question for the                                 windows in flink is always um ok it's                                 starting and so now we saw jun                                         and so it is now apparently you have                                    minutes left m                                 so now it's slowly starting to write the                                 top n apparently I'm getting some I                                 don't know arab tweets so I'm just it's                                 not the full stream of Twitter it's just                                 the sample of the of the tweets from                                 them um so yeah so now you see it's it's                                 starting to write events to the top n                                 and you also see in the web interface                                 and that it's processing data in real                                 time and s events come from the from the                                 Twitter sauce also I've created a little                                 I'm really not an expert in cabana                                 cabana dashboard and where you can see                                 the where you can see the data so here                                 you can see the distribution of the                                 languages so English is of course the                                 success english is of course the most                                 frequent language and angie's also see                                 the M the counts of the languages so                                 this is produced by this part of the of                                 the streaming flow so we've looked at                                 this one so here's the cough casting                                 which is writing the top end to Kafka                                 this is the aggregation to elasticsearch                                 so there we have this                                                  which is counting the frequency of the                                 languages and writing it to                                 elasticsearch and this is the rolling                                 file sync so we can also take a look                                 into em The Rolling fight sing and so                                 it's located                                 so here the rolling sink and here you                                 see that for every minute we are                                 creating a new directory em I can go                                 into the                                                           there's a file it's three megabytes and                                 there you see all the raw this is where                                 the signs are coming from yeah so here                                 you see the raw and tweets written to a                                 file and so this way you can see that                                 there are different data sings where                                 this one source is writing different                                 kinds of data to different ends all                                 right so let me close with the talk so                                 if you like this stuff if you would like                                 to do more stuff like this I've had a                                 virtual machine running at Google Cloud                                 for the last seven days where I was                                 collecting Twitter data so I've like                                     gigabytes of Twitter stream data and                                 tomorrow we will organize a flink                                 hackathon and which is in a to telecom                                 building at bitten bet yeah it's in                                 closer quads back and not it fit in my                                 Bloods and so you can still sign up I've                                 asked there are                                                     you're interested in working with                                 fleeing and you can do a self-paced and                                 workshop you can work with the data that                                 I collected we have also other data                                 sources all you can do your own stuff                                 and some of the fling computers will be                                 there to help you so this is a great                                 opportunity to to learn more about flink                                 it's also free and if you like this                                 location and if you would like to learn                                 more about flink this fling forward in                                 September in Berlin and the M sub you                                 can still submit talks for the                                 conference and you can still get tickets                                 for the early bird rate so this is going                                 to be a great conference last year was                                 also very good and yeah my employer is                                 currently hiring so if you're interested                                 in working with open source projects                                 then please                                 talk to me and now's the time for you to                                 ask questions and you can also send me                                 an email follow me on twitter follow the                                 patchy fling project on twitter we have                                 the mailing lists known from other                                 apache projects and we have two very                                 good blocks where you can read about the                                 latest features and developments of link                                 so are there any questions um yes                                 I can also repeat the question okay you                                 can also say the question and repeat                                 itself so I'm kind of new in the                                 streaming world so so the top right                                 corner would you like to the aggregates                                 in one minute windows mm-hmm how do you                                 tend that console it all that on the                                 larger scale what's the way here mmhmm                                 yeah yeah and the question is how do you                                 do windows that are larger than                                    seconds or one minute um and so so let                                 me let me first answer like one part of                                 the questions or fling supports so the                                 windows are using this internal State                                 back ends of fling so they allow you to                                 store state and on the local file system                                 in in rocks TP so we're using roxy be as                                 a state back-end and which is like a key                                 value store on the hard disk and this                                 way we we are not limited to the the                                 heap size we can use basically infinite                                 amount or the amount of hard disk space                                 available for keeping window contents                                 and around so you can build windows                                 multi-month or whatever depending on the                                 amount of data that you're having and                                 then and there are different ways of                                 getting data from the window so this you                                 can basically get an iterator over all                                 the events in the window and then you                                 can do whatever analysis you want and                                 they are also aggregated windows where                                 you for example specify that you want to                                 do em just account and in this case                                 you're not keeping all the data we are                                 just keeping the counts for each key and                                 this way the the space requirements are                                 much lower because you only to store one                                 there you perky and not all the data for                                 multiple month yeah but what you're                                 actually doing with the data and window                                 depends on you so you can specify the                                 size of your window you can also specify                                 windows on number of events you can also                                 say just the last                                                    million events and then you get an                                 iterator over the events and then you                                 can do your analysis and                                 omit the outcome of the window so this                                 is just an outcome of one window                                 consider the following use case where                                 you who receive a file on HDFS every                                 five seconds and then say one gigabyte                                 file every five seconds and then you                                 need to make some transformation some                                 aggregations joins with data from hive                                 and some transformation on that data the                                 incoming data and the output is elastic                                 search or hive mm-hmm what would be the                                 advantage is to use fleeing over a spark                                 streaming for example and so in flink we                                 will soon have a new em like streaming                                 five source and flink                                              current is also file sauce but the new                                 one will also be integrated with the                                 check pointing and so you can just tell                                 Frank look this is directory watch it                                 and every time something you appears in                                 this directory it will be ingested                                 immediately and introduced streaming                                 engine and then you can submit it to                                 elasticsearch to aggregations right and                                 so on basically what I presented and so                                 there is support for watching of                                 directory and collecting events from the                                 directory and and yet the difference                                 between flink and spark streaming is                                 basically what I had also my                                 presentation that flink is like a real                                 streaming engine so I am supports low                                 latency and it has support for event                                 time and it has these more sophisticated                                 window constructs you can also build                                 custom windows with it the thing about                                 this microbe etching approach is that                                 you always have latency your ass basic                                 guarantee that you have latency because                                 of this scheduling time so you redeploy                                 a bad shop every five seconds or so so                                 your latency is guaranteed to be at                                 least five seconds and also for example                                 if you have a connection to                                 elasticsearch or some JDBC or so you                                 need to reestablish this connection                                 every five seconds because every five                                 seconds you are redeploying another job                                 and also it doesn't feel natural to as                                 an application developer you always have                                 to be aware that my job is really broad                                 every five seconds I have to                                 re-establish a connection to a lessee                                 search every five                                 seconds in flink it just feels much more                                 natural to have this operator and you                                 know it's running all the time so it's                                 just a more natural way of processing                                 data there was another question can you                                 explain more how you deal with out of                                 order items and yeah em so how do we                                 deal with out of other events and so                                 basically what we are doing is in flink                                 that is um so in this in this topology                                 here damn okay now scroll so here there                                 is an operator called timestamp                                 extractor so here we are just getting                                 some Jason from Kafka and here there is                                 a timestamp extractor which is and                                 extracting along from the chase and data                                 and this long is the time of the event                                 in reality so it's not the time when the                                 event arrived in the system because that                                 can be out of order to to various                                 reasons and and we are using this long                                 field and putting it through the record                                 so the system has em to each record                                 there's a special internal field with a                                 long representing the time of the event                                 okay so this way operators are aware of                                 that time so for example this tumbling m                                 / language count window which is doing a                                 window /                                                                time stamp field for determining the the                                                                                                     field and then puts it to the right                                 window so it can happen that we have                                 multiple windows open because i don't                                 know maybe four so because events are                                 arriving out of order we have to                                 basically start different windows it                                 parallel and as soon as a watermark                                 arrives so from time to time this                                 timestamp x factor is sending a                                 watermark and it's what I'm access I                                 guarantee that there will be no later                                 event after this watermark                                 and once the watermark arrives the                                 window it knows okay I've received all                                 the events that belong to this time                                 window and then I'm triggering the                                 window and I'm doing the evaluation and                                 sending the results are to elasticsearch                                 so we're using water marks and this                                 event time field for the time you're how                                 do you configure the water marks yeah so                                 that's up to the user so the thing is                                 the water max and it really depends on                                 your use case and so what fling provides                                 by default is for example watermark                                 extractor which is chest looking at the                                 highest M times M seen so far and then                                 subtracting a certain amount of time so                                 basically this says I'm accepting                                 lateness of the events up to one minute                                 so then you have to basically wait one                                 minute until you get the results but it                                 can still happen that events arrive                                 after this                                                               then you have to do some custom handling                                 m of these events but it's just reality                                 I mean the three processor cannot change                                 reality events arrived out of order in                                 the system we're just providing the                                 tools for the user to to cope with that                                 yeah but there's very nice documentation                                 fling with nice pictures and so on                                 showing how this exactly works with the                                 different operators in the low water max                                 how to propagate and so on because you                                 always need to use the lowest water mac                                 form all incoming streams also connect                                 to this question are when you talk about                                 the event time are you you shown this                                 example before when you're when the                                 clocks on your client devices are not in                                 sync mm-hmm and you assume this is the                                 event time that also means you might not                                 get the real order because the field you                                 described is sent by the client who's                                 yeah there might be not only a network                                 delay but clocks out of sync yeah so you                                 only providing infrastructure for a                                 problem that exists and not a new                                 solution to the problem inside yeah the                                 thing is the advantage is that the                                 system is aware of event time so in                                 storm and also in this tradition                                 betch approaches so if you're back then                                 like my talk like how etl with a badge                                 system there there's no infrastructure                                 at all for treating this time issue you                                 solution to the problem itself it's just                                 you don't have to implement exactly we                                 just provide infrastructure for the user                                 to handle this problem and it's quite                                 easy because we have standard tools for                                 this our windowing our windows am aware                                 of this yeah but you are right I mean                                 it's not a magic solution for this                                 problem yeah sorry thanks a lot
YouTube URL: https://www.youtube.com/watch?v=4sV7-37uRFU


