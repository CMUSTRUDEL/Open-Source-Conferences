Title: Berlin Buzzwords 2016: Britta Weber - BM25 demystified #bbuzz
Publication date: 2016-06-11
Playlist: Berlin Buzzwords 2016 #bbuzz
Description: 
	Lucene will change the default scoring from TF/IDF to BM25 in the next major release. So unless you really enjoy surprises you better learn about it now! TF/IDF was easy enough to understand intuitively but how is it with BM25? What do all these parameters do? And what do people mean when they say it is "probabilistic"? 

In this talk I will tell the story of how we came from the Probability Ranking Principle to BM25 with a minimum of math and a maximum of explaining. I will also show how BM25 differs from TF/IDF, what it means in practice and give and intuition on what the parameters of this method actually do. You will leave this talk feeling good about Lucene changing the default.

Read more:
https://2016.berlinbuzzwords.de/session/scoring-human-beings

About Britta Weber:
https://2016.berlinbuzzwords.de/users/britta-weber

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              okay                               so i'm britta i'm a developer at elastic                               and I work on elastic search core and                               today I'm going to talk about BM                                  which is a method to sort documents that                               contain natural language text like                               articles or tweets or males according to                               their relevance to some keywords okay so                               this is about keyword search and the                                reason why I'll be talking about it is                                because from Lucene                                                   major release the scheme will be                                changing the default which is currently                                tf-idf to be m                                                      search and so I guess we'll someone so                                maybe solar device yeah okay so now is a                                good time to learn about it because                                what's the stuff with the new release                                and you're scoring might be a little                                different and that's why I'm gonna talk                                about it so so usually when I talk to                                people about Bo                                                     know anything about it and say okay so                                what's BM                                                             you all know because you've stared at                                the slide for                                                        okay that's the probabilistic approach                                to scoring and I had this conversation                                which some people never understood what                                it means and then with one colleague and                                I said so what does it mean anything I                                don't know you're the one who knows the                                math you'll figure it out                                or even better give a talk about it so                                this is where I am here thought I should                                figure out what's probabilistic about it                                and then give a talk about it so I went                                to look at this thing and so here it is                                this is p.m.                                                          don't know if you immediately see                                something probabilistic about it here                                where's the probabilities who sees it ok                                ok so my first reaction was more like                                that got a little scared because I had                                to talk the abstract or a submit it and                                get it ok um ok but I promise that this                                is actually not as complicated as might                                seem at first glance                                actually it's it was super trivial but                                it's a sort of easy to explain and we're                                gonna go through all the different parts                                of that during my talk but before we do                                that it is worth maybe spending a few                                minutes                                remembering                                why we should even bother to come up                                with such a monster why is this even so                                complicated either why's it so                                complicated to keyword search so some of                                you didn't even have that problem                                sometimes when you use a search engine                                for something you're not so interested                                in any relevancy or any ordering of your                                documents but you're only interested in                                finding something or maybe only counting                                so if you have some hard criteria like a                                particular date or an order item or                                particular price range and you just want                                to know which document has this criteria                                or how many have this criteria you don't                                care in which order they are returned                                when you search in text it's usually a                                little more problematic because when you                                do a keyword search many documents might                                match but you want to find those that                                best match your information need okay                                but this is a little bit problematic                                because language by itself is fuzzy                                right languages were both so many words                                that are in there don't even need for                                anything then of course words are                                ambivalent and documents can cover many                                topics and it's not clear just from your                                keywords if this document covers any                                topic that you're interested in or has                                your information need                                so that's tricky and on the other side                                what is even more tricky or what's also                                tricky is for you how do you formulate a                                search how do you oppress your                                information lead into a few keywords                                okay in consider for example consider                                this example you have some sort of                                database where people that want to apply                                at your company put in their profile                                right and it can put in okay so what                                kind of languages do they speack how old                                are they how much working experience do                                they have do they want to what what do                                 they want to earn and you can filter by                                 all these criterias but when you look                                 for somebody that actually matches your                                 company or matches your needs you                                 usually look at the self description                                 what do they write about themselves and                                 I suppose you had this database and you                                 would want to find someone who matches                                 your criteria who fits well into your                                 team who brings all the stuff that you                                 really need to fill this job and you                                 would look in the South description when                                 I build a search engine hey how do you                                 put that anyone somebody who's a quick                                 learner you want somebody who works hard                                 you when somebody was reliable enduring                                 and but you only have this search                                 interface so what you type might be                                 something like well hard                                 working self-motivated masochist and                                 this is a really really inaccurate                                 description of your actual information                                 need okay so when you having to build a                                 search engine it deals with natural                                 language text on keyword queries on the                                 one hand the search editing has since                                 really verbose and really ambivalent                                 text and on the other hand it has this                                 information it was just pressed into two                                 or three key words so it's all super                                 inaccurate and fuzzy and it has to match                                 these two things together to get the                                 document that actually relevant for the                                 user and this is a super hard problem as                                 you can imagine because you have so                                 little information available this is why                                 it's so hard there has been so much                                 research on it and also why I'm giving                                 now                                                                  what's the purpose of my talk so the                                 first thing is we're going to go through                                 all these terms in this equation I'm                                 going to show you what they actually                                 mean and also in particular what these                                 parameters do see some parameters                                 they're gonna discuss them later what                                 they do in practice                                 what does it mean for your scoring and                                 what's going to happen through the                                 scores I will also talk about why PM                                    has label probabilistic and this is                                 actually the major part of this talk and                                 to be absolutely honest it has not much                                 practical relevance for you this is                                 mainly for your entertainment and so                                 that you can later on go and show off                                 okay so I hope that I'll be able to                                 convince you that switching to beyond                                    is the right thing to do and that's                                 actually the hardest part of the talk                                 I'm not sure if I'll be able to convince                                 you but but I hope and again I hope that                                 you're able to learn some buzz words                                 that you can show off with during the                                 breaks although there's not so many now                                 more but yeah okay so before we talk                                 about being                                                            quick remind them what the current                                 default is currently folders tf-idf and                                 we stick with this little artificial                                 example we're looking for an intern I                                 will see you later why we need so many                                 interns so we're looking for an intern                                 and we look at the self description for                                 self-motivated hard-working masochist                                 and we want to find order the                                 applications that there are many because                                 your company is very successful you want                                 to order the applications by their                                 relevance to these this keyword for me                                 then the only evidence that we actually                                 have for that something is relevant or                                 not is what is called the term                                 frequencies and that it's the number of                                 times these key words that you type in                                 are contained in the text okay so what                                 you do is you go go through the text and                                 count how often are these two key words                                 in there that's the term frequency for                                 each of these terms and as you know I                                 mean it's tricky even to figure out                                 where what start where what ends how to                                 lowercase them and what-have-you this is                                 all the same part so we just assume we                                 have that thing okay so we get to turn                                 frequencies and from this tf-idf                                 computes some score by which the                                 document is ordered and there is just                                 summarizing it very very quickly so                                 there's three major tweaks okay first is                                 we take this term frequency sum this up                                 for each key word that matches and we                                 say more is better okay so the higher                                 the term frequency the more score it                                 gets and this is a it's not a linear                                 function it's a square root of the term                                 frequency but this is basically what                                 tf-idf and leucine does right now okay                                 this is the first thing it's                                 unfortunately a little tricky to just                                 use that because if you have very common                                 terms for example the or a or who and                                 use the terms that occur in nearly every                                 English document that you're looking at                                 so the term frequency will be rather                                 high in each English document so if a                                 user types one of these words in their                                 search query this will screw up your                                 score completely okay this is why I this                                 criteria is weighted by something else                                 it's called the inverse document                                 frequency which basically means that                                 common words or words that appear more                                 of them in your corpus that are common                                 to many documents are less important                                 okay and this is called the inverse                                 document frequency so document frequency                                 is how often does or in how many                                 documents does a term occur and inverse                                 document frequency means okay so the                                 more of them it occurs the lower this                                 value gets so this is this higher term                                 frequency meets lower score and this is                                 actually multiplied with this term                                 frequency and then it's all summed up                                 okay that's the second tweak and the                                 third is and I imagine for example you                                 have a tweet that Metro swallow has a                                 certain term frequency and you have a                                 book has the same terrific                                 frequency and                                 you might imagine that this tweet is                                 actually about that term whereas a book                                 that has a thousand pages and the term                                 only occurs five times it's maybe about                                 something completely different okay                                 and to also incorporate this intuition                                 into tf-idf we've seen now also uses the                                 length of the document and the length of                                 the document means how many terms are                                 contained in your field okay and again                                 the more terms are in there the less                                 well relevant the whole document                                 actually is so this is here and this                                 also take one divided by the square-root                                 right so shorter higher score longer                                 lower score this is a three major things                                 in the scene okay there's one more thing                                 that was seen this was tf-idf and i                                 consider for example the following                                 example you have so you're looking for                                 something you wanna do a holiday in                                 China and you're looking for documents                                 that match that to get a description of                                 what's good what's not good and so on                                 and so forth and yet these two documents                                 one is describes my holiday in Beijing                                 whereas the other one is the economic                                 development of citron from                                              them both will probably contain the word                                 China rather often holiday might only be                                 contained in this blog post right but if                                 this document contains China very very                                 often it has the same length and this                                 only sometimes and hold the only                                 sometimes this one will score higher                                 even though it's definitely not what you                                 really intend to find right so and this                                 stems from the fact that tf-idf always                                 assumes more is better and to adjust                                 your score again a little bit to that                                 Lucina something that's called the                                 cohort factor and in roughly what it                                 does it rewards documents that match                                 more than one query term so this one                                 would be adjusted accordingly so that it                                 scores higher than the one that only                                 matches one query term okay this is a                                 hack it's a little bit of a hack and                                 bull query and you'll see developers                                 here okay I can trust arrangement                                 oh yeah the art of the scene program has                                 made this hack so                                 okay so what's wrong with you if I def                                 I'm in TNS actually for tonight's right                                 that's been successful since oh what                                 just happened                                 oh boo                                 yeah I'm sorry it's the rest of God huh                                 okay good okay see if I do as I said                                 successful since the very beginning it                                 was the score the the first scoring                                 algorithm they had and it's still the                                 default right so successful since the                                 beginning of                                                             it is super easy to understand as you                                 just saw I hope um                                 and it's well it's a one size fits most                                 it most people are more or less happy                                 with it so the question is well why                                 don't even bother way why would we have                                 to come up with something else                                 and the thing is that even yes it is a                                 heuristic but it also means it's                                 somewhat a guess                                 it's somebody sat down and thought okay                                 so um if I add up TF that's nice if I                                 take the square root is even a little                                 bit better so let's take the square root                                 or okay if I divide by the length that's                                 fine but if you divide by the square                                 root of four legs that's also nice so                                 try all these different things and then                                 this is this is the final result and the                                 question is maybe maybe we can do better                                 maybe can be a little bit smarter about                                 it and then my score might be better                                 that was the basic idea okay so how did                                 we get to be m                                                          something that is called the probability                                 ranking principle and I'll just give you                                 the abridged version it's a much longer                                 version of that and a much better                                 definition but basically what it said is                                 is if retrieve documents are ordered by                                 a decreasing probability of relevance on                                 the data available then the systems                                 effectiveness is the best that can be                                 obtained from the data which by itself                                 maybe sounds a little trivial because I                                 mean you could think okay you have your                                 document that just has some probability                                 that it's relevant and of course the one                                 that has the highest probability should                                 be upfront and the other should be                                 scored lower right so um there is two                                 things that are actually really exciting                                 about it and I mean the basic idea is                                 you you try to put your intuition into                                 some                                 medical framework of probabilities right                                 and this means on the one hand if you                                 actually managed to put it into some                                 probabilistic framework you might                                 actually be able to incorporate prior                                 knowledge on your data into your scoring                                 algorithm meaning you can do machine                                 learning                                 yeah so that's the one thing that's                                 exciting about it and in the other is if                                 you manage to put it into a                                 probabilistic framework you don't have                                 to do the math because hundreds of                                 mathematicians have done the work                                 already for you you just have to look up                                 at the right places and so so that's the                                 cool thing about trying to use a                                 probabilistic framework it also means                                 that now we're going to do a little bit                                 of math okay so for the next                                            or so think it's mathematicians even if                                 you're not into math okay by that I mean                                 let go of all practical considerations                                 you don't care how many CPUs you need or                                 how many infants you need to hire if                                 they jump from the ruler here right and                                 the other is think of yourself as                                 super-smart why are you laughing no                                 seriously it's you might say I mean I'm                                 not sure if I get to the world enough to                                 actually explain everything on the next                                 slides if you don't get it immediately                                 don't worry you're super smart you could                                 figure it out at home okay so okay okay                                 so what's the basic idea uh separating                                 two fancy machine learning what do we                                 want to do okay so we try to estimate                                 the probability of relevancy and we                                 start with a simplification that is we                                 say okay relevance is binary document is                                 relevant given a query or a document is                                 not relevant and note that even though                                 this is have one zero it does not mean                                 that the probability is                                         probability can still be a float right                                 but in first ok relevance is something                                 that's binary it cannot be half relevant                                 okay and now what we could do                                 potentially to gather some data is we                                 could have our interns give them a                                 document give them a query and then they                                 tell us ok this is relevant oh this is                                 not relevant                                 hey and then the hope is that we get                                 they can use this data to actually                                 estimate the probability of relevancy                                 given some query that's the basic idea                                 so we get all our interns we let them                                 click all day                                 then we end up with a set come on                                 so it works actually you have users                                 right how many can you use user clicks                                 don't need all these infants okay                                 so you want to have the set of all the                                 documents per query and that some of                                 those are relevant and some of those are                                 not relevant okay so this is the machine                                 learning part we're gonna see later how                                 this goes but not a mousepad what does                                 this actually mean in math probability                                 of relevancy and in math how it actually                                 looks like is we want to estimate the                                 probability that the relevancy is one                                 given a document and a query okay so                                 this is the the relevant sister random                                 variable can be                                                         this bar here means okay given that so                                 given for some document and some query                                 give me what the relevance if the                                 probability that the relevancy is one                                 item P just means the probability and                                 just it's just a suppose you're not too                                 familiar with that the basic idea is you                                 have some list right yes all your                                 documents and this is for one query all                                 your documents and relevance is one this                                 we don't not interested in and you want                                 to get these numbers and then order the                                 documents by well this number highest                                 gets up in front so in this case                                 document                                                                 know what's here so okay the problem is                                 that you need this list for all                                 documents that you have in all documents                                 that you haven't seen yet                                 and you need it for each query that you                                 have in this particular formulation                                 right it is just a basic concept which                                 is not super helpful right now and now                                 here comes the part where the other                                 exciting hard way I said before okay so                                 it's good that it's in math because the                                 math means we can't just look up the the                                 places that we need and somebody else                                 will spend the work already for us so                                 what we what we need is we want to try                                 get this this formulation of the problem                                 into something that we can actually use                                 meaning we want something that just like                                 tf--idf or you can take the term                                 frequencies and from these term                                 frequencies                                 estimate this probability right so we                                 have to reformulate that in terms of                                 term frequencies we really have to make                                 some independence assumptions and so on                                 and so forth okay and I said before                                 somebody else's done that already so I'm                                 not going to show exactly how that goes                                 just gonna say here be math                                 if you if you want to know the details                                 the absolute details of how this                                 computation works it's not super lengthy                                 or hard to explain but if it wouldn't                                 fit in the talk um you can look it up in                                 this paper the probabilistic relevance                                 framework beyond                                                         exceptionally well written paper it's                                 it's really what seriously                                               what time is it okay                                 read this paper if you want to know the                                 details I don't want a thing okay so                                 they did all this computation try to fit                                 in the term frequencies and so on and so                                 forth and get to some final result I'm                                 just gonna discuss the result and then                                 what comes after that okay this is the                                 final oh yeah here's a W that's a wait                                 that's a W of the okay so what we are                                 now but something that's actually not a                                 probability in between this computation                                 we got rid of the constraint that it has                                 to be a probability instead it's just                                 some weight that would order the                                 documents the same way that an actual                                 probability would but it was interesting                                 here is the right hand side so this                                 little Sigma just means okay sum up all                                 the terms in the query where the term                                 frequency in this document is greater                                 than zero here just sum all of this up                                 and and these parts here these                                 probabilities what this actually mean is                                 we have a list so we have a list for the                                 probability of the term frequency of                                 hard-working is                                                  relevancy is                                                         hardworking is                                                                                                                                given the to relevance of                                             and so forth and for each of these terms                                 in our corpus we need this list of                                 numbers and in the same way we also need                                 the same for hard-working does not occur                                 in a document and if you think and this                                 is still a little weird is this much                                 better than what we had before because                                 what we now need is just to maintain one                                 list per term in our documents and if we                                 have this list and if you have that                                 somewhere stored then once the query                                 comes in we can just look up these                                 values in this list plug it in here sum                                 it up and be done and we had a perfect                                 relevancy score okay but then of course                                 the question is well where do we get                                 this list from it's a little better than                                 before but where do we get them from                                 okay so how do we estimate this                                 probabilities I first shot at that is we                                 make a in dramatic but useful                                 simplification as we use the binary                                 independence model we say we don't care                                 about frequencies at all we only care                                 about if a term occurs in a document of                                 it doesn't occur in a document and this                                 sometimes makes sense in practice too if                                 you look in the tweets or a chat                                 messages where you wouldn't think that                                 the term occurs more often than once a                                 way you wouldn't care if it does okay so                                 in that case you can actually make that                                 assumption and then per query and per                                 term                                 you only have or per term you only have                                 a set that contains well documents that                                 are not relevant and do not contain the                                 query term some are relevant some                                 contain the quarter at some are relevant                                 and contain the query term right and if                                 we had this set for each of our terms                                 because her infant pixel endlessly if we                                 could get these numbers maybe we can put                                 estimate as probabilities from these                                 numbers okay and we can so this is the                                 way this would go I would say okay                                 probability that the frequency is                                   given that the relevancy is                                             just the relevant documents that contain                                 the term divided by the relevant                                 documents this is this probability it's                                 a max maximum likelihood estimate and if                                 you're in the statistics you might say                                 well now but okay and there's some                                 smoothing factors there that are not                                 super important for the course of this                                 talk but anyway if the headers numbers                                 this is how we actually could compute                                 that number and then we could do the                                 same for frequencies one given relevancy                                 is zero right and then get the frequency                                 is zero just by one - the other thing                                 okay and then for frequency - irrelevant                                 so we don't care right we could spirit                                 this binary independence assumption                                 so to summarize right we take these sets                                 right we compute these four                                 probabilities we plug this in to this                                 weight equation into these four terms                                 whenever we encounter a term sum them up                                 for each of the query terms where that                                 actually occurs and then we have our                                 perfect score okay                                 and if we do that if we plug if we plug                                 these these these terms that we just saw                                 really in here what we end up with is                                 the Robertson Spike Jonze wait and this                                 is something that you might have might                                 not have heard ever since but something                                 you can impress people with if somebody                                 says okay oh yeah I'm losing the scene                                 sex yeah it's uses the Robertson spark                                 Jones wait okay anyway so this is what                                 you ended up it so you have these this                                 this equation yeah this is really just                                 the counts that are in these sets right                                 and this is how it was derived okay so                                 if you have an unlimited supply of                                 insurance you let them click all day you                                 get these sets you compute for each of                                 the terms compute this number end up                                 with one list for a motivated work in                                 experience and so on and so forth and                                 then whenever something matches the                                 query you just sum these numbers up for                                 these terms and then you get your                                 perfect score that's the basic idea of                                 that unfortunately we don't have that                                 normally and the fun thing is you can                                 still use the Robertson spike jonze wait                                 but make a really funny assumption and                                 that is that the number of documents                                 that are relevant is zero which which                                 sounds a little odd in the very                                 beginning but if you actually do the                                 math go back and set this to zero here                                 then you actually end up with something                                 doesn't just vanish but you end up with                                 something that's actually useful and                                 this is called the inverse document                                 frequency for                                                            idea how that looks like so this is on a                                 logarithmic scale here you have the                                 document frequency you have the inverse                                 document frequency I can in logarithmic                                 scale you can see okay it decreases                                 right from                                                              decreases even further to                                               comparison to the the IDF that you get                                 from tf-idf decreases decreases but then                                 stays the same and what this means is                                 that very very common words are scaled                                 even lower than they would be in tf-idf                                 hey so this is just a major difference                                 here ok so it's that part it's the                                 inverse document frequency that says                                 okay how popular is the term actually in                                 the corpus it comes from this Robertson                                 Spock Jones bike and the assumption that                                 relevant document or number of relevant                                 documents is                                   okay so now second part we said okay we                                 don't care about TM frequencies                                 that's what if we do care about turn                                 frequencies right what does the term                                 frequency tell us about relevancy and                                 then tf-idf it was easy we said okay                                 more it's just better but for BM twenty                                 five people were asking actually a                                 different question and it was well what                                 does the term if we can see actually                                 tell us about if a document is about a                                 term or not about a term this is a this                                 property is called elite Ness and it's a                                 little weird concept so I'll give you an                                 example example four liters if you vary                                 some some look for the term tourism in                                 India PD pages you will find the word                                 tourism on nearly every page that is                                 about a country nearly every page okay                                 but these these pages are not actually                                 about tourism right there is a pages                                 that are dedicated to tourism like                                 there's a page for ecotourism or there's                                 a page for doctor is omit these are                                 actually about tourism the others are                                 not so just because it occurs that                                 doesn't mean it's actually the document                                 is about this particular term this might                                 be something that is important for a                                 scoring so the questions can we can we                                 use a prior knowledge on the                                 distribution of term frequency and                                 delete this to get an even better result                                 okay and the idea the basic idea is we                                 have two cases we say a document is not                                 about what is described by the term and                                 I way we would probably assume that the                                 term frequencies are distributed as such                                 so the probability that this document                                 for to observe this term frequency in a                                 document that not actually is about that                                 term would be higher than that the term                                 frequency is higher right and then if                                 the document actually is about the term                                 we would usually assume that well the                                 probability that we observe a higher                                 term frequency is also higher if the                                 document is about that term note that                                 this might not always be the case right                                 this red line might also be here right                                 or somewhere else but this is the this                                 is the basic assumption okay so we need                                 to get this distribution once for the                                 documents that are not about the term                                 and once for those that are about the                                 term it so we need in many documents and                                 have different term frequencies and                                 we'll see in a minute what we do so we                                 need a different documents with                                 different term frequencies and again as                                 our intern or                                 here's this document here is this term                                 is this term actually elite to this                                 document okay okay then our intents will                                 go ahead all right                                 hopefully they say yeah it's elite or                                 it's not elite but we need many more                                 right many more because we need all                                 these different frequencies - hello -                                 okay so we need more inference click all                                 date do that so they may get a notion of                                 a lightness but then a question is a                                 case that we know it's elite but we do                                 not know yet is it relevant or is it not                                 it's the document relevant or not to our                                 query right so we need another                                 relationship of elite nurse and                                 relevancy okay and here so it's right                                 these curves here how does the title                                 relevancy I won't get into here is we                                 could do the same trick that we did                                 before right you can say okay we have                                 another distribution with the                                 probability of elite nurse given                                 relevancy and if we know the elite nurse                                 already and can tie this other                                 distribution into that maybe we get                                 something better out of it and here I'm                                 growing a little faster because yeah                                 that have that much time but we would do                                 the same trick as before we try to get                                 our sets because this is a binary                                 variable it's a binary thing so we can                                 just take these sets again compute a                                 have elite documents have relevant                                 documents have elite documents that are                                 relevant on some that are neither and                                 then try from that to estimate estimate                                 this probability okay but it means we                                 need to set okay so we need even more                                 interns okay but suppose we had that                                 again we're mathematicians and bear with                                 me just one more minute right so we're                                 mathematicians we don't care okay we                                 have all that we can get as many interns                                 as we want so we can get this                                 distribution of the term frequency given                                 the elite nurse we can get the                                 probability of elite as given relevancy                                 right we can try to combine this two                                 into a probability of frequency given                                 elite this and this is really what                                 relevancy and this is really what's the                                 one what we want yeah plug this in here                                 and then hopefully get something that we                                 can use in practice okay and again here                                 do math I'm not going to show you that's                                 not so much that happens there to be                                 honest so here we met and we get to so                                 okay something that is actually too long                                 to fit on the slide it doesn't really                                 help so all the math all the result of                                 other people doesn't really help we get                                 something that is pretty nasty and then                                 this paper that I cited before this all                                 described in there                                 you can take a look and it just quote                                 from that that the result is a somewhat                                 messy formula and furthermore we do not                                 generally know the values of the                                 parameters or have an easy way to                                 estimate them because we don't have all                                 these interns and that's a bummer                                 and I think it was also a bummer for the                                 office I guess of pier                                                think I think the major achievement of                                 the authors of beyond                                                  was not just to give up and say hey okay                                 we reach the dead and there is nothing                                 we can do but to still cling on to that                                 and make something out of that okay so                                 what did they do                                 I found a video on YouTube where guy                                 described said he says they took a leap                                 of faith okay so what is this leap of                                 faith L they say well okay so we cannot                                 really get out the proper shape of this                                 this monster which I didn't show you but                                 we can still have some user Mouse and                                 get some description out of that so for                                 example we know that this curve would                                 start at                                                             monotonically we know what it would                                 approach a maximum at some point and                                 then not any further and the maximum                                 would be the inverse document frequency                                 that we computed before which is nice                                 okay so they said hey we just use                                 something similar even though we don't                                 know what it is and so what they came up                                 with was called the term frequency                                 saturation curve and that basically                                 looks like this I take the TRO frequency                                 but you don't just sum them up or some                                 the square root up but you divided by                                 the term frequency plus some parameter                                 and as you can see as term frequency                                 grows this whole term will approach                                   right I'm speaking as quick as I can                                 okay and this is here multiplied by the                                 IDF so this parameter here you can                                 choose it freely I think the default is                                                                                                         have if I have a higher K it'll approach                                 the slower if you have a smaller K it'll                                 approach it quicker okay and this is the                                 so this is the basic difference also to                                 to tf-idf because in tf-idf the square                                 root of the term frequency then we'll                                 just grow                                 infinitely right okay so we are here                                 this is the part the last part of the                                 this is a saturation curve of being                                    okay                                 and now finally and then I'm gonna cover                                 this a little more quicker um so one                                 thing that that it was not really                                 mentioning when I talked about the                                 Poisson distribution is we sort of                                 assumed that all documents have the same                                 length okay and this is something that                                 also doesn't work for a b                                              somehow take the lengths into account                                 and the way they did it is they say okay                                 the interpolate between one and the                                 darker than the the average document                                 length and have this effector to this                                 Kay okay so this is an interpolation of                                 one and the average document length and                                 this is important later on they did take                                 the average document lengths and not                                 just lengths or the square root of the                                 lengths or something like that this is                                 something relevant if to what you                                 encounter in your corpus and just to                                 give you an idea what that does now this                                 is here's the document length right and                                 here's a score that would be computed                                 for different parameters again B is a                                 parameter if you choose B to be very                                 small it will have little influence on                                 the on the score or little influence on                                 the difference of the score if you                                 choose B to be a little bigger it'll                                 have more influence that this would be                                 this this black curve here again this is                                 different to two tf-idf where while you                                 still have some decrease but you don't                                 have the flexibility to actually choose                                 how much influence the legs actually has                                 okay so we're done this is a V and                                    and this is all the parameters and let                                 you see so we have the IDF how popular                                 is the term in the corpus limit the                                 influence of the term frequency and then                                 also the lengths waiting okay so and so                                 finally question is B m                                                 I mean I showed you not all of the                                 approximations but some of the                                 approximations and as I said the it is                                 originally probabilistic but since it's                                 impossible to get the only possible to                                 get a probability right with unlimited                                 data I would say no it's not really it's                                 inspired by probabilistic ranking this                                 is also from this YouTube video                                 and I would totally agree okay I shot                                 history just to give you an idea what                                 time frame were talking about it started                                 about                                                                   Spike Jonze wait and probably                                 probability ranking principle and all                                 this Poisson distribution that's what it                                 means here and and it took until                                      until the final beyond                                                 published right and after that was the                                 first to see in release but then still                                 took tf-idf and then only in                                            named esky and it's a guy who                                 participated in a google Summer of Code                                 together with Robert Moore actually made                                 similarities in lucien pluggable and                                 also implemented beyond                                                until now so                                                            was switched to beyond                                                  to give you an idea how long it took                                 okay so well I got about scoring and                                 p.m.                                                                     there's some pros and there's not pros                                 with beyond                                                             cutoff I so tf-idf common words                                 influence the score much more than they                                 would possibly be and                                             because beyond                                                        limits the influence okay it also means                                 that we don't need this court factor                                 hack anymore if you remember the example                                 with China and the blog post about                                 citron and in purpose and this other                                 Oracle so attract you could actually                                 switch it off if you're if you're using                                 elasticsearch                                 you can switch it off by index similar                                 if you don't default or type p.m.                                        then it'll not be used anymore okay so                                 other benefits of no I'm not sure I                                 should advertise that too much as you                                 can potentially tweak the parameters all                                 that's tedious and I'm not sure if it's                                 really worth it                                 but you can if you want if you're using                                 elasticsearch you close your index                                 update your mappings and then reopen the                                 index and then it'll use different                                 parameters the thing that's more                                 interesting is that this whole paper                                 actually describes a mathematical                                 framework to build non textual features                                 into your scoring so if you're looking                                 into adding additional signals to your                                 score as might be worth looking at the                                 details there they have a dedicated                                 chapter for that okay one warning                                 tf-idf has an automatic boost for short                                 field so for title fields for example if                                 you're using title fields right now and                                 rely on this automatic boost it will not                                 work as well or not work as well well                                 we're different with BM                                                  takes the average documents length into                                 account and adjusts accordingly                                 okay so it's just a little bit of                                 warming okay and I was it better finally                                 literacy says yes challenger said yes                                 users some users say so                                 Lusine developers say so a guy works at                                 elastic had a blog post about it had to                                 fill in it was so but actually we don't                                 know making an educated guess here so it                                 depends on the features of your corpus                                 and it defines and depends on how your                                 queries look like we believe it will be                                 better but yeah you'll have to try it                                 out and you can try it out right now if                                 you losing elasticsearch you can                                 actually try it out by closing your                                 index updating the similarity                                 accordingly opening it again and then                                 see what happens okay and then here is                                 some useful literature if you want to                                 read that stuff up okay and that's the                                 end of my talk                                 thank you                                 too much thanks a lot                                 nice okay
YouTube URL: https://www.youtube.com/watch?v=v3Ko0CwgTZ0


