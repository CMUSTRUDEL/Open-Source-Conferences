Title: Berlin Buzzwords 2016: Ted Dunning - Fast Cars, Big Data - How Streaming Can Help Formula 1 #bbuzz
Publication date: 2016-06-12
Playlist: Berlin Buzzwords 2016 #bbuzz
Description: 
	Modern cars produce data. Lots of data. And Formula 1 cars produce more than their share.

I will present a working demonstration of how modern data streaming can be applied to the data acquisition and analysis problem posed by modern motorsports.

Instead of bringing multiple Formula 1 cars to the talk, I will show how we instrumented a high fidelity physics-based automotive simulator to produce realistic data from simulated cars running on the Spa-Francorchamps track. We move data from the cars, to the pits, to the engineers back at HQ.

The result is near real-time visualization and comparison of performance and a great exposition of how to move data using messaging systems like Kafka. The code from this talk will be made available as open source.

Read more:
https://2016.berlinbuzzwords.de/session/fast-cars-big-data-how-streaming-can-help-formula-1

About Ted Dunning:
https://2016.berlinbuzzwords.de/users/ted-dunning

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              yeah hmm I don't think those would be                               fast cars not if we're in traffic that                               would be the slow cars I don't know how                               to fix those so I'm going to talk though                               a little bit about how we can apply                               streaming data fast data big data too                               fast cars in particular some examples                               from Formula One but also some more                               general examples you can catch me by a                                number of different ways I work for map                                our chief application architect I work                                with a lot of projects at apache I'm                                currently chief a blur or no I'm sorry                                VP of incubator at Apache that's a                                temporary role and you can find buzz                                words is that blinking hmm i also have                                email addresses that you can use so                                today what I want to talk about is                                what's the point of big data in                                motorsports what what can it help with                                how we can play with it too without                                buying a Formula One car some people I                                know who have small race cars and these                                sort of solutions will help them I'm                                also going to talk about a particular                                general technique of building random                                data which is as good as it needs to be                                in a particularly appropriate way every                                time I look away it blinks every time I                                look at it it doesn't I'm not sure how                                we simulate that hopefully it will get                                better and then I'll talk a little bit                                about how the particular simulation in                                this case works let's see if that helps                                so in Formula One sports right now the                                that's pretty famous that they use data                                at least fast data made no difference at                                all they use fast data to to improve                                their operations to improve their                                ability to perform in a number of ways                                often what you'll see are these kinds of                                diagrams these are                                many of the car parameters that are                                sampled in real time in fact normally                                there are about                                                      streams that sample I can't tell if I'm                                improving it or not the top line when                                you can see it is the RPM of the Inchon                                you can see during fast straightaways                                that's fairly constant when it's                                decelerating it's downshifting very fast                                it's difficult to see here but some of                                the downshifts very difficult to see                                you'll have to trust me the second line                                the blue line which is completely now no                                longer intermediate the second line is                                the speed of the car certainly help here                                did somebody could get on this so okay                                does it alternative no                                so we'll try again the blue line is the                                speed of the car this will appear at a                                moment everybody hold their breath I                                think its upstream okay so we're going                                to continue the blue line then is the                                speed of the car you can see it                                accelerating in the straightaways you                                can see a decelerating in in going into                                curves the gearshift shifts down so this                                car was so fast at it shifts that within                                                                                                                                                                           kilometers an hour and still only goes                                                                                                      seven times which is amazingly fast more                                most important in one of these sorts of                                measurements these graphs are some of                                the bottom ones which show the throttle                                in the golden color come on come on I                                will start talking about that in a                                moment this is the throttle here you can                                see that they have it maximum most of                                the time minimum sometimes a little bit                                intermediate and the bottom is the brake                                and then they overlay these so very                                often you might this x-axis is distance                                not time distance around the track so                                when you overlay them you can see how                                two different cars negotiated the track                                differently the blue one was able to                                maintain considerably more speed going                                into the curve switch from decelerating                                to accelerating much more quickly and                                over the entire graph much wider than                                this they gained about                                                 single lap in the controls you can see                                the red card started breaking much more                                quickly the blues car braked much more                                vigorously and quickly and then was able                                to shift back into the higher gears more                                 quickly by using kinetic energy recovery                                 so there's a lot of visual sort of                                 visualization sorts of intuitions that                                 people have been getting for about                                                                                                      nowadays they're going much further                                 they're doing predictive analysis based                                 on the track temperature the outside                                 temperature how the person is trach                                 taking the track what will be the tire                                 temperatures they normally run the tires                                 at                                                                    hot but if they go just a few degrees                                 hotter that the tires start bleeding                                 more quickly and depending on how hard                                 they break into the corners they can                                 predict how high the temperatures will                                 be coming out of there and how quickly                                 they'll go back down that then drives                                 how quickly the tires wear out how                                 quickly the tires were out how much drag                                 they have in the corners due to slippage                                 or the use of the aerodynamics will                                 drive how much fuel they use they also                                 then can show how those changing weights                                 will affect the car and the driver                                 different drivers have different skills                                 at different times as the tires wear                                 down the car gets slower as the gas                                 becomes lighter than the car gets faster                                 so there's a lot of that they also do                                 game theoretic alternatives because when                                 they pit stop against when the other                                 people take pit stops that will put new                                 tires on new fuel make the car slower                                 faster against the other people in the                                 race it'll also there's very few over                                 takings except for pit stops and then                                 they do monte carlo analysis against                                 possible weather during the                                           race it's very complex programming they                                 all want to be able to do this on                                 large-scale computers back at the                                 factory they can't do this in the                                 computational systems they have in the                                 pit so they need to be able to move the                                 data back and then of course they want                                 to look at this in a larger sense across                                 multiple races how are they going to be                                 improving the chances of different                                 drivers on the team so the outputs then                                 our tactical decisions what are the                                 probable outcomes and so on there's also                                 a lot of value in marketing because                                 Grand Prix Formula one fans tend to be                                 gearheads and data freaks so they just                                 love these and                                 mated representations of races and                                 things like that which are of course all                                 driven by data and the more data the                                 better now you also get people leaking                                 information this for instance is based I                                 just plotted it this morning if you go                                 to these bit Lee's over here you can                                 find that this person did screenshots                                 and character recognition in real time                                 during the race of the Australian Grand                                 Prix and told data out of those and                                 managed to get the position velocity                                 throttle settings and everything during                                 the entire race for one car driver this                                 crazy and you know you get occasional                                 little inputs like this but it just it                                 isn't really going to work it's not                                 going to help us outside the very small                                 community to build interesting software                                 it's there's going to be no data that's                                 consistent no data that's going to be                                 high enough quality no data that's                                 that's going to be usable except in very                                 special cases you'll be able to make one                                 plot from one race for one driver                                 sometimes you can't really do fun                                 interesting work when data is so                                 intermittent and so highly variable so                                 this highlights a problem that we have                                 in trying to build something interesting                                 distributable interesting more than                                 trying to sell the one customer and that                                 is real data has real problems real data                                 is something we can't share typically if                                 we do get it then we're going to be                                 contractually bound to not share with                                 anybody else we can't usually even get                                 it even when we're partnering with the                                 people who generate the data they have                                 obligations to the people who are                                 actually having them equipment being                                 measured so they can't even release it                                 to us and we certainly can't release it                                 to somebody else we can't break it i                                 mean the interesting thing about data                                 like this is what happens if and that if                                 of course                                 is a non-starter we aren't allowed to                                 have                                                              happens with the data we're not allowed                                 to have the real systems go down and see                                 how the failure tolerance handles all of                                 that none of this really can be done and                                 that's would be the fun stuff you know                                 break it what's one thing if it runs                                 it's much more interesting if it doesn't                                 and also there's this continual problem                                 with real data is it comes from the real                                 world and we're not in charge that means                                 that whatever is happening there we                                 don't necessarily know except via the                                 data and so we can't really tell if                                 we're seeing the phenomena in the data                                 that are real whereas if we were to make                                 the data if we control the world we                                 would be able to inject true phenomena                                 that we really knew and then watch how                                 it burbles through the system we'd be                                 able to see that so real data has real                                 problems there and fake data would have                                 some very nice properties that we might                                 like to take advantage of it can be                                 built at any scale we can build it to be                                 as odd or as normal as possible we can                                 build it with true knowledge of the                                 state of the universe and we can build                                 it with arbitrary fidelity now this this                                 idea of arbitrary fidelity is not that                                 it will match exactly the mechanisms the                                 exact form of the data but instead this                                 is a thing we've been doing recently                                 with customers is what we do is build                                 KPI preserving data generators the idea                                 is here we have live data real data we                                 have a security boundary which hides all                                 of the data and all of the real                                 mechanisms away from anybody's useful                                 observation so we have live data going                                 into some system under test and we have                                 key performance indicators or failure                                 modes that we can observe in the system                                 if it's a machine learning system then                                 those are like false positive ratios or                                 there                                 score distributions whatever we care                                 about in the data if it's going to be                                 cars and such it's going to be speeds or                                 in the range that's reasonable                                 accelerations look reasonable data is                                 going to be coming in at a plausible                                 rate and so on so whatever we care most                                 about the system according to the task                                 that we're setting to ourselves will be                                 replicated in the data if we're                                 designing a system and trying to prove                                 that it will handle volumes then volumes                                 are the key thing if we're trying to                                 prove that we can resolve certain                                 frequencies of input then speed of                                 sampling and insertion of those kinds of                                 things are the key but then if all we do                                 is we take fake data put it into the                                 same system and make it so that the                                 failure signatures and the KPIs match                                 the data itself here could be very very                                 different in furthermore we can just                                 transport the seed and the schema of                                 that randomly generated data out of the                                 security boundary that's easy to do                                 because it's easy to convince somebody                                 that within one kilobyte of data that's                                 in spectabile we could not have                                 compromised millions of data points                                 millions of people's data or hours of                                 machine data so this is a relatively                                 easy thing to do getting this live data                                 out is essentially impossible but once                                 we get the generator for this fig date                                 out we can generate new fake data and                                 have some confidence that if we put it                                 into similar systems as are running in                                 here possibly with innovations and                                 novelties that it will produce similar                                 results when we find good candidates                                 things that appear to work better than                                 the live system we can bring that new                                 system under test inside the security                                 boundary and verify that live data and                                 fake data work the same to the extent                                 they don't we can modify the generator                                 again to produce more and more fadila                                 test data under any scenarios that we                                 like this is a technique for generating                                 data                                 that has far fewer far more degrees of                                 freedom excuse me far fewer parameters                                 that we need to match and therefore it's                                 much much easier technique than actually                                 matching the exact distribution it's                                 also more secure in many cases we may                                 not in certain circumstances even need                                 to match the dimensionality of the                                 original input in order to get useful                                 simulations the fundamental idea here is                                 that if it breaks the same if those fail                                 your indicators if the KPIs match it's                                 as good as the original if it weren't                                 true that there's some important                                 difference then fine we'll just make a                                 self-fulfilling prophecy we will add                                 that important difference into the KPIs                                 and we will turn again to match so by a                                 circular argument data that matches this                                 white match is in every important way so                                 let's do that all we need to do is pick                                 some reasonable and plausible KPIs in                                 this sort of thing we need sample data                                 we need rates and volumes to be about                                 right we need the number of samples to                                 be realistic we need the complexity of                                 the samples to be realistic we need                                 somewhat plausible physics you know the                                 car can't go from                                                     per hour in                                                           reasonable accelerations so that we get                                 decent visualizations we need to have                                 plausible data semantics it should                                 roughly look like the real data so that                                 it compresses the same way can be                                 understood by humans looking at it the                                 same way and of course as always with                                 cars your mileage may vary we may have                                 other KPIs that you'd like that would                                 change your way of doing this we're                                 going to build an emulation that roughly                                 with very whole lot of hand waving                                 matches the physics of the situation                                 matches the physics of the cars and                                 we're going to turn that data spec into                                 KPIs and we're going to match by tuning                                 the data spec until we get decent way of                                 generating data                                 so it turns out this is pretty easy to                                 do pretty easy to build realistic things                                 now the real system is complex this is                                 the real system that we might be                                 designing there's an RF link the cars                                 are out there on the track spinning                                 around they transmit data via an RF                                 length they have about                                                 second they come in here the data first                                 goes to the FIA that the referees pit it                                 needs to be caught their first in a                                 secure data set then it'll be                                 distributed to each team I've only drawn                                 one team what happens there is local                                 analytics we need to do replication to                                 an engineering work station so that if                                 somebody has a laptop or something data                                 can be streamed into that and persisted                                 there so when they disconnect and go to                                 a hotel that I they have access to a                                 full history we also need to be                                 streaming back to the factory to be                                 doing all of those Monte Carlo and                                 strategic simulations back there and of                                 course archiving it hopefully something                                 like Apache drill to be analyzed offline                                 the particular implementation that we're                                 going to use here today uses vampire                                 streams for that but at these low data                                 rates that were demonstrating kafka                                 would work just fine the particular demo                                 we're going to use here is considerably                                 simplified because it's supposed to run                                 in just a few VMs it's going to have a                                 physics-based pseudo physics-based                                 simulator there it's going to be                                 generating engine and performance                                 criterion they're based on emulations of                                 a robot driven race going to drive that                                 into streams out to a simple interface                                 built on bootstrap and d                                      visualization jetty for a reddit rest                                 interface to data and of course archive                                 into a database and access the data via                                 drill I think the part we're going to                                 demo today is just this upper channel to                                 show it generating data and accessing it                                 make sense                                 the hard part is this idea of KPI                                 matching you'll see how some of the KPIs                                 are things that were not yet matching                                 the key generator here that we're going                                 to use and that we're going to tune to                                 produce the data we need and data                                 volumes that we need is something called                                 torques which is extensively used in                                 research largely ironically in AI you'd                                 think it was used in video games when it                                 produces things like that but the idea                                 is that a quasi physical situation like                                 that where you have opponents who are                                 doing actual physical things and you're                                 trying to overtake them and they're                                 trying to keep you from overtaking them                                 is a very interesting domain to work in                                 and having a simulator to do that gives                                 everybody access to a common ground for                                 competing in building these artificially                                 intelligent drivers you can also of                                 course control these manually but the                                 primary use lately over the last five or                                 so years for torques is actually in                                 building automated systems there's a an                                 ongoing grand prix of of robots that                                 compete in various kinds of races                                 various kinds of absurd situations that                                 they're subjected to so we use torts and                                 tune its inputs to produce boys are                                 realistic outputs and we would like to                                 prove out that the particular task here                                 has proved out software architectures                                 test certain software architectures for                                 building data pipelines and for pushing                                 data through all of those replicas we                                 also want to turn you is to match the                                 customer expectations not actually to                                 play video games no no instead what                                 we're going to be doing is simulating a                                 production system and especially failure                                 scenarios in order to prove to the                                 customers that the systems downstream of                                 the the RF link can handle things in                                 adverse circumstances and we want to see                                 supposing that the cars produced                                    times as much volume that they then they                                 do too                                 eh could we build a system that handles                                 that as well so the current status it                                 works in a limited fashion it's                                 available on github or at least it will                                 be available shortly on github it's on                                 github just hasn't been turned to public                                 yet all of the systems you'll see here                                 today are available the idea is that                                 it's built with one vm for the physics                                 simulator pushing data out to a data                                 collection vm those both run in fairly                                 small instances so you're not going to                                 see huge performance out of them you can                                 replicate either one on to more capable                                 hardware and get much more capable                                 results now we don't currently simulate                                 enough of the parameters to get the data                                 volumes that would be normally required                                 in a production setting notably things                                 like tire temperature times for at least                                 multiple breaks disks and things like                                 that are not simulated the                                           data sensors in the car which are                                 sampled in rates from once every                                    seconds to                                                             here represented because of the limits                                 of this simulator x samples                                              second against for six or eight                                 parameters but this will be enhanced                                 over time as we go forward the data rate                                 is currently also fixed the real data                                 rate changes over time as the car goes                                 into corners for instance in a real                                 formula                                                                  there because a lot of self is changing                                 in a straightaway the data weight rate                                 drops a lot because things change much                                 less quickly the data is currently                                 collected in pure JSON but in practice                                 it would be collected in the in the RF                                 link through the RF link even in a                                 columnar compressed form if you think                                 about it data is commonly thought of in                                 rows but if you store it as columns here                                 are kind of json versions of those if                                 you store it in columns then each of                                 these arrays which                                 a single column have the same kind of                                 data from a same kind of distribution if                                 for instance column c                                                they're going to step forward by                                 relatively constant amounts and so                                 they're subject to a lot of compression                                 you can stick one of these columnar sub                                 tables in as a single value if you have                                 sufficiently general data structures and                                 so in a message stream you could have                                 messages coming along in time that                                 actually have small blobs in them which                                 are columnar data structures themselves                                 those are subject to lots of compression                                 and a very efficient standard way of                                 storing time series data here's an                                 example of how much from compression you                                 can get we took                                                          stamps we have differing amounts of                                 jitter on a hundred micro second inter                                 sample period and if the jitter is small                                 the compressed size becomes very very                                 small even with five microseconds out of                                 a hundred jitter you get                                         compression of those timestamps using                                 very simple I can't quite read it but                                 basically the idea here is you XOR                                 adjacent samples and then you do binary                                 packing of the residuals the residuals                                 are mostly zeros because very few bits                                 change this is a very common technique                                 it's even more general than Delta                                 sampling and other data the actual                                 samples will also compress comparably in                                 many cases now another thing to keep in                                 mind is we want to have something that                                 has the power of JSON in terms of                                 flexibility here's a sample piece of                                 data that only has three sensors for                                 instance we'd like to be able to extend                                 it fairly transparently adding another                                 or one or two data samples and have the                                 queries on new and old data work                                 transparently across all of that that's                                 one of the key advantages for using                                 something like drill so I'm going to ask                                 tug to come on up he's got the same                                 I should running on his machine we'll                                 see if we can plug it in and show how it                                 looks if Murphy is is happy with us that                                 is this is live demo after all that's                                 right here                                 do you want to the noisemaker you want                                 me to talk he wants me to talk I will                                 translate he'll speak English and I'll                                 translate there comes a raise lots of                                 cars that's a car you can see multiple                                 cars here they're jostling for position                                 and you can see that their speeds are                                 quite comparable as they go along that's                                 a time axis on the horizontal right now                                 on this visualization not a distance                                 axis there we go here's the distance                                 measurement this is for rpms here they                                 these cars you can see Jags in the speed                                 curve those are typically caused by                                 shifting anomalies and you can see                                 things shifting as they go along all of                                 this is controlled automatically by a                                 bot ridden to plug into the simulator                                 here comes a drill query so the data is                                 being archived in a map our DB table in                                 JSON format i believe it's nested json                                 format so that you get a header and then                                 you get a raise of records this is the                                 row embedded form instead of the column                                 embedded form but the same idea of                                 flattening would apply to the column at                                 vetted form so what query was that so                                 the average speed by car and race so                                 let's take a look at the core again can                                 you make that bigger and so this is a                                 real-time query as things have been                                 collected right so there's a sub-query                                 in there which is flattening the data                                 that makes it look completely relational                                 it's being pulled out of a map our DB                                 table but that's the same as the HBase                                 API that could as well have been                                 in HBase table there the flattened here                                 takes that array and makes records for                                 every element in the flattened array and                                 then you get normal sequel syntax above                                 that for computing averages except for                                 the fact that we have nested values and                                 we saw the nested values previously in                                 those slides and again that's an                                 extension to sequel that's very useful                                 for these IOT sorts of applications the                                 race is still going on red seems to be                                 doing quite well oops there it is                                 finished somebody won okay so there you                                 go any questions anybody have an                                 application like this with real-time                                 data real-time measurements any physics                                 sorts of things happening out there okay                                 we're going to start assigning questions                                 so yes okay so it's time for QA anyone                                 wants to go first i'm going to victimize                                 people i know first we aren't careful                                 I'm not sure it's an entirely related                                 question but you said there are boats                                 that are controlling these simulators                                 has anyone working on well there is a                                 lot of hype about self-driving cars is                                 anyone building self-driving racing cars                                 these are self-driving racing cars if I                                 actually had to build my own racing car                                 and pay for that I don't think I'd want                                 one of these spots to to be running it                                 because a lot of times these things are                                 tuned to take chances particularly                                 that's how you win sometimes against                                 other BOTS but these are very similar to                                 those self-driving cars except that they                                 get a few extra cues from the race                                 simulator so for instance the in torques                                 the centerline of the track is given as                                 an input to the bots so that they can                                 tell what the veer off angle is                                 for the center line for where they're                                 going at the given moment and steering                                 toward that center line is the simplest                                 and quickest but you don't do nearly as                                 well as if you do cutting into the apex                                 of corners and things like that the                                 advanced spots in these simulations and                                 in the follow-on Stu torques actually do                                 neural net learning as they're doing                                 laps so as the track conditions change                                 or where as competitors change they will                                 change how they take each curve in order                                 to learn fast laps so these are                                 comparable but much much simpler because                                 the sensory input is simpler I don't                                 know if anybody's building real race                                 cars my guess is that would be a very                                 expensive hobby at least initially if                                 you've ever seen the Google cars                                 especially in the rain they drive kind                                 of like this they're very hesitant in                                 any sort of difficult situation so if                                 cars were passing them like that they                                 would just kind of back out there                                 wouldn't be a great race car and that's                                 because of the cost of the cars of                                 course and the cost of the liability in                                 this sort of situation they would be                                 very expensive I think thank you are                                 there other questions so do they really                                 drive back do they really good crumbs                                 what you said that they are baking                                 copper you know when someone is passing                                 by do they stop I've never seen a Google                                 car back up but I have seen it stop and                                 then move just an inch at a time forward                                 in situations where it can't really see                                 around the corner rain is on the                                 scanning sensors and so it's nearly                                 blinded as a real human would be but                                 they tend to have a do no harm sort of                                 attitude one did get a ticket recently                                 because it was driving too slowly and                                 they have matched so far the accident                                 rates of                                                               you know cautious driving but humans                                 assume that the other people are going                                 to be reasonable and take chances with                                 generally good results and generally                                 without some                                 prising the drivers behind them sure                                 there was a question in the middle of                                 the back oh yeah he's going to throw you                                 I just translate for you too so so the                                 question is why did we choose JSON                                 instead of Avro or proto buffs or                                 something like that well the simplest                                 reason is because it has to go on a                                 slide and that looks just a whole lot                                 better on slides but a more important                                 reason is that this is the first version                                 of this and so visual debug ability is                                 very important right now a there would                                 be a variety of formats that would be                                 usable keep going forward some are not                                 because it has to still be a record by                                 record format so something that proto                                 before Avro would be acceptable as long                                 as we have a schema registry in the case                                 of both of those some other / formats                                 like arrow might be very useful for the                                 little glob of data so where we have                                 Colin or compressed data arrow would be                                 a very reasonable candidate for that                                 compressed columns once you get to it                                 any sort of binary form of JSON oh hi is                                 a very reasonable category there because                                 it's a binary JSON encoding SBE simple                                 binary encoding which is designed by the                                 financial world would it be another one                                 given that we started and are using oh                                 hi in other areas we would probably                                 gravitate toward that it would be                                 efficient for the binary and compressed                                 and coatings and it would still have                                 jasons maddox which many of these other                                 things do not have so that's probably                                 where we'll go there's no good reason to                                 use just plain JSON unless data rates                                 are low and you know Jason's pretty fast                                 but not as fast as a dedicated binary                                 encoding it's got good physical                                 properties starting punched guns better                                 than punch cards you say yeah yeah been                                 there done that i hope i don't do it                                 ever again hi it seemed to seems to me                                 that if you have like                                                    thousand kpi's you will have to do                                 millions of simulations before you feed                                 on them correctly is this true so the                                 data rate the computational load for                                 different kpi's varies a lot the data                                 rates already for doing the                                 visualization dwarf the physics based                                 computations in the in the current                                 situation so if we want to make things                                 more efficient and produce lots and lots                                 of kpi's so for instance we can pretty                                 easily emulate the tire temperature                                 based on a simple cooling model and a                                 simple energy conversion model due to                                 tire slippage so we could get tire                                 temperatures break temperatures and and                                 many other things like that very quickly                                 and those change very slowly outside of                                 the corners and so a variable speed                                 integrator on those will have no                                 problems keeping up the overall engine                                 speed and car speed and acceleration                                 model runs at a                                                      rate that sampling on that and it runs a                                 variable step integrator within that but                                 the total amount of floating-point                                 computation is really quite long so I                                 don't think that adding another hundred                                 kpi's would be nearly as expensive as                                 simulating the cars and doing the                                 collision detection and I think it will                                 be far far less expensive than rendering                                 the the visualization of the race itself                                 so I don't think that should be a                                 problem but you compress the entire data                                 set in a singular random seed in this                                 case we compress the entire data set                                 into a single race and car configuration                                 file we                                 don't constrain the seed even we could                                 and that is good practice it's often                                 true that you can match your KPIs                                 suppose I have Gaussian distributions if                                 i set the seed i might be able to say                                 big one close little one far another big                                 one far away if i don't constrain the                                 seeds i might get distribution over many                                 many different configurations and i                                 might not match the KPIs the specific                                 kpi's i want so constraining the seed                                 and learning across different seed                                 values might help me match the KPIs in                                 certain situations and I've certainly                                 seen that in this one we have very very                                 loose KPIs and so I don't think that                                 that's necessary I think we do need to                                 add measurements to match the volume and                                 data right kpi's but I don't think we                                 need to do much else ok you is this                                 github what is not available publicly                                 right now it is public so yes it is it's                                 under github.com / there it is it's                                 almost invisible there it is more                                 visible racing time series so yeah                                 clever man I believe there is a time for                                 one more question everyone is looking                                 surely there's more video game players                                 here and that they just don't want to                                 admit it I think they already thinking                                 about this chip that they need to                                 collect in single-threaded q yeah during                                 the months does not necessarily need to                                 be single threaded that's an interesting                                 point it's very common here that you'll                                 have a lot of different delays between                                 different channels so you've already                                 lost ordering between cars of data and                                 the ordering of data within a single car                                 versus another car doesn't entirely make                                 sense except at the level of                                 milliseconds not at the level of                                 microseconds and so would be very                                 natural to partition the stream on car                                 so we would already have quite a bit of                                 parallelism there and then within                                 certain parameters you need to maintain                                 lockstep of the sampling but other slow                                 ones could also be pulled out into                                 different topics so we wouldn't have                                 single threaded I was actually talking                                 about this chips distribution that                                 people will be collecting you know                                 outside of this room yesterday United so                                 maybe next time you will paralyze their                                 task on those two sure we have two                                 partitions for that each of us okay so                                 we add the top of the hour Thank You Ted                                 thank you
YouTube URL: https://www.youtube.com/watch?v=0jeBnTpyzyA


