Title: Berlin Buzzwords 2016: Andrew Clegg - Learning to Rank: where search meets machine learning #bbuzz
Publication date: 2016-06-11
Playlist: Berlin Buzzwords 2016 #bbuzz
Description: 
	Learning to Rank (LTR), once the domain of academic researchers in machine learning and information retrieval, has begun to make great headway in practical applications on the web. Its tools and techniques offer a new way to think about challenges in relevance ranking, personalization, localization, ad targeting and multimedia search, but it shares enough conceptual foundations with traditional search relevance that it’s easy for hands-on engineers to get started with.

In this talk, Andrew will introduce the field and its key concepts, before diving into one of its best-known algorithms, Ranking SVM, which adapts Support Vector Machines to ranking tasks instead of classification problems.

With real examples taken from work at Etsy and elsewhere, he’ll talk about how you can use this algorithm and others like it to incorporate a huge variety of features into your search ranking model: query-specific term weights, implicit user feedback, temporal and geographic data, and even image features. And he’ll touch on applications of LTR beyond traditional search, including ad click prediction and content-based recommendations.

No past experience of machine learning will be required. Attendees can expect to leave this talk with an understanding of LTR and its applications, and enough insight into Ranking SVM to enable them to experiment with ranking models on their own data.

Read more:
https://2016.berlinbuzzwords.de/session/learning-rank-where-search-meets-machine-learning

About Andrew Clegg:
https://2016.berlinbuzzwords.de/users/andrew-clegg

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              hello hi I'm Andy I'm a data scientist                               at Etsy the global marketplace for                               handmade goods vintage goods and arts                               and crafts and it's great to be back at                               Berlin buzzwords and my sound is gone no                               it's okay                               so hi my talks about learning to rank or                               LTR a field which applies the methods of                               machine learning to the challenges of                                search rovings ranking it's quite a big                                topic so I'm hoping to give you an                                introduction along with some pointers to                                more information first i'm going to set                                the scene by introducing the problem                                that ltr' tries to solve then i'll talk                                a bit about how to turn documents or                                other items from your site or product                                into a representation that machine                                learning algorithm can train on and then                                i'll describe how that training process                                works using the example of support                                vector machines or SVM's and then at the                                end i'll talk about some of the issues                                you might encounter around using ltr'                                and production so to introduce the topic                                let's talk quickly about how search                                relevance ranking typically works and                                we'll keep it fairly brief because this                                is buzzwords this is the search track so                                I guess it has probably be familiar to                                many of you so imagine you're running an                                e-commerce site and your items consist                                of product titles or descriptions like                                say purple hand-woven unicorn hair                                sweater                                now when you index these items into a                                search engine like Lucene they're                                represented internally as a collection                                of terms and associated weights based on                                their turn frequencies and their                                document frequencies now when you user                                enters a query the query terms are                                assigned weights based on their document                                frequencies and we can think of the term                                sorry the item in the query as sparse                                term vectors in other words key value                                maps where the keys are terms and the                                values of the weights associated with                                those terms now any terms not present in                                a particular term vector implicitly have                                a value of zero                                so for each item that contains the query                                terms y either passes any filters that                                your search engine is applied the search                                end will calculate the cosine similarity                                between the query and the item if you                                think of the query in the item as                                vectors in a multi-dimensional space                                it's just the cosine of the angle                                between them so then if you sort the                                items by that score in descending order                                that will give you the search ranking                                now this is a huge oversimplification of                                something of how something like Lucene                                actually scores documents under the                                covers but it's close enough for the                                purposes of the example now things get a                                bit more complex                                if each item has multiple fields like                                say a news article where you might have                                title body text and comments the search                                engine will score the query against each                                of those fields separately and then                                combine the scores scaling them by                                whatever constants you provide this lets                                you weight hits against the title more                                or hits against the comment section less                                for example now you might also want to                                modify the relevance score by other                                external factors like the items                                popularity or age or even its proximity                                to the user for example if you're                                ranking                                hotels or restaurants or something like                                that you might even want to multiply the                                relevant score by an arbitrary constant                                here one point five or two point naught                                or something if the user has previously                                favorited that item in order to start                                personalizing the search results                                you probably need to experiment with                                different scaling constants and in fact                                different scaling formulae to actually                                weight the original textual similarity                                against all these other factors so let's                                instead just think of each of these as a                                function of the original score and the                                other signals that you're taking into                                account you know age or distance or a                                 binary flag like user has favorited this                                 item the functions will encapsulate any                                 scaling constants that you've set but                                 then how can we combine all of these                                 different factors into a single overall                                 scoring function in a principled way                                 a function of all of these different                                 signals and the original relevant scores                                 and then how can we keep the individual                                 scaling factors up-to-date without them                                 becoming                                 magic numbers that you end up trusting                                 without ever revisiting or retesting so                                 one way to approach this challenge is to                                 treat search relevance as a machine                                 learning problem and learn this overall                                 ranking function from historical data                                 and this is the essence of learnings                                 ranked so to do this you need to treat                                 each item as a sparse vector of features                                 with weights and this is a                                 generalization of the idea of a term                                 vector the features can include terms in                                 specific fields with their tf-idf weight                                 for example hair appears in the title                                 field and its tf-idf weight is                                         this case but it could also include                                 other attributes of the item or even                                 attributes that depend on the user the                                 query or the context a query time each                                 feature has a value which could be a                                 floating-point number or just a one for                                 present the weighted sum of these                                 feature values gives you a relevant                                 score that you can sort by but whether                                 those feature weights come from so the                                 output of the machine learning process                                 is called a model which is just a weight                                 for every feature found in any item in                                 the training data so it's also just a                                 map from feature names to values                                 I'll talk shortly about how the training                                 process actually finds those weights so                                 notice here that scoring an item against                                 a model by taking the weighted sum of                                 its features is analogous to scoring an                                 item gets to query by taking the cosine                                 similarity of the item in the query                                 before we train the model we need to                                 provide a target ranking and this is                                 sometimes called the ground truth or                                 gold standard so this is the ideal                                 ranking that the model wallet writer                                 will try to approximate and then                                 generalize from now in traditional                                 academic information retrieval research                                 this will be made based on manual                                 relevance labeling by experts                                 but in the online context you can mine                                 your site search logs to build the                                 target ranking from historical data so                                 the key idea of this is that each time a                                 user runs a query and then click some                                 results but ignores others they're                                 providing an implicit signal that some                                 items are more relevant to that query                                 than others are now that's the noisy                                 signal but aggregated over many user                                 sessions it provides a consensus                                 judgment about the comparative relevance                                 to the results that you served for each                                 query during training the learning                                 algorithm will iteratively modify the                                 weights on the features have seen                                 starting generally with random weights                                 and then after each change it will                                 compare the target ranking to the                                 ranking that it would get if it had used                                 the current weights if the ranking isn't                                 correct it will modify the weights in a                                 direction that's likely to improve the                                 ranking and then try again and this                                 continues until the ranking stops                                 improving that's called convergence now                                 this this process is generally these                                 days implemented using an algorithm                                 called gradient descent at least a large                                 scale or some variant of gradient                                 descent again this is a very big                                 simplification but it illustrates the                                 general idea I'm not going to go into                                 the maths of how gradient descent works                                 because that would be you know a whole                                 lecture series in itself now I'm going                                 to talk a little bit about building                                 features to represent items but also to                                 represent queries and other contextual                                 information this is the part of data                                 science which is generally more like an                                 art than a science and we're having good                                 knowledge of your domain comes in and                                 really essential so any attribute that                                 describes an item can be used as a                                 feature now it could include terms in                                 particular fields like the title or                                 description or tags the from a taxonomy                                 that are associated with that item and                                 you could use the tf-idf weights of                                 those items as the values of those                                 features or you could just use one to                                 mean present and leave them out if                                 they're absent I'm not going to give any                                 advice about what works better than what                                 because actually                                 that's very much dependent on your own                                 data sir in your own domain of usage so                                 experimentation is key you can also use                                 metadata like historical click rates or                                 conversion rates or pricing information                                 or if you want to get bit more                                 sophisticated you can pre-process the                                 items we say a topic modeling algorithm                                 like Lda or LSI or a clustering                                 algorithm or even a neural network and                                 then include the outputs of those                                 algorithms as features to represent                                 attributes of the document now some                                 colleagues of mine wrote a paper for                                 this year's kdd conference that                                 describes combining textual features                                 from product descriptions with image                                 features from those products photographs                                 extracted using a convolutional neural                                 network which is called images don't lie                                 transferring deep visual semantic                                 features to large-scale multimodal                                 learning to rank so if you're interested                                 in how that works you can look up that                                 reference online now all of these                                 features just describe the items                                 themselves so far so a ranking model                                 that's learnt using only those features                                 will just provide a global kind of                                 quality or click ability score that can                                 be useful in some settings but usually                                 we want to be able to to rank the items                                 with respect to a particular query                                 because that's what you need to do for a                                 set of search results                                 I want simple ways to calculate the                                 relevance normally and then use the                                 global quality score from your machine                                 learning model as modifier flat but that                                 means you still have to manually decide                                 how to weight those two scores with                                 respect to each other so it's kind of                                 step backwards to where we were at the                                 start another way is to let each                                 training instance describe not just an                                 item but an item in the context of a                                 specific query by adding features that                                 describe the query that was run at the                                 time or the relationship between the                                 query and the item now you can actually                                 incorporate traditional relevant scores                                 like tf-idf or BM                                                       to use more than one of them and let the                                 model work out which is most important                                 at training time and you can also just                                 use features that just                                 the query itself for example linguistic                                 features or some kind of query                                 segmentation or query categorization                                 features and just as an aside here if                                 you have any features that aren't in the                                 range                                                                  so they are because it makes it easier                                 for the model to converge and it also                                 makes it easier to interpret the weights                                 of those models as feature importance                                 later on now the most fine-grained way                                 to model context is to include terms                                 from the queries themselves in these                                 query item interaction vectors by                                 building composite features for each                                 combination of a query term that                                 appeared in the query and some appeared                                 in a field in the item like I've done                                 here this lets the model learn implicit                                 associations between those terms but a                                 very high computational cost you get a                                 very large model with a lot of features                                 in it you could even use this approach                                 to learn a model for recommendations or                                 personalization instead of explicit                                 search queries by including terms                                 representing the page or session context                                 or the users themselves but in general                                 the best way is to train a separate                                 model for each query that appears in                                 your logs or maybe just the top end most                                 common queries or maybe enough queries                                 to cover the majority of searches then                                 you can fall back to using traditional                                 turn-based relevance ranking when a user                                 runs a query that there isn't a model                                 for for the rest of the talk for                                 simplicity I'm just going to assume that                                 we're working in this model in this                                 paradigm with a separate model for each                                 query now I'm going to talk a bit about                                 how to actually train this model using                                 the example of support vector machines                                 or SVM's                                 these are used widely in classification                                 tasks and pretty much any                                 general-purpose machine learning library                                 will come with an SVM implementation if                                 you're just starting out the Python                                 package scikit-learn is a very good                                 place to get going because it has great                                 documentation and is an easy API                                 so first let's take a quick digression                                 to talk about how you train a binary                                 classifier that is a classifier with two                                 classes like a spam detector each                                 training instance in this case consists                                 of a single example so a single email in                                 the spam scenario and a label which will                                 be positive plus one for spam emails and                                 negative minus                                                         some will have to prepare all of your                                 training data by manually tighten those                                 unless you have a way of crowd sourcing                                 now or something                                 SVM's work by treating each feature in                                 the model as a dimension in a space so                                 each training instance corresponds to                                 point in in that space in this image                                 there are only two dimensions because                                 any more than that starts getting very                                 hard to visualize but in reality you                                 might have thousands or even potentially                                 millions of features in the model so                                 thousands or millions of dimensions in                                 our space then the SVM trainer will                                 learn where the best dividing line or                                 hyperplane in the terminology falls to                                 where to place the dividing line so as                                 to best split between positive and                                 negative examples that's cool it's                                 called max margin classification because                                 you can kind of see that that dividing                                 line in the middle the black line could                                 be tilted to the left or to the right                                 slightly and it would still separate the                                 positive and negative examples but                                 there's only one unique place to put it                                 that maximizes the margin between the                                 line and any example on either side so                                 that makes it more robust to outliers                                 then once you've trained the model you                                 can just classify new items as being                                 positive or negative based on which side                                 of that line they fall on and yes this                                 is also a very simplified explanation                                 now you can think of the feature weights                                 and the modelers defining another line                                 which is perpendicular to that boundary                                 line                                 but how can we use this same approach                                 for ranking well in the ranking context                                 we don't just want each example to be                                 classified as positive or negative with                                 respect to the boundary line                                 we actually want to know how far along                                 this perpendicular line each example                                 Falls how positive or how negative it is                                 so number one is the most positive and                                 this one down in the far right corner                                 that I have numbered is the most                                 negative and you can see how we've kind                                 of ignored the other dimension of the                                 model projected those items onto the                                 line and we're just numbering them in                                 order where they fall along that line so                                 the the approach that I'm going to                                 describe to train an SVM to do this was                                 first presented in                                                    called optimizing search engines using                                 click-through data so it's it's kind of                                 old now this isn't state-of-the-art as                                 learning to rank goes but it's fairly                                 easy to understand fairly seminal paper                                 and also scales quite well so the way we                                 approach this problem relies on a like a                                 cunning bit of pre-processing before you                                 start training the model instead of                                 training the SVM learner on individual                                 items as you would with the classifier                                 you train it on pairs of items where a                                 user expressed a preference for one item                                 over the other the model then learns to                                 mimic those pairwise preference                                 judgments and generalize from them to                                 new data essentially it learns to                                 classify pairs of items as either                                 correctly or incorrectly ordered based                                 on what users have done in the past on                                 your side so because SVM two classifiers                                 we still need to provide a class label                                 this is plus one if the user preferred                                 item one to item                                                        correctly ordered with regard to a                                 specific search session and it's minus                                   if the user preferred item                                             so they're incorrectly ordered we                                 sometimes call the members of each pair                                 the winner or the loser depending on                                 which one the user preferred in that                                 particular                                 action in your logs before we start                                 training we need to convert those pairs                                 of item vectors into a single vector                                 that describes the differences between                                 the two items so how do you define the                                 difference between two vectors it's easy                                 you just subtract one vector from the                                 other so say here we've got item                                       the features that make it up and we've                                 got item                                                              that up and we subtract item                                                                                                               describes the differences between those                                 two items you maritally remember that                                 any missing features are implicitly                                      I've shown those in gray any features                                 with the same value in item                                              for example title purple here will be                                   in the resulting vector so you can just                                 leave them out intuitively that makes                                 sense because if if the loop winner and                                 loser of it is of a preference decision                                 both contain the same feature or if it's                                 a constant or if it's a continuous                                 feature have the same value then it                                 means that the learner can't learn                                 anything about that feature from that                                 particular preference decision so when                                 you pass a training instance with a                                 positive label to the learner you're                                 telling it to learn that those feature                                 differences are associated with item                                   being preferred to item                                              switch the sign up and provide the same                                 training instance with a negative label                                 you tell it that the future differences                                 are associated with item                                             first item                                                             do is just randomly train on half of                                 their half of those pairs in the correct                                 order and half those pairs in the                                 incorrect order and then the the learner                                 can learn positive and negative                                 associations between features and                                 preference decisions now even though                                 you've trained the model on differences                                 between items you still need to apply it                                 to individual items to obtain a ranking                                 score for each one so that what that                                 means intuitively is that an item score                                 should be positively affected by having                                 features that are often found in the                                 winner of a preference decision                                 because they'll get a positive score in                                 the weights in the model and we                                 negatively affected by having features                                 they're often found in the loser                                 preference decision because they'll have                                 a negative way in the model so you get                                 the feature values of each item in a set                                 of search results and you calculate                                 their weighted sum using the weights                                 from the model that's where we started                                 and then you just order the results in                                 descending order of those scores and                                 that gives you the ranking that you need                                 so as I said it's it's not exactly a                                 state of the art method there are                                 machine learning methods that are                                 specific for learning to rank what they                                 actually take into account the the total                                 ordering of the list for example rather                                 than just individual pairwise                                 rearrangements but you know it's a it's                                 a seminal method it's a good place to                                 start and it scales well especially if                                 you're training separate models for each                                 query in parallel there's a good paper                                 from Google called large-scale learning                                 to rank which demonstrates how to do                                 that at Google scales okay so before we                                 finish let's talk a bit about some                                 practical issues that you might want to                                 consider if you're using ltr' and                                 production first off you need to think                                 about when and where to calculate the                                 ranking scores for the items so the                                 models going to be trained offline                                 probably using a batch process or it                                 could be trained you iteratively using a                                 stream of new data if you've got                                 streaming data ingestion especially if                                 you're using gradient descent because                                 that's a training method which works                                 very well with iterative updates as new                                 data comes in but you still need to                                 apply the model to individual items to                                 get their ranking scores as in perform                                 that weighted sum operation the simplest                                 way in engineering terms is to also                                 apply the model offline in something                                 like a dupe or some other scheduled task                                 or near line if if you have heard of                                 that latest buzzword in something like                                 spark so you're you're keeping your rank                                 is up to date close to when new data                                 arrives so in other words what you do in                                 this scenario is build a static pre                                 ranked result set for for each query                                 that you want to build a model for you                                 know that could be your top                                        queries or something like that but it                                 might not be computationally feasible to                                 do that for every item under every query                                 because you know that could be a lot of                                 query item pairs another option is to                                 calculate the ranking score for each                                 item at query time after you retrieve a                                 set of items matching the query from                                 your search engine so this you need to                                 catch all the model weights in memory                                 and in the search server or in some                                 separate key value store or something                                 and then bills or retrieve the feature                                 vectors for each item for a search or                                 database query or some other key value                                 store lookup and then if you're using                                 contextual features from the query or                                 the pager context or whatever then the                                 application layer could kind of fill                                 those in concatenate them with the with                                 the feature describing each individual                                 item and then you perform the the                                 weighted sum between the model and each                                 item on the fly and then sort by that                                 result and then of course you can cache                                 those those scores in the context of the                                 query because you know any contextual                                 features that you add these ones on the                                 left there you wouldn't want to ignore                                 those when you're caching scores that                                 result from from doing that score                                 calculation on several items                                 now that can be very expensive to do on                                 the fly for every single item especially                                 for queries that have very broad                                 coverage so quite a popular compromise                                 is to initially rank the results by the                                 term based relevant score that your                                 search engine gives you and then to rear                                 ank only the top k results using the                                 dynamic method from the previous slide                                 so you can adjust the value of K up and                                 down to as a trade-off between cost and                                 ranking accuracy there's a talk at                                 leucine revolution last year called                                 learning to rank in solar which showed                                 how to do this with a solar plugin so                                 here's an example of how that works when                                 K equals                                                              left is based on leucine relevant score                                 for each item and then you calculate the                                 SVM ranking school for the top five hits                                 using that weighted sum against the                                 machine learning model and then yuri                                 ranked those by that score and then the                                 ordering of any items from position six                                 and below is unaffected you just use the                                 original ranking for those this is                                 actually a general kind of top case                                 strategy there are that you can perform                                 these days using using using plugins of                                 any kind they don't have to be machine                                 learning if you go any other kind of                                 like complex scoring function that you                                 just want to apply to the top few                                 results then this is a good strategy in                                 general because quite often people don't                                 look past the top few results anyway so                                 finally it's important to make sure that                                 you're solving the right problem here                                 the model that you train will learn to                                 approximate any target ranking that you                                 give it given enough data so you have to                                 make sure that that target ranking                                 reflects what you want to achieve                                 reflects you all your business problems                                 one aspect of that is avoiding feedback                                 loops and so-called filter bubbles you                                 know if you're learning if you're                                 building a target ranking and then                                 presenting search results based on that                                 and then using clicks on those same                                 search results to improve the the model                                 and so on and so on                                 can see how that gets kind of circular                                 it introduces a feedback loop so you                                 have to be careful not to train your                                 model to just recreate existing rankings                                 one aspect of that is you need to make                                 sure that you give new content a chance                                 to actually earn some clicks so you get                                 some signal about how popular or                                 unpopular it is so you can do that by                                 introducing some level of randomization                                 into search results like artificially                                 promoting items to see see whether                                 anyone bites if you like you could also                                 to avoid the feedback loop you could                                 train the model on one product feature                                 like your search page and then use the                                 model to to perform ranking for a                                 totally different feature like maybe                                 your insight ad placements or something                                 like that                                 promoted listings if you're doing some                                 kind of product search because then                                 there's there's no feedback loop you're                                 learning here and applying the model                                 there you also need to consider the                                 effects of position bias in the training                                 data so that's the tendency for users to                                 click on items just because they appear                                 higher in the results there are a bunch                                 of different strategies for mitigating                                 this and they all kind of have pros and                                 cons and you know you could do a talk                                 just on how to how to do those in                                 general so I would recommend that if                                 you're interested in pursuing this in                                 production then talk to your data                                 science team because they probably will                                 have some good ideas about how to remove                                 remove those sources of bias or at least                                 account for them or correct for them                                 when you apply the model and good a/b                                 testing is absolutely essential so some                                 of these will work better in some                                 contexts than others                                 I mentioned that you need to ensure that                                 the target ranking matches the behavior                                 that you're trying to promote so ranking                                 by click popularity alone might be fine                                 for ad placement if you get paid by the                                 click but in search results a click                                 alone isn't necessarily a guarantee of                                 relevance you know a user might                                 click-through to something and then                                 immediately click back or they might                                 click-through to something and then                                 immediately leave your site so you might                                 want to actually filter the click locks                                 to disregard those click backs and those                                 bounces you can also take other signals                                 apart from just clicks into account if                                 you want to try and promote behavior of                                 different kinds so an example of that                                 would be you might want to build a                                 target ranking which places articles                                 that were often shared at the top then                                 articles that were less shared in                                 articles which were often read and to                                 end but not you know not necessarily                                 shared and then below that articles                                 which were often clicked but not                                 necessarily read end to end so you have                                 different bands that might interleave                                 slightly at the edges and then you're                                 the model that you train based on that                                 target ranking will learn to present new                                 content eventually based on its                                 likelihood of resulting in those actions                                 so we're kind of getting towards the end                                 I know it's been a very very brief                                 overview with a lot of different ideas                                 in it but hopefully a useful                                 introduction to the field I know                                 everyone's probably starting to slump a                                 bit as it's getting close to lunchtime                                 if you want to learn more about how                                 different approaches and different                                 learning algorithms work in this context                                 it's worth checking out this book                                 learning to rank for information                                 retrieval which is very comprehensive                                 survey of the various different                                 approaches and the pros and cons of each                                 one and what what again it doesn't                                 actually cover in detail is the feature                                 engineering side of things like how to                                 choose which features work best for your                                 data because everybody's data nobody                                 uses very different so that's something                                 that you can only really arrive at                                 through a little bit of trial and error                                 at the end of the day you can put pretty                                 much any feature into a model and if you                                 configure the learner properly again to                                 beat your data scientists about making                                 sure you do that right then it should                                 learn to just ignore ones which are no                                 use at all                                 so I'm around for the rest today so feel                                 free to come and find me and if you want                                 to talk about any of this in more detail                                 also just as a final note we're looking                                 for engineers at the moment in data                                 science data engineering search ranking                                 and search infrastructure so if you're                                 interested in problems like these then                                 then please get in touch                                 thank you in the front we have some time                                 for questions hello thanks for the nice                                 talk I have a couple of questions sure                                 you said that your training on pairwise                                 training examples so this means you're                                 actually learning a preference function                                 however then how do you use this                                 preference function to extract features                                 per item and rank them it's it's kind of                                 hard to get your head round but it took                                 me a while for this to find it sink in                                 but because the the model will learn                                 positive weights for features that tend                                 to appear as the winner of a parent so                                 literally positive as in greater than                                 zero and it will learn negative weights                                 for features that tend to appear in the                                 loser of a pair so if you kind of                                 average that out across all of the pairs                                 that you see in your data then when you                                 get a new item if you just do the the                                 dot product the weighted sum between the                                 items features and the features that                                 your model has learned it will push                                 items that share features with typical                                 winners higher in the list and it will                                 push items that share features with                                 typical losers lower down the list                                 because the winners features tend be                                 positive the loose ones tend to be                                 negative and there's a proof that I                                 think was in one of the papers that I                                 cited that eventually given enough data                                 the                                 learn to reconstruct the exact ranking                                 that you fed into it based on those                                 pairs in real life you might not                                 necessarily want to use every single                                 pair of results from every single search                                 because you know say you present                                    items and the user clicks on one then                                 each search session gives you                                        know different pairs then actually then                                 that could actually cause a big                                 explosion in the map states you have to                                 train on so a large scale for something                                 that Google paper people actually sample                                 from pairs but also you know given                                 enough given an unbiased sampling                                 eventually the ranking will converge on                                 the original target ranking okay thank                                 you my second question I saw in your                                 input sets you have conversion rate for                                 items however this is not an observable                                 variable to the user by the time he does                                 his search query so how do you think                                 this is a relevant feature for learning                                 well it can act as a proxy for other                                 features that you haven't explicitly                                 modeled so for example maybe you haven't                                 put in price or distance to the user as                                 a feature but maybe your users actually                                 often make a purchase decision based on                                 price or distance then by putting in                                 conversion rate then you can that will                                 actually act as a kind of noisy proxy                                 for that in the model it'll capture any                                 missing things that might be important                                 what can do thank you very much my final                                 question how do you calculate your                                 precision and call recall so typically                                 for for assessing these models you would                                 use something other than precision and                                 recall because they they assume a kind                                 of hard cutoff between relevant and not                                 relevant but there are some other                                 evaluation metrics for example a typical                                 one is called normalized discounted                                 cumulative gain or MD CG                                 which looks at the actual rank order of                                 the hits and gives more results two hits                                 but yeah more weight two hits near at                                 the top so it means it's more important                                 for the model to reconstruct the the                                 upper part of the search results                                 correctly so again this this book also                                 covers different evaluation metrics and                                 the pros and cons and how to fit those                                 two different business needs and that                                 kind of thing thank you very much                                 are there further questions we have time                                 for                                                                     how do you handle missing data when you                                 have distraction of the pair how do you                                 handle that for example in one result                                 you have a lot a very long feature set                                 and yet one most of the features are                                 missing well I'm I'm not sure what how                                 you would get missing features in this                                 instance because if it's your your own                                 inventory on your website and you're                                 using features like you know text or                                 other tokens category oh right yes yeah                                 so I mean it usually you can just kind                                 of leave those out it depends on what                                 the what the proportion of you know                                 correctly completed items there are and                                 once with missing data I mean obviously                                 the the more the more cases of missing                                 data you encounter the harder it will be                                 overall for the model to use that                                 feature in a meaningful way but I mean                                 the advantage of using a wide variety of                                 different kinds of features is that the                                 individual weights on each of them tend                                 to be quite small so even if you do have                                 items that are missing a particular                                 feature and just leaving those out of                                 the term Veck of the feature vector for                                 that item isn't necessarily a                                 showstopper so could you give us thank                                 you for the dog could                                 give us some rough estimate of how                                 better is using re-ranking after a                                 tf-idf or this is what I got from your                                 tablet you're actually using leucine or                                 some search engine first and then you do                                 some rewriting so in or adduction or I                                 would like to know a rough number of how                                 better it is so two things to say one is                                 that none of the things I've described                                 are actually in production at Etsy yet                                 but if you look at that paper from kdd                                 this year the images don't lie one                                 that's got some numbers on the uplift in                                 the mdc-t metric that i just mentioned                                 by using image features as well as text                                 features but I can't remember off the                                 top of my head all that is but the thing                                 is I mean these things are all very very                                 dependent on the particular data set                                 that you use and this is why in academic                                 IR research people publish standardized                                 data sets and everybody tests against                                 the same one because quite frequently                                 the results you get on one will not be                                 in any way sort of good predictors of                                 the differences and results of get not                                 on another one the features will all be                                 different the amount of missing data                                 however will be different and I've                                 showed you that you have will be                                 different so yeah I would steer away                                 from kind of saying expect a                                             or expect that a                                                         like that because it really does depend                                 on your domain in your data okay let's                                 think endru again thank you
YouTube URL: https://www.youtube.com/watch?v=dKppAG0cdkM


