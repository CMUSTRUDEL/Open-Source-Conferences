Title: Berlin Buzzwords 2016: Frank Conrad & Ranbir Chawla - The world wide 60B transaction per day journey
Publication date: 2016-06-12
Playlist: Berlin Buzzwords 2016 #bbuzz
Description: 
	A case study on how we grew the AudienceScience Helios programmatic advertising management system to more than 1 Million transactions per second through our entire data pipeline including Kafka, Storm and Hadoop.

The AudienceScience Helios system acts as:
A real time bidding client to all major exchanges (more than 70 integrations world wide) A service for direct integration with publisher’s website / ad server A engine data collection and management of billions of users The system runs world wide across 6 global data centers.

We’ll share what we learned along our journey and how we are moving to even greater scale in the Cloud for 2016.

Read more:
https://2016.berlinbuzzwords.de/session/world-wide-60-billion-transaction-day-journey

About Frank Conrad:
https://2016.berlinbuzzwords.de/users/frank-conrad

About Ranbir Chawla:
https://2016.berlinbuzzwords.de/users/ranbir-chawla

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              okay good well we're not doing a demo                               today so we got all the technical                               difficulties out at the beginning my                               name is Rob your towel on the vp of                               engineering at audience science yeah I'm                               Francona dimes a sweet tea file she                               ticked off audience signs and a little                               bit of background on audience science or                               a fully integrated end-to-end                                advertising solution for some of the                                world's largest digital advertisers we                                process and respond to                                           advertising requests a day and we do                                this in over                                                       including China our solution allows                                advertisers to effectively manage and                                leverage their consumer data to produce                                industry-leading roi we have five points                                and presents two in the United States                                one in Europe to in a pack yep when we                                wanted to talk to you a little bit about                                today was just the challenges of doing                                what we do at such a large scale I'll                                hand it over to Frank yeah and basically                                we started                                                           now                                                                 forward and we had a lot of challenges                                with it and it's really about the                                details if you're if you look to a                                Formula One car it's every screw matters                                if you do the wrong stuff in the engine                                will die or something like that and it's                                really the things about that every step                                must must and must be working and you                                are at the starting time always looks to                                the complex problem to the heavy things                                work what you have to do but the things                                what will kill you probably are the more                                trivial stuff more basic stuff or things                                which you start thinking about later too                                late and so sorry                                the things are what I really want to say                                you have to start with the right data                                model what is the things what you do you                                have to think about as the scale you                                need does does your data model allows                                the sufficient parallelism to do this                                store store your data in a way that you                                can effectively use it many times you                                start our stores the data in a simple                                way but then the usage get very very                                expensive and you have a lot of problems                                with it and another thing of what you                                really need to care about is your out                                layers so the standard stuff typically                                you get relatively good done what you                                kill you is your out layers users which                                have ten thousand impressions for                                whatever reasons or your data records                                get huge for only a couple of users but                                this this big users basically killed                                kill your process because it needs too                                much memory and then then your jvm dies                                are you have a stream processing and                                certain users take too much cpu times                                and because of the synchronization what                                you internally have it delays everything                                so such a things is is really really the                                stuff at productions what you need to                                care care about and then also how to                                design things think about how you add                                things on on top of it because sing                                requirements from from production from a                                product will become put more and more                                stuff on it and if you not think a                                little bit ahead you get at that scale                                really really problems and then what                                many guys also forget it's easy to                                create data or it's a challenge to                                create                                but it's a much bigger challenge to                                delete and expire data so so a lot of                                databases what you have for example                                user-oriented stuff and so on if you do                                a data expiring not by design in a in a                                good way you will really suffer this                                cleanup jobs which runs over your                                database will kill your performance so                                so basically this is a very big learning                                that in in any data storage what you                                design dancing don't forget about                                deletion of data if you not have                                naturally time time-oriented data then                                it's easy but everything which is                                user-oriented or campaign oriented or                                whatever as a witch don't have a real                                time component in in it it gets                                problematic and also the important thing                                is one on scale as I said said before                                 you have to have the right parallelism                                 otherwise you will not get the amount of                                 performance you need about dynamic                                 parallelism think about dynamic                                 parallelism what I mean the set is that                                 you can scale up and scale down your                                 your your runtime to the need what you                                 have specially if you leverage the cloud                                 or something like this it can start more                                 service to your rush hour and low others                                 down later again but if you don't have a                                 designed in an interior system it will                                 not work and you can't really leverage                                 it think about really do as an Crone                                 processing for everything what you                                 couldn't where else is possible what you                                 don't have to do in in in your real-time                                 response for example to to a bit request                                 or to request which comes from the user                                 browser don't do it do it later then                                 Zhen Zhen really look tools the needed                                 latency and throughput because you can                                 design systems with a low latency but                                 and and don't have a high throughput and                                 visa versa so you have to find you                                 needed compromise for that then another                                 thing really which do need to think                                 about is eventual consistency this                                 concept can you help you a lot in a huge                                 distributed way we we have as as we said                                 we have five data centers and in a world                                 where we receive all the bit requests                                 and user requests and if you want to                                 have some consistency this is something                                 which producing huge huge effort and has                                 done so much value if you think about a                                 little bit more eventual consistency you                                 can solve a lot of problems but with                                 much less resources then the other piece                                 in in your design is really think about                                 production-oriented this means have some                                 dynamic limiting so if if you get more                                 traffic or if your new code release has                                 some problems or other things that you                                 have a chance to limit the incoming                                 requests do some straddling do some                                 operations on ten percent of C requests                                 is better than do nothing because your                                 complete system get overloaded and do                                 nothing's in it anymore so basically                                 that is a big thing which helps you to                                 survive and in certain situations think                                 about how you can good do a good catch                                 up so if you have a stream processing                                 and it goes behind because of                                 performance problems of hardware issues                                 of whatever that you have ways of                                 efficient ketchup processing which needs                                 earth to you need to care about it's                                 also how to handle unreliable networks                                 so if you transfer data from one data                                 center to the other you will have                                 sometimes the problems that then your                                 network doesn't                                 doesn't work as your local network at                                 home in a good ways that you have packet                                 loss that you get unexpected Layton sees                                 things and so on how you mitigate them                                 in into production everything in your                                 test environment will always work well                                 because you don't have then typically                                 network problems the network problems                                 comes in in the real world and                                 especially if you have to go to China                                 and have the great firewall between that                                 that is really something it a which can                                 cause any possible network issue what                                 you can think about and there will be                                 happened because of Murphy and this is                                 basically also the next piece in another                                 direction of your heart where's your                                 service think about that you have not i                                 equal service you have some which have                                                                                                       cause they have a little bit more                                 memories a slightly different cpu                                 performance characteristics that your                                 system still can can completely leverage                                 them and not too slow service dear                                 slower server in your cluster dictator                                 the overall performance of the system                                 it's easily to achieve such things if                                 you do not not look too then as i said                                 before it's really as the things about                                 the out layers in terms of compute power                                 what you need for the certain requests                                 memory things and also look up time into                                 databases or stores or what you have to                                 do and see really most important stuff                                 for production-oriented is monitor                                 monitor monitor design in your                                 application that you can monitor all all                                 the critical values have some available                                 to graph anna or something like this as                                 well as you can leverage to a monitoring                                 system like nagios where you can check                                 certain thresholds and and to do                                 alerting to it that is really something                                 well like what I can must recommend and                                 typically the development is forget the                                 developers forget about monitoring but                                 if you don't do it right you fly blind                                 and the things what we do in end in                                 monitoring is we use a simple tool                                 called called nagios for that it's very                                 known and and the reason why we use it                                 is it's easy to script and it's easy to                                 version control because if you go in a                                 larger deployment where you have a lot                                 of servers a lot of service instances                                 which you need to check and you have to                                 check every instance not a general one                                 general to the load balancer you have to                                 check every bitter behind the load                                 balancers that they're behaving as you                                 expected because because Murphy said                                 that one JVM or one server get a problem                                 and not all all all at the same same                                 same time and that you can achieve this                                 also on a larger scale is really neat                                 that the configuration is done in a                                 completely automated way in a also part                                 of your deployment you have to see it as                                 a part of your deployment of your total                                 stuff and as I said that Hardware can                                 can really result that your workflows in                                 a cluster or your bitter response time                                 get really bad and a partners like                                 Google they're looking to the                                 ninety-five percent percentile offer its                                 once time and if you are not good in                                 that                                                                 powder on and it is basically a                                 functional monitoring so many many times                                 guys using check proc and sees as a                                 process which chooses drop so everything                                 is fine this is something which is                                 typically by far                                 not sufficient you should really do                                 implement functional checks implement a                                 kind of l                                                          simulate something at the front end and                                 basically looking on every step on the                                 whole chain until reporting everything                                 that this data is showing up because was                                 that you can make sure that your whole                                 pipeline is working and you have to do                                 this really on every instance because if                                 one bidder or                                                           there's their data records to the                                 reporting system you have some number                                 discrepancies and you can only figure                                 that out if you're if you have a check                                 against each instance of your sort of                                 your software what you are running and                                 with end-to-end monitoring you do this                                 you will see a lot of more problems                                 earlier earlier as the customer see it                                 if you make it properly you know the                                 problems before the customer okay now                                 now now we go a little bit to the dupe                                 staff as a as busy we reprocess huge                                 amount of data and how to do it right                                 now or how we do it right now is really                                 mainly MapReduce jobs which we are                                 running two processes we have started a                                 couple of years ago with                                                now we are more than                                                  running them right now based on a cloud                                 era infrastructure and the thing is                                 where are the pieces what we are learned                                 learned learns there is really do as                                 much as possible in the mapper face from                                 from from the work minimize the shuffle                                 data even if you have a very good                                 Network                                                         so this this stuff we're mapreduce get                                 can easily get inefficient and this is                                 one of the things why spark and many                                 times can be more more efficient because                                 they do a clever way a better way in in                                 traveling data around then then it has a                                 big piece is the output of the drops                                 must be in a format and in a in a way                                 that also optimal for the next input so                                 it doesn't fit that you're your actual                                 job running well and produce a huge                                 amount of small output files which end                                 up sets the next job who's consuming                                 this get a lot of mappers and you get                                 there z.z inefficiencies and if you get                                 more and more nodes many guys say oh                                 have one big clusters and everything                                 runs faster and every we run everything                                 on one cluster it it's theoretically a                                 good thing in a practice it's not so                                 good because if this class i get for                                 whatever reasons a problem you don't you                                 don't have any cluster also if you want                                 to upgrade things can you upgrade your                                 whole software at one point typically                                 not typically you have a group                                 immigration process if you have mitral                                 the stuff already split it across                                 multiple clusters it helps you to to do                                 this operational things in a in a much                                 easier way what you should care about is                                 that you can relatively easy move a note                                 from one class I into the other things                                 that the networking isn't this very                                 transparent to to do such things as i                                 said the optimization on on just drops                                 itself is the runtime of each caste                                 should be in a in a reason of a couple                                 of minutes                                                            good optimum                                 the number of meta mapas entering                                 reducers must be really another contest                                 eration is how much shuffle you need                                 does this get into a problem as then you                                 have to do their redesign stuff as as                                 well as the size and the amount of                                 output files then another thing what you                                 sometimes will see on the cluster that                                 four get a huge load that so you load on                                 the service goes two hundred or even                                 more even higher this is see a good                                 indicator that you have a GC problem                                 that as your GC runs crazy and this is                                 one thing what is relatively difficult                                 to monitor and on the Hadoop classes                                 that the GC is is a problem and it will                                 be have a huge effect to Z huge effect                                 to the total run time of of your tasks                                 typically you should really enables                                 speculative task execution this will                                 help you especially on hardware problems                                 a lot to make the things but not on s                                  rights there you get a huge problem to                                 do this then then another important                                 learning is a create file system                                 instance for every input and output what                                 you do separately otherwise you are                                 limited to one kind of file system so if                                 you run in Amazon or Google Cloud and                                 have someone HDFS someone on the objects                                 toward you need to have separated                                 otherwise you get a huge amount of error                                 messages and your drop don't run good                                 now                                 and talk a little bit also about how we                                 use storm use storm to do a lot of our                                 real-time processing for user                                 segmentation so user information will                                 come in and we still stream directly                                 from Kafka to storm into Cassandra and                                 into Hadoop and we found a couple of                                 things you know that we've learned over                                 time right really only group things if                                 you need to within the storm topology                                 right if you're processing time for the                                 if you're a few streams and the                                 processing time have a large                                 distribution so some pieces that you're                                 going to go for so if you if you in your                                 topology or if in your streaming you go                                 out to an external system to gather                                 information about a step within that                                 stream and you can have a large                                 distribution of response time right that                                 will slow your whole topology down so if                                 you're going to get external data look                                 for an in-memory data store like                                 Baltimore it's something that you can                                 quickly get with a with a normalized                                 response time that you can kind of count                                 on within a very short window of time                                 also minimize again shuffling right this                                 is the same problem whether you're in                                 spark cassandra hadoop storm any of                                 these things right keep the data where                                 the data is to the absolute maximum that                                 you can and same message all the time                                 right it's a java-based process don't                                 create any extra garbage that you don't                                 need think it through a lot right when                                 you get to this massive amount of scale                                 the less you can create the absolute                                 better right there's no amount of GC                                 tuning that makes up for                                               these a day if you're being sloppy and                                 it's just it's the little things in here                                 that make all the difference so as I                                 mentioned we use Baltimore it as a large                                 scale ski store right we use both in                                 memory Baltimore and we also use the                                 read-only stores right the in memory of                                 all the more it's very stable it scales                                 well it's really performant but it's                                 really hard to monitor what's in memory                                 right so you have to be careful make                                 sure that the input is quality because                                 we don't have quality input it's hard to                                 go back and debug data that you're                                 getting out of that data store later the                                 read only stores the file based                                 read-only stores are extremely efficient                                 we can produce some of MapReduce very                                 quickly                                 swap those in it scales well it's very                                 performant it's very very stable we use                                 the newest Baltimore client which is                                 doing handles failure and edge cases a                                 lot better earlier versions we've had                                 issues with in the past one of our                                 challenges has been how to swap in the                                 data from the big rhian Olynyk stores                                 right how to move tremendous amount of                                 data into a read-only store versus you                                 know what was previously there we've                                 done it a number of different ways just                                 to let you know one of the things we're                                 trying now is removing some of these to                                 the cloud to an M amazon environment is                                 literally using block storage in Amazon                                 and loading an EBS volume with the new                                 data store right and basically                                 unmounting the old one mounting the new                                 one restarting voldemort and using the                                 cloud to its advantage right for the                                 first time in that particular scenario                                 you actually can change the whole file                                 system out so the download to that other                                 EBS volume might happen on a smaller                                 instance off to the side you can get it                                 all ready to go and swap it in                                 milliseconds it works much faster yeah                                 hmm ok just ya Kefka ok we use Kafka as                                 our main pipeline delivery system right                                 given how much traffic we use we're                                 fairly conservative with this so we                                 haven't moved up to Kafka nine yet but                                 what this is a great lesson from a                                 scaling perspective right my message to                                 everyone is don't design your software                                 around your hardware right or configure                                 your software based around your hardware                                 so this originally the pipeline was                                 designed to run on a bunch of dell                                 servers that had capacity for a massive                                 number of disk drives in them right                                 which resulted in brokers it had large                                 amounts of data                                                          of data on one kafka broker right that                                 sounds great you can pack them in you                                 can get a lot of stuff through the rack                                 in and out the only problem is when one                                 of them dies you're now replicating and                                 trying to fix                                                            right so it turns out that just because                                 you have the hardware to do something                                 its scale doesn't necessarily make it                                 the best way to do it it's actually                                 turned out better for us to start                                 building them as smaller more dynamic                                 brokers many more of them smaller amount                                 of drive if you lose something you lose                                 so much smaller percentage replication                                 and rebalancing actually turns out to be                                 much better so just a quick less                                 learn from us on that one again you know                                 we were going up                                                     more efficient and are from we haven't                                 tried Kafka streaming or anything else                                 like that yet we use the mirror maker                                 requires a lot of tuning and                                 experimentation from our perspective                                 right uses a lot of memory but it does                                 work but it does we are advice to you is                                 is definitely be in a tuning form with                                 that and be ready to play with that                                 we've moved now to using the mirrors                                 within docker containers and we'll talk                                 a little bit more about how we run                                 docker and our organization but that                                 gives us the ability to their stateless                                 so it gives us the ability to fire them                                 up and go quickly you want to talk about                                 this on droid you already did okay I'll                                 go ahead so just some lessons learned                                 with the scaling Cassandra as well we                                 run this for a number of different                                 things we run it for some time series                                 database which I'll talk about in near                                 the end and we also use it as a key                                 store in a user store cross data center                                 replication and our experience needs a                                 very very solid network and very focused                                 deployment right repairing across                                 worldwide data center replications can                                 be quite challenging we've actually                                 moved away from that we've actually                                 moved to a much more eventual                                 consistency type model just based on how                                 we're growing as a company in some of                                 the places we're having a network it                                 just didn't work for us again as Frank                                 said this is a real key thing here                                 optimize your data model to avoid                                 deletes if you can find a natural way                                 for Cassandra to delete your data do                                 that it's much better and it's in it                                 compacts a lot better with the different                                 compaction strategies than physically                                 deleting data it works wonderfully for                                 time series data because of that right                                 if you can use the time series and the                                 TTL is to naturally reduce your data                                 you'll win a lot again removing an                                 unnecessary cleanup just try not to do                                 that in Cassandra key value mapping if                                 the key and the value of very similar                                 size right it's very inefficient needs                                 specific tuning we found Baltimore works                                 better in situations like that and the                                 number one thing for that our learning                                 is no matter what SSD you get to put in                                 Cassandra make sure the trim works make                                 sure your controller card works do some                                 bench testing on your hardware before                                 you try to do this scale                                 so we use and we've moved to measles                                 marathon for our micro services or                                 architecture and our in our points of                                 distribution and our outside data                                 centers for our bidders ad servers so we                                 run docker with measles marathon we find                                 that works really well for us it means                                 this is really good at finding just that                                 little extra spot to put just that extra                                 little docker container compared to                                 running VMS and having to shrink or                                 expand or try to find the the optimum vm                                 layout within a data center right                                 measles provides us a lot more                                 flexibility scaling across a lot of                                 heterogeneous equipment as Frank said                                 right you very often find over time that                                 you've built up one machine in a iraq                                 that has                                                               right means this is great for finding                                 ways to fit everything in there it's                                 also allowed us to do some other                                 experimentations for example instead of                                 having a heterogeneous Bank of two three                                 four hundred bidders talking to every ad                                 provider out there we can actually make                                 groupings now and have a set of docker                                 containers scaled for Google and another                                 set of docker container scaled for                                 library land another scale for a                                 different provider and with marathon                                 it's very very easy for us to just say                                 hey we're getting pounded by googled                                 this week for this campaign just make                                 this bigger right and we can actually                                 shut off the ones that aren't we still                                 don't do that automatically we still do                                 that with human control but it provides                                 tremendous flexibility there's some                                 hints to about using docker here in this                                 kind of environment right never try to                                 do when you're in a big you know three                                 four or five hundred containers even                                     containers don't do them all at once                                 when you have an orchestration mechanism                                 whether it's warm weather it's measles                                 do ten percent do twenty percent do                                 thirty percent give the poor docker                                 registry a chance to recover from                                 everybody hitting it all at once right                                 you'll find much more smoother                                 deployments and it gives you a great                                 opportunity to do canary deployments as                                 well do ten percent double check it if                                 it's good to another ten do another ten                                 to another ten clean up make sure you                                 get rid of your old instances right make                                 sure you get rid of old instance data                                 make sure you've got a logging strategy                                 that works right where your logs are you                                 going to use this log you're not going                                 to use syslog just don't let your                                 instance your docker container hosts                                 fill up with log files to come a mistake                                 and we've had that happen to us in the                                 minimize your doctor images you cows                                 have all heard this right make them as                                 small as possible but don't leave off                                 some of the core tools in our experience                                 this is our opinion right if you need to                                 be able to make you more apt get in your                                 docker container be able to go and get                                 that one tool you need right now on that                                 one container and production for                                 debugging allow yourself to do that lots                                 of people say you're not going to debug                                 in production and the reality is                                 sometimes you are so allow yourself the                                 ability to do that just makes your life                                 a lot easier two in the morning and                                 let's talk a little bit about some our                                 move to the cloud so right now we                                 started this off as a                                                  in a fixed data center right and you                                 have some beautiful advantages when you                                 do that right you have a networking team                                 that builds you                                                         absolutely positively screams and the                                 data works beautifully and everything                                 works but you have                                                       that you need                                                            to have them right and so everybody                                 stacks up and things stack up so you                                 don't have the elasticity so you move to                                 the cloud okay and that sounds great and                                 everything's wonderful and it's going to                                 be elastic but there's gotchas there too                                 you've lost that                                                     you've lost that network team that you                                 can call in the middle of the night to                                 find out why you're having trouble                                 you've lost sometimes the capacity to                                 scale the number of instances you think                                 you have aren't always there also object                                 store versus HDFS the two different                                 animals be very careful as you start                                 using object stores you're going to want                                 to make sure that you don't go back and                                 forth if you have multiple jobs and                                 you're going to read from s                                             do local HDFS for your temporary data                                 right s                                                               take a lot longer than local file to                                 write so you'll slow yourself down grab                                 your data do all your processing at the                                 end of an extended workflow write it                                 back out and we right now are leveraging                                 q-ball for scalable Hadoop and spark so                                 that's an interesting product to check                                 out cue ball actually orchestrates ec                                  instances or google cloud or as your                                 instances to make cluster manages                                 automatic scaling up and down and using                                 spot instances etc it's an interesting                                 product we're just starting to use that                                 and then lastly talked real quick about                                 how we're scaling our time series data                                 so as you imagine is we start building                                 more data centers and we start scaling                                 out                                 we've gotten to the point where as much                                 as we like to monitor right writing                                     to                                                                     graphite we've run out of ways to scale                                 carbon and whisper and make that all                                 work so we've come up with some                                 interesting ideas we've kind of done a                                 little homegrown experiment we spent a                                 lot of time looking at what was in the                                 environment in the open source                                 environment especially with Cassandra                                 and HBase in terms of time series to                                 basically made a very simple time series                                 data base out of Cassandra it works                                 beautifully for it right wyd tables it's                                 that everybody's doing it we did a                                 couple of interesting things we thought                                 we'd mentioned we use the strata o                                 leucine plugin in Cassandra and we                                 actually take our metric names and our                                 metric tags we put them in a second                                 table and we use leucine to scan that                                 table and index that table so if you                                 want to do a wild card of a bunch of                                 metrics like you would in graphite right                                 you can find those metric names and get                                 that list and then Joe and use that to                                 extract your time series rose separately                                 we use spark is our custom query engine                                 against this it's worked out really                                 really well sparking it's time to work                                 very well together so we actually have a                                 custom Scala based query engine that                                 goes in through spark extracts data out                                 through Cassandra and is able to process                                 it do the necessary math and all the                                 necessary functions to that data and                                 present one array back to a Java                                 microservice that actually mocks the                                 graphite API so basically what we've                                 done is if you want to use graphite or                                 graphene you go to that mock service it                                 goes to spark it does the queries and it                                 reads out through Cassandra a next                                 experiment that we're starting to do now                                 is how do we start doing anomaly                                 detection and curve detection right so                                 looking at you know if you've got                                 everything's writing fine and your ad                                 servers are fine and all of a sudden                                 something drops off we're still at the                                 point now we've got humans watching some                                 of that our next experiment is how do we                                 get spark to start watching that right                                 and start telling us when the                                 differentials are tanking or something's                                 going wrong with curves etc but it's                                 it's an exciting project that we're                                 doing and we think that scales really                                 well we've been able to do it from much                                 lower cost in Amazon than trying to run                                 graphite so we suggested you know to                                 people if you're looking for a way to                                 scale your time series definitely look                                 at Cassandra there's lots of tools out                                 there for that and that's pretty much                                 what we have for you any questions                                 ok                                 so I have it I question okay well these                                 these amount of data is is it a new                                 process well I I saw it in IT systems                                 but your what do you process with it so                                 we have will take a will take an                                    billion bid request today right for                                 advertising bidding from different open                                 our TV providers right and so we've got                                 to go through those at the bid edge and                                 decide what we're bidding on what we're                                 not bidding on based on users that we                                 find in our system based on the number                                 of other criteria and then basically                                 either accept or reject those been make                                 a bit or not make a bit right then what                                 our system has to do is then also take                                 that data and bring it back so it's                                 obviously important to us what ads we've                                 sold what bids we've made what we                                 haven't been on and why right because                                 that also tells us interesting                                 information about what's in the ad                                 ecosphere right now what users are                                 coming in and what they're looking at                                 and what people are doing on the web so                                 we also process all of that data that we                                 don't bid on so we take some of that is                                 sampled data right and some of that is                                 actual data into the system and bring                                 that all back and then process all of                                 that prediction think is the next you do                                 with it and why is it integrated in your                                 system already so good it's working yeah                                 things the predictor suffered she was                                 talking about is basically the stuff for                                 the monitoring and for the calculation                                 numbers our system based we have an                                 internal da DMP data management platform                                 where we also collecting information                                 about user behavior in a similar way as                                 you had heard from bol before in in in                                 the other talk and we combine this with                                 the data what what we get out of our bit                                 requests and compression requests and                                 built out of them z models for future                                 bidding for certain users or four                                 generals                                 strategies and there's also a lot of                                 machine learning in it which feeds ends                                 and back and to give you some some                                 numbers we are right now writing                                 approximately                                                          history from worldwide to one data                                 center so and there's a couple of                                 challenges in this if you want to scale                                 is                                                                     then thanks a lot thank you
YouTube URL: https://www.youtube.com/watch?v=bbrgIMD2vnA


