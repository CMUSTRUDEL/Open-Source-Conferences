Title: Berlin Buzzwords 2016: Volker Janz - Real Time Marketing with Kafka, Storm, Cassandra & Spark #bbuzz
Publication date: 2016-06-11
Playlist: Berlin Buzzwords 2016 #bbuzz
Description: 
	The combination of Apache Kafka as a event bus, Apache Storm for real- or neartime processing, Apache Cassandra as an operational storage layer as well as Apache Spark to perform analytical queries against this storage turned out to be a extremely well performing system.

With increasing marketing costs per registration, it is even more important to keep players within the game as well as provide them with attractive offers aiming to increase the customer lifetime value and also create a better game experience.

To that end, we introduced interstitials that offer premium features or discounts for the player at InnoGames. Even though this is already a useful instrument, we aimed to customize those interstitials according to the behavior of the player. Therefore, we created a system that works with generic messages that contain data about user interactions, in real- or neartime -- later referred to as events. The system builds up a player profile that contains all game-relevant information about the players in a central location.

Read more:
https://2016.berlinbuzzwords.de/session/real-time-marketing-kafka-storm-cassandra-and-pinch-spark

About Volker Janz:
https://2016.berlinbuzzwords.de/users/volker-janz

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              okay good thank you everybody for coming                               actually like you can see in the title                               there a lot of passwords a lot of                               technologies so my presentation is quite                               packed so I have to be fast to finish in                               time so I jump directly into throt                               introduction so my name is Fargo and i'm                               a software developer team lead software                               developer actually foreigner games and                                you can find me on twitter linkedin sing                                whatever you like and actually I'm                                leading a team of data engineers and we                                are dealing with the whole data                                infrastructure there no games and                                shortly about inner games so we are                                online gaming company and we have more                                than                                                                   world and more than                                                   for no games from                                                       and the main office is in Hamburg but we                                also have the office in Dusseldorf well                                and the first thing I want to do is I                                want to tell you a little secret so the                                secret about how to create a successful                                company with free to play online games                                so inner games is a successful company                                in that direction so i want to tell you                                the whole secret about it and it's                                fairly simple so first of all you                                develop an awesome game then you get                                some users then you sell virtual goods                                and then you have profit and that's                                basically it so that's a whole secret                                about well of course it's not that easy                                to do actually develop an awesome game                                is quite hard to do and then you have to                                get users somehow and also to sell                                virtual goods is not that easy so what                                you have to do for that is you have to                                do kind of marketing and also kind of                                CRM so customer relationship management                                and my talk will focus on a use case in                                that direction so it's about CRM and                                especially it's about those                                interstitials and i will show you such                                an interstitial on this slide so in                                interstitial Ferno games is basically                                nothing but in game pop-up message so                                imagine you play game and then this                                pop-up appears and you can get ten                                percent bonus if you buy crowns in this                                example and this is basically something                                that we're doing batch why's that means                                every night we are selecting a bunch of                                users based on some yeah some                                requirements and then we are sending out                                those campaigns basically now the whole                                idea that we have now for this project i                                want to show you today i want to tell                                you a little story for it so this is Bob                                and Bob plays forge of Empires obviously                                that's the game that in games has                                implemented so if you like to give it a                                try just feel free well if the talk is                                boring or something please play forge of                                Empires at least okay so Bob plays for                                two vampires and now he gets such an                                interstitial message but he doesn't know                                what to do with it so it's not very                                attractive for him it's just getting in                                ten percent free diamonds now and yeah                                he doesn't know what to do was it so he                                clicks it away and then he continues                                playing the game and then all of a                                sudden he loses three battles in a row                                so he is stuck in the game he doesn't                                know what to do and he's very frustrated                                now after a few seconds later he now                                gets this interstitial and it's an                                interstitial that is tailored for him so                                that means it's exactly what he needs in                                this moment he needs for battle tanks to                                win this battle obviously so that's                                what's missing in his army composition                                basically and yeah he gets this                                interstitial he is very happy he accepts                                the offer and yeah he's happy we have a                                new paying user basically and we                                prevented a term so that means he                                continues to play our game and he's                                happy was it and this is the motivation                                behind the project real-time cm and                                that's what i want to show you today and                                basically what it is it's individualized                                marketing based on user behavior and                                real-time and what do you need for that                                so first of all you need data of course                                and the dinner games we are mainly                                tracking so-called events which                                 basically show the user interaction that                                 means when you play our game you                                 interact with the game and we record                                 those interactions as events so for                                 example this is forge of Empires again                                 and if you know for example build a                                 building then you get a build event if                                 you yeah do a fight then you get a fight                                 event and so on and so forth so it's                                 fairly simple but you can imagine it's                                 quite a lot of data that you can                                 generate that way and actually talking                                 about passwords and numbers I brought                                 you a number that's big number it's                                     million and this is actually the number                                 of events that we collect per day so                                 that's the more the data that we                                 have actually that's of course compared                                 to some other bigger companies it's not                                 that much but I think for our business                                 and our company size it's already quite                                 quite a bit so that's what we have to                                 deal with basically and to collect this                                 data and to process the data we build up                                 kind of a data infrastructure at dinner                                 games so that's what my team is doing                                 basically and it looks like this so this                                 is basically the data processing                                 infrastructure that inner games has and                                 well it's fairly simple but imagine this                                 is all in a distributed world so it's                                 very simplified that picture but let me                                 guide you use route from the left to the                                 right basically so on the left side as                                 you can see we have games and other                                 systems they are sending events to a                                 gateway which is the entry point for the                                 whole system and then it continues to                                 push those messages into a message bus                                 and that's where the real time                                 processing happens for the return                                 processing we offer also have kind of a                                 persistence layer that is optimized for                                 write and read performance so this is                                 all production storage and then in the                                 end we have a loop of course for                                 distributed storage and distributed                                 processing and we are mainly doing batch                                 processing in that area in the end you                                 can see that we have different systems                                 that are connected to our Hadoop cluster                                 so we have a business intelligence                                 system based on SQL server that is                                 fetching the data from Hadoop we also                                 have a great analytics Department so                                 with data scientists and they are                                 working with the data day by day and of                                 course we have other systems such as                                 power bi for example water blue and                                 talking about timings as you can see on                                 the top as soon as an event gets                                 generated in the game it takes about                                 three to five seconds until it arrives                                 at the gateway this is because we have a                                 special process that is that decouples                                 the sending of the event from the game                                 itself so it makes it very safe if the                                 whole system explodes the game will                                 still work result of any problems so                                 we're talking about seconds in that area                                 after it arrives at the gateway we are                                 talking about milliseconds so after that                                 we can process all those events within                                 milliseconds and then of course on the                                 batch side we are talking about hours                                 because we are processing the etsy less                                 our the last month's whatever                                 so that's basically the whole yeah the                                 whole data infrastructure that we have                                 and when we talk about the left side we                                 talk about the data pipeline at little                                 games and the right side for us is                                 basically data platform and since we're                                 on the billion passwords I have brought                                 you some passwords basically those are                                 the technologies that we use for it so                                 on the client side we have a client                                 implemented with the gold language oh                                 it's quite cool and quite handy and for                                 the Gateway we used Java basically a                                 wizard with drop wizard framework which                                 is a framework to implement rest api so                                 it's very simple then of course we use                                 Kafka like everyone and then for the                                 near real-time processing we're using                                 apache storm patchy Cassandra for the                                 production storage and on the Hadoop                                 side we are basically using most of the                                 stuff that yeah the Hadoop                                 infrastructure officers likes hive and                                 spark and different other technologies                                 so that's basically how it looks like                                 and when we are now talking about this                                 real-time cm project that I showed you                                 in the beginning it's happening in the                                 data pipeline part so I focus on that                                 part right now and to give you an idea                                 of what happens there it's fairly simple                                 basically so if five simple steps that                                 are happening within this real-time CRM                                 process first of all we have events they                                 get sent to the Gateway the Gateway is                                 then pushing it to the message bus and                                 then the events gets processed event by                                 event so as a stream and then two things                                 happen basically first of all we update                                 a player profile with those events in                                 Cassandra that means it's basically some                                 kind of storage that can tell you                                 everything about a player in real time                                 so of course we do not record everything                                 so every single event but those events                                 that are interesting for us and then                                 after that we trigger campaign based on                                 the current player profile and the                                 incoming event and that's basically it                                 so that's the whole magic behind the                                 real time see I'm project and the key                                 technologies for that are basically                                 kafka storm and cassandra in our case                                 and i want to focus on how we use those                                 technologies for that project right now                                 so starting with Kafka I think most of                                 you know                                 I use it in production so who's using it                                 in production just quick heads up so                                 okay that looks good so I can make it                                 very fast it's very good okay so                                 basically you know that in Kafka you                                 have brokers which is basically Africa                                 class i have several producers and                                 several continued consumers and the cool                                 thing about it is that you can wyd runs                                 but read it many times as often as you                                 want and internally you know that as                                 well i'm pretty sure you have brokers                                 you have petitions so from this example                                 we have a topic called locks it has two                                 partitions and replication factor of two                                 and it's yeah it's spreaded around the                                 cluster on those two brokers so that's                                 basically the idea behind it but I think                                 more interesting is how does it look at                                 innoGames so I dinner games we have five                                 notes five Kafka notes with eight cores                                                                                                     space each and those are basically the                                 settings you can see them on the right                                 we are using Kafka                                                     update soon because they're cool new                                 features that we want to use actually                                 and this is how it basically look                                 basically look looks like it in all                                 games and talking about the topics that                                 are no games we have currently just                                 three topics so it's kind of the first                                 project that we have added business                                 pipeline so that's why we only have                                 sweet topics for now but let me talk                                 about those topics so I start from the                                 bottom and then I go to the top actually                                 so I start with the raw topic that's a                                 topic where we actually store the events                                 so it's our main topic for everything                                 then we have a maintenance topic it's                                 clear what it is so we install                                 maintenance information or the                                 components in the pipeline that you've                                 seen sent some information to this topic                                 and you can just check everything on the                                 maintenance topic and then we have the                                 serum trigger topic which is quite                                 interesting because we actually use                                 Kafka as a message bus means if we want                                 to communicate that this interstitial                                 should be shown to a player we write it                                 back to Kafka and then another system is                                 reading it from Kafka and actually is                                 performing this interstitial metric                                 basically and you can see in the bottom                                 or raw partition has or while topic has                                                                                                          and it's thought I think seven days of                                 dating or four days of data                                 anything fails over the weekend we                                 should be safe hopefully that's about                                 Kafka now it's a bit more interesting                                 because we chose storm for the stream                                 processing part and actually that's                                 where all the logic happens so I think                                 many of you know storm already it's it's                                 kind of the old school distributed                                 real-time computation system and still I                                 want to show you the basic features or                                 components of storm of the idea behind                                 of storm so storm has spots which are                                 basically the sources of data streams                                 storm has balls within bolts you do the                                 actually processing you can also                                 generate new data streams out of those                                 bolts and in the end you have a topology                                 which is just the composition of those                                 components so very very simple basically                                 and to show to you by an example this                                 for example is a temperature alerting                                 topology so very simple on the left side                                 you see a temperature ball that we'd see                                 current temperature former sensor and                                 every few milliseconds it's emitting                                 those values to the next bolt which is                                 the filter board and the filter balls                                 actually checking if the temperature                                 value is more than a specific threshold                                 and if that happens it will send another                                 message to the next bolt which is then                                 the Warren Bould and it will just I                                 don't know write an email or raise an                                 alarm or something like that so that's                                 really simple example for storm topology                                 and if you look at the implementation                                 it's also not that communicated so this                                 will be the filter bolt basically you                                 can see it on the right side that you                                 have a prepare message where you can                                 prepare your bold you have an execute                                 method that gets the actual tuple couple                                 so that's where all your logic goes and                                 then you have a declare output fields                                 message where you can declare what kind                                 of output your bald is generating so                                 that's basically it so extremely simple                                 but also very low level storm also has a                                 high level API called trident but that's                                 not part of this talk and you can look                                 it up if you like but what we did with                                 storm is this so this is the CRM                                 topology which is the main processor for                                 the whole reason MCM stuff so let me go                                 through it step by step we have the                                 kafka sport that's reading stuff from                                 Kafka obviously it's                                 meaning the raw topic the events then we                                 have a validation bolt because like                                 every Big Data project data quality is                                 always an issue so we have schemas and                                 we are validating our events with those                                 schemas but that's a different story                                 just what is important is we only want                                 valid events of course and those valid                                 events get then partitioned by player                                 and we do this using player petitioner                                 bold and that's a very important                                 component because if you if you work in                                 a distributed system and you don't think                                 about purchase how to partition your                                 data you can get really easy very much                                 in trouble because in this example                                 imagine you have much more instances                                 than just one of those CRM bold of this                                 young bold and now the question is if                                 you have multiple events how get they                                 distributed to those instances of the CM                                 bold and by default storm is doing this                                 randomly but if you know yeah once we                                 have the events in a specific order for                                 player you can get in trouble that's why                                 we have a player petitioner board                                 actually very simple but extremely                                 important so think about partitioning                                 can stream processing and this year in                                 bold is something that it will show you                                 in a bit so just imagine that's where                                 the magic happens and then as I already                                 told you before we are writing the                                 information that it interstitial has to                                 be triggered back to cough guards so                                 that another system can read it and                                 actually perform this action right okay                                 so this is a CRM bolt um let me drink a                                 bit of water just okay so this is the                                 c-arm bolt this is where all the logic                                 happens and this is actually quite cool                                 because it's very simple but also very                                 powerful it's doing just two things                                 basically first of all it gets an event                                 of course and then you have every event                                 has a type like you've seen in the                                 beginning there's a fight event for                                 example and for each event type you can                                 define handlers update handlers or                                 campaign handlers or both if you like an                                 update handler is basically just                                 updating the player profile so it's                                 doing some updates in Cassandra the                                 campaign Henda is then actually trigger                                 triggering those campaigns by                                 using the event that's currently in                                 coming and also using the current player                                 profile so by those two informations it                                 will trigger a campaign event eventually                                 and now the interesting part is that                                 those handlers are actually defined in                                 JavaScript and we are using the NASA on                                 JavaScript engine which is part of Java                                                                                                        build a fancy class that we just used to                                 call this a JavaScript engine and then                                 in the end you have the updater and                                 campaign handlers defined in JavaScript                                 and that's quite cool so actually what                                 storm is doing for us it's applying the                                 JavaScript snippets that we have to each                                 and every event and those JavaScript                                 snippets are basically containing the                                 whole logic that we want to apply so if                                 you look in the example on the top this                                 one is counting the login events so you                                 can apply photo I only want to do it for                                 login events and then you can actually                                 perform some actions with a framework                                 object that we provided to this script                                 basically so that's how we define the                                 logic and the cool thing about it is you                                 can actually change the logic at one                                 time so you don't have to redeploy the                                 topology you just let it run you                                 integrate new JavaScript and so you can                                 change the behavior of the whole system                                 at runtime which is quite cool ok so                                 that's basically how we do it in storm                                 and talking about storm also wanted to                                 talk a bit about alternatives we use                                 song because of historical reasons so we                                 was the first thing that we tried out it                                 worked for us and so we got stuck with                                 it but it's fine I mean it has some                                 drawbacks but if you look at the latest                                 versions they actually improved a lot so                                 in November                                                              and it had some cool features like for                                 example Flux which is a declarative                                 language to define topologies so before                                 that it was really like you had to write                                 a lot of glue code to define a topology                                 a lot of Java code that is I don't know                                 it's very hard to test and it's it's not                                 very developer-friendly actually but                                 with looks you have just the Yama                                 description of your topology you have                                 property files you put it together and                                 the                                 you can deploy a topology so it's quite                                 cool beside that you have rolling                                 upgrades you have some security features                                 and you have improved logging so that's                                 already really really good milestone and                                 then this year in April actually when                                 the dupe summit was happening in Dublin                                 they release version wonder though which                                 is quite cool because it has a lot of                                 performance improvements so on the                                 website it says like                                                    least three times faster so of course it                                 depends on your use case but they                                 actually works on the performance which                                 is quite good then you have pacemaker                                 which is Zhu keep a replacement for work                                 our heartbeats also very cool and useful                                 and as you can see on the bottom they                                 also thought about how to store state                                 for the different components so now you                                 have a distributed cache and you offer                                 also a state management so it has quite                                 cool features and even more features                                 like back pressure resource away                                 scheduler it now has native windowing                                 API and you have improved debug and                                 profiling especially in the storm you I                                 of you haven't seen that so you can have                                 double sampling and stuff like that so                                 they improved a lot so storm is not it's                                 not dead or something so you can still                                 use it for those projects even though                                 there are a lot of alternatives as we                                 have seen on the conference and yeah you                                 have to choose wisely and for example we                                 have caprica streams you have spark                                 streaming and you have linked so those                                 are the things that are actually quite                                 hot right now and yeah you have to think                                 about those alternatives for our use                                 case storm was completely fine because                                 we decoupled the whole logic in this                                 JavaScript snippet so we could do it                                 with any of those systems of course we                                 require low latency and that's why we                                 also chose storm basically and to give                                 you some more input you should check out                                 those presentations actually the first                                 two were happening yesterday about kafka                                 streams and apache chief link and the                                 last one was to talk about the new storm                                 version that the hadoop summit so if you                                 I'm the state where you have to choose a                                 stream processing system just have a                                 look at those presentation they're very                                 good and very informative about that ok                                 the next component in this whole                                 infrastructure that we use for the way                                 to MCM stuff was Cassandra                                 and like I said we use it to store                                 basically a player profile so talking                                 about Cassandra I'm just quick question                                 who views using Cassandra and production                                 does anyone use it okay it's well that's                                 quite a lot actually yeah I will make it                                 short so basically cassandra has a                                 project developed at Facebook its own                                 source and it's based on Dena Modi be by                                 amazon and a big table by google and for                                 this presentation i thought how can I                                 explain Cassandra with just one slide                                 basically and I think that's quite                                 accurate it's it's not perfect but I                                 think it describes it very well so if                                 you think about Cassandra and the data                                 model behind Cassandra just think about                                 it as a sort of map that has a rocky and                                 then again a sorted map with key value                                 pairs I think that's basically the idea                                 behind Cassandra and you can see based                                 on the rocky you have a hash value and                                 then with consistent hashing at the data                                 gets distributed around the cluster and                                 you have also data replication so I                                 think that's basically the idea behind                                 cassandra is also explaining it very                                 well so that's basically kasama and                                 going more in detail about the data                                 model in Cassandra on the left side you                                 have an example for bad data model on                                 the right side of an example for good                                 data model so what you should not do is                                 you should not have something like a                                 rocky based on the hour or the day and                                 then x them payloads timestamp a lotus                                 the rose as the column sorry because the                                 issue that you have is what you want is                                 you want to spread the data evenly                                 across the cluster and if you have the                                 hour for example is a rocky you have the                                 issue that the queries of the same all                                 will always go to the same node so you                                 don't have parallelization at all so                                 it's it's very bad to do it like this on                                 the other hand if you use the timestamp                                 for example so for time series data you                                 might have the issue that you have two                                 different payloads with the same time                                 stem and you overwrite your data so a                                 good model would be if you choose                                 something that spreads the data evenly                                 across the cluster like the combination                                 of the hour or the day together with an                                 ID for us it's the player ID for example                                 and you have to use the time give your                                 ID instead of just the time stamps that                                 will be a good model and basically as                                 you can see on this slide your gold                                 should really be you should spread the                                 data even across the cluster of course                                 you should minimize the number of                                 petition sweet means if you want to know                                 something it's good to only read as few                                 partitions as possible of course and                                 what is also very useful is if you start                                 thinking about your queries and then                                 start thinking about your data model                                 when working with Cassandra well and                                 that's what we came up with for our data                                 model for the player profile so                                 basically what we did there was we have                                 the unique player ID which is a unique                                 identifier for the player we have the                                 day and that's basically our okey so by                                 this roki we distribute the data across                                 our cluster and then for the column keys                                 we have time you your IDs like I                                 mentioned before and then we just store                                 the type of the event and the event                                 itself and then we can use it in your                                 application and can read the data from                                 it so that's basically how that works                                 and yeah the issue was it is you are                                 very fast if you want to know something                                 about a play on a specific day for                                 example if you think back to Bob yeah                                 Bob lost three fights in a row if you                                 want to know that it's very simple you                                 just I'm Crary Bob by its play ID and                                 also for the specific day and then you                                 know okay he was losing three fights                                 fights already that day now the issues                                 if you want to know all the players had                                 already lost three fights to turn it                                 around and you want to do an analytical                                 query on that data then you might have                                 an issue because you have to scan all                                 the row keys and that might take some                                 time and that's why we also threw an                                 pinch of spark so we use sparkfun order                                 to get queries in that case and I'm sure                                 most of you know spark already so again                                 the question who's using Sparkle                                 production I think a lot of you yes                                 that's what I saw it so again spark is                                 basically a general-purpose classic                                 computation system that means that you                                 can use it for any purpose and it can                                 run standalone it can run on missiles                                 that come on                                 so it's very flexible and environment                                 you have different data source API so                                 you can use data from Hadoop from                                 Cassandra hive HBase whatever you like                                 and then you have for example the data                                 frame API right now there's also data                                 set API which is even it's even on a                                 higher level basically and with this                                 data frame API you can then use                                 different libraries and different                                 languages to work with your data after                                 that spark is generating a daj order for                                 your application code and will process                                 the data on the cluster basically that's                                 what spark is doing now what we did with                                 it is that we actually install this bar                                 cluster alongside our Cassandra cluster                                 solve this issue and this is quite cool                                 because there is a actually a spark                                 Cassandra connector by datastax that we                                 use together with the fact that we have                                 data locality and based on that we can                                 do very fast analytical queries on                                 Cassandra data and actually this solved                                 the issue that we are not able to find                                 out how many players lost three fights                                 already beside that we tried another                                 project called the spark job server I                                 don't know if you know that but it's                                 quite cool it offers you a REST API that                                 is able to to run spark logic on a                                 sparker cluster and yeah you can                                 interact with it using a REST API and                                 the cool fact about it is that it has so                                 it's managing context objects so in this                                 case in this example as you can see we                                 are cashing a Cassandra table so we're                                 using Cassandra SQL context and we are                                 cashing a Cassandra table if it's not                                 cached already so in the bottom part                                 right here the first time you run the                                 code this will take quite a while but                                 the second time we run the code will be                                 very fast and if you have different                                 analytical crevasse than that on the                                 data they will also be fast because the                                 data is cached already of course you                                 have to clear the cache from time to                                 time like like how we like it for                                 example every hour so we have to recast                                 it but using that spark drop servers                                 quite powerful to do those analytical                                 this is actually something that we don't                                 use in production right now it's just                                 playing around with it and because we                                 have to find a solution how to run those                                 analytical crows and for now it works                                 quite good so if you are interested in                                 that give it a try and give us feedback                                 if it works for you actually ok so now                                 want to put all that stuff together                                 again so I've shown you several                                 components of this real-time see I'm                                 project that we run a dental games and                                 it's all happening within this data                                 infrastructure basically on the left                                 sides on the data pipeline and there are                                 some takeaways that we learned from that                                 so first of all Kafka is quite awesome I                                 think we agree with that because it                                 works really well first before we use                                 Kafka we used Kestrel and we had several                                 issues with it and then we switch to                                 Kafka now everything is fine and                                 actually the more we use it as its                                 desire to be so as a message bus the                                 better it gets so it's quite cool storm                                 actually it works quite well for us so                                 it makes it easy to actually write those                                 real time computation applications but                                 we had performance issues and it was                                 also in the beginning very hard to                                 define topologies because you had such                                 so much glue cult there was really hard                                 to test and it was not developer                                 friendly but like I've shown you those                                 disadvantages were addressed and also                                 resolved partially in the newest version                                                                                                    that's about stone last but not least                                 reacting to user behavior is quite                                 powerful so I cannot give you any                                 numbers on uplifts how much uplift it                                 generates because it's just running on                                 some test markets right now but it looks                                 already quite promising and if you think                                 about it it's quite powerful because                                 it's basically the next step to do                                 marketing or CRM based on the user                                 behavior real-time now the issue is with                                 that it's not only a challenge for the                                 developers it's also a challenge for the                                 game designers in our case for example                                 imagine you have some back in the                                 JavaScript some wrong definition and all                                 of a sudden all the players we get those                                 for battle tanks for free and                                 Sonia complete game is ruined and the                                 whole game balance is broken so it's                                 quite dangerous to to react in real time                                 to the user behavior if you not think                                 enough about it before so it's also                                 challenged for the game designers and                                 combining storm actually with Nassau was                                 a great idea it was very fun to do we                                 didn't had any performance issues so far                                 and it's quite cool because you can                                 define the whole logic in JavaScript                                 which is quite a good feeling and it can                                 be changed at one time of course that                                 being said the whole project was very                                 fun to do and we used a lot of those                                 technologies actually the next step for                                 us would be to think more about the                                 analytical queries on Cassandra also                                 maybe we think about switching from                                 storm tough link or just give it a try                                 maybe and yeah and then we have of                                 course to bring it from the test markets                                 to the production markets to see what                                 kind of uplifted can generate for us so                                 that was actually quite fast and there                                 was a project already so I hope I could                                 give you some yeah small insights about                                 what to do what you can do with those                                 technologies some practically use case                                 and if you have any questions feel free                                 to ask so thank you very much thank you                                 very much                                 questions we have                                                        questions any questions I think oh yeah                                 I see a hand here and one day right okay                                 I'm curious if how it has if it has                                 worked well for you to run this park on                                 top of Cassandra and what the largest                                 rdd sizes that you process or so you get                                 the question and I'm curious if you had                                 any problems running a spark on top of                                 Sandra and especially since it's part of                                 the critical pipeline for the real time                                 processing actually know so far we                                 didn't have any issues but i have to say                                 that it's we are the use case with                                 Spargo Cassandra that we that we tried                                 all this very small so it's it's really                                 currently we are not using all events                                 and to be stored in the player profile                                 so it's it fits a memory it's fine for                                 for what I can say right now but yeah it                                 might be well it can be hard in the when                                 you have too much data that doesn't fit                                 into memory but for now it works quite                                 well and it's also just yet we just gave                                 it a try it's just testing how to do                                 those another ticket crow is actually                                 it's based on a blog post from data sex                                 so they have a presentation about it you                                 can google for it and you get more                                 details about it but for us it works so                                 far so that's what I can tell thank you                                 any more questions                                 so really simple question if you already                                 have storm cluster yes Oh like flink why                                 not to do some analytics already on that                                 step right like all this chain you can                                 click precompute something so requesters                                 why of doing the analytics not already                                 installed you mean that's why basically                                 what we build up is I can go few slides                                 back so this one maybe so the production                                 storage in that case or the whole data                                 pipeline doesn't know the whole history                                 of of the player so that's one issue for                                 us and we keep the whole history in                                 Hadoop and that's why we need some data                                 from a tube as well so that's why we do                                 it later on the analytics basically for                                 now of course you can think of doing                                 more analytics already on the left side                                 actually we are thinking about it but                                 for us the issue is that we have a lot                                 of data and it's all within Hadoop                                 currently that's the issue so yeah but                                 good point yes any more questions super                                 how big is your Cassandra cluster                                 current is just eight notes so it's it's                                 quite big notes but yeah I got screwy                                 right and I saw you had what                                    partitions in Kafka like / tapia how                                 come you chose                                                         question will work for us so I could go                                 back to the slide actually yeah we chose                                                                                                     quite well so it was trying out it's not                                 by the book basically but that work for                                 us quite well so it's just by                                 benchmarking yeah because I was thinking                                 you know you might I want to use more                                 because what you had five note                                         would we have five to eight cores yes                                 caicos that makes what                                                   if you then have                                                      know it seems that you have a bottleneck                                 on the petition side yeah at some point                                 maybe yeah what's up well that might be                                 true yes good point okay                                 any other questions thank you so much                                 for good that was a neat presentation                                 thanks again thank you
YouTube URL: https://www.youtube.com/watch?v=8WlpnmQzqww


