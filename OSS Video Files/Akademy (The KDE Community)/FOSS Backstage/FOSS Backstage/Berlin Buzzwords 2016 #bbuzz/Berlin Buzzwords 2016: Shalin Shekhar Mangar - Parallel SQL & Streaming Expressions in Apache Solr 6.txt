Title: Berlin Buzzwords 2016: Shalin Shekhar Mangar - Parallel SQL & Streaming Expressions in Apache Solr 6
Publication date: 2016-06-12
Playlist: Berlin Buzzwords 2016 #bbuzz
Description: 
	Apache Solr is a powerful search and analytics engine with features such as full-text search, faceting, joins, sorting and capable of handling large amounts of data across a large number of servers. However, with all that power and scalability comes complexity. 

Solr 6 supports a Parallel SQL feature which provides a simplified, well-known interface to your data in Solr, performs key operations such as sorts and shuffling inside Solr for massive speedups, provides best-practices based query optimization and by leveraging the scalability of SolrCloud and a clever implementation, allows you to throw massive amounts of computation power behind analytical queries.

In this talk, we will explore the why, what and how of Parallel SQL and its building block Streaming Expressions in Solr 6 with a hint of the exciting new developments around this feature.

This is a talk sponsored by LucidWorks.

Read more:
https://2016.berlinbuzzwords.de/session/parallel-sql-and-streaming-expressions-apache-solr-6

About Shalin Shekhar Mangar:
https://2016.berlinbuzzwords.de/users/shalin-shekhar-mangar

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              cool so thank you guys for coming my                               name is Shallon shakin monger I work at                               lucid works and one of the commuters on                               apache Lucene solar and a PMC member i                               have been working with lucid works for                               the past three years mostly full time on                               solar before that I've worked at a will                               where I vogue Donna and a few very large                               scale solar deployments specifically the                                one for AOL webmail so I'm sure a lot of                                you are already familiar with solar                                solar is the enterprise standard for                                search we have a lot of watching finite                                companies who use solar from day to day                                in fact most likely you already use                                solar even if you don't use it directly                                because you consume it through some                                service or some company which internally                                uses it it's a full-text search engine                                you basically send in documents and you                                send in full text queries you can also                                facet into facets sorry sergeant sticks                                and aggregations on top of that data you                                have language detection it's a                                completely modular system so you can                                plug in almost anything and in fact most                                expert users do plug in things like                                query parsers or ranking and things like                                that part of solar is what we call as                                solar cloud which is the system that you                                deploy on a large number of nodes and                                which allows you to scale you can add                                nodes you can split charge you can                                replicate etc etc we are specifically                                going to talk about a new feature which                                was introduced in solar                                          parallel SQL and actually parallel SQL                                is just one part of that feature because                                there is a whole building block of                                streaming transformations and streaming                                expressions on which the panel SQL stuff                                is built on so what we are going to do                                in this presentation is we are going to                                start from the parallel scale stuff                                because that's the most easiest to                                understand and then I'm going to tell                                you how to use it with solar the already                                leased versions and then we are going to                                dive it into the internals of this and                                we're going to see how it is internally                                implemented and what it is capable of                                okay so the first question is why                                implement SQL in solar why now like so                                let's been around for a long time so I                                think the reason mainly is ease of use                                because solar is it so you know it's                                it's very powerful but it's also quite                                complex so with SQL you basically have                                an easy way to carry the system which is                                very very well-known there is a whole                                lot of tooling available which can work                                with a with a system that exposes a JDBC                                endpoint right the other thing is if you                                have a system which speaks SQL then                                internally we can compile it and we can                                execute that query according to the most                                efficient way possible so there's a lot                                of work being done that area I don't                                think they're quite there yet we don't                                do a whole lot of automatic optimization                                right now but we have the possibility to                                do that so the political stuff is very                                very new it was just released in                                        first point release of it was made                                actually a week back six point zero                                point one which fixes a few more bugs in                                it so it's a little bit rough around the                                edges because it's still under active                                development but it the whole thing                                actually is quite powerful and it has                                uses beyond SQL as well which we are                                going to see at the end of the slides so                                what is parallel skill first of all why                                why do I call it parallel SQL and not                                just SQL the reason is that any query                                that you make first of all it's it's a                                solar cloud only feature so it only                                works with solar cloud if you're using                                the old style master-slave replication                                you know you can't use SQL in that it                                has to be select loud so when you                                execute a query on solar cloud it can                                actually go to each replica of each                                shard ask for a subset of data and                                operate on that data moyes the results                                and give it back to you so it all                                happens in parallel so this is not there                                are many systems which which do things                                like that for example hive and Impala                                 and all these systems speak SQL but they                                 are mostly made for you know                                 long-running analytical workloads on                                 extremely large amounts of data and on                                 raw data which is not pre pro                                 just like you know indexed in a in a                                 search engine so what's happening here                                 is we can use the best parts of the                                 search engine to do certain things which                                 are most easily done inside the search                                 engine and then we can do that the                                 intensive parts the ones for example the                                 joints involved which are not done very                                 well by search engines externally inside                                 a worker collection and we're going to                                 cover that in a bit so the SQL is                                 parallely executed across solar cloud                                 instances it is compiled internally to                                 what we call streaming expressions and                                 the streaming io API in in when you                                 write this SQL the the solar cloud                                 collection that you already have is what                                 is the table so if you say select star                                 from collection one or select star from                                 collection                                                               name is right so each collection is                                 automatically exposed as a table in this                                 SQL and the soul eject line the Java                                 client that we have already ships with a                                 thin jdbc client it's not completely                                 full-featured jaidev's a client because                                 there are lots of things inside jdbc                                 which we don't implement and for that                                 reason them you might find a few tools                                 which refuse to work with solar right                                 now because they might require some                                 capabilities which we do not expose so                                 what is happening is people are using                                 one or a few tools as the reference and                                 trying to ensure that we are compatible                                 with them and you will probably see that                                 list of tools being expanded as this                                 feature becomes more and more mature so                                 the first one that is being used is the                                 DB visualizer and there are a few folks                                 who are working to ensure that apache                                 zeppelin which is a visualization tool                                 basically a notebook style visualization                                 tool that also works with the solar JDBC                                 driver so let's dive into the SQL                                 interface of solar there are two ways in                                 which the SQL is executed inside solar                                 cloud so as I said there are there is                                 probably work which is going to happen                                 which will make some of these decisions                                 automatically for you in the future but                                 right now you have to consciously choose                                 between two different implementations                                 depending on the kind of data and the                                 kind of queries that you're gonna make                                 the first one is what we call it                                 SQL / BAP reduce and the second one is                                 SQL / facets in the first one what is                                 happening is the first one is basically                                 suited for when you have a very high                                 cardinality and you want to do                                 aggregations you want to do counts or                                 some or min or max or things like that                                 on top of it in the second one you have                                 SQL / facets in which all this                                 aggregation and all this unique the keys                                 are actually computed inside a single                                 solar instance so you if you are already                                 familiar with facets what are facets you                                 get the top terms for a given field                                 sorted by some count right so in a way                                 you are d duplicating them and operating                                 on that data so if you're doing SQ lower                                 facets then it is using facets to get                                 that topped and list and then operating                                 on top of that okay so in that way it is                                 actually pretty fast but that doesn't                                 work well if you have to get all the                                 facets all the top terms out of the                                 index because that will you know blow up                                 the memory in when you want to do                                 something like that it's prolly better                                 to switch to SQL or MapReduce and not do                                 a scale over facets the SQL where clause                                 is basically a solar query so you can do                                 range queries you can do you know face                                 queries whatever is possible with solar                                 that same kind of query you can actually                                 put inside the very laws there is also                                 an HTTP endpoint so you can make a query                                 to slash SQL and we'll see an example                                 here this is probably a good example so                                 in this case what we have is we have a                                 collection which has the end wrong data                                 set if you're familiar with that                                 basically the company and draw until                                 there was a huge email set which was                                 leaked out of it so each document here                                 is an email and what we are doing is we                                 are saying from this end on data set I                                 want to find the emails to which the                                 most emails were sent okay so this is                                 giving out a list of addresses to which                                 the most emails were sent out so you can                                 see we do a group by we do an order by                                 there is no where clause but you can                                 obviously add a at a where Clause here                                 as well so                                 the where Clause as i said is basically                                 an SQL query so you can do a simple term                                 search you can do a phrase search you                                 can you arrange search any kind of                                 search that is supported by solar so                                 this is also varied slightly differs                                 from a regular SQL expression because in                                 SQL you could also do some number less                                 than                                                                  work here with solar you have to express                                 that as a range query which is                                 understood by solar I am assuming that                                 that it's a small limitation it will                                 probably go away at some point but right                                 now that is what it is so if you are                                 going to try this feature on solo                                     then you have to keep this thing in mind                                 you can obviously provide more than one                                 clause and you can join them in and make                                 an arbitrary boolean expression out of                                 it and that will all work you can also                                 do select distinct the silica distinct                                 is always pushed down to it can be                                 pushed down to solar or it can do the                                 MapReduce approach in which case the the                                 terms are exported to some worker node                                 where they are aggregated the stacks                                 aggregation actually used that starts                                 component inside solar I'm a kind of                                 assuming a lot of inside solar knowledge                                 for this talk so in case there's                                 something not very clear please feel                                 free to ask a question during the                                 question time or you can come meet me                                 after the talk because I'm going to talk                                 about stats component and all these                                 things if you're not familiar so low                                 then you might find a bit odd but I                                 think to explain these concepts I have                                 to skip over certain things okay the sad                                 segregation are always pushed down to                                 solar so the starch component is the one                                 which is going to compute all these                                 aggregations for you it does a                                 approximate count it can do min max                                 exoteric set some average all those                                 things the only exception is when you do                                 group bias if you're doing group buys                                 then it's not done by the search                                 component it is actually computed                                 externally for efficiency reasons so you                                 guys are all familiar with SQL this is                                 nothing special right it's it's just in                                 simple a skill expression you have a                                 group by I have a having you have                                 order by and all those kind of things                                 the interesting thing to note here is                                 when you use the MapReduce                                 implementation you can join across                                 collections you can even join across                                 different solar clusters by providing                                 the Zika address you can even combine                                 data from different external sources for                                 example there is a source for an                                 external jdbc databases so if you can                                 connect connect to that you can join                                 data between your solar instance and the                                 data being kept in some other jdbc                                 compliant database so this makes the                                 whole thing very very powerful you can                                 you know you can join between arbitrary                                 data sources and you don't have to worry                                 a lot about how much data you are                                 joining because this whole thing does                                 not keep a lot of things in memory it's                                 all streaming okay so it doesn't it you                                 know it doesn't spill to disk it just                                 keeps on streaming data and it stream                                 sorts of stuff on the fly so you're not                                 going to be worried about worry too much                                 about how much memory you are going to                                 consume by these joints it's all just                                 going to work the JDBC driver as I said                                 it's part of the solar J jar that we                                 shipped with solar it already provides                                 solar cloud away load balancing so it                                 will it knows the cluster state it knows                                 how many shots you have it knows how                                 many replicas you have it will ensure                                 that the query goes to the right place                                 it will randomly select some node and                                 send the skill segment to that which                                 will again you know pan out the query to                                 all of the nodes you can choose whether                                 you're going to use the MapReduce or the                                 facet style aggregation by specifying a                                 parameter called the aggregation mode so                                 that aggregation mode is specified as                                 part of the JDBC connect string so as                                 you can see you have jdbc Colin solar /                                 / the Zika connection string the                                 collection equals to the collection name                                 and the aggregation mode equal to facet                                 in this case okay any questions until                                 now this is this is just the most                                 simplest usage part of SQL yes                                 I don't think the necessary data is                                 supported right now this is just the                                 first cut of the feature I think it just                                 assumes that each document is basically                                 a couple but what I think what you can                                 do is if you're not going to omit both                                 the parent and the child at the same                                 time if you're going to omit for example                                 just the child documents then it                                 probably can still work so if you have                                 if you can structure a query which will                                 limit just a child documents from one                                 place then you can join it with the                                 child documents of another collection so                                 that's possible but it can't like it                                 can't execute on a complete block at one                                 time so with that let's go a little bit                                 deep into how it is actually implemented                                 there are five important concepts that                                 you need to understand in order to                                 completely use this feature thank you                                 those five things are this there's a                                 streaming API which is actually almost                                 completely inside solar J there is                                 something called a streaming expressions                                 you have the concept of shuffling which                                 is basically partitioning plus sorting                                 and streaming out results you have                                 something called worker collections and                                 then of course the parallel scale stuff                                 so let's go over each of them one by one                                 so first is the streaming API the                                 streaming API was actually the first                                 thing that was built it is actually                                 partially in fact a lot of it was                                 already available in the                                             because that's when it was started being                                 it started being developed the streaming                                 API is basically it's just java classes                                 so you use those java classes to compose                                 queries you specify you know your                                 transformations or you if you want a                                 group or if you in a count or if you                                 want to sum or if you want to have a                                 where clause and things like that so you                                 write code to build that and then you                                 send it to solar and say execute this                                 and give me the results in a streaming                                 fashion okay so that                                 basically what it is and what it                                 operates on our tuples what's a topple a                                 double is basically just a list of key                                 value pairs okay so you have a key value                                 key value key value streaming in and the                                 last one is what we call as an end of                                 file double that tupple basically says                                 end of file true so when you see that                                 you stop okay so this is all streaming                                 so you never sure how many results                                 you're going to get you always have to                                 look at the last couple okay so if you                                 find end of file is just wrong it's very                                 actually very similar to the job I oh                                 it's I mean you basically have rappers                                 and decorators and it this also works in                                 quite the same way so there are two                                 parts for this you have a streaming                                 transformation and you have the                                 streaming aggregations transformations                                 transform the data so you can do things                                 like group by roll-up Union intersection                                 compliments things like that with                                 streaming aggregations you perform some                                 functions which collect data over some                                 buckets so for example you can do count                                 average min Max etc etc right so pretty                                 simple now comes streaming expressions                                 so what is streaming expressions                                 streaming expressions it's it's                                 basically a query language which was                                 built to specify so streaming is great                                 you can do everything with streaming io                                 the reason why we needed streaming                                 fishings was twofold the first one was                                 streaming io is only accessible to Java                                 developers you have to write Java code                                 to do anything and you don't want don't                                 always want to write Java code to do the                                 most simplest tasks so there should be a                                 way to do more complex tasks without                                 writing Java code right the second thing                                 was when you perform when you execute an                                 SQL statement on some node that node has                                 to send that whole thing again to some                                 of the worker nodes or some of the                                 replicas for it to actually be executed                                 and that is only part of the whole query                                 plan it's not the complete SQL statement                                 so we needed a way to represent what is                                 going to happen on each of those nodes                                 or each of those replicas so that they                                 can be executed so the streaming                                 expression is basically a query language                                 as well as a civilization format for                                 parallel                                 SQL and later we will see that there are                                 there some other work being done which                                 will be which will it will always also                                 be useful for so the streaming                                 expressions come when you compile them                                 they actually compile to the trouble                                 stream objects double stream objects are                                 part of the streaming io API so topple                                 stream is basically a stream of tuples                                 which are coming in as I said the last                                 one would be an UF double you can also                                 send a streaming expression directly to                                 solar or why I HTTP in fact that is how                                 it is that is how part of the current                                 plan is communicated to other replicas                                 when you're parallel cable is being                                 executed so this is an example of a                                 streaming expression again we are                                 looking at the Enron emails dataset what                                 it is trying to do here is it is trying                                 to give you the data of which were the                                 addresses to which flowers for sent from                                                                                                        email for sent from the email belonging                                 to                                                                 flowers vendor in in the u.s. so you                                 have expression equal to search the                                 collection name is end on emails the                                 query is from                                                          mail matching                                                         this query the fields that you want is                                 from n                                                                   okay so in the result set you will see                                 that you get from a United flowers                                 different email addresses the two are                                 the addresses from belonging to                                 employees of Enron and at the end you                                 see you f so the response is actually                                 exactly the same if you were to write a                                 Java program with the streaming IO API                                 except that in this case it is                                 automatically compiled and executed for                                 you without writing any Java code ok and                                 this is Polly well this is a very very                                 simple example you can nest the                                 expression so for example i have this                                 search i can try to                                 to completely nest this within a group                                 by or account or things like that so you                                 can create very very complex nested                                 structures and do a lot of very                                 interesting things with it in fact you                                 can also paralyzed this so if you want                                 to for example group by you can do the                                 group by in parallel on different vocal                                 nodes and there is a streaming                                 expression for that also okay so                                 streaming expressions are off to ties                                 basically the first one is a stream                                 source so these are the expressions                                 which emit the streams which can later                                 be consumed and you can do certain                                 things with it so for example you have                                 stream sources such as a search so a                                 search result will emit the tuples or                                 you can have a JDBC source or a facet                                 source or a starch source or a topic                                 source so what's a topic source the                                 topic source is a little bit interesting                                 because what it allows you to do is to                                 subscribe to a given query and then                                 whenever a new document comes which                                 matches that query you can get a                                 response back you can get an you can get                                 a callback and then you can do something                                 useful with it okay so this is something                                 which is still new and it is being                                 flushed out I don't think it's quite                                 ready to use yet but it will be                                 interesting how it how it pans out the                                 second kinds of expressions are                                 decorators the decorators vApp they wrap                                 around other stream functions and they                                 perform operations so for example you                                 can do a compliment or a hash join or a                                 partitioning or intersect and                                 interesting things like that right so                                 you will if you we have the solar                                 reference guide which which we in which                                 we have a page which details all the                                 string expressions which are available                                 and that list is expanding most but not                                 all of the streaming expression                                 functions can be parallelized across                                 different collections so the ones which                                 can be paralyzed usually accept a                                 parameter called workers in which you                                 say i want for workers or i want                                    workers and so on and so forth and that                                 may number of nodes will be used for the                                 paralyzation                                 now without we come to shuffling                                 shuffling is probably one of the most                                 important concepts because it determines                                 the performance of your streaming                                 expression or your or your SQL statement                                 this is where the aggregation mode is                                 actually internally used because what                                 happens is so what is shuffling                                 shuffling is basically partitioning and                                 stream sorting results ok so you                                 partition the results and you sort them                                 and you stream them ok so the sorting is                                 done by the / export handler if you guys                                 don't know what the / export handler                                 it's a request online inside solar it                                 operates on Doc value fields what's a                                 doc value doc value is a it's a                                 column-oriented store so basically all                                 values of a given field sorted and                                 deduplicated store together okay so when                                 you have such a data structure available                                 on the disk if you want to just stream                                 it out in a sorted fashion it's very                                 easy right you just seek to the right                                 point in just stream out stuff which is                                 relevant to you okay you might apply a                                 function to figure out which one should                                 be streamed out but it's like it's                                 extremely fast so we have a few clients                                 who are actually using this in                                 production and they tell us that with a                                 few fields they can do about                                         tuples per second per node of shuffling                                 that's pretty fast so if you're doing it                                 in parallel across a large cluster you                                 can do very complex analytical queries                                 and still get results in sub second or a                                 few second times okay and that's pretty                                 powerful so the sorting is done by the /                                 export handler you must use doc values                                 for the fields which you want to use                                 here that is a limitation but I think                                 using doc well is in general is anyway                                 good idea if you using complex faceting                                 or sorting etc the partitioning is done                                 by what we call a hash queue partial                                 plug in it's basically a hashing                                 function you basically give it the                                 number of worker nodes and it uses one                                 or more fields values so it goes over                                 the value for each document applies the                                 hash function sees on which worker node                                 that hash functions value falls to                                 then that worker node will emit value                                 for that particular document so if you                                 have five workers each worker gets one                                 fifth of the total results to process so                                 that that is how it happens internally                                 the moment this the moment you send a                                 request to the / export handler on a                                 given node it basically starts streaming                                 instantly and it just keeps on swimming                                 until the data aims okay so on the                                 worker node get starts getting the data                                 almost immediately and it can start                                 doing useful stuff with it so this is                                 not really a replacement for Hadoop or                                 or basically high over all those kind of                                 systems where you have to wait for                                 minutes it is not built for that purpose                                 in mind it is built where you have a                                 decent or a large amount of data in                                 solar and you want to do stuff which                                 maybe takes a few seconds or sub seconds                                 it's not suitable when you want to do                                 some stuff which is going to take                                 minutes and minutes of your time okay so                                 because it's all streaming it's all                                 pretty fast and the good thing about                                 this is because it is all streaming and                                 it is all happening really fast and it                                 is happening in parallel if you can                                 afford to throw a lot of hardware at it                                 you can scale it really well so even if                                 you have very very complex queries you                                 can still get the data you can still get                                 your responses in a small amount of time                                 because you can use all that hardware so                                 all replicas shuffle in parallel for the                                 same query which allows for very large                                 throughputs the Verger collection is                                 basically it's just a made up name                                 really the vocal collection is just like                                 any other solo cloud collection it                                 doesn't even have to be a separate                                 collection and by default it is the same                                 collection which holds the data which                                 you are trying to operate on but if you                                 really do want to separate the the major                                 processing from the place the data                                 actually lives then you can create a new                                 collection and say that this is my                                 worker collection and then all the all                                 the difficult stuff all the major                                 processing the aggregations will happen                                 on those nodes                                 okay so by default the worker collection                                 is the same as the collection which                                 holds your data but you can choose to                                 separate it out okay so it so this guy                                 performs the aggregations there is an                                 HTTP endpoint called / stream which is                                 which has to be available and actually                                 that endpoint is by default available in                                 solar                                                                   have to do anything extraoral to enable                                 this as I said it might just be empty or                                 you might just created you know just in                                 time or you might just keep it around                                 with some empty nodes and not use it for                                 any other purposes then you send in an                                 SQL to solar what happens is we use the                                 presto SQL parcel which is a parser                                 created by Facebook so that parser                                 internally compiles that SQL statement                                 to the streaming io API is the streaming                                 io API is RC lies to swimming                                 expressions and then send to each worker                                 nodes and the worker nodes make requests                                 to each replica of each shard and get                                 the appropriate data out of it for                                 further processing so here we have a                                 diagram which which shows so this is                                 example where where we are going to use                                 the aggregation mode of MapReduce so the                                 client send sent in a skill statement to                                 the / SQL handler on some node this can                                 be any node part of the collection away                                 from the collection as long as it's you                                 know as long as it's inside the cluster                                 yerb you're good so use in this SQL                                 statement you probably had five worker                                 collections sorry five worker nodes as                                 part of a worker collection so you have                                 five workers as you can so this is an                                 interesting difference between a regular                                 distributed search and this SQL                                 execution in a regular distributed                                 search solar would choose one replica                                 from each shard right and it would send                                 there is the query to that replica get                                 the responses from all the shots merge                                 and give it back to you so at any given                                 time your query is using one replica                                 from each chart but in this case your                                 query is using all                                 replicas from all the shards okay so by                                 adding more replicas or by splitting and                                 creating more shots you can put massive                                 amounts of computation power behind a                                 single query which means you can do very                                 complex stuff very very fast okay so                                 this is an interesting difference                                 between the regular this which search                                 and this one so you have this skill                                 handler which parts the query created a                                 streaming io representation of it which                                 was serialized to swimming expressions                                 that expression was sent to each other                                 worker nodes the worker nodes again                                 requested data by using the / export                                 handler or maybe they use the facet                                 method in case you were using the first                                 method in this case we're not so they                                 will send requests to one replica from                                 each chart every worker will get one                                 fifth of the overall data to process                                 they will roll it up and then they will                                 send the whole response back there's                                 good handler so this is all happening in                                 a streaming fashion there's nothing                                 which is being kept to memory or being                                 spilled to disk here okay so any                                 questions until this point yes                                 everywhere how do you schedule your i/o                                 capacity basically it's totally possibly                                 to jam everything suggests yeah so so                                 this is probably not going to be useful                                 if you are also going to use it for                                 real-time queries because you are                                 basically going to suck up all the                                 bandwidth available right but then this                                 example is an example of very high                                 throughput very high cardinality                                 aggregation happening on a lot of data                                 in which case you want all that                                 computation power ok so again it depends                                 on your use case if you you you probably                                 wouldn't want to see anyway it's a bad                                 idea to mix very heavy analytical                                 workloads with real-time query workloads                                 right that's probably the a good advice                                 so you have to separate them out anyway                                 ok so this would probably be a bad idea                                 for the use case that you described yes                                 search                                 well it depends for example if you have                                 heavy distribute join requirements then                                 this poly would be the only way to do it                                 unless those features also get it see                                 the problem is when you have distributed                                 joints it becomes more and more                                 difficult to specify as it as part of a                                 classical solar query this way it's much                                 more easier because you can nest stuff                                 and build more complex expressions so in                                 fact I know of someone who is actually                                 using it for regular search actually                                 planning to use it for regular search                                 the only drawback here is because it is                                 streaming you are not going to get you                                 go you're not going to know the number                                 of results to show to the user so you                                 can even right now you can use it as a                                 replacement of classical solar query but                                 you know that there's a limitation to it                                 now there's no pagination here so you                                 can provide a limit laws if you're using                                 SQL you can do a limit and you can say I                                 just want the top                                                       if you actually in fact if you actually                                 provide a limit loss then you don't                                 really have to operate on Doc values you                                 can also get this the stored fees and                                 you can also use the regular solar                                 ranking but if you want to do analytical                                 stuff and then you can do exports of the                                 complete data sets so exporting complete                                 data sets has not been a very strong                                 point of search engines which is why the                                 the cursor mark feature was developed to                                 solar that allows you to paginate very                                 deep but this guy is actually built for                                 streaming out entire result sites yes                                 that's a great question so why was the                                 presto parcel picked I I think the                                 reason that Joey who initially wrote                                 this system gave was so he considered                                 calcite as well but calcite was far more                                 involved because it had its optimizer                                 and all those things so the API was far                                 more difficult for him to grasp at that                                 time and he just wanted to get something                                 up and running in fact now there is a                                 gia issue where he wants to move away                                 from presto and start using calcite                                 because he wants to use that optimizer                                 and all the stuff yeah so it's actually                                 one of you know in one of my slides on                                 the next things which are ya alright so                                 just quick two minutes on what's next                                 what's happening so in                                                  a graph traversals with streaming                                 expressions so this is interesting                                 because right now you have seen that                                 panel SQL is compiled to swimming                                 expressions what they plan is eventually                                 to have a gremlin language                                 instrumentation which is also compiled                                 to swimming expressions so then you will                                 have a gremlin language implementation                                 inside solar which can do distributed                                 graph traversals across collections                                 maybe okay so we already all we already                                 have a function for shortest path we                                 also have functions for random node                                 walking you can provide a query provide                                 filters and start walking nodes and                                 collect all the nodes you can also                                 record all the edges and then use it in                                 some place there is already a graph                                 email response format so if you want to                                 visualize it in something which                                 understands graph ml you can do that and                                 this is just the start you will you will                                 probably see a lot more happening in the                                 graph part this is probably its own talk                                 now on its own this is such a such a                                 vast topic the second interesting thing                                 is there is an implementation of a                                 logistic regression query which is                                 implemented as a streaming expression so                                 you can use documents inside solar to                                 train models and then you can use them                                 in some other way and how you can use                                 them is you can probably use the top                                 extremes that                                 mentioned earlier you can subscribe to                                 something get an alert you know execute                                 that document against a model which is                                 already trained and then make some                                 decision and send some alert to some                                 place so for example you can see hey                                 this document is very very important to                                 me or maybe that customer wants it or                                 something maybe more complex so I think                                 some of the use cases behind all these                                 things are not quite clear even to the                                 people who are implementing this they're                                 implementing this because it is yeah no                                 seriously I don't think the use cases                                 are quite clear to them it's just it's                                 possible now which whereas it was not                                 possible or it was not very easy earlier                                 and I think people are just waiting to                                 see what kind of use cases the users                                 come up with okay and then the                                 development will probably adjust                                 automatically because I know there was a                                 logistic regression part and the graph                                 traversal part was started almost at the                                 same time but there was a lot more                                 interest in the graph traversal part so                                 that's where the focus happened and it's                                 actually going to be release with                                     it's already committed to the solar                                 master branch right now so I'm probably                                 gonna see you you probably gonna see the                                 logic deviation and some other                                 algorithms implemented as more important                                 volved and start building stuff with it                                 you can do alerts using the topics and                                 the model streams there is also                                 something called a daemon stream a demon                                 stream is something which is like a                                 demon it it is running all the time okay                                 so you have a new document coming in it                                 smashed that demon stream can actually                                 send that alert to you so it's not just                                 based on the request it's something                                 which is there all the time and can do                                 stuff in the background you also have                                 something called an update stream so                                 right now it's just for queries but                                 hopefully you will also support things                                 like select into so you can do some                                 solar query some sorry some SQL query                                 and then get the responses and probably                                 update it into a difference or maybe the                                 same sort of collection so that should                                 also be possible very very soon as I                                 said calcite integration is being worked                                 on presto is going to go away it will be                                 calcite all throughout jdbc support is                                 being expanded more and more new kinds                                 of tools are                                 being supported and certified and the                                 top extreme and models team are also                                 going to be used for publish-subscribe                                 so there are lots of ideas are lots of                                 GI issues which are open a lot of them                                 don't have patches right now some of                                 them do some of them are already                                 committed this is actually a great time                                 to get involved with this feature                                 because even we don't know where it is                                 going to go I think it's all dependent                                 on what users build with it and what is                                 the interest of the community so with                                 that I am going to close this                                 presentation if you do we have time for                                 questions Thank You Charlene I think we                                 have time for only one question ok no                                 questions probably they can reach out to                                 your flying ok we have a coffee break                                 anytime all right thank you so much for                                 coming thank you
YouTube URL: https://www.youtube.com/watch?v=oOaMPsQ_RkU


