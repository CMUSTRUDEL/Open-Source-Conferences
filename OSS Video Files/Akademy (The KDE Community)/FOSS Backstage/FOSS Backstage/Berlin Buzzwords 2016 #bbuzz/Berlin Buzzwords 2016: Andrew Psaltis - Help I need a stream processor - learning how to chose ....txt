Title: Berlin Buzzwords 2016: Andrew Psaltis - Help I need a stream processor - learning how to chose ...
Publication date: 2016-06-11
Playlist: Berlin Buzzwords 2016 #bbuzz
Description: 
	Today if a byte of data were a gallon of water, in only 10 seconds there would be enough data to fill an average home, in 2020 it will only take 2 seconds. With this explosive growth comes the demand from consumers and businesses to leverage and act on what is happening right now. Without stream processing these demands will never be met, and there will be no big data and no Internet of Things. It is only a matter of time before you will be faced with building a real-time streaming pipeline. 

As soon as you embark on this journey, you will be faced with a myriad of questions. A major key decision you will need to quickly answer is which stream-processing framework should you use? When you survey the landscape you will find many contenders. In this session we will focus on the most popular open source frameworks, in particular: Apache Spark Streaming, Apache Storm, Apache Flink, and Apache Samza. 

We will dive into each of these tools and tease out all of the essential pieces you need to consider, compare and contrast them and end up with an understanding of how to evaluate each as well as future products.

Read more:
https://2016.berlinbuzzwords.de/session/help-i-need-stream-processor-learning-how-chose-between-spark-flink-samza-and-storm

About Andrew Psaltis:
https://2016.berlinbuzzwords.de/users/andrew-psaltis

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              stand here and go over benchmarks of                               which one's faster yeah but really to                               kind of go over the high-level concepts                               that you need to keep in mind that                               you're going to find throughout all                               these tools those fundamental concepts                               really don't change as the projects                               mature there's somewhat baked in and                               changing them could be hard so let's                                start with kind of thinking through a                                scenario here to get things going so                                let's imagine that we're working for an                                energy company that traditionally has                                gotten one record of data every                                        small format consistent data structure                                put it in your data warehouse                                everything's good but now you kind of                                want to take advantage of this IOT stuff                                going on jump on that bandwagon and you                                decide you're going to deploy power                                meters that are smart meters and as you                                do that now you're going to have                                commercial businesses that instead of                                sending you a record once a month are                                now going to send you a reading fifth                                every five minutes throughout the day so                                from one time to somewhere around like                                eighty five hundred times a month and                                then residential customers can't leave                                them out instead of that one time a                                month in this fixed structure are now                                going to be about every                                                ends up being somewhere around like                                                                                                        the course working for progressive                                energy company maybe want to provide                                different types of services to customers                                all right you want to allow yours                                consumers to log in and see what their                                power consumption is maybe you have an                                idea that you want to be able to incent                                people to use appliances at off hours                                well discount your energy bill if you                                run your washing machine at night or                                your dishwasher at night and I'd imagine                                that perhaps we want to look at our                                overall system usage to make                                infrastructure decisions all right so if                                you think about that it's a streaming                                problem that we would now have right so                                now we've got to set out to figure out                                what are we going to do and what do we                                use this is what we end up looking like                                all right a kid in the candy store he's                                sitting there going okay there's all                                these different projects what do I                                possibly                                as its spark is it flank based on you                                know the conference this week seems like                                flings a good choice right yeah Psalms a                                Kafka streaming they said there's all                                sorts from right as Apache apex there's                                different ones that come out all the                                time concorde know if you've heard of                                that recently came out it's now going to                                be the spark you know d throne spark so                                brand new project so as we start to                                think about this right there's some                                things that we're going to think to                                think about and how do we get out of                                this position the first thing we want to                                do is start thinking about time all                                right so as you're working with the                                stream of data time becomes important                                all rights no longer just at rest but we                                went from having one record every                                   days going into a warehouse to possibly                                thousands coming in every day every                                month that now you got to figure out                                what to do right not the same structure                                may have all the measurements from every                                device in your house because everyone                                now has a smart house the first thing                                we're going to look at is event time but                                so whether it's coming off of your                                washing machine cell phone smart car                                every time a message is generated that                                would be considered the event time of                                when it actually originated let's just                                assume for now it's going to go into a                                queue kafka is a likely choice could be                                something else but we're going to take                                this data and stream it into a queue                                somehow and coming over the wire from                                our house from our car we then have the                                streaming platform where we're going to                                pull that data in all right we're going                                to receive that data this you could call                                stream time fling folks did a good job                                 of calling it ingestion called ingestion                                 time stream time really is kind of the                                 same thing right it's the point that the                                 data is coming into that streaming                                 platform whatever one you use after you                                 receive it you're they're going to do                                 something with it it doesn't matter                                 whether it's an operator and flank                                 whether it's a bolt and storm where                                 there's a transformation spark you're                                 going to do something with this data and                                 there's going to be a processing time                                 where this happens                                 so as we look across these you got to                                 think about which does which right so                                 Psalms a spark storm all handled stream                                 time flink handles all these cases okay                                 can handle a vent time stream time                                 processing time these become important                                 as your processing data you're looking                                 at things right there's this notion like                                 I said of event time and then you have                                 stream timer and just in time and then                                 if we plot it without processing time                                 but what end up happening is you have                                 events that are generated going across                                 the bottom maybe for your washing                                 machine a car whatever it is that are                                 coming in at a certain time that a                                 time-stamped and then you have the time                                 that you're actually receiving them in                                 the platform and then possibly a time                                 that you're processing them over time                                 not to abuse the word but there's going                                 to be drift between those two and you're                                 going to get skew and you're going to                                 want to understand how does the                                 framework deal with that if we overlaid                                 on here the processing time we'd hope                                 that it follows the line of stream time                                 but it may not all right maybe doing                                 something that takes longer to compute                                 and there may be some skew there as well                                 so this is something to keep in mind as                                 you're working with these platforms and                                 as you're thinking about which one to                                 use and thinking about your data in some                                 cases it really doesn't matter in other                                 cases it matters a lot you may be                                 working with mobile devices or working                                 with things that are sending your data                                 add a batch interval and it's real time                                 for them and it's still a stream coming                                 in in the aggregate and you want to be                                 able to look at when did that actually                                 happen not do analysis and when you                                 received it or when you're processing it                                 but when the event actually occurred so                                 if you keeping score let's see how we're                                 doing with the concepts of time so storm                                 handle event time it also has the                                 ability to handle the processing time                                 spark string time only or ingesting time                                 it has no ability today it is obviously                                 changes that are                                 happening and there's things happening                                 with spark                                                       shipping so all this is based on what's                                 out there today only handle stream time                                 has no ability for you to look and run                                 windowing over when an event occurred                                 flink handles all three cases and psalms                                 as handling stream time alright its                                 preferred input is coming from Kafka and                                 it's just at that point of consumption                                 that you're seeing the data it's one                                 thing that we should mention about storm                                 as far as the processing time the way                                 that it works with trident and we'll see                                 this we look at like out of order is                                 when you're using it excuse me when                                 you're using that if there is a lag in                                 the data it will drop the data it will                                 log in but drop it okay and talk about                                 that as we go over some other stuff                                 someone doing so the next thing after                                 time is you start to think about windows                                 and how you're going to operate on the                                 data the first type of window we talk                                 about stumbling windows we're not going                                 to go over all the different types of                                 windows that are out there but really                                 talk about a couple of the major ones                                 that you'll see and depending upon the                                 framework you may see a lot more support                                 and more advanced support or less than                                 what we're even going to go over so                                 tumbling windows we look at two types                                 tumbling time it's the first one and the                                 way that this works is there's a window                                 length and there's a sliding interval so                                 there's a trigger that's the length for                                 when the data is processed and then an                                 eviction policy for what happens in a                                 doubling window and in tumbling time                                 these are both based on time as the                                 interval so time becomes the policy for                                 when things get evicted time becomes the                                 trigger for when to determine that a                                 window is full and after its process its                                 emptied                                 okay spark in Psalms oh don't support                                 this flink and storm do with the latest                                 version of storm the next version of                                 tumbling ends up dealing with counting                                 similar type of principles apply where                                 there's an eviction policy and a trigger                                 policy but these are based off of a                                 count not off of adoration okay so in                                 this case we have a interval of two and                                 a window length of two so Italy evicted                                 when you exceed more than two and the                                 window be processed when you have two                                 against park in psalms a-- don't support                                 this but flank and storm do the next one                                 we talked about is sliding window                                 similar type of thing where there is a                                 trigger and there's an eviction policy                                 except we're going to see with sliding                                 windows it works a little bit                                 differently as you can imagine based on                                 the name it slides the first type is                                 sliding time window if you look at the                                 spark documentation is just a sliding                                 window there's an interval and a length                                 this is the sliding when doing that                                 they're talking about so the way that it                                 works is you have a length based on time                                 that you're going to look at a window of                                 data and then you'd have an interval                                 that you're executing and seeing that                                 piece of data right so as the data moves                                 across you see the whole window and then                                 the eviction is going to happen on that                                 sliding interval as new data comes in                                 it's going to evict the oldest data okay                                 so it's going to you to slide forward                                 flink storm spark all support this sums                                 up if you squint hard enough and look at                                 it close enough kind of maybe it has the                                 notion of a counter you know in a                                 callback that you could get notified at                                 an interval of when something happened                                 but you don't get the window of data                                 alright so and all the other processing                                 frameworks you'd actually get the whole                                 window a data operate on                                 on in Psalms oh you're going to be able                                 to keep track of account so kind of                                 maybe depending upon how much you squint                                 may kind of feel like it's a sliding                                 window the opposite side that similar to                                 what we saw with tumbling is a sliding                                 count so in this case you can have a                                 window length that's going to be the                                 total size the number of tuples in the                                 stream and then the interval that's                                 going to be the number of tuples as well                                 so here we have a window length of eight                                 elements and a sliding interval of four                                 okay so you would see those eight                                 elements in your window of data and as                                 it slides and four come in you're going                                 to get others pushed out spark in Psalms                                 of no support for this flink and storm                                 do so the next thing that comes up is                                 and it often comes up when you start to                                 look at real data is it's messy right                                 data doesn't obey rules that you want                                 right it comes in when it wants people                                 shut off their device they turn on the                                 device things happen in batch mode when                                 it's on sailor or it son satellite and                                 things happen out of order I need to                                 kind of reason about how do you handle                                 it and what do you do and what kind of                                 support do you get from a framework you                                 may choose so if we imagine it kind of                                 looking like this that data is coming                                 along and time is marching on and we get                                 data at these intervals that are really                                                                                                          what do you do and how do you handle                                 that spark in Psalms up you're on your                                 own it has no support for it right spark                                 ends up being micro batch it knows                                 nothing about this data being out of                                 order all right this is a based on                                 knowing event time and it doesn't know                                 the event time it just knows the                                 ingestion time so you're kind of on your                                 own at that point to try and do it flink                                 not a problem carry on all right works                                 without a problem                                 storm it'll notice it and if that data                                 lags it will log it but it will discard                                 the data so you won't see it in the                                 processing okay so out of order data is                                 basically just dropped on the floor but                                 there's logging to let you know that                                 it's dropping data on the floor so at                                 least you could try and do something                                 with it so let's see where we are on the                                 scorecard for this so windowing                                 windowing scorecard temporal tumbling                                 windows could do in storm to do in flank                                 counting temporal windows or counting                                 tumbling windows not a problem in storm                                 or fling sliding time windows supported                                 by all and again Psalms or the caveat                                 that you really got to kind of think                                 about it differently and it's much more                                 of just getting notified at a certain                                 interval something happen every                                    seconds you want to have a call back                                 into a handler you get notified that                                 happen you could count the events that                                 are going on sliding windows as support                                 count again storm flink and then flink                                 probably not surprisingly looking at                                 this has a lot of custom when doing that                                 you could do and you can create your own                                 windowing and different advanced windows                                 that you could do as well so if you                                 keeping score here I'd say from the                                 windowing standpoint of window becomes                                 pretty important to your use case it's a                                 leader right there out of order data                                 sparks not going to give you direct                                 support for it you got to figure out                                 what to do you could probably do                                 something with it again it doesn't know                                 anything about event time flink can                                 handle it just fine and storms going to                                 discard the data for you but then log it                                 that it let go of your data for you                                 the next thing to think about after                                 we've covered time and then covering                                 windowing is the processing semantics                                 and what type of semantics is a                                 framework you want to use support and                                 really need to also bounce us against                                 what makes sense in your use case so the                                 first one which is the weakest guarantee                                 is really just at most once so in here                                 we have this incoming stream of data                                 message is coming in in this case we                                 have to we'll just call this a stream                                 processor of somewhere inside of the                                 streaming platform our business logic is                                 running and doing work and then one                                 message and goes out that's it right so                                 at most one time but it may not go                                 through all right so you're kind of just                                 left going okay at least maybe get the                                 data similar type of thing in this case                                 if it crashes you may not get any data                                 that comes out right at most once                                 doesn't guarantee that it'll actually                                 deliver anything so Samsa does not                                 support this spark flank storm and do                                 moving up from a guarantee standpoint to                                 stronger guarantee we'd have at least                                 once right and in this case you're going                                 to be guaranteed that a message is going                                 to be processed by the framework at                                 least one time okay so you're guaranteed                                 to see it at least once you may see it                                 more so in this case if we send two                                 messages in and something crashes we                                 have an undocumented feature that takes                                 down our processor comes back up perhaps                                 there was some state that was saved and                                 now we may see three messages that come                                 through right so as you're thinking                                 about this and as you're building things                                 that downstream from there you need to                                 be able to take into consideration that                                 you may see the same message multiple                                 times okay it's not a guarantee that                                 it's just once it's at least once                                 so storm you can make this work you have                                 some work to do to do it it gives you                                 the facility to do it but you have to do                                 the work for it in spark flanken Samsa                                 you have this the next one that everyone                                 clamors for and everyone wants in a lot                                 of cases though it may or may not make                                 sense you have exactly once and this is                                 really what it says you get two meses                                 going in your processor could crash                                 something could you know pull the cable                                 out of the wall whatever may happen                                 going to guarantee that the message was                                 processed exactly one time okay to pull                                 this off a lot of the frameworks going                                 to have to interact with some other                                 system right if you're using Kafka as                                 the incoming stream of data it provides                                 you the mechanism to implement exactly                                 once but it doesn't implement it for you                                 right and then it's same with another                                 system so coffee gives you the offsets                                 but you have to storm somewhere else                                 writes you storm in some other data                                 store some distributed system so that                                 you have control over those and you go                                 back and ask for if the data goes away                                 before you get a chance to go back and                                 get it you're kind of you got to look at                                 that point but at least from the                                 streaming side of things assuming the                                 data is still in whatever q you're using                                 that's feeding it then you'd be able to                                 reach over and grab that data and pull                                 it in okay so storm spark and flink all                                 could do this and sums a can't at this                                 time so let's check our scorecard for                                 those keeping track so storm spark flink                                 all ok without most once at least once                                 in storm you have to do if you use a                                 non-transactional you get at least once                                 you get that support across the rest of                                 the frameworks exactly once you just                                 tried it and you can get that with storm                                 not going to go into the pros and cons I                                 tried                                 some people fans some people don't like                                 it that becomes a religious battle as to                                 which way you fall in the fence there                                 and with sparking flink you get the                                 support storm also has another guarantee                                 another semantics that's called best                                 effort not quite sure what's really                                 meant there but it supports best effort                                 so I guess if it falls outside of one of                                 these and something really goes haywire                                 is a kind of you know a disclaimer of we                                 gave it a best effort and tried okay to                                 do the exactly once with Trident though                                 and do exactly once a storm your left                                 doing work okay the recommended pattern                                 if you look at it is to get that to work                                 when you recover you need to keep the                                 data in another store and you need to                                 keep track of current value previous                                 value transaction ID the exactly once                                 guarantee they're able to pull off                                 because they guarantee that the batch of                                 data you're looking at always has the                                 same transaction ID and that would be                                 the same if you restart okay so you'd                                 see those again but you may get the data                                 downstream and have to do the work to                                 figure out did I see this before so that                                 brings us to the next thing we need to                                 think about and then ends up being state                                 and there's really two types of state                                 that we're going to be interested in the                                 first one is application state or user                                 defined state alright and this really                                 has to do with your applications running                                 what type of state do you want to keep                                 what type of state does a framework                                 provide for you or state facilities for                                 you so you could keep it right if you                                 spark you have update state by key right                                 so you have these different state                                 notions so you could hold on to state                                 for your computations all right it                                 really kind of looks like this right if                                 we just had this simple state that we're                                 keeping we have this incoming stream                                 let's assume it has some IDs and you                                 know letters associated with them                                 perhaps it's which device I was using                                 you want to group those                                 they want to account and we're going to                                 want to keep say that state for the                                 current hour right so we have the state                                 in that count by user ID that we're                                 holding on to and we want to have a                                 running aggregate of what's happening                                 the current hour and then we send it to                                 some output right if you attended                                 Stefan's talked yesterday about flanking                                 about some of the benchmarking and                                 things that they're working on or you                                 know as you start to really push that                                 and really try and make that work in                                 production at high volume the bottleneck                                 becomes that output alright that output                                 is another data store you're going to                                 have limitations on the network or                                 limitations on this other store that                                 you're trying to push these aggregates                                 to somewhere you need to get the data                                 out to present it and to do take some                                 action with it right at this count by                                 user ID area right here we're keeping it                                 you're still inside of this platform                                 right and the output would be some                                 destination you sending it to whether                                 it's to Cassandra or to hbase or the                                 Redis or TV or whatever it may be it's                                 going somewhere so to get away from that                                 you could kind of do something like this                                 so this queryable simple state                                 ironically this was talked about                                 yesterday and there is a flink jira for                                 this so it's going to be put into flink                                 as an option or has a capability that                                 now as you're keeping state for the hour                                 you'd be able to go back and query that                                 system it's the only framework once                                 that's complete that will actually have                                 a story to tell with this I've done this                                 before in storm a couple years back so                                 if you're interested in talking about it                                 be more than happy to offline from here                                 some interesting things that you have to                                 do to make it work but there was not a                                 whole lot of framework infrastructure                                 provided to make it happen so pretty                                 compelling use case if that is actually                                 going to be a capability within sight of                                 link because now you could have clients                                 that are live querying a stream without                                 having to go elsewhere all right so it                                 becomes pretty powerful from a                                 dashboarding standpoint or from a real                                 real time if you will notification of                                 what's happening in stream all right in                                 this case we're just showing you know                                 counting by user ID but you                                 imagine being a variety of other things                                 when you think about why it's not there                                 you know you really look about this                                 chart here of like the complexity and                                 the features you're doing it in memory                                 of you just holding it and even if it's                                 for an hour and you're provided you know                                 the ability from a framework to say you                                 could hold on to the state and we'll                                 make sure that if you crash we have it                                 it's persisted and when you restart                                 it'll be rehydrated it's pretty low in                                 the complexity graph as you go across                                 though and now you want to have this                                 replicated queryable persistence store                                 that's live in the stream the complexity                                 goes up and the features required go up                                 so hard for some systems to bake this in                                 after the fact you know flink based upon                                 design as you may have heard yesterday                                 during Stefan's talk a lot of the                                 underlying plumbing was already there                                 it's a matter of surfacing it in spark                                 this becomes hard because it's micro                                 batch and storm becomes hard because                                 it's just not designed to handle some of                                 this and in sums of its it's not going                                 to happen soon it's just not not its                                 model right and its really nothing about                                 Songza but this is not designed for                                 doing this it has the state but it's not                                 part of the model the next state which                                 is really kind of what you see when you                                 start to look across is a system state                                 all right this is everything else that                                 these frameworks provides you to make                                 sure that when things crash because they                                 will that it's recoverable                                 state most of the time involves                                 checkpointing or snapshotting we're                                 going to kind of lump them together and                                 just call them checkpointing for right                                 now we're just going to look at how each                                 of them handle checkpointing you're so                                 spark is running along and at different                                 time intervals you could choose to check                                 point data okay the checkpointing is                                 only going to happen to have it actually                                 be reliable in a CFS or s                                             distributed store today those are the                                 two supported and that may change over                                 time but it's based upon those batches                                 of time I can see as it goes through                                 each time you're not losing data but                                 it's doing checkpointing and keeping                                 track of what's going on something that                                 works differently it has local state by                                 default rocks DB and it will constantly                                 be writing data to it and at the same                                 time it's streaming that data to Kafka                                 okay so as a changelog that's constantly                                 feeding into Kafka so in this bit from                                 this perspective you're pretty safe                                 rights you have data going to a local                                 rocks TV and you have data being                                 committed to Kafka as well so it's                                 constantly checkpointing flink it's a                                 little bit different right it stays a                                 hundred percent as a stream so in this                                 case it actually have these esas                                 indicating like a snapshot not a time                                 window but a snapshot that's occurring                                 you could do it in memory at a certain                                 point though you're going to want to use                                 some sort of distributed resilience                                 store perhaps HDFS to capture this                                 information but again it's not turning                                 it into a micro badge this is still                                 happening in the stream as part of                                 snapshots                                 storm gets turned into a microbe at                                 system when you want to do this so went                                 from being an event-based to now be a                                 micro batch okay so as windows of time                                 go on as batches happen now you need to                                 commit these checkpoints to their HDFS                                 into memory pretty risky into Cassandra                                 into some data store that you could                                 trust so we just turn storm from a vent                                 base to micro batch so let's see how                                 we're doing in relation to that and                                 again this checkpointing is for recovery                                 purposes so at least once Samsa still                                 maintains that guarantee and exactly                                 once storm spark and flank maintain that                                 guarantee as well so you don't lose the                                 guarantee even when you have this                                 involved and again with storm there's                                 work to do so hopefully now we kind of                                 got closer to this right we kind of                                 almost have an idea as to what Jar candy                                 we want to reach for and now it's a                                 matter of applying the rules and                                 figuring out what makes sense in your                                 business thank you for your time and                                 take any questions they may have                                 hi thanks for talk yeah we heard a                                 little bit in the keynote this morning                                 about Kafka streams have you had a                                 chance to look at that and how that fits                                 into this landscape resuscitation you                                 looked at it a little bit I haven't put                                 it through paces but sire I can't speak                                 that much to it but it's going to try to                                 accomplish some of these things and it's                                 going after the similar thing right and                                 trying to make streaming not depend upon                                 a platform right for all the bells and                                 whistles you get from all these                                 platforms there is a pretty unique                                 advantage from Kafka streaming that you                                 install Kafka you have a library you're                                 ready to go there's definitely times                                 where having to have one more cluster                                 deployed becomes a lot right so I'd                                 imagine there'll be some opportunity                                 there and it definitely will serve some                                 use cases how would a matchup I think is                                 kind of early to tell any more questions                                 thanks a lot Andrew thank you
YouTube URL: https://www.youtube.com/watch?v=sZ2w0e8taDs


