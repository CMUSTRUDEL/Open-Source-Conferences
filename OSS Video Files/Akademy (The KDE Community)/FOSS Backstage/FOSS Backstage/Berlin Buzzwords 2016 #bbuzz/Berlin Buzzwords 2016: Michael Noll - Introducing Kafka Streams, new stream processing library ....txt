Title: Berlin Buzzwords 2016: Michael Noll - Introducing Kafka Streams, new stream processing library ...
Publication date: 2016-06-12
Playlist: Berlin Buzzwords 2016 #bbuzz
Description: 
	In the past few years Apache Kafka has established itself as the world's most popular real-time, large-scale messaging system. It is used across a wide range of industries by thousands of companies such as Netflix, Cisco, PayPal, Twitter, and many others.

In this session I am introducing the audience to Kafka Streams, which is the latest addition to the Apache Kafka project. Kafka Streams is a stream processing library natively integrated with Kafka. It has a very low barrier to entry, easy operationalization, and a high-level DSL for writing stream processing applications. As such it is the most convenient yet scalable option to process and analyze data that is backed by Kafka.  

We will provide the audience with an overview of Kafka Streams including its design and API, typical use cases, code examples, and an outlook of its upcoming roadmap. We will also compare Kafka Streams' light-weight library approach with heavier, framework-based tools such as Apache Storm and Spark Streaming, which require you to understand and operate a whole different infrastructure for processing real-time data in Kafka.

Read more:
https://2016.berlinbuzzwords.de/session/introducing-kafka-streams-new-stream-processing-library-apache-kafka

About Michael Noll:
https://2016.berlinbuzzwords.de/users/michael-noll

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              hey thank you                               thanks for joining I know it's been a                               very hot day at a very cool conference                               and you're close to the end of the first                               day so thanks for attending this session                               is about Apache Kafka and Kafka streams                               which is part of Apache Kafka my name is                               Michael naal I am an engineer turned                               product manager working for confluent                                confluent is a US based company funded                                by the creators of Apache Kafka we are                                headquartered in Palo Alto California                                and we're building a stream data                                platform at the heart of which is Apache                                kafka since we started about one and a                                half years ago we attracted the great                                majority of kafka committers to join our                                team and we are working on improving                                Apache kafka and introducing kafka                                streams of course and also to build the                                larger confluence platform so personally                                I am native German based in Switzerland                                not in California so I'm actually quite                                happy that I have this opportunity to                                come to my home country to Berlin                                Bosphorus and speak with all of you                                about interesting technology so thanks                                for having me now before we start let me                                also get to know you a little bit so let                                me ask the question who of you has heard                                of Apache Kafka before if you have just                                raised your hand well that's I think                                basically everyone who if you are using                                Apache kafka in production whatever the                                different different of production is                                it's                                                                    so still there are some of you that some                                of you that don't really know yet what                                the Apache kafka is and what Kefka                                streams is so let me start with an                                analogies that we are all on the same                                page and since we're in Berlin where we                                have so many critical phase I want to                                use proving delicious espresso as an                                analogy so imagine you have a                                data-driven product and that product                                would be a very create a sprayer so what                                would you need to prove that coffee so                                if you're at home what you would need is                                two things you need water which is a key                                ingredient for a good coffee and you                                need to get the water to your home the                                water pipes so Kafka would be these                                water pipes                                what's important is that this                                infrastructure is reliable there is a                                continuous stream of the water available                                to you because if it isn't you can't for                                coffee and there are further challenges                                like you know you have some problems to                                go to the restroom but all of that is                                not sufficient for prune your coffee so                                you need something that turns to water                                and some chronic coffee into your                                espresso so we need a coffee machine for                                that and kafka streams is the equivalent                                of a coffee machine in that analogy now                                once you have all these things in place                                you have the water you have convenient                                access to the water in a reliable way                                there oh no you know water leaks and so                                on and you have a very good coffee                                machine then you can create a great                                product and have happy users now before                                we talk about kafka streams let me take                                a step back and revisit what the current                                experience is if you venture into the                                realm of stream processing so that is                                before kafka streams and of course I'm                                somewhat exaggerating to try for my                                point but not all that much so if you                                decide that my problem is processing                                data let me pick a stream processing                                tool and then I'm not good the reality                                is a bit different any one of you has                                really worked with that in production                                you know this is not a frictionless                                experience and figuratively speaking the                                question that I wanna ask is how did                                something that is as simple as this if                                you're running on a single machine turn                                into something like that even if more                                than one machine so I'll not be talking                                about lambda architecture or going like                                through all these things that are good                                and bad my point is there is this                                tremendous discrepancy between how we                                would like to work and how we actually                                do work in practice so when you're                                 setting out to rethink how you can do                                 this rehearsal or when you set out to                                 evaluate existing stream processing                                 tools you should ask yourself the                                 question is the tool part of the                                 solution or part of the problem or is it                                 actually adding another problem to the                                 ones that you're already                                 and I think if you want to build a tool                                 that is part of the solution a good                                 strategy is to make complex things                                 simple and eventually thereby easy and                                 fun now what do we gain if we follow                                 that strategy one of the games is                                 developer efficiency and that sounds a                                 bit like a management password so let me                                 rephrase that                                 what we gain is that we scale the UN                                 site like us our teams because we don't                                 get better every                                                       Apache Kafka has done a great job of                                 providing a solution for data messaging                                 transporting the data getting it from A                                 to B and so on you've heard a couple of                                 tours already today that we're making                                 reference to Kafka as a key part of                                 infrastructure now we wanted to do the                                 same thing for stream processing with                                 Kafka streams so following the same                                 principle of making complex things                                 simple easy and fun now what is Kafka                                 streams Kafka streams is a powerful yet                                 easy to use Java library it's part of                                 open source Kafka and the source code is                                 available under the Apache cover project                                 using Kafka streams you can build your                                 own applications that are thanks to                                 Kafka and Kafka streams highly scalable                                 follow tolerant stateful and so on we                                 talked about that in more detail now                                 before we take a closer look I want to                                 give you the bigger picture if you have                                 Kafka in the center of the                                 infrastructure like your central nervous                                 system for all your data                                 Kafka streams is the part that reads                                 data from Kafka transforms it and adds                                 value to that data and writes the result                                 back to Kafka and then there are other                                 parts of your infrastructure that get                                 the data into Kafka and out of Kafka                                 such as Kafka Connect which is also part                                 of the Apache Kafka project so again the                                 coffee machine that is Kafka streams now                                 what that picture might be good for                                 people that are already familiar with                                 Kafka some of you might not be that                                 familiar yet so let me use a UNIX                                 analogy if you have a command pipeline                                 Kafka would be the pipes Kafka's dreams                                 would be the commands and cough connect                                 or the producer and client applications                                 that you can build would be the parts                                 that get data into and out of the                                 pipeline the last an energy I want to                                 make is for those people that are more                                 familiar with Java                                 so when Java was released in its initial                                 version in                                                             just a single core so that's when you                                 have things like Java dot Lang some                                 years later mighty core machines were a                                 much more common at which point things                                 like Java you to concurrently introduced                                 now today if you have multi-core                                 machines and you run applications that                                 spend Multi machines but unfortunately                                 there is no Java distributed but thanks                                 we have Kafka's dreams that will fill                                 that gap for you so before we take the                                 extra closer look on Kafka's dreams let                                 me talk a little bit about when you                                 would like to use it so things that you                                 would like to use it for is really                                 building stream processing applications                                 today there are already some reference                                 to fast data applications so things that                                 create continuous stream of data and do                                 Tonkinese continuous transformation to                                 it                                 things like reactive and stateful                                 applications and so on things where you                                 might look elsewhere at least at this                                 point is everything that is like heavy                                 lifting                                 data mining machine learning and so on                                 now here yeah car shut up just show me                                 some code right okay so in Kafka streams                                 you have two options the first API is a                                 DSL that allows you to express your                                 programming logic in a declarative way                                 that looks pretty much like Scala and                                 that would actually be the equivalent of                                 the scale exam that I showed at the very                                 beginning the second option is a lower                                 level processor API which looks a bit                                 more like imperative programming so you                                 have options to you know start up your                                 processors you can tell it how to                                 process every incoming record you can                                 schedule periodic activities and you can                                 specify what happens on shutdown                                 so that's a quick look at the API but                                 actually you will not hear me talking a                                 lot about the API and the reason for                                 that is I think the API is just a tiny                                 part of what is important when you're                                 picking a tool such as a stream                                 processing tool there is a whole lot                                 that happens underneath underneath the                                 surface that is very very important and                                 actually much more important than what                                 you see as a developer for example                                 operations so when you look at a tool or                                 when you set out to design such a tool                                 you want to do a full stack evaluation                                 of everything that will happen what is                                 the benefit that you're getting and at                                 which cause does this benefit come so in                                 kafka streams just like in Kafka we                                 wanted to reduce everything that lurks                                 underneath the surface as much as we can                                 and we're doing this by making complex                                 things simple easy and fun but how do we                                 actually do this at a high level and the                                 answer is we are outsourcing hard                                 problems to Kafka they're already solved                                 there why should we reinvent that now at                                 this point you know typically see like                                 people like raising their eyebrows so                                 now okay you know how tell me how do I                                 install this thing said well you don't                                 install Kafka streams and there should                                 be no installation it's a Java library                                 edit reallocation just like any other                                 library so then people typically have a                                 pause and they stirred me given this                                 suspicious look an Aspie all right where                                 is the cluster and the answer is I think                                 some of you might already expect the                                 answer there is no cluster if you don't                                 mean the cluster why should you since                                 when did we have to use the cluster when                                 we set out to do cool things with data                                 so Kafka streams allows you to stay lean                                 and lightweight if you want to and go                                 all out with clustering if you want to                                 now the next question then is okay how                                 do I parent deploy my application so how                                 do I XYZ and I think the reason why                                 people ask that question is if you're                                 working on data in your company your                                 typical like the guy in the yellow shirt                                 who's the social order or the                                 party-pooper who does things differently                                 than all the                                 of the organization you have a different                                 rate to run your applications to                                 interface with applications and you                                 actually have a hard time making a case                                 for why people should go for whatever                                 tool you're suggesting                                 so with Kafka streams what we wanted to                                 do is give you the option to do whatever                                 you want to and we feel it's very                                 important that the stream processing                                 tool should not be opinionated and                                 dictate rules how you package it how                                 deploy it and so on so that means you                                 can look at this in a coolness fraction                                 of let's say deployment technologies you                                 can do either very uncool boring or                                 technology that actually work or you can                                 move all the way to the other extreme of                                 the spectrum and pick the latest                                 craziness on Twitter whatever is                                 mentioned there that probably works so                                 this was more like the look and feel of                                 Kafka streams let's talk a little bit of                                 it up about the underlying concept of                                 Kafka streams and I'll start first with                                 the concepts of Kafka so since most of                                 you are familiar with it I'll just be                                 very proven concise so in Kafka you have                                 producers that write day dr. Kafka and                                 consumers that read that data from Kafka                                 and the important part is that reading                                 and writing is decoupled the service in                                 your Kafka cluster other procures who                                 are responsible for serving and storing                                 the data now if we zoom into the Kafka                                 cluster your data is being organized by                                 you into topics for example a topic that                                 captures user activity events within a                                 topic that has been partitioned within a                                 partition that is strongly ordered and                                 these partitions are distributed across                                 the Kafka cluster for enough scalability                                 allowed balancing and if you add                                 replication also for fault tolerance now                                 within a partition the messages are                                 actually key value pairs and I'm only                                 mentioning this because you will see a                                 very strong similarity now that we're                                 switching to the Kafka streams concepts                                 in Kafka streams you have a stream of                                 data which is an ordered sequence of key                                 value records                                 these data streams are being processed                                 by processor topologies where the notes                                 are stream processors and the edges are                                 the streams if you zoom into that                                 similar like Kafka has topic rotations                                 Kafka has a streams has stream rotations                                 and stream tasks are basically copies of                                 the same processing logic that are                                 getting a subset of the data that needs                                 to be processed so I will not go into                                 detail here just give it like a higher                                 level view of what that is the key                                 takeaway is Kafka partitions data for                                 transport and Kafka streams partitions                                 data for processing so there is a close                                 mapping between the two concepts the                                 reason why I am very brief here is                                 because I want to talk about something                                 that is more interesting and that is                                 there is a close relationship between                                 streams and tables now if you look at                                 the first column that's like your                                 typical database table and what you see                                 is you have like insert and update                                 statements that mutate the table if you                                 capture all these changes then you get a                                 stream of changes a changelog stream                                 once you have that you can reconstruct                                 the table on the left side at another                                 location which is what you see on the                                 right side so you can take a table turn                                 into a stream turn it into a table turn                                 it into stream and so on and we'll see                                 why this is so important and let me                                 start with an example so here we have a                                 very simple stream of data and we look                                 how this stream that is interpreted                                 differently depending on whether you are                                 a stream or a table if you are a stream                                 you look at that data and think okay I                                 improved this as a stream of data                                 records if your are at the table you                                 interpret the same data as a change book                                 stream and the table becomes a                                 continuously updated materialized view                                 of that stream so at this point in time                                 both the stream and the table would say                                 Alice clicked two times if there is a                                 further update for Ellis the stream                                 would say actually Alex clicked five                                 times first the tables have no no                                 she'd neatly - she clicked three times                                 so that example is a bit maybe not so                                 easy to understand but I wanted to show                                 the same data let me use a more                                 practical example where we have                                 different data the first example is user                                 purchase records that are interpreted as                                 a string and the other example is a                                 table with user profile information at                                 this point in time the stream would say                                 Alice bought eggs in the supermarket the                                 table would say LS is currently in Paris                                 moving forward in time the stream would                                 say Alice Bock bought both eggs and milk                                 and the table would say Alice is not                                 anymore in Paris she's now in Berlin at                                 buzzwords so the difference is the                                 stream shows you all the values for key                                 and the table gives you just the very                                 latest value of the key so in terms of                                 the Kafka streams DSL there are                                 operations that transform from a table                                 to a stream and from a stream to a table                                 you can always convert the table to a                                 stream by just iterating through the                                 table but if you are converting from a                                 stream to a table need to provide                                 semantics if you have a list of input                                 values you must define when this is                                 something only use user know how to                                 squash all these values into one single                                 value as an end result so a simple                                 example is a joint so let's say you                                 compute user clicks by region and your                                 input would be a list click                                              the other input would be alice is                                 currently in Europe this is how it would                                 look like in the Kafka streams DSL and                                 again I'm not walking through the code                                 I'm only saying that there's actually                                 this part that we will look at now in an                                 example and also mentioning there is                                 this region with clicks thing that you                                 may or not be able to read and the                                 reason we have that is only because Java                                 doesn't support tuples natively in Scala                                 it's much more natural so in this                                 example we have an input stream which is                                 a stream we're joining it with a table                                 to know when a let's click                                              was in Europe and then we are changing                                 the key because we want to aggravate by                                 user region and not by user name and                                 it's to the string and now we are                                 creating this information to come up                                 with                                 total count in this case Europe is                                    clicks after the first event and then                                 it's                                                                     going back to that picture there are                                 certain transformations that change the                                 shape of your data structure and certain                                 transformations that don't and it's                                 important in our opinion that your                                 stream processing tool understands that                                 difference and gives you the tools it                                 needs to make proper decisions what is                                 correct for your use case so lastly one                                 thing that might not be totally obvious                                 from that example this table when you're                                 doing a join for example it's                                 continuously being updated with the                                 latest data behind the scenes because                                 it's being backed by a cafe topic so as                                 soon as your your stream that your that                                 is joined against that table gets                                 another input the latest value is                                 automatically retrieved from that table                                 so that is very cool for building                                 stateful applications all right so we                                 talked a little bit about you know look                                 and feel of Kafka screams what it is                                 when you should use it we talked a bit                                 about key concepts and so on now let me                                 talk a little bit about the key features                                 of the current implementation first of                                 all let me take a drink so of course by                                 definition Kafka streams is                                      compatible with Kafka so if you have                                 ever had various that your latest                                       interface well with Kafka that problem                                 is served in Kafka streams also Kafka                                 streams inherits Kafka security model so                                 you can encrypt data in transit for                                 example restrict access to certain                                 topics which is great if you reckon on                                 sensitive data like personally                                 identifiable information in terms of the                                 API of course functions that turn a                                 Kafka topic inter stream a Kafka topic                                 into a table or a table back to Kafka                                 topic or stewing bag into a casket topic                                 and when you're writing applications                                 that use the Kafka streams library you                                 can figure both Kafka stream specific                                 settings as well as settings of the                                 underlying Kafka producer and consumer                                 clients if you're already familiar with                                 Kafka I think that will be kraid if                                 you're not yet familiar just skip that                                 part it just means that yeah you have a                                 lot of flexibility to fine-tune your                                 application if you need to                                 when you're using kafka streams you                                 automatically get the following                                 properties for your application high                                 scalability fault tolerance and                                 elasticity and how does that work so                                 imagine you have data in a cupcake                                 cluster and your task is it to create an                                 application that process the data to                                 create something like an espresso                                 initially one machine might be                                 sufficient for presence all the data but                                 it's someone need to scale out how do                                 you do that in Kafka streams                                 well you just start literally the same                                 application the same code on another                                 machine these two instances will become                                 aware of each other and start dividing                                 the work if you need more machines just                                 add more three four ten                                                one of the machines goes offline failure                                 maintenance you don't need the capacity                                 and longer the other machines will                                 become aware of that and start taking                                 over the work now when I started out                                 memory design data that is just I don't                                 believe that that just can't be true                                 right and the reason why that works is                                 and I'm not going into details here is                                 because we are outsourcing hard problems                                 to Kafka Kafka has already concepts like                                 consumer groups and so on that does this                                 rebalancing work between new machines                                 machines that go offline so we're just                                 using that don't need to implement                                 anything ourselves in cupca streams you                                 can also do stateful and stateless                                 computations and an example of that                                 would be aggregations or joints like the                                 ones that we have in an example so state                                 stores in Kafka streams allows your                                 application to manage state these state                                 stores are isolated that is per stream                                 task they are local for best performance                                 you don't have to do like lookups over                                 the network and they are replicated to                                 Kafka for allowing two things elasticity                                 scale out shrink down and fault                                 tolerance let's go back to that example                                 and caveat this is a very simplified                                 view because of lack of time so be                                 careful when you jump to conclusions hey                                 this doesn't scale well or so on because                                 it's really a simplified view so imagine                                 you're if you're three instances of your                                 application and they do some stateful                                 stuff so these applications will                                 this instance would have state stores                                 one or more now remember that we talked                                 about how a table and the key value set                                 store is a table is also a stream and                                 that again can be converted back into                                 the table so whenever your state stores                                 change when they're being mutated we're                                 sending this information to Kafka and                                 persist it there so as soon as one of                                 the instances goes offline the other                                 instances can take care of the network                                 by reconstructing the local state at the                                 point of failure taking down for                                 maintenance and so on and resumed the                                 processing it doesn't need to God always                                 to one machine like in this example but                                 as I said it's a simplified view how                                 does it work again we're punting the                                 hard problems to Kafka there's nothing                                 that we need to do in Kafka streams just                                 like back pressure I will not be talking                                 about back pressure here because there                                 is no need for something like that in                                 the way Kafka and Kafka streams work now                                 state stores like how complicated is                                 that actually if you use the Kafka                                 streams DSL you won't even be aware of                                 these state stores they are abstracted                                 away from you so if you doing counts for                                 do you see aggregations you will not                                 even see them if you use the low level                                 processor API you have direct access to                                 these state stores so you're very                                 flexible to have more money at work but                                 still it's relatively simple when your                                 processors start up you can get a                                 reference to one or more state stores                                 and then when you're processing records                                 you can do like gets and puts and so on                                 so the other thing that is important for                                 a stream processing tool is that it                                 properly models time I think the                                 canonical example is the Star Wars                                 franchise so in Star Wars we have seven                                 episodes and if we look at the event                                 time when things happen in that universe                                 the first episode was the Phantom Menace                                 with Jar Jar Binks unfortunately                                 processing time was when they were                                 released and filmed it's quite different                                 the first one was in                                                   so if you convert that to Kafka streams                                 and Kafka event on would be the time                                 when the producers would generate the                                 original event and write its end                                 - Kafka processing time would be when                                 your cough costumes application actually                                 looks at the data and does something to                                 it and there's also ingestion time which                                 is the time when the prokhor like the                                 Kafka cluster received the magic message                                 for the first time and in Kafka strains                                 you configure the desired time semantics                                 through times than extractors and by                                 default you get event time semantics and                                 here you see an example of event time                                 and processing time it's pretty simple                                 it's a one-liner how to do that                                 you can use the same to extract existing                                 time stand out of your payload if you                                 already have a stream that has that                                 information available so the other part                                 that is already relevant in the first                                 version of Kafka streams is windowing                                 it's very very related to time so I'm                                 trying to make a link between the two if                                 you have a continuous stream of data                                 sometimes you've used cases where you                                 want to break this into discrete chunks                                 so for example you want to say something                                 like I want to get aggregates over a                                 period of three seconds and every few                                 seconds I want to get an update order                                 latest aggregation result is like and                                 that that's what you would call a                                 tumbling time window so you can easily                                 express that in Kafka streams similarly                                 there is a generalization of that called                                 hopping windows where you say I want to                                 get aggregations for three second                                 periods of time but I want to get                                 updates sooner like every second not                                 just every three seconds and then you                                 have multiple of these windows in fly                                 that you won't even notice what Kafka                                 streams manage that behind-the-scenes                                 for you and why would you need that                                 there are many use cases I just show                                 maybe the most obvious one is if you                                 have a monitoring application like                                 confident control center you want to                                 compute and visualize times years of                                 data and you want to do this across                                 different levels of granularity for                                 example one minute aggregates                                       attacker gets and so on so lastly there                                 are three things I've seen like we have                                 five minutes left                                 there are few things that I just talked                                 about very briefly one Kafka stream                                 supports late arriving and out of order                                 data so for example if you have a                                 five-minute window                                 sometimes messages don't arrive within                                 these five minutes imagine you have an                                 Internet of Things platform and you're                                 collecting data from devices that are                                 distributed all over the planet and                                 sometimes these devices have unreliable                                 internet connectivity so you might get                                 an update from an advice but within five                                 minutes but maybe after an hour or maybe                                 after a day so with Kafka streams you                                 can define how these lady writing data                                 should be interpreted by application on                                 describe them do want to send an update                                 downstream and so on in terms of how                                 fast or what's low latency of Kafka                                 streams applications it has millisecond                                 processing latency because it's it's                                 doing actually one message at a time                                 processing and has no micro batching so                                 if you have a use case that requires low                                 latency Kafka stream student should be a                                 good fit and talking about processing                                 the guarantees that we give for                                 processing in Kafka streams at the                                 moment is at least once because again                                 we're building on the shoulders of Kafka                                 and Kafka das at least once messaging                                 that said the work on adding exactly one                                 support is already in the works that                                 work has already started and actually                                 one of the engineers who is responsible                                 as it is in this room and I'm not                                 pointing out now so wrapping up so we                                 did talk briefly about what is Kafka was                                 Kafka streams when would you use it and                                 why we think it hopefully advances the                                 study of the art a little bit so that                                 you can build cooler applications with                                 less sweat and tears now assume that I                                 raced your interest a little bit you can                                 get it right now it was released a few                                 days ago alongside Kafka Oh point                                        it's also available as part of the                                 confluent platformer II as I said build                                 more than just the core of Apache Kafka                                 in in and one big platform we also                                 provide a lot of demos and examples to                                 get you started                                 like showcasing how to do the canonical                                 word count how to do joins how to read                                 invite every data and so on and if this                                 talk I mean I couldn't really cover                                 a lot of that in detail but we have                                 extensive documentation that I think is                                 too concise so we have quick starts deep                                 dive on concept architecture developer                                 guide and so on if you're interested in                                 that and lastly we also do bi-weekly ask                                 me anything sessions if you're                                 interested in joining these just drop me                                 in out some of the things to come so                                 what will happen next I already                                 mentioned we are working on adding                                 exactly one semantics of Kafka and two                                 Kafka streams the other thing that we                                 are working on is Kerrville State so if                                 you think of you know these tables that                                 are being consciously updated and kept                                 up-to-date oftentimes this is already                                 everything that you need for whatever                                 you're doing like your espresso so we                                 want to allow you to directly tap into                                 that state and expose it to you that                                 will open a whole lot of different use                                 cases that would be great fit for Kafka                                 stream so they were making your life                                 easier the other thing that we're                                 working on is a sequel interface so that                                 you would not need a developer API to                                 express how you want to compute your                                 data and lastly but it's not least it's                                 the first version that we just released                                 in Kafka o                                                               you to tell us what you like and what                                 you don't like so can we make it better                                 so if you have any feedback I encourage                                 you to share it by letting us know in                                 the car can use a manual list so in                                 regards to things to come tomorrow                                 morning we have a keynote by Neon Ikeda                                 she is our CTO at confluent and one of                                 our co-founders and she'll be talking                                 about the bigger picture of stream                                 processing so if this talk was a                                 technique of you focusing on kafka                                 streams now I will talk tomorrow about                                 the bigger picture the strategic view of                                 what stream processing is where it fits                                 in and so on so I think we still have                                 time for some questions now but if                                 you're not feeling confident in front of                                 everyone you can just find                                 for the conference thank you so I have a                                 question about the state store and I                                 think it's pretty clever pretty smart to                                 push to push the data back into Kafka                                 but wouldn't that mean like if one nodes                                 goes down that it can take quite a while                                 to recreate the state in another node so                                 shall I repeat the question for the                                 recording or is it fine it's fine                                 okay so one thing that we're exploiting                                 here is a feature called lock compaction                                 in Apache Kafka and I hope I find it a                                 very quick way to summarize that if                                 you're having a stream of data let's say                                 with a lot of updates for Ellis as in                                 one of the examples that we showed what                                 lock compassionately do is if we say I                                 have                                                      but since state's doors are interpreted                                 as a change lock stream I can throw away                                 all the updates but the very latest one                                 for Ellis so if you have                                           updates for Ellis what will end up in                                 Kafka after log compaction is just a                                 single entry so that is why it will be                                 very efficient this doesn't answer your                                 question ok ok                                 what if after compaction I still have a                                 million entries they would have to be                                 recreated on another note right yeah it                                 will just take time so even though it                                 might be like normally like millisecond                                 latency there might be one minute pass                                 or something in processing right there                                 will not be a pause but it will take a                                 while until the data is being a restored                                 to exactly state when the failure                                 happened yeah so we pay Suncoast in the                                 presence of failures there's quite a few                                 things that I think originally might                                 have been in Sansa or Sam's it was kind                                 of on the way to getting towards doing                                 is there any reason that is Kafka                                 streams like a new rewrite or does it                                 include Sam's or his sons are gonna sort                                 of fade away now I'm not really sure                                 what do you think so I I will not be                                 able to speak for the santur project but                                 of course there are some strong                                 similarities between Samsa and Kafka                                 streams to the point that some people                                 not us are saying like it's like Samsa                                 plus plus or so I think that that is not                                 a fair comparison but some of the people                                 that work on Samsa now in our team so                                 what we actually did with Kafka streams                                 was we looked at you know all the prior                                 art in that space                                 academia open source tools like Samsa                                 and we said okay what are the things                                 that injera peak here and what are the                                 things that we consciously decided not                                 to do to keep Kafka stream simple which                                 is like a pretty abstract answer to your                                 question but I think that's the                                 higher-level answer a concrete example                                 would be that in Samsa there are two                                 things one is it typically wants you to                                 run in in yarn and brew felt like oh                                 that is a pretty huge investment I mean                                 if you have ever worked in larger                                 enterprise telling the governance board                                 your manager whatever just because you                                 have a stream processing problem to                                 install Yaron that will not fly so we                                 didn't want to make the same mistake or                                 basically we didn't want to make it you                                 run into the same problems the second                                 part is the second example is that in                                 Samsa                                 is used as the default messaging layer                                 but it is pluggable what that means is                                 your is like like this the smallest                                 common denominator for messaging and in                                 Kafka stream you said no we're not going                                 to do that no one ever swapped out Kafka                                 for Samsa so we're actually embracing                                 Kafka and by doing so yes we're telling                                 you that normally you will always read                                 from Kafka and right back to Kafka but                                 we can punt a lot of hard problems to                                 Kafka and by really embracing it so if                                 you ever are used and hope there's no                                 long line in answer to your question if                                 you ever use one of the other stream                                 processing tools they often implement                                 their own messaging layer so what does                                 that mean so let let's think about it in                                 terms of flag economies of scale what                                 you want to do is you want to build a                                 stream processing tool that is very good                                 at stream processing and then you feel                                 like oh wait a minute I also need to                                 pass messages around so we can do the                                 processing in a distributed system                                 let's also solve that problem but                                 actually you know we learned in Kafka                                 serving the messaging problem it's                                 really tough all by itself so why would                                 you want to re-implement this as like a                                 second or third or fourth priority if                                 what we want to do is stream processing                                 so and with Kafka streams what we can do                                 is whenever there is improvement to                                 Kafka hey we added security guess what                                 you get it for free in Kafka streams you                                 have better performance in Kafka you get                                 it for free in Kafka streams so for us                                 it's also economies of scale when you're                                 maintaining the project by making it                                 simpler and benefiting from all these                                 other projects when they're doing                                 innovative stuff and I think a sample                                 that I was alluding to his deployment                                 frameworks resource managers whatever                                 you're doing in your stream processing                                 project he will never be as good as                                 resource managers like me sauce and why                                 is that the case because their purpose                                 is to solve this exact problem he will                                 never get better with like some that you                                 do at this on the side so I think some                                 of the lessons learned and the                                 similarities with between Samsa and                                 Kafka streams is that we looked at what                                 worked and what didn't and try to                                 improve upon that but in terms of like                                 actual taking over code that didn't                                 happen very good thank you                                 anyone                                 I have a hard time seeing as most of you                                 it's thanks a lot okay                                 thank you very much
YouTube URL: https://www.youtube.com/watch?v=o7zSLNiTZbA


