Title: Berlin Buzzwords 2016: Fabian Hueske & Till Rohrmann - Streaming analytics and CEP ... #bbuzz
Publication date: 2016-06-11
Playlist: Berlin Buzzwords 2016 #bbuzz
Description: 
	Complex event processing (CEP) and stream analytics are commonly treated as distinct classes of stream processing applications. While CEP workloads identify patterns from event streams in near real-time, stream analytics queries ingest and aggregate high-volume streams. Both types of use cases have very different requirements which resulted in diverging system designs. CEP systems excel at low-latency processing whereas engines for stream analytics achieve high throughput usually due to distributed scale-out architectures.

Recent advances in open source stream processing yielded systems that can process several millions of events per second at sub-second latency. Systems like Apache Flink enable applications that include typical CEP features as well as heavy aggregations. An example of these use cases is an application that ingests network monitoring events, identifies access patterns such as intrusion attempts using CEP technology, and analyzes and aggregates identified access patterns.

In this talk we will show how Apache Flink unifies CEP and stream analytics workloads. Guided by examples, we introduce Flinkâ€™s CEP-enriched StreamSQL interface and discuss how queries are compiled, optimized, and executed on Flink.

Read more:
https://2016.berlinbuzzwords.de/session/computing-recommendations-extreme-scale-apache-flink

About Fabian Hueske:
https://2016.berlinbuzzwords.de/users/fabian-hueske

About Till Rohrmann:
https://2016.berlinbuzzwords.de/users/till-rohrmann

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              yeah welcome everybody um in this talk                               till and I are ya going to talk about                               two different kinds of Street processing                               applications streaming analytics and                               complex event processing yeah till I                               myself were both committees at Apache                               fling and currently working at data                               artisans so you might have noticed the                               stream person is really picking up it's                                one of the dominating topics here this                                conference and also at previous                                conferences and this is because most                                data is actually produced s stream so                                there's usually not one big block of                                data that you try to analyze but data is                                constantly produced so maybe coming from                                from server server logs or application                                locks click locks mobile devices                                Internet of Things is another big topic                                here so data is constantly produced and                                arrives in data centers and so no people                                are thinking about okay how can I really                                leverage this continuously produced data                                what can I do with it and how can I                                analyze this data as it arrives and                                there are different kinds of                                applications and use cases for this so                                one of these use cases is complex event                                processing so the goal of CP is to                                usually detect patterns in an event                                stream and then whenever you detect a                                certain pattern you want to yeah                                assemble or or merge or aggregate and a                                new kind of event that aggregates the                                the events that basically trigger this                                pattern to yeah or trigger this pattern                                and there are a number of applications                                for this kinds of workloads for instance                                there is like process monitoring so                                imagine your warehouse have detect all                                your items with RFID tags then you can                                basically automatically or get automatic                                whenever something enters your warehouse                                whenever something is moved within the                                warehouse or whenever something leaves                                the warehouse there is intrusion                                detection for networks so whenever a                                certain suspicious pattern occurs that                                some some pot ranges are are tested so                                these are kinds of patterns that you can                                try to detect from server logs and or                                for financial trading so whenever you                                see that certain buy or sell patterns                                occur you can you can you can trigger                                certain certain actions either to buy                                our stocks or to sell stocks and these                                kinds of workloads are very very                                demanding on the on if you want to                                execute that on a stream processor it's                                very very demanding on the stream                                process because you need very low                                latency to actually do that so you don't                                want to react like one minute after a                                certain pattern pattern occurred you                                usually want to want to detect the                                pattern instantly and also instantly a                                trigger an action so imagine a certain                                pattern occurs in a in a stock I you                                wanted instantly Iraq you don't want to                                wait for one minute and everybody else                                has already yeah leveraged there's                                certain the certain event and you're                                basically the last another thing that's                                very important is you that yeah you need                                to be able to take the time when the                                event was created into account so it's                                not not enough to win whenever you                                detect patterns it's not enough to                                detect the patterns on basically looking                                looking at the time when the event                                arrives at the operator or at the                                processing at the data processor you                                want to want to take the time when the                                event was created into account so this                                is what what we call events on and you                                also don't want to lose certain patterns                                right when something when when you when                                you lose an event or an event occurs                                twice this might met mess up your                                pattern you might not be able to detect                                that another yeah so another application                                for for stream processing is analytics                                so this is like the traditional approach                                to                                energox like the batch approach so you                                have yeah clients updating a                                transactional database systems and                                periodically you load the data from the                                transactional database into a data                                 warehouse or HDFS and then periodically                                 you analyze the data in this in this bed                                 store using a patch engine and the                                 problem with this is there is a delay                                 right so whenever you do this periodic                                 et al there are some some some some                                 delay if you do that once a day your                                 data might be one day old when you when                                 you analyze it so stream processing or                                 stream analytics it's an application                                 where you basically try to analyze the                                 data as it arrives so you get continuous                                 results and the data is basically and                                 lies as it arrives you can also think of                                 this this stream analytics as a superset                                 of the previous batch analytics ride so                                 if you consider like a like a batch                                 being a finite stream so whenever you                                 read something from a from a from a file                                 system this is basically a stream so                                 even like the programming languages                                 consider this as a stream streaming                                 abstraction so this is just a finite                                 stream so bad energy axis it's kinda                                 like a yeah it's upset from from stream                                 analytics and the requirements or the                                 stream process so if you want to do                                 something such a workload is yeah I need                                 to be able to cope with very high                                 throughput so there are use cases where                                 millions of events arrive per second so                                 your stream process I needs to be able                                 to to handle that so massive massive                                 parallelism is required here you also                                 want to do that of of course exactly                                 once you don't want to count some things                                 twice or or lose your date loose data                                 and again event time is important and                                 also like advanced windowing                                 capabilities what I'm going to talk                                 about this briefly uh later um yeah so                                 as I said before Dylan I we're both                                 committees at Apogee fling and this talk                                 will also be                                 about how flink handed see these kinds                                 of different applications and yeah fling                                 is a scalable data processor that for                                 for for streaming data and it actually                                 meets the requirements of both of CP and                                 streaming analytics so it's provides low                                 latency which is required by our complex                                 event processing and also high                                 throughput so you can scale out fling                                 and achieve very high throughput with a                                 system flick also supports arm event                                 time has very good support for window                                 and features exactly one semantics and                                 so the core programming abstraction or                                 API for flink is the support data stream                                 API which is available in Scala and java                                 and yeah this is I wouldn't say it's not                                 really a low-level API but it's if you                                 want to want to implement like something                                 like a certain detecting patterns or                                 certain kinds of stream analytics there                                 is some some ovett involved if you want                                 to implement that so this talk will be                                 about yeah introducing or showing some                                 new api's that that that fling has                                 recently added or will add with the next                                 version and this API so for I am complex                                 event processing so with version                                       which was released I don't know maybe                                 two month ago we added this cep library                                 which is basically library to define                                 define patterns and actions so what you                                 do when a certain pattern occurred and                                 the community is currently working on on                                 stream sequel so basically a way to ease                                 the definition of stream analytics                                 queries we also a brief you talk about                                 how these two different kinds of                                 applications can actually arm can I can                                 actually be merged or show you some some                                 applications where it makes sense to                                 combine CP and stream analytics in the                                 same in the same yeah epic                                 occasion and why it might be why it's a                                 good idea to use a stream processor                                 that's actually capable of doing both                                 like providing low latency for a strip                                 for for complex event processing and                                 high throughput for the analytical part                                 all of this is should say is still quite                                 early work as I said so CP has been                                 added with a light yeah latest version                                 of fling stream sequel will be released                                 for the next version and so this is all                                 basically very very early so no I'm                                 going to talk a little bit about a use                                 case that we use like this the driving                                 example in this talk and this is about                                 yeah tracking tracking events in an                                 order process so this is a very very                                 simple example here we have on the left                                 hand side we have the customer the                                 customer places an order at a warehouse                                 so firstly the end the warehouse                                 generates and order received event                                 whenever it receives such an order and                                 then somebody in the warehouse will go                                 assemble all the parts that the order                                 requests it in a package whenever the                                 package is ready the package is given to                                 a delivery service so whenever this                                 happens there is a order shipped event                                 triggered and sent into the data                                 infrastructure of the of the company and                                 whenever then this order is actually                                 delivered to the customer then the                                 delivery service will again send an                                 event and say okay this order was                                 delivered so it's a pretty simple thing                                 so we have the order received at the                                 warehouse the order is shipped basically                                 leaves the warehouse and then the order                                 arrives at the customer the RS delivered                                 so these kinds of events are in our use                                 case the order received we mocked up                                 with this smaller envelope icon the                                 authorship is the package and then the                                 t-shirt is when the package arrives at                                 the customer pick he yeah                                 pex the package end yeah receives it is                                 a nice t-shirt yeah we're gonna model                                 that as a stream of events so the events                                 are very simple just                                                   first one is an order ID so we want to                                 be able to track the different orders                                 are there's a time stamp whenever this                                 event occurred and the last one is a                                 status which is received shipped or                                 delivered depending on what kind of                                 event happened yeah so this is the use                                 case that we're going to talk about in                                 this talk no I'm yeah talking about the                                 stream analytics side of this talk                                 basically yeah aggregating this this                                 data so I talked before a little bit                                 about the traditional approach where the                                 data was loaded from the transactional                                 transactional data bases into an data                                 warehouse or HDFS or whatever whatever                                 other data storm and usually what you do                                 when you have such an infrastructure                                 usually if you repeat the queries over                                 changing data sets right you say okay                                 give me a report for the last week give                                 me a report for the last week and so on                                 so it's a usually the same query with                                 different parameters but all over again                                 so you repeat the query and these clear                                 is tend to join and aggregate large                                 tables when now so this is the                                 traditional batch approach when you know                                 look at how stream processors do that so                                 stream processors if you do that with a                                 stream processor um you don't issued                                 multiple queries you just start one                                 query and this is like a standing theory                                 so the query does never terminate it                                 consumes the stream of events and it                                 produces a stream of results so there's                                 one query which is active the whole time                                 so the data is inject ingested into this                                 stream processor and the processor does                                 some computation aggregation on the data                                 and produces the results                                 and I said before so usually on there                                 there are lots of use cases where this                                 is actually done a very high volume                                 stream so where millions of events enter                                 the system for a second but now one                                 question arises so how do how can you                                 can you actually do aggregations on                                 stream so this stream is infinite so it                                 does never end whenever when when can                                 you close your you're a contract account                                 aggregation for instance yeah so this                                 problem is solved with the concept of                                 windows so windows split an infinite                                 stream into finite batches so one of                                 these batches is called a window and in                                 this window you can then it's it's a                                 finite thing you can compute an                                 aggregate so it's a very simple and the                                 different kinds of windows so the most                                 common ones are like a tumbling window                                 which has a fixed size and tumbling                                 winners do not overlap so you say for                                 instance I want to count how many orders                                 arrived in each hour so then you start                                 at eleven o'clock the window is then                                 collect the data from eleven o'clock to                                 twelve o'clock or two eleven                                          then once you're at eleven of                                        then say okay this window is closed I                                 can do my aggregation and emit the                                 result so this is the tumbling window                                 case they are two sliding windows which                                 have a fixed size but you also say I                                 want to count let's say every five                                 minutes data worth of ten minutes then                                 you basically start a window every five                                 minutes and every window has the size of                                                                                                         obtain consist of the meaningful results                                 out of this he actually need this                                 support for event time because if you if                                 you have an operator which says okay                                 collect my data worth of one hour and                                 this one hour is basically determined by                                 the by the operator by the machine                                 looking at the wall clock time then this                                 basically then the result determines or                                 the the processing speed of this machine                                 determines the                                 result right so if this machine is very                                 slow and can only consume very very                                 little data in one hour the result will                                 be different from a machine which is                                 very fast they can consume a lot of data                                 right so you want to have the result                                 being independent of the processing                                 speed of the system of the machine and                                 so on and this is why this support for                                 event time is very crucial when you use                                 event time you look at the data in the                                 records in the events and this one                                 basically determines whenever you can                                 what goes into which window and when it                                 window can be closed so here's exactly                                 this example here counting orders by by                                 our so we have this event stream here                                 and the events arriving at the system we                                 have a two o'clock from two o'clock to                                 three o'clock that's just one order                                 arriving so here the purple envelope so                                 when the logical time of the FD of the                                 stream processor arrives at three                                 o'clock it says okay let me count what I                                 got in this last hour and then it says                                 okay I just got                                                        is                                                                    that be expressed in string sequel so                                 basically the work that we're currently                                 doing the fling community so here's a                                 simple simple sequel statement that                                 exactly performs this computation so we                                 have a select stream which is a little                                 bit different from the from regular                                 secret so we have this stream key words                                 here which basically says okay and the                                 result is going to be a stream and the                                 and the table from which I read is also                                 stream it says from events events is the                                 stream the input stream like a in                                 regular secure this would be the like                                 the table then we have the way across                                 where you put a put a predicate and                                 local predicate we say okay we only                                 interested in the events with the status                                 received so we only want to count how                                 many orders arrived at the warehouse and                                 then in the group by clause there is                                 something special right so there's this                                 tumbler function which basically                                 computes window ID a we know d so the ID                                 in which window a certain record where                                 will be aggregated and hear you say                                 tumble times Tim so we're looking at the                                 timestamp column of the events so we                                 want to make this I want one to use the                                 time of time of the event to impugn the                                 aggregates and then we say interval one                                 hour which basically means okay so every                                 hour so I basically round down the the                                 timestamp for each hour and with each                                 for for every record that arrives with a                                 certain times am I can put this record                                 in exactly                                                               can aggregate and the select class you                                 if you see the current star as cancer                                 this is like a regular aggregation                                 function which is evaluated on the                                 window and then it says also a tumble                                 start which is just a function to                                 determine the beginning timestamp of the                                 window yeah this slide is a little bit                                 about the implementation details behind                                 how flink is going to going to implement                                 this stream sequel feature so we are                                 heavily leveraging a patrick outside                                 care that is a is a sequel parza and                                 planar framework so it comes with an as                                 its own sequel optimizer and it's very                                 very extendable so we are using that so                                 it also has its own catalog so we you                                 can register data streams or data set so                                 the same feature can also be used for                                 for sequel of a static data sets and                                 when a secret curie comes we are the                                 cats at pazzo takes the query so the                                 syndics are shot before is exactly                                 basically taken from from the cat side                                 project it passes the secret theory it                                 looks up the tables in the cat's a                                 catalog it then generates a logical plan                                 so whoever is familiar with curie up                                 Tomas a shin this is basically like a                                 blueprint for a query optimizer we then                                 pass the logical plan to the cats at                                 optimizer and pain                                 on whether the query will be executed on                                 a stream or on a data set so whether                                 it's going to be analyzed aesthetic data                                 or streaming data we use different                                 optimizations for the optimization and                                 then generate either regular data set                                 API plan weather data stream API plan so                                 all of this is basically compiled down                                 to the regular fling ApS they did a set                                 at API for batch processing a data                                 stream it we have for stream processing                                 all right so with this I'm handing over                                 to term he's going to talk about the CP                                 side and how CP and stream seeker can be                                 integrated yeah thnkx oven for the great                                 introduction to stream sequel I'm till                                 and now I'm going to talk a little bit                                 about Madame pattern detection on event                                 streams using complex event processing                                 so consider again the use case fargin                                 just presented to you the ala                                 fulfillment but this times we not only                                 want to count the number of orders we've                                 received within the last hour but we                                 want to monitor all the processes a                                 little bit more so for example we would                                 be interested in detecting when a                                 delivery is delayed or even lost so that                                 we can inform the customer or center the                                 customer notification to improve his                                 user experience or other interesting                                 information would be how long does it                                 actually take from receiving an order to                                 preparing the item to giving it to the                                 delivery service and that's that can be                                 done using complex event processing but                                 first we have to define for the                                 different processes on the service level                                 agreements to distinguish when process                                 is delayed or when it is successfully                                 completed furthermore we introduce for                                 new events which represents the assaults                                 of the DA the CP computation so we have                                 a person success event which tells that                                 the the receiving an order and the                                 preparation of the item giving to the                                 delivery sir                                 us was successfully completed in time                                 and then we have the process warning                                 event which does that it has not been                                 completed in time and the same holds                                 true for all the same things for the                                 delivery process so when you see for                                 example the delivery warning event you                                 know that you can tell the customer that                                 is shipment might not be or delivered in                                 time since all theory is great let's                                 take a look at a concrete example so we                                 have again our stream of input events                                 where the letters are the orders the                                 parcels are the the shipment events and                                 the t-shirts denote the successfully                                 delivered items events so what we are                                 now looking for is events which don't                                 have a corresponding successor so for                                 example if you take a look at the blue                                 parcel it is not succeeded by a blue                                 t-shirt which would denote that this                                 parcel has been given to the customer so                                 if we don't see such an event within a                                 given time interval so for our use case                                 I think I've we will see later on this                                 light that's one day we want to generate                                 this delivery warning and the same                                 applies to the to the all events if we                                 don't process the orders in time then                                 there are some problems in our warehouse                                 which we have to fix so these these                                 warning events can trigger some kind of                                 countermeasures to solve these problems                                 but we are also interested in a                                 successfully completed event so if you                                 take a look at the green letter we see                                 that it is succeeded by a green passer                                 which denotes that the internal                                 processing has been completed now we                                 interested in the time it took and the                                 same applies to the green passer which                                 is succeeded by the green t-shirt okay                                 how would we model that with flink so                                 first of all we have to define what                                 successful                                 processing actions mean posting action                                 means that is if we see if we receive an                                 audit event we also want to we see or we                                 want to see here a blue passer which is                                 a shipment event within one hour for                                 example if you see that we are safe and                                 we can generate our processing success                                 event if we see the auto event and it is                                 not followed by this shipment event we                                 want to generate the process warning so                                 in order to define the the success at                                 pattern we have to we have to do the                                 following with link so first of all we                                 define a pattern consisting of a                                 starting event which is identified by                                 the name received and additionally has a                                 subtype constraint so event is the super                                 type of all events so the order shipment                                 a delivery event and so without the                                 subtype event every event would match to                                 the first received event but since we                                 are interested in all events we tell the                                 system that the event has to be a                                 subtype order next we say that we the                                 audit event has to be followed by an                                 event called shipped here this this                                 event must be a shipment event otherwise                                 it wouldn't be a valid pattern so we                                 could have specified again a subtype                                 constraint but to show you that you can                                 also specify more complex conditions I                                 used the where clause where I checked                                 the event or the status of the event                                 that the Stella slee equals shipped                                 which is the case for all shipment                                 events and additionally we define a time                                 interval for a valid pattern so we say                                 that the pattern must occur within one                                 hour otherwise it times out                                 okay after we've defined this pattern we                                 apply it on our input stream of events                                 which is keyed by the odd ID because we                                 want to do I want to do this this                                 pattern matching for every order                                 individually and the result is this                                 processing pattern stream on which we                                 can apply a Select clause which                                 basically generates the result of the                                 these complex event processing so here                                 as user one defines two functions the                                 first function is the timeout handler                                 which is called for every partial match                                 which has not been completed and the                                 given time interval of one hour in this                                 case and since we wanted to generate the                                 process warning in this case we simply                                 return a process one event the side of                                 this time attempt on the second function                                 till this select call is the user select                                 function which is called for every                                 successfully match pattern sequence and                                 here we simply return the poster success                                 event with the all ID the timestamp of                                 the shipment event and the duration it                                 took form from like receiving the order                                 to sending the item to the delivery                                 serves are giving the item to the                                 delivery service and we basically do the                                 same for for the delivery success and                                 warning events as well just with a                                 different like time devil okay so far so                                 good we've seen how we can analyze event                                 streams and generate we searched from it                                 using a complex event processing but                                 what if we are also interested in like                                 calculating aggregations on these newly                                 generated events that could be                                 interesting for example to to calculate                                 the number of of delayed shipments                                 day so that you can assess the                                 reliability of your delivery service for                                 example here in our example we see that                                 that we have in the first day we've to                                 deliver your warnings which means that                                 two deliveries we're delayed and in the                                 on this second day we see there were                                 three so it's basically counting these                                 the results of the CP the delivery                                 warnings another interesting metric for                                 the business could be the average and                                 processing time so how long does it take                                 um from while receiving the item to                                 processing it and giving it to the                                 delivery service on average mmm so it's                                 this kind of aggregation is a natural                                 fit for for for stream secret so what we                                 are doing now is combining both                                 approaches we first have as a result a                                 data stream of either delivery warnings                                 or delivery successes as we side of our                                 previous CP processing and we know mmm                                 sit out sorry the delivery success                                 events because we only want to count the                                 delivery warnings next we have to                                 register this data stream giving it a                                 unique name to to a reference it from a                                 stream sequel statement and last but not                                 least we can apply our stream sequel or                                 swim secure query the one you've already                                 seen for counting like orders per hour                                 just this time delayed shipments per day                                 I think yes on our data stream of                                 delivery warnings that's it basically                                 that's quite nice hmm but it still feels                                 a bit clumsy right it's I mean on the                                 one hand you have this the stream seeker                                 which is really a declarative and easy                                 to use and on the other hand you still                                 have to program java so it would be                                 better to                                 have like all the clarity of language                                 for both things so what we are currently                                 working on and what's our vision is to                                 unify the CP and swim sequence so that                                 we have a CP and which stream sequel                                 language and with this language the                                 second use case like calculating the                                 average processing time I could be                                 realized well and the way you see on the                                 slide it's not carved in stones of my                                 change but here we see that and we have                                 a have nested query the inner create                                 calculates the the pattern there we see                                 that we want to detect one and event a                                 which is followed by B and partitioned                                 by order ID so that we do it per order                                 and order by the time so that we have                                 event I'm guarantees we say that these                                 two events have to appear within one                                 hour and that a the status I should be                                 received and B should be shipped which                                 is equivalent to a being an all day                                 event and B being a shipment event and                                 for these two events we calculate the                                 duration which is then the input for the                                 outer our query which averages them and                                 per day that it basically that's it                                 basically I think that's much nicer and                                 then like mixing like Java programming                                 with sequel okay that brings me to my                                 confusion or to our conclusion hmm so                                 what have you seen today you've seen                                 that Apache fling is well suited for                                 analytics as well as CP work lots it                                 offers like intuitive easy-to-use api's                                 for force secret as well as CP arm which                                 gets you easily started and helps to get                                 a yo-yo a problem soft on a high level                                 and the combination of both all things                                 allows you to two opens up                                 the new field of applications where not                                 only can do the usual sequel things but                                 also can easily detect and extract                                 temporal patterns in your event streams                                 which well allows you to extract more                                 information out of the data and that                                 brings me only to two things which I                                 left one thing is to draw attention to                                 the upcoming think forward conference                                 which is taking place at the same venue                                 from the                                                         September so if you have experience with                                 flink and want to talk about it then                                 well please submit a talk and talk about                                 it if you're not that fired but still                                 want to hear more and learn more about                                 fling then get a ticket and well listen                                 to others talking about the experience                                 with think it's a cool cool conference I                                 can tell you and if you can't wait that                                 long and once you get started and                                 working on fling and even get paid ferd                                 then check out the curious webpage on of                                 data absence because we are currently                                 looking for new hires for our team yeah                                 and thank you very much for your                                 attention and I hope you have enjoyed                                 the talk                                 thank you till thank you for being for                                 this talk and we have some time for                                 questions so if you do window operations                                 where do you keep state and what do you                                 do if the state becomes excessively                                 large so the state packet and fling is                                 actually pluggable so there are                                 different kinds of ways to store the                                 state so right now we support or fling                                 comes with two different state backends                                 one is basically the JVM heap which is                                 very fast and of course yeah you can run                                 out of memory there so it doesn't grow                                 infinite and another state back end is                                 it's using rocks to be so which is                                 persisting to disk so and the state is                                 regularly back up to a stable storage                                 like HDFS or and then when a failure                                 occurs it's basically yeah restart from                                 there more questions all right what's                                 the roadmap for a stream sequel and when                                 can we expect aggregates yeah the rock                                 tricky question so actually what I what                                 I've done here is not working yet so                                 this is more like I look into the future                                 so with the next version so fling                                    we're gonna we will have a support for a                                 very basic stream sequel support which                                 includes filters and projections and                                 unions so this is rather made for four                                 transformations of transforming data                                 stream and then we are basically address                                 the the next features windows aggregates                                 and possibly even join us for the next                                 version for flink                                                       mean in the end it's a community                                 decision but i would say six weeks are                                 realistic for for pushing out of                                    release thanks                                 I think we have time for one more                                 question hi in the examples you're using                                 the timestamp of the event so is there                                 an easy way to deal with the delayed                                 messages the messages that derive after                                 the window yeah so um that's a very good                                 point so i was a bit simplifying here so                                 when you look at the event at the time                                 of the event you cannot really guarantee                                 that the events arrive in order right so                                 the order the events might arrive out of                                 order flick is using the concept of                                 water marks which is basic like an upper                                 bound for the for the current time so                                 whenever when the operator receives a                                 watermark or which says hey it's three                                 o'clock no more events should appear                                 that are before three o'clock then we                                 close the window of course it might                                 happen that there's still one more event                                 or more events before that time but                                 flick is also able to to handle these                                 kinds of events are you you have to                                 write custom code for it but in                                 principle as possible thanks again and                                 if you want to learn more about fling                                 then just come back to this room after                                 lunch where Dylan Fabien will have a                                 workshop so perhaps we should rename                                 this room from instead of saying Frank's                                 along this will be Flinx along see you                                 after lunch
YouTube URL: https://www.youtube.com/watch?v=gvzOXyluqbo


