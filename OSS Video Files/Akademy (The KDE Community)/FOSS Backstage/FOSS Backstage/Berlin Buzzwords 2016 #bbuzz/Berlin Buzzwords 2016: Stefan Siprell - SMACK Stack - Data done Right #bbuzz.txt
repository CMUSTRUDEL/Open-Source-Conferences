Title: Berlin Buzzwords 2016: Stefan Siprell - SMACK Stack - Data done Right #bbuzz
Publication date: 2016-06-12
Playlist: Berlin Buzzwords 2016 #bbuzz
Description: 
	A talk covering the best-of-breed platform consisting of Spark, Mesos, Akka, Cassandra and Kafka. SMACK is more of a toolbox of technologies to allow the building of resilient ingestion pipelines, offering a high degree of freedom in the selection of analysis and query possibilities and baked-in support for flow-control. More and more customers are using this stack, which is rapidly becoming the new industry standard for Big Data solutions. 

Read more:
https://2016.berlinbuzzwords.de/session/smack-stack-data-done-right

About Stefan Siprell:
https://2016.berlinbuzzwords.de/users/stefan-siprell

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              okay guys just prepared will be skipping                               some slides probably this is comfortably                               fills an hour you get                                              we'll manage somehow I won't reduce                               myself too much my name is Stefan Stefan                               super al you'll find the slides and                               everything later a record code centric                               and if the doors are sealed on the sales                               guy but don't worry this is going to be                                a pure technical talk but this is what                                we do everyday never pre-sales I've                                Thunder starius technical conferences                                and they've never been thrown off stage                                so please endure who sort of smack ok                                I'm surprised as our phones is the                                buzzword do you know fast data as                                opposed to big data ok fast data to talk                                it's a quarter it's been coined by by                                typesafe the company or not types of                                light Bend and this basically it's a                                stack consisting of five different                                technology spark mezzos acha Cassandra                                and Kafka so it's basically kind of like                                lamp you know yet linux apache mysql PHP                                or Python this goes to same direction                                it's so sits there there six different                                tools they're not too powerful alone but                                you can do pretty cool stuff of them                                together so let's look at the tool's                                first soo no spark ok good actually it's                                not a Swiss Army knife for data spark is                                a distributed computing platform but the                                ecosystem of spark is pretty rich and                                its most of the ecosystem resides and                                data so if you're doing etl jobs extract                                transform load you can do this very                                nicely in spark with the batch                                topologies you have the spark cluster                                you can maintain it                                                     your jobs on it and you have a real                                enterprise level etl sweet you want to                                do micro batching on streams no problem                                I mean we've heard a lot about Flint the                                last couple days                                about the big fight about latency and                                throughput if through producer game if                                you can wait seconds for results spark                                can make you happy running SQL and join                                so non-relational database systems no                                problem so if you have a bi department                                who's used working SQL is using                                something like tableau or whatever you                                can you can talk to elasticsearch                                Cassandra whatever you want to using the                                SQL dialect running graph operations on                                line graph databases in the problem                                either last but not least MapReduce and                                super fast thank you but I'll skip the                                spark stuff as your people involved that                                measures or measles this is always spark                                everybody knows measles is not as known                                even I think it's a pity because it's                                really cool technology if you look at                                the operating system with what the                                operating system really really does it                                manage resources so you have                                        processes running and the processes are                                sharing the same CPU to say memory the                                same network hard to say mass data and                                the data and the operating system                                schedules it makes sure that each                                process think he's running alone on the                                system and he gets the resources he                                needs this is what one operating system                                what Colonel does mrs. does this for                                data center so every resource you have                                if it's if it's a hard disk solid-state                                disk network cards memory CPUs is                                managed by measles and it makes sure                                that your process is running in the                                cluster in your data center gets the                                resources it needs so the nice thing is                                if you have a measles deployment what                                you do you install the measles lays on                                all of your machines it's one image to                                deploy all of your machines and during                                the workload measles will decide how                                many instances of rich process to run on                                which machines so if you're thinking                                about they                                data auto scaling a resilience of failed                                nodes missus is a friend he can he can                                make sure that the right process of                                being are gonna be run as opposed to to                                VN layer solutions or two darker                                solutions you're very open here you can                                use whatever you want to Hadoop Kafka                                spark and akka they run as native Java                                jobs on the under guest on the host                                 operating system there's no                                 virtualization no isolation no                                 containerization is just starting up the                                 jobs so you have very efficient                                 processing and again cooper nate is i                                 think they just released the newest                                 version where they say you can have up                                 to thousands of parts you never had this                                 limitation misses you could always run                                 data centers acha okay this is my this                                 is my weak spot the beauty if acha is                                 you can freight these tally performing                                 reactive applications so you just think                                 of your application as there's the big                                 tree of actors working together in the                                 assam chris fashion it's highly                                 performant because any machine can take                                 over any requests concurrency is built                                 in you always work with messages in your                                 mailboxes it's elastic no single point                                 of failure yada yada yada anybody looked                                 at legume yet the new the follow-up of                                 acha okay nor if I then I have nobody to                                 talk to you later and we're talking                                 about Big Data let's talk about big                                 performance the beauty of it is you                                 working a circle issue and actors                                 communicating to its other actor verse                                 Maya asynchronous messages if you do                                 this the typical JMS environment you                                 would you would limit the number of                                 messages you're putting over the bus                                 because the message is a very expensive                                 cause the transactional behavior in etc                                 you don't have to worry about this if                                 you work in soccer because you can                                 excellent actually send out                                            requests per machine per second                                 so this whole delay send a message never                                 hurts in the aqha environment so we                                 covered the STM in the a let's go to                                 Cassandra known ok cassandra is the                                 performance always up no SQL database                                 it's it throws out love garbage from the                                 rtms world so it doesn't try to do                                 transactions it doesn't try to do                                 foreign key constraints that doesn't do                                 a lot of kind of stuff you use some                                 databases and therefore what's left of                                 cassandra is super fast so the rule of                                 thumb is                                                                 second this is insert or read up                                 operations who don't really matter                                 there's no downtime because Cassandra                                 doesn't have a single point of failure                                 it doesn't even have a single data                                 center a failure if you have multiple                                 availability zones in your cluster                                 Cassandra can spread across these and                                 and survive a Down of availability zone                                 it's a column index so you have                                 something like sequel which it smells                                 like SQL but you have to append only                                 performance so any operations are doing                                 is actually just append only mechanism                                 so that's why cassandra is such a nice                                 sweet spot mixing only read and write                                 load multiple data centers we have that                                 covered and anybody who worked with                                 Cassandra it's it's really mind-boggling                                 cause the first couple of weeks you                                 always want to normalize you always want                                 to do a joint statement you always want                                 to say but I already know the customer                                 I'll just join it from this table it's                                 wrong what you want to do in Cassandra                                 is you want to have denormalized modes                                 so every time you're thinking of new                                 Creary you probably end up writing a new                                 table in Cassandra this is where the                                 performance comes from because it's not                                 doing any of this of disjoining and                                 resolution of normalizing ok we'll have                                 to last part cover                                 the second Kafka can't do anything with                                 caffeine nowadays it's it's messaging                                 for big data I just mentioned this                                 before active and Qi work with this a                                 lot and in the past ipad apache activemq                                 the problem is it wants to have                                 guarantees once guarantee the client the                                 consumers message is being processed                                 exactly once this is expensive because                                 but what could but act activemq will do                                 in a cluster mode it's going to use the                                 database for lock handling okay so every                                 time you're pushing or reading something                                 from the data from the from the queue                                 you're going to have hits in the                                 database and this just does not scale                                 because you're trying to avoid                                 relational databases you're getting them                                 back in here so let's Kafka does it                                 Kafka can consume hundreds of megabytes                                 per second and then push them to next                                 clients and it also breaks down the data                                 to manageable volumes also since you can                                 append only mode you can safely buffer                                 terabytes of data without a performance                                 without a performance hit consuming the                                 data because it's just pushed behind the                                 old data and obviously is the superior                                 from the ground up so now we just saw                                 these loose technologies most of you                                 heard of them I think except mesosphere                                 there's nothing new we can give you the                                 talk up to now but the question is what                                 do we do with this and in the beginning                                 of big data from my point of view six                                 seven eight years ago everything was                                 Hadoop okay Hadoop was the first system                                 which actually allowed to do big data                                 big data means that the that the                                 instructions go to the data and get                                 processed locally as compared to the old                                 database model where you had to load the                                 data into memory first two up two                                 processes you're breaking this paradigm                                 you're turning it up and around and what                                 you done which is didn't beginning was                                 always MapReduce you were in there                                 purely big batch mode I think this is                                 the first examples of MapReduce were the                                 page rank calculations of Google this                                 was the perfect                                 sample that you MapReduce cruncher that                                 night business kind of learned okay                                 what's kind of neat I can classify my                                 customers i know that Stefan is probably                                 a a father because he's constantly blind                                 bluray videos of snow white but he's                                 also a nerd because he's still buying                                 his magic the gathering cards so Amazon                                 kind of learned new stuff about me but                                 the business always wants to find out                                 this stuff faster it wants to know                                 Stefan is still in the checkout queue                                 what can we show him we just noticed in                                 Stefan's bank around a bank account                                 statement that is moving money around                                 maybe you want to interrupt it and see                                 if he can take the money from him and                                 this is what we've seen so far as                                 consultants is that a latency between                                 sensor reading the inside has to become                                 faster and faster obviously the dupe                                 ecosystems reacted to it MapReduce is                                 not the only mechanism they have                                 nowadays storm sparkle all these                                 technologies also work on the HDFS but                                 we're looking at smack today on the                                 other hand so we had to slow overnight                                 bad fronts with myth a myth MapReduce                                 and their other end we have this high                                 for high frequency processing so this is                                 like high frequency trading the stock                                 market you know when you swap stocks                                 thousand times a second if you're                                 monitoring a continental power grid this                                 is very demanding to find out exactly                                 how much what are unpaired when eating                                 my system monitoring data centers all of                                 these are processes we actually have                                 guaranteed execution time of                                 milliseconds you have to promise to your                                 customer you have to promise to client                                 you will execute in milliseconds                                 obviously this can only be achieved by                                 being very sloppy in working in a very                                 small context if you have to respond to                                 Millie or microseconds you can only use                                 the l                                                                data set very limited context you can                                 work on and it's sloppy before you lose                                 the cadence before you trip and fall                                 you're just going to ignore the request                                 and take the next request and this is                                 something most of our customers are not                                 willing to do so this the sweet spot for                                 me for smack is everything in between                                 you want to react in seconds yeah but                                 you want to react in seconds of the big                                 context you want to make you want to                                 know what did Stefan do in the past what                                 is Stefan doing right now you want to                                 have a big context you want to work in                                 seconds this is for me the sweet spot                                 where we would see smack so examples                                 would be updating news pages classifying                                 users is this valuable users is a fraud                                 user I've seen this a couple of times                                 today real yesterday real-time bidding                                 for advertising our motive in IOT we're                                 doing projects or industry                                               the typical use cases the machine is                                 behaving oddly is this having any effect                                 of my output this is these are second                                 decisions does that make sense so far if                                 you don't say yes I'll just repeat                                 myself okay so let's get to the juicy                                 parts um what do we want we want to have                                 a reliable ingestion libel ingestion                                 means that when your data producers keep                                 producing data and you have a little                                 hiccup in your system because you cannot                                 consume the data fast enough because                                 your system crashed because the data                                 scientists made a new algorithm which                                 doesn't scale as well all of this kind                                 of can't create trouble okay and if you                                 then you want to be able to to handle                                 this trouble you'll get to this in a                                 second you want to have flexible storage                                 and Creole alternatives the talk which                                 is hurt before you know how you can                                 build grass on the fly some problems are                                 really easy to solve if you consider the                                 problem to be a graph some problems are                                 really simple to solve if you see them                                 as a number of tuples you can do                                 MapReduce on but you don't want to be                                 limited by your database choice you want                                 to pick the database which is the right                                 one for your job                                 and you want some what a management out                                 of the box okay if you've ever set up                                 cough cough if you've ever set up a                                 spark cluster did these can be royal                                 pains in the neck and you don't want to                                 do this all the time manually don't want                                 to monitor this manually you want to                                 have some kind of management features so                                 what is Mac it's an architecture toolbox                                 we looking at the architecture in a                                 second it's the best of reach platform                                 we've seen customers who don't use spark                                 who use flink you've seen customers you                                 use a droid or elastic search of                                 Cassandra it's open do whatever feels                                 best for you guys it's just an example                                 of how you can set up a mesosphere                                 introduced infinity this is a new tool                                 from mesosphere that's the company                                 backing mezzos and they have all these                                 tools or in print printer graded as                                 binary so who's done micro batching in                                 his life okay I already feel a little                                 old school everybody's doing fling where                                 you actually do where you process each                                 event if individually micro batching is                                 little different I think was a good                                 example if you think of a stream                                 processing you can have like a pipeline                                 if you think micro batching you think                                 little buckets so you fill your water in                                 your bucket and then you can process the                                 bucket as a total bucket so what you're                                 doing is you're taking the stream and                                 you're creating windows or static                                 versions of this new one once the bucket                                 is full you can see the bucket and you                                 can process it we typically use micro                                 batching for to establish a context this                                 means I want to find out what's                                 happening in my environment I don't want                                 to react to the environment what other                                 what's happening how many do I have any                                 bots on the system to trying to do to do                                 give me wrong recommendations do I have                                 users who are trying to brute force                                 login are there any specific categories                                 or products which are selling really                                 well my website this is something we'll                                 just establish context which is purely                                 passive                                 so what spark will do for you will take                                 events and I'll generate these r dds out                                 of this or denise is a concept known to                                 everybody okay I'll just do a quick                                 version the rdd is resilient distributed                                 data set and it's a data container                                 distributed across the cluster and it's                                 in it's shorted across the cluster and                                 the rd DS they have like relations to                                 each other you can say this is my basic                                 creamy I do a filter operation I get a                                 new RTD i do a map operation i get a new                                 RTD I join it with other data I get a                                 new rtd and so you have a whole cascade                                 of already DS blondie together and this                                 is the basic work unit in spark you can                                 fill the RT DS whatever you want to if                                 you have Kafka go ahead if you have a                                 database you can pull it the new                                 versions of Cassandra they're going to                                 even support triggers so you can                                 actually have like a stored procedure                                 running you can have a car or any other                                 stream filling your data instead of                                 stream these windows they can be flushed                                 to persistent storage so what you can do                                 is you can run your aggregations so you                                 can do your classifications and store                                 the high that the data of the high                                 information load you can extract it from                                 the windows you can also query them you                                 can you can Korean modify them with SQL                                 or a memory for instance or you can save                                 the results again so what I'm trying to                                 say is any kind of events you just                                 window it and you do a gregation zana                                 stand this is the context you can you                                 can work with if you know this user is                                 looking at different products he looked                                 at in the past you can you can probably                                 assume something changed in this life if                                 you see that users that a user is trying                                 to hack the system because he's giving                                 wrong IP addresses you can react to this                                 not a nice thing is we've you seen this                                 quite often in the projects that always                                 clear when do i do wanna calculation in                                 the beginning you want to do everything                                 in memory but then after all you realize                                 you're building such large windows you                                 having so much memory in data that                                 application is getting sloggi so there's                                 the developers will always be shifting                                 lodging around between stream processing                                 and batch processing the nice thing is                                 if you're working with spark I assume                                 this is the same fling nowadays is if                                 you're having aggregation in aggregation                                 runs on the fast lane or in the batch                                 Lane doesn't really matter because it's                                 the same RP so what we what we learned                                 is that this flexibility between moving                                 data from fast or slow lane or                                 operations it happens more often than                                 you expect okay now we have a feeling                                 that I know what's happening the system                                 I know if I have any for our general                                 users I know if have any BOTS but it's                                 not no it's not enough enough to know it                                 something's happening you have to act                                 upon this okay so when you found the                                 body we want the app the back to stop                                 when they've classified my user I want                                 to decide which ad do I want to show                                 which off upsell offering do I want to                                 show and this is pretty much where we                                 typically use acha so what we can do is                                 that when the when we get the request                                 the show me an ad or some an upsell you                                 give acha the request let acha handled                                 it okay a knocker I has some pretty                                 strong friends the background you can go                                 to spark you can go to cassandra and ask                                 him give me a context ok naka also has a                                 nice way that since everything's a                                 sickness lean passed down you can you                                 can actually deal with your own timeouts                                 you can set a locker okay acha ask spark                                 what Stephanus looking at right now                                 Stefan earth ask Cassandra buddy would                                 it last year and if I can't make up my                                 mind or if Cassandra is too slow or                                 whatever I respond i'm gonna make a                                 best-effort estimate and just give him                                 the product which was shown the most                                 time                                 was all the most time so acha you can                                 build his hierarchies of strategies and                                 how to deal with a request and also                                 having worked in different service                                 levels does that make sense if you don't                                 say yes I'm going to repeat it okay at                                 least some knots so we mentioned the old                                 problem that we want to have a reliable                                 ingestion it's the old school I mean                                 when I did enterprise Java I was                                 typically you had a couple thousand                                 requests per hour and if the request                                 would collect over the wild was okay                                 okay because if you're storing millions                                 or thousands or billions of rows didn't                                 really hurt you that much the problem is                                 nowadays there there's so much load so                                 much data being passed around you have                                 to be a little more careful yeah if                                 friday night a system crashes and it's                                 filling your ftp server with                                    gigabytes an hour you probably have to                                 react over the weekend okay because the                                 the the boats are keeping or getting                                 bigger you're dealing with and the                                 bigger the boat the battery is when it                                 sinks so um core concept of this whole                                 system are the reactive streams anybody                                 heard of them before okay it's going to                                 be boring for you guys I'm sorry the but                                 the basic idea is you have three                                 different kinds of streams there's a                                 direct stream I'm not going to dive into                                 this between Kafka and spark but there's                                 something like the raw streams this is                                 what we typically avah developers know                                 this is when I consume something from a                                 TCP connection or when I want to push                                 something into a TCP connection and                                 they're reactive streams the they behave                                 like streams but they have a much better                                 back pressure support back pressure                                 known to anybody ok this is this is flow                                 controls is when you still have the old                                 modems you can adjust the flow control                                 if you want to have software hardware                                 flow control the flow controlled hell's                                 basically                                 is it tells the system stop talking to                                 me I have enough information right now                                 okay this just means if I buffer is full                                 it's saturated and if you give me more                                 information I'm just one I just won't                                 listen to you anymore so this is this is                                 something which creates back pressure if                                 you if you're pushing data in too fast                                 for a system to consume it basically                                 this is physics and then the back                                 pressure is if you take a strong you                                 blow through it and then tighter the                                 holes destroys the more back pressure                                 you have this is a back pressure in                                 American how so called wastewater system                                 with the manhole covers blown off but we                                 look at this in a second again just keep                                 this in mind now so how does the data                                 flow in and out of the system preferably                                 over Casca why it's it's an append only                                 system again this means that consumers                                 may be offline for days so if you write                                 some kind of logic with machine learning                                 which processes your data you want to                                 have the data coming in over kashka so                                 in case your machine learning algorithm                                 crashes or whatever you still have the                                 data you can really put a repeat that                                 analysis you won't have a broker broker                                 reduces the number of point-to-point                                 connections all you're a cop doesn't                                 have to know all the other acha jobs it                                 just has to know its broker and what the                                 topics to use you can use routing on                                 streams you can multiplex demultiplex                                 events example was from the thought                                 before sometimes it makes sense to                                 monitor trains individually so each                                 strain gets its own topic sometimes                                 makes more sense to analyze all trains                                 of a certain mark of a certain                                 manufacturer maybe you want to Wilda                                 Plex your events different kind of way                                 this is something which you can do quite                                 elegantly in Kafka we'll have a look at                                 this in a second you will have overloads                                 mechanisms you can react to this you can                                 either scale up on the hardware level                                 you can scale out in the hardware level                                 I'm sorry you can also tell the client                                 to communicate list or you can just                                 buffer it and the buffer is the first                                 line of defense and the best part about                                 acha about casca is my point of view the                                 replay mechanism you usually have a                                 couple days worth of data in your calf                                  you and your Kafka system this is                                 especially nice because most big data                                 environments I've seen so far they have                                 very little possibilities to do load                                 tests & test environments yeah because                                 you can't have a test environment which                                 cost you millions of dollars a year most                                 customers won't do this so you usually                                 do a lot of a lot of deployments                                 directly into production which is fine                                 if you know if you know what you're                                 doing if you deploy broken algorithm                                 which marks all of your customers was                                 fraud customers then you have a big                                 boo-boo because they can't order there                                 anymore fix your algorithm replay the                                 data from Kafka and you can fix the data                                 okay I think this I think this is the                                 best part cuz I'm always relaxed doing                                 the release in production when I know I                                 can get out of it again this is your                                 ticket out okay this is um I'm a German                                 guy we drum engineers who sometimes                                 really tough giving talks and add this                                 one guy and he talked to me for an hour                                 after talk saying Kafka was because                                 it didn't support exactly once nothing                                 works exactly once in distributed system                                 so don't even think about doing it just                                 right your systems I didn't pretend so                                 you can deal with double requests okay                                 but this is probably for another                                 audience okay cloud bare metal who's                                 running in the cloud public or your own                                 okay you can there are multiple reasons                                 to do things different ways some                                 customers they have data privacy issue                                 some customers are worried about the own                                 IT department but in the whole smack                                 stack since you're running with Messrs                                 you can do both message does not require                                 any virtualization that's the require                                 information infrastructure as a service                                 you can run mezzos directly on the bare                                 metal if you want to but if you have a                                 cloud you have redundancy and elasticity                                 in it go ahead and use it I mentioned                                 this before you do not eat any                                 ritualization or continues ation so if                                 you can I would try to run tools like                                 Kafka tools like spark I'd like to run                                 them natively on the host because                                 there's no reason you want to separate                                 room there's no reason you need a own                                 kernel there's no reason you need a                                 share your i/o ops with other processes                                 just try to run in bare metal if                                 possible but you don't have to                                 so i think i'm doing right way too fast                                 let's see basically you can do whatever                                 you want to there's this there's no                                 governing agency telling you how to use                                 smack as I mentioned before I try to use                                 Kafka as much as possible if you have                                 systems which have to which require                                 response milliseconds because they're                                 coming in over web socket or the coming                                 of the rest service you probably want to                                 process directly naka and have a car to                                 do the service level agreements but if                                 this can be done later always push it                                 push it through Kafka your Aaka can ask                                 spark your sparking right read from                                 Cassandra to read and write from HDFS so                                 this is a lot of customers have                                 different flows like one customer they                                 had a lot they used I UT that a lot of                                 data coming in so we push them in the                                 kafka we would have a spark chop                                 listening which would create a right                                 repeat log in HDFS so they could keep it                                 over years could make sense doesn't have                                 to make sense Nessa said before if you                                 have a droid here you can you can sit                                 right here if you have elasticsearch                                 running there or HBase go ahead nobody's                                 going to stop you but it's important for                                 me at least that's that's my two cents I                                 learned from from big data is the tools                                 or so they have very narrow sweet spots                                 they do very few things but they do it                                 very very well I grew up with Miss                                 Oracle and database and then db                                          and everybody told you use Oracle use                                 Oracle for any problem we had but this                                 is changing now and this should reflect                                 in your stacks                                 does it make sense okay look at the                                 streams one more time thanks stream                                 processing probably her saw this slide                                 and thousands of different variations                                 last couple two days there an unbound                                 and continuous sequence of events this                                 means we do not know when that will end                                 we do not know how many events will be                                 coming in and all of this is open and I                                 think these Layton's these shifts load                                 are very important as well especially if                                 you diverted iut environment you do not                                 know when your sensors are going to fire                                 so what do I do typically when I work                                 the streams um I usually do not have                                 this problems long periods of time and                                 threatening fluctuation load usually                                 even I deal with streams I do custom                                 buffer copying I would create a buffered                                 streams whatever it is what I usually                                 did the Java were all right if I would                                 use a buffer to which is bound I can I                                 can survive more okay when the client is                                 pushing more down Anakin process I can                                 always use the bound buffer the if it's                                 ram or                                                                 okay if it doesn't fit in my register                                 anymore just push it off to another                                 system but these buffers can exhaust                                 quite quickly so the questions always                                 what do I do when the custom in a client                                 is sending more than I can process do i                                 drop the old data do i drop the new data                                 or do I just reduce the sampling rate                                 and ignore every tenth every first every                                 second request now this is where                                 reactive streams come in they they have                                 a nice mechanism and interactive streams                                 you have like a pearl necklace or graph                                 of consumers listening to previous                                 consumers okay so for instance I know                                 that I curse working on the stream it's                                 doing some selects then it comes to me I                                 should actor I should write some                                 aggregations to database and then it                                 would pass to recurse on the next system                                 to do something                                 with it and usually this works in the                                 push mechanism so the so the practices                                 are pushes events to me and I just                                 consume them now what happens is when                                 I'm too slow and I can that process the                                 data anymore the predecessor is informed                                 nice clear switch to a pool mechanism so                                 it's very human so when I come home and                                 my wife talks to me and I'm just                                 overloaded I tell her I'll pick up I'll                                 come to you and ask you in                                            okay and this is how the reactive                                 streams work and this and this mechanism                                 does this this fail-over can go through                                 the entire graph so it can start the                                 very end where somebody's not fast                                 enough and it's failover of push to pull                                 is going to go through the entire graph                                 until until the source and the source                                 has to decide what to do with this okay                                 if the source is some kind of a car HTTP                                 claw server it has to know what to do do                                 I stop listening to clients do I cue the                                 results but if your source is Kafka                                 everything's easy because Kafka is                                 capable of buffering this data for days                                 and weeks and as soon as your consumer                                 picks up again because this congestion                                 is removed the whole failback is going                                 to revert itself is going to fall back                                 to a push mechanism and these are the                                 kind of nifty things which save your                                 neck later in production okay so it's                                 not like you know the problems when you                                 come to work and you in and you see the                                 locks are full of exceptions you can see                                 a lot of poisonous pills and your back                                 locks what happens you don't really know                                 the system here can help itself blah                                 blah blah blah basically the slide says                                 that all these components we talked so                                 far use the same protocol of reactive                                 streams so you can what what comes out                                 of spark streaming you can you reuse                                 again which is reactive icona supports                                 it so you could this whole flow to hop                                 it is completely open and as I mentioned                                 before                                 kafka is a perfect candidate for a bound                                 buffer use it as much as possible and                                 measles can scale consumers on the fly                                 during the fall back so this is my mo ok                                 I'm done a second what Maysles can do it                                 can scale up so for instance if you have                                 a stream consumed where we're trying to                                 write to cassandra and cassandra is                                 getting too slow measles can go ahead                                 and do a scale up on your cassandra                                 installation but this takes time if you                                 have a big cassandra distribute                                 installation and you should activate                                 more nodes it's going to rebalance the                                 data and it can take                                                     this time your measles is scaling up you                                 can have Kafka covering your back then                                 missus is going to be up then cuff                                 Kraken can switch over to a regular mode                                 again and you can work off the back load                                 does that make sense we've been a                                 type-safe partner for quite some time                                 but we've never seen so much interest in                                 spark and scala then since we didn't                                 spend working spark projects and i don't                                 think it's a coincidence so screams love                                 to be processed in parallel ok and                                 streams love scala so every time you do                                 something sparked you in a scholar i saw                                 all the flink examples were written in                                 scala because it has some features                                 buildings are quite nice you have this                                 built-in preference for immutability so                                 when you working when you work on                                 streams never manipulate the events                                 create new events and push them down to                                 another stream this way it's always                                 reproducible what happened to the events                                 Scala has this notion of having a better                                 support for functions functions meaning                                 that you don't have any side effects of                                 a function if you look at a typical Java                                 construction like a servlet you can have                                 instance variables bound to disturb the                                 instance meaning that the service will                                 respond in different ways depending on                                 when you ask the servlet but you don't                                 want this in functions functions should                                 be neutral to any side effects and                                 another nice thing this is where I                                 started love JavaScript when you can use                                 functions first class                                 citizens and pass them around I'll skip                                 the inverted index example but like to                                 show my kitten slide any questions any                                 feedback including oh my god this was                                 boring or one moment thanks for the talk                                 is there any really huge case a really                                 huge case you've applied will this case                                 for the smack stack um what I mean is                                 real use case are you talking about                                 projects we're doing are you talking yes                                 there are projects for this it's                                 emphasizes really strongly about context                                 detection and context reaction and as                                 soon as you have a problem where the                                 context does not fit into a heap stack                                 at the browser anymore then you have a                                 use case so if you have a website and                                 you're dealing with million customers                                 and you want to classify the customers                                 everything you're probably better                                 writing everything in regular java.util                                 classes but as soon as your context gets                                 too big to decide on this on the fly or                                 dis items in one machine you want to                                 you're moving towards this max stack                                 then that's my feeling please discuss so                                 our examples was we have two customers                                 from the automotive industry where we're                                 doing these evaluation if all the                                 sensors to put the cars are delivering                                 we have one customer doing real-time                                 advertising and those are the most                                 interesting ones at the moment                                 any more thoughts any more questions to                                 add thank you so much for your session
YouTube URL: https://www.youtube.com/watch?v=ZNrDzjxkGrE


