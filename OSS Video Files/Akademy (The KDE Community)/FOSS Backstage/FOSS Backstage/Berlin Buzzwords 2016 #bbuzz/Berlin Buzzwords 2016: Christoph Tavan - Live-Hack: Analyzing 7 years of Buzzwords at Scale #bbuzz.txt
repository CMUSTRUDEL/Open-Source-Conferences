Title: Berlin Buzzwords 2016: Christoph Tavan - Live-Hack: Analyzing 7 years of Buzzwords at Scale #bbuzz
Publication date: 2016-06-11
Playlist: Berlin Buzzwords 2016 #bbuzz
Description: 
	We're coming together for Berlin Buzzwords' 7th edition and over the course of the years a lot has changed in the Big Data Technology ecosystem. Once-hot buzzwords have vanished and new buzzwords arose.

While you would probably have written a MapReduce job in Java to crawl the web and analyze it on a massive scale this has now become much simpler with tools like Spark and Flink at hand.

I want to do a live coding session where I show that today it is possible to write a scalable web crawler and analytics tool which scrapes the past 6 years of Berlin Buzzwords (websites) and shows some interesting insights in the Big Data trends of the past 6 years. While I will run the tool on the very limited data set of the historical Berlin Buzzwords websites I want to highlight that it would in principle scale to crawl millions of websites and analyze petabytes of data.

Read more:

About Christoph Tavan:

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              Jules okay thank you for the                               introduction and welcome so you're                               almost done last two sessions for today                               I hope you enjoyed this year's buzzword                               so far and my talk today is going to be                               something like a little recap of what                               happened over the last seven years                               because its seventh Burton buzz words so                               thank you ready for joining I wanted to                                start my talk with a little hands                                raising so for whom of you is this the                                first Berlin buzzwords okay quite a few                                and who did anyone of you attend the                                very first building buzz words in                                                                                                         than than twice attending okay also                                quite a few so yeah I'm christov working                                as a CEO at mb are targeting we're a                                real-time bidding on an advertising                                company and I've been attending Berlin                                buzzword since                                                     first two years but since then I've been                                a regular visitor and I've given my                                first talk last year and well it came to                                me that I was really wondering well now                                this conference is about buzz words but                                what were these buzzwords again over the                                last                                                                  obviously some buzz words that were                                really like hot topics in one year and                                no one ever talked about it again in the                                locker in the next year and then there                                were other buzzwords who were like                                trending and becoming more and more                                important over the years so I don't know                                just to mention a few I put up some                                logos on this slide I don't know is                                anybody still talking about Apache                                mahout I remember it being a very                                important topic in two thousand                                                                                                   similar for a pet GH bass on the other                                hand we have topics like link which is                                all over the place this year for example                                but I thought since we are like it's a                                tech called a tech conference and we                                were all sort of quantitative people it                                should actually be possible to answer                                this question in a quantitative way so                                what were the actual buzzwords                                                                                                            think a lot of stuff has happened over                                the past seven years that will actually                                should actually make it fairly easy to                                analyze that so my plan for today is to                                basically scrape the websites of the                                seven conferences which are still                                available on sub domains under building                                buzz words d and then extract and                                analyze the buzzwords so yeah that's the                                plan for today and yeah sure I want to                                do that live I want to kind of see if I                                can can do it in a scalable manner and I                                have something like                                                 that so let's see if you can if we can                                manage so first step will be scraped the                                historic websites so if you want to do                                web scraping there are basically two                                options in general you can either simply                                create everything and then try to filter                                and make sense out of your content later                                this is particularly or I would say this                                is the approach when you basically don't                                know yet what you're looking for in                                particular you have no idea about the                                structure of the content that you will                                be scraping or you can do the more like                                a tailored targeted away or like the                                more manual way if you already know what                                you're looking for then you can                                basically try to only scrape that                                content that you're really interested in                                in the first place                                I mean they're of course tons of am I                                still there yeah okay so there of course                                lots of options that are open source                                projects that help you help you with                                this so I mean I think one of the most                                well-known software projects for                                scraping which has been there ever since                                we were talking about Hadoop stuff is                                Apache notch it's still an active                                development and it's really like                                probably the first choice if you want to                                do a large-scale web scraping with an                                open source project but then there are                                smaller more tailored or like then                                there's a solution for like this more                                targeted scraping approach that I found                                 which is Python library called scrapie                                 or scrape I I don't know how that's                                 pronounced so while the notch one is                                 definitely the huge scalable thing I                                 think for scraping the Berlin buzzwords                                 websites the other one will do as well                                 so here I'm going to use scrapy and it's                                 actually pretty well-documented very                                 nice project so this is taken from the                                 home page and you all you need to do                                 basically to run your own scrapers is to                                 extend a spider a class that that's                                 create exposes and then you specify some                                 start URLs whereas cape scrape it will                                 start and you specify at ours method                                 that part is the content of this side                                 and tells crappy what to do for example                                 with the links that are found if those                                 things shall be followed and so on and                                 so forth and then here you can specify a                                 call back of what scrape you should                                 shall do if it visited a link and                                 let me see so I've written can you read                                 that actually shall I make it a big bit                                 bigger or bigger better still bigger                                 okay so yeah all I need to do is define                                 this class or extend to crawl spider                                 class then here I basically specify                                 allowed domains which is basically just                                 to make sure that my crawler doesn't                                 leave the Berlin buzzwords website and I                                 specify the start URLs where I basically                                 explicitly put all the prefixes from                                                                                                          and then there is some more magic which                                 I will talk about in a minute but what                                 that all ends up with is a parse method                                 that parses the HTML content of these of                                 these sites and amid some data and as                                 you can already see here I'm looking for                                 some very particular data so I'm looking                                 for the title of the page and i'm                                 particularly looking for the session                                 abstract pages actually I'm looking for                                 the content which is basically the                                 session abstract and I'm also extracting                                 the speakers of these sessions and I'm                                 also storing the link so more about that                                 later so since that will take a moment                                 let me just run this in the background                                 so as you can see this is the class that                                 I just showed you and I will now                                 activate my virtual environment and then                                 I will run this spider so it comes with                                 a tool called sporadic Scrappy's as run                                 spider I just have to provide my                                 class and I can specify an s                                           as an output so okay so while I will                                 continue with some some slides hopefully                                 it's greatly will now crawl all the web                                 pages from                                                          background and place the results on an                                 s                                                                     out I actually realized that scraping                                 web pages is still quite hard and so for                                 example I realized that the website from                                                                                                  content-type header I mean you can't                                 read it here but simply the response                                 header content type is missing and that                                 makes greatly think it's not a web page                                 but something else and it will simply                                 not pars it so one of the additional                                 code that you saw in the in that crappy                                 file was actually a workaround to                                 artificially set this HTTP header for                                 these old web pages then another thing                                 which actually wasn't that easy was to                                 identify what page is actually a session                                 abstract page because I want to restrict                                 myself to the session abstracts and the                                 analysis and and what is not and I                                 initially thought well there is a an                                 element within CSS class this date                                 display single and it turns out to be                                 there in two thousand ten and eleven but                                 not                                                                      other fields that actually show that so                                 figure that out how to identify a                                 session page same for the speaker how do                                 i get the speaker again is it one filled                                 in all the pages no apparently                                 technology has changed in the meantime                                 so i have different selectors for                                 different years and then as well for the                                 session abstract it was also hidden in                                 different HTML elements so let me check                                 how scrape he's doing it's still working                                 okay so I have one suggestion to well or                                 first of all I can draw an intermediate                                 conclusion that basically scraping the                                 web is still pretty hard even in                                      especially if you're trying to scrape                                 content which is made available in a                                 form that not that's not very scraper                                 friendly so actually it does make sense                                 to structure web pages with proper HTML                                 so that they can so that actually also                                 machines can understand them and i think                                 it's a it's quite an impressive job that                                 search engine companies like Google and                                 so on are doing by still being able to                                 somehow make sense out of not so well                                 designed HTML pages yeah let's see if                                 the content is there now so it isn't I                                 suggest not to not lose lose any time I                                 will simply use a file that I have                                 crawled before i was expecting myself to                                 take a little longer talking so that                                 scrape you could finish in the                                 background but instead i want to                                 continue with the analysis and that                                 should be the empty more interesting                                 part so to analyze that data i was                                 actually looking for a suitable a                                 suitable solution to do that in a way                                 with                                 which can be easily presented so a few                                 years ago interactive notebooks came up                                 which allow you to manipulate data and                                 have some sort of distributed computing                                 engine in the background so you edit                                 your data as if or you you you and can                                 analyze the data as if it was like on                                 your local machine but in fact you have                                 an arbitrarily big cluster in the                                 background that does the actual                                 computations but still you get your                                 results back in a very nice way and                                 there are some commercial solutions                                 which I just listed here but there are                                 also very great open-source alternatives                                 like Jupiter which was previously known                                 as ipython notebooks or Zeppelin is a is                                 a very young and promising project and                                 if you're intending to use spark there's                                 a project called spark notebook where                                 you can download a whole bundle which                                 includes spark and everything else and                                 you just unzip it and run it and you                                 have a fully running low local of course                                 in that case a spark notebook so I mean                                 the dropping of these open source things                                 is of course that you have to somehow                                 provide urine the infrastructure on your                                 own but it's open source and for those                                 commercial solutions they will provide                                 you with the infrastructure and you                                 basically have a cloud like paper paper                                 use pricing usually and for my analysis                                 I will I made some very arbitrary choice                                 and I pick data bricks which is a made                                 by the people behind apache spark and i                                 will now switch over to that so i will                                 follow the Golden Rule and as its shown                                 in Wikipedia it says you should always                                 do live coding during conference talks                                 and I will do that now and hope that                                 internet connection will work ok as you                                 see                                 rape is still not finished so I will use                                 the file which I have always great so                                 this is how it looks like a better I                                 have to make it bigger again it's it's a                                 notebook you can write commands there                                 they're executed server side and you get                                 the presentation of the results so all I                                 did so far was preparing it with some                                 imports some import statements which I                                 will run now and the first time I run                                 something I'm being asked if I want to                                 spoil a cluster and i'm here on the                                 Community Edition which will launch some                                 spark one point six point one cluster                                 with six gigabytes of RAM and I think                                 it's all running on on AWS but in the                                 paid version you can basically spawn                                 arbitrarily big clusters in there okay                                 that's it then in the next field i have                                 my AWS credentials which i have hidden                                 here and then this is basically some                                 code which i copied from the data bricks                                 documentation which is what would shows                                 you how to import your data i will give                                 it a last ride oh it's crappy finished                                 so after six minutes bit slower than                                 than before scraping managed to to                                 scrape all the websites from last seven                                 years and it put the stuff into an s                                  bucket be bus                                                          and it's simply json objects line                                 separated so i can actually mount this                                 bucket in data bricks which is what this                                 code does                                 well it's frozen exception here because                                 director is already mounted and I can                                 have a look at the contents so oh okay                                 seems like I didn't set the AWS                                 credentials correctly so my data is not                                 there i will then instead use a                                 different bucket where i have this all                                 years jason lines file which i wanted to                                 to put into that bucket still available                                 sorry for that okay so i have my bucket                                 with this file first thing i want to do                                 is reading that file because i want to                                 work with that data so i will go ahead                                 and say i will use the sparks equal                                 context here it's a nice nice rather new                                 interface of spark which allows you to                                 do sequel like and also data frame like                                 operations on your data and it has a                                 read Jason method which I can use to                                 read this Jason phone and then I can                                 basically register this data as a temp                                 table and let's call it be both raw and                                 then I can go ahead and say display                                 something like sequel contacts dot                                 sequel and then i can type any sequel                                 command and can treat it just like as if                                 it was a likely database table so i can                                 say                                 star from bee buzz roll limit                                         that and in fact it shows me the data so                                 what I realize here is that apparently                                 in the scraping it didn't work perfectly                                 well so I don't only have session pages                                 here but I apparently also have user                                 pages so let's maybe look at that in a                                 bit more detail where content not equals                                 empty okay but now I have I already see                                 that that I have session abstracts here                                 so maybe some of you saw this learning                                 to rank talked earlier this day okay and                                 now I want to go ahead and and really                                 draw some conclusions from this data so                                 if you want to do natural language                                 processing I mean oh there are of course                                 great great alternatives for dads so                                 there are natural language processing                                 libraries that help you with tokenizing                                 and cleaning up your your string that                                 would be a little bit overkill for this                                 talk so I will simply go ahead and just                                 apply some very simple transformations                                 on this text to be able to get a get                                 some meaning meaningful results all of                                 it so basically first thing I want to do                                 is because I want to count words later                                 and find out which we're actually the                                 the buzzwords in the years I will first                                 of all lower case the contents to not                                 like to not have any problems with with                                 case sensitiveness well then                                 as you can see there are still lots of                                 yeah lots of dots and commas and so on                                 which are of course also posing some                                 problems when when you want to when you                                 want to analyze the text so let's remove                                 that to not stress your your time too                                 much i will copy some code which does                                 that so here i have prepared some code                                 which basically all that it does it runs                                 a regular expression and its first of                                 all substitutes all multiple white                                 spaces by one white space and what it                                 does as well is it basically substitutes                                 any character that is not like a number                                 or a letter yeah it just removes them                                 and it also removes trailing and leading                                 spaces and like spark sequel has it has                                 a nice or as it has a pretty easy way to                                 register user defined functions you                                 simply say sequel context register                                 function and then you can register any                                 arbitrary a Python function that you                                 defined here and you can later apply it                                 in a sequel query so that's basically                                 what's being done here so I registered                                 at cleaning function and I use it here                                 in the sequel expression I can run that                                 again ok as you can see comparing to                                 here so you have learning to rank and                                 then like parentheses and a comma and so                                 on this now all has been removed and it                                 already looks much better so if I would                                 run a word count on that now I would of                                 course have a problem at all these stop                                 words like off and in and and                                 so on and so forth would definitely be                                 the highest-ranking words so I'm going                                 to go ahead and remove stop words and I                                 also don't want to bore you too much and                                 we'll copy some more code so data breaks                                 provides the stop words file and some of                                 their s                                                                 I have a look at it oh yes ah so you can                                 see typical list of stop words and I                                 define another function which will                                 simply remove those toppers so like the                                 naive way would be to simply nest these                                 user-defined functions in the sparks                                 equal expression but unfortunately that                                 doesn't work because there is a balance                                 park it's it's known and not yet fixed                                 and so unfortunately I have to register                                 another rapper which I called clear all                                 which will nest my user defined                                 functions so first of all it will clean                                 the string then second it will remove                                 the stopper and I can run this again and                                 I should already have a much better text                                 that I can now finally analyzed there's                                 another thing which I want to get rid of                                 and i will also just copy and paste that                                 it's a called limit limit ization so you                                 have plurals and singular of the same                                 word and I have a very cheap version                                 here so i just want to remove all the SS                                 in the end of in the end                                 of a word so that I don't have these as                                 two two words so I don't know where                                 whoops compa companies that's of course                                 a bad example because it will groups ah                                 because it will where was it never mind                                 you you get the picture so I just don't                                 want to want to have the plural and                                 singular of the same word being counted                                 as two different words and then final                                 step and I promise this is the last one                                 in the preparation phase and there again                                 I mean there are probably of course                                 libraries for that that helps you like                                 detecting that something like big data                                 should not be treated as big and data                                 and open source should not be treated as                                 open and source but instead as one token                                 and I have a very cheap variant of that                                 which I will simply run okay so finally                                 we should have some rather clean content                                 and I will now go ahead and from my                                 initially scraped raw content I will                                 create a new table which contains the                                 clean version of it so I select the link                                 the speakers I extract with this                                 impression the year from the link and i                                 clear I clean the the title and the                                 content and I concatenate title and                                 content into a body which I will now                                 analyze oops already better but still                                 okay we still have these users pages in                                 here so we should filter them out as                                 well and there is another expression for                                 that fairly simple which simply simply                                 says if the link is not lie                                 / content or / session or if the title                                 is something like lunch or coffee break                                 and so on we don't want to consider this                                 document okay and I'm finally I finally                                 have a table which contains cleaned and                                 like queryable data and I can now do                                 word count on that or I can first of all                                 count the number of documents so i can i                                 registered my sessions here as a as a                                 table and i can simply say i want to                                 group these these sessions by year and i                                 want to count them and i want to order                                 it by ear and then data breaks give me                                 gives me this nice visualization so i                                 can actually see Oh in                                                 it says only                                                          not really true problems rather that the                                 website doesn't contain all the sessions                                 anymore but I checked for the other                                 years that these numbers are actually                                 fairly correct so two thousand twelve                                 and fourteen were the years with the                                 most talks in                                                         lot less talks and it's actually fairly                                 stable in the in the last few years ok                                 that's just talks I now wanna have a                                 look at speakers so who were the top                                 speakers over the years so for this I am                                 simply going to do some word count                                 basically so in my table I have a if you                                 remember I have a column called speakers                                 and this column already contains an                                 array because there was some talks where                                 there were multiple speakers so I'm                                 gonna access that here so                                 right so session is my table i can write                                 i want to select the speakers i want a                                 flat map that because this is a spark                                 data frame basically it will when you                                 iterate or when you map over over over a                                 column it gives you row objects where                                 you then have to access the field once                                 again so the field is called speakers                                 and its property of this line then I do                                 the canonical world word count with it                                 so my flat map basically explodes the                                 speaker arrays and my map produces the                                 word count pairs and then I reduce by                                 key groups and to transform it back                                 reduce by he does the actual counting                                 and tend to transform it back to a spark                                 data frame I create a row object which                                 contains the speaker and the number of                                 talks and then I have to wrap all of                                 that into a sequel context dot create                                 data frame and I can order this by                                 number of talks and I can say sending                                 false because I want to order it                                 descending and now the question is who                                 would you guess was the top speaker of                                 Berlin buzzwords any guesses ok that was                                 obvious but who are a second                                 not bad so um sessions in fact you were                                 quite good so first place is Ted Dunning                                 with nine talks so more than there were                                 buzz words actually and then grant Eric                                 and over all of them with six talks i                                 think these numbers are not even really                                 correct because as i said some talks                                 from                                                                   them gave a talk and in all of the seven                                 years well and then there's many people                                 also was just one talk okay so no big                                 surprises on this end now let's look at                                 the buzz words because that was my final                                 promise and I'm running a little bit out                                 of time so i don't i will not i will                                 copy and paste the relevant commands so                                 what would you say is the single most                                 buzz word or the buzzword that happened                                 at the debt that appeared the most well                                 its data who would have guessed and any                                 of things like Apache and Hadoop and use                                 and so on and so forth but I would what                                 I actually was asking in the beginning                                 was how did that develop over the years                                 so let me copy and paste some more code                                 to be finally able to look at some                                 graphs so basically what i've been doing                                 here is word count per year and we can                                 now actually draw graphs that that show                                 us the word counts per year so is there                                 are there any particular buzzwords that                                 you're interested in to see the                                 development over the years otherwise i                                 will start with                                 those streaming frameworks and I will                                 oops make plot out of this ah how can I                                 know sorry I have to make this smaller                                 ones okay so that's the relative                                 occurrence of the words storms barking                                 fling so basically the number of that                                 word count over total count of words                                 over the years and as you can see flink                                 took over sparked this year at least at                                 berlin buzzword in truth and that's                                 actually quite a quite a quite great                                 friend i would say well it hasn't it                                 didn't even exist in                                                    already already there it's quite obvious                                 that now it's really something people                                 are talking about any other things that                                 you're interested in                                 Kafka or I can basically edit edit to                                 this graph may be okay cough guys even                                 more popular than fling probably because                                 it always comes in in conjunction with                                 it let's try batch and stream Oh looks                                 like we're really in the age of                                 screaming now okay anything else                                 otherwise since i'm running a bit out of                                 time let me conclude oops okay I tried                                 it live it worked kind of scalable well                                 the analysis part in principle was                                 scalable because you as I said you can                                 run an arbitrarily large sparkless than                                 the in the background in                                                 kind of but if I remember what it would                                 have taken me in                                                       not have even walked through this like                                 classical MapReduce example back then in                                                                                                     obvious that there were quite quite some                                 advancements over the last seven years                                 and yeah what were my conclusion so                                 scraping is still hard even in                                         have the number one top speaker of                                 Berlin buzz words which is obvious some                                 other speakers who were we apparently                                 really have something important to say                                 every year so I think that's also quite                                 impressive flink seems to be a bigger                                 buzz word and sparked this year I found                                 that interesting as well and as I said                                 we were clearly in the age of streaming                                 now well who would have guessed but i                                 think it's apparent so yeah I hope or                                 thanks for staying with me so long I                                 hope I                                 and bore you too much and just confirmed                                 and quantified what you had expected                                 before I will put up all the code on                                 github and the slides will be online as                                 well of course and get in touch on                                 twitter or catch me after the talk and I                                 fear we only have time for one more                                 question thanks for that cur stuff                                 thanks for that do we have questions or                                 comments in the audience seems like                                 we're good thanks again to Christophe ok                                 thank you
YouTube URL: https://www.youtube.com/watch?v=ShGWOVWjKIU


