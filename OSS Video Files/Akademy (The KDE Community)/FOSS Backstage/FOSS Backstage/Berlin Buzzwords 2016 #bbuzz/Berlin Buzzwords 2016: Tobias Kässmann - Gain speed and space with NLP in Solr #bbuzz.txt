Title: Berlin Buzzwords 2016: Tobias Kässmann - Gain speed and space with NLP in Solr #bbuzz
Publication date: 2016-06-12
Playlist: Berlin Buzzwords 2016 #bbuzz
Description: 
	With 100M+ products in a single Solr index it is hard work to keep response times as low as possible. A key factor is decreasing the index size and the number of terms indexed. We try to store the significant terms only. For long product descriptions this is challenging. Applying NLP at index time is a costly operation. In this talk I’ll present a smart algorithm that run fast enough to be applied at index time.

Read more:
https://2016.berlinbuzzwords.de/session/gain-speed-and-space-nlp-solr

About Tobias Kässmann:
https://2016.berlinbuzzwords.de/users/tobias-kassmann

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              hey hello my name is device and today i                               want to show you how you can gain gain                               speed in space with an LPN solar as you                               know my mansion I just renamed my talk                               to gain speed and precision with NLP and                               solar because I think it's a more                               interesting topic in this talk I first                               want to start with some basic                               information about me I'm husband and                                father of two beautiful children i'm                                from hamburg and i'm working in hamburg                                for shopping                                                           initiator and organizer of the search                                technology meet up in hamburg so if you                                visit hamburg feel free to join the                                meetup feel free to learn something                                about search technology the next meetup                                will be on wednesday and we're having a                                great time with two talks from the                                berlin password and a third talk from                                ebay clannad saying we have free time if                                you are in hamburg feel free to join yes                                i want to start with informations about                                shopping                                                        shopping                                                               so we do not sell any product we just                                deliver terrific to shops and we've got                                a lot of different website where we show                                products to customers in our back end                                you will see on the left side we are                                basically using apache solr as a search                                entering we using spring for our api we                                using postgres for all of our bag and                                processes and varnish and github for for                                our projects in the last year our                                management team came around and said                                okay we've got a new requirement for a                                began team and we're currently                                absolutely fine with three million                                products on our website but in the next                                year we want to scale up to                                            products and we are in our bag and team                                sit together and we're like okay now                                that's going to be a hard challenge but                                we saw and we saw three problems we saw                                the disk space of our solar in ecstasy                                indexes explode we saw that is the                                precision you know search we saw                                because when you throw another                                   million products into the search index                                what happens with their with a precision                                in our search result does it preserve                                does it increase decrease what happens                                and we currently have no shouting we're                                now working on it but it's really hard                                to manage all of these products we sit                                together and we finally came out of this                                meeting this meeting and we say okay we                                have to reduce as a first step our data                                we have to reduce text to do not store                                the whole text that we that we've got in                                our products we've we store products and                                they consist of title description                                product media data attributes price                                something else and we saw two simple                                winnings ahead when we do the reduction                                of the text or the index data we saw                                that the speed will increases and the                                space because due to fewer index terms                                in our index so i want to show you in                                this presentation in this short talks                                just two solutions how we do this how do                                we reduce the text and the first                                solution is for just for product                                descriptions and the second solution is                                for usual text so we take a look into                                our products in our data that we store                                in solar and we saw that a usual                                description consists of about ninety                                five percent of irrelevant text or                                rename that SEO text so we've just a                                very very small amount of relevant text                                and this is pretty bad for a for a                                search engine because every chokin that                                is in the SEO or irrelevant text is a                                signal for our search engine and so we                                finally figured out SEO text equals evil                                equals not relevant or irrelevant what                                do we have in our descriptions we have a                                lot of sentences like okay we we have                                here really good-looking blue jeans and                                you can                                on these blue jeans with a great red t                                shirt with blue stripes and something                                else you can sit in it on your couch and                                enjoy this television with a friend so                                and so on we've got a lot of text that                                is not not relevant for a search engine                                and we're in a team sit together and                                 thought about okay how can we filter out                                 these bad sentences and we are like okay                                 how they're really good stuff out there                                 okay we can use the OpenAPI framework                                 with a lot of great algorithm                                 implementation we can use SVM's we can                                 use deep learning technologies we can                                 use neural network neural networks to                                 train a model and that differ between                                 good or bad sentences we can use                                 keyboard instruction but we're ended up                                 with a much much more simpler solution                                 we blaze this solution into the last                                 step here in the solar indexing process                                 and this is our ahora old product                                 pipeline at shoving                                                      the data from our customers we do some                                 basic prepossession some normalization                                 and after that we do some named entity                                 recognition where we choose to recognize                                 entities that we know but you can we                                 also want the the unknown entities in                                 the description so we have to use them                                 as a signal for our search engine we                                 placed we named our component the                                 analyzing sentence tokenizer and we                                 place it in a third step at our in our                                 solar indexing process so we've got a                                 few field type that is called text we                                 first do some basic HTML strip charm to                                 get rid of all of the HTML text then we                                 do dublicate whitespace remove mminton                                 and then comes the analyzing sentence                                 organizer which has a one and only                                 required I commend a list of stop words                                 and here's how it works it's so pretty                                 simple because it                                 relies on the key assumption that                                 sentences in product description with                                 useful information do not contain a lot                                 of stop words so how it works here we've                                 got a description from from a product we                                 just do another basic clean up in this                                 cleanup we do some things to to get a                                 better analysis to get a better                                 sentenced organizing we split the                                 description into sentences and then                                 you've ended up with a simple list of                                 sentences just iterate over these                                 sentences and here comes a little helper                                 if the sentence contains a lot of signs                                 a lot of science means if the the ratio                                 of the signs is above a given threshold                                 then you have to split the sentence                                 again this is a little helper to get rid                                 of all of the the bad data we get from                                 our customers where are no punctuation                                 any description or something like that                                 and after that you've got the completely                                 splitted sentences and just build a                                 ratio between the stopper account and a                                 word count and check is under or above a                                 given threshold and this works pretty                                 well so we've got                                                       first learning was that for the sentence                                 tokenizing process we we've got a first                                 version of the sentence organizer where                                 we use the sentence organized from the                                 open NLP framework and it delivers a                                 really really good quality but it's                                 absolutely not as fast as a regular                                 expression so this is our sentence                                 organizer we do not use any fancy stuff                                 this is our yeah this is it and the                                 other learnings are that you do not have                                 to analyze text that is just one                                 sentence if you have any set of just one                                 sentence pestle through except you have                                 to split it again so this is very simple                                 approach and we finally ended up with a                                 word reduction in our descriptions of                                 about sixty to eighty percent                                 and the precision of our search result                                 preserves or if not better increases to                                 do this simple stuff yeah but what                                 happens if you if you do not have any                                 product description because on a product                                 description you can you can define bad                                 sentences by by the the ratio of a stop                                 word in a sentence what happens if you                                 have usual text here's an example from                                 Wikipedia the Apache Solr entry and what                                 happens if you have you used the                                 analyzing sentence organizer to analyze                                 these sentences he will throw away all                                 of these sentences this is pretty bad                                 but what happens if you also want to                                 reduce the data if you also if you just                                 want to store the relevant text or the                                 relevant keywords of a text we're using                                 an other component the rake algorithm                                 and this is just another Swiss Army                                 knife for you two to get home with and I                                 want to show you a really short example                                 how the rake algorithm works here's a                                 here's the first sentence of of the                                 Apache Solr Wikipedia article and the                                 reg algorithm works as follow following                                 he uses to stop words and punctuation is                                 boundaries for keyword candidates so you                                 can see you've got the keyword candidate                                 solar open source enterprise search                                 platform written java apache Lucene                                 project and his an in from the are stop                                 words or the boundaries and the                                 punctuation after that he calculates a                                 score for for each candidate so imagine                                 you have a really large text this makes                                 a lot of more sense but this is just an                                 example he calculates a score for each                                 candidate and returns one-third of the                                 top candidates as keyword results so                                 this is pretty simple not a lot higher                                 istics needed                                 and it works absolutely good we use the                                 rake keyword extraction algorithm to                                 with two and a half enhancement we                                 define or when you use it you have to                                 define your own stubble domain-specific                                 stop words you can use just the usual                                 stop words that are flowing around like                                 this that and or something like that you                                 have to define additional work types                                 like verbs or something like that you                                 have to define more signs or URLs if                                 it's fits into your domain of text and                                 we've implemented another feature into                                 the rake algorithm we propagate score                                 from overlapping or related keywords so                                 we what happens if you have the text and                                 you extracted the keyword you go boss                                 and the keyword you go and you have a                                 score for each keyword you can propagate                                 the score or multiply a factor into each                                 other keyword because you know this is a                                 overlapping keyword this will bring it                                 to a much better score calculation of                                 the rake of the Ray algorithm and after                                 all of our keyword extraction algorithms                                 we use an additional filtering of the                                 extracted keyword so this is one half                                 enhancement because i think it's is very                                 common yes the analyzing sentence                                 tokenizer is available for free at                                 shopping                                                             have any questions you can mail me at                                 work at tobias kathmandu te or visit the                                 developer block of shopping                                              des                                                                    talk about how to gain speed in space                                 with an MP and solar I think I was a                                 little bit faster as previously                                 previously thought yeah thank you                                 are there any questions well thank you                                 for your for your insightful and fast                                 talk yeah you mentioned that it's                                 helpful to have kind of domain-specific                                 stop words could you give an example of                                 what you mean by this yes so we've got a                                 lot of texts that are coming from our                                 partners so okay I can give you an                                 example where we where use this rake                                 algorithm we've when we've got out when                                 we've got a website with a long-running                                 text and you want to show products to to                                 the text we just get the text let the                                 reiki word extraction algorithm extract                                 the key words and then throw the                                 extracted keywords just into our search                                 engine this is pretty good this works                                 pretty well and in that case we we                                 define a lot of verbs as stop words so                                 we just do not have these usual stop                                 words like yeah this end or something                                 like that so you can use other word                                 types like nouns verbs but they are                                 absolutely domain-specific so                                 do you have access to query logs query                                 locked yeah because I think from that                                 you can gain a lot of information and                                 actually like all these stop words                                 usually don't appear in query logs so if                                 you if you get your hands on query logs                                 so yeah yeah we have access but you                                 think to to generate a stoppered file or                                 what for what yeah I mean you get the                                 data what people are really searching                                 for so you could get your stop words                                 from but you can also include it into                                 your ranking yeah yeah we do this we do                                 this but so you mean the first first                                 approach for the first approach yeah I                                 mean in general okay like yeah yeah we                                 do that you can you can like also create                                 some some relevance score for words                                 based on queer locks seeing which terms                                 are really important yeah yeah we do                                 this thank you hello thank you for your                                 presentation how did you make sure your                                 recall didn't suffer it because                                 potentially you're filtering out                                 sentences that still have important                                 information sorry flurry again so how                                 did you make sure your recall that in                                 software you're optimizing for improving                                 precision or not losing precision but                                 since you're using this simple heuristic                                 right that if the sentence doesn't have                                 many stop words then it's potentially                                 not very rich in information how do you                                 make sure that you're not losing                                 information by using that use yeah we                                 tested it manually with a last lot of                                 data so we are not                                                      throw away some mystery interesting                                 information but it's a lot of data and                                 we're more happy about to throw away                                 near nearly about sixty to eighty                                 percent of the Unruh lovin data and we                                 we implemented the testing and it looks                                 pretty good so I have no exact numbers                                 but humans looking at humans looking at                                 the results in basically making                                 judgments yeah                                 yeah I also thanks my one question about                                 this slide we mentioned that you return                                 a third of the top candidates how does                                 rake do this scoring and is it done on                                 the sentence level on a whole document                                 level so do you have a whole heuristics                                 that the algorithm uses for this scoring                                 or now the the reg algorithm chest                                 calculates a score with a term frequency                                 and document frequency and another                                 factor and then or is the list and then                                 returns one-third of the top keyword                                 candidates ok so for phrases justice um                                 yeah combination of those yet numbers                                 okay hi hi I'm how does this your                                 solution scale because in the beginning                                 you're describing a problem of gun from                                                                                                         hmm and with this approach it seems like                                 yeah you can shorten the text but you                                 cannot keep shortening it or can you we                                 we shorten the text sorry again so I                                 like my colleague has an answer it's                                 just a hi I'm Tristan it's just one                                 building block to to reducing or to                                 increasing the number of documents was                                 and the first building block was                                 reducing the index eyes with the                                 existing three million documents and                                 there was a key success so we could                                 reduce the overall solar index to gain                                 space to blow up to                                                      aside from the speed concerns how how do                                 you check your improving your precision                                 and you are not hurting your recall                                 we've got a lot of bad queries so a list                                 of bad queries and we just check these                                 queries the list of curries and saw that                                 precision has decreased increased
YouTube URL: https://www.youtube.com/watch?v=LD0TCESH_eI


