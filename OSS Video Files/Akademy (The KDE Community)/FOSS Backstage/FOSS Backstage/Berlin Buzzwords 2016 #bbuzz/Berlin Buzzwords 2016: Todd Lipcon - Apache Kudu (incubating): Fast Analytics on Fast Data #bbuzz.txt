Title: Berlin Buzzwords 2016: Todd Lipcon - Apache Kudu (incubating): Fast Analytics on Fast Data #bbuzz
Publication date: 2016-06-12
Playlist: Berlin Buzzwords 2016 #bbuzz
Description: 
	Over the past several years, the Hadoop ecosystem has made great strides in its real-time access capabilities, narrowing the gap compared to traditional database technologies. With systems such as Impala and Spark, analysts can now run complex queries or jobs over large datasets within a matter of seconds. With systems such as Apache HBase and Apache Phoenix, applications can achieve millisecond-scale random access to arbitrarily-sized datasets.

Despite these advances, some important gaps remain that prevent many applications from transitioning to Hadoop-based architectures. Users are often caught between a rock and a hard place: columnar formats such as Apache Parquet offer extremely fast scan rates for analytics, but little to no ability for real-time modification or row-by-row indexed access. Online systems such as HBase offer very fast random access, but scan rates that are too slow for large scale data warehousing workloads.

This talk will investigate the trade-offs between real-time transactional access and fast analytic performance from the perspective of storage engine internals. It will also describe Apache Kudu (incubating), a new addition to the open source Hadoop ecosystem that fills the gap described above, complementing HDFS and HBase to provide a new option to achieve fast scans and fast random access from a single API.

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              thank you yeah please I hope you had                               coffee i'm talking about a database                               today it's a little bit boring please                               don't fall asleep so a reminder this is                               an open source project that i'm talking                               about today please tweet about it at                               apache kudu is our our user you can feel                               free to ask any questions you don't get                               a chance to ask today via twitter or                                emailing me I'm taught at Cloudera calm                                so to quickly set the stage how many                                people here are Hadoop users any part of                                the Hadoop ecosystem spark HDFS etc ok a                                third a half maybe so for those of you                                who aren't Hadoop users this is kind of                                the most marketing oriented slide i have                                here with the Hadoop ecosystem so Hadoop                                is not just one open source project the                                way people usually talk about it it's                                actually an ecosystem there is of course                                one official Apache Hadoop people                                usually mean had do plus things like                                spark and hive and flume and scoop and                                uzi and all these different Apache                                projects you might have heard bandied                                about and creative is a new part of that                                ecosystem and specifically kudu is on                                the bottom layer of the ecosystem it's a                                storage engine that's meant to work with                                the other components that already exist                                so this is just to clear up some                                confusion kudu itself doesn't do any                                sequel it doesn't do any stream                                processing it doesn't do any job                                management all that is the existing                                Hadoop components and cooed is just the                                storage engine and we'll talk about that                                in upcoming slides so before I talk too                                much about what we do is I wanted to set                                the stage for why we decided to build                                kudu starting about three and a half                                almost four years ago so to introduce                                myself a little bit more i've been at                                Cloudera for about seven and a half                                years so i worked with a lot of                                customers and users in the community                                using the Hadoop ecosystem projects for                                different applications and I saw that                                basically there was two options that                                these customers and users could choose                                for storage I was specifically working                                on HDFS and HBase for most of those                                years and I took those two systems HDFS                                and HBase and plotted them on this graph                                I always joke this is an MBA graph not                                an engineering graph because there's no                                numbers it's just kind of these                                relaxes that we're placing icons on and                                on the top left here we have Hadoop HDFS                                HDFS is kind of the traditional original                                high deep storage manager based on                                Google's gfs paper and it may be a                                little small to read but the y-axis here                                is performance for analytics so                                specifically I mean they're the ability                                to run a job or a query over a large                                amount of data like looking at a day's                                worth or a month or a year's worth of                                data to extract some kind of insights do                                aggregations build a search index build                                a machine learning model something where                                you're looking at aggregate data sets                                and Hadoop is very very fast at that                                it's very high up on this y-axis and                                that's all great until you realize that                                you occasionally need to do something                                that's more like random access you want                                to update an individual record or insert                                one record at a time or look look up one                                record very quickly low latency and                                that's where HBase came along so about                                two or three years after HDFS started to                                gain some steam HBase came out and HBase                                is very good at the opposite                                characteristics it's super fast for a                                low latency random access you can look                                up right read an individual row in a                                couple of milliseconds but people who                                chose HBase found that they didn't                                really get the analytics they were used                                to on HDFS so it's pretty far right for                                this online random access characteristic                                but not very high up for the analytic                                use case and talking to a lot of our                                early users in a Hadoop ecosystem I                                found there is this big gap in the                                middle or people had use cases that                                weren't solidly analytics or solidly                                random access but actually had some of                                 both characteristics maybe they're                                 mostly doing analytics but need to do                                 updates or they're mostly doing random                                 reads and writes but they want to run                                 some end of day analytics or build a                                 search index on that online data set                                 there's this big empty spot in the                                 middle so essentially identified this                                 big gap and I went and pitched to our                                 co-founders and said hey we should build                                 a new thing called kudu that fits in                                 this gap and kuta has two main design                                 goals one is to be high throughput                                 meaning high up on the y-axis for                                 analytics and the other is to maintain                                 that low latency I like HBase you'll                                 notice though we're not you know up into                                 the right of both we're not trying to be                                 a                                 hdfs we're not trying to be a better                                 HBase in fact our goal is a little bit                                 less lofty we don't want to be more than                                 twice as slow as HDFS and we just want                                 to be in that same ballpark as HBase so                                 if you have an application that's really                                 great on HDFS great please use it I've                                 got a lot of friends teammates who                                 continue to work on it we're continuing                                 to invest in it and same with HBase I                                 said right next the HBase team and we're                                 continuing to work on that software but                                 if you have an application that needs                                 some aspects of both this might be a                                 kind of happy medium Goldilocks kind of                                 system to make your life easier so the                                 other main goal and this was actually                                 touched on by eric evans who just                                 presented in this room prior the                                 changing Hardware landscape machines are                                 changing quite a bit compared to the                                 early                                                                 big table papers were written so Google                                 published the GFS paper in                                              table in                                                                were quoting in these papers in the                                 benchmarks are kind of quaint now you're                                 talking maybe two or four cores early                                 like pentium                                                           the biggest trend though that affects                                 storage is the actual storage media so                                 from the early                                                      couple of years we saw a big shift from                                 spinning disks onto solid state disks                                 specifically for random access certainly                                 for analytics the spinning disks are                                 still pretty good and most cost                                 effective but if you're measuring on                                 price / io performance the SSDs are a                                 lot faster for the money so the price is                                 continuing to drop and the throughput                                 both in terms of random ayos and                                 sequential is just continuing to go up                                 and up so with NAND flash kind of the                                 current generation of flash that most                                 people use today we're up to several                                 hundred thousand iOS per second on a                                 device you can get for only a couple                                 thousand dollars for a terabyte so this                                 is maybe a third of a cost of a server                                 machine and it gives you this really                                 terrific i/o performance completely                                 different than the spinning disks we had                                 before and the next generation was                                 recently announced about eight months                                 ago by intel called                                                spelled with an X so you know it's                                 really cool and this has another couple                                 orders of magnitude improvement                                 and flash so it's about a thousand times                                 faster for random access compared to                                 nand flash but it's cheaper than ram so                                 this is a cool technology you take this                                 this storage media they put it on a dim                                 so it's actually it looks like a memory                                 stick and you stick it in your Xeon                                 machine and it basically looks like                                 memory to everything except that those                                 rights to memory are actually persistent                                 across the power off and reboot so it's                                 completely changes the ways we want to                                 think about storage and think about                                 writing software on top of that storage                                 so the other trend is a little bit less                                 glamorous but it's just the steady march                                 of Moore's Law Ram is getting bigger and                                 bigger I think the big table paper                                 quotes machines with                                                     i started at Cloudera                                                    end now                                                                amazon you can now get a machine i think                                 a month or two ago they announced a two                                 terabyte machine on amazon so these are                                 totally commodity machines nat with tera                                 bytes of ram and that trend is just                                 going to keep on marching forward so the                                 takeaway here is that the actual                                 bottleneck for these database systems is                                 the CPU it's not actually the discs                                 anymore so in the big table gfs papers                                 which sort of underlie HDFS and HBase                                 they talk a lot about all these tricks                                 to sequential eyes all of the rights and                                 avoid seeks especially on rights in                                 order to get low latency that stuff not                                 as important anymore compared to                                 actually optimizing for CPU we can do                                    disc seeks in a couple of microseconds                                 if your disk is actually this persistent                                 memory technology so we can redesign the                                 way we do storage and that's why we                                 decided with kudu to start from the                                 ground up with something that was                                 designed for this modern hardware some                                 of the specific things we do is written                                 entirely in C++ we've been collaborating                                 with intel on support for this                                 persistent memory technology since even                                 before it was announced we have a                                 special relationship with them and we're                                 already ready to be running on these                                 machines with tons of ram and superfast                                 storage so that's sort of the why why                                 did we go and build this new project                                 when there are already so many projects                                 existing for storage so now the what in                                 a sentence guddu is scalable and fast                                 tabular storage so we'll break that down                                 scalable it means it should scale like                                 the rest of the hoodie because                                 system so far we've only tested                                     nodes in my experience most people are                                 probably under that scale some are                                 higher and we're planning to continue to                                 test up to larger scale it was fine when                                 we ran on                                                                that's why we stopped we stopped because                                 that's the size cluster that they gave                                 me to test on we're planning on                                 deploying on a much larger cluster in                                 the coming months fast we just talked                                 about how fast the storages crew should                                 be able to drive that storage to the                                 same speed the storage is physically                                 capable of so that means on a you know                                                                                                     millions of random reads or writes per                                 second in fact we benchmarked a single                                 node kudiye cluster doing                                                reads a second that's an in RAM data set                                 of course but that's not too too strange                                 to think about in RAM data sets when you                                 have a terabyte of ram per machine and                                 for sequential true but for analytics we                                 expect multiple gigabytes a second per                                 node so a single flash device can give                                 you three gigs a second we don't want to                                 be cpu-bound we need to be able to take                                 all of that bandwidth from the disk                                 shove it through the CPU as efficiently                                 as possible and the actual data model of                                 kudu is tabular this is really important                                 kudu is not a file system we're not                                 trying to have a file system like API                                 and it's also not a key value store like                                 HBase or Cassandra it has tables and                                 tables are the only construct that are                                 stored in kudu and those tables have                                 finite numbers of columns very much like                                 post grass my sequel Oracle you create a                                 table you say here my thirty fifty                                 hundred columns here are the types here                                 are the names and that's that there's no                                 kind of freeform creating columns on the                                 fly and this makes it much easier for                                 users to use you can hook this into a                                 sequel engine and start querying these                                 tables immediately there's no data                                 mapping step or any kind of complicated                                 bite wise serialization and encoding of                                 course big data is changing frequently                                 and you need to be able to add new                                 columns as your data evolves so we                                 designed alter table to be very fast in                                 fact it's constant time to do alter                                 table regardless how large your data set                                 is and lastly we realize that sequel                                 data models are useful but a lot of                                 people prefer programming                                 with a no sequel style API especially                                 ingesting data from a streaming system                                 or doing random reads and writes so we                                 have Java C++ and Python API s you can                                 use those to directly contact kudu and                                 get millisecond scale random read and                                 write of course your analysts probably                                 want to use something like sequel or                                 spark so we have integrations with                                 things like sparks equal Impala and                                 drill and there's a presto integration                                 also kind of in the works in order to                                 query query data from those systems so                                 let's talk about the use cases and                                 architectures where kudu can make sense                                 cooter is designed for use cases that                                 combine as we saw earlier the random                                 access and the analytic performance so                                 you have some accesses that are these                                 quick look ups and some that are these                                 large scans so to take one specific                                 example we'll look at financial time                                 series so I've talked to a lot of                                 customers of ours in New York and London                                 and other financial centres and they                                 have these data sets that are streaming                                 in from market data providers so they                                 get you know thousands of UDP messages                                 per second with this little binary                                 format called fix I don't know if                                 anybody here is works in finance and                                 they take this data and need to ingest                                 it and they need to ingest it at real                                 time they don't want to wait until some                                 batch load the next day to start doing                                 their analytics because they're trying                                 to build applications like fraud                                 detection and fraud prevention and if                                 you wait until the next day's bulk load                                 to see that you had fraud yesterday you                                 kind of missed the opportunity to do                                 anything about it and even more                                 important if you're trying to do actual                                 trading based on this data you need to                                 ingest the data run analytics                                 immediately have results so the workload                                 has these inserts coming from these                                 platforms that are delivering data they                                 have scans in order to build models and                                 look up individual information and if                                 you go want to see the history of one                                 particular stock or equity then you need                                 a low-latency lookup of that particular                                 time range for that particular equity                                 include you can do all of these things                                 in one system I would very good                                 performance and the really surprising                                 thing that I found talking to these                                 users is that they also have updates and                                 deletes you wouldn't really guess this                                 but with these market data providers                                 sometimes they call you up literally on                                 the phone and say we sent you this bad                                 data yesterday can you actually go                                 delete all the records we sent in this                                 time range because they were just                                 incorrect or we said it was in euros but                                 actually that was US dollars so go                                 update all those values and if you have                                 a system that only supports append only                                 you can't really handle these things                                 very well and you end up with a lot of                                 work to go fix these issues so people do                                 build these kind of systems today on                                 Hadoop and this is an architecture                                 diagram kind of boils down but a lot of                                 our users are doing in the Hadoop                                 ecosystem so you see the top left we                                 have incoming data from some kind of a                                 messaging system maybe that's via flew                                 more scribe or Kafka or flink or UDP                                 packets anything that's kind of                                 streaming data in ingest and typically                                 when this data arrives they write it                                 into a system something like HBase it                                 could also be cassandra or you know                                 another system that supports streaming                                 ingest and that's because you can't do                                 individual row by row writes into                                 something like HDFS but if you tried to                                 run the analytics directly on HBase                                 you'd find the performance isn't very                                 good it works but it's maybe                                          times slower than running that same job                                 against HDFS so they have these cron                                 jobs which look for a threshold like                                 once an hour or after a certain number                                 of gigabytes have been accumulated and                                 they dumped HBase into Parque files                                 again you could choose different                                 technologies it might be                                              something like that but basically some                                 sort of efficient columnar format on                                 HDFS and you've got these files now once                                 every hour and you get another cron job                                 which looks for these new files to show                                 up and run some kind of a metadata                                 operation to add them into your hive                                 meta store or move them into the right                                 directory so they can be queried maybe                                 creating a new partition for that hour                                 so you've all these cron jobs working                                 and eventually you get your data on HDFS                                 in the fastest format and you can run                                 reporting with Impala or sparks equal or                                 presto whatever your reporting tool of                                 choice is so the problems here are many                                 for one it took about an hour between                                 data arriving and actually being                                 available for analytics it's not very                                 real time you can miss a lot of business                                 value in an hour in second it doesn't                                 really handle updates at all or deletes                                 if you need to update or delete some                                 data that's already in the historic data                                 store you have to take out an entire                                 partition apply those updates and then                                 do this swapping mechanism where you try                                 to move the new partition with the                                 updates and remove the old partition and                                 you hope that a query that's running at                                 the same time doesn't fail because you                                 deleted a file or move the file under it                                 it's a lot of complexity and talking to                                 a lot of users who built this kind of                                 system they spend a lot of engineering                                 time just managing keeping all these                                 files moving around and worrying about                                 partitioning and the other problem is                                 the thing called compaction where will                                 accumulate maybe thousands or hundreds                                 of thousands of these little files in a                                 new file every five minutes or ten                                 minutes and when we run a query on a                                 hundred thousand files the performance                                 actually isn't very good these query                                 engines prefer to have a smaller number                                 of large files versus a large number of                                 small files so we have these other jobs                                 called compaction switch take a bunch of                                 small files merge them together create a                                 big file basically I've talked too long                                 about this slide already and that's how                                 people feel when they implement these                                 systems it's a lot of complexity so the                                 idea with kudu is it just simplifies                                 everything you have one system you can                                 insert row by row you can run your                                 reporting on that system you don't need                                 to worry about all these file management                                 as metadata these two different storage                                 systems that have different                                 characteristics it's all built in into                                 one system updates just apply an update                                 you don't need to do something different                                 so this really simplifies everything                                 we're not trying to say that your query                                 at the end is going to run faster it                                 probably won't probably run around the                                 same speed but it makes your life much                                 much easier and that's the main selling                                 point of kudu so I want to show a real                                 use case I don't know if people here are                                 familiar with Xiaomi they make this                                 wonderful mobile phone here that I'm                                 using it's a smartphone very very                                 popular in China and                                 spanning to the rest of the world now so                                 the world's fourth largest smartphone                                 maker they've got I think about a                                 billion customers now and they provide a                                 lot of online apps if you use apples                                 like iCloud they have a message                                 messaging photo sharing social things                                 all these different features and all of                                 these online services are gathering                                 tracing information about how people are                                 using the service the performance                                 they're seeing any errors they're                                 hitting the standard kind of web service                                 monitoring you might do they're also                                 gathering a lot of telemetry from the                                 phones themselves so if an app crashes                                 it will report back this crash trace                                 including recent log messages the                                 phone's device information etc so this                                 is very high throughput they've got a                                 billion phones out there and they're all                                 collecting data all the time so it's                                 about                                                                growing fast and they need to take this                                 data and do two things with it one they                                 need to be able to query the data to see                                 trends and understand how people are                                 using the software and two they need to                                 be able to respond quickly if they have                                 new bugs so when they're releasing a new                                 version of their Android fork and they                                 put it out to a bunch of phones maybe                                                                                                          the crash rate is elevating they can do                                 analytics really quickly to see oh this                                 particular model of phone using this                                 particular version of the mail                                 application is crashing here is a                                 particular crash log and then the                                 developers want to respond quickly look                                 up that phone information look up the                                 crash log get the symbols for the the                                 application that crashed and do this                                 interactive debugging so they need very                                 low latency lookups of this data so they                                 can troubleshoot and fix the problem or                                 roll back the OS upgrade so they built                                 this application before it's not a new                                 application for them and they built it                                 using a couple of the older technologies                                 so they're using scribe writing to HDFS                                 in the sequence file format which is                                 this append only format and they're also                                 writing to hbase so they can get some                                 data more quickly they have cron jobs                                 running etl using hive MapReduce and                                 spark once an hour and that's resulting                                 in parque files which they can then                                 query via impala so the problem here is                                 the                                 see new data arrives and it takes at                                 least an hour before they can even see                                 it for analysis and that means they                                 can't respond quickly to problems when                                 problems are happening online and when                                 something starts to go wrong in this                                 complex pipeline they've got two                                 different storage systems three or four                                 different file formats different ETL                                 engines it's really complicated so                                 things go wrong pretty often it can take                                 up to a day with this pipeline is shut                                 down before they can get that data so                                 with ku the pipeline is a little bit                                 different the only storage system here                                 now is kudu they're taking the data                                 using scribing Kostka as a sort of                                 buffer and then using storm which reads                                 from Kafka does a little bit of online                                 etl data processing lunging into a                                 tabular format and then writes to kudu                                 and some other applications like the                                 back end web services can actually write                                 directly from the data source into kudu                                 using the Java API so the latency here                                 is between about                                                      case and worst case maybe                                               something is going wrong on Kafka it has                                 to buffer for a couple seconds so this                                 et al is all real time and as soon as                                 the data lands in kudu their users can                                 query it with Impala or use the API                                 directly to look up individual records                                 so essentially their their pipelines a                                 lot simpler and the latency is a lot                                 lower alright getting it to how it works                                 we'll start from the high level how guru                                 looks as a distributed system so like                                 other systems we have this concept of                                 horizontal partitioning so every table                                 which might be a trillion rose is                                 chunked into smaller chunks called                                 tablets and we can use either range or                                 hash partitioning or a combination                                 thereof so this is best described with                                 an example if you have a time series you                                 might have host metric and time stamp                                 and you want to distribute these evenly                                 across the cluster so you can just say                                 distribute by hash of time stamp into                                                                                                    computes the hash code does the correct                                 encoding to spread the data if you're                                 familiar with something like HBase                                 you're probably doing this by hand                                 there's techniques called key salting                                 we'll spend a lot of effort building                                 their row keys to achieve this so with                                 cootie we just built it in so once we                                 run our partitioning we split the table                                 up into tablets each tablet then has to                                 be replicated because one of the servers                                 the tablet server is hosting those                                 tablets could crash so we make three                                 replicas by default could be at five or                                 seven if you need more at fault                                 tolerance and we use raft consensus                                 which is an algorithm for building                                 agreement on the state of the database                                 between three or five servers and this                                 gives us very very fast fault tolerance                                 if a server crashes we have two more                                 replicas ready to take over typically                                 four and a half seconds it's three                                 heartbeat periods or heartby is                                     seconds and these tablets servers then                                 host the data directly on their local                                 hard drives because we're doing                                 replication with raft we don't need a                                 distributed file system underneath we                                 can just install directly kudu on your                                 servers there's no other dependencies                                 there's no zookeeper there's no HDFS etc                                 so it's very easy to install and run the                                 other component beyond the tablet                                 servers is the master so the master acts                                 as a tablet directory this is basically                                 the phone book for the cluster when you                                 need to read or write data the client                                 needs to be able to find where those                                 tablets are located and it goes to the                                 master to find out that information it                                 also keeps the metadata about the actual                                 tables that exist and their schemas and                                 that other you know small pieces of                                 information about the cluster and lastly                                 it enforces policies about the cluster                                 having this centralized point for policy                                 enforcement makes it a lot easier to                                 reason about what's happening the                                 previous presenter mentioned some issues                                 around throttling rear application after                                 a crash things like that from a                                 centralized component are much easier to                                 understand versus a fully peer-to-peer                                 system where there can be emergent                                 behavior which is more difficult to                                 understand so because this only does                                 these simple metadata operations it's                                 very very fast all the data is so small                                 we can keep it cashed in memory and                                 every lookup is essentially receive an                                 RPC look up in a C++ map respond the                                 result so the                                                     sub-millisecond in a                                                  test                                                                 everything is on the order of a few                                 microseconds to service these requests                                 so this is easier to understand in a                                 form of a diagram so on the top left we                                 have the client the client is the java                                 jar or the c++ library shared object or                                 the python module running on some                                 machine and the client needs to read or                                 write some data so the first thing it                                 does is it sends an RPC to the master so                                 the master is logically one unit but                                 actually is replicated as well also                                 using raft so it contacts the master and                                 says hey I'd like to write the row                                 called T live con maybe it's a user's                                 table and I'm the user being written and                                 the master looks that information up in                                 memory cached and respond saying it's                                 part of tablet too and tablet                                         server Z Y and X here's their host names                                 and the information to connect to them                                 the client then caches that information                                 in this little cloud on the top left                                 called the meta cash this is so that                                 doesn't have to keep going back and                                 forth every time to the master for every                                 row it reads or writes it can keep this                                 information automatically and validate                                 if something becomes incorrect the                                 client then sends the update request                                 directly to the tablet server hosting                                 that data so the master you'll notice is                                 not on the data path at all and doesn't                                 form any kind of bottleneck and because                                 the master is replicated it also doesn't                                 form any kind of availability issue so                                 now I want to transition and talk more                                 about what's happening inside the tablet                                 server on each node specifically about                                 how we store data on disk for fast                                 analytics so here we have a table this                                 is an example table for the Twitter fire                                 hose and we're just showing four columns                                 here at in reality that's probably                                                                                                             that's ever been written and we have the                                 tweet ID the username the creation                                 timestamp and the actual text a                                 traditional database likes a post grads                                 from my sequel is called a roast or                                 and that means that on disk the data is                                 actually laid out in the same order that                                 you would read it so starting with the                                 top row you'll see all the columns of                                 that top row and then following that in                                 the file you see the next row and so on                                 so imagine a TSV file on disk stored in                                 the same order might be binary but                                 essentially the same order sts-v so a                                 column store is different a column store                                 actually creates a separate area on disk                                 you can think of a separate file for                                 each of the columns so this tweet ID is                                 stored with all the tweet IDs together                                 and similarly the other columns so if                                 you look at this naively you think this                                 is probably a dumb idea when we go read                                 one of those tweets we don't just read                                 one file we have to go read four                                 different files do for disk seeks to go                                 find all the information stitch them                                 back together spend some work to do that                                 and in return the row and that's true if                                 you look back in the academic literature                                 from                                                                  thought it was crazy to do random access                                 on a column store and in fact if you try                                 it on a system like vertica it's not                                 very good these older systems weren't                                 built for this but as I talked about                                 earlier the storage is getting so fast                                 and the RAM is getting so large that                                 these four discs seeks to go read those                                 individual pieces and put them back                                 together no longer cost very much that's                                 actually pretty much trivial to do if                                 it's in RAM for cache misses not a big                                 deal so that's why it's not so bad to do                                 random access but why is it actually                                 good to organize your data like this the                                 first thing to notice is the sizes of                                 the columns so the tweet IDs and the                                 creation timestamps are integers maybe                                 there are                                                              is text may be a little bit larger                                   gigabytes and then the actual tweet is                                 likely to be orders of magnitude larger                                 than the other data if you                                               and imagine you've got an analyst who                                 comes and runs a very simple query                                 select count star where username equals                                 some username the hacker news bot think                                 about how you need to                                 implement this query if you're you know                                 Java program we're giving these data                                 sets you wouldn't open and read all the                                 files you just read the username file                                 scan through find the matching usernames                                 and count them and that's what cooter                                 does as well so you're reading two                                 gigabytes of data in a row store where                                 we've got all these columns mixed                                 together this would be really slow                                 because we have to skip over all those                                 useless bites from the tweets we'd be                                 reading                                                                serving this query a hundred times                                 faster by reading a hundred times less                                 data and this is very very common for                                 analytic workloads most analytic queries                                 access a subset of the columns so you'll                                 notice here I said it's a hundred times                                 faster I can make this arbitrarily more                                 faster so if you ever see a benchmark                                 between a column store like kudu and a                                 roast or like my sequel you should just                                 realize you're comparing apples and                                 oranges I can be a million times faster                                 than my sequel by just adding a bunch of                                 columns to my sequel and querying one of                                 them so you have to be very careful to                                 understand what workload is being                                 benchmarked and what work that you have                                 so the other benefit of the columns is                                 compression so take this example                                 creation timestamp column and maybe a                                 little small to read for those of you in                                 the back but the first five or six                                 digits of these timestamps are all the                                 same                                                                  common in real datasets you're inserting                                 data essentially as the data arrives the                                 timestamps are clustered together with                                 nearby rose so CUDA you can notice this                                 information and instead of storing the                                 column with the the full                                        timestamps well actually compute the                                 difference between each successive                                 timestamp and the differences are much                                 much smaller than the original                                 timestamps and this difference                                 calculation we can do in a single CPU                                 operation                                                               there's an instruction set called AVX in                                 modern CPUs that allows you to load a                                 vector register with a bunch of integers                                 and then you shift that by one and do a                                 subtraction so in two cycles we've done                                 eight subtractions these are essentially                                 free to implement on modern CPUs                                 and we get                                                              much much faster than using something                                 like lzo or gzip and you get great                                 compression so a lot of compression a                                 lot of columns can compress down to only                                 a few bits per row things like time                                 series values time stamps things that                                 don't change much and also low                                 cardinality strings like the hostname                                 example for a time series store you may                                 be only have a hundred hosts but the                                 strings are                                                              hosts we could code that in a dictionary                                 and use seven bits and kruti we'll                                 figure that out and do that and give you                                 very very good compression for free it's                                 much easier to do in this columnar form                                 compared to a row form so it saves you a                                 lot of space and as we saw in that                                 previous example saving space means less                                 data is coming off of your devices less                                 data is flowing through your CPU caches                                 and things just run much faster so                                 usually people will use guddu API                                 directly for so the online random access                                 but it's important to integrate with                                 other systems for analytics so I won't                                 read all the syntax out to you you can                                 go find these slides later but we have                                 spark data source integration so you can                                 use data frames are dd's sparks equal                                 really easily we've integrated with                                 Impala so we have new sequel syntax for                                 insert update delete creating kudu                                 tables drop included tables etc and                                 there's a few other sequel engines with                                 work in progress integration as well and                                 if anybody is still writing new jobs in                                 MapReduce it's a little bit legacy at                                 this point but we do have MapReduce                                 integration as well we test it heavily                                 we do a lot of stress testing if                                 directness testing using MapReduce all                                 right so it's a sort of wrap up here I                                 wanted to show a couple of performance                                 benchmarks to back up my claim that we                                 do is actually fast so the first                                 benchmark is t PCH G BCH is an analytics                                 benchmark put together by this                                 transaction processing consortium                                 basically a bunch of people get together                                 in a room and come up with a bunch of                                 queries that they think their                                 competitors will do poorly on and                                 they'll do well on and then they publish                                 these as a big PDF                                 go pay someone to audit so this is a                                 essentially                                                             a standardized data set modeled around                                 data warehousing and this is an example                                 query the queries are not trivial they                                 have things like group by order by                                 aggregations etc and we ran this                                 benchmark with a hundred gigabytes of                                 generated data on a relatively large                                 cluster                                                              you're if you're quick this fits and RAM                                 on this data set this data set fits in                                 RAM on this cluster so you might think                                 I'm cheating but as we said earlier the                                 storage is actually getting faster and                                 faster to approach the speed of RAM so                                 this is the benchmark that will matter                                 in the future for your interactive                                 analytics it's going to be on fast                                 storage so we ran this benchmark                                 comparing Impala on kudu versus Impala                                 on parque by using Impala                                               little bit old at this point but will                                 rerun it soon and these are the results                                 we found that kudu actually was                                 thirty-one percent faster than parque                                 for this workload we didn't expect that                                 we actually double check the results to                                 make sure we didn't switch the two                                 columns but it actually is the case for                                 this benchmark kudu was faster for a                                 much larger data set where it doesn't                                 fit in RAM guddu was about forty or                                 fifty percent slower but the the main                                 idea here is a kudu is fast for                                 analytics it's not a big trade-off to                                 choose kudu vs barca vs our see file for                                 analytics will give you a good speed                                 comparing vs phoenix i don't know if                                 anybody here uses phoenix it's a sequel                                 engine on each base and basically kudu                                 impala blows away phoenix on HBase                                 Phoenix is great for running random                                 access workloads but for analytics it's                                 between                                                                                                                                      queries so if you want to use Phoenix as                                 a front end for random access HBase                                 great for analytics you should probably                                 check out kudiye and the last one is a                                 benchmark that we lose just to show that                                 i'm not completely marketing this thing                                 running why csb which is a purely random                                 access workload we were between route                                 neck and neck for uniform be workload                                 and some cases up to four times slower                                 than a                                 space we've closed some of this gap in                                 more recent versions but basically if                                 you have purely no sequel random access                                 only workload probably better to use                                 something like HBase or Cassandra one of                                 these more traditional no sequel stores                                 alright so if you're convinced and you'd                                 like to try out kudu we're an open                                 source beta the latest release is being                                 voted upon right now or part of the                                 apache foundation so it's purely a                                 community project I of course worked for                                 a vendor but we are treating this like a                                 pure open source project and a lot of                                 people in the community contributing and                                 using an early users running in                                 production you can go download find out                                 all the information I get kuduo email                                 our mailing list or join our slack if                                 you want to chat with us we're always on                                 the slack happy to chat with anybody                                 about use cases or getting started and                                 if you want to help with developer                                 you'll find these slides online you can                                 click the links we're happy to have more                                 developers working with us so I think we                                 have three minutes left for questions                                 that's all right yep thanks ok thank you                                 okay so sorry this might be a knave                                 question that you talked about random                                 access so do you have indexes on your                                 coolant in order to be able to identify                                 a specific rules yes so there every                                 table has a primary key I which can be a                                 composite primary key so the example I                                 gave with host metric time stamp makes                                 up the primary key and that's indexed                                 the other columns are not indexed but                                 you get the single composite key primary                                 key index okay and Marissa question was                                 built you talked about the fact that                                 packet would be faster on larger data                                 for an analytical use case in which part                                 of the design of kuru makes it slightly                                 slow in that case so the two big                                 differences that I've identified so far                                 is park a uses much larger block sizes                                 the default is                                                         are red whereas kudu by default uses                                     kilobytes so on spinning disks in                                 particular the small iOS cause a lot                                 more seeks so it's slower the other                                 major difference is specifically with                                 impalas Park a reader they have this i/o                                 manager io scheduler which keeps all the                                 disks like a hundred percent busy and                                 really saturate CIO and kudu doesn't                                 have the kind of advanced read-ahead                                 scheduler that Impala does for parque so                                 i think with some work we could probably                                 close that gap significantly but at the                                 end of the day parque is simpler it                                 doesn't have index as it doesn't have                                 all these features for random access so                                 it probably will be a little bit faster                                 for that use case thank you                                 thanks for the talk there was somebody                                 mentioning druid in an earlier talk and                                 from from what you've been saying about                                 kudu a lot of this sounds like this                                 there's there's overlap boat and the I                                 mean both in the target use case I guess                                 and also in the techniques internally                                 like the like the compression techniques                                 look like they're very simple and so on                                 what you what you would you say are the                                  biggest differences and we're with the                                  sweet spots are two different systems                                  line yes I think I think druids                                  interesting and really great project for                                  what it's good at I think we do have                                  some different use cases so druid for                                  example doesn't really support these                                  random lookups they're very much about                                  aggregation pre aggregating along                                  different dimensions in order to make                                  the analytics fast but you can't go look                                  up an individual row in one millisecond                                  it's not really designed for that or do                                                                                                            one thing it does well is it integrates                                  very well with s                                                          which kudu doesn't do another big                                  difference is that druid has sort of                                  combined the storage engine and the                                  query execution you can't really run                                  like a arbitrary Impala query with joins                                  and order buys and group buys against                                  drew it there's a sort of set of things                                  that droid can execute whereas with kudu                                  because we're only a storage engine we                                  can run whatever you want and we have                                  the raw data so you could build                                  clustering you could do anything you can                                  do a spark you can do on kudu okay thank                                  you unfortunately we need to give time                                  for a break and for people to change                                  rooms but if you want to continue a                                  discussion you are more than welcome to                                  do this in the corridor thank you
YouTube URL: https://www.youtube.com/watch?v=z3rApSRXNMw


