Title: Berlin Buzzwords 2016: Neha Narkhede - Application development and data in the emerging world ...
Publication date: 2016-06-11
Playlist: Berlin Buzzwords 2016 #bbuzz
Description: 
	For a long time, a substantial portion of data processing that companies did ran as big batch jobs -- CSV files dumped out of databases, log files collected at the end of the day etc. But businesses operate in real-time and the software they run is catching up. 

Rather than processing data only at the end of the day, why not react to it continuously as the data arrives? This is the emerging world of stream processing. But stream processing only becomes possible when the fundamental data capture is done in a streaming fashion; after all, you canâ€™t process a daily batch of CSV dumps as a stream. 

This shift towards stream processing has driven the popularity of Apache Kafka. Making all the organization's data is available centrally as free-flowing streams enables a company's business logic to be represented as stream processing operations. Essentially, applications are stream processors in this new world of stream processing.

In her keynote, she will explain how the fundamental nature of application development will change as stream processing goes mainstream.

Read more:
https://2016.berlinbuzzwords.de/session/application-development-and-data-emerging-world-stream-processing

About Neha Narkhede:
https://2016.berlinbuzzwords.de/users/neha-narkhede

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              thank you for the kind introduction                               hello Berlin it's my first time here and                               I think this is a great conference thank                               you for spending your morning at this                               talk today I'm going to talk about                               applications                               micro-services in this emerging world of                               stream processing as you can tell just                               from the schedule of Berlin buzzwords                                stream processing stream data is                                everywhere more than half the talks are                                about this subject and there is a lot                                happening in this world related to this                                topic                                in fact unbounded unordered large-scale                                data sets are increasingly common in                                day-to-day business whether it's web                                blogs mobile usage statistics data from                                sensor networks or the Internet of                                Things stream data is everywhere and                                there is a huge push towards you know                                getting faster results so the question                                is you know how do you move to this                                stream oriented architecture know for                                some time stream processing was thought                                of as or maybe he still thought of as a                                faster MapReduce layer may be                                appropriate for niche problems like                                faster machine learning or ir analytics                                but in my five years of working on Kafka                                and on stream processing I've learned                                that stream processing is in fact                                software that computes core functions in                                the business implements the business                                logic and is much more than computing                                analytics about your business and that                                insight has a lot of impact on how we                                develop applications and will completely                                change how we will develop stateful you                                know loosely coupled micro services in                                the days to come that is what I want to                                focus on in today's talk and tell you                                where Kafka and Kafka streams fits in                                this big picture so let's start off with                                you know what is stream processing we've                                we've used a couple of definitions for                                this term so let me you know use my own                                definition which is that it is a                                paradigm for processing unbounded                                datasets so if you focus on paradigms                                for programming you know there are many                                ways of splitting this pie                                but today I'm going to focus on a                                specific way which is the way an                                application gets its import and the way                                it produces its output and if you focus                                on this input-output dimension then                                there are essentially three paradigms                                for programming the first one is request                                response systems these are services that                                we are most familiar with you know they                                are synchronous they are tightly coupled                                the latency sensitive you send one input                                in and you wait for an output and the                                only way to scale these services is by                                deploying more instances of the service                                then on the other end of the spectrum                                you have you know bad systems here you                                send all your input in and you wait for                                systems to crunch all that data before                                you know a couple of hours later they                                send you all the output back essentially                                the big difference is that bad systems                                view data is being bounded in nature the                                difference from request response is that                                they're loosely coupled the latency                                expectation is very different now we                                arrive at you know stream processing                                here you send some inputs in and you get                                some outputs back and the definition of                                some is left to the program you can have                                you know it be either one input or all                                your inputs the output is available at                                variable times two                                either you have one output item for                                every input item or one output item for                                every n input item in a stream                                processing is conflated as a notion that                                is related to you know the concept of                                real time but really it is a                                generalization of the two extremes                                request response and badge the other                                thing about stream processing is that it                                is misunderstood as being something that                                is probably like you know transient or                                approximate or lossy now some systems                                might have been that way but that is a                                drawback of the way some systems are                                designed it isn't really an inherent                                 property of the stream processing                                 paradigm in fact stream processing                                 systems can be made to compute                                 accurate results much like bad systems                                 do as well as do that efficiently and                                 there is this you know misunderstanding                                 because while processing unbounded                                 datasets                                 there are various tricky trade-offs                                 involved and those trade-offs are                                 correctness latency and cost and you                                 know some systems specifically choose to                                 optimize or design along some of these                                 dimensions while giving other dimensions                                 up when what we really should be doing                                 is giving the user the flexibility to                                 pick the trade-offs that are right for                                 their application and this is really                                 important because one size does not fit                                 all you know some applications are                                 really care about correctness like                                 billing while some may not like clock                                 processing some might care about latency                                 like alerting while some may not like                                 ETL while some applications actually may                                 not be okay with fully optimizing you                                 know the cost associated with fully                                 optimizing along the other two                                 dimensions so in this talk you know I                                 want to focus on stream processing and                                 its impact on mainstream application                                 development how you know the entire                                 company's business logic can be                                 architected as loosely coupled stateful                                 stream processors and in this talk I'm                                 going to you know walk you through this                                 example from a brick and mortar retailer                                 you know for retailers there are many                                 inputs that are interesting but there                                 are really two inputs that are really                                 important and that is sales of things                                 and shipments of things and for healthy                                 retailer hopefully these two inputs are                                 never-ending so you can really represent                                 these as streams of sales and shipments                                 now it's worth noting that these two                                 inputs are useful for a large variety of                                 applications down streams you can say                                 you need to send it to Hadoop or the                                 warehouse for analytics you send it to                                 some kind of monitoring system for fraud                                 alerting or you index it on a search                                 system in addition to that you might                                 want to do some sort of incremental                                 processing now this may not be working                                 this way in in brick-and-mortar                                 retailers today                                 which is that the ability to make price                                 adjustments and inventory adjustments as                                 demand and sales for items keep changing                                 over time so taking a look at this                                 picture a little differently at the top                                 are things that fall in the what                                 happened category these are essentially                                 events that are interesting to this                                 retail business and everything else is a                                 response to that event as a response you                                 might send it to Hadoop or you might                                 index it on a search system or is a                                 response you might choose to process it                                 and trigger inventory adjustments and                                 this insight actually influences you                                 know some some choices for the                                 application architecture for this                                 example you know if you focus on the way                                 applications get their input and output                                 like we talked about there are a couple                                 of choices the first one is you don't                                 have to worry about that at all                                 if you architect this as one large                                 monolith then the set of problems we                                 will talk about are very different but                                 practically you know companies don't                                 write applications this way parts of                                 your business logic are written by                                 different teams they're managed                                 differently and because they are written                                 as separate services they in fact need                                 to communicate and exchange data and                                 oftentimes you know the most common                                 mechanism is of using synchronous                                 communication request response this                                 actually works but there are downsides                                 worth talking about first one is the                                 total amount of parallelism if you think                                 about it of processing an event through                                 all these downstream services is fairly                                 limited with synchronous communication                                 the same sales event has to get                                 processed one by one by every service                                 which means the total you know latency                                 of processing an event is the addition                                 of all the you know time taken by                                 downstream services instead of the time                                 taken by just the slowest service the                                 second downside is lack of resiliency                                 things are tightly coupled a slight                                 degradation or failure of a service                                 actually affects a whole bunch of                                 downstream services I will argue that we                                 need to make a fundamental shift towards                                 thing                                 thinking about you know asynchronous                                 communication and this is at the heart                                 of you know the reactive manifesto for                                 those of you who are following it is                                 this concept of an asynchronous boundary                                 between applications the goal is you                                 know with this asynchronous boundary you                                 in fact decouple the various components                                 of your architecture and if you do that                                 then you get a whole bunch of advantages                                 that you you know did not have prior to                                 this which is with decoupling first of                                 all all the downstream service services                                 can process events in parallel things                                 are a resilient you know failure of one                                 service is decoupled from failure of you                                 know affecting other services now in                                 addition you know in order to achieve                                 this asynchronous communication you                                 would likely buffer this data and for                                 buffering in order and processing in                                 order you might use a queue if you did                                 that then now you have some choices you                                 know if you pick a traditional queue                                 then you'd have to you know send an                                 event to every downstream queue one by                                 one the same sales event is processed by                                 several applications you might want to                                 send that event by one by one to all                                 those applications that is wasteful it                                 might work it's wasteful you know as as                                 one service fails it might fall out of                                 sync with other services so a natural                                 choice is to use a pub subsystem the                                 need is you write it once and you read                                 it multiple times this greatly                                 simplifies your architecture because now                                 you just have one pub sub Q versus                                 several ones this is essentially you                                 know what Kafka does you know Kafka is a                                 pub subsystem it is a very                                 high-performance pub sub system that                                 allows you to write at high throughput                                 in low latency okay so the second                                 component you know of this reactive                                 manifesto is being able to handle back                                 pressure if you buffer events then you                                 have to worry about what happens when a                                 service fails how how long do you have                                 to buffer data in order to not run out                                 of memory here I'd like to say that if                                 your queue was in fact persistent then                                 you don't have to worry                                 backpressure it is solved by definition                                 this is what Kafka does it is a                                 high-performance persistent queue it can                                 buffer way more data that can ever live                                 in an applications memory so in this                                 talk I want to you know walk you through                                 how to build a reactive retail business                                 you know an architecture that relies on                                 asynchronous communication it ends up                                 with you know applications that are                                 loosely coupled they're stateful they're                                 resilient                                 and easier to develop and also degrade                                 gracefully now part of the reactive                                 manifesto is reactor streams in case any                                 of you following that it is it is a                                 standard for streaming libraries that                                 you know that dictates that a streaming                                 library is one that processes unbounded                                 streams of events it asynchronously                                 passes passes messages around in                                 sequence and it definitely has the                                 ability to tackle back pressure so in                                 the remainder of the talk you will see                                 how Kafka and Kafka streams actually                                 does conform to this reactor streams                                 manifesto however has a possibly                                 different set of api's okay so back to                                 stream processing you know it is a set                                 of functions written on top of what                                 happened data which we've seen that I'll                                 just events over time what it noticed is                                 Kafka is sort of rising as the de facto                                 standard for storing stream data and so                                 stream processing boils down to writing                                 functions on top of Kafka events to                                 produce more Kafka events now over time                                 as in the last five years as Kafka has                                 been adopted across thousands of                                 companies worldwide I've noticed that                                 there are you know two broad approaches                                 that have emerged as stream processing                                 approaches on Kafka the first one is you                                 know what I'd call do-it-yourself stream                                 processing this is you take the Kafka                                 libraries and you decide to do the rest                                 of the stream processing yourself and if                                 you did that then there are you know a                                 couple of hard problems that you should                                 know that you'd have to deal with                                 basically even if you don't use Kafka                                 you have to deal with a lot of these                                 problems to be able to do stream                                 processing correctly and efficiently                                 and those are problems like you know                                 ensuring that data arrives in order as                                 well as gets processed in order ensuring                                 that you have the ability to scale both                                 data and processing out ensuring that                                 you have semantics even as machines fail                                 ensuring that you have the ability to                                 manage state to be able to support                                 stream processing operations like                                 windowed aggregates and joints having                                 the ability to reprocess data so when                                 you you know upgrade your applications                                 you can make sure that all your data are                                 conforms to the new logic of your                                 application and last but not the least                                 the notion of time is very important to                                 correctness in stream processing now                                 before we talk about you know how Kafka                                 and Kafka 'seems solves these problems                                 very differently let's first take a look                                 at the second approach which is quite                                 popular which is using one of the many                                 stream processing frameworks that have                                 emerged since Kafka has been adopted                                 widely you know spark has of streaming                                 module there is storm there Sam's are                                 which some of us built at LinkedIn there                                 is flink which is a really cool                                 technology upcoming built locally here                                 now these you know systems are pretty                                 cool there's a ton of innovation                                 happening in this space if you'd notice                                 that you know that one of the core                                 design traits of these systems is that                                 they're coming from the world of making                                 a faster MapReduce layer and as a result                                 of that there are some design traits                                 that these systems share at the core the                                 core concept is taking part of your                                 application which does stream processing                                 and it modeling that as a job that runs                                 on these a cluster of machines that runs                                 the stream processing framework and                                 because of that there are several                                 implications there is a specific way to                                 configure your job a specific way to                                 package and deploy your job on a central                                 cluster and these systems need to think                                 about resource management how do you                                 multiplex several jobs on a shared                                 cluster but these systems are actually                                 useful                                 for a large class of stream processing                                 problems basically any kind of iterative                                 processing where you know like machine                                 learning or graph processing or any kind                                 of long-running                                 you know analytics queries like C that                                 require a sequel interface actually run                                 really well on these systems the                                 problems you know that these systems aim                                 to solve they actually influence the                                 design the system design that these                                 systems have adopted and I think that                                 you know the only way to know if a                                 system design is applicable to the real                                 world is to build a system that conforms                                 to that design to deploy it you know for                                 real applications and then observe the                                 adoption when my time at LinkedIn I was                                 lucky enough to be part of a team that                                 built Apache Sam's which essentially one                                 of these systems and we built it and we                                 deployed it for in-house applications                                 and we also open sourced it and through                                 that we learned a lot of things the core                                 sort of lesson learned was that one of                                 the key misconceptions in stream                                 processing is that it will be used only                                 as a sort of a fast MapReduce layer when                                 in fact it turned out that stream                                 processing and the most compelling                                 applications that want to do any kind of                                 stream processing actually look                                 different from what a hive query or a                                 spark job looks like and in fact look a                                 lot like what an asynchronous micro                                 service might look like which is                                 different from just a fast version of                                 your batch job so then we arrived at                                 well you know what needs to be done to                                 make stream processing more applicable                                 or approachable to application                                 developers since application developers                                 develop these asynchronous micro                                 services and there are some of these                                 design traits have you know interesting                                 implications if you observe you know                                 they're just for config management and                                 packaging alone there are tons and tons                                 of tools there's docker and chef and                                 puppet and salt and whole list of custom                                 tools for resource management you have                                 mezzos and kubernetes and that whole                                 ecosystem is changing essentially                                 application developers are now                                 standardizing on any one of these                                 problems anytime soon so we can't really                                 you know dictate how you know                                 applications want to solve these                                 problems the second is you know for for                                 stream processing to be approachable to                                 applications it has to be lightweight it                                 has to be something that can be embedded                                 by just about any application and so we                                 arrived at this architecture decision of                                 making this as a library in fact it is                                 architected as a Kafka library and that                                 is done very much on purpose                                 Kafka oddly provides several primitives                                 that are core and fundamental to stream                                 processing the idea is in order to make                                 this a lightweight layer we have to                                 rebuild you know you have to build on                                 top of Kafka primitives not we are                                 collect and rebuild                                 you know what cough quality solves so                                 just to give you an idea you know some                                 of us some of you might have attended                                 the Kafka stream's library talks so this                                 might seem familiar but for those of you                                 who didn't you know streams is a java                                 library it has two interfaces it's                                 pretty simple one is a callback api and                                 the other reserve dsm this callback api                                 looks some you know pretty simple you                                 have a key in a value which is                                 essentially a message in Kafka you                                 implement the process API which is your                                 logic as part of the process API you                                 might send they went out and that's                                 basically it so in some sense this                                 encapsulates the entire scope of stream                                 processing you have input events you                                 write your processing logic you have                                 output events here's the code that you                                 actually write to you know create some                                 of these processors the config tells it                                 where to connect to Kafka you have your                                 source which tells it which topics to                                 ingest you have your processor class and                                 then you have a sync node to tell it                                 where to send that data and that's                                 basically it this is you know in fact                                 all that you write for your application                                 you can deploy it as one process or you                                 can deploy it as multiple processes or                                 you can put it in a docker image or run                                 it on mezzos Kafka streams is just a                                 library it doesn't dictate anywhere                                 anything about how you should package                                 and deploy your application okay so the                                 second API is actually                                 a high-level DSL you know it provides a                                 sort of a functional API is like filter                                 and map and join and produce and so on                                 whether you use the low-level API or the                                 DSL you know or even any other stream                                 processing system oftentimes the                                 operators form a topology where the                                 first and the last knowns are possibly                                 the source and the sink nodes and then                                 the remaining our operators are                                 dependent on each other so many times                                 the question is like how do you map this                                 logical topology to physical processes                                 in Kafka streams it's very simple you                                 just write your code in you deploy it it                                 does the scale out and partitioning                                 transparently as you will see okay                                 so now I'm going to go through you know                                 some of these problems in how Kafka as                                 well as Kafka seems offers a solution to                                 some of these you know problems the                                 first one is ordering if if you familiar                                 with Kafka this might seem like a                                 familiar abstraction this is a                                 fundamental in abstraction at the heart                                 of Kafka which is a log no a log is                                 nothing but an abstract data structure                                 that has some properties it is like a                                 structured array of messages so data is                                 ordered it is a pent only and hence                                 immutable so data written once does not                                 change and records are identified by a                                 unique log sequence number which in                                 Kafka we call it an offset now not only                                 is data written or appended in order                                 it's also read in order you just give it                                 an offset and you scan ahead from there                                 on                                 so this log is what provides the                                 ordering semantics that are required for                                 stream processing now that is the                                 logical version of the log but                                 physically if you wanted to scale this                                 log out                                 then you'd shard it into multiple                                 partitions and if you did that then that                                 is exactly the back end of apache Kafka                                 where logically a log is a topic                                 physically a topic lives in partitions                                 that are replicated across a whole bunch                                 of brokers as events come in they're                                 appended to one of the partitions logs                                 and we have a policy for maintaining a                                 fixed window of the log no part of                                 processing you know part of sort of                                 ability is data parallelism which is how                                 do you distribute data across machines                                 which is what we just looked at but the                                 other part is how do you you know scale                                 processing how do you scale processing                                 across machines and to that effect you                                 know Kafka has this primitive called                                 group management which essentially                                 allows a group of processes which is                                 your application to form a group and                                 coordinate some action and that action                                 changes whether you're a Kafka consumer                                 or streams libraries or a Kafka consumer                                 the coordinated action is you know which                                 consumers own which topics so that in                                 total they consume the entire topic in                                 collection as the user you don't have to                                 worry about any of this all you do is                                 you embed the Kafka consumer library you                                 tell it which topics to ingest data from                                 whether you start one instance of the                                 application or in instances what this                                 had the way it works underneath the                                 covers is this consumer library does                                 this group management underneath the                                 covers so it is actually operationally                                 simple to consume large amounts of data                                 as you can see the important thing bit                                 is that this is actually the same thing                                 that's required for stream processing                                 this                                 now in this Kafka streams your consumer                                 is replaced by this topology and the                                 group management does the same thing                                 which is distribute the input partitions                                 across the instances of your application                                 that embed the streams library the the                                 really important part is that each of                                 the processes involved in your                                 application are actually peers there is                                 no special master or slave there is no                                 special job manager or task manager it                                 is hence operationally simple to deploy                                 as well as it scales well from                                 development all the way to production in                                 terms of fault tolerance the same group                                 management capability is extremely                                 important it not only does it not only                                 allows you to coordinate actions it also                                 detects failures so if one of the                                 instances of your consumer applications                                 fail the whole group rebalance is so                                 that it takes you know the remaining                                 instances take up the load of the failed                                 instance and the same thing is actually                                 useful                                 Kafka streams if one of the machines                                 that runs your topology fails streams                                 Kafka streams automatically detects that                                 failure and it restarts your topology on                                 another live instance of your                                 application so we actually don't have to                                 rebuild any of this it's just available                                 through the Kafka client protocol then                                 comes state management you know first of                                 all you know why does state management                                 appear in stream processing at all and                                 the reason is you know if you look at                                 some of these common stream processing                                 operations you'll notice that there are                                 some operations like filter and map                                 which are essentially a record at a time                                 you know either take a record and you                                 filter it out or you send it across or                                 you take a record and you map it out you                                 don't have to remember anything about a                                 record in some of these operations                                 essentially there's stateless then there                                 are some operations like joins and                                 window drag records where you in fact                                 have to remember something about a group                                 of records in order to compute that                                 operation so for state management now                                 you have two choices                                 you know first one is remote state it's                                 really easy you take your state and you                                 just stick it at some key value store                                 that you know and trust this works but                                 there is an inherent performance                                 impedance mismatch it matters Kafka can                                 process data at a very fast rate at                                 almost hundreds of thousands of messages                                 per second but a key value store when                                 used in this manner can only process                                 maybe thousands of messages per second                                 so then the second option is to push                                 your state within your processor so that                                 it's available locally since all the                                 data that is required for a stream                                 processor is available locally it is                                 incredibly fast but then there are                                 several upsides as well a it provides                                 better isolation one fast processor                                 cannot take down the state used by many                                 other life services which could happen                                 here and being it is flexible                                 this local status pluggable it could be                                 as simple as a in-memory hash map or it                                 could be an embedded rocks TB store or                                 it could be your customized read or                                 write optimized data                                 structure so Kafka streams provides                                 local state and it provides local state                                 that is fault-tolerant that is important                                 since if your processor fails part of                                 your state disappears                                 that is not grained so what Kafka James                                 does is it every right made to a state                                 store is transparently written to a                                 highly durable and highly available                                 Kafka topping so even if your processor                                 dies in it it gets restarted somewhere                                 else which is also something that it                                 does it merely reads the state from this                                 highly available topic and it recreates                                 that state before you process more data                                 so you might think you know if it if                                 every single write in a state database                                 is written to Kafka wouldn't it run out                                 of space and it wouldn't because of                                 capability in Kafka called                                 log compaction essentially it is a                                 special garbage collection policy that                                 applies to key data                                 you know topics that have keys so the                                 way this works is you know imagine or                                 consider that every write made to your                                 external database is available as a                                 message in Kafka where a message has a                                 key and a value the key is the primary                                 key of the row and the values the entire                                 content of the ROM so for a row that                                 gets frequently updated you have several                                 messages that belong to that key in this                                 Kafka log like the key a here and for                                 rows that never get updated                                 there's just the single message that                                 lives in this Kafka log forever what law                                 compaction does is in fact it's it scans                                 the log periodically and it deletes                                 older values to maintain only the latest                                 value for a key which is what you want                                 in your database which is the latest                                 value of a key what that means is that                                 every row that exists in your external                                 database also exists in this log                                 compacted topic so if you want to                                 recreate your state of database you set                                 your offset to zero and scan this log                                 compacted table in this log abstraction                                 when combined with the law compaction                                 capability actually solves a really hard                                 problem for stream processing which is                                 reprocessing data now the reason you did                                 you do want this is you know let's                                 consider the example of an application                                 that reads a stream of user visits to                                 your website and it computes aggregates                                 per geo region and it updates some kind                                 of Google Analytics tile dashboard that                                 your user then feeds but then assume                                 that you know you have to upgrade our                                 application to completely remove some                                 kind of Geo codes now when you deploy                                 their application you know that your                                 dashboard shows incorrect counts for                                 some past windows which is not a great                                 experience so what you want to do is in                                 fact reprocess past windows to reflect                                 the counts to reflect your latest                                 business logic so note that with law                                 compaction every row that exists in your                                 external state also exists in this law                                 compactor topic so if you wanted to                                 reprocess something you set the offset                                 to zero and you scan ahead and you                                 recompute your state so here's how this                                 works you have some you have your                                 application that is reading from the                                 tail and it's updating the state where                                 your user reads are happening if you                                 wanted to reprocess the data you start a                                 separate independent instance of this                                 state and you set your offset to zero                                 this is actually possible in a pub sub                                 system like Kafka because you can                                 independently consume the same topic                                 initially the state starts off empty but                                 then as it reads it catches up it fills                                 up slowly until when it reaches the end                                 when it reaches the end now you have two                                 versions of your state store your                                 application is in fact reading from the                                 old one but you can flip the switch on                                 your load balancer and have your reads                                 happen from this new version of your                                 application which now you know updates                                 the concert now it shows the counts that                                 reflect your latest business logic and                                 last but not the least you know time the                                 concept of time as I mentioned is very                                 important to correctness in stream                                 processing in fact we've paid a ton of                                 attention to modeling time correctly in                                 Kafka streams and her work is influenced                                 by this insight shared by the dataflow                                 team at                                 which essentially says that no stream                                 data is never complete and Canon will                                 always you know arrive out of order what                                 that means is you know your stream                                 processing library needs to have a                                 first-class ability to deal with out of                                 order data it shouldn't be an                                 afterthought so with respect to time                                 there are two concepts worth paying                                 attention to one is event time which is                                 when your event occurs or gets created                                 and the second is processing time which                                 is when it gets processed now due to                                 various delays and bottlenecks these two                                 things can actually converge and diverge                                 and the loss of correctness in a lot of                                 stream processing systems actually                                 occurs because they conflate these two                                 concepts which leads to incorrect                                 results so as an example let's take the                                 example of windowing back to our example                                 of the application that reads user                                 visits updates a dashboard it's likely                                 it is doing some kind of window tag                                 Ricketts let's assume that a mobile user                                 visits the website and right before the                                 event reaches the application servers it                                 loses Network coverage only to regain it                                 you know maybe                                                      hours later when the event reaches the                                 application servers now you have two                                 choices you can either count it in the                                 current                                                             count it in the                                                                                                                        actually happened if you did the latter                                 then it reflects the true state of the                                 world which is that you're counting by                                 event time but if you did the former                                 then you're counting by processing time                                 which in fact reflects incorrect counts                                 the event did not happen right now it                                 happened                                                                that this insight has is that your                                 stream processing thing has to have the                                 ability to update past data for a stream                                 or update past Windows and this is                                 closely tied to a fundamental                                 abstraction that kafka streams provides                                 which is what I'm going to talk about                                 next but before that you know we've                                 rounded up talking about the fundamental                                 problems in stream processing as well as                                 reactor streams and how                                 Kafka and Kafka stream's manages these                                 next up I'm gonna talk about important                                 abstraction that I just mentioned is                                 fundamental to solving you know updating                                 past problems or past data for your                                 stream which is essentially the stream                                 table duality now traditional databases                                 are all about maintaining tables full of                                 state but what they don't do such a good                                 job of is reacting to streams of events                                 where event could be anything from ping                                 from a mobile device or weblog stream                                 processing systems like storm are coming                                 from the other end of the equation they                                 are great at processing streams of                                 events but computing state off of that                                 stream is somewhat of an afterthought I                                 will argue that you know fundamentally                                 what needs to happen in an asynchronous                                 application is combining tables which                                 represent the current state of the world                                 with streams that represents how that                                 state is changing let's take an example                                 to understand this duality you know this                                 is a very simple example of a Kafka                                 stream it has three messages in every                                 message in Kafka is a key in a value the                                 keys in fact optional but exists here if                                 you notice closely the stream is special                                 it updates the value you know later                                 messages update value for keys that are                                 present in the previous messages so lets                                 you know call this an update string as a                                 thought exercise                                 let's try converting this update stream                                 to a table so with the first message you                                 create the first row where the key is                                 the primary key of the row if you read                                 the second message you add a second row                                 and the third message in fact just                                 updates the first row okay that's great                                 but let's take a step further and take a                                 look at the changelog stream for this                                 table for those fear not aware databases                                 to this but change logs are streams                                 where message corresponds to any row                                 that got updated so notice the changelog                                 stream                                 actually is exactly the update stream                                 that we initially started with so update                                 streams can be converted to tables can                                 be converted to streams streams and                                 tables are you on so you might think you                                 know so what so let's go back to you                                 know our a retail example and let's                                 explore this duality in both directions                                 starting from converting streams to                                 tables you know recollect that streams                                 in our example or streams of sales and                                 shipments let's say if we join these two                                 streams of sales and shipments then you                                 get a really important view which is a                                 real-time view of the inventory on hand                                 table so here's how this works                                 you have sales and shipments streams for                                 simplicity they have the same format our                                 key is the item ID and the store code                                 which is a composite key and the count                                 is the value now note that both these                                 streams are actually update streams                                 later messages give updates for keys                                 that were appeared earlier in the                                 message so we could convert and these                                 streams are represented internally as                                 tables now as part of the join operation                                 what happens here is pretty simple with                                 a message on the shipment stream you                                 increment the value in the row in the                                 table and for a message on the sales                                 stream you decrement the value for that                                 item in the store so the end of this                                 we've converted these two important                                 streams into a table which is the                                 real-time view of the inventory on hang                                 now let's explore this duality from the                                 other direction which is you know tables                                 to streams so we have this table which                                 is the inventory on hand as a result of                                 the join operation as I mentioned every                                 table is backed by a Kafka topic the                                 concept of this is a changelog stream so                                 this is what a changelog stream looks                                 like for this specific inventory on hand                                 table the things in orange on the left                                 are the rows that got updated and every                                 message here is you know a message where                                 a row guard updated okay so this change                                 lock stream actually implements or it                                 enables two very important                                 functions of retail companies that are                                 possibly not implemented this way which                                 is price adjustments which is what                                 Amazon does a great job of in real-time                                 and inventory adjustments so what the                                 you know the reorder sort of inventory                                 application does is it consumes all it                                 subscribes to this changelog stream and                                 for every message it checks the latest                                 inventory count if it drops below a                                 threshold it triggers the reorder                                 inventory our action the price                                 adjustment application does something                                 completely different it subscribes to                                 the same changelog stream but it                                 consumes messages it computes some kind                                 of a demand model which is possibly                                 internal state stored within the                                 processor and then it and it when it                                 triggers the change price events now                                 going one step further the demand model                                 that is computed by this application in                                 fact will have its own change log stream                                 that might enable some real-time                                 marketing campaign applications so this                                 is in fact useful for several downstream                                 applications you can see an emergent                                 property of some of the concepts I just                                 introduced that may not be as obvious is                                 then this stream table duality when                                 combined with local state that is made                                 queryable allows us to develop these                                 stateful applications very easily here's                                 how this works for this example back to                                 our example of this joint operation                                 where we get an inventory on hand table                                 if you look at the application view of                                 things we have several instances of your                                 application that does this join                                 operation every instance of your                                 application has a subset of the sales in                                 shipment stream as a result of that it                                 has a subset or a shards of the                                 inventory on hand table which is                                 embedded internally now let's consider                                 that kafka streams in fact maize makes                                 this you know internal table queryable                                 and this is in fact a plan feature and                                 kafka streams coming up soon which is                                 queryable state if you had this in                                 Journal state table which is variable                                 than you your application can now expose                                 the rest api which allows you to get the                                 latest gown for every item in every                                 store so what we've done there you know                                 here with respect to this application is                                 recollect this is an asynchronous                                 stateful application what we've done is                                 we've combined the table which                                 represents the current state of the                                 world which represents the current state                                 of the inventory for this retail                                 business and we've combined it with                                 streams that represent how that state is                                 changing now this may not be you know                                 practical in all sorts of applications                                 oftentimes you just want to stick your                                 state in an external database and get                                 done with it however if you have some                                 kind of lightweight operation you                                 probably don't want to maintain a                                 distributed database or if you have ton                                 of data that you need to process your                                 data then you possibly will benefit from                                 using local state but is either a hash                                 table or a rocks TB store and can be                                 very powerful now some of the things you                                 know to conclude sort of Kafka seems                                 helps you build these loosely coupled                                 you know stateful stateful applications                                 with ease in other words it allows you                                 to build reactive and stateful                                 applications within a common theme that                                 you will observe and that is worth                                 mentioning here in Kafka as well as                                 Kafka shims is that simplicity is valued                                 what our experience has been is that for                                 successful adoption of infrastructure                                 operational simplicity is you know key                                 for example you know this is the                                 architecture that you end up with if for                                 example your stream processing system                                 does not have any support for aggregates                                  so it has to store the aggregates in                                  some external key value store and hence                                  you know deploy and maintain and                                  understand that key value store keep it                                  in sync or with your stream processing                                  job if your stream processing system has                                  no ability to allow reprocessing of data                                  then you have to reprocess that data                                  anyway through a batch pipeline that                                  might                                  this might work in fact this is um a                                  popular lambda architecture but in the                                  end what your application now looks like                                  is it's pretty complex it has to query                                  these two independent views one is                                  offline one is online merge the results                                  before showing that to the user this                                  works but the number of distributed                                  thing is that you have now committed to                                  write a simple stream processing                                  application is has increased a lot this                                  is lot of moving parts and that is what                                  might you know stop application                                  developers from writing stream                                  processing systems or with ease in fact                                  with Kafka streams we've paid a ton of                                  attention to this we've taken aggregates                                  we've provided that as optional local                                  state that is available out of the box                                  we've taken reprocessing and provided                                  that as a primitive that is available                                  from Kafka and Kafka shrimps so now what                                  your application looks like is fairly                                  simple and that is important in order to                                  make stream processing approachable to                                  mainstream application development so                                  you might have concluded you know you                                  love Kafka streams but as you've                                  observed it only works on top of Kafka                                  so how do you get your data in and out                                  of Kafka in order to use Kafka streams                                  and the answer is a Kafka Connect in the                                  O                                                                community released a feature called                                  Kafka Connect which is essentially a                                  framework that makes it easy to develop                                  and use connections to Kafka connectors                                  to Kafka to all sorts of other systems                                  and sources what it does is it it solves                                  a lot of common problems that every                                  connector needs to worry about which is                                  scale out partitioning offset management                                  you know exactly ones or at least one                                  semantics and so on and in the five or                                  six months that we've released this the                                  community has written more than                                     connectors to all sorts of systems from                                  Cassandra elasticsearch to Oracle in my                                  sequel databases and the list will keep                                  on growing so this is how you know this                                  all plays out in the big picture this is                                  what I am interested in exploring which                                  is                                  the ability to deploy this kafka base                                  stream data platform as essentially a                                  central nervous system of the company it                                  is the basis for development and                                  communication between asynchronous micro                                  services and applications it is a                                  building block for stateful stream                                  processing applications as well as an                                  enabler for other stream processing                                  frameworks and clusters and last but not                                  least it is the central source of truth                                  pipeline that feeds your hadoop in your                                  warehouse this is what some of us made                                  possible at LinkedIn as well as at                                  several other companies that have                                  adopted Kafka at scale if you were                                  interested in trying out Kafka streams                                  you can download the confluent platform                                  or check out the OU                                                       Kafka admittedly some of the things I've                                  talked about our ahead of what Kafka                                  streams has in today's version of the                                  software but we are looking for feedback                                  these are early days you know give it a                                  try if you're already a Kafka user this                                  is a great way to try out new                                  functionality a lot of the ideas you                                  know that we've talked about here and in                                  fact some of the talks at Berlin                                  buzzwords have adopted are actually                                  written on the confluent blog so if                                  you're interested check it out and last                                  but not least you can also check out the                                  book that is coming up on Kafka which is                                  Kafka the definitive guide thank you                                  very much                                  Thank You Neha for this interesting talk                                  I'm sure there are questions we have two                                  microphones up here so if you would just                                  want to step forward and ask questions                                  to Neha we have one hi                                  I have a question about partitioning                                  Kafka yeah so normally if you want to to                                  scale out processing of a topic you                                  partition the topic and you define                                  consumer group and as I know Sam Kafka                                  will automatically detect failures                                  inside this consumer group and                                  reallocate consumers to cover all the                                  partitions Wood Kafka how would Kafka                                  react to situations where consumers are                                  unbalanced so some of them are there is                                  to consumers one is two times faster                                  than the other so each of them will                                  consume half of the data but you can                                  easily imagine a situation where the                                  first one have nothing to do and the                                  slow one will get stuck forever and the                                  queue will grow infinitely how does this                                  situation so it's solved yes there are                                  two you know parts to your question one                                  is how does you know how does in general                                  Kafka consumers deal with you know                                  falling behind do you run out of queue                                  space and that answer is you know tied                                  to how Kafka handles back pressure so it                                  is a persistent queue don't you don't                                  run out of you know you don't know not                                  of memory it just sits in this Kafka log                                  what you will see is your car your                                  consumers offset starts falling behind                                  and that is because Kafka's consumer                                  model is pull based not push based so it                                  actually pulls data as it requires if                                  it's slow it's the only thing that gets                                  affected the second question you had is                                  you know how does the load balancing                                  actually work on the consumer so the way                                  it works is the policy is pluggable so                                  the default policy that might chip with                                  the Kafka consumer might be something as                                  simple as wrong Robin or it could be you                                  know the one that Kafka streams uses is                                  Co partitioning if you want to use                                  joints where the two partitions from the                                  same topic need to                                  when the same processor so in fact it's                                  pluggable you know you could use various                                  different mechanisms I from my                                  experience would have noted is that the                                  simplest one is you over partition your                                  topic and you have the ability to just                                  round-robin the partitions amongst                                  consumers if if there happened to be you                                  know this if you happen to be one                                  particular partition that is hot then                                  you do end up with a case where you know                                  you either have two repartition your                                  data so that it skills well or you just                                  deploy more instances of your                                  application does that answer you if the                                  problem is not in the partitioning but                                  in a consumer is one of the consumers is                                  slow so if there a way to partition the                                  data according to consumer performance                                  yes that's that's what I meant by you                                  know sort of custom policies that might                                  be applicable here yeah so some custom                                  policies are you might want to you know                                  partition or load balanced data in your                                  consumer based on resource consumption                                  where you know some of your consumers                                  might be on you know sort of more                                  powerful nodes while some may not and                                  from from my expense I haven't really                                  seen you know sort of load balancing                                  policies that work that way but it's                                  certainly possible if you write your own                                  you can plug it in your consumer and you                                  know see the effect of that any more                                  questions well it seems that you have                                  explained everything well or I will hang                                  out right after this talk in case any of                                  you have more questions thank you very                                  much so there was no way                                  you
YouTube URL: https://www.youtube.com/watch?v=JQnNHO5506w


