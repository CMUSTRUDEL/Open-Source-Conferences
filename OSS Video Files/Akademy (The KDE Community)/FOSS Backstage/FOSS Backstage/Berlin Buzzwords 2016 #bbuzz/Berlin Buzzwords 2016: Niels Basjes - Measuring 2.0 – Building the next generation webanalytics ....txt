Title: Berlin Buzzwords 2016: Niels Basjes - Measuring 2.0 – Building the next generation webanalytics ...
Publication date: 2016-06-12
Playlist: Berlin Buzzwords 2016 #bbuzz
Description: 
	At bol.com we want to help the customer find what they wanted. To automate this process we need to understand what products/promotions we showed them and which of those made them happy. With the fine grained personalization that has been introduced over the last few years we see that just measuring ‘what page’ we showed - like all the standard web analytics systems do - is no longer enough. So we need something different. In order to get a solution that will support our business for the coming years we raised the bar to the top: Measure everything and analyze in near-realtime.

In this talk Niels Basjes will explain the project "Measuring 2.0", our next generation web analytics measuring and processing stack, that is to go live in the spring of 2016. Niels will go into:
- the custom built measuring system that will produce over 50.000 measurements per second
- the processing system and the algorithms implemented with Apache Flink
- why we did not choose Storm or Spark for this task
- the development and operational hurdles needed to make this type of solution run in production
- the architectural concepts to make this data available in the personalization services we have

Read more:
https://2016.berlinbuzzwords.de/session/measuring-20-building-next-generation-webanalytics-solution-using-apache-flink

About Niels Basjes:
https://2016.berlinbuzzwords.de/users/niels-basjes

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              okay uh welcome everybody we had some                               technical hiccups so I'm running it now                               on a Mac so the phones will look                               slightly different                               my name is news brushes I'm going to                               delete IT architects at ball become the                               biggest online retailer in the Dutch                               language area of the Benelux I'm going                               to have talk about measuring                                      project has started a few years ago as                                you know a bit of frustration of the                                status quo the overviews of talk I'll                                talk a bit about balls comm what we do                                what we use to to build those things                                upon and what's wrong with that                                and then I go into measuring                                           next generation thing that also includes                                a lot of real time streaming parts based                                on Apache flink nowadays okay um bit of                                backgrounds I have a master's degree in                                both computer science and business I've                                had various software development                                research and architect roles and I'm a                                heavy contributor to a lot of the Big                                Data projects and I'm a committer of                                Apache opera so what is ball has come                                like I said biggest online retailer in                                the Dutch language area to give you a                                bit of sense of scale our catalog is any                                                                                                  million are available for sale right now                                                                                                  millions of active visits each month our                                number of pageviews per year is over                                  and a half billion and it may not be                                surprising to you that we've been using                                Hadoop in production since                                              application we use it for is                                personalization we want to make the                                customers happy help them visiting our                                website and the first thing we built                                back then was search suggestions when                                you type some letters you get suggested                                search terms that's based on click                                traffic by customers nowadays we also                                look at your purchase history what you                                looked at what you put in your shopping                                list and what you didn't buy so if you                                look at the homepage as a red                                customer there's a lot there heart this                                is based on what I call measuring                                    well there are some issues with that and                                the biggest one is that the data arrives                                in blocks of                                                personalization is the day behind when                                we know what a better recommendation is                                the customer has already left and we                                have to hope they come back and as you                                may know the value of data decreases                                rapidly with time and what you would                                want is to stay in the low minutes maybe                                even seconds range to give feedback to                                improve health them new measurements are                                done using using javascript so we have                                this kind of stuff in the HTML and as                                you see here there are variables they're                                called llevarse and properties and                                they're limited you know it's heavy on                                the client we cannot measure everything                                and they're unclear so what happens also                                with the limitation of the llevarse they                                get reused depending on the page some                                llevarse mean different things makes it                                very hard to use the data for our data                                science guys data is incomplete it's                                inaccurate sometimes the JavaScript is                                slow and it's important to realize that                                this technology puts everything in the                                URL now a product the description we use                                for a single product looks something                                like this                                and the worst case I've seen over the                                years is this thing it's our all deals                                page all the things we are actively                                promoting at this at a point in time and                                this one had                                                          website and that resulted in this baby                                if you put that in URL things break                                they really do so you miss data as you                                may understand things needed to change                                and our business goal was that we wanted                                to be able to know exactly what was                                going on on the webshop right now                                because we wanted to assist the visitors                                right now so while sorry                                while the visitor is still on the site                                we want to give them the best experience                                possible based on everything they've                                done so far including the last few                                 clicks                                 but in addition some processes in terms                                 of their algorithm our batch oriented                                 and even those we want to run multiple                                 times per day things like search                                 suggestions search rank the sorting of                                 our products and recommendations so                                 that's why I started the project which                                 we now call measuring                                              really all about making better                                 measurements being able to process them                                 faster with lower latency and as a                                 consequence making applications more                                 relevant so the goals are really high I                                 just want everything all interactions                                 all users even Googlebot because they                                 don't do JavaScript all the details I                                 want to know exactly what was on the                                 page I wanted data to be more reliable                                 lower load on the clients and the lowest                                 possible agency so just you know                                 everything but in addition because of                                 the complexity of making these                                 measurements I want to tell you a lot                                 easier for the developers and I also                                 want to allow business people to ask new                                 questions we didn't think of before okay                                 but over time over the last few years                                 the laws have means playing a part of                                 this nowadays we have privacy laws that                                 say you can't do profiling form on data                                 for more than two years of course                                 security don't want us to record                                 security sensitive stuff and on the                                 other hand our                                 data science guys want to do profiling                                 on multiple years you know recommender                                 system we have currently uses over five                                 years of clickstream data as an example                                 so how do we solve that one well let's                                 start with measuring everything when I                                 mean measuring everything I really mean                                 everything in terms of the interactions                                 so regular weapon all ethics they                                 measure the page just the page and then                                 maybe what kind of pages was it who                                 visited but what I want to know is all                                 the parts of the page and why they were                                 put there and what was then put in so                                 example in the bottom right you see a                                 block of recommended recommended                                 products I want to know that that group                                 was there because it was a                                 recommendation and not some kind of                                 campaign cause and effect in these                                 events is the most important thing for                                 us if you look at the type of analytics                                 we do the type of applications its                                 banner optimization you show something                                 they click or they don't a be testing                                 same thing search suggestions look at                                 the search term in relation to what                                 people did with it actually beauty                                 modeling the banner in terms of what                                 people did with it so it's all                                 behavioral analytics it's all cause and                                 effect type of processing the high-level                                 data structure I designed for this                                 project is therefore a very simple                                 record that has an entity and the effect                                 on that entity and a cause and that                                 cause on the other end you also have                                 hooks you know in terms of website the                                 things you can click on usually Add to                                 Cart go to product page and so if I have                                 a cause a reason for entering the                                 website maybe a banner I record a bunch                                 of things these are the products I put                                 on the page for various reasons and then                                 somebody clicks ok I know which one they                                 clicked and I know what the consequences                                 were and that continues all the way up                                 to hey I bought something and now                                 because I have all the causes I can                                 trace it back to                                 the route and our uncles all can also                                 trace which other things competed and                                 which one okay so that's that's that's                                 the high level cause-and-effect data                                 structure how do we get these                                 measurements because that is also bit of                                 a challenge the JavaScript is really bad                                 now if we look at how can we measure if                                 we take the basic high level technical                                 process                                 it's the browser doing a request through                                 the web server to the application server                                 and the response going back doing it in                                 JavaScript means you know what really                                 happened on the browser but the quality                                 of the data is really poor if you loo                                 use the Apache access locks for example                                 you actually only know at URL level what                                 happened and but you don't know what was                                 in there and what wide was put in there                                 if you dig a little deeper some tools                                 supports sniffing so you extract the                                 HTML so you can know in detail what was                                 put in there but you still don't know                                 why the only place you know why is at                                 the application server level so what we                                 chose as an implementation strategy is                                 to do a hybrid measure as many details                                 as possible at the application server                                 level tag everything with an ID and then                                 those IDs get shipped to the browser                                 where we know what happened to it                                 and in case of users like Googlebot we                                 don't know what they did with it but we                                 know the user agent so that's fine too                                 we still know exactly what we ship to                                 them and in case we do stuff on the on                                 the client side it has to be as                                 lightweight as possible we do as as few                                 things as possible there so that that                                 will give us a data so we have the data                                 we have the advanced model how do we                                 process it okay remember we said hey for                                 online situations we want to be in the                                 low millicent in the low second ranges                                 we want to have long history in offline                                 we want to do incremental batches hey                                 that sounds familiar people call that a                                 lot of architecture that                                 that sounds really good but the nasty                                 thing about a lot the architecture is                                 that it is about combining the real-time                                 and a batch part at query time so far so                                 good                                 but if you look at all the papers and                                 presentations about it most of them as                                 far as I've seen all of them                                 they say you cut the boundary between                                 those two at a specific millisecond and                                 if you do that you cut the streams you                                 cut the caused event streams that is                                 fine if your twitter where everything is                                 a single message that's not fine for us                                 because if you if this is you know an                                 idea of the traffic pattern and these                                 streams are the visits you know cutting                                 at a daily basis at the the you know the                                 quiet point of the day you cut only a                                 few if you want to do batches every                                    minutes                                 you get this this results in data that                                 is near impossible to process for our                                 data science guys so few years ago I sat                                 down and I made a really complicated                                 design and I call it this session I love                                 the architecture which is essentially                                 the lovely architecture with a few                                 additions that I don't see in other                                 presentations and the first one is                                 bounded event streams that cause and                                 effect relationship because that is that                                 is paramount for us queries take time                                 something I also miss is the the fact                                 that everywhere I read they assume                                 careers take zero seconds so there is no                                 locking between expiring the data from                                 the real-time part and etc you need that                                 and service-orientation few years ago I                                 introduced micro services at our company                                 and everything we built has to fit in                                 there we have to fit it in such a way                                 that other people can build applications                                 on top of it now as you might understand                                 in terms of web browsers there is a few                                 terms I need to explain otherwise it                                 it's hard to understand the browser                                 it's something that it's installed on                                 the computer now if you open it and                                 visit a website you usually get a cookie                                 with a unique ID a session ID but that                                 stays active for weeks                                 people don't close their browser anymore                                 they suspend them so the value is                                 retained over a long periods of time now                                 as I said I need bounded streams so I                                 have to enforce a bound and that's what                                 I call a visit if you're inactive for                                 thirty minutes I cut it and if you're                                 active continuously for more                                         than                                                                    cut it now and most cause and effect                                 relationships are really only                                 interesting when it's a human so this is                                 this is this is fine so everything we do                                 is towards building these visits streams                                 so how do we process that okay the                                 high-level design I came up with and                                 it's this is actually the thing I'm                                 building and I'm testing right now is                                 that new events arrive from the                                 measurement infrastructure which I'll                                 show in a few minutes                                 then I have some kind of administration                                 about the active visits you know people                                 actively on the website and when they                                 started and have been active for the                                 last time etc that visit ID is generated                                 and tagged on to the measurement those                                 go out in real time via Kafka so others                                 can use their real time stream with the                                 visit IDs but they're also persisted in                                 a buffer and in a time every so many                                 minutes I extract the visits that have                                 been completed into files those get                                 stored on HDFS so you can do large-scale                                 queries and analytics on that in case                                 you want to build an application against                                 this that actually implements the full                                 session I love the architecture it will                                 look something like this and the query                                 boundary you see here is an                                 administration of which visits are still                                 in the stream and not yet in the batch                                 layer and vice versa                                 and all the locking that has to do with                                 currying and inspiring those things I                                 have a design I'm not going to build it                                 it's too                                 nobody needs it because talking to all                                 the other people that build applications                                 what I see is that most only need the                                 real-time stream in case they want to do                                 stuff with long-term history of a                                 customer of a visitor we can have a                                 small aggregate pre-computed which may                                 be one or two kilobytes with the                                 products they're interested in the                                 categories are interested in and things                                 like that and just load them in memory                                 in the stream component as soon as you                                 see the first event with that customer                                 ID and of course you have the batch                                 operation processes that now can run                                 every                                                                    mentioned earlier is a bit of a question                                 how do you handle that how do you how                                 can you do the privacy analytics at                                 long-term now privacy protection says                                 identifiable data cannot be used for                                 personally identifiable profiling over                                 more than two years and and that is                                 really about data elements that you can                                 use to point to this individual like IP                                 address and customer number and things                                 like that but what is profiling really                                 over long term it's looking for behavior                                 patterns because the personalized things                                 lose value when they're more than                                     and a half years old then they're not as                                 relevant anymore for the behavior                                 patterns of that visitor people change                                 and after about a year it becomes less                                 relevant so this is only about more                                 anonymous profiling and the query                                 pattern you see in those kinds of                                 analytics are usually about group by                                 customer ID because I want to know what                                 that person did over a longer period but                                 if that's the real question then these                                 queries give the same answer because if                                 you encrypt it or you hash it or even                                 salted in hashes the group I will still                                 yield the same result hash collisions                                 and hash relations are if you look for                                 large-scale patents are not important so                                 what we chose                                 that for the data in motion the                                 streaming stuff we persist that in Kafka                                 and that expires after a couple of weeks                                 and for the data at rest the                                 identifiable ones are simply deleted and                                 the anonymous ones are salted with a                                 yearly salt so we in our IT operations                                 we have a system that will have a secret                                 salt everything it's salted with that                                 hashed and we only persist the hash and                                 nobody not even me not an order data                                 says science guys have access to the                                 salt so nobody can reverse-engineer it                                 and by splitting the access permissions                                 between the two we keep that safe so                                 what this is that is that unnamed                                 individuals we can do analysis over the                                 last two years but anonymized we can do                                 analysis on a yearly basis for much                                 longer                                 but it's anonymized and you still have                                 all the details to do the real business                                 value in our analytics so how do you                                 build something like this well the                                 design I created is I'm going into a                                 little bit more detail now is that we're                                 going to introduce a measuring several                                 measuring components into our web                                 front-end part of it is in the                                 application server that records the                                 details of what and why and puts that in                                 Kafka and puts the IDS of those                                 measurements in the HTML that are then                                 via JavaScript put back also into Kafka                                 that's mostly about what was put on the                                 screen and what was clicks and things                                 like that then the session riser which                                 has showed you just before with one                                 extra thing and that's geoip we would                                 like to know what country that visitor                                 came from was it Netherlands Belgium                                 Germany US whatever just the country                                 code and we output that as I showed                                 earlier in Kafka and as files this part                                 is the personally identifiable stuff so                                 from there we have an anonymous Asian                                 filter that anonymizes it and outputs                                 the date the same data anonymized also                                 in Kafka and also as files and those can                                 be then used for dashboarding and for                                 recommender systems and things like that                                 now a couple of years ago I started my                                 first prototype to build this ah this                                 gives a good impression of what it was                                 like because I tried it at that time                                 with using Apache storm stream                                 processing low latency lots of people                                 using it sounds like a plan until you                                 try to do it back then the API is for me                                 really unfriendly this was actually                                 copied and pasted at last week from                                 their website so the API hasn't changed                                 much you have to make sure that the                                 names of the bolts it's all match                                 otherwise it simply doesn't work it's an                                 at least once streaming thing if you                                 need exactly once you go to trident if                                 you go to try did you go to micro                                 batching and that means latency much                                 more than I want and storm when I tried                                 it is completely stateless now a few                                 weeks ago they said it they now have                                 stateful things but the thing I tried is                                 completely stateless so building in the                                 state of the visit is incredibly hard                                 and at that time it was said you can run                                 it on yarn but I never get that to run                                 perhaps that now works but back then it                                 didn't so how about spark streaming you                                 know lots of libraries and tools are                                 very good toolkit but also micro                                 batching and I don't want that latency                                 so then came last year at Berlin                                 buzzwords I talked to Stefan and I asked                                 him some questions because I already had                                 a lot quite a bit of experience in doing                                 it with storm how does it work in flink                                 well flank is also low latency but it's                                 exactly once and that's mainly because                                 it has the recoverability of the stage                                 built in and it runs on yarn and the                                 state managed in the framework also                                 supports all these checkpoints and safe                                 points and things that make it easy to                                 recover after                                 failure or restart of the application                                 it's also really nice is that as a                                 concept of Windows a window is a group                                 of elements that you can combine into a                                 well extremely streaming lis built up                                 batch which makes it really useful for                                 the type of analysis I want to do and to                                 me this feature event time is the                                 biggest plus of flink I've ever seen it                                 means that if you bring down the                                 application for say a day extreme case                                 and you bring it up again so it reads                                 from Kafka then all the events of timing                                 out                                                                      on the time in the data so you can                                 process a day worth of data it may be I                                 don't know                                                               the timeouts correct that is the biggest                                 plus of link there are challenges the                                 first one I ran into was reading and                                 writing from HBase                                 on a secure cluster so I fixed that and                                 we still have the problem that after                                 seven days the Hadoop delegation tokens                                 timeout and then any job dies also the                                 spark jobs we run on the cluster does                                 just die after seven days it's a setting                                 but still it's it's a finite time and my                                 windows the data in my windows is too                                 big to fit into memory so right now I'm                                 shoving them into HBase that's what I                                 haven't graphs a little later also but I                                 have to evaluate the rocks DB which is                                 in the newer versions of link available                                 also very careful the exactly ones is on                                 the processing and on the kafka inputs                                 but not on the Kafka output in case of a                                 distortion and a replay of some events                                 you get duplicate events in the Kafka                                 output something to be aware of so how                                 do I assign deficit IDs in string the                                 window implementation buffers until the                                 window is complete and then flushes it                                 so that's too long                                 so I had to build my own custom operator                                 I had somehow                                 from the data artisan guys thank you                                 very much for that so I built that and                                 how does that look well there is the                                 sign visit ID operator that received the                                 events has a bit of state and shoves                                 them out again then they're stored in                                 HBase and in Kafka and then threw in on                                 an anonymization filter and they're also                                 stored in HBase and Kafka then the                                 watermarks arrived and based on the                                 watermarks I say hey there is an expire                                 of this visit i time it out and I send                                 out an end of visit event those are then                                 grouped in a window to avoid too many                                 files on the Hadoop cluster I group the                                 the end of visit events and then when I                                 have a window of intended visits I pull                                 out all the events from the HBase and I                                 store them in park' files when I started                                 I made this design and then when I                                 started to build it I turns out that you                                 can only have one output type of an                                 operator instead of two so I had to do                                 some stuff and the watermarks are really                                 really important so what I ended up                                 building is well this is the same but I                                 had to multiplex them because one type                                 of output and then they go out and then                                 I pull out the ended visit events again                                 and there I had to generate new bottom                                 arts because without those watermarks                                 the windows don't work and from there                                 it's more or less the same while                                 building this I realized that those end                                 of visiting events may be useful for                                 downstream applications so I decided to                                 keep them in if you're building                                 something like this debugging the                                 watermarks is essential and it turns out                                 that they're quite tricky to get right                                 if your data is too messy so that's not                                 a flink problem is really how clean is                                 your data so I needed to do some                                 debugging of the combination of the                                 watermark and event and                                 do that I wrote a very simple operator                                 that that simply prints them because in                                 an operator code you can process                                 watermarks and process the events and                                 all it does is prints them to the                                 console for development purposes now if                                 you're building a streaming application                                 you have to realize that requirements                                 will change it happens and what we do is                                 we use Apache Avro the ideal schema                                 language to define the schemas and that                                 is nice because it generates really                                 useable Java classes and it supports                                 name-based schema evolution                                 out-of-the-box so if you have a class                                 with a new if you have a record with a                                 newer schema you can simply read it                                 using Apache Avro in the newer situation                                 we have chosen parque to persist the                                 data in files because I'm writing the                                 event streams on a per session basis on                                 a per visit basis what you see is in                                 that range they all have the same IP                                 address same user agent most of them                                 have the same customer number and a lot                                 of attributes are all the same so using                                 the column-oriented parque format makes                                 that compress really really well also                                 porque is supported by the tools we use                                 to read and query again so that's also                                 big plus and writing our records into                                 park' files is really easy because part                                 a has actually actually has supports for                                 that now in the streaming scenario we                                 use Kafka like I mentioned so it's a                                 highly it's a high throughput low                                 latency transport pipe that persists the                                 data for a couple of weeks so how do we                                 put that record we have as a byte array                                 in Kafka okay how do you see realize                                 that the important thing to realize is                                 that Apache Avro needs the original                                 schema to deserialize a byte array again                                 so you need the original schema now in                                 the test case I'm running right now the                                 original                                 schema is like                                                      record is like foreign bytes and I don't                                 want to attach the entire schema to                                 every message so how do you do that                                 because the schema will change if the                                 schema changes and you know you have                                 like say you persist the messages for                                 four weeks and you have two weeks of old                                 schema and two weeks of new schema and                                 you hook up a new application it must be                                 able to read all the records how do you                                 do that well that is something I'm                                 working on right now it's an opera                                 project where I've said I'm going to                                 make a wrapper around the normal                                 serialize afro record that I call a                                 message which essentially contains the                                 fingerprint of the schema if you combine                                 that with a pluggable schema database                                 which you can do in well any system                                 really maybe a property file maybe maybe                                 a Redis or whatever then you can store                                 all the versions of all the schemas in                                 there and retrieve them by fingerprint                                 so any une consuming system can then                                 deserialize the record and read it in                                 their local version of the schema now                                 this at bottom we have been doing these                                 Big Data things for quite a while                                 so you may realize that it's this is not                                 a hype this is what most of our teams                                 are required to do most of our                                 development teams use HBase and the                                 Hadoop cluster to do part of their work                                 so if you think this is cool                                 don't hesitate join us contact me and                                 talk questions                                 hello I don't thank for the joke so I                                 have a question regarding this our                                 schema mm why don't you guys use our                                 registry for starting your schema sorry                                 our registry schema registry for storing                                 a schema that is already built one in                                 yeah I think confluence right I wrote                                 one yeah a confluence yeah yeah yeah I                                 know I know but for now I'm saying that                                 within the Afro project I'm making it                                 pluggable so if somebody says I want to                                 use the confluent implementation they                                 can they can simply attach it to that                                 pluggable interface but if somebody says                                 no I just want to use property files or                                 I want to use a proprietary thing like                                 we have internally then that becomes                                 also possible so I want to want to keep                                 that a bit more flexible and something                                 I'm also building in to keep it a bit                                 more flexible is that the actual                                 serialization of the message body I want                                 it to be pluggable also in order to                                 support encryption so if somebody says                                 no I want to persist all the streaming                                 messages in an encrypted form in Kafka                                 that is also possible but still a                                 work-in-progress it's not yet finished                                 but I'm working on it yeah                                 I I you said you had to put the state                                 into HBase so what was the size of your                                 window state on each worker okay um I                                 did some calculations because I know the                                 traffic level of our website and the                                 estimate is that we have to keep about                                 we will get something like                                               per second during the peak of our season                                 and the events are in the experiments                                 I'm running now they're about half a                                 kilobyte in size each and then with all                                 the timeouts taken into account that                                 that runs into the to many gigabytes in                                 RAM that is so that's why I'm saying and                                 I have to persist it to disk or I have                                 to convince the boss to get a lot more                                 servers to keep it in memory but you                                 know persisting it to disk and I'm doing                                 low tests using HBase that that works                                 quite well                                 hi nice the phone hey thanks for the                                 talk quick question on this like                                 security and expiry of the access tokens                                 what do you actually do to work around                                 this do you have something like a script                                 that periodically yeah just make sure                                 the tropas yeah completely killed and                                 restarted sort of new tokens yes                                 the timeout set for the hadoop                                 delegation tokens is                                                  the default setting in the Hadoop                                 cluster it turns out for a very strange                                 reason that the the jobs died after                                 exactly a hundred and seventy three and                                 a half hours so that's five and a half                                 hours later and it's so exact that I've                                 run the experiment several times and                                 they all died within a range of                                    seconds so for some reason that's the                                 cutoff point and what we have now is for                                 the fling jobs running on a cluster we                                 have picked a point in the week where                                 it's most quiet and there we simply kill                                 them and restart them so that's once                                 every                                                                    to spare and because we control the                                 restart that works quite well for now                                 are you actually using a safe point or                                 so to resume from from the previous                                 execution after you basically kill it                                 and restart it or eventually let it pick                                 up the applications we have right now in                                 production                                 don't use Windows and in those specific                                 applications with our background                                 analytics jobs losing one or two records                                 is not that much of a deal so at this                                 point we don't use it yet okay then                                 maybe the final remark I'm not sure if                                 you've seen it there's actually a threat                                 that is stealing exactly with this like                                 Kerberos tokens and an exploration and                                 so on where we're trying to add some                                 functionality to you know attach key                                 taps to a job so it will actually                                 automatically renew these things Oh                                 excellent hopefully gonna be in in a few                                 months                                 okay that's good thank you umm I suspect                                 don't know why your tokens are expiring                                 a bit after that time is because they do                                 the renewal in advance and twelve and a                                 half hours later all dice okay                                 as someone who has painful experience or                                 for Kerberos and yon I will say that                                 actually what you do is ask the flink                                 people to do some token renewal which                                 can not only be done with a key tab                                 because people hate key tabs around but                                 actually just have something push out                                 fresh tokens your desktop or Lucy or                                 something like that                                 that's what other people do okay thanks                                 have you thought about an end-to-end ray                                 meant when it impotency way to get                                 exactly ones output for the Kefka sink I                                 have thought about it I put up my idea                                 on the flink user list but that would                                 require quite a hefty change in the                                 output because the idea I had is that                                 when you output messages into Kafka you                                 record the in the output sink in terms                                 of state where you left periodically and                                 then in a recovery scenario you would                                 have the sink acts as a client and read                                 in till the point where it sees the                                 message that it has to output next but                                 that would be a bit of a hefty rewrite                                 and I haven't taken the time yet to to                                 try to build that yet but if you feel                                 like billing it please do we'd be useful                                 for to everybody thank you                                 then Thank You Niels okay thank you
YouTube URL: https://www.youtube.com/watch?v=qwbGQDpxmVg


