Title: Berlin Buzzwords 2016: Jan Graßegger - Real-time analytics with Flink and Druid #bbuzz
Publication date: 2016-06-12
Playlist: Berlin Buzzwords 2016 #bbuzz
Description: 
	Real time insights into multi-dimensional data is a key asset for data-driven businesses. We present the architecture of our fast and reliable streaming-only data processing pipeline which harnesses the qualities of Kafka, Flink and Druid. This trio turns out to be a very good choice for building real-time online analytics systems.

In recent years Apache Kafka has become the de-facto standard for highly available and highly scalable messaging.

Apache Flink allows us to consume, process and produce data with minimum delay. When using a streaming-only approach the challenge is to guarantee the correctness of your data. Flink’s capability of using different sources (in our case Kafka for real-time and HDFS for historical data) easily lets us reprocess data without any need of maintaining multiple code bases often needed in Lambda Architectures.

Druid is a datastore designed for real-time multidimensional analytics and overcomes weaknesses of alternative approaches like RDBs and Key-Value stores. It’s streaming ingestion plays extremely well with Flink which is able to process every event as it arrives. We have already contributed our Flink sink to the Druid project so that you can use it out-of-the-box.

Read more:
https://2016.berlinbuzzwords.de/session/real-time-analytics-flink-and-druid

About Jan Graßegger:
https://2016.berlinbuzzwords.de/users/jan-grassegger

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              hello okay yeah welcome everybody to my                               talk about real-time analytics both                               flink intrude yeah my name is Ian kosaka                               and I'm working as a data engineer with                               mb are targeting MBR steering real-time                               advertising means that we are providing                               a demand-side platform this P to help                               advertisers which the customers more                               efficiently and the last time we were at                                last time we working you were working on                                real-time analysis and reporting to and                                this is very important for us and there                                are customers key to get live insights                                into their campaigns that are running on                                our platform but this task is not that                                review since we have some data like                                                                                                     event as up to                                                        and we are calculating                                                 on these events even types and                                dimensions so this is kind of a lot of                                data and we had to look for for storage                                that is able at first to consume that                                amount of data and show it to us                                immediately and it is also able to to                                handle queries where we can combine any                                dimension with each other and get a                                response in an acceptable amount of time                                like in under                                                           also looking for an open source source                                solution and since the problem is is                                quite hard and it's not easy to solve                                them with relational databases or QA                                stores since you ever have dimension                                with a quite high card                                that he liked top-level domain or                                whatever their if million of millions of                                entries of different entries and these                                columns and four for example for a key                                where you would have to pre-compute all                                the possible combinations of these                                different dimensions so we were looking                                for for storage that does all this for                                us and we found route through this is an                                open source online analytical processing                                system that is made for this purpose for                                having high dimensional data by creating                                it very fast and also it has                                capabilities to handle this data in real                                time yet well it's a calamari entered                                storage it's distributed distributed                                means that you have multiple different                                roads like like a real-time node that                                handles the real-time data estoy que                                node that is able to read historic data                                also from distributed storage just like                                HDFS or as free and you have have a                                broker well that knows where to carry                                forward data and we you're crazy it has                                to go to it has built-in data charting                                capabilities means all your data is                                organized in in so-called segments and                                these segments are shot up by time when                                you create data include you you're                                saying yeah I want segments that contain                                data of one hour or one day or one                                minute and whatever and you carry food                                with adjacent crappy language yr rest                                like HTTP API so the question is now                                what makes food fast and why is it how                                is it able to to handle this                                multi-dimensional data                                and it's one thing is that it is the way                                how it creates how it creates the data                                structure for example we have a column                                of top private domains just two here we                                have two different domains with two                                entries each and what does it maps all                                string values of columns to integers and                                so it is able to to encode the column                                data very efficiently that's one thing                                so it this trains the amount of data a                                lot by a lot and a second data structure                                our bitmap indices that means for every                                column it stores is is that where you in                                that column or not so if if we have for                                example we have bet on that on the first                                two columns so it was the one for these                                two columns and the zero for the other                                columns and this makes it quite                                efficiently to to further on on entries                                like for example if we want all entries                                that that contain battle net and knocks                                it come when we could do your operation                                on these two areas and get all columns                                that contain these two entries yeah                                that's on data structures now's the                                question how do we get our data into                                truth and they are so-called fire hoses                                 which are the data ingestion                                 capabilities of two didn't usually a                                 data stream poll based means that you                                 create these fire hoses wire why at the                                 JSON API and then you give a local file                                 paths or HDFS file paths or you give a                                 Kafka topic and                                 possible but what we finally wanted for                                 our system was something different we                                 wanted to to push data into food to read                                 it from stream that comes from Kafka and                                 put it on transform it a bit and then                                 put it into you interviewed and so now                                 we decided to to use truth as our                                 storage which was quite because it was                                 quite impressive to us but we needed                                 some some other tool to to put the data                                 entered food and so we took a look look                                 at fling fling is a platform for                                 distributed stream and batch processing                                 but the interesting thing on fling is                                 that everything is stream based so you                                 can work on every single event it's                                 quite nice yeah i think the talk                                 afterwards we'll get a more deeply into                                 thing so i will keep it short but flink                                 it's really nice to work with and so we                                 came up with these really kind of                                 architecture are designing how we want                                 to process our data we want to consume                                 it we have it can conquer once you                                 consume it will fling and put it into it                                 now the question is how to how to                                 combine fling and intrude and there's a                                 tool sub-project of of truth that's                                 called funk ility and fertility is made                                 for this purpose of putting real time                                 data touch-based into truth and it                                 provides several adapters like for them                                 the spark storm and now it also contains                                 an flink adapter because we committed it                                 to the project but it was quite fast                                 forward to do that and now it also has a                                 fling sink worm that enables you to                                 easily put your data into food and also                                 provides some standalone HTTP                                 server or Kafka application where each                                 data from calf computed into to it but                                 yeah doesn't matter for us so now we we                                 know how to put the data from fling to                                 truth but there was one open problem                                 that we discovered or two things that                                 we're still missing and the main thing                                 is how how are we doing replaced with                                 this architecture because tranquility is                                 really made for the purpose of of                                 putting putting real-time data in and                                 it's made with the there was an image                                 matter made from the idea of a lambda                                 architecture where you have different                                 processing paths for stream for                                 streaming and for batch processing means                                 that he means that you put data here's                                 the image means that he put data while                                 streams and real-time into truth but                                 there's but he repossess everything                                 again with period and reading it from a                                 HDFS and use is dead sese ground truth                                 so if we would have gone with this kind                                 of architecture we would have to                                 maintain two different code paths two                                 different tools that that put the data                                 into our into our storage and also to                                 put to prepare the data for true to                                      and write it prepare to HDFS and this is                                 something that we didn't want what we                                 would like to do is to put this together                                 to have something of have like one code                                 paths that processes everything and so                                 we came up with                                 kappa of the idea we with of a Kappa                                 architecture and the Kappa architecture                                 relies on relies on the idea of that                                 streaming is more reliable nowadays that                                 you can also have a crown truth bate                                 bate only based on streaming but there                                 was one thing that the cafe the idea of                                 copper is like even just working with                                 with Kafka you have like thirty                                         of your data stored as historical data                                 and Kafka and if you want to reach                                 possess something you just read the                                 kafka stream again but in our case we we                                 have a lot of data already in our                                 warehouse we have like almost all data                                 that ever came to to us in our warehouse                                 so we would like to be able to process                                 that and not to stir it in also in Kafka                                 and so we came up with this kind of an                                 idea of of an architecture where we have                                 Kafka and HD f SS sources for fling and                                 the dicen are the only to you that the                                 sources are the only difference in the                                 code path when we want to process our                                 data that's still thinks it's really                                 nice idea and weather was really really                                 hard to achieve that because food was                                 really designed and also tranquility was                                 designed to you to work with lambda                                 architectures and for example                                 tranquility drops every event that is if                                 you if you have segments that are                                 partitioned by by our for example then                                 it drops every message that is older                                 than one hour and there was not built                                 into to be able to to add all the data                                 and also it awesome food you cannot                                 reopen a segment                                 you have to delete it and then create a                                 new one and this is all of this we had                                 to you I first find out and then                                 implement it on ourselves but this is                                 something that we finally achieved so                                 now we have this couple like                                 architecture that is able to do we place                                 from HDFS and Kafka and do does                                 everything with linked with the same                                 code pass with a streaming API off link                                 and yeah that's what was quite nice to                                 achieve and to to summit now up a bit we                                 as I already said we already committed                                 the fling sink too to tranquility we                                 have these kind of hacked replays that                                 we edited faculty but this is still on                                 our in our repository we are going to                                 try to commit this to oncology but i'm                                 not sure if they will accept it but                                 you're still free to have a look at our                                 tranquility repository from mb are                                 targeting so also on github so yeah and                                 finally to to the final goal that we                                 want you each is the to have a real-time                                 reporting and analysis tool that's                                 something that we achieved and yeah that                                 was quite cool in it an interesting way                                 we learned a lot on this way yeah thank                                 you very much questions please ok                                 I'm running I'm curious about how you                                 got it all together like the iterators                                 in Kafka the checkpoints from fling the                                 druid segments and I guess the Hadoop                                 files or partitions I mean they all have                                 to be in sync can you tell us a little                                 bit about that yeah we have let's start                                 with the data in HDFS so that something                                 we already had and it's petitioned by by                                 it's already petitioned by our it's                                 these are five cables that are potential                                 buyer then we have these fling job that                                 runs all the time and fetches the data                                 from from Kafka and thus checkpointing                                 so if you if it fails it starts again on                                 the same offset and so just and it's                                 quite simple because we also have to do                                 some joining in our pipeline that but                                 that was all happening before so we are                                 just reading already joint events                                 transforming them to and filtering                                 filtering out the dimensions we don't                                 need for food and then it runs all the                                 time if it fails we just restarted and                                 it keeps up running and the HDFS replay                                 is something that is more that we                                 developed you to handle failures either                                 in the workflow with a note crashes and                                 we have there for some reason I events                                 missing or if something was                                 misconfigured in our campaigns or so and                                 then we are able to restart this replace                                 manually so we are not doing this lambda                                 architecture weights usually                                 way works the way that he did screaming                                 all the time but you always override the                                 data by the batch process that's                                 something we do again nothing yeah okay                                 one more minute left for questions just                                 give me a second okay have you tried to                                 to send the message from a gfs back into                                 calc and we reuse your your flow in a                                 production system I did something like                                 these but the amount of data that I have                                 to send back to to to calc is very                                 limited in my use case but in yours                                 because you have to do the full                                 processing yeah if you have to recompute                                 everything I expect you to have to send                                 a lot of data back into kafir kafir                                 probably more than the                                                   you want to recompute a fuel history but                                 yeah in that case you would really need                                 to to to be able to read from HDFS but                                 it's just an idea and you guys can send                                 the data from ATF has back into caffeine                                 to a different topic and if the data to                                 repossess is not that much you can reuse                                 the the code that reads from Kafka yeah                                 yeah we thought about it but the point                                 is that its kind kind of trivially to                                 eat from HDFS with link so to write a                                 process that to develop the software                                 that puts it to you to Kafka again and                                 then you read it from calf gets more                                 complex than just reading it from a TFS                                 and it's both data is in the same format                                 so it's really for us it's in our case                                 it's just a difference of the source and                                 the data is equal and that so wasn't                                 really a challenge to each you have to                                 read from HDFS and it works quite well                                 okay thank you with this question we                                 should finish this session and in                                    minutes we will continue                                 with another session on the flink topic                                 thank you
YouTube URL: https://www.youtube.com/watch?v=mYGF4BUwtaw


