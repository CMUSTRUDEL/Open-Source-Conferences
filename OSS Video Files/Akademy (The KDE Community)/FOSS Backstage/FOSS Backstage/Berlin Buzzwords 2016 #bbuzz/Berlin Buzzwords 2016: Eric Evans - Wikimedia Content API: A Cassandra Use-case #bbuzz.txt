Title: Berlin Buzzwords 2016: Eric Evans - Wikimedia Content API: A Cassandra Use-case #bbuzz
Publication date: 2016-06-12
Playlist: Berlin Buzzwords 2016 #bbuzz
Description: 
	The Wikimedia Foundation is a non-profit and charitable organization driven by a vision of a world where every human can freely share in the sum of all knowledge. Each month Wikimedia sites serve over 18 billion page views to 500 million unique visitors around the world.

Among the many resources offered by Wikimedia is a public-facing API that provides low-latency, programmatic access to full-history content and meta-data, in a variety of formats.  Commonly, results from this system are the product of computationally intensive transformations, and must be pre-generated and persisted to meet latency expectations.  Unsurprisingly, there are numerous challenges to providing low-latency storage of such a massive data-set, in a demanding, globally distributed environment.

In this talk, we will cover the Wikimedia content API, and it's use of Apache Cassandra, a massively-scalable distributed database, as storage for a diverse and growing set of use-cases. Trials, tribulations, and triumphs, of both a development and operational nature will be discussed.

Read more:
https://2016.berlinbuzzwords.de/session/wikimedia-content-api-cassandra-use-case

About Eric Evans:
https://2016.berlinbuzzwords.de/users/eric-evans

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              right so thank you all for coming my                               name is eric evans i work for the                               wikimedia foundation full disclosure i                               am also a member of the cassandra pmc                               and there was a time when i worked on                               Cassandra a full time that was my job                               that is not the capacity that I'm                               presenting today my time at the                               foundation has kind of given me the best                                opportunity I've had so far at easton                                from modern modern day cassandra to                                develop an application against Cassandra                                and to be involved in the operations of                                the cluster and it's been you know quite                                an eye-opener so to be on the other side                                of the fence so that's kind of the frame                                of reference for this for this talk the                                wikimedia foundation well Wikimedia in                                general is an umbrella that represents a                                number of member projects I'm all sort                                of in the scope of free knowledge                                collaboratively edited free knowledge it                                is our mission for a world in which                                every single human can freely access to                                some of all knowledge the projects in                                total amount to over                                                views every month I believe it is my                                understanding that if you were to add up                                the aggregate traffic that it would be                                equal to about the number of                                           web property wikipedia alone at                                   million articles in almost                                                                                                                     number                                                                 the architecture a little bit just to                                give some background I'm going to build                                a use case and then talk a little bit                                about our use of cassandra and sort of                                our experiences but let me start with                                some background one of those logos on                                the on the screen of all the projects                                was for mediawiki which isn't                                technically a free knowledge project is                                a free software project it's the                                software that we host all of the wiki's                                at                                at at wikimedia and it's it's kind of                                old school it's very mature applications                                been around for quite a while and so you                                know for all those people who are who                                jumped all the latest languages and                                frameworks it's kind of boring it's a                                lamp stack linux apache mysql and PHP                                but very simple very straightforward and                                yet the most comprehensive architecture                                diagram i could find was this one and                                even i can tell that this oversimplifies                                a great deal and emits a great deal                                clearly that's a lot more complicated                                looking than a simple lamp stack                                architecture i'm not going to dive into                                that architecture diagram and try to                                explain everything even if even if I                                could it was just sort of to demonstrate                                that when you're hosting it Wikipedia                                scale and the number of features and the                                number of integrations that at Wikimedia                                hosts things get rapidly much more                                complex I'm going to focus on just one                                of those complexities which is visual                                editor so as most of you probably know                                any in a wiki we have this concept of                                well in mediawiki at least is called                                wikitext of a a plain text format that                                is relatively friendly for humans to                                edit and the reason for that is what we                                need to send to the browser is HTML and                                editing that directly is decidedly not                                friendly so the way this works is you                                know you have a web form you enter in                                your wiki text you know you match save                                and you're presented with you know the                                HTML representation of that document and                                at the core of all this obviously is a                                conversion from wiki text to HTML when                                you want to go back and edit again you                                go back to the last edited version of                                the wiki text document you edit it and                                click Save and you Marshall that into                                HTML for the browser what visual editor                                gives you is a wussy wig editor what you                                see is what you get so when you match                                edit and visual editor is enabled you                                stay on that HTML representation editor                                controls appear and allow you to select                                formatting you know                                whether you're editing a section header                                or paragraph bullets links the sort of                                thing and then when you manage to save                                 the editor controls disappear this is                                 really nice it's not a JavaScript hacked                                 this uses html                                                           a very nice editing experience this is                                 really great because as as much as much                                 friendlier as wikitext is for editing                                 HTML it's still kind of off-putting to                                 it to most people so this is really                                 lowers the barrier to contribution but                                 it goes without saying that it also                                 requires that we're able to convert from                                 HTML back to wiki text because we have                                                                                                         in Wikipedia alone worth of history a                                 lot of people are still going to prefer                                 editing wikitext we can't just change to                                 editing HTML it's a big ship take a long                                 time to steer so we need to be able to                                 go not only from wiki text to HTML from                                 HTML back to wiki text and that's not as                                 simple as it might sound one example of                                 a complication there is history we                                 expect to be able to do character-based                                 diffs between arbitrary revisions of a                                 document so if in the conversion from                                 HTML back to wiki text you normalize any                                 portion of the document outside of what                                 the the editor did themselves then you                                 create a dirty diff so the way you do                                 that is at the time that you're                                 marshalling the wiki text to HTML for                                 that given version the given revision of                                 wiki text you kind of evaluate as you go                                 what would it take to preserve this                                 formatting on the round trip back from                                 HTML and you supply that information in                                 the form of additional metadata so if                                 you look at this first line this is an                                 example in wiki text of of a wiki link                                 of an internal link to a page named food                                 in the same namespace that we're editing                                 it and the link text will be bar but                                 when Marshall to HTML you know it's just                                 an anchor so there's really no way to                                 tell it from you know one link type from                                 another so in this case we pass an rdf a                                 attribute that indicates that when we                                 round trip back to to wiki text that                                 this should be a wiki link a little bit                                 more complex example if the link text is                                 the product of a template and we want to                                 trance clued that template into the                                 final output we don't want to take that                                 output then and push that back into the                                 wiki text we want to get the template                                 back again so in this case you know                                 we're out that value text the link value                                 in a span with attributes to indicate                                 that that's a template and then we have                                 this data par soy tu attribute is                                 private attribute that stores the actual                                 template itself and in practice and more                                 complicated examples that private                                 attribute can actually get really you                                 know really verbose can get right down                                 to character and bite offsets in an                                 effort to you have all the information                                 necessary to to reconstitute that                                 original text formatting so we do this                                 conversion to HTML it's round trip about                                 HTML with a stateless microservice                                 called parsley which is written in                                 nodejs it exists specifically for this                                 service taking specific revision of wiki                                 text and create the HTML representation                                 that can then be edited and and                                 converted back to wiki text without                                 altering any any without introducing any                                 non-normative changes the reason this is                                 an external service primarily is because                                 visual editor is is a is an extension                                 it's an add-on to mediawiki and because                                 this wouldn't be a good general purpose                                 solution for converting to HTML                                 converting like this is computationally                                 intensive so it's slower the output is                                 larger so it takes longer to transfer                                 which would make page load times higher                                 the DOM is just even more complex and                                 takes browsers longer to load so you                                 know we targeted only at users who are                                 going to need to edit that each tml and                                 we use a lighter weight conversion for                                 the general general case also having it                                 as an external service lets the scale it                                 out horizontally so this is one of those                                 things that contributes to that extra                                 complexity on that on that architecture                                 diagram part so it is stateless though                                 so we host it behind another service                                 from the from Wickham                                 you called rest base which is a sort of                                 services aggregator that performs                                 durable caching of responses so it looks                                 something like this those durable Cashin                                 occurs in Cassandra we persist those                                 response values in Cassandra and there                                 are other services that we aggregate                                 behind rest base as well examples                                 include mobile content service we have                                 Android and iOS clients applications                                 that exist to provide a more Native feel                                 for those in those those platforms                                 sometimes that involves transforming the                                 content the mobile content service does                                 those transformations and then can                                 persist those in rest base so that's my                                 use case let's look a little bit at                                 cassandra cassandra is use in that this                                 purpose so we have two data centers we                                 use Cassandra's network topology                                 strategy to distribute a total of six                                 six replicas three each across two data                                 centers in in Cassandra parlance we also                                 distribute within the data center across                                 three racks in reality for us the row is                                 the level of infrastructure isolation                                 we're looking for that's where we have a                                 where we share common p to use so we can                                 lose an entire row in a data center and                                 still be available we can lose an entire                                 data center and be available it's just                                 really nice we run                                                      machines but we have a                                                  cassandra cluster so we have three times                                 more cassandra nodes than we do hosts                                 and i will expound on that in a little                                 bit here we use deflate compression and                                 cassandra which is a little unusual it's                                 not the default there aren't very many                                 people using the plate and we also get                                 higher compression sizes which I will                                 explain as well and it is a read heavy                                 workload for Cassandra which probably                                 comes as no surprise given I described                                 it as that's a durable cash but                                        maybe not as as higher Reed Reed heavy                                 as you would think on every single                                 document edit we fire off an                                 asynchronous job too                                 I'm the cash you know hedging our bets                                 against someone might maybe wanting to                                 edit that in that visual editor so the                                 right right rate is maybe a bit higher                                 and you might expect and this is kind of                                 a second-tier cash to our CDN so rest                                 base is hit to produce this output only                                 when there's a cache miss on the CDN all                                 right so let's look at the the data                                 model if you look at this in Cassandra                                 ddl like what you would use to create                                 the table and I'll limit this just to                                 the HTML storage and this is a bit                                 oversimplified we have these attributes                                 domain title revision tid and value                                 domain is the is the the Wikimedia site                                 domain so en.wikipedia.org wikipedia.org                                 something like that title is the                                 document title revision is the                                 monotonically increasing revision ID                                 supplied by mediawiki so whenever                                 someone edits a document in mediawiki                                 mediawiki assigns it a new revision this                                 is that revision the tid value here is                                 kind of a misnomer I guess is also a                                 sort of revision it's one that is                                 created by rest base it is meant to                                 indicate the revision of the HTML for                                 that revision of document and the reason                                 this is necessary is that we do full                                 page renders and so there's always some                                 sort of trance cluded material in there                                 if you think in terms of say a template                                 if a template is edited some were                                 centrally every document that includes                                 that template would then need to be rear                                 end ered even if those those documents                                 themselves didn't change so it's sort of                                 a revision of the revision is the HTML                                 revision of the wiki text revision and                                 then blob blob of course value is where                                 we store the HTML itself the way these                                 clustered primary keys work in Cassandra                                 determines the layout of the data so the                                 first first component of that                                 parenthesized value is always the                                 partition key is what determines                                 placement in the cluster the term is                                 distribution around around the network                                 in this case that is also you know                                 parenthesized value it's the domain and                                 title                                 it's like a                                                         domain title unique domain title                                 combination is used to do to determine                                 placement within the cluster these                                 subsequent values set up a mini to one                                 relationship between them themselves and                                 the values superior adjacent to the left                                 so for each unique domain title we can                                 have an arbitrary number of revisions                                 and for each unique domain title                                 revision we can have an arbitrary number                                 of these T IDs the renders and since the                                 value does not appear in that that                                 definition for each unique domain title                                 revision tid will be exactly one value                                 and this also sets up ordering ordering                                 of the data the order the data is                                 actually persisted on disk so that it is                                 ordered first by revision and then by                                 tid so if you select and limit one you                                 essentially get the most recent value                                 another way of looking at that is a like                                 this again for a domain and title pair                                 we can have one or more revisions and                                 for each revision we can have one or                                 more of the renders of that HTML this is                                 kind of important because this is the                                 way the data ends up being laid out on                                 disk at least in each table file the                                 data is laid out like this in sorted                                 order which is important because of                                 compression for example one thing that                                 all general-purpose compression                                 algorithms have in common is that they                                 look for repetition and attempt to store                                 back references to a single copy of that                                 repetitive material that works well for                                 us because most revisions most whether                                 the renders individual HTML renders of a                                 particular revision or the revisions                                 themselves is it is most typical that                                 the content alt changes between them                                 very little that the amount of                                 repetition is very very high so                                 Cassandra gives us this tunable chunk                                 length kb that determines the size of                                 data as it's being streamed off disk                                 that will be given to the compressor                                 implementation so by widening that chunk                                 length we're able to find more of that                                 petition and bring the compression sizes                                 down with with within reasonable limits                                 deflate has a fixed                                                    documents that exceed                                                   the HTML render you know that starts to                                 fall off rather quickly the benefits of                                 given this which is why we've been                                 looking at a compression algorithm                                 called brawley which is relatively new                                 brought to you by the folks at Google                                 we've written a cassandra implementation                                 for it I didn't have ready for this for                                 this talk and you know formal results                                 anything I really felt confident to                                 stand behind but the initial results are                                 very promising we've done quite a bit of                                 testing using settings that are                                 comparable that allow an                                 apples-to-apples comparison between                                 deflate and the results match up pretty                                 well with the you know publicly posted                                 comparisons of broadly to deflate which                                 is that you get smaller compressed sizes                                 at a lower computational cost so it's a                                 win either way but one of the things                                 that broadly will let you do is will let                                 you open that window up so now we can                                 kind of almost treat it like a binary                                 differ and one of the examples i'll post                                 these slides afterwards i have links to                                 some tickets here there's a little bit                                 more detail in this ticket on the slide                                 but one of the examples there were                                 documents that are two                                                  with a for Meg chunk length and a                                 corresponding brought the window size we                                 were able to get compressed sizes of one                                 point seven three percent of the                                 original which is really really good                                 okay so compression has been a pretty                                 important topic for us something we                                 spend a lot of time on impaction is also                                 something that's occupied a fair amount                                 of our time compaction and cassandra is                                 is basically the price you pay for                                 having a log structured storage engine                                 it's an asynchronous background process                                 that attempts to to keep up with the                                 with the rights that are occurring and                                 reorganize the data to make it more                                 optimal for the reeds cassandra presents                                 a metric a histogram called SS tables /                                 read essentially the number of SS tables                                 that need to be                                 contacted in order to satisfy reads and                                 we watch that very closely because read                                 latency tracks pretty closely with the                                 number of SS tables that you have to                                 sort of merge together to get the                                 picture of your your results and it                                 speaks to the efficacy of compaction so                                 with compaction at a minimum what you                                 want to do is get the total number of                                 files down you know minimize that you                                 know bound the number of SS table files                                 you have if possible you'd like to to                                 order them in such a way that the                                 results are closer together cassandra                                 has three different kinds of compaction                                 strategies sighs tiered levelled and                                 date tears sighs tiered is the simplest                                 it just combines files of similar size                                 together to create you know fewer larger                                 files but it's completely oblivious to                                 column distribution level compaction                                 which is the compaction strategy we                                 started out with is perhaps the most                                 optimized for read and breeds in like                                 the general sense it uses files of fixed                                 sizes and organizes them in levels where                                 each successive level has an                                 exponentially increasing number of SS                                 table files but the files within each of                                 these levels are non-overlapping so if                                 you imagine a simple point query that                                 tends to be the SS tables / read for                                 level compaction tends to be no worse                                 than the number of levels which is which                                 is very good it's the downside of that                                 and I the reason I say we used to use it                                 is that a the compaction throughput is                                 very high it's just constantly busy you                                 madly compacting data in the background                                 so it uses a lot of i/o which brings us                                 to day tiered compaction which tends to                                 gain the fact that we know the                                 timestamps of the data that was written                                 and so if your data set is total ordered                                 if your values that you're storing and                                 querying against are only ever                                 increasing then so is time and so if we                                 compact together data according to                                 similarity in time                                 no tears based on the date datetime then                                 that will you know also place your data                                 closer together so given the sort of                                 MVCC structure of our data model this                                 seems like an absolute way in like this                                 should be the one for us it has turned                                 out in practice though that that is not                                 the case i would try to explain in                                 detail why it is not the case but it's                                 i'm worried i would just get it wrong                                 it's very hard to reason about the way                                 dtcs actually works what i can tell you                                 is that the optimizations are easily                                 defeated by any kind of out of order                                 right and cassandra that is it is almost                                 no i don't want to use hyperbole but it                                 feels almost impossible to avoid you                                 know any any out of order right so read                                 repair for example is something we                                 usually rely on to keep the integrity of                                 the data read repair will create out of                                 order rights and it will essentially                                 essentially to feel the optimizations                                 provided here you know updates of the                                 data of any kind produce out of order                                 rights again i've got a ticket here if                                 you'd like to read like a more detailed                                 explanation of how it has failed in our                                 specific environment you can go look at                                 that but the the summary is we're                                 currently using dtcs and it does not                                 work well for us what we're planning to                                 do about that is win when these                                 optimizations are defeated you're                                 essentially left with one tier one date                                 here you know it's kind of flattens the                                 structure out and since dtcs does size                                 tiered compaction within within the                                 tears we're essentially running sighs to                                 your compaction so we may need to switch                                 to that to be to be clear about about                                 how this is working there's also another                                 compaction strategies that exists                                 outside the Cassandra tree right now                                 that is gaining a lot of popularity                                 called time window compaction strategy                                 that attempts to do to build the same                                 sort of optimizations that again you                                 know if you ordered by time and your                                 data set is meant to be total ordered                                 then it will place your results near                                 each other so we're planning to take a                                 good look at that one of the things we                                 may just do is just lower note density                                 just                                 reduce the size of the individual                                 Cassandra nodes by deploying more of                                 them and the reason that works is again                                 you're interested as as tables / read                                 and so if the efficacy of the compactor                                 strategy is poor and the number of SS                                 tables versus the total number of s's                                 tables that we read from is too high                                 then one way of solving that is just to                                 simply reduce the total number of SS                                 tables which reducing the data set size                                 will do alright so moving on garbage                                 collection it's a Java app it's latency                                 sensitive it's got a lot of throughput                                 so obviously garbage collection has been                                 an issue I should come as no surprise                                 which is one of the reasons that we have                                 been fairly early adopters of G                                       been using it for quite a while now g                                  is short for garbage first it is meant                                 to be a successor to concurrent mark                                 sweep is different in that it is an                                 incremental parallel compacting                                 collector so it's always kind of looking                                 for the low-hanging fruit and copying                                 regions from one location to another to                                 keep keep the heap continually compacted                                 and it works really well it works in                                 fact it works fantastic particularly if                                 you can give it a little more memory                                 than what you might normally need it can                                 kind of paper over some of the some of                                 some of the sort of impedance mismatches                                 between the way an application that                                 Cassandra works and a generational                                 theory the one exception the one problem                                 we have had is is what the you know this                                 is this is the the g                                              humongous objects is with these yes its                                 so-called humongous objects so what g                                  does is at startup if you have not                                 explicitly specified otherwise it                                 divides the heap up into it makes its                                 own determination to divide the heap up                                 into some number of regions of some size                                 if the objects is your allocating to                                 these regions are one half the size of                                 the region or larger than it sort of                                 special cases that it treats them                                 exceptionally it will only allocate one                                 object / humongous region and these                                 humongous regions have to be                                 contiguous so if you have too many of                                 them then they will you know they'll be                                 the usage of the heat will be very                                 inefficient you'll waste a lot of heap                                 and you will fragment the heat they're                                 meant to be treated exceptional so the                                 number of them should be exceptional in                                 our case we saw with                                                   saw region sizes being calculated for                                 mags which meant anything to Meg's or                                 larger was being allocated humongous                                 they were a bit too many of them and it                                 kind of destroyed the efficiency of the                                 the collector so we have since set that                                 up to eight that's something to keep in                                 mind enable GC logging have a look at                                 that it's pretty obvious when you're                                 getting too many too many human mungus                                 allocations okay so on node density if                                 you talk to anybody in the Cassandra                                 community pretty much everybody who has                                 any experience with Cassandra will                                 always recommend that if given a choice                                 you should run more nodes of a smaller                                 size than you know larger larger nodes                                 and fewer larger nodes it's not really                                 documented anywhere you know it's not in                                 any of the formal documentation that's                                 probably because rightfully so anybody                                 who says this ought to be ashamed it                                 kind of it kind of ignores the realities                                 of growing hardware you know hardware is                                 becoming more and more capable and you                                 know data center costs that you can't                                 vertically scale across available                                 hardware issues that we've had running                                 nodes that were to dance are quite                                 numerous but I'll just limit into the                                 ones that I've discussed so far again if                                 the efficacy compact compaction is not                                 good then lowering no density lowers the                                 data set size under control of a single                                 node and that means that you know the                                 number of SS tables contacted will be                                 smaller also the throughput this is                                 created by compaction and it will be                                 less and if compacted the throughput for                                 compaction is high along with all the                                 rest of the throughput that's higher                                 will in turn make GC works if you're no                                 density is too high so we've had                                 problems with no density being too high                                 and had that creep issues for us and                                 we've had to deal with it and that's why                                 on that earlier slide I mentioned                                 we have three times as many Cassandra                                 nodes as we do host to run them on and                                 how we deal with that is we spin up                                 processes so we use puppet in our                                 environment so we just use puppet to to                                 create you know multiple configuration                                 directories that reference multiple data                                 file directories and we have multiple                                 systemd units that just spin up multiple                                 processes with no effort to limit                                 contention or anything and to make                                 matters worse we have them all running                                 on one single shared raid                                               have quite the impressive blast radius                                 going on here we lose one disk we lose                                 the array we lose you know five                                 terabytes of storage and                                              notes go down probably not the best what                                 I wish we had done was used a mechanism                                 you know design for no allocating                                 hardware resources to the processes like                                 virtualization containers just                                 purchasing blades would have been better                                 i think you know to get to keep that one                                 to one host parody if necessary anything                                 but about what we did probably and so to                                 kind of wrap things up what has worked                                 well for us with Cassandra in general is                                 sort of the core features the things                                 that differentiate Cassandra from from                                 the other databases are still you know                                 still make it the right choice the way                                 to nabol consistency works to let us                                 trade away you know point in time a bit                                 consistency in order to buy availability                                 the multi data center awareness you know                                 the visibility it's very well                                 instrumented it makes it you know rather                                 rather easy if you if you need to dig in                                 and troubleshoot something it's getting                                 to the point where kassandra's reached                                 that kind of critical mass and there's a                                 ubiquity there it makes it you know much                                 easier to find you know libraries and                                 supported applications and you know to                                 find help if you you know you search the                                 web or you know to find people who are                                 willing to share their experiences all                                 in all it has been you know if I sounded                                 critical at any point it has worked out                                 really well and                                 so that is just attributed as sort of                                 the core the core functionality what                                 isn't so good is usability and I                                 probably would have balked at this a                                 year ago I'm sure you know many of my                                 colleagues within the project would balk                                 at it and probably rightfully point out                                 that usability is so much better than it                                 used to be that actually makes it worse                                 because usability is still pretty bad                                 that just means it was worse it would                                 have one point compaction is maybe this                                 is perhaps getting over critical but                                 compaction is a pretty good example the                                 fact that you have to worry about this                                 at all i think is you know you know that                                 it's more than simply implementation                                 detail it's kind of bad from the                                 usability standpoint that you have to                                 not only understand your data model in                                 your access patterns which i think is                                 reasonable but you also have to                                 thoroughly understand those compaction                                 strategy algorithms and then and and you                                 know really get low level into the you                                 know to the to the analysis of how                                 they're working in the trouble you know                                 the troubleshooting and tuning feels                                 very much like in any other application                                 like that ought to be an implementation                                 detail not to say that it would be                                 that's an easy problem to solve but it                                 does make kassandra's not that not that                                 usable controlling a streaming for you                                 know like bootstraps and d commissions                                 and the sort of things really like crazy                                 wonky I mean your concurrency you're                                 available throughput is a function of                                 how many nodes are available to stream                                 like there's no per node concurrency so                                 you can either overwhelm the node if you                                 don't throttle it down or you can end up                                 you know with terabytes of data to                                 transfer it just a few megabytes per                                 second and the only option would be to                                 increase the number of nodes that's                                 that's bizarre to me but it took you                                 know like again getting on the other                                 side of the fence before I realized that                                 was even an issue jmx is a miserable way                                 of having to interact with the system                                 for monitoring and management and you                                 know full disclosure I probably had more                                 to do with this decision and it really                                 it is Cassandra than anybody else you                                 can probably lay this at my feet more                                 than                                 more than anyone elses but it just means                                 that everything has to know it's kind of                                 a baroque you know way of dealing with                                 you know way of coding against Cassandra                                 anyway but it's very very much limited                                 to Java so you know this is the kind of                                 thing where I think you know if it were                                 open to dynamic languages you probably                                 probably see a lot better tooling for                                 Cassandra at this point and then as I                                 mentioned the the vertical scaling story                                 is just awful we run                                                  with                                                             terabytes of disk space and we simply                                 cannot fully utilize that hardware in a                                 good way in a performant way what we're                                 forced to to run more than one cassandra                                 process on the machine just to be able                                 to to fully utilize that hardware and                                 then to get away from the bad and into                                 the ugly upgrading between major                                 versions has been really really painful                                 it has always been painful I was kind of                                 surprised to see that it's just still is                                 painful some of this is kind of just                                 normal quality control issues things                                 that somehow managed to get by without                                 getting fixed others are just like                                 there's just no excuse for you know like                                 what seems like wholly arbitrary changes                                 to metric names for example so you have                                 to go in and retool all your graphs                                 because I don't know because somebody                                 wanted to change the case of a you know                                 the capitalization of a metric name or                                 something really painful the upgrade                                 process I know a lot of people have                                 related to me that they don't even                                 consider Cassandra upgradeable I                                 wouldn't go that far but I do know more                                 than one person who's actually said that                                 it is their policy it has been their                                 policy since the beginning and are                                 standing by it that they would just                                 stand up a new cluster in migrant to it                                 and then the release process or that you                                 know or the quality of releases is bad                                 it's always been bad it's kind of been                                 you know the folklore that sort of the                                 the passed down knowledge for people                                 who've been around only to upgrade                                 unlike the sixth release you know just                                 sit out the first five because there's                                 going to be lots of problems let other                                 people shake that out                                 like so bad to the point that there's                                 nobody shaking out those bugs because                                 everybody is waiting for the dot the dot                                 six and the problem here has always been                                 you know one of trying to balance the                                 amount of features in the churn in the                                 code you know with with those bug fixes                                 and maintaining a stable stable                                 trajectory so we adopted this new                                 release process modeled after Intel's                                 tick tock that makes even-numbered                                 releases mean one thing an odd-numbered                                 releases mean another and feature more                                 feature type stuff goes into one and                                 more bug fixes going to the other and if                                 this has done anything to solve the                                 problem I don't think you know I don't                                 think you could prove that one way or                                 the other all it's done is obfuscated                                 where that sweet spot is you know where                                 it is that where the stability that                                 you're looking for lies and anybody that                                 disputes that you just look on the                                 Cassandra mailing list at all the people                                 who ask you know what version should I                                 be running is you know it's maddening                                 with trying to trying to figure that out                                 so that is definitely not that is                                 definitely not worked out in my opinion                                 and so with that I think I have a few                                 minutes left for questions if anybody's                                 got them thank you                                 any questions okay um I was wondering                                 how painful scaling out like adding                                 nodes to a cluster is for that's easy                                 yeah feeling out horizontally is easy                                 but the impact of transfers or anything                                 like that when between notes when you                                 add in one does i have as i repeat that                                 last part does the impact of transfers                                 from one node to another when you add                                 nodes have any impact or well they don't                                 need to i mentioned from usability                                 standpoint streaming if you're using a                                 rack where data center set up like we                                 are what Cassandra does when it builds a                                 stream plan for a bootstrap to add a new                                 node is it looks for you know the                                 minimal number of nodes that have the                                 replicas it needs that are nearest to it                                 so if you're running in a you know                                 rocawear data center where's                                 configuration that's the nodes they're                                 in the same same rack so if for example                                 you have two nodes or you're                                 bootstrapping in the second node your                                 stream concurrency is one and if you're                                 using compressed compression then that                                 one thread can become bottleneck on cpu                                 utilization we see about four megabytes                                 per second if you have several terabytes                                 of data to transfer at four megabytes                                 per second going to take a very long                                 time to do the only way to get that                                 through put up is to have more nodes in                                 that rack that can be involved in the                                 stream but if you have too many than a                                 u.s. it could be overwhelming you know                                 it could you could you know the stream                                 concurrency then goes up but that's okay                                 because you can throttle that down by                                 adjusting the outbound throughput of all                                 of the nodes that are streaming to the                                 node so if you're bootstrapping new                                 nodes into into iraq you have to change                                 all of the outbound throttles all the                                 outbound throughputs as you add each new                                 node and the total available through                                 prayer changes so yeah the usability                                 story this is a mess there                                 okay any other questions hi it looks                                 like you've had your share of problems                                 with Cassandra if you were to do it all                                 over again would use shoes Cassandra or                                 would you choose anything else yeah I                                 don't really know you know again some of                                 the features that were important i mean                                 we're trying to hit certain performance                                 targets and we're trying to do it at a                                 certain costs and we're you know we're                                 trying to you know to we're trying to                                 end up with it you know like an active                                 active set up across multiple data                                 centers and we're trying to maintain you                                 know certain availability and i don't                                 know of any alternative that would have                                 solved all of those you know in any                                 better better way so i don't know that                                 it changes the you know that it changes                                 you know what i would do if i could go                                 back again oh I also came into this                                 after it was already deployed so it                                 wasn't something it wasn't a decision                                 that I influenced either way but I don't                                 really know of an alternative that would                                 have been better of course if somebody                                 knows of an alternative it'd be better                                 okay thank you                                 to what extent do you think are the                                 vertical scaling problems problems that                                 you experience are attributable to Java                                 rather to Java than to Cassandra itself                                 because the garbage collection and the                                 for example the point of compressions                                 and problems that every Java process                                 probably has yeah definitely I think                                 there's there's kind of an impedance                                 mismatch between an application like                                 Cassandra and I you know any garbage                                 collected languages you know people                                 pretty quick to jump on Java you know                                 and GC issues but i think it probably                                 has one of the most robust you know set                                 of garbage collectors of any language                                 but it's just kind of a you know                                 mismatch between that kind of                                 application and anna garbage collector                                 and garden you know generational garbage                                 collectors nor operate on the premise                                 that everything is either very very                                 short-lived or very long-lived perhaps                                 some of the lifespan of the application                                 I kassandra's got a lot of middle ground                                 there it's got a lot of sort of medium                                 lived objects that kind of get you have                                 the potential to be prematurely tenured                                 out of out of young Gen and you know                                 which really screws up the efficiency                                 and then on top of all that it's                                 extremely latency sensitive because it's                                 a database so you know even if the piles                                 times aren't really that bad under you                                 know like for most typical applications                                 that you know just just inconsistent                                 pause times as a problem for database                                 even if the pauses themselves aren't                                 like that aren't that bad so yeah some                                 of that is definitely JVM and cassandra                                 has done a lot of there's been a lot of                                 interesting you know like off heap                                 optimizations in Cassandra a lot of a                                 lot of good work to make that less of a                                 problem we are at the top of the hour so                                 is it short question that you've got                                 okay that's well maybe trivial well I'll                                 do search within mckee Wikimedia so I                                 was that implemented I'm sorry I did                                 search whole searching in Wikimedia                                 commedia implemented can search me                                 implemented in Cassandra and not not                                 really I mean                                 I believe the question was what                                 technology is used for the search                                 functionality in Wikipedia oh yeah                                 probably not the best person to ask                                 another resistors a I believe you know                                 in mediawiki it's it's serious search                                 but you know under the Wikimedia                                 infrastructure we also use lastic search                                 okay we need to give time for next                                 speaker to set his session up thank you                                 Eric once again and if anyone wants to                                 talk
YouTube URL: https://www.youtube.com/watch?v=xIpiGLdIQzU


