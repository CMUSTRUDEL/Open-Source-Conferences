Title: Berlin Buzzwords 2015: Andrew Psaltis - Going deep with Spark Streaming #bbuzz
Publication date: 2015-06-04
Playlist: Berlin Buzzwords 2015 #bbuzz
Description: 
	Today if a byte of data were a gallon of water, in only 10 seconds there would be enough data to fill an average home, in 2020 it will only take 2 seconds. The Internet of Things is driving a tremendous amount of this growth, providing more data at a higher rate then weâ€™ve ever seen. With this explosive growth comes the demand from consumers and businesses to leverage and act on what is happening right now. 

Without stream processing these demands will never be met, and there will be no big data and no Internet of Things. Apache Spark, and Spark Streaming in particular can be used to fulfill this stream processing need now and in the future. 

In this talk I will peel back the covers and we will take a deep dive into the inner workings of Spark Streaming; discussing topics such as DStreams, input and output operations, transformations, and fault tolerance.  After this talk you will be ready to take on the world of stream processing using Apache Spark.

Read more:
https://2015.berlinbuzzwords.de/session/going-deep-spark-streaming

About Andrew Psaltis:
https://2015.berlinbuzzwords.de/users/andrew-psaltis

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              hi my name is Andrew selfish thoughts                               going to be about going deep with spark                               streaming it's kind of an outline of                               this talk got their introduction talk                               about d streams which is a fundamental                               component of spark streaming talk about                               how we think about time and streaming go                               over a little about recovery of all                               tolerance and then conclude and have                                questions quick little bit about myself                                I'm a data engineering shutterstock                                outsider shutterstock sometimes ramble                                on Twitter not often author of streaming                                data I started dreaming about streaming                                since about two thousand eight and                                working with streaming systems speaking                                provide content for skillsoft and                                outside of that I live in la crosse                                crazed family so why streaming this is a                                great quote from a Danice and do that                                without stream processing there's no big                                data and no internet of things it's                                really kind of interesting that this is                                a little device from Texas Instruments                                call it smart tag it's got ten different                                sensors so everything from Mike                                temperature accelerometer all sorts of                                movement so ten of them in here $                                     US dollars for this little device a                                little chip if you will if you haven't                                art enough cases this last day and a                                half as to why streaming here's a couple                                more operational efficiency one extra                                mile an hour on a locomotive on its                                daily route can save                                                    in US dollars and that's when Norfolk                                Southern and it's not that they can't                                get the locomotives to go fast enough                                it's the logistics of getting them in of                                the yard and routing them from the EU                                Commission bit congestion accounts for                                about a hundred billion euros or one                                percent of EU GTP annually so improving                                these things with streaming systems                                becomes not just advertising but real                                money so I think we have a shared                                problem here either today of a byte of                                data was a gallon of water here we could                                fill an average house in                                              about twenty twenty it'll take only two                                so the goal of this talk is to really                                kind of go through how perhaps heart                                streaming could help us solve this                                problem                                and this is only going to continue to                                grow and it's not the data that we're                                producing browsing the web or doing                                social things but stuff like this thats                                everywhere and becoming more pervasive                                so if you haven't used spark stream it                                just kind of quick little background on                                it so the typical stack you see with                                spark spark streaming just being one                                component of it provides an efficient                                fault tolerant staple stream processing                                provides a simple API integrates with                                sparks batch processing and the other                                libraries okay so you could use mlib                                could use graphics you could use sparks                                equal along with the spark streaming                                today we're just going to focus on spark                                streaming is a high level architecture                                of how it all works so in the far left                                you have your program that's running                                it's a driver no different than a                                regular SPARC program it gets sent out                                to the workers and that's where your                                algorithm runs and your sports streaming                                job is running and then you have data                                sources that are coming as input okay                                those could be anything from Twitter a                                tag a device network whatever it is you                                want to consume data from saudi streams                                discretized streams are the basic                                abstraction provided by spark streaming                                in the way it pretty much works we look                                at it is if this is time going across it                                really is just a little micro batches of                                our DD is being generated as time is                                progressing okay as three things that we                                want to do with them and this is really                                similar to the three things we're going                                to do with almost any data processing                                system right we want to be able to                                ingest data we want to do some sort of                                transformation or something on it and                                then we're going to want to output it                                somewhere to something ingestion of d                                streams there's three ways you could get                                 data in there's basic sources advanced                                 sources and those custom sources                                 the basic sources are things that are                                 built in to spark streaming come with it                                 there's nothing you have to include it's                                 things like the file system network                                 socket acha actors stuff that's not                                 built in but is available is things like                                 Avro and CSB there's a spark projects                                 site out there that you can find other                                 libraries one thing to note about the                                 basic sources is they're not reliable or                                 ever think about it a file system or                                 reading a file or off of a socket it's                                 probably not a reliable operation the                                 advanced dip which streams on the other                                 hand are things like Twitter Kafka flume                                 Kinesis MQTT or other protocols that you                                 may see those all require an external                                 library not really a big deal but just                                 something to be aware of that you'll                                 have to get a library for that it's not                                 part of the core Spartan package and                                 depending upon the source these could be                                 the reliable or unreliable and we'll go                                 over the difference between them and how                                 you could do each one and then there's                                 custom input streams to implement a                                 custom input stream really just have two                                 things to do you have to input a dish                                 and input D stream and you have to                                 implement a receiver putting together a                                 custom input stream is pretty much as                                 simple as this you would extend this                                 receiver input stream you have one                                 method to get receiver and you're going                                 to return back the receiver and that's                                 really it                                 the receiver side of it is a little bit                                 more work involved in this case you                                 extend receiver there's an on start                                 method which has to be non blocking case                                 it can get called when it gets pushed to                                 your workers when it starts there's a                                 stop method and on stop clean everything                                 up and then there's a store method and                                 that's defined in the receiver class in                                 that store method is where you have                                 decisions about reliability                                 unreliability right kind of where the                                 rubber meets the road as to what you're                                 going to do so let's talk about the                                 receiver reliabilities as we mention                                 there's unreliable and reliable                                 receivers unreliable pretty simple too                                 intimate implement sorry no fault                                 tolerance but you're going to lose data                                 whenever the receiver fails you pretty                                 much just consume it you lose messages                                 you lose messages you want but pretty                                 simple to get up and running all right                                 you're going to consume data from a file                                 from a network socket really simple the                                 reliable receiver on the other hand                                 complexity could depend on the source                                 all right depends on where you're going                                 to try and read that data from what you                                 need to do it could have strong fault                                 tolerance guarantee so you can have a                                 zero data loss but the data source must                                 also support acknowledgment to have that                                 okay so you have a data source that does                                 not support acknowledgment you're going                                 to have trouble having this fault                                 tolerance and zero data loss it's kind                                 of a walkthrough of when you have a D                                 string input D stream and receiver                                 whether it's a custom one or those on                                 the built-in ones of how this kind of                                 works so when you submit your job to the                                 master it's going to go ahead and get                                 call that get receiver on your input D                                 stream it's then going to turn around                                 and send it to the spark workers so one                                 thing to note which is probably                                 potentially obvious here is that when it                                 does send it that receiver has to be                                 serializable right that's going across                                 the network to this other machine so                                 it's going to send this receiver you                                 receive its going to start running it's                                 going to consume data from                                 ever your data source maybe it's going                                 to call a store method that we saw right                                 that's going to store the blocks                                 potentially going to replicate the                                 blocks to other nodes and other workers                                 so now you have that data replicated                                 across if it's a reliable receiver this                                 would be your opportunity to say that                                 when these blocks were replicated you                                 could then acknowledged back that source                                 system that you process that data you                                 have it right because when this returns                                 your guarantee that data is now multiple                                 nodes and it's been persistent after                                 that happens then you return back to the                                 spark master and two drivers saying that                                 here's the block IDs that I have okay so                                 from that point it could then scheduled                                 jobs to get run okay if we remember                                 spark streaming is spark most part in                                 micro batches and some things are                                 different with the most part it's parked                                 with micro batches right like a half a                                 second this is two ways that we could                                 create a d stream one we saw some sort                                 of streaming source basic advanced the                                 custom one the others by doing a                                 transformation                                 creating a d stream via transformation                                 is really any sort of transformative                                 operation you're doing right you get a                                 look through the spark documentation                                 there's a whole bunch of transformations                                 that you could perform map flatmap and                                 so forth right so you'd have an input D                                 stream you perform some transformation                                 on it and you get an output D string you                                 can see these general classifications                                 just the standard ones that you'd expect                                 to see in spark and expect to see in                                 other places map count by value reduced                                 by key join and so forth and then                                 there's stateful operations window                                 updates state by key a transforming one                                 count by value and window and some                                 others and we'll go through a couple of                                 those in more detail so we look at                                 transforming input here we have a stream                                 that we created just call create custom                                 stream passing the streaming context and                                 then we have my streamed up map and do                                 whatever going to do there and get back                                 save these events if this is time                                 running across I recited every interval                                 we're basically producing an RDD right                                 so you'd have this batch here at time T                                 plus one plus two it's what every time                                 this runs you can get your input stream                                 you perform your map and you can get                                 your r DD coming out and this can help                                 it on every single time that that batch                                 is running                                 let's talk a little bit about stateful                                 operations the first one is update state                                 by key so this is interesting it                                 provides you a way to maintain arbitrary                                 state and continuously update it while                                 your jobs running say you had you know                                 in session advertising or in track                                 Twitter sentiment or there's something                                 else you want to do you want to the                                 state around as data is flowing through                                 this would be a way that you could do it                                 two things that have to be done in order                                 for you to take advantage of that one                                 you to define the state this could be                                 anything that you want it to be and yet                                 to find the update function and your                                 update function is going to need to know                                 how to update the state using the                                 previous and the current okay so it                                 could be your data whatever you want it                                 to be maybe you're tracking visitors are                                 tracking sentiment and as the sentiments                                 changing and you need to have a way that                                 you could update it and pass in the                                 function that's going to perform that                                 operation the other thing that's                                 required is the check pointing has to be                                 configured if you think about this is                                 keeping state as a job is running and                                 you're going to want to have checkpoints                                 going right so that you don't lose data                                 here's an example of using it let's say                                 we wanted to do that sentiment right we                                 want to keep track of per user mode of                                 state and want to update it with his or                                 her tweets as they're flowing through                                 the system meant to say we have an RDD                                 that's called tweets we're going to call                                 update state by key we pass in this                                 function that's going to be update mood                                 and takes a tweet okay so update mood is                                 going to take the new tweets and the                                 last mood and produce the new mood                                 kisses good see this is going along each                                 batch here as it's executing right we're                                 going to be updating these moods and                                 keeping them all right so every batch                                 interval that's running we're going to                                 have these moods that world and onto and                                 constantly updating as its flowing the                                 other stateful operation that requires                                 more discussion is transform so                                 transformed allows you to do arbitrary                                 RDD to our DD functions on your D string                                 so you can really do anything you want                                 so this transform takes in some                                 transform function that produces an RDD                                 returning by that d stream so if you had                                 it like we say have here they want to                                 eliminate noise words from crawl                                 documents right say we sucked in data                                 from Hadoop we have this noise words our                                 DD and then we have this crawled corpus                                 that we're crawling and we want to                                 transform it and get rid of noise all                                 right this is adi stream that's here                                 we're going to call transform on it for                                 every RTD in that batch we're going to                                 basically call join on that nose word                                 nose noise word our DD and then filter                                 mask you do whatever you want there this                                 could be a regular old r DD that you got                                 from anywhere ok so it gives you an                                 opportunity to have historical data that                                 you're combining with a stream ok to                                 note when this gets called this gets                                 evaluated and this joint or whatever it                                 is you're going to do here                                 is going to get evaluated on every                                 single batch run so if you want it to be                                 updating historical data as the day's                                 going along you'd have an opportunity to                                 do that here so you could be updating                                 historical data you could have a stream                                 going and you could transform that                                 incoming stream with the historical data                                 maybe tack on to it maybe it's the                                 weather maybe it's a visitor or                                 something that you want to do and tack                                 on to and we saw on the previous screen                                 that we have that join that we're using                                 you could also do a join on                                            the joint on the previous scream of the                                 transform again is an RTD to our DD in                                 this case we're going to join two                                 streams so say as an example you know                                 you have a group of users that are using                                 some Fitbit device and maybe an                                 application like say map my run and you                                 have two streams coming in and both of                                 them have some user ID on them then you                                 could call say this Fitbit stream until                                 to join with your map my run strain                                 that's going to turn around and if you                                 have to dees trims say Fitbit stream has                                 an ID and the data and let's say the map                                 my run stream has an ID in the data key                                 and the value what you're going to get                                 coming back from this joint is a key                                 that has that user ID that was found in                                 both with a tuple that has the values                                 from each okay so now you've joined                                 those two streams of data as it's                                 flowing you can't do this with addie                                 stream at an RDD you have to do                                 transforms that you're looking at each                                 rtd in the batch is after we've ingested                                 data we've transformed the data and then                                 we're going to want to output the data                                 okay so let's just say soon we create                                 this customer stream again we perform                                 some sort of map and then we're going to                                 perform some sort of action right if you                                 spark before sparks trimming is the same                                 way right it's very lazy all right it's                                 going to do everything that                                 you do as a transformation is really                                 just a recording of yep need to do that                                 need to do that need to do that but                                 nothing really happens into actions                                 performed so when we hit this for each                                 right we have some action that's                                 performed so the same thing we're going                                 across these batches of time can be                                 looking these days r dds can perform                                 some map we're going to get ahead and                                 have this events d stream and then we're                                 going to eventually do this for each rtd                                 thing to keep in mind with that for each                                 and you're outputting data right you                                 want to make sure that that's non                                 blocking and it's performing because                                 that's going to be writing to some                                 external system and so you start to hold                                 things up if that's not performing ok so                                 that's going to pull in data and start                                 writing somewhere                                 so this is the representation of that                                 little spark streaming job of how you go                                 from this D string to that spark job                                 rights we create this input stream we                                 perform some sort of map form some sort                                 of count and then we have a for each                                 which is the action under the covers                                 that's converted into a spark job for                                 each execution of the batch so you're                                 going to have an action that caused it                                 at                                                                      and you can have a map and you have the                                 input so in every batch cycle that's                                 what's going to end up happening so you                                 go from here to here and this is what's                                 executing                                 let's talk about time a little bit                                 there's a couple of things that need to                                 think about when thinking about time                                 windowing which you've probably heard                                 discussed in any of the streaming talks                                 you've been to in the last day and a                                 half the notion of string time versa                                 vent time and then how you deal with out                                 of order data                                 is a couple of common windowing types                                 that you see out there a tumbling and                                 sliding spark streaming today only                                 implements sliding windows for better or                                 for worse that's where it has there's                                 ways to work around somas limitations                                 but that's the windowing that's there                                 and will step into that so let's first                                 look at tumbling just so we have some                                 perspective of the difference between                                 the two so in a tumbling window and this                                 would be like a tumbling count the                                 window size is fixed and the data is                                 evicted from the window at a specified                                 size right so if we set here that the                                 window length is going to be say one                                 second whatever data came in that's it                                 it's ejected okay and that doesn't                                 matter that's considered the window be                                 in full and the next window okay                                 this is tumbling in a temple way and I                                 think I so the wrong thing to you on the                                 slide before the tumbling count is based                                 upon the size of the window not the time                                 of the window okay so in this case say                                 we have a window size of two it's going                                 to be two elements in the window and                                 then it'll be a new window that gets                                 created temporal windowing again in                                 tumbling this is going to be based on                                 time so we have this being several                                 seconds it doesn't matter whether                                 there's three elements of data or five                                 elements of data you're going to have a                                 window that tumbles at every demarcation                                 of time that was sent the sliding window                                 is what's provided in spark streaming                                 today and a way that this works is you                                 have an overall length of the window and                                 then you have an interval okay so we may                                 be looking at a window length of two                                 seconds but our sliding interval of when                                 we're going to execute is going to be                                 over one second so you'd see this data                                 for the length of the window but you're                                 executing in the interval and in this                                 case of vic shins happening as the data                                 is sliding as far as this window is                                 sliding across and time is continuing to                                 run then data is going to continue to                                 get evicted so a little bit different                                 than temporal way of doing it                                 there's two types of sliding window in                                 that spark shimming supports there's non                                 incremental and incremental that's just                                 a slight twist on sliding window so non                                 incremental is the easiest way in this                                 case we have this reduced by key and                                 window we're going to pass in a function                                 that's going to go ahead and aggregate                                 the data and we're going to tell it that                                 we want a length of five and that we're                                 going to run every second so you can see                                 as this happens we're doing interval                                 counts every time and then go ahead and                                 sending them up because you can see how                                 that could get expensive over time                                 incremental sliding is a little bit more                                 efficient right instead of having to do                                 the aggregate for the previous time of                                 our window we're only having to look at                                 at first time and the time we're at now                                 the caveat is you need to provide an                                 inverse function to be able to do that                                 okay so if we look at this previous                                 slide but we just need to provide an                                 aggregate of how to get to this number                                 by aggregating days but when we have                                 incremental right not only do we need to                                 know how to aggregate it but we need to                                 have it basically you know subtract it                                 right do the inverse of whatever that                                 computation is so at that window we                                 could go from this value and this value                                 and produce the right some butter rid of                                 the outputs going to be so if you could                                 do it and if your problem supports you                                 doing that the data you're storing that                                 could be much more efficient way of                                 having the window lang than having a non                                 incremental                                 so steak mode talk about string time                                 verse event time it's extreme time but                                 is the time when the record arrives into                                 the streaming system so when spark                                 streaming gets the data that stream time                                 right all the windowing is based upon                                 the time it receives the data event time                                 is a time that the event was actually                                 generated at so if I have this you know                                 smart tag here and it's sending data                                 pulses when that's getting created                                 that's the event time that may end up in                                 Kafka and they're getting sucked into                                 spark streaming when it gets into spark                                 streaming again that's going to be                                 streamed time and you can have the same                                 thing in Kafka unless you set up your                                 data to all go to one partition right                                 that the guarantee is all the data is in                                 order in the partition that you're                                 reading it from but that has nothing to                                 do with the time that your data was                                 actually created right so you have the                                 same type of thing so in the case of                                 spark streaming you're dealing with                                 string time so if you wanted to be able                                 to do computations based upon when the                                 event was created you're going to have                                 to be conscious of that and make sure                                 you're handling it correctly because as                                 you're scrolling across those windows a                                 time there's nothing to do with when you                                 created it and so that's where a                                 tumbling window or other windowing would                                 be able to help where you could say you                                 know what's that the window based upon                                 this time stamp on my data and have the                                 window based upon that not based upon                                 the time that the data was received into                                 the system other thing to think about is                                 out of order data in some applications                                 it doesn't matter in other applications                                 it's a big deal of whether the data in                                 order out of order okay again this is                                 left up to you to figure out how you're                                 going to handle that situation as the                                 data is flowing through your system from                                 sparks streaming standpoint if the data                                 source you're reading from is providing                                 the data in order it's an order right                                 but it really has no understanding as to                                 what that data is                                 here's a way that you could handle out                                 of order data so say we want to track ad                                 impressions between some time and time                                 plus i right you could have this                                 interval count that you're keeping and                                 keeping track of this state right so you                                 always be keeping track of what it was                                 you know you have this sliding interval                                 here of T plus                                                       these interval counts for that time plus                                 time plus                                                                you go along this is based on this                                 continuous analytics / discontinuous                                 streams so if the problem that you need                                 to deal with that's a pretty interesting                                 paper to take a look at and you can                                 think of ways that you could apply this                                 to your data by following this basically                                 keeping the count the current count and                                 the previous count as you're going along                                 let's take a moment to look at the check                                 pointing in part of the recovery and                                 fault tolerance this is two types of                                 check pointing that you can turn on in                                 spark streaming one deals with metadata                                 check pointing and one deals with data                                 check pointing right so if we imagine                                 this being our application running and                                 we're keeping all this state at every                                 time interval we want to make sure that                                 if something goes south which is going                                 to that we don't lose data                                 so without having checkpointing it looks                                 just like we saw in the previous slide                                 all right we're going along we have the                                 state that's building up and if                                 something fails on our application we're                                 going to lose that state information                                 when you enable check pointing at                                 regular intervals it's going to write to                                 some Brazilian store it could be HDFS it                                 could be s                                                               data somewhere in the case of metadata                                 checkpointing which is writing                                 information about the lineage and just a                                 metadata of what's running in the case                                 of data will see that we're writing data                                 okay so that we know what's being run                                 it's a balancing act if we're writing                                 too quickly to HDFS then that's going to                                 be a bottleneck if we run too                                 infrequently then the lineages that                                 we're building out right they are dd's                                 again just like spark right so you have                                 this lineage that's building imagine                                 these are running say every half second                                 you've got this lineage that's building                                 up over time if you're not writing                                 checkpoints often enough then that's                                 going to grow the default setting today                                 in sparks dreaming is the multiple                                 imaginable that's at least ten seconds                                 and the recommendation currently is to                                 have an interval of five to ten times                                 you're sliding interval                                 so fault tolerance the effusion layer                                 would spark and all the properties of                                 our td's that they're immutable their                                 deterministically computed and they're                                 distributed data set spark streaming has                                 all those properties that tag along with                                 it right there's two things that we're                                 trying to prevent we're trying to                                 prevent the failure of the worker that's                                 going to have all the data that's there                                 and we're trying to prevent the failure                                 of your driver that's going to have the                                 block IDs and the lineage information                                 and everything else that contained right                                 scheduling the jobs so trying to prevent                                 failure of both of those checkpointing                                 provides you a way to do that these                                 different semantics that you could                                 achieve as well we have at most once                                 right that at most the data is can be                                 processed one time or that message                                 coming through is once at least once and                                 then exactly once depending upon the                                 data source that you're using and the                                 provider you're using the receiver you                                 could achieve different levels of this                                 if you use write ahead logging and you                                 use a data source that allows                                 acknowledgment likes a Kafka and you                                 could get exactly once if it's an                                 unreliable than not right the places                                 you'll need to think about it or it's                                 going to be in your receivers in the                                 transformations that you're doing and                                 then in the output but if you spend all                                 this time getting data from say Kafka                                 making sure you're processing message is                                 one time if your output is not reliable                                 and it can't handle the same message                                 coming multiple times that's going to be                                 a problem right so those places to think                                 about it all along the path                                 so in conclusion just coming over you no                                 introduction just high level                                 architecture d streams and kind of some                                 under the covers about those talked                                 about how we think about time and then                                 just touch on recovery and fault                                 tolerance so thanks and take any                                 questions
YouTube URL: https://www.youtube.com/watch?v=vSSGhqWqP9E


