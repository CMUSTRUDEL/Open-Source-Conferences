Title: Berlin Buzzwords 2015: Stefan Savev - Using Random Projections to Make Sense of High-Dimensional ...
Publication date: 2015-06-03
Playlist: Berlin Buzzwords 2015 #bbuzz
Description: 
	It is hard to understand what is hidden in big high dimensional data. However, a moderate number of simple one dimensional projections is enough to answer hard questions about the data via techniques such as visualization, classification and clustering. Random projections have emerged as an extremely effective component of many algorithms for high dimensional data. For example, they are used in the context of nearest neighbor search (via locality sensitive hashing), dimensionality reduction and clustering. 

The goal of the talk is to give a pleasant journey into the rich area of random projections via many graphical illustrations and intuitive examples. We present how and why random projections work and where they break. We discuss several interesting properties of high dimensional data. For example, why data in high dimensions is likely to look Gaussian when projected in low dimensions; how to spot interesting patterns in high dimensional data by projecting into a lower dimension; and how to choose meaningful low dimensional projections. 

The method of random projections has a number of good properties: 1) scalability; 2) it reduces the machine learning problem to search and can take advantage of existing infrastructure; 3) it is relatively simple to implement and 4) it is robust to noisy data.

Read more:
https://2015.berlinbuzzwords.de/session/using-random-projections-make-sense-high-dimensional-big-data

About Stefan Savev:
https://2015.berlinbuzzwords.de/users/stefan-savev

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              Oh                               hi Jess                               about my                               working for research                               but this doesn't change anything so                               we're still gonna have a great time so                               I'm very excited to stay in front of you                               because I think you are a great audience                               to tell a great story and in this story                                I have incorporated systems from                                algorithm I having incorporated elements                                from systems algorithms a little bit of                                philosophy and even a joke and this is                                because these are multiple projections                                of the world and the talk is about                                random projections the talk is also                                connected to two big data and I think                                big data requires big questions and and                                big thinking so I have a big question                                for you so listen listen well so which                                do you think is harder is it search or                                is it machine learning so we'll revisit                                this question at the end and maybe you                                see another perspective in which you can                                answer this question we'll start with                                search and we start with text search                                because text search is is the most                                popular kind of search and why does text                                search work well it's not only because                                of the systems that have implemented                                text search engine but because of the                                properties of the data and if you think                                of textual data so this data has two                                important properties so text text search                                can work first the data is very very                                sparse                                                                some document this document is going to                                have                                                                  in the language and then the second                                reason that it works is that when we                                query we query with words which are very                                strong query features okay so this is a                                special data that's why that's why it                                works but if you switch the domain say                                images okay images have have different                                proper                                so first data is not so sparse so for                                example here's the image of the digit                                  so it has                                                              that carry information but you try to                                search similar images to this image now                                you cannot just take                                                text search you can take                                               images you can't take                                                  are properties that are fundamentally                                different and there is basically                                different kind of data okay so to                                position the talk I choose to think of                                two kinds of search one search is for                                sparse data where the query tends to be                                short and this is keyword search solved                                with the inverted index now if your data                                is dense and your query is long for                                example the image it's going to be a                                long query with lots of pixels now this                                requires a different different search                                method and the random projection method                                it turns out that it can work for dense                                data not so good for sparse but there                                are some use cases for the dense data                                including image search semantic search                                and recommendation systems so my my my                                plan is to walk you first through some                                for some basic concepts about                                representation of data and this is                                material we're going to just go through                                to put everybody on the same page and                                because it should be familiar but then                                we revisit this algorithm of random                                projections and third we actually                                demonstrate how it works on on real data                                so let's get started                                so we have an image and image to the                                computer is just pixels and if you are a                                data guy you take the pixels and you and                                you create a vector okay so if these                                Peaks if this image has                                               here would be                                                          be a long vector if you work with a data                                set with lots of images you take each                                image make it a vector and then stack                                the vectors on top of each other make                                matrix so this should be this is pretty                                much common currency in the                                the world so people were working with                                matrices so same story for documents you                                can represent them as vectors the                                dimensions are the words but there's a                                fundamental difference to these data is                                 sparse okay and the image data was was                                 dense if you get another kind of data                                 click data from your website it's a gain                                 can be represented vector so this vector                                 is going to represent the user who                                 clicked and the user clicked on the                                 first book so we're going to write one                                 in this vector okay so two kinds of data                                 going to the beginning and dense data                                 represent with dense matrix and sparse                                 data represented with sparse matrix so                                 one you can search with loose in the                                 other you cannot search now people in                                 especially in recommendation systems                                 like to take these sparse matrix and                                 make it dance okay so once you make it                                 dance                                 you cannot searched anymore okay so why                                 do people choose to make the sparse                                 matrix dance by the way this has a name                                 called dimensionality reduction and here                                 is a very simple method to accomplish                                 dimensionality reduction simply find                                 words like beer and wine that are                                 correlated or somehow semantically                                 similar and they are represented by                                 these two columns and then merge that                                 comes together now if you merge a lot of                                 columns together you're going to end up                                 with matrix that is with fewer columns                                 but the information density is higher so                                 we have condensed your matrix okay so                                 why do we want to do this well if this                                 example is not convincing let's look                                 what happens to two images so we have                                 image if you take images and you run                                 some dimensionality reduction algorithm                                 then this dimensional reduction                                 discovers patterns for example four four                                 three it discovers that three can be                                 created by by this by superimposing                                 these two patterns and by the way these                                 are not the only patterns that that are                                 needed to represent the three but these                                 are one of the most dominant so when                                 people say dimensionality reduction they                                 actually want to do pattern this car                                 and when they do this dimensionality                                 reduction the data gets dense now we're                                 reaching the method that I will talk                                 about so it has a fancy name random                                 projections and projection this is a                                 geometrical notion but if you look at                                 the the field of machine learning so                                 projection this is basically a dot                                 product if you take linear regression so                                 linear regression is a projection okay                                 if you take a tree it also uses multiple                                 projections so a lot of stuff in machine                                 learning is about dot products and about                                 projections and here it's doing this                                 stuff randomly okay so it's really                                 surprising that it works if you come                                 think this way but also what is                                 interesting here is that this is a very                                 foundational topic that connects a bunch                                 of learning algorithms like random                                 forest so are people familiar with                                 random forest ok support vector machine                                 ok can you connect the random forest to                                 the support vector machine these are                                 quite different ok                                 but there are some recent advances that                                 when then they represent the support                                 vector machines through random                                 projections and you can see some                                 connections this is very interesting and                                 on the other hand the SVD one of the                                 most capable algorithms for                                 dimensionality reduction they rely on                                 these techniques to make it scalable and                                 finally which is the main topic for                                 today is the nearest neighbor ok nearest                                 neighbor machine learning algorithm but                                 it's also a search so this is a machine                                 learning which you do through search so                                 there are connection between these two                                 different fields now in practice                                 previous slide I talked about the                                 theoretical foundations and the                                 theoretical interests about this method                                 but in practice some companies have                                 incorporated this method into their                                 systems for example                                 Spotify they developed a tool which is                                 available online and which I will                                 demonstrate they use it they use this                                 method for music recommendations okay                                 another company Etsy also uses this as                                 part of their pipeline you should also                                 know this random projection method                                 sometimes is referred as locality                                 sensitive hashing okay so this is                                 another view there are multiple views of                                 this projection one is geometric Oh                                 another is hashing and this makes it                                 very exciting                                 now we've reached the second stop for                                 our four for today so how does this                                 thing work we just know it's random and                                 it's a projection and let's see how it                                 works at the high level I said you want                                 to search through this matrix so if you                                 if the first row could be some user okay                                 it's the vector it's a vector which is                                 representing a user of your website and                                 every row it's a different user so if                                 you want to find more similar users you                                 can basically do brute force yes just go                                 scan through the matrix compare every                                 role to the query but what you can think                                 here you could and what I'm going to                                 show here you could do it a bit smarter                                 more efficiently if you take the                                 information from this matrix and                                 represent it as trees and trees are                                 searchable okay let's see how it works                                 if you have a data set that has only two                                 attributes now we are living in two                                 dimensions and two dimensions you can                                 visualize and the first row is red it's                                 a data point so I'm going to represent                                 it here okay it's this red dot now the                                 Green Row is a data point here okay so                                 every row is a data point now the search                                 in this framework is also known as the                                 nearest neighbor is the nearest neighbor                                 problem and you start with a query you                                 want to find the closest point to your                                 query so in two dimensions you can                                 visualize it but in high dimensions it's                                 very difficult and                                 so here's the first tip okay instead of                                 doing brute force on the whole data set                                 we can restrict the region around the                                 query and do brute force only in the                                 region so there will be two stages one                                 stage is candidate selection just select                                 somehow find this point in the circle                                 and then with brute force evaluate every                                 point okay but finding this neighborhood                                 is very hard so now we do another                                 approximation and the core idea is to                                 partition our space into into random                                 grits so these grits will be dynamic                                 okay so we put more lines more                                 partitioning where there are more points                                 and we do it randomly so there are a                                 bunch of reasons why I want to do it                                 like this we can take we can talk later                                 but one reason is that random is                                 actually cheap okay and later we'll see                                 there's also another reason why you want                                 to do it randomly okay so let's start I                                 put a random hyperplane and I split the                                 data set into two okay now going back to                                 this foundational concept of the of the                                 projection so what is the projection                                 well the projection is basically shining                                 a light perpendicular to this shining a                                 light parallel to the splitting line and                                 here it's like you have a random                                 direction and you put your data set on                                 this random line and this means a                                 histogram now in one dimension you can                                 look at the date okay so basically this                                 is the method it is nothing nothing                                 fancy linear regression does like this                                 works in exactly the same way it you can                                 think okay linear regression with lots                                 of attributes but in the end it's going                                 to project the data and it's going to                                 put the whole data into one dimension                                 okay and this reminds me of Plato Eric                                 allegory of the cave and here's the                                 philosophy component that I put in the                                 top so                                 Plato wrote this story about the                                 prisoners who live in a cave so their                                 change in the cave but there's a whole                                 world outside the cave which they can                                 only only see the shadow of the world                                 they can only see the projection of the                                 outside world on the cave wall so the                                 outside world could be three dimensions                                 but they can see only two dimensions so                                 the philosopher's goal is is to escape                                 from the cave and to understand the                                 outside world but they can only see in                                 two dimensions and now it's similar for                                 the data scientist okay so the data                                 scientists can see only this thing                                 okay but the good data scientists they                                 want to escape from from the cave which                                 by the way is known as the curse of                                 dimensionality in machine learning so                                 they want to escape from the cave and                                 and bring the light let's continue with                                 the algorithm partitioning into two this                                 can be represented as a tree so the red                                 points are in the red branch of the tree                                 the green points in the green branch of                                 the tree now you basically take this                                 idea and do it recursively                                 okay now you split again we end up with                                 four partitions let's run the whole this                                 method as simple as it is for a few                                 iterations to get a few what happens so                                 this is the second iteration we have                                 four partitions then third iteration                                 eight partitions now sixteen partitions                                 it's already very fine-grained so we are                                 going to stop and what's the purpose of                                 this well the purpose was we started                                 with this because we wanted to find the                                 nearest neighbors okay so a nearest                                 neighbors means close points when two                                 points are close they're going to end up                                 in the same partition even if you do it                                 randomly and I'm going to emphasize this                                 concept okay they're likely okay they're                                 not certain okay so when something is                                 likely to make it certain what you do it                                 what you do is you repeat it multiple                                 times and now from likely if you repeat                                 it enough time                                 it becomes certain and you could think                                 of each partitioning as exploring the                                 space of the possible neighbors so                                 here's the red point and we built one                                 tree and this tree explored the lower                                 left corner of the space okay but you                                 see their points that are close to the                                 query in the yellow part okay so what                                 about this point we're missing on them                                 okay then what you do is you run the                                 second tree and it explores a bit of                                 point some some points in the upper part                                 of the space a third tree explores a                                 different part of the space so basically                                 build more trees that's the conclusion                                 and if you run it and visualize it what                                 happens how this exploration goals okay                                 the more trees you add the more you                                 explore around but if you add a lot of                                 trees you now you are now certain to                                 have explored the nearest neighbor you                                 have seen them okay but if you have seen                                 too much stuff okay so now you are                                 spending too much computation on this                                 okay so next we we do a constraint so                                 you will we will constraint the we will                                 constrain the region in such a way that                                 it gets more so we can basically ask                                 okay we see neighbors so you run your                                 query through the trees and every tree                                 comes with with candidates okay so if                                 some candidates appear in more trees                                 they're basically more likely so this is                                 this threshold in how many trees does a                                 nearest neighbor candidate appear and if                                 you put a high threshold now from this                                 this region which is without any                                 threshold is going to shrink so now your                                 your region of nearest neighbors of                                 candidate neighbors shrinks and you                                 you're going to to because and you are                                 going to to actually because you have to                                 evaluate them fully with brute force so                                 one this region to shrink and this idea                                 it kind of reminds me of this joke about                                 about three statisticians so I'm going                                 to tell you the joke because I want to                                 remember this this method used in this                                 algorithm but also this method is in                                 many other algorithms so there are these                                 three statisticians they go hunting for                                 moose the first statistician shoots and                                 he misses one meter to the left the                                 second statistician shoots he misses one                                 meter to the right and the third one                                 well you think you'll shoot but he just                                 says good job guys we've got him because                                 he's averaging so he key point is when                                 you go hunting for data you know bring                                 some statisticians and you've got him                                 okay now we have reached our our third                                 stop which is to see how the the method                                 that that I described how this method                                 actually performs okay I put I choose a                                 data set that some people can be                                 familiar with this is from the website                                 Iago and these data set people used to                                 teach themselves machine learning in the                                 past people use this data set to develop                                 neural networks and other learning                                 methods and these data sets so what I'm                                 going to do is I'm going to use an image                                 search as a prediction system so put an                                 image okay this looks like a three could                                 be a three could be a five I don't know                                 so I want to find the label of this                                 imaging and one way to do it is                                 basically put it in a search system and                                 see what comes what comes he has labels                                 from our training set and then we say                                 okay the first game that is a five so                                 I'm gonna say this is a five so machine                                 learning actually works with two data                                 sets the test data set and the training                                 date the the training dataset and the                                 test dataset now because we're doing                                 machine learning with with a search the                                 so called nearest-neighbor                                 we have to do some indexing so indexing                                 means take the data and put it into an                                 index so roughly it looks something like                                 this python code so the key idea is that                                 every                                 which it has a vector okay which we use                                 for indexing and it has a label which we                                 remember because we're gonna need the                                 label later now for stage two the I                                 already described but you put in an                                 image to search and you get the label of                                 the of the top result okay and this is                                 Python code that looks like this by the                                 way this is very similar to the Spotify                                 too which uses Python so this code is                                 very close to what you can run now I use                                 the Spotify to mainly but I also                                 developed a tool on my own because I                                 want to get deep you know and make sure                                 that I understand everything because                                 when I come here and present you're                                 going to ask me tough questions I have                                 to understand everything so I developed                                 it on my own okay so I'm going to see                                 these two tools and this is also a open                                 source so Randy Spotify tool okay so the                                 Spotify tool in the Spotify tool I built                                 ten trees okay and each tree explores a                                 thousand candidates so this is a                                 thousand candidates that need to be                                 related fully it has a decent accuracy                                 ninety seven percent but it's very slow                                 okay five milliseconds per query okay                                 you can do better so how do you do                                 better where we take this dimension and                                 you bring it down using this                                 dimensionality reduction technique and                                 this is another bonus for this                                 dimensionality reduction technique that                                 it makes cheaper the data smaller and it                                 so it's cheaper to compute with this                                 data and once you did this okay                                 the search time went down                                     milliseconds per query but also the                                 accuracy went up and why is that because                                 dimensionality reduction as I said in                                 the beginning it discovers patrons so                                 the first line it worked at the pixel                                 level while the second line you could                                 think it it worked at the concept level                                 so this concept layer is                                 here is some patches from the image okay                                 now there's another bottleneck we                                 basically valuate fully to many                                 candidates okay and here I put my tool                                 and I got the time roughly half of what                                 I had before but I had to build more                                 trees now to be fair so it could be that                                 I didn't really do a lot of a lot of                                 tweaking on the on the Spotify tool it                                 could be that you could tweak it and                                 reduce these candidates and you and                                 these two could really even get lower so                                 this is this is quite quite possible but                                 at the main point actually here is not                                 about whether one two is better or not                                 so it's more about some algorithmic                                 heuristics and the there is some some                                 difference in the algorithms of these                                 two but kind of small difference and the                                 main point I want to carry across is                                 that okay if you tweak a system like                                 this you are likely to get good                                 performance and in many cases you're                                 likely to get good takers okay now we've                                 reached the question I asked from the                                 beginning so which is harder is it                                 search or or machine learning and                                 hearing this talk we present I presented                                 a system that searches dense data dense                                 vectors using these trees and this                                 indexing structure it looks the                                 following we have the tree and at the                                 bottom you have the point IDs okay so                                 when you search you're walking these                                 trees and you find some some candidate                                 points in you are evaluating them okay                                 and if you do machine learning you could                                 use the same thing however you could be                                 you don't have to store the food data                                 okay you could just what is important                                 for machine learning is predicting the                                 label so this label can                                 from some many neighbors and we can                                 aggregate so this is a key point here                                 that in the leaves we could store a                                 histogram so in this way of thinking you                                 could think okay this is search ok                                 search has to store more data and when                                 it searches it has to be precise and it                                 really cannot borrow what's called                                 statistical strength from from many                                 other points but if you do machine                                 learning okay you can aggregate your                                 data so this is cheaper and because you                                 you borrow knowledge from many data                                 points aggregating this histogram it's                                 actually it looks like machine learning                                 is easier than search so this is my take                                 away point and in conclusion I would say                                 that this method is useful when you                                 search for for in dense data and the                                 other application is that other                                 algorithms are using it to become faster                                 and this is the end thank you very much                                 you
YouTube URL: https://www.youtube.com/watch?v=V9zl09w1SGM


