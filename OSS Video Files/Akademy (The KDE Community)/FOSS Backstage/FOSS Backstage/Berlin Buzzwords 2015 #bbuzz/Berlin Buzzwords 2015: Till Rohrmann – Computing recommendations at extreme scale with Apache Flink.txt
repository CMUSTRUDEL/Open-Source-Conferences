Title: Berlin Buzzwords 2015: Till Rohrmann – Computing recommendations at extreme scale with Apache Flink
Publication date: 2015-06-02
Playlist: Berlin Buzzwords 2015 #bbuzz
Description: 
	Recommender Systems are a very successful application of large scale data processing. They are used to recommend new items of interest to users of a service, such as new movies on Netflix, or shopping articles on Amazon. Recommender systems have become an essential part of most web-based services to enhance the user experience. 

A powerful approach for implementing recommenders are the so called "latent factor models", a special case of the collaborative filtering techniques, which exploit the similarity between user tastes and item characteristics, which are automatically extracted.:

This talk details our experience with implementing three variants of the ALS (Alternating Least Squares) algorithm to train a latent factor model using the Apache Flink system and scaling them to large clusters and data sets. In order to scale these algorithms to extremely large data sets, Flink’s functionality was significantly enhanced to be able to distribute and process very large records efficiently.

Read more:
https://2015.berlinbuzzwords.de/session/computing-recommendations-extreme-scale-apache-flink

About Till Rohrmann:
https://2015.berlinbuzzwords.de/users/till-rohrmann

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              yeah hi everyone                               I'm till as you already know and I'm a                               think emitter these days working mainly                               on the the machine learning level of                               Apache flink and today I'm going to tell                               you a little bit about how to compute                               recommendations at an extremely large                               scale with a pet chief link as the title                               of my presentation indicates i have                                divided my target to three part in the                                first part I'm going to talk a little                                bit about recommendations in general how                                you do it with a collaborative filtering                                then I will introduce a petty thing for                                those of you haven't heard about it and                                last but at least I will fuse these two                                things and show you how to implement a                                recommender system which can deal with                                we really really large waiting matrices                                so let's get started first I guess that                                every one of you got somehow in touch                                with some kind of recommender systems                                because these days they are basically                                everywhere on the internet so no matter                                whether you buy some products in on                                Amazon or you you watch some serious on                                netflix or listen to some songs on                                Spotify there's some system and the                                background trying to analyze here are                                your preferences and based on these                                preferences gives you new suggestions of                                what you should consume next and well of                                course this is not done because of                                charity by the companies they they want                                you to improve the user experience and                                thus born the user strong a two to their                                product so that eventually the the sales                                and revenue is increased and due to that                                the whole recommendation systems                                recommender system thing is really big                                these days but how does it actually work                                what to give you a short example let's                                let's assume we want to recommend some                                websites to our customers and our                                customers is what are the king the Queen                                and the daughter the princess and they                                visit the three websites Amazon Facebook                                and Twitter so what we know is from our                                luck data                                that the King really likes to spend some                                time on Amazon because they can spend                                his wealth furthermore he he spends a                                good amount of his time on on facebook                                where he keeps in touch with the fellow                                kings and queens and what I can can brag                                about his latest conquest for example so                                and and additionally we know that the                                the time his daughter the princess                                spends on Amazon is also not too too                                little because apparently that the                                pocket money is not too bad either so                                from the the shared preference of the                                King for Amazon and and the the                                preference of the princess for Amazon we                                can deduce that may be that the princess                                will also look would also like to to                                spend some time on Facebook because the                                print the King heads of the same                                preference and so we could simply                                recommend Facebook to hear that's                                basically how collaborative filtering                                works the idea so to do um say a little                                bit more abstractly is if you want you                                to recommend items for and user a and                                what you do is you look for users who                                have shown in the past similar                                consumption behavior and based on what                                these group of people have consumed you                                can then make recommendations to to this                                user a and well how do you find out                                whether some users share a similar                                preference for for some kind of products                                well here's where the the latent factor                                models come into play which basically                                assigned for for each assigned to each                                user a set of preferences and to each to                                each item a set of characteristics which                                somehow describes on the the product and                                usually what is done is that that these                                kind of features are like represented as                                numeric numerical vectors such such that                                the scalar product of these vectors                                gives you a preference value for for                                user and an item so the higher this                                value is the more likely it is that the                                user                                well likes this item before I get to the                                point where tell you how to calculate                                these latent factors let's get one step                                 back and consider what we have initially                                 as input and what we have is we have                                 user item ratings this can either be                                 explicit or implicit explicitly ratings                                 means that for example the user our                                 faith watched a series of Netflix gives                                 one to five stars to it that's explicit                                 waiting an example for implicit waiting                                 would be there that you analyze the                                 behavior of the user for example you you                                 measure how long he stays on a website                                 or how often he watches a certain film                                 to get a kind of waiting value foot and                                 these waiting values can be organized in                                 a matrix where the roles are the users                                 and the columns are the items and of                                 course this mate matrix is by no means                                 dance so it's it contains a lot of                                 blanks because not every user has seen                                 all movies or listen to all songs so the                                 the goal is now to fill in these planks                                 and that's what we going to try to do                                 next please bear with me on this slide                                 that's going to be one of two slides                                 with formulas but actually really easy                                 so what we have is we have a rating                                 matrix and to find the latent factors we                                 have to find and two matrices the user                                 matrix X and the item matrix Y such that                                 the product of these made of this these                                 two matrices gives you approximately the                                 rating matrix once we found these two                                 matrices which are usually a flowing we                                 also have found our latent factors                                 because the wealth of the user matrix X                                 are the latent factors of the users and                                 the columns of the item matrix are the                                 the latent factors of the items so but                                 how can we find these two matrices                                 X of Y well here well that's that's a                                 case for a machine learning where we can                                 apply some methods but in order to apply                                 some machine learning methods we usually                                 need some kind of loss function which                                 tells us whether our solution we found                                 is a good solution and in our case we                                 use the root mean squared error which                                 sounds might sound like complicated but                                 actually it's it's only the sum of the                                 the sum of the squared differences of                                 the prediction values and the writing                                 values so the smaller the sum get the                                 more closely the better serve I                                 approximation of the right images so                                 yeah that's it basically now since we                                 have like a loss function we can apply                                 optimization algorithm which will find                                 us our X of Y values well the problem                                 here is that the loss function is                                 depends on two variables that's the item                                 matrix X Y and the user matrix X what we                                 could use for example is su casa                                 gradient descent this always works but                                 today I want to show you a different                                 approach which is called alternating                                 least squares the idea of alternative                                 least squares is that that by fixing one                                 of the variables let's say the item                                 matrix why we obtain a quadratic                                 quadratic optimization problem and we                                 know that's easy to solve we only have                                 to take the first derivative set to zero                                 and solve for the unknown I bear with                                 the math here espero spare the math you                                 and you have to trust me that this is                                 right if you do that on to some with                                 reformulation of the equation you end up                                 with this update formula for user vector                                 X you what is interesting to see here is                                 what we actually need to compute an                                 updated I use a vector and that is                                 actually all the waiting's the use of                                 the user has done so the writing values                                 and only                                 those item vectors for which the user                                 has given a rating that's going to be                                 important later on okay now we've seen                                 how we can update the user vector                                 vectors with respect to the item vectors                                 so if it's fixed so how can we finally                                 end up with an overall solution of our                                 optimization problem so that's quite                                 simple we first fix the item matrix why                                 update the user matrix X and use this                                 value and the next half step to update                                 the item matrix Y and these two half                                 steps gives you on the what what we call                                 like an a last iteration steps so we                                 executing this these two steps                                 repeatedly improves incrementally your                                 your item and user matrix x and y and if                                 you do that until you can watch you have                                 solved your optimization problem and                                 that's how you factorize a can factorize                                 matrices okay at the beginning I said                                 that we want to do recommendations at an                                 extreme large scale so this means we are                                 talking about data sizes which no longer                                 fit in the memory of a single machine so                                 what we have to do to to make it still                                 feasible is to implement a distributed                                 alternating least square a square                                 algorithm and that's where our chief                                 link comes into play for those of you                                 who have never had a about our chief                                 link will give you a short introduction                                 Apache chief link is a power stream                                 processing engine and as such a system                                 the the key component of it is is a                                 streaming data for one time the                                 streaming data flow one time allows you                                 to or enables you to execute dataflow                                 programs in parallel and the data flow                                 can be either a bad job or a streaming                                 job actually and it's executed on the                                 the same one time                                 in order to to define these data flows a                                 petty fling offers you one API for for                                 batch programs so the data set API which                                 is available for java scale and pison as                                 well as a data stream ipi which is                                 available available for Java and Scala                                 but he is actually not worthy that that                                 batch processing is actually a special                                 case of stream processing when you do                                 batch processing you can think of having                                 a finite stream for example and they are                                 well there are some special code paths                                 in Apache flink which makes the batch                                 execution more efficient but to speed it                                 up on top of the bad batch processing                                 API by now well we could say which                                 ecosystem has been developed so there                                 are some extension libraries some of                                 them already part of the the current                                 release some of them will be part of the                                 upcoming                                                        hopefully be released in one to two                                 weeks and some of them are the most                                 important additions are the the a dupe                                 compatibility layer which allows you to                                 run a dupe jobs on on a budget thing                                 then we have a library for a graph                                 processing which is called jelly then                                 recently we started working on the                                 machine learning library where I                                 currently spend most of my time and we                                 added what we call the table API which                                 is a really nice sql-like declarative                                 programming interface so if you if you                                 know SQL then you quickly will be                                 familiar with the table API as well and                                 it's really nice to work with that                                 actually and the table API is also                                 available for on the the screaming part                                 of a patchy thing but since the swimming                                 part is is younger                                 the best part the ecosystem isn't as                                 which yet as for the batch part and                                 besides these these libraries and                                 extension think is already is also used                                 by many other projects as a back-end                                 execution back-end so for example the                                 well it is it can be used by google data                                 flow API as a back-end meq/l is executed                                 on flink we are currently working on                                 bringing support like a skating support                                 so that cascading jobs can be run on a                                 patch if link and the link was added as                                 a stream processing engine to the Apache                                 Samoa project the summer project is well                                 it's it deals with as viewing machine                                 learning so in that sense some more                                 brings assuming machine learning to                                 Apache think as well that is really cool                                 actually and all that can either be one                                 locally on your machine you can start a                                 center long cluster somewhere on some                                 computers and one your jobs you can                                 deploy fling on yarn that's really easy                                 to run some jobs your job can be                                 executed on tests or embedded lee on the                                 next slides I will mainly concentrate on                                 highlighting the the reasons why I think                                 is a good fit for implementing these                                 kind of recommender systems but if you                                 want to learn more about the the in                                 terms of thing are about the individual                                 components then i can highly recommend                                 you the apache fling deep dive talk                                 which will be given by stephan even                                 tomorrow i think from                                                  on stage three so if you have time you                                 should definitely check it out is it's                                 going to be a good talk okay why why                                 shall we use thing for ALS well to make                                 a long story short there are four four                                 main points one of them is that it has a                                 really expressive API it has a stream                                 pipeline screen possessor it offers                                 closed loop iterations and its                                 operations are executed on manage memory                                 so I will explain all of these points                                 and second in more detail so let's start                                 with the expressive API and well the the                                 recommender is implemented as a bad job                                 so we have to talk about the batch API                                 right the key abstraction of the the                                 batch JP I is a what we call a data set                                 the data set is and well we present some                                 data which is possibly distributed in                                 our custom and you define your                                 computations are by applying                                 transformations to this data set which                                 which generates a new data set on which                                 you can apply new sets of computations                                 and that way you specify a more or less                                 the data flow which encapsulate your                                 your program logic well since--since all                                 theory is gray let's let's take a look                                 at an example which is like the hell                                 world of big data the word count well                                 they be a goal is we have a text and we                                 want to count how off each word appears                                 on this text how can we do that well                                 first of all we have to define some case                                 class which basically only stores for                                 each word how often we've seen it so                                 there's a string and interviewed then we                                 have to weaken our data which is done in                                 the second line the the text is                                 represented as a data set of strings                                 whereas each element will be the data                                 set is one line of the text so what we                                 have to do in order to calculate the                                 number of words how often what appears                                 on this text we first have to split each                                 line into it worse and this is done by                                 the first line with this flat map                                 operation we take a line then call split                                 on it and pack it or weapon in a word                                 instant of this case class next we have                                 to to collect all these were done senses                                 so all identical words so that we can                                 count a half they appear that's done by                                 goodbye and the following some function                                 which simply accounts how often the word                                 appears and that's all to obtain the the                                 word counts                                 so this this exemption show how easy it                                 is to to program with a Apache link and                                 in fact it's a lot of fun to do and made                                 the development of this recommender                                 system really easy next I said we have                                 the pipeline stream processor well                                 unlike many other data flow systems                                 which which executes the data flow stage                                 wise so one Operator after another think                                 twice to bring as many operators online                                 as possible by doing it between this we                                 can avoid to materialize intermediate                                 results so that so we can simply take                                 one element of your fear of the input                                 give it to one operator take the result                                 give it to the next so to pipe it                                 through the data for more or less and                                 for jobs where some of the enemy                                 intermediate results are so blown up                                 that they have to to write your data of                                 the Demeter side to disk or that even                                 your your system crashes the ad helps                                 that we have this pipelining execution                                 mode because flink doesn't have to                                 materialize the intermediate data next                                 I've talked about closed loop iterations                                 well many many systems offer you an ad                                 hoc duration mechanism in that sense                                 that the client somehow we submit part                                 of your job job repeatedly unlike these                                 systems are flink offers a real                                 iteration operator which is part of the                                 data flow and which allows the the                                 optimizer to to optimize programs which                                 which contain iterations this is                                 important for cases where where you                                 include some some computations in your                                 iterations which are actually static so                                 which only has to be computed once so                                 flink and the optimizer think can detect                                 these kinds of computations pulls it out                                 of the iteration computes at once cancel                                 it caches it on on the note and so makes                                 it available for for the extra iteration                                 so you basically save some computations                                 how the iterations are specified within                                 think is                                 that you give a step function to it                                 which is a part of a data flow which                                 basically calculates a form an input                                 data set the recital the next iteration                                 and what happens at the one time is that                                 this step function is first chord or                                 others dataflow part is first called                                 with the initial input which generates                                 the the result of the first situation I                                 just beside is given back to the step                                 function which then computes the with                                 the result of the second iteration and                                 this goes on until either the maximum                                 number of iterations is reached or some                                 kind of convergence criterion has been                                 reached so last but not least I've said                                 that flink thinks operations work on                                 managed memory what does this mean well                                 you understand that I want to give you                                 some some feedback or some background                                 information on how the JVM works                                 whenever you you create some some                                 objects in like a Java program they                                 usually stored on the heap and for these                                 objects is really hard to estimate the                                 size of these objects so this means it                                 is hard to detect when your heap                                 actually gets full and when you would                                 have to to take some of the elements                                 from the heap put them somewhere on disk                                 and make make new space available for                                 four new elements this is often the case                                 for these kinds of systems because you                                 usually work on data which is larger                                 than the total amount of memory you have                                 blink circumvent this problem by working                                 internally mostly on on serialized data                                 and additionally it comes with its own                                 see like memory manager so what happens                                 whenever an element is given to think                                 that it is serialized so it's given to                                 think see well as a framework which see                                 you lies and the objects into some                                 purpose of the managed memory and the                                 this memory is managed by the memory                                 manager and the manner mensia notice                                 when there are no more buffers free                                 buffers left this tells him that some of                                 the buffers                                 which are full have to be split two                                 discs from where they can at a later                                 point of time of the execution can be                                 retrieved and that way think avoids to                                 the thing think of voids too are you in                                 most well you will we can say never see                                 out of memory exception caused by one of                                 the internal components of link because                                 it will always gracefully speed to disk                                 if it's needed okay so much for think                                 now let's get back to the actual                                 implementation okay to understand or to                                 see what we have to do we have to take a                                 look at the update formula again so we                                 see we or we can update the the user                                 vector X you by well we need the                                 writings of the user you and the rated                                 item vectors of the well the items which                                 the user you waited to compute the                                 updated value of x you to illustrate                                 that let's let's try to do it for the                                 first user so we see in the weighting                                 matrix that the first user has waited                                 the second and fourth item right so what                                 we need to calculate the the new X                                   vector is these two writings as well as                                 the item vectors on the second and the                                 fourth item vector we have these                                 informations and get them on one note we                                 can calculate the updated vector so we                                 can can generate or get this information                                 to one place by first joining the rating                                 matrix with the the item matrix where we                                 use the column ID as John Key and then                                 group these pairs on the on the user ID                                 or row index of the rating matrix that's                                 all we have to do to get all information                                 in one place to compute the updated                                 values for X                                                        example we already see one of the                                 drawbacks of this approach that is the                                 user                                                                    the same amount right additionally we                                 see that user                                                         right of the second item so even though                                 they are kept in the same machine the                                 second item vector it has to be sent                                 twice to the same note so we are                                 basically sending redundant information                                 over the network which is which is bad                                 actually but let's first take a look at                                 the at the pros of this approach well                                 the paws of this naive implementation is                                 that it is straightforward and thus                                 really easy to implement and maintain so                                 the code is rather well it's it's yeah                                 easy however we have a lot of network IO                                 with this approach because we first of                                 all we send redundant information over                                 the network and second sickly since we                                 did not pre a partition the ratings                                 matrix we have to shuffle steps one                                 after the first joint operation and the                                 second shuffle step after the well for                                 the group would use operation and                                 shuffles means always like sending the                                 data from from well across the network                                 which is bad so can we do any better                                 well in fact we we can we can do better                                 by addressing these two last problems so                                 namely sending redundant information and                                 having to shuffle steps so first of all                                 what we can do is we simply group all                                 the users which are computed on one node                                 together so for example for node                                      would create a user block which contains                                 the first and the second user once we                                 have done that we can also group all the                                 item vectors which we need to calculate                                 the updated values for these to our                                 users so basically the second and fourth                                 item vector but here it's important to                                 note that that that all these item                                 vectors are only sent at most once so                                 this this drastically reduces the amount                                 of                                 traffic you have and last but not least                                 we can can pre-partition the weighting                                 matrix once according to the user blocks                                 and once for the the item blocks and                                 keep them on the machines that way on                                 the the first joint operation is happens                                 completely locally and doesn't doesn't                                 where you don't have to do a shuffle                                 step which is nice so if we summed it up                                 well the report of this this approach is                                 that we significantly significantly                                 could reduce the network I off first of                                 all by avoiding this application of data                                 we said and by catching the right X                                 right ratings which well it gets gets                                 rid of one shuffle step however the the                                 con is that first of all that we have to                                 store the the rating matrix twice                                 because it's applicated because the                                 different partitioning and the code well                                 I haven't put it here the code gods or                                 more complex or significantly more                                 complex however when we take a look at                                 the performance research we see that                                 it's the advantages clearly outweigh the                                 disadvantages so what we've done here is                                 we've learned some lend some experiments                                 on a matrix accusation on a                                         Google compute engine cluster where each                                 of the instances instances was a eight                                 core with fifty two gigabytes machine we                                 calculated ten ALS iterations with                                    latent factors for rating matrices of                                 varying sizes we started with really                                 small matrices and slowly scaled up what                                 we can see use that the naive ailis                                 implementation performed worse than the                                 blocked LS implementation that's not so                                 surprising and at some point there was I                                 think eight billion where the rating                                 waitress had eight billion nonzero                                 entries it for matrices bigger than that                                 it took so long that well the                                 calculation didn't finish so encounters                                 that                                 see that the block areas implementation                                 scales much much better and we could                                 could factorize matrices with up to I'm                                                                                                        the scale off of netflix and spotify so                                 that there is a real world problem                                 actually it took us                                                finish the                                                         doubling the number of course and the                                 memory we have we could even cut the                                 time down to five and half hours which                                 now allows two to calculate the matrix                                 accurate factorization multiple times a                                 day which allows you to do well more                                 like up-to-date recommendations okay now                                 you might say well this this is quite                                 nice but if I have to implement myself                                 then maybe it's not worth the effort                                 however you don't have to worry the the                                 ALS implementation is part of the                                 machine learning library so it's really                                 easy to use and that's we can see on the                                 right side how you would use it first of                                 all you would have to instantiate the                                 AAS facto miser object we end the                                 weighting matrix which is represented as                                 a data set of tuples well index callum                                 index writing value then you specify                                 some parameters for ALS namely the                                 number of iterations so many latent                                 factors you want to have the lambda                                 value which is important for the                                 regulars like regularization the actual                                 factorization happens in the fig method                                 where you provide the rating matrix and                                 the parameters once you've done that you                                 can basically query your calculate for                                 well or user ID item ID pairs the                                 predictions by simply calling ALS                                 predict or where you provide the repairs                                 for which you want to ok with the                                 predictions yeah but that's not all um                                 you can do with the current machine                                 learning lab we they also I wouldn't                                 include it for clustering regression and                                 classification tasks                                 and all comes with a psychic learn like                                 pipeline mechanisms so it's really easy                                 to set up even complex analytical                                 pipelines yeah this brings me to the                                 conclusion of my talk so what if what                                 have you seen today well first of all                                 you've seen how one can make                                 recommendations using collaborative                                 filtering even seen some some math then                                 you have seen Apache flink the that                                 Apache flink is a powerful powerful                                 stream processing engine with which you                                 can implement an efficient matrix                                 accusation system which scales up to                                 really we large major sizes yeah the                                 last thing a which is left for me to do                                 now is to draw your attention to the                                 upcoming fling forward conference this                                 will take place on the twelfth and                                 thirteenth of October so if you got                                 interested or maybe even want to give a                                 talk there then save the date if you                                 can't wait that long you should you                                 should go to a flink dot apache lock and                                 check check out link or follow playing                                 on twitter at edgy fink and yes with                                 that I'd like to thank you for attention                                 and I hope you have enjoyed the talk                                 very much                                 you
YouTube URL: https://www.youtube.com/watch?v=CmHPxASIj6Y


