Title: Berlin Buzzwords 2015: Ameya Kanitkar - Real Time Big Data Analytics with Kafka, Storm, HBase #bbuzz
Publication date: 2015-06-03
Playlist: Berlin Buzzwords 2015 #bbuzz
Description: 
	Relevance and Personalization is crucial to building personalized local commerce experience at Groupon. We have built infrastructure that processes real time user interaction stream and produces personalized real time analytics that are further enhanced to present relevant personalized experience to hundreds of millions of users of Groupon across the world. 

This talk covers the use case and use of our Kafka-Storm-HBase-Redis pipeline to ingest over 3 million data points per second in real time which in turn brings in millions of dollars in additional revenue. Specially we will discuss how we scaled this system for hundreds of millions of users including solution choices, different techniques and strategies, traditional and innovative approaches. 

Solution includes some interesting algorithmic choices to reduce data size such as bloom filters and HyperLogLog, as well as use of big data technologies such as HBase, Kafka & Storm. Attendees can take away learnings from our real-life experience that can help them understand various tuning methods, their tradeoffs and apply them in their solutions.

Read more:
https://2015.berlinbuzzwords.de/session/real-time-big-data-analytics-kafka-storm-hbase

About Ameya Kanitkar:
https://2015.berlinbuzzwords.de/users/ameya-kanitkar

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              hi Thank thanks for your introduction I                               just want to get sense or                               who all are in the audience how many of                               you are from Berlin all right                               interesting okay and how many of you are                               aware of the big data stuff like our at                               least have worked on like Hadoop or                               hbase or storm or Kafka like cool lot of                               you cool awesome that makes my life                                easier let's see so let me introduce                                myself a little bit more or I'll just                                wait for a few more seconds till people                                settle in                                one more question how many of you know                                Groupon no I'm not advertising but that                                way I can skip some of the parts and go                                to the main stuff but now I'm                                advertising advertise we have office                                here in Berlin groupon office but I i                                work in San Francisco office and all                                right I'll start now so I'm a lead                                engineer on the real-time infrastructure                                at Groupon so let me go back a little                                bit we we are about six year old now                                Groupon and we have been through you                                know sort of lot of changes you know you                                call them p votes or whatever but when                                we started groupon started with a very                                simple model like something like this                                you know you have one deal a day this is                                some some kiteboarding deal or something                                like that and what will do is that will                                have one dealer day it will be for like                                fifty dollars                                                          that and we'll send out this deal to all                                our users that day and a lot of people                                will buy them and lot of people will not                                buy them right and as groupon grew very                                quickly we we are mass lot of users in a                                lot of cities so to give you an example                                in San Francisco we had over a million                                users in I don't know like under eight                                months or something like that so what                                what that means is that you know it kind                                of became much harder to scale at that                                point and it created short of a lose                                lose lose situation from a business                                standpoint and let me explain you how so                                let's say we ran a massage deal and                                we'll sell say whatever fifty-dollar                                massage for                                                             something like that to a million people                                in say Berlin and what would happen is                                that you have a lot of them it's                                irrelevant deal right so they they will                                stops opening their emails or something                                like that but out of a million users you                                know lot of people will actually buy                                them right so let's say we sold seven                                thousand massage or spa appointments now                                the the small business that is going to                                serve those appointments they're not                                going to sell it they're not going to                                serve that many appointments easily                                right                                so it's bad for the the business that we                                are doing group                                                customers because they're getting a lot                                of irrelevant emails and it's bad for                                Groupon because we could only run one                                deal a day in a large city like Berlin                                or bad city like San Francisco so this                                became kind of a lose lose lose                                situation and we had to come up with                                something so that you know it becomes                                much easier for everybody so one idea of                                doing that was okay let's instead of                                running one dealer day let's run                                multiple density right so that idea                                sounded easy the problem is that you                                know in order to do that you know you                                have to match the deals to the right                                users right if you have there if you can                                match the right deal to the right user                                then we come from that lose lose lose                                situation I just explained to sort of a                                win-win-win situation so the users will                                get more relevant deals the business can                                handle only as many customers as they                                want to handle or as few as a more or                                less right and for Groupon we could                                scale with more deals without                                necessarily going into more cities right                                so great idea so the reason I'm                                emphasizing on this is because you know                                for a lot of other businesses the                                relevance or recommendation systems are                                kind of an optimization layer on top of                                their business model maybe for Amazon                                 like because you also bought or                                 something like that but for Groupon it's                                 a fundamental part of our business model                                 itself without these systems groupon                                 cannot exist right all right cool so in                                 order to do that you know what are the                                 things we need to build so that we could                                 build this relevant system there are                                 sort of two parts to it one is that you                                 know when you if you build a good graph                                 about users like what what those users                                 are what each user likes what their                                 preferences are and you also go and                                 build a good understanding about the                                 deals like what kind of a deal it is                                 where it's located what price it is what                                 category it is you know etcetera                                 etcetera then if then then if you match                                 these users to these deals then we can                                 get started with some sort of irrelevant                                 system right so those are the                                 fundamental parts of our                                               where you build some understanding about                                 users and you build some understanding                                 about deals the other part of this is                                 that once you build that there are two                                 sort of systems from which we have to                                 deliver these recommendations or                                 personalization aspects right one is the                                 email which groupon relied on for a                                 first half of its life all we still rely                                 on it but it's much less now so that was                                 the email system or we call it offline                                 system so every day will will go and you                                 know generate new emails new                                 personalized emails for everyone and                                 will compute you know what would be the                                 best deal for you amongst the deals we                                 have and the other scenario is you open                                 your groupon app or you could go to a                                 groupon website and then that particular                                 experience is personalized for you so                                 that's more of a real-time                                 personalization system right so we had                                 sort of two systems so I'll give you a                                 little bit of a history on like how                                 these systems started and this mostly                                 this talk is how we moved from the older                                 system we had to the new system we have                                 now which is more real-time so about two                                 years ago we we had something system                                 something like this most of our business                                 was relied on emails so we had this                                 pipeline of you know all this data                                 pipeline which will bring all the data                                 about users email records what emails we                                 have sent you which what emails you have                                 opened which you have not opened what                                 things you have clicked on so on and so                                 forth and data about deals and that data                                 pipeline would bring all this data would                                 put this into a Hadoop system and in                                 Hadoop we'll run some MapReduce jobs to                                 compute what goes into your email and                                 those emails will be sent out next day                                 so this was kind of a delayed pipeline                                 about a day worth of delay this use this                                 used to take quite a long time I think                                 like                                                                   large cluster and then emails will be                                 sent out a real time scenario was a far                                 less advanced so what we used to do is                                 that we will compute some of this                                 part of this was dumped into my sickle                                 store and then we had some API on top                                 that would serve out the the app the the                                 website or the app the two more things                                 happen during this time one is that you                                 know when we built this system you know                                 this was more like                                                       built that this so this is how                                           for us you know like very few deals like                                 we moved away from daily deal to a few                                 more deals right but we still had you                                 know like like eight or ten thousand is                                 total on our platform right in a given                                 city we would only have you know like                                    days or something like that but that                                 moved too much longer so today we have I                                 think over                                                             be more now and some of the cities have                                 even thousands of dates so we had to                                 scale on the on the business side where                                 the structure of the problem is change                                 earlier problem was given this user and                                 out of                                                                 to send right now the problem is given                                 this user and is hundreds of thousands                                 of deals which is the best deal to                                 choose for this user right which is a                                 which is not scaling from                                                or something but it's still                                 significantly different and what we                                 initially started out with because we                                 initially started out with some                                 something simple like okay just for loop                                 over all the deals score each deal and                                 the best deal is at the top right but                                 when you have hundreds of thousands of                                 deals you can't just for loop over it                                 right the other other fundamental                                 business change happened was that people                                 started moving from email to web app or                                 mobile right the mobile change so today                                 we have over                                                             what that means is that people are no                                 longer you know like opening their                                 emails or not opening as much of emails                                 but rather checking things on the app so                                 what that means this crappy architecture                                 for real-time scenario wouldn't work                                 right where I'll get into some of the                                 issues with this but the clear problem                                 here is this my sequel guy right which                                 doesn't scale much so in order to keep                                 users engage we have to have much better                                 real-time personalization scenario                                 otherwise we'll be in bit of a trouble                                 it's the same thing grows with mobile                                 business reduce dependence on email                                 marketing change in strategy from daily                                 deal market Louis to the reason I'm                                 explaining this is lot of times what                                 happens you have systems and and                                 business changes and then you have to                                 have you have to respond how you change                                 your technology to respond to your                                 business right so this is kind of a okay                                 so what were the issues with the old                                 system the first issue is this data                                 pipeline which is if you open an email                                 or if you click on some web page or                                 something like that we wouldn't know                                 about it for almost a day or even                                 sometimes longer right so the data comes                                 a day later and then we compute emails                                 for you which takes another                                             and then we send out an email so if you                                 do some actions on Monday you won't see                                 a result of for which on until wednesday                                 right so many times the deals are                                 expired by that time so it's kind of                                 useless the other thing is this my                                 sequel store right so it had some basic                                 data like some gender and location and                                 something like that but it was very hard                                 to go scale this I mean you could but at                                 least for us out of the box to scale my                                 sequel as is to Euro                                                   with all the real-time aspects that we                                 wanted to bring in so this this system                                 is not scaling business is changing we                                 have to do something all right so we                                 started out with thinking okay what what                                 would be the ideal system you know if we                                 want to build this again and if you want                                 to build this right what how would it                                 look like that would scale for us so                                 here are sort of our wish list so a                                 common data store that serves both                                 online and offline systems so remember                                 here the problem is that we have this                                 Hadoop system which is completely                                 disconnected to the real-time system so                                 our wish list is ok we don't want like                                 two separate pipelines one going for                                 emails one going for app let's let's                                 have a common one well data store that                                 scales to hundreds of millions of                                 Records which was this my sequel issue i                                 have mentioned earlier data store that                                 plays well with our existing hadoop                                 based system so                                 so we had a lot of data scientists and                                 engineers who had written a lot of                                 algorithms we didn't want to just throw                                 it away right and that that was written                                 in Java MapReduce kind of a system so we                                 don't want to you know just completely                                 just throw that away and start from                                 scratch so let's build a system that                                 plays well with that infrastructure and                                 we want this to be real time right we                                 don't want this add a delay you know if                                 you make certain actions on the previous                                 page the next time you go to the website                                 or the next page you go to we want that                                 to be incorporated in our algorithms                                 ride or in at least are in our                                 experience so what that also means is                                 that we should be able to handle at                                 about one hundred thousand messages per                                 second right all right cool those are                                 that's a wish list it's good to start                                 with a wish list so this is what we came                                 up with and I won't spend a lot of time                                 on explaining why certain components we                                 used and not the other computing                                 components but i'll tell you like now we                                 have we are using these components what                                 were the issues in scaling this and what                                 are some of the learnings we had right                                 so let me explain how how what this                                 system is about so we have these website                                 logs and mobile logs and also the email                                 logs and they go into Kafka and from                                 Kafka we read into storm so storm is a                                 real-time sort of a processing system                                 Kafka's like a messaging system for                                 locks and we dump them into HBase and we                                 we would we would write some memories                                 are algorithms that we were written                                 MapReduce they could run on this data                                 insert HBase and generate emails and for                                 online online personalization that data                                 will be in HBase so you can just read                                 from HBase sense of your web app our                                 mobile app right okay it sounded good a                                 lot of these components are kind of                                 default so I'll just given idea about                                 that so Kafka is a great system for                                 moving logs from one place to other I                                 think it's a clear winner now so there's                                 not much of a choice storm you can                                 perhaps consider other systems but we                                 chose storm I'll get into what was our                                 experience with storm HBase the reason                                 we chose is where each base was because                                 it                                 it's in the same huh Luke family and it                                 had native MapReduce integration it                                 built on Hadoop we had already had a                                 harder cluster so it was easier to go                                 with that cool so sounds good if we do                                 put this together it should probably                                 work right okay but let's see what were                                 the issues so first issue like let's say                                 if we have h base which what what we                                 need to do with that every base first it                                 needs to handle                                                    second so if we if we have all the data                                 coming from all web apps and mobile apps                                 and email coming in the real time at the                                 peak we we have to write                                                per second into HBase otherwise we would                                 be behind real time right and if we if                                 let's say we achieve that then what we                                 need to do is that because it's also                                 serving layer for the real time web site                                 it needs to have it needs to keep a very                                 tight SL on late Reed Layton sees while                                 our MapReduce jobs are running on the                                 cluster so if you have a MapReduce job                                 that's ring on the HBase cluster                                 computing your recommendations and and                                 also users who also being served on the                                 app or website the latency needs to be                                 consistent and they cannot just exterior                                 SLE and sometimes you know like we will                                 also have bunch of data that will                                 compute offline in the MapReduce cluster                                 and will upload back into HBase to be                                 served in the real time now now again                                 that those Layton sees needs to be                                 within your SLE ok so it didn't work                                 just how we imagined initially right we                                 couldn't write hundred thousand messages                                 per second as is and when the MapReduce                                 jobs started running on HBase their                                 latencies we went to the roof all right                                 so what do we do so this is what we did                                 to write hundred thousand writes per                                 second so the data data is very simple                                 so the key is a user key and the the                                 data we want to write in that value is                                 just user events so user clicked on this                                 page click on this deal open this thing                                 open this email received this email blah                                 blah blah so or purchase this deal or                                 something like that right so that the                                 data is pretty straightforward it's just                                 if i read that users data then we get                                 all the data but at you                                 so one way to do that is perhaps have                                 some sort of a read end and the right                                 thing so you you read the data from from                                 in it for that key you update the new                                 event and you write it back we've found                                 out that it's it's very hard to do that                                 at                                                                      do now is HBase comes with this sort of                                 a concept of dynamic columns are you                                 know qualifiers so you have a column                                 family but within that column family you                                 can write whatever column then move on                                 so each base is kind of a hash map of a                                 hashmap so all right so we said good if                                 we can come up with a sort of a unique                                 column name or qualifier name then we                                 can just keep on appending data for that                                 user so here's an example let's say this                                 user looked at some sort of a deal and                                 clicked on a deal then for this user                                 will create this column name so it's a                                 timestamp in the in milliseconds deal ID                                 and in the event type right so whatever                                 that event type is excuse me whatever                                 that event type is so that way we could                                 keep on writing as many events as we                                 want with some some amount of buffering                                 like to second buffering or one second                                 buffering we could write hundred                                 thousand messages per second good so                                 that solve the first problem what about                                 the next problem that was relatively                                 easy to solve so all we did is that we                                 just put two clusters one is the real                                 time cluster and the other is a batch                                 cluster and we use the ready-made                                 application that each base comes with so                                 all the data will go all the all the                                 data will be written into the search                                 base and get replicated two batches base                                 all the math addresses will run on this                                 HBase so because of that we don't have                                 to worry about how the latencies are                                 affected on a GPS right so because it's                                 on a separate cluster and then we'll use                                 the the functionality that edge base                                 comes with called bulk upload so what it                                 does is that creates whatever you want                                 to write e in in a direct H file so the                                 underlying format of HBase in a separate                                 MapReduce job and then it just copies                                 those files                                 literally just like a disk copy like a                                 Hadoop Hadoop copy and it brings all the                                 data in so like we could you could I                                 mean in our cluster we could copy you                                 know like                                                             cluster and they're like                                               right because it's literally just a copy                                 there's nothing you can just rename the                                 file that's it so cool so we could write                                 hundred thousand writes per second we                                 could load bunch of data into HBase in                                 real time and now the HBase park works                                 perfect so let's that so each place is                                 kind of a user side story right so user                                 and all the data about users and we                                 could run MapReduce to find user                                 personalization all that stuff so the                                 other interesting part is the deal side                                 right so let's see what that part is so                                 I'll give you idea about what relevance                                 is or what is the sort of intuition                                 behind is relevance algorithms so we                                 said okay if we if we if we know you                                 know like for example questions to these                                 answers to these questions like say how                                 do women in bar Berlin convert for peas                                 are deals right or how are women in                                 Berlin are converting for a particular                                 piece our deal you know conversion rate                                 being you know say if we show this deal                                 two hundred users how many of them are                                 buying you know that's your conversion                                 rate then we could potentially start                                 thinking about how we can use the                                 relevance of girls right we could make                                 it more interesting and add a few more                                 dimensions to it so let's see how that                                 works so how are women in Berlin from me                                 ta area age                                                           York style pizza wendy is located within                                 two miles and when deal is priced                                 between                                                             added more dimensions and imagine you                                 know if you have answers to these                                 questions like you know for all these                                 attributes what is the conversion rate                                 and you take a user and you find which                                 bucket that user belongs to in all those                                 attributes and find which deal is                                 converting best for that user you                                 potentially have a ranking right what we                                 found out is that you can't just keep it                                 at a category level so this is just a                                 category level at a piece are right or                                 new upsell piece or something like that                                 each deal performs very differently so                                 to give you an example a neighborhood                                 coffee shop deal performs much                                 differently                                 say a starbucks deal right so deals are                                 very different but although both our                                 coffee deals right so we can't just keep                                 it at the complete at a category level                                 but we want to we need to compute these                                 things at ed level not just a complete                                 category level so the same thing but                                 since everything on the left side on the                                 right hand side only that it's for a                                 particular deal more more conv but but                                 this is not enough right so we wanted to                                 do more complex stuff so here is an                                 example so so how are women in Berlin                                 from a particular area age                                          convert for New York style pizza when                                 deal is located within two miles and                                 when Dale is priced between                                           euros and that user likes activities                                 such as biking and who have been very                                 active user of Groupon and they serve on                                 mobile platform right something like                                 that the problem is that you know you                                 start with very something simple right                                 you started with you know peas our deal                                 for women right and now you have added                                 these extra dimensions now the problem                                 is that this is actually an exponential                                 problem because every time you add a                                 dimension it you know to two genders and                                 then you add you know like an area then                                 to into that number of areas you have                                 women just all the way back in two                                 locations into price categories into age                                 categories and what we found out was                                 that you know if we have to answer all                                 these questions we have to compute                                 probably you know like                                                  different events or separate buckets in                                 order to answer those questions so I                                 said okay that's interesting problem ok                                 let's try to solve that just before that                                 this is what i mean by a conversion rate                                 number of purchases in that bucket over                                 number of impressions impression can be                                 anything every time you see something or                                 on email web or mobile we counted as                                 impression right and then every time you                                 purchase that's a conversion so how many                                 times you saw it how many times you                                 bought it or in that bucket so i'll come                                 come to you know how we solve that                                    billion different buckets question but                                 let's first talk about how how to                                 compute these different buckets in the                                 real time right we want to do this in                                 the real time so the moment deal is                                 launched within                                                                                                         that deal and we can target it better                                 right so this is what we use remember I                                 mentioned earlier we had we have cough                                 cough which which has a stream of events                                 so we can just plug into that stream and                                 get all the data so that's the kafka                                 part then we have a strong one of the                                 topology rights in to hbase but that's                                 the user side we are talking about the                                 inside now so what we do in that                                 topology is that you know can we say                                 fine will will will read the event and                                 then we'll if we can increase the                                 appropriate counter then we have a                                 conversion rate remember conversion rate                                 is just a counter right so if we could                                 just count how many impressions we have                                 in each bucket and how will you purchase                                 in each bucket the problem is solved                                 this is really a counting problem this                                 is not any harder than that just that                                 the problem is hard because we are                                 dealing with lot of data across lot of                                 buckets so so we used a Redis for                                 counting so each place is good for                                                                                                        stuff but for this we we needed about                                   million updates per second right because                                 the problem is that you know when you've                                 in it when a went come say this                                 particular user bought this deal it's                                 not just one event it gets into multiple                                 buckets so that user is also a male that                                 user is from this area this deal is this                                 price and we have to count against all                                 those buckets right not just one bucket                                 so we had to write or update about three                                 million times per second so so we use                                 Redis for that so this is the high level                                 high level idea so data we read from                                 Kafka stream in analytics topology we                                 decide which exact buckets to increment                                 increment our counters and then we write                                 into Redis this is kind of the same just                                 I just described Kafka read data events                                 in Kafka find which buckets even falls                                 into increase event counter for                                 appropriate buckets in the Redis and red                                 is a single Redis was not enough so we                                 did our own sharding on the client side                                 and we updated that data in the Redis                                 cool so I'll just go into you know some                                 of the scaling challenges so that that's                                 really a tree you know like how                                 that it's really simple the data comes                                 in we decide which bucket show update we                                 update the counter done right if we do                                 that for all users and against all                                 buckets the problem is solved you have                                 the relevant system which is real time                                 well as it turned out you know every                                 component is not that straightforward to                                 scale so that's the scaling part I am                                 going to talk about so the one of the                                 first issues is the cough kind storm                                 part the scaling of cough kind strong                                 but in Group one we have a sort of a                                 centralized kafka cluster where you know                                 if all the data comes there and I don't                                 work on that team but they kind of                                 manage our scaling in which is a very                                 nice setup for us to have but the the                                 challenging part was storm especially                                 when the messages count became you know                                 like                                                                  two million updates per second and all                                 that stuff the storm became harder to                                 scale the problem is that you know in                                 real time systems you have to manage the                                 flow so you have say step one two and                                 three and step three is slightly slower                                 then it creates back pressure on step                                 two and if step two is now slower it                                 creates a back pressure on step one                                 because there are only so many events in                                 the funnel you can have right and you                                 have to sort of I just you know like you                                 based on your computation you know each                                 each step may take more or less time                                 right and you have to sort of at just                                 how many bolts you need in each step and                                 that becomes kind of a challenge you                                 know we spend a lot of time tuning that                                 there are other things that you have to                                 make sure for example the try to do a                                 localized so let's say this is all                                 distributed systems right so from step                                 one you move on to the step two if                                 possible make sure that you send that                                 data from step one to step two on the                                 same machine instead of through the                                 network because then the network becomes                                 a bottling so for example if you just                                 implement storm as is out of default it                                 has kind of a random passing the                                 messages and what we saw that a network                                 was out of bandwidth right very quickly                                 and then we moved to this kind of                                 localized thing and you know we up it                                 came down to like                                                       was earlier                                 so there are likes a lot of little                                 things that you have to do for a storm                                 to work at that this scale there were                                 other things like because the speed of                                 each processing part is different you                                 have to sort of stop how many messages                                 you are getting into the storm system                                 otherwise the spout that you have it                                 keeps on getting messages and if the                                 messages are not getting processed you                                 know it just creates a lot of ram                                 pressure or memory pressure and you know                                 bad things happen so there are settings                                 like use macs pout pending etcetera                                 basically that stops how many                                 unprocessed messages you have in your                                 topology okay so here is another                                 interesting part what happens is that                                 you know all these systems are                                 distributed and they do not give you any                                 guarantees of the most of them give you                                 at least once guarantee right but they                                 don't give you at exactly once guarantee                                 so what I'm what I mean by that is that                                 you can get duplicate messages so which                                 is now an interesting problem because if                                 you count purchases twice your                                 conversion rates are completely going to                                 be wrong right so what do we do with                                 that with HBase that was easy because if                                 the message comes again will create the                                 same same column name for it so we'll                                 just overwrite that message so it's not                                 a problem on each base side but the                                 problem is definitely there in the                                 another this radish analytics topology                                 that I just explained because now your                                 counters are going to be wrong right so                                 what we do is we use bloom filters so it                                 kind of covers you know ninety nine                                 point sort of nine percent of duplicate                                 error cases but this is something to                                 remember if you are building these                                 systems you have to build it so that you                                 know that there will be duplicate                                 messages there will be delayed messages                                 and your system needs to adjust to that                                 Redis so this is a interesting right                                 because the the fundamental part of                                 scaling Redis was managing our memory                                 footprint because as I mention you know                                 we had this                                                              of something like that if we just use                                 another                                 normal ready ski for                                                    the memory footprint was huge right and                                 what we found out in the Redis is that                                 they have this nice hash hash                                 functionality where you have sort of a                                 top-level key and like inside you have                                 another hash if you use that you could                                 potentially reduce your memory footprint                                 by a factor of                                                                                                                              margin so we are talking about instead                                 of using two hundred machines using                                    machines right so it's much much much                                 big it's a huge difference and the                                 reason it works that way is because like                                 things like ex-pirate expiry and all                                 that stuff is maintained at at the top                                 level hash key instead of at each key                                 level so you can use Redis with hashes                                 to have multiple different keys but your                                 overall member memory footprint would be                                 much much less you know the other issue                                 we have with the system is that you know                                 like in order to even keep reddy's up                                 and keep on going redis comes up with                                 sort of two persistent formats one is                                 called aof which is every time you write                                 Redis updates the disk and you know                                 right starting and the other is our DB                                 which is kind of a snapshotting system                                 so you can say every five minutes or                                    minutes take a dump from memory into                                 disk and that's what you have so what we                                 did is that we we turned off aof so not                                 every not every operation gets to the                                 disk right away we we have we kind of                                 lazy implementation here so if say                                 something breaks for                                                    kind of lose data for                                                    that way we could we could scale Redis                                 the other thing is that I want to                                 mention is that Redis was probably the                                 easiest for us to scale I mean it's                                 really has no overhead you just start                                 and it just works I think I just briefly                                 mentioned about bloom filters but they                                 are very handy you can especially for                                 the systems like these you know your                                 overall memory deduplication is a huge                                 problem and we could solve that pretty                                 quickly with this                                 another interesting part about this                                 system is that the architecture of the                                 system is relatively simple because it                                 is kafka strong the Redis and the other                                 part is just Kafka storm HBase the                                 problem is that the error scenarios                                 right because this is a real-time system                                 if something happens something wrong                                 happens you have to make sure you can                                 come back to it at a reasonable level                                 and because lot of business and lot of                                 revenue is actually riding on these                                 systems so bringing systems back to the                                 correct status is very critical so how                                 do we do that so Reddy's I mentioned you                                 know we have these are DB backup so what                                 we do is we have a slaver it is also                                 which gets replicated and we take these                                 already backups and copy them to Hadoop                                 cluster HDFS every I think                                               something like that so if something                                 happens we can go back to the state                                 where whatever Redis was                                                all right at each                                                        I think last seven days or something                                 like that hbase HBase comes up with a                                 snapshot functionality so you can tell                                 edge base to take a snapshot at whatever                                 time you want and you can just tell                                 HBase to go back to that snapshot so                                 let's say something happened some bad                                 data is coming remember the data that we                                 have we don't control the data you know                                 whoever is writing the mobile app where                                 is writing the web app they are doing                                 the log and this is all log base data                                 right so it's very possible that                                 somebody writes some bad code and we get                                 bad data right so let's say some date                                 bad data is coming as long as we are                                 taking these snapshots we can say oh                                 that bad data is coming from you know                                 yesterday two o'clock or something so we                                 just take all our systems rewind back to                                 two o'clock based on these snapshots and                                 we replay the data when that problem is                                 fixed and we get all the right data                                 right Kafka and storm so what we also                                 have is on the kafka we have a topic                                 that reads from CAF chondrites to HDFS                                 so all the data that's coming to Kafka                                 because the kafka has some I think three                                 day expiry or something like that so we                                 don't have all the data inside Kafka but                                 all the data inside car gets to HDFS so                                 let's say you want to replay data from                                 two weeks ago for the two hours period                                 which which were we had a bad data we                                 could potentially do that right we could                                 just replay that data from HDFS back                                 into cough cough                                 to this system one more interesting                                 thing we did is that you know we wrote                                 lot of monitors and cpu monitors this                                 model to that monitor but there was                                 another interesting monitor we wrote and                                 in practicality that was the most                                 important monitor we had written and                                 what we did that what we what was that                                 monitor was we actually wrote a crawler                                 that goes and crawls our website or                                 mobile app and then we check back in                                 each base if those events have arrived                                 right and it takes continuously every                                 two minutes or something like that so we                                 crawl our own website we fake a by our                                 fig we fake the birch purchases of fake                                 deals and then we check back in each                                 base and with that what happens it's                                 kind of an integration sort of a monitor                                 test whatever you call it but it takes                                 the whole thing you know it takes the                                 app it takes the website it takes Kafka                                 check storm it takes HBase it checks                                 traders you know it takes the whole                                 thing and if something is wrong                                 somewhere there are so many moving                                 components if something is wrong                                 somewhere you find out within a couple                                 of minutes that something is wrong right                                 and then we just go back and say okay we                                 have to stop this now we'll see what                                 happens and all that stuff so that that                                 that's a very useful monitor we have we                                 are hiring my manager asked me to put                                 this slide he won't otherwise let me                                 speak so and questions                                 you
YouTube URL: https://www.youtube.com/watch?v=-pOGiVA9ll4


