Title: #bbuzz 2015: Christoph Tavan -  From Machine Learning Startup to Big Data Company
Publication date: 2015-06-04
Playlist: Berlin Buzzwords 2015 #bbuzz
Description: 
	How to start a company based on a machine learning idea? And how to scale it into the "Big Data" region?

In this talk I want to share some insights that I gathered during the last 3 years while founding and successfully scaling a real-time bidding (RTB) company from a two-person startup to a leading technology provider in the field:

- From fancy algorithms to production-proof algorithms.
- From thousands of model evaluations per day to trillions.
- From megabytes to petabytes.
- From real-time to batch to real-time.
- From two people to entire teams of data scientists and engineers.

Read more:
https://2015.berlinbuzzwords.de/session/machine-learning-startup-big-data-company

About Christoph Tavan:
https://2015.berlinbuzzwords.de/users/christoph-tavan

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              Oh                               yeah hi welcome everybody so I hope I                               will keep you awake although it's                               already in the evening and second                               conference day thank you for joining so                               my topic today will be from machine                               learning startup to big data company and                               while it will be probably a little less                               technical than many of the talks that                                you've seen here so far I still hope                                that you will be able to gain some                                relevant insights and before we get                                started I'd like to get to know you a                                little better so maybe a little hands up                                around who of you is currently working                                in a start-up yeah at least some who                                would say that he's working in a sort of                                an established company Oh more maybe due                                to the fact that it's a big data                                conference who's working with machine                                learning ok also quite a lot and who's                                working with Hadoop like anything from                                the ecosystem alright good so yeah my                                talk today I want to share some lessons                                that we learned while founding and                                scaling a machine learning startup and                                in particular I want to talk about all                                the major mistakes that we made so if                                you happen to still be in a startup                                phase then you can maybe avoid them                                before I get started what does it work                                ok yeah I'm Christoph and that's me in                                San Francisco where I had a great                                holiday recently I got into developing                                and about                                                        decided to study physics after school                                and I also finished my bachelor's degree                                and then sometime in                                                 doing my master studies there was this                                great and interesting startup                                opportunity and this is basically also                                where the story begins                                so back then I was getting in touch with                                a few physicists                                they were doing research and machine                                learning for about I guess something                                more than                                                               because I mean a lot of stuff that he                                doing theoretical physics is what you                                would now adays call machine learning                                they were particularly into clustering                                and it was already some years ago that                                they had done an interesting research on                                a large online market which was eBay and                                then they realized it with their machine                                learning techniques they could find out                                lots of stuff about the behavior of                                users on this on this on its market and                                they said hey our our algorithms                                actually seem to be really really useful                                for marketing                                we should make money with it and they                                had the idea that basically all it takes                                is just to build some API around it and                                you have a great product because you                                have great we have great algorithms and                                we will have a useful product and to do                                this they basically found some business                                guy for the making money part and they                                found me for the just building an API                                part and we founded a company and well                                at that point in time I thought sounds                                great                                making money but we're just doing API I                                should do that so I decided to quit my                                university studies and started that                                startup and as it turned out well not                                quite so the first thing that this                                business guy and me who were basically                                actively starting this startup was to do                                was like finding a product I mean we had                                bread algorithms Ben knew kind of what                                to do with it but we didn't really have                                a product yet and we had to find out                                about it the first thing we did was                                building recommend the engines you know                                that from e-commerce sites you you                                recommend some products it was                                relatively easy to start with                                straightforward we had good results we                                also get got in touch with some with                                some test customers who were willing to                                include our                                our technology could do a quick go live                                but we also realized that with each new                                customer that we wanted to implement                                with we had a lot of effort and we                                realized it simply wasn't a business                                that we could really scale because it                                 was just too much effort for too little                                 money so actually we found out that it                                 wasn't really about finding a product to                                 make use of our machine learning                                 algorithms                                 it was rather about finding the right                                 product and what we did is we went a lot                                 to conferences presented the concepts                                 that we had we talked to potential                                 customers and we were actually trying to                                 find a product that also scales                                 business-wise and it was at some                                 conference here in Berlin as well the                                 next conference where we got in touch                                 with somebody from online advertising                                 from a company called at meld who says                                 well our marketplaces and online                                 advertising are incredibly inefficient                                 and we need machine learning technology                                 like yours to make this more efficient                                 so we got involved with this company and                                 want you to start a project with them                                 which didn't work out unfortunately                                 because they were acquired by Google but                                 it showed to us that it's actually                                 apparently a market where you can build                                 a business that business business that                                 scales so basically we settled down to                                 building a real time bidding system for                                 online advertising but that also came                                 with a cost that we realized Wow                                 now we have to build even more                                 infrastructure and it's even further                                 away from the initial idea of just                                 building an API around some algorithms                                 so and that's basically my first lesson                                 learned from an algorithm to a product                                 so if you're coming from this maybe                                 scientific or tech side and think you                                 have great technology but you want and                                 want to make money with it the first                                 crucial thing is to find out about your                                 actual product because even if                                 everything is great and                                 lucien arey that doesn't mean that you                                 will immediately make make money with it                                 you really have to build a product with                                 it and also one of the key lessons was                                 that probably we would have been a lot                                 faster if back at that time we would                                 have found somebody from this field from                                 online advertising to get into the                                 startup team because we had basically to                                 find out about all the needs of our                                 product the hard way by talking to                                 customers by making lots of mistakes and                                 fixing them if we had had somebody at                                 that point in time who was more into                                 this whole thing we would probably have                                 been a lot faster so let me come to the                                 three stages of a machine learning                                 company that I or we have identified                                 during the course of the year so first                                 stage I would call the bootstrapping                                 stage and I always have put up pictures                                 of our offices onto these slides so this                                 was basically two or three people                                 sitting in a room trying to find the                                 right product first and then trying to                                 build a proof of concept and don't worry                                 these beer bottles were not always on                                 this desk                                 and then we somehow managed to build                                 this this proof of concept and then the                                 next challenge is to enter the market                                 and at that point in time hopefully your                                 proof of concept shows enough the power                                 of your innovation that you will                                 convince some venture capitalists to                                 give you some money because I mean you                                 might you might make it without them but                                 in many cases you simply need more                                 resources to be able to enter the market                                 and to start making customers happy so                                 you will need some venture capital and                                 this is basically the stage to where                                 it's really about to establish your                                 product get into the market and then                                 eventually if you're successful with                                 that you will get more and more                                 customers this will also mean you will                                 gather more and more data                                 and hopefully you will have the need for                                 scaling your product and of course you                                 should also add some more features make                                 everything more robust etc and this is                                 basically the the point in time where                                 you getting this big data region so                                 quick summary the three steps                                 bootstrapping find a proof-of-concept                                 funded startup may make the market entry                                 and pick data company it's all about                                 scaling so now this what is the                                 structure of a machine-learning company                                 well first of all you need a team I will                                 briefly talk about it in the end you                                 need a product I already talked about it                                 and you need an architecture because                                 this is what it's all about and since                                 this is always also tech company I want                                 to also focus on this technology part                                 for the rest of my talk and I think you                                 have seen similar images to that already                                 quite a lot of times and other talks on                                 this conferences because this is what                                 typical architectures for machine                                 learning stacks look like so you have                                 some source of events I heard the term                                 internet of everything yesterday because                                 it can be like the classical internet it                                 can be Internet of Things sensor stock                                 market data whatever so this is the                                 outside this is where the data comes                                 from then you usually have some real                                 time component which accepts these                                 events and which is basically the                                 gateway into your internal architecture                                 then these events are somehow promote                                 moated into a pipeline where they are                                 processed and they are usually also                                 stored and then in the case of a machine                                 learning company you will have some                                 machine learning algorithms that make                                 use of this data or be it in a batch                                 fashion by accessing the stored data or                                 in a streaming fashion by simply                                 listening to these streams and they will                                 do some math generate what I call models                                 in this slide and feed that back into                                 the real time part because of course you                                 want to answer questions from the                                 outside world in real time do                                 predictions                                 what was that in our case or for the                                 rest of the talk I mean basically I                                 think any of these parts would be at                                 work on its own so I will focus on the                                 data part for the rest                                 what was it in our case of the product                                 was a real-time bidding engine so the                                 data source is the internet classic                                 internet our real-time component                                 consists of a bidding engine and a                                 tracking engine these events are                                 receiving store and processed and stored                                 and our models are predictions for                                 clicks conversions and that sort of                                 stuff yeah maybe very brief interlude                                 what is real-time bidding maybe some of                                 you have seen the presentation by                                 critter yesterday morning well many                                 website owners or most website owners                                 actually make their money selling ad                                 spaces so you have a dedicated space on                                 this website that that that is sold for                                 for advertisement and how that usually                                 works nowadays is that the request is                                 forwarded to something called a                                 supply-side platform which is basically                                 a broker that offers this ad space to a                                 handful or a couple of hundred so-called                                 you mind sets demand-side platforms this                                 is what we are doing and then we have a                                 couple of milliseconds time to decide                                 whether we want to buy this ad                                 impression or not specialty website                                 owners sell their ads basis advertisers                                 buy the impressions yeah this is what                                 we're doing just to show again that this                                 has a real time component this round                                 trip is bound to                                                      let's look at the events again basically                                 the events that concern us so basically                                 it's the bid requests coming from the                                 supply side platform then we figure out                                 if we want to buy an advertiser in                                 compression if we do so we sent back a                                 bit and if we happen to have placed the                                 highest bid we will receive the                                 impression and later on we can track                                 the user clicked or in the best case                                 even bought something in a shop so the                                 goal for us from a machine learning                                 perspective is to predict these clicks                                 or even better conversions so now let's                                 have a look at the events or alone let's                                 sleep for for the goal form for a one                                 moment so what we have to pay is                                 basically every impression each time we                                 buy and displayed advertisement we have                                 to pay what our advertisers actually                                 want to want to pay is only for                                 successes so basically only when they                                 get a click or conversion later on that                                 means that the revenue that we gain is                                 only whenever success happens but so we                                 have to figure out what we're actually                                 willing to pay each time and this is why                                 we have to predict the probability of a                                 clink and this probability of click of                                 course is dependant of lots of features                                 via the user or the web site where is                                 browsing the browser with is using etc                                 so basically it's we predicted from data                                 yeah let's go into this data pipeline                                 now how does this data typically                                 typically look like                                 so it's essentially HTTP requests which                                 means that it comes with a cookie IDE or                                 with some user agent string with an IP                                 address with the URL that this user is                                 currently visiting all that sort of                                 stuff so this is basically the raw data                                 and well let's of course somehow unhandy                                 at least for human beings so one of the                                 very typical steps that happens is of                                 course that you take such a data point                                 and you see what features you can                                 extract from it it's basically from a                                 URL for example you can extract domain                                 and or even just a private part or from                                 an IP address you can extract you                                 information or you can extract the ISP                                 that this user is currently surfing with                                 or from a user agent string you can                                 extract information about the device                                 that this guy's using it's basically to                                 summarize the incoming events they sort                                 of have what I would call they they they                                 make up for the analytics dimensions so                                 this is basically where data scientists                                 will look into and will try to figure                                 out what interesting features are                                 actually in that raw data and on the                                 right-hand side after a step that we                                 call feature ization you sort of have                                 these extracted features something that                                 also humans can actually often reason                                 about so for example somebody or a                                 customer might want to know how many                                 impressions and clicks he or she                                 received in Berlin in the last few days                                 yeah so this is basically the data that                                 we were dealing with yeah and now let's                                 have a look at how this data processing                                 evolved in the three stages and what we                                 all what we did wrong on the way                                 all right so in the bootstrapping phase                                 you have to imagine we were just like                                 three people and we had to build such a                                 system so we did it please don't laugh                                 at me now when looking at these slides                                 so we did it very very simple we just                                 said this real-time tracking component                                 it was getting these events and it was                                 writing them as Jason to lock files we                                 then eventually compressed these log                                 files and rsync them to a separate                                 server because we didn't want to do our                                 math on the same server that was doing                                 the real-time part then for reporting                                 purposes we were basically just reading                                 these files and batches extracting all                                 the features that I showed you and                                 basically piping them into a my sequel                                 load data command so that we had at                                 least some sequel database where we                                 could could                                 reports and for analytics those who are                                 doing we were concerned with the data                                 science part of our product they were                                 actually taking enter our log files                                 parsing them into Python extracting the                                 stuff from the from the from the JSON                                 objects and then doing their machine                                 learning stuff with it                                 so this is of course I mean it's obvious                                 that this will not scale because I mean                                 having just one storage server at some                                 point the disk will simply run full also                                 we were ingesting single events into my                                 sequel and we were doing all                                 aggregations life there which of course                                 also became slow at some point in time                                 yeah and this whole handling these raw                                 lock messages with Python wasn't really                                 feasible or at least we wasted a lot of                                 time there but still with this very                                 simple setup we succeeded in building a                                 proof-of-concept and convincing some                                 investors to give us money so we were                                 able to hire more people have a bigger                                 team and of course what do you do if you                                 are stuck with your conventional sequel                                 staff you go to go Hadoop so let's have                                 a look what we did to these particular                                 problems that we identified okay for the                                 storage part straight forward we moved                                 to HDFS of course but we still did like                                 this first of all writing the data to                                 load the disk and then eventually                                 copying it to HDFS for the problem with                                 the live aggregations we switched to an                                 ETL process on top of Hadoop we already                                 back then                                 basically used hive on top of the gzip                                 json files that were in HDFS to generate                                 aggregates which we then simply exported                                 to phosphorus and for also some more                                 complex reporting queries we wrote some                                 MapReduce                                 jobs ourselves in Java and for these                                 analytics workloads                                 well now we had hive on top of the Jesus                                 JSON files and it was finally possible                                 to write queries on these on this data                                 on the raw data this unfortunately also                                 had problems with that because we were                                 writing these files in the on the                                 front-end service so we often had lots                                 of small files and you know lots of                                 small files in Hadoop is a problem which                                 is why we sort of build a heck of                                 materialized views in parkade format so                                 that we were able to query the raw data                                 faster so it was basically an                                 intermediate HEC at this stage already                                 but eventually the system grew and grew                                 and also this setup had its problems                                 actually many of them first of all we                                 were using Jason from the beginning and                                 I will come back to that later but at                                 some point we realized having this                                 flexibility in Jason can be a huge                                 benefit but it can also be a huge pain                                 because well you can't really rely on on                                 anything I mentioned already we had this                                 small file problem because basically the                                 files that ended up in HDFS they were                                 generated on front end servers which                                 well we had to find this trade-off                                 between timely ingestion like not                                 waiting too long on the front end                                 servers until the files are big enough                                 and yeah and managing somehow the file                                 size of the ground truth raw log files                                 but still we had not how many ingestion                                 then well this is just a detail we were                                 really unhappy with the Uzi like a Uzi                                 ADSL caused us a lot of pain I mentioned                                 it before                                 this whole ETL process was extremely                                 slow because it was running on the raw                                 data but we really wanted it to run on                                 the raw data because we wanted to make                                 sure that our reports are always based                                 on the ground truth which are our logs                                 that we will never touch again                                 also these MapReduce jobs were really                                 not not a lot of fun to maintain okay I                                 mentioned that already okay so what's                                 next oh there was one more of course we                                 had basically all our data duplicated                                 which is also not nice how to fix that                                 stuff for the no reliable data structure                                 part we moved to protobuf well you have                                 a schema there and this is something you                                 can rely on this becomes important when                                 you have many teams working with the                                 data it becomes more important that                                 people can rely on it for the small file                                 problem and the node family ingestion                                 problem and we just heard about it again                                 in the talk before we moved to Kafka                                 because that basically decouples the the                                 real-time component from from the from                                 this whole data pipeline because the                                 consumer of the events that come from                                 Kafka can finally decide how much it                                 will consume until it's bills to HDFS so                                 we actually have much better                                 manageability of file sizes in HDFS                                 which is actually a still I think one of                                 the most crucial parts unfortunately for                                 optimizing query speed well you see just                                 a tiny detail we switch to Luigi which                                 feels much more productive and for this                                 whole MapReduce stuff we started to                                 change that to spark because it just                                 also feels much more productive to work                                 with well now that we had this have this                                 whole Kafka pipeline in place we were                                 actually able to write our data directly                                 into Parque format which basically also                                 solves the last few problems because                                 we're no longer forced to do our whole                                 ETL stuff on top of gzip Jason but                                 instead we can basically do it quite                                 fast directly on pork eight tables and                                 this also made this whole analytics                                 workloads a lot faster because well we                                 can not only use all the tools that are                                 available now on in the Hadoop ecosystem                                 directly on top of Parque tables good so                                 let's summarize that the three stages we                                 basically managed to build a proof of                                 concept without a dupe but we of course                                 weren't able to to scale that but it was                                 fine because we still our main focus in                                 the beginning was still our machine                                 learning algorithms was not so much                                 about scale it was just about proving in                                 a smaller scale that that our technology                                 actually works but then eventually of                                 course we had to move to something                                 bigger had to introduce Hadoop and the                                 biggest mistake that we did back then                                 was basically that we kind of naively                                 just thought well we will we will manage                                 it and we didn't really take a lot of                                 consultant consultancy or hire any                                 experts so we also did a lot of mistakes                                 there and this basically forced us to do                                 another huge step of changes for finally                                 being able to to get to a scalable                                 infrastructure maybe some numbers just                                 that you have an idea and the first day                                 something like five                                 hundred requests per second on average                                 just ten gigabytes per day additional                                 data set growth which we handled with                                 with one server for a while then that                                 basically all multiplied by a factor of                                                                                                       lessons that we learned in this process                                 I don't know maybe some of you know this                                 comic so it's basically an analyst                                 asking an engineer how to query a                                 database                                 the new database and the engineer                                 replies well it's not a database it's a                                 key value store okay so the database how                                 directquery it                                 well you don't query it because you                                 write a distributed MapReduce job in                                 Erlang and the analyst asks did you just                                 tell me to go screw myself and the                                 engineer actually realizes well probably                                 he did exactly that what we learned is                                 that it will help you incredibly when                                 you're really concerned with with the                                 data business with a machine learning                                 business if you make all your raw data                                 accessible through SQL and if you make                                 sure that this that it is accessible in                                 a fast manner because this will allow                                 your data scientists to query that rate                                 data and to to look into it and they                                 will find out great things and if they                                 can't do it and they will also not do it                                 and this is also why I think it's so                                 great that we see so much progress on                                 this whole SQL on Hadoop thing and I                                 also think that it's basically the                                 explanation on why we see so much                                 progress on this SQL Hadoop same thing                                 because it's just so damn useful and I'm                                 also actually quite excited about what                                 for example Apache drill might might                                 bring here because that just goes into                                 the same direction what we also learned                                 is that you should probably leave other                                 ESL's like                                 more complex ones MapReduce peak                                 etcetera to the engineers because SQL is                                 actually almost always good enough                                 there's one interesting exception which                                 we are realizing at the moment which is                                 spark I mean everybody's talking about                                 it                                 anyways at least if they or if we will                                 see an improvement in the Python                                 interface that might be an exception to                                 this rule of thumb because it integrates                                 very nicely into all these machine                                 learning tools that many data scientists                                 really love to use another lesson                                 learned about introducing Hadoop well                                 maybe this is a bit outdated now from my                                 memories it was quite a rocky road to                                 introduce her to three years ago I think                                 it has become a lot simpler so maybe the                                 barrier is much lower now but you should                                 really ask yourself at what point in                                 time do you really need a dupe because                                 usually at least when you're doing                                 machine learning as a startup you will                                 not have big data because you don't have                                 the customers and the question is really                                 do you really need to build your whole                                 infrastructure based on Big Data                                 technologies right from the beginning                                 because before you have your first                                 customer so my answer to this question                                 is simply well when conventional SQL                                 stuff becomes too slow than the probably                                 the weapon of choice nowadays is                                 something Hadoop based yeah as I                                 mentioned many things can still be                                 achieved without it but when you do                                 introduces unless you happen to have                                 some Hadoop experts in your team already                                 make sure to find some because that will                                 allow you to avoid lots of mistakes that                                 stayed around for us with us for much                                 longer than necessary                                 for example the small-time problem that                                 I mentioned before or Big Al's political                                 files which also slow down the whole                                 thing or MapReduce jobs that we wrote in                                 Java where we have actually should have                                 chosen a better higher level abstraction                                 another lesson learned data format I                                 think Jason is great because it's human                                 readable it's very flexible                                 you will need this flexibility in the                                 beginning because you will change your                                 product over and over again you can even                                 I don't know listen to TCP streams and                                 parse it directly out of it because it's                                 human readable and not binary but at                                 some point it might also become a                                 problem and format with a schema like                                 proto before Avro or whatever might                                 become beneficial especially when you                                 have a growing team you need to maintain                                 compatibility between the teams and                                 components but if you do introduce                                 something like that you should really                                 make sure and we saw that in the talk                                 before as well you should really make                                 sure that you provide tooling for your                                 users to look into this binary format                                 because otherwise you are sitting in                                 front of your data and in the case you                                 have to debug it and you can just can't                                 look into it I linked a nice article by                                 Twitter here which also discusses this                                 topic alright so before I conclude let                                 me talk a little bit about team which                                 also goes hand-in-hand with the                                 technology decisions that we have seen                                 over time when you're bootstrapping                                 you're usually just a handful of                                 founders and with without money and so                                 you I don't know if you if you saw the                                 keynote this morning                                 then it's Eric Eric mentioned that you                                 will always have very short run runways                                 you will always have deadlines so you                                 better use the tools your most                                 productive with once we have some                                 funding and you can afford hiring people                                 I would say pays off to have a small                                 team of generalists because as I                                 mentioned before your product will                                 change over and over again so you rather                                 have some people that are flexible and                                 can change technologies and and work                                 flexibly on the on the product so you                                 also better use widespread technologies                                 where you can Google all problems but                                 then of course at some point eventually                                 for the sake of scalability you will be                                 able to build up specialized teams or                                 you will eventually write your own                                 database like many of the big companies                                 at all Twitter a build storm and                                 Facebook build high etc conclusion for                                 building a machine-learning company yeah                                 first of all most important thing find                                 the right product find something that                                 has the potential to scale otherwise you                                 will not become a big data company at                                 least then don't underestimate the whole                                 infrastructure development that you will                                 have to do around your core machine                                 learning ideas this will take up a huge                                 part of your effort something that we                                 realized too late and which would have                                 sped up a lot provide fast sequel access                                 to all your data at all time focus on                                 this right from the beginning when                                 introducing Hadoop either you already                                 have an export expert in your team or                                 find one and also important takeaway I                                 would say is start with a flexible yet                                 you and human readable data format                                 and that's already it I thank you for                                 your attention and I think oh well you                                 can reach me on Twitter or email if you                                 like otherwise I think we have a few                                 minutes for questions okay so
YouTube URL: https://www.youtube.com/watch?v=a5zLmRMfS9Y


