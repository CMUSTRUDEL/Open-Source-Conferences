Title: Berlin Buzzwords 2015: Julien Nioche - Low latency scalable web crawling on Apache Storm #bbuzz
Publication date: 2015-06-04
Playlist: Berlin Buzzwords 2015 #bbuzz
Description: 
	In this talk I will introduce Storm-Crawler, a collection of resources for building low-latency, large scale web crawlers on Apache Storm. We will compare with similar projects like Apache Nutch and present several use cases where the storm-crawler is being used.  In particular we will see how the Storm-crawler can be used with ElasticSearch and Kibana for crawling and indexing web pages.

Read more:
https://2015.berlinbuzzwords.de/session/low-latency-scalable-web-crawling-apache-storm

About Julien Nioche:
https://2015.berlinbuzzwords.de/users/julien-nioche

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              thanks for coming and thanks to you the                               organizers for giving me the opportunity                               to give these                               this talk so yeah this top is this                               working no it's not working no mines                               just use this some right i watch blood                               yeah no simple                               since we plug the sorry about that all                               right next year's a track for portal                                yeah okay and the arrows are not working                                right                                ah yeah see probably I've been to I                                think it's my third time here in Berlin                                it's nice to be back so some of you                                might have seen me in the past talking                                about other things if not so i run this                                door pebble which is a small control                                consultancy based in bristol in the UK                                doing pretty much anything with tags on                                a large scale so going from web crawling                                which is what I'll be talking about                                today to your natural language                                processing building search systems or                                machine learning what we do a dish of                                Babel is mostly using open source and                                especially Apache stuff so I use our                                contribute or I'm a committer encore fum                                                                                                       crawler which I'll mention in my sides                                but other Apache things like tea cow and                                so on who knows about apache notch okay                                well supposing and you so who's been to                                the talk this morning about storm yeah                                cool excellent great now the project                                that I want to tell you about today is                                called stone quarry which is a very                                original name and it basically does what                                it says on the tin it's about building                                web crawlers on Apache storm so it's a                                collection of resources it's an SDK the                                curd lives on github the it's and apache                                license and all the artifacts azhar                                maven central so you can pull the stuff                                the other jars in there in the usual                                ways it's a relatively young project I                                mean compared to notch it's been around                                for probably a half but it's it's very                                active it's growing fast it went from me                                being the only committed to having three                                more committees and yes we've just                                released a new version last week                                literally so what I wanted to achieve                                with the storm caller was to have                                something that that can scale what                                doesn't have to mean we don't all need                                to do billions of pages but something                                that can potentially scale if needed                                something that can do low latency also                                be able to use more easy to use and                                extend we'll have a comparison with much                                later and and also of course all the                                nice features that we want to have like                                processing site Maps or be able to do                                web scraping which is it of you know                                those people need to do be able to do                                that in a nice way and also your things                                like your site map politeness all these                                things that we want the web crawler what                                it's not being an sdk is a well packaged                                application so there's a minimum amount                                of code and configuration that you need                                to do there's also no a global global                                processing of of the pages like doing a                                pagerank like no she can do that's                                because it's that's backend specific it                                depends on on where you store your your                                your data and it will be done typically                                with them you know a MapReduce or a                                spark job so that's that's without side                                the scope of the storm corner and yeah                                it doesn't have any fancy you I just or                                dashboard it you use the you'd use the                                storm UI and but also you can plug the                                the store metrics to various back end so                                in a way it's quite flexible you can can                                integrate it in whatever you're planning                                to do with your project so yeah I                                mentioned nudge nudge is as you know a                                very popular piece of software not only                                because you gave birth to Hadoop just a                                minor detail but also because it's one                                of the few projects that you can use to                                build large-scale crawlers and I happen                                to know it pretty well I've been a                                committer on nudge fourth quarter but a                                while now so yeah people often ask me                                why feed on the storm quarter you                                already have nudge so one of the reasons                                was that yeah no chiz batch driven so                                 imagine you do you know it works by                                 steps you do a first step of generations                                 you select all the                                 or else that you want to fetch then you                                 do the actual fetching of these URLs                                 then the you pass them to get the out                                 links and also the actual content of the                                 pages and next step is to update the                                 what's called the crawl DB which is a                                 sort of database about the URLs and then                                 you can index of course the other pages                                 so that's five or six different steps                                 they all run sequentially which means                                 that when you're fetching you're not                                 passing when you're passing or updating                                 or generating you're not fetching so                                 you're not in terms of the use of the                                 resources is it's not really optimal you                                 should be you should be always fetching                                 always passing always updating so you're                                 optimizing your network you'll CPU your                                 disk access everything the main thing is                                 that with all these bad jobs you have                                 very little control as to when URLs                                 actually fetched which means that for if                                 you need to handle low latency imagine                                 you have you need to scrape a site that                                 uses sessions sessions are often time                                 limited with with nerds you have to do                                 all sorts of contortions to be able to                                 do that it's not a very natural fit with                                 the stone color yeah it's a much better                                 fit what else yeah but storm corners                                 were is a lot more flexible to use very                                 often you just reuse the existing                                 classes in the SDK and I've had quite a                                 few users who literally had like three                                 java classes a couple of configuration                                 files and bank they could be a scalable                                 crawler just just with that so it's um                                 it's quite elegant in a way so but it's                                 not you know as ready to use as much of                                 course not you install it and hopefully                                 luckily you are you able to use it and                                 that's it you can play the configuration                                 again the song caller is nasty case you                                 need to build a bit of code but you know                                 a lot of the sum of the codes and all of                                 the concepts were both from nudge so                                 some color pretty would not existed                                 without it but the nice thing is that                                 a man should contribute some of this                                 stuff back to nudge so it works both                                 ways so that there was a talk this                                 morning about storms was really good so                                 I can go very quickly over the my storm                                 slides so yeah stories a distributed                                 real-time computation system top level                                 project and apache so mix of closure and                                 java and it's used for all sorts of                                 things you're terminally takes continues                                 computation and so on twitter very                                 popular piece of software the concept                                 and that's where we like with with storm                                 is that there are it's nice and easy to                                 understand but it is quite powerful at                                 the same time so as we've seen this                                 morning have spouts that generate                                 doubles on two streams and then you have                                 bolts operating on these tuples and                                 modifying them or creating new ones so                                 basically a spout both streams so what                                 do we have in the toolbox what does                                 strong quarter provide as a project so                                 we've separated the occurred into two                                 modules one core along with year the                                 stuff you would necessarily probably                                 definitely use with the somme quarter                                 and the other one we put all the young                                 stuff which is related to external                                 projects like elastic search or tikal                                 just to keep things separated and simple                                 they also got it here walker if you at                                 least one repository of external                                 contribution so people have their own                                 github repository wow already half we                                 have to speed up so next one so the main                                 both you need is obviously the one that                                 does the fetching so it's a yeah the                                 ones don't of you who are familiar with                                 natural recognized some of the some of                                 the concept so it's yeah multi threaded                                 rod bolts that has internally multiple                                 threads to do the fetching and puts all                                 the incoming tuples in                                               that's way of handling the uprightness                                 that you make you can you can have                                 control as to the the rate at which you                                 send queries to your server there's also                                 a simpler version of the bolts                                 but that other one requires two terms or                                 the politeness at the spot level yeah                                 and there's the the protocols are it's                                 nutan protocol neutral in the sense that                                 you know the protocols are a separate                                 component it's not part of the defector                                 itself so you can can we use the default                                 one based on them apache HTTP client but                                 you can also write a custom one if you                                 needed to the second most important                                 component is the one that does the other                                 passing of the pages so this one is yeah                                 it's based on J soup and it handles on                                 the HTML but if you need other formats                                 remember I mentioned earlier there is a                                 one based on tika which is in the                                 external part of the code and so this is                                 possible to extract the text from the                                 documents and also the Earthlings and                                 endless metadata from from the web page                                 but it does two things it calls the url                                 url filters which are pluggable sort of                                 glasses which allow you to normalize or                                 filter out links so it's basically how                                 you control the expansion of your you're                                 cruel and also pass filters which are                                 again pluggable classes which is where                                 you can do for instance extract metadata                                 with xpath or do whatever you want to do                                 in terms of scraping so these are the                                 most commonly use things unless that's                                 all all happens within them j tsubasa so                                 as I said yeah pass filter is just                                 basically a simple simple interface it's                                 configured with the JSON file and it                                 comes with a few default implementation                                 so there's this one BMX pass filter                                 where in the adjacent file you just give                                 it a number of XPath expressions and                                 then it will store all the matches into                                 the metadata for the page so it's nice                                 and easy way of doing scraping really                                 same with your URL filters so that's how                                 you control the how your your call                                 expands and yeah there's number of                                 filters available I won't go into the                                 details because I'm running out of time                                 so a very basic topology                                 like this you have some spout putting                                 your l's from say a queue or any any                                 form of what some form of external                                 storage it has to come from somewhere                                 and then yeah some partitioning to make                                 sure that the URLs are sent to your                                 single so your else from a single domain                                 go to the same instance of a fetcher                                 again you can have just more than one                                 instance of you see of all these bolt                                 yeah the past thing and then you do                                 something with it you would index it                                 with solar elasticsearch or will store                                 it in a database so that's a very very                                 very basic topology from the point of                                 view of the topples this is the sort of                                 fields and topples pass between the the                                 these various components yeah the slides                                 would be online be easier to find them                                 later so as I said the spout could be                                 something as simple as rabbitmq or AWS                                 SQS and the indexer can be elastic shot                                 search or solar all the components in                                 gray are you know the ones provided by                                 storm crow by default so what and then                                 some of the new ones for the indexing                                 could also come from some Cola so it's                                 very likely you'll need to basically                                 just provide a spout implementation the                                 topology class itself and that's it but                                 but your uncle right what about                                 recursive calls so when we have that                                 works fine when you have just a set list                                 of URLs and everything but when we have                                 recursive calls we need something a                                 little bit more or failure you need                                 something a little bit more complex so                                 that's what a proper call would look                                 like you have you start from seeds and                                 then you follow our link so some some                                 links can take you back to the URLs                                 you've already visited some of the links                                 you don't want to go the ones with the                                 year your stop sign that's what you need                                 the your url filters for so one thing                                 that the storm called stone color does                                 for handling guests like like this is                                 that we use I mentioned this                                 rooms before and in storm you can have                                 there's a default stream but you can                                 have as many different streams and give                                 them a name as you want so the strategy                                 we use is to have one which is called                                 status so the components like the                                 fetcher or pass will omit the the normal                                 or the output for a given document on                                 the standard stream but also generate                                 other typos on the status stream to send                                 that to another bolt that you'd have                                 which is I called it state to sub data                                 here and that boat will be in charge of                                 basically send that to some sort of                                 storage where a DBE or elasticsearch or                                 whatever you want what you'd have you                                 know the urls will be stored you'd                                 basically have one entry per URL if you                                 are sending that back to cuse yeah you                                 don't want to add a URL to the queue                                 every time you find it as not linking so                                 bit more more complex but yeah with                                 these two streams you can that's how you                                 can deal with it so yeah that's the                                 status that topple can have so it's been                                 either discovered or already fetched or                                 every direction or an error if you think                                 of you have a dodgy PDF document and                                 Nick mentioned some of the stuff you can                                 come across with with tika for instance                                 um so yeah you can get an hour so the                                 status of data just its function is to                                 write to the storage and and what you                                 can do is you can extend there's this                                 natural abstract class you can reuse and                                 makes your life easier to write such                                 components yeah I mentioned the core                                 part of the project and the external                                 parts there is there are very few of                                 things for elasticsearch I mentioned                                 them now not now that we've seen the                                 other state to stream and so on so yeah                                 there's obviously an indexer bolt that                                 takes document document index index them                                 but there's also a status of data boat                                 and an elastic search spout bolt so                                 these are useful doing recursive calls                                 and there's also one thing I read like                                 is the matrix consumer so storm can                                 generate metrics as now                                 CPI for that and this class allows you                                 to send the metrics to elasticsearch                                 then you can you can use cabana for                                 instance to display the metrics of your                                 corner so you can see the pages per                                 second and so on so I've already                                 mentioned some of it so how to use it                                 well the thing you need to do is maybe                                 at least write a topology class so where                                 you define you know the bolts and are                                 they they they are connected to each                                 other or you can take the example worn                                 in code and Hacket you probably need to                                 write if your resource files for their                                 URL filters and pass filters package the                                 stuff with with maven or any other tool                                 you like and and then yeah you call                                 storm on it and bang you have a crawler                                 that can scale and with minimal amount                                 of code to write and minimal                                 configuration so that's why the topology                                 class would look like and I'm not sure                                 you can see very well but yeah we can                                 see all the different bolts and how they                                 are connected and an RV they are grouped                                 as well just got important for                                 politeness and also for performance                                 that's year the storm you I so if you                                 launch storm and distributed mode you'd                                 see something like this you can see how                                 your boats are performing and the                                 latency of each bolt so you can you can                                 find bottlenecks it's a pretty good                                 pretty good tool and a good way of                                 monitoring your your crawler so quickly                                 through that it's been used them yeah                                 bye-bye I've used it for califica lines                                 and political committees well people who                                 are now committers have used it for                                 their for that project so we've had                                 cases from in a very simple ones where                                 you have a list of it existing list of                                 URLs we wanted to fetch and pass them so                                 that's pretty straightforward so that                                 was just rabbitmq as a for the spelt and                                 then elastic for the indexing to pretty                                 much every possible use case in between                                                                                                          full recursive thing and I choose                                 dynamic DB and that's was running on it                                 that's running on production and on on                                 ec                                                                      point                                   there's been some work done on the                                 documentation but who likes writing                                 documentation so this yet if you                                 definitely be more work on that also                                 said that's an sdk once we probably try                                 and do and have like an example that you                                 know an example project showing the fool                                 because if corner things probably based                                 on the existing components we have for                                 elasticsearch that would be a good                                 example of to show how to use them use                                 storm quota and yeah probably add add a                                 few more paths and URL filters and and                                 and so on one thing that will certainly                                 happen is to have a selenium based                                 protocol implementation so that you can                                 deal with these pesky Ajax pages that's                                 it few resources so there's a very good                                 book published by Manning on storm which                                 will really helped me on understand it                                 better but the otherwise yeah the body                                 said the project on github and as yeah                                 links for storming notch as well that's                                 it journey questions thanks let's oh I                                 think the speaker for observers
YouTube URL: https://www.youtube.com/watch?v=97G2Ur2VIPI


