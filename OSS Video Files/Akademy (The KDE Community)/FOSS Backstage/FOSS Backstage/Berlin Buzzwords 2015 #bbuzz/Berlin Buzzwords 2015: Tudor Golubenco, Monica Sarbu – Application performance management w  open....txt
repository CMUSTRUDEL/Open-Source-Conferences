Title: Berlin Buzzwords 2015: Tudor Golubenco, Monica Sarbu â€“ Application performance management w  open...
Publication date: 2015-06-02
Playlist: Berlin Buzzwords 2015 #bbuzz
Description: 
	The talk proposes and demonstrates a methodology for monitoring  and troubleshooting the performance of a typical web application by using Elasticsearch as an event analytics engine. It discusses ways of collecting performance metrics for the transaction data (response time, error codes, URL path, data base tables, etc.), the appropriate level of details captured, sampling techniques and tips for efficiently storing the data.

It then shows how to use Elasticsearch aggregations to provide the application performance specific metrics (response time percentiles, error rates) at a global level and also segmented over multiple dimensions (service, server, URL path, etc.). It shows how to use these techniques and visualisations to perform a top-down analysis through the data in order to identify a performance issue.

During the talk, the audience learns how Packetbeat, Yahoo Boomerang, Logstash, ElasticSearch, Kibana and Bonito work together for a complete application performance management solution that is more applicable and more flexible than the commercial offerings.

Read more:
https://2015.berlinbuzzwords.de/session/application-performance-management-open-source-tools

About Tudor Golubenco:
https://2015.berlinbuzzwords.de/users/tudor-golubenco

About Monica Sarbu:
https://2015.berlinbuzzwords.de/users/monica-sarbu

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              thank you and welcome to our talk so                               for background information on us we are                               too soft                               we have lots of experience in a                               voice-over-ip monitoring because they're                               used to your work on a startup that was                               doing just that a voice over IP                               monitoring product this startup was                               acquired first by Acme packet which at                                its turn it was acquired by Oracle and                                then we started to work the two of us                                started to work full-time on this packet                                did project that we're going to talk                                about today and because we are on the                                scale track of the of the conference I'm                                going to start by talking about scaling                                a little bit and there are two aspects                                of scaling that I find interesting one                                is the technical side as your startup                                has a lot of success and you want to                                reach more and more users you will scale                                your infrastructure to tens hundreds                                thousands of servers at the same time                                there's the organism Oracle aspect in                                which as you grow your company you scale                                to hundreds thousands and tens of                                thousands of employees may be engineers                                product managers marketing people and so                                on and it's interesting that these two                                aspects of scaling are actually closely                                interlinked they are linked by what's                                known as the Conway's law which says                                that organizations which design systems                                are constrained to produce designs which                                are copies of the communication                                structures of these organizations right                                so what often happens is that at the                                beginning the organization looks                                something like this it's split into a UI                                team back-end team a database team and                                and so on                                and the system that they produce also                                looks like this right this is how it                                happens as the company grows they start                                to realize that this doesn't scale very                                well and for multiple reasons but one of                                the reasons is that when new features                                comes up there needs to be implemented                                it affects all these teams and all these                                parts of the systems so you get a lots                                of interlocking between them and lots of                                communication overhead so that's why in                                the most more recent years people have                                started to adopt micro services they are                                used to successful                                by companies like Netflix and Spotify                                and so on where they split the                                functionality of the of the overall                                service in lots of small small services                                and you have small teams working on them                                so each of these services are                                self-contained they talk between                                themselves with an API so they can be                                simple they can be written in different                                programming languages which is a good                                way of bringing fresh technology in your                                company and everyone is happy about this                                usually and while each of these systems                                in by themselves are very simple the                                overall system gets can get quite                                complex right the complexity is not                                removed is more like moved into the                                communication between them and in fact                                it often happens that things get so                                complex that no one in the company has                                an overview of really what's going on                                with the whole system and is that a bad                                thing or is that a good thing I think                                it's a good thing and I think we have to                                accept the fact that if we want to                                create truly advanced systems they will                                be more advanced that any of us can can                                understand them even if we help help                                creating them and in fact I think this                                is how if we do read this right this                                will resemble the way evolution works                                right just like the DNA evolves to adapt                                to new environments by mutations you                                have your system which adapt to new                                requirements using git commits and pull                                requests and so on right and it's                                important to choose the right mutations                                because if you choose the bad mutations                                you're going to be extinct as a company                                right or you will become a rat or a                                kitchen bug or something like this                                that's not very pleasant                                on the other hand if you choose all the                                good mutations you will become maybe one                                 of the greater apps it's or dolphin                                 which is the most intelligent lifeform                                 on earth okay and how do you how do you                                 choose the                                 good mutations from the bad mutations                                 they are of course multiple multiple                                 approaches that you can have but I think                                 one of the things that's really                                 important is monitoring you you need to                                 define a set of KPIs also on the                                 business tiles side of things you have                                 KPIs and you apply mutations and you see                                 if things are going well or things are                                 going bad                                 and equally on the technical side you                                 should monitor things like response                                 times latency availability error rates                                 and so on so that you know when things                                 are going well and where things are not                                 going well also when things are not                                 going well you want to be quickly be                                 able to identify the mutations that are                                 problematic so you can remove them as                                 soon as possible so because we recognize                                 that this problem is is both very                                 important and also difficult we decided                                 to work on this on monitoring and                                 troubleshooting distributed applications                                 okay now let's see what our requirements                                 for this kind of monitoring solution so                                 it should be scalable and reliable I                                 will say it needs to be more scalable                                 and real reliable than an application in                                 monitors it has to extract data from                                 different sources it has to have low                                 overhead                                 we don't want to question if we want to                                 enable monitoring or not low                                 configuration in a highly dynamic                                 infrastructure servers go down and up                                 and we don't want to do any manual                                 configuration and it should be simple                                 and easy to understand so everyone from                                 the company should be able to interpret                                 what the monetary system it shows so we                                 think a good start for monitoring                                 solution is to look at the communication                                 between the components distributed                                 systems nowadays they are very different                                 one from each other they use different                                 programming languages different framers                                 even different operating systems but                                 what they have in common is the                                 the communication between them it's                                 using the network and the protocol they                                 are using are more and more standard for                                 example they use HTTP API and different                                 RPC is like JSON RPC treats RPC they are                                 so proto database protocols like my                                 sickle and Postgres SSL and so on                                 my key data is objective so we don't                                 have to rely on locks when we want to                                 debug an issue and no latency overhead                                 so it works the monitoring solution it                                 works by having a cop looking at a copy                                 of the traffic okay so we started a                                 packet bit and the packet with project                                 and it was launched almost one year ago                                 it's open source is written in golang                                 yeah so let's see what is pack a bit so                                 pick a bit consists of a qubit shippers                                 that are installing an application                                 servers they follow the TPC TCP stream                                 stickers upper layer protocols like HTTP                                 red is my sequel PostgreSQL thrift RPC                                 and so on then they correlate the                                 request is a response and they get data                                 and measurements from from the                                 transactions and also from the from the                                 environment and generate a JSON object                                 for each transaction here is an example                                 of a JSON object this is the Select                                 query to a post SQL as you can see the                                 method is select the number of rows                                 returned by the queries to response time                                 is                                                                     representing invite out so we have all                                 the shippers installed on your in an in                                 your system and they are collecting a                                 lot of data what we do is the data why                                 can we analyze it how can you analyze it                                 so the traditional way to do this                                 first you decide what matrix you want to                                 measure for example the number of                                 requests a server can have or the number                                 of the ORS the response time percentage                                 then your write code we strike the                                 matrix and then you store them in a                                 database for example it can be a time                                 series database and if you want to drill                                 down to look at the transactions that                                 cause an issue or a peak then you have                                 to store the transactional database and                                 this time this database can be a normal                                 database and features like drilling down                                 and topping queries are very difficult                                 to implement in the traditional way                                 because you have the matrix and the                                 transaction stored in different way so                                 our solution to this is to use the elk                                 spec and to push all the generated JSON                                 object to elasticsearch and here we have                                 optionally you can use Redis and                                 logstash                                 to have a bit of buffering before                                 inserting the elasticsearch additionally                                 you can use log stash for changing the                                 data for example if you want to add a                                 new tag or you want to change your field                                 name you can do that with log stash and                                 if you you if you want you can also                                 configure package B to insert the data                                 data directly into elastic search on the                                 top of the elastic search you can use                                 Cabana to visualize the data yeah so why                                 did we choose Alex death because it                                 proved to be a good solution for logs                                 and our solution is more more or less                                 similar with this and it's scalable it                                 proves that it there our installation                                 with more than a million events per                                 second it offers the clear and simple                                 flow for the data first you have the                                 shippers that are generating the data                                 then optionally you have the log fish                                 that are transporting the data                                 then you have the elasticsearch that are                                 in this in the data and Cabana to                                 visualize the data and the most                                 important one is that you don't have to                                 create the metrics beforehand and it                                 comes with features like drilling down                                 and top and features out of the box                                 all right now to part the visualizing                                 part of the data which is which is more                                 fun I think so we're using Cabana Cabana                                 for for visualization this is the                                 discover top for it you might know it                                 and we just show here how easy it is to                                 find all my sequel errors for example                                 right we put a filter on status not okay                                 and a filter on type my sequel and there                                 you have it you can also of course the                                 free text search because well it's                                 elastic search so we can search packet                                 beat also comes with a set of predefined                                 dashboards right so usually when you see                                 Bona you start with a blank page and you                                 have a bit of work to create your                                 widgets and so on but we speak a bit                                 because we know exactly how the data                                 looks like we can predefined some of                                 these dashboards for you so then you                                 have some pretty good examples                                        dashboards that you can you can start                                 from for example here in the first row                                 we have a simple count of the of the                                 different transactions types so for                                 different protocols HTTP database RPC                                 and so on and on the second row we have                                 some little bit more advanced                                 visualizations for response times based                                 only on the response times and I                                 actually that's what I want to go a bit                                 deeper into for example this one the                                 percentage values over time they are                                 done in elastic search by combining the                                 date histogram with the percentage                                 aggregations in general if you have a                                 monitoring solution that gives you just                                 averages for things like response time                                 or load time or delay and so on that's                                 not very good right averages are really                                 not good for that thing it's much better                                 to use things like percentiles and                                 luckily in elastic search there                                 the percentile segregation which does                                 exactly that this is how the request                                 looks like looks like you just give it                                 the field where the data is found in our                                 case in the packet the data case it's                                 called response time and the percents                                 that you want and this is how the                                 response looks like right                                 you just get aggregation and free values                                 one thing you should know is that the                                 percentage aggregation is using an                                 approximate algorithm it's the digest by                                 Ted learning and the funny thing is that                                 Ted Anning has talked the Balian                                 buzzwords I think right here just after                                 lunch so I'm not going to talk too much                                 about this but what you should remember                                 is that it's an approximate algorithm                                 but its properties make it very very                                 convenient for for this type of data                                 that we have okay and I as I said we                                 need to come to create that graph we                                 need to combine it with a date histogram                                 and what the day histogram does is just                                 blitz your your time interval in a small                                 in your data in a small time intervals                                 small or large in this case we use one                                 minute but it's important to note that                                 it can actually be one second or one                                 millisecond so there's really no                                 limitation in the resolution you get on                                 graphs which i think is pretty cool                                 all right and if you if if you so if you                                 send that the request you get a response                                 like this the key is always the start of                                 the interval so the of the time interval                                 in this case and the top count is                                 included by default and in order to                                 produce that graph we combine the two so                                 you have a date histogram and nested                                 within it is the percentage aggregation                                 that we saw before and if you put them                                 together the answer looks like this so                                 now you have a list of intervals and for                                 each intervals the free values for the                                 for the percentile three we requested                                 okay so if you plot that it looks like                                 this which is what we wanted to start                                 from now the Cabana config for that it                                 it's also very simple on the y-axis you                                 select the percentage percentage and on                                 the x-axis you select the date histogram                                 and this is how Cabana creates the                                 request very similar with whatever I                                 show it before and you get dive graph                                 right                                 okay so this is the latency histogram                                 that shows it so this is a good way to                                 show you an overview of the response                                 time of your application for example in                                 this diagram you see that most of your                                 requests are a certain lasted                                    milliseconds and there are a few that                                 take up to                                                       implementation elasticsearch you create                                 a histogram aggregation on the response                                 time field within                                                 interval and this splits the data in in                                 buckets by response time and you have a                                 bucket in the head time interval from                                   to                                                                     one from                                                       milliseconds and so on this is how the                                 request will look like and the response                                 that we get back contains the key which                                 is a start of the interval that in the                                 dock count which is a number of                                 transactions that are included in this                                 bucket if we put this in a graph we get                                 something like this and we get a later                                 cigarette the histogram that is                                 constructed for the entire interval but                                 what if we want to see the evolution of                                 the latency histogram over time we can                                 implement this in any elastic search by                                 combining with a date histogram                                 aggregation and is as you can see in the                                 request we have our latency histogram                                 nested in the                                 that histogram aggregation and this is a                                 response that we get back for the                                 request and it contains a list of                                 objects for each interval and each                                 object contains all all the buckets with                                 all the values for the response time and                                 if we put this in a graph we see                                 something like this where each part is                                 represented it is representing in a in a                                 time interval and the stacked colors                                 represent latency histogram if we want                                 to configure this in key bar now we need                                 to set the y-axis to count and as access                                 is to date histogram on time step and                                 also to add the subrogation histogram on                                 the response time all right                                 another interesting feature that we make                                 use of a lot from elasticsearch is the                                 aggregations where you do top                                    something right so you can do for                                 example top                                                              or top                                                           requests and so on right these are very                                 useful for monitoring and let me pick                                 one of the examples here it's it's maybe                                 not the most visually appealing one but                                 it's very useful it so shows you the                                 slowest RPC methods sorted not by                                 average as I said the average doesn't                                 really make sense for this kind of data                                 but but by the                                                          way that works is that it combines the                                 terms and percentage aggregations the                                 term segregation it splits the data in                                 buckets like any aggregation does but                                 what's different here is that it the                                 buckets are dynamically built right it                                 creates one for each unique value and by                                 default it only returns the top                                          document count because of course there                                 can be lots of unique values in the                                 general case                                 so the yeah it's also important to note                                 here that the counts are                                 proximate because each sharp might have                                 a different opinion about what the top                                                                                                          it precise from there that reason but                                 the good news is that elasticsearch will                                 tell you how big there or can be so if                                 you send this request                                 where will you just choose field method                                 and size                                                                 and you see in doc count error upper                                 bound you get that in this case there is                                 no error that you should expect and then                                 you get the list of as we as I did it                                 here with no without any ordering just                                 the default one by dog count you get the                                 most frequent RPC method calls from from                                 the data right which are calculate                                 depending in this case but now if we                                 want to order it by the                                                  do that by using a sub aggregation on                                 the on the percentage and use the order                                 close to to say that which which of                                 these sub obligations we want to use and                                 order descending or ascending so if you                                 send them this request then the response                                 will look something like this                                 right so it's it's it's no longer sorted                                 by dog count but it's sorted by the                                 response times percentiles and you also                                 get the values there so you can Det use                                 that to put into a into a graph if you                                 want to do this in a in Cubana                                 you choose the metric percentage and                                 then you can when you configure the                                 terms aggregations it offers you to                                 order by it so that's also very                                 convenient all right before we move to                                 the next thing I just want to mention                                 and a couple of tips one is that there                                 is a live demo demo dot elastic dot go                                 slash packet bit which you where you can                                 see and play with this this type of data                                 I also put examples for aggregations                                 that we have in this github repo and                                 also I wanted to mention that you should                                 use sense it's a chromatin I have to                                 admit I learned the way to too late                                 about it and I                                 fiddling with Carl to do these kind of                                 things you should yeah if you just in                                 case you don't know about it                                 you should really use it right another                                 tip is that when you use Cabana you can                                 always see what aggregations are sent                                 this was reality in Cabana free as well                                 but it's also works in Cleburne ofor so                                 you can click on the on the arrow there                                 and choose the request tab and you can                                 see exactly what the aggregations are                                 sent to elasticsearch so it's a good way                                 of learning new aggregations you just                                 look how Cabana does it okay now let's                                 talk a bit about the future and as                                 Monica said in the beginning we the the                                 problem we are trying to solve was about                                 monitoring distributed systems and we                                 thought it's a good way to start with                                 packet did they respect data but that                                 was always meant to be just the                                 beginning the beginning and actually we                                 wanted to add over time more sources of                                 data so like OS readings for CPU memory                                 and so on maybe called instrumentation                                 tracing data from API gateways or just                                 internal stats from from different                                 servers right and we are talking with                                 Shai been on who you might know is the                                 city of elastic the company behind                                 elastic search cabana and log stash and                                 we realized that we have quite a bit of                                 a common vision regarding these things                                 so so here's some great news to announce                                 that we recently join forces elastic and                                 so packet bit will is from now on is                                 part of elastic project is one of the                                 elastic project like a last like keep an                                 eye on the log stash and we also also                                 have to change your bits of mass code to                                 include the elastic bubble and we are                                 keeping the project open-source moreover                                 we change the license from GPL version                                   to a posse                                 and yeah yeah we can yeah so our mission                                 is inelastic now is to ship operational                                 data to elasticsearch not only a packet                                 data but all kinds of of data right so                                 we we came up with this idea that we can                                 we have now packet like we have now                                 packet bit for packet data we could have                                 also filed bit for log files for example                                 this actually already exists in the form                                 of locks that for order but we want to                                 improve it and make it a real bit and in                                 the future we can also have other other                                 bits as well these are just ideas but                                 they could have like a top bit for CPU                                 memory and so on a metric speed which                                 could use plugins like Nagios or sense                                 of dies or you can have ROM bit which                                 comes from real user monitoring bit                                 which takes data from the browser these                                 are just just different ideas that we                                 could implement in the future yeah so if                                 you want to stay in touch with us we                                 have a Twitter account take it at packet                                 bit we also have a forum and discuss dot                                 Elysee dot coded slash this slash bits                                 and also we are preparing for the                                 webinar and we encourage you to sign for                                 our webinar the webinar will be a bit                                 different than our talk it will contain                                 more practical hands-on like how to                                 configure Becky B's how to run it and so                                 on so feel free to sign up alright and                                 that this was it if you have any                                 questions
YouTube URL: https://www.youtube.com/watch?v=x5vA-HIYHmE


