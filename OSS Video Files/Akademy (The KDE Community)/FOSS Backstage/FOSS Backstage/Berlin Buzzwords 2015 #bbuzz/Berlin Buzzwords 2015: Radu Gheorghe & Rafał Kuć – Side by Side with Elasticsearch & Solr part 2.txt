Title: Berlin Buzzwords 2015: Radu Gheorghe & Rafał Kuć – Side by Side with Elasticsearch & Solr part 2
Publication date: 2015-06-04
Playlist: Berlin Buzzwords 2015 #bbuzz
Description: 
	A brand new take on our Solr & Elasticsearch talk from last year! As the title suggests, this time we’ll dive deeper into how these two search engines scale and perform. And of course, we’ll take into account all the goodies that came with Elasticsearch and Solr since.

Both search engines are based on Lucene, so you’d expect similar numbers, but:
- at scale, small differences can give different numbers
- as with most functionality, Elasticsearch and Solr take different paths to achieve similar results, so you’d tune them differently
- their distributed models are quite different

We’ll show how you’d tune Elasticsearch and Solr for 2 common use-cases - logging and product search - and what numbers we got after tuning. Also, we’ll share some best practices for scaling out massive Elasticsearch and Solr clusters. For example, how to divide data into shards and indices/collections that account for growth, when to use routing and how to make sure that coordinating nodes don’t become unresponsive.

By the end you’ll see how Elasticsearch and Solr compare when you dive deeper into their functionality. You’ll know which important Lucene knobs to turn and how to do that in each search engine. Also, you’ll know how to use specific scaling features such as automatic rebalancing for Elasticsearch and shard splitting for Solr.

Read more:
https://2015.berlinbuzzwords.de/session/side-side-elasticsearch-solr-part-2-performance-scalability

About Radu Gheorghe:
https://2015.berlinbuzzwords.de/users/radu-gheorghe

About Rafał Kuć:
https://2015.berlinbuzzwords.de/users/gr0

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              Oh                               thank you very much for coming welcome                               to                               Chancellor the lettuce this time I will                               focus on a bit of the scaling and a bit                               more of the performance first let us                               introduce ourselves we are both search                               consultants at SEMATECH stoke-on-trent                               is toward the latest he's also a Family                                Guy                                these are hips his two kids he's only                                two kids not only playtest                                yeah thanks                                okay and this is Raju as he said we                                worked together                                he's also consultant the trainer he also                                happened to write a book elasticsearch                                in action which are actually personally                                like if you want there is a discount                                code he also is a father and the family                                guy and but that's for the introduction                                let's ask continued why do you do okay                                so last year we did the side by side                                with soul and elasticsearch talk in buzz                                words and we talked briefly about both                                of those great products so we took the                                opportunity to talk to tell you about                                how the indexing works how the querying                                works what are the other functionalities                                like that data analysis faceting                                consoler site aggregations on                                elasticsearch we also we also looked at                                the tools available and other things                                like the api's we also compared the                                community engagement in for those both                                products like committers code the                                mailing list activities and stuff like                                that basically the last year conclusions                                were that most project worked well with                                both solar and elasticsearch there are                                very very small amount of showstoppers                                that would say don't go elasticsearch                                don't go solar but there are a very                                large number in small differences                                because those are different products                                based on leucine                                in addition we said that choose the best                                that you like that your comfort                                with choose the one that you worked for                                your use case and that's how it go went                                last year however we wanted to also                                bring you back with the retrospection of                                what happened during that year between                                those two talks so last year which we've                                you've seen a slide like this so we                                talked that wind solar we will see the                                faceting by function analytics component                                and solar a standalone application now                                when the release candidate tree is                                already in the works for Solar version                                point five point two all those things                                are active and we can use them with a                                stable release almost the on the side of                                elasticsearch we said the top hits                                aggregation to enable field collapsing                                and not only the minimum should match on                                his child to allow us to actually match                                documents on the number basic on the                                number of the children hits and the                                filters aggregation which we can use                                filters to pocket the aggregations and                                that's again everything in current in                                the current versions we use which for                                elasticsearch is one point five                                something there but that's not all                                in the upcoming releases actually we'll                                be seeing a lot more from those two                                products starting from elasticsearch                                into o will see the computational                                aggregations introduced which allow us                                to do computation on the pockets of                                aggregation which is again great because                                it for example allows us to do more                                sophisticated data analysis like the                                moving averages aggregation which is                                already a part of to over release which                                will be released hopefully sometimes                                this year we will have the cluster                                different support to easier handle                                accuse cases where you have a large                                number of shards in your cluster and the                                shadow replicas are already here which                                means that you can just reuse the binary                                copy of loosing index instead of using                                the transaction log and index on each of                                the replicas on solar side five to                                version and five one starting from five                                one we have the JSON facets that we can                                use JSON similar to what we have already                                 in elasticsearch to construct our files                                 that will say have the nested facet                                 point two will have the backup and                                 restore functionality with your                                 applications and allowing us to just                                 back up and use the backup with an API                                 call again similar to what we already                                 have in elasticsearch the streaming                                 aggregations which allows us to build                                 very fast processing for the data like                                 sorting engines and the cross data                                 center replication which is hopefully                                 coming in five point three which allows                                 us to replicate the data between data                                 centers and have more fault tolerant                                 setups but for this year we wanted to                                 tell you about three things first of all                                 some kind of theory behind horizontal                                 scaling of both because they are similar                                 when it comes to the theory and quite                                 different when it comes to practice then                                 we'll show you two use case the product                                 ones and the logs use case will show you                                 the performance in those cases and a few                                 nice lights hopefully so when it comes                                 to horizontal scaling in theory we have                                 the nodes in both elastic certain sore                                 will each index in elastic social                                 collection in solar is built of shard at                                 least one we can have multiple nodes                                 then we need multiple shards so our                                 index or collection needs to be built of                                 multiple shards divided and put into                                 onto the nodes this is done                                 automatically of course if we need to                                 plan for the future we can have multiple                                 shots already put already created nodes                                 so we have over sharding to allow us to                                 split to place the shard on new nodes in                                 in the future whenever we walk of course                                 for a rich throughput increase we have                                 the replicas for each shard we can                                 create both in elasticsearch and solar                                 we can use the API and trade the                                 replicas whatever we want on demand and                                 that will be done automatically I'd like                                 to add a few things about how these                                 sharks and replicas are being managed                                 with both elasticsearch and solar                                 so with elasticsearch have the nice                                 thing that it automatically balances                                 charge the replicas across your cluster                                 and when you add nodes and remove them                                 things are balanced on the fly if they                                 are not as you want you can always move                                 shards around by using the cluster                                 reroute API if you want to change number                                 of replicas or other index settings you                                 can use the update indices settings API                                 and the other cool thing that it has                                 that solar hasn't at least right now you                                 can use all sorts of rules to do to                                 place your shards for example you can                                 have no tears                                 for example you can have fast nodes to                                 to hold your recent data and slow nodes                                 to hold your archived data and then you                                 can tag them and have elasticsearch                                 automatically place indices in those                                 servers for you or you can have rack                                 awareness and things like that on the                                 solar side you have the collections API                                 which you can use to create indices                                 create replicas and things like that                                 you can also use the collections API to                                 move shards around though this is a bit                                 more clumsy you have to create a new                                 replica on the node you want to move the                                 shard to and you have to then after it's                                 completed delete the old replica from                                 the old node but you have some nice                                 features here as well that are not                                 present in elasticsearch at least for                                 now you can split a shard on-the-fly                                 though this is an expensive operation                                 because it has to sort of copy data but                                 anyway you can do that and you can also                                 migrate a slice of your data for example                                 if you have multiple users data into the                                 same shard and you use a routing key for                                 each user you can take one users data                                 out migrate it to its own collection                                 again using the API okay so when it                                 comes to product we did a few                                 assumptions before the tests so we                                 assumed and when you have a product's                                 use case the data growth is quite steady                                 it grows over time but it's not                                 something like daily data or anything                                 like that so we usually use one or few                                 in this is not like in the logs use case                                 in which rather we'll talk about we also                                 have spikes in traffic's that we need to                                 be prepared for because during                                 promotions or holidays we can see a lot                                 of traffic more than we are usually used                                 to everyday in addition to that we need                                 to be prepared for large QPS because the                                 it in that use case you usually don't                                 index that the number of data that you                                 index when it comes to logs usually have                                 a static index with small number of                                 updates like the prices or the likes or                                 views or anything like that it's also                                 the data is also common because it has a                                 common structure and the thing to                                 remember is that it's mostly steady when                                 it comes to indexing so the caches and                                 all that stuff is not refreshed                                 very often the hardware we've used for                                 tests were too easy to see three large                                 instances each having to see two virtual                                 CPUs three and a half gigs of RAM few                                 Meg's of SSDs actually two of them two                                 of sixteen gigabytes SSDs in right:                                      we used Wikipedia data Spanish Wikipedia                                 with more or less than gigabytes per                                 node of of index the test requests look                                 like this we have the one common query                                 that were run using jmeter                                 hammering the instances and we have a                                 dictionary of common and uncommon terms                                 that we just used to simulate more or                                 less simulate what we would expect from                                 the users however remember those were                                 syntactic tests you can do them you can                                 see the here is a github account that                                 holds the data and all that stuff that                                 we used for tests but still this is                                 a synthetic test nowhere near production                                 we wanted to show a few things actually                                 so when it came to the product search at                                 five threats of Demeter we had a true                                 put like this about five hundred queries                                 per second for elastic search almost                                 three hundred twenty queries per solar                                 so we should say that in this case                                 elastic search is faster than solar but                                 we've noticed one thing yesterday                                 everyone makes mistakes and we also did                                 one actually when we were indexing data                                 we noticed that solar had about forty                                 percent more Wikipedia data than elastic                                 search we don't know why we used the                                 same XML but it was five o'clock                                 yesterday so we needed to do a few more                                 tests with it yeah so a scrub not to say                                 it harder so we did another test the                                 second try and this time we took simpler                                 data we took a video sir to index                                    million of documents for each and now                                 the funny thing happened when we start a                                 jmeter with                                                       something like this the blue one is the                                 solar throughput and orange one is                                 elasticsearch and now i would like to                                 ask you a question why that did that                                 happen                                 anyone knows                                 close close close but not close enough                                 so basic or mop run first and then do                                 yes basically you how many of you know                                 solar okay I suppose the rest of us                                 normal a stick search right okay so                                 basically in solar you have those top                                 level caches the ones that we are                                 usually used to not counting the per                                 segment one for nested and block and                                 blocked okay furnace the documents let's                                 let's end it we have three x types of                                 caches the filter catch the query result                                 cache and document cache in that case                                 because of the query is were not diverse                                 enough we just after the first time the                                 query was run it was put into the cache                                 and when the second query of similar                                 structure came with the same phrase the                                 same star trolls and sort soul didn't                                 have to go to the same index it just                                 took an entry from the query result                                 cache and returned and return the data                                 this is why you see the difference when                                 we turned on the caches the situation                                 was completely different you can now see                                 that we ran exactly the same test but                                 without solar caching enabled we throw                                 it away from the configuration from                                 Solar config completely and now you can                                 see that the queries per minute still                                 for elastic search we have the same the                                 machines were overloaded so it's not                                 something that you should expect your                                 elastic search to all only have                                     words per minute of throughput but for                                 that use case it was it was like this                                 and Solar hat                                                      Triple A queries per minute so you can                                 see that the caches here play a major                                 role and what we actually wanted to say                                 is that                                 they're about fast you need to plan for                                 the very fact road and prepare for a                                 hike ups but remember that configuration                                 matters a lot in both cases so if you go                                 with your use case and you take the                                 older all the stuff that is out of the                                 box you may actually not have the                                 optimal performance or for both when it                                 comes to elasticsearch and when it comes                                 to solar all right let's move on to the                                 logs use case which typically has                                 completely different pattern you have                                 lots of data coming in lots of data                                 being deleted every day because you have                                 a fixed retention usually you don't have                                 that many queries but those queries may                                 be more expensive if you run them one                                 more data there are no updates and                                 things like that once you write a log in                                 we have the same clusters for testing                                 only this time we indexed Apache logs                                 over and over again on the settings side                                 we tried to have equivalent                                 configurations again like with the                                 products we had the same sort of schemas                                 we had Dogg values in both use cases for                                 not analyzed fields we did soft commits                                 every                                                                   hard commits every                                                   because elasticsearch has that under                                 underscore all field to index everything                                 by default we did the same for for solar                                 sort of the same we had a catch-all                                 field where we copied everything so the                                 queries we had                                                           of filters from starting from a simple                                 filtering by a client IP to a more                                 expensive filter like filtering by well                                 card and then we had a bunch of                                 analytics queries from a date histogram                                 to the most expensive one which was a                                 nested a nested aggregation if you were                                 you're wondering how we did the nested                                 aggregation on Soler is because we use                                 the                                                         that feature for elasticsearch we use                                 the snapshot of the upcoming                                        about two weeks to a month ago so we did                                 three tests in this use case one the                                 first one was to write as much as we can                                 to a single index and then the second                                 one would be to write steadily at a                                 lower rate and do a more realistic                                 measure of query latency and then we                                 moved to an even more realistic                                 production setup where we had time-based                                 indices and we also have node layers                                 like the hot notes to hold the latest                                 index and to get all the indexing hits                                 and spikes and then cold notes to hold                                 the archive data and we ran the queries                                 on on this setup as well all right so                                 the first test we we just had a single                                 index or single collection and wrote as                                 much as we could both have very good                                 indexing throughput I think it's much                                 more than it would probably need because                                 the index will just fill up in terms of                                 disk space in in a few hours                                 solar is slightly better here indexing                                 but you can see both that our                                 performance is degrading as the index                                 grows and this happens because merges                                 are happening in the background so for                                 those who don't know when you're right                                 to loosen index                                 this happens in segments which are                                 immutable and searches go through all                                 these segments so in order to keep the                                 number of segments in check in                                 background you have to merge smaller                                 segments into bigger ones and this is IO                                 intensive and also quite CPU intensive                                 zuv this bites from the from the                                 indexing performance you can see that                                 the solar throughput falls down a bit                                 more and I think this is because the                                 index grows a bit more so a bit faster                                 yeah a bit                                 and during that run we also did the                                 queries and you can see here that there                                 are quite a few differences already                                 elasticsearch does a lot better when it                                 comes to filter in performance and this                                 is again caused by caching because we                                 run the same queries over and over again                                 elastic search was able to catch them in                                 per segment caches solar doesn't have                                 that it has the overall caches which are                                 invalidated with each right so we can                                 see with solar we have more or less the                                 same latency no matter how expensive the                                 query is however solar is seems to be                                 better when it comes to nesting at                                 nested aggregations and both are quite                                 spiky because you know the the servers                                 were were hammered quite a lot but you                                 can see solar is even more spike here                                 and when you see the zero                                 yeah the zero Laden sees that is when                                 queries actually timed out so now the                                 second test we indexed at                                                second which may not sound like a lot                                 but for this cluster it would fill up                                 the disk space of nodes in two or three                                 days and usually if you need more you                                 need more usually you need more                                 attention than that so you would                                 probably have a bigger cluster so we                                 have for this cluster                                                second is quite a lot in terms of being                                 realistic it's not a lot in terms of                                 load it barely feels it but anyway we                                 were interested in query latencies and                                 we can see more of the same elastic                                 search is faster at filtering because of                                 caching and solar is faster at the                                 nested aggregation the simple                                 aggregations take more or less the same                                 time                                 then we move to time-based indices and I                                 would like to take a minute to explain                                 why that is a good idea because if                                 you're writing to a smaller index only                                 to today's index for example you're                                 invalidating less caches you're doing                                 less merges so indexing will be faster                                 also when you're searching because data                                 is sliced up you can for example                                 normally when you're searching for logs                                 you're searching only on the recent data                                 and only occasionally you do like                                 analytics over the whole data set and                                 things like that and if you have time                                 based indices then you can just search                                 the chunk of indices that you're                                 interested in so these searches will be                                 faster and also when you when you delete                                 data you can delete entire indices                                 instead of deleting documents from                                 within an index and causing even more                                 merging and more load of course having                                 too small indices has an overhead                                 because each index has an overhead in                                 terms of memory when you're monitoring                                 elasticsearch and solar you have stats                                 that are per index or per collection and                                 you know the cluster state is bigger and                                 things like that so it's a trade-off                                 there but yeah in this case we just went                                 with maximum indexing throughput which                                 would which would spanned the whole test                                 in a few hours and our index slices were                                                                                                       per hour we had about                                                  in total okay so let me give you a few                                 words about how this works in practice                                 actually so we start - we'll start with                                 elastic search in elastic search we have                                 the notion of properties we can set                                 properties for a node and we will use                                 that in called the cold node setup so                                 for the code nodes we just said for                                 example node dot tag property - just                                 explicit in the hot and the same we do                                 for cold nodes so we have a from two                                 types of nodes once one set with not dr.                                 called and the other called                                 then we use the index templates and                                 short allocation awareness to actually                                 create new indices the good thing about                                 the elasticsearch here is that whenever                                 your data comes in and the index is not                                 created by default elasticsearch would                                 create a new index so if we would for                                 example start logging start sending logs                                 right now to elasticsearch that was just                                 started it would create the proper index                                 use the index templates that we put                                 there which has the short allocation                                 awareness attributes so we say that for                                 new indices that are newly created they                                 should be they should have that node dot                                 tag equals to hot that means that                                 elasticsearch will create them on the                                 hot nodes when that happens and that                                 happens automatically we don't have to                                 care about it at all then when there is                                 a time that we want that hot index to be                                 actually moved to a cold storage that                                 happens after one day two days depending                                 on what we call the hot storage because                                 we are not indexing history called the                                 historical data usually we can force                                 merge the index so we lower down the                                 number of segments to a single segment                                 to speed up the queries because the more                                 segments we have in Lucene index the                                 slower the queries will be at least                                 slightly so we have a crunch up on this                                 elastic search side that will optimize                                 the index to do the first merge with max                                 segments equal to                                                        allocation attribute to point to no dot                                 tag code which means that the elastic                                 search will just move automatically the                                 index from the hot nodes to the cold one                                 and again that happened automatically we                                 don't have to do anything about that                                 when it comes to solar things are a bit                                 more complicated because we have to do                                 everything with a cron job we have the                                 API but still we need to do the                                 automation ourselves so first of all                                 solar won't create collections in                                 advance or when the data comes in if you                                 pull the data to a collection that is                                 not                                 and he's not grated soul we'll just say                                 go away with the late I don't like it so                                 we need to create in the collections in                                 advance to point out to a given nodes we                                 again use property in a in a command                                 that creates the collection called the                                 create node set and we said and we                                 specify the names of the notes for                                 example so node                                                       this is where the core hot the                                 collection will be deployed then we have                                 the again the situation that after a few                                 days we want to move that to that                                 collection to cold nodes and first we of                                 course optimize again with the API call                                 and here we can't update the properties                                 we just need to create new replicas for                                 that particular collection on the cold                                 node allow solar to replicate the data                                 using the replication functionality and                                 after the application has been done we                                 can remove the collection from hot nodes                                 and this is what the process for all for                                 both of them so as we can see in in that                                 case elasticsearch is allows us to                                 easier handle the process of moving                                 between hot and cold notes so to give                                 you a notion of how the indexing was                                 working during that time we can see that                                 the top top two graphs are the before                                 before implementing the procedure of hot                                 and cold note after we've implemented it                                 we can see that force or we have almost                                 constant wearing                                 indexing throughput but spiky why spiky                                 we are not sure yet whether there is a                                 bargain sore or something like that but                                 when we were running that with a cron                                 job if we were switching the collections                                 from hope to cold notes when the                                 indexing happened we get indexed                                 indexing stalled so we need to figure                                 out yet why that happened but if we                                 would actually stop the indexing for a                                 second everything would be ok and this                                 is actually the spikes are                                 what because we stopped the indexing I                                 suppose that if when we found the case                                 cause it will be mostly static and in                                 elasticsearch again we can see the                                 increasing throughput because we are                                 indexing to smaller indices the segment                                 merging is less stressful for the                                 operating system and for us in itself                                 and we can see the increase in                                 throughput from                                                     about                                                                    pretty nice when it comes to indexing                                 logs and for the queries which we can                                 see more of what we've seen before in                                 terms of how they scale but with a hot                                 and cold setup because the cold nodes                                 hold data that is now optimized and                                 they're not bothered by any indexing or                                 cache invalidation now they can hold up                                 to three or four times more data than                                 than the nodes that we had before in in                                 both cases and the elasticsearch again                                 is very good at doing filtering and                                 solar now in this in this case was about                                 twice as fast when it comes to simple                                 aggregations or facets and about ten                                 times as fast when it comes to the                                 nested irrigation yeah the filters on                                 the elastic search side are are about                                 two times two times faster yeah still                                 okay so to wrap this up with with the                                 logs use case using time based indices                                 and hot and cold nodes policy helps in                                 both use cases and elastic search tends                                 to be faster at filtering and solar                                 tends to be faster at faceting it is                                 worth noting here that elastic search                                 has more capable with solar                                              nested facet and things like that                                 with elastic search you can do more                                 kinds of aggregations so we                                 in this case we tried to compare common                                 functionality just to look at the                                 performance so just as a summary we                                 speed up things slightly so just as a                                 summary for the things we did for the                                 talk the tests we've noticed that in                                 most cases the difference in                                 configuration you views matters more                                 than it comes then when it comes to the                                 product you use because if you configure                                 soror elasticsearch in a bad way it will                                 perform bad so this is this is what you                                 what you will get for example you've                                 seen the querying quids or having the                                 curl result cache by default set of                                     and you've seen the throughput increase                                 in a in a large in a large way however                                 if you would remove the caches                                 completely then we have lower lower                                 performance way lower performance but in                                 a real-world scenario in such case this                                 cash would be invalidated constantly                                 because you would probably have updates                                 to the data because this is a top-level                                 cache so whatever it segments is                                 refreshed actually that cache needs to                                 be invalidated and because of that that                                 performance won't be as good as as                                 you've seen we have to be honest we                                 expect two slightly different results                                 from the tests we did we were amazed                                 about some some results we repeated the                                 test to just make sure that everything                                 is ok but we have the constant results                                 like we've shown so if you would like to                                 repeat that we've shown the github                                 account please do that and let us know                                 if that if you get the similar results                                 or at least a similar pattern results                                 because on different machines you'll get                                 the different results that's that's                                 normal and finally we would really                                 encourage you to do your own test don't                                 believe even us in the slides we have                                 don't believe everything you see just do                                 your own your own tests with your own                                 data with your own queries and then you                                 can say that for your use case this is                                 how solar elasticsearch works                                 take your numbers if you like to fake                                 numbers like we do we are hiring this is                                 a slide that I saw three years ago from                                 a fall stock and this is how I got here                                 so I think this is it colas might be and                                 if you have any questions we have time                                 yeah
YouTube URL: https://www.youtube.com/watch?v=01mXpZ0F-_o


