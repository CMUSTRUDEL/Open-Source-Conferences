Title: Berlin Buzzwords 2015: Ted Dunning -What and Why and How: Apache Drill 1.0 #bbuzz
Publication date: 2015-06-05
Playlist: Berlin Buzzwords 2015 #bbuzz
Description: 
	The 1.0 release of Apache Drill does SQL on Hadoop, but with some big differences. 
The biggest difference is that Drill changes SQL from a strongly typed language into a late binding language without losing performance. This allows Drill to process complex structured data in addition to relational data. By dynamically generating code that matches the data types and structures observed in the data, Drill can be both agile as well as very fast. Drill can analyze complex data directly with no ETL steps.

Drill also introduces a view-based security model that uses file-system permissions to control access to data at an extremely fine-grained level that makes secure access easy to control.

These changes have huge practical impact when it comes to writing real applications.
I will give several practical examples of how Drill makes it easier to analyze data. This will include examples of how to use Drill to analyze real complex data.

Read more:
https://2015.berlinbuzzwords.de/session/what-and-why-and-how-apache-drill-10

About Ted Dunning:
https://2015.berlinbuzzwords.de/users/ted-dunning

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              so we have a lot of things to talk about                               here this is going to be about                               drill which just recently had a one                               point O release it's a big deal it's an                               interesting project I'm going to be                               talking about that the original speaker                               was Michael housing bloss he says that                               he's very sorry couldn't make it but he                               can't make it he had to be at a big                                product launch so I want to talk about                                drill and drill is a surprising project                                its sequel on Hadoop that sort of thing                                actually its sequel on all kinds of                                things and that's what Andrew Bruce said                                about it recently in GigaOM that it                                really is not just sequel on a dupe but                                its sequel on just about everything you                                you could imagine so we're going to talk                                about it what is it what makes it                                special and a little bit on how this is                                done now unfortunately I have a lot of                                slides so I'm going to have to go fast                                the slides will be available though for                                reference and you can go in it may be                                difficult to read some of the displays                                and things like that but you can go back                                in and take a look at it later but                                you're going to also try these things                                very easily vampire has a special by                                registration sort of button you can push                                it will spin up a five node cluster that                                will last for six hours and you can play                                with it drill will be pre-installed data                                will be pre-installed so you can just                                try it you could also just download it                                practically during the talk here it                                takes about five minutes to install you                                ontar it and it just runs now the first                                thing that drill is is its sequel you                                know maybe not in your family but in my                                family of course we had these things at                                home and in the kitchen and that sort of                                thing so this is sequel like you've                                always expected and always known it has                                the standard sort of syntax no no                                strange exceptions it has all of the                                standard types all of the decimal types                                the VAR binary the VAR char the                                different forms of doubles and so on sub                                queries correlated sub-queries pruning                                an enormous range of complex sequel                                constructs are there                                it therefore supports standard bi tools                                tableau anything that uses odbc excel                                all work with it and all of the standard                                hive UDF censored es also work with it                                works with a hive metadata catalog so                                it's also Punk SQL because it does a lot                                of different things as well starts with                                a standard but then extends it in some                                very very interesting ways notably it                                has more modern syntax available so that                                we can access very directly recursive                                objects so if it's an array you use                                square brackets to index into it if it's                                objects you do standard JavaScript                                styled dot notation to navigate into the                                objects that are stored in the data and                                those objects are first-class in drill                                so you really can do interesting things                                that way you also have file systems as                                organizing principles these were largely                                you know sequel is so old this sounds                                like a joke but it's not sequel is so                                old that it was invented effectively                                before modern file systems were invented                                so the idea of wild cards and path names                                are after sequel therefore sequel has                                nothing in it that really recognizes it                                drill extends that so that directories                                of tables can be viewed as a single                                table by itself and you have elements of                                the columns that you get back which                                describe the path that drill took to get                                to the data we also have more                                interesting data types we have one                                that's called any which means any type                                could be there we have maps we have                                lists and so on the any type is the key                                to another capability drill has which is                                late typing which is almost like dynamic                                typing sequel it self-regulation                                ordinary sequel is a strongly typed                                language and an early binding typed                                language it has to have all of the types                                of everything to                                aired in a schema before you can even                                parse sequel drill has extended that so                                you can parse with partial typing                                 information ultimately you may just say                                 they seem to claim that that's a table                                 therefore I assume it has records of                                 type any inside it now later I may find                                 out more about the typing and I may be                                 able to compile and optimize to a more                                 detailed level and then the type will be                                 resolved down to a primitive type but                                 that will be late binding of typing and                                 that gives some very very flexible                                 capabilities like the ability to                                 interrogate data where then column names                                 are not even known until you read the                                 data so that's exciting that that then                                 requires a number of other operators to                                 make that true i'm going to show you                                 examples of that so if you have sensor                                 analytics or if you have data that's                                 changing its structure and its type over                                 time you don't have to use alter table                                 to go in and fix that you can just query                                 the data as it is so let's talk a little                                 bit about how it does it drill is                                 flexible which of course leads to a                                 certain agility and ability to jump high                                 buildings it's flexible and it's simple                                 to install drill you need one process to                                 run it could be an interactive process                                 with an embedded so-called drill bit or                                 you could have a drill bit that runs                                 it's a long life process on your machine                                 like a database server even though it's                                 not really a database or you can have                                 clusters of them running we've tested up                                 to hundreds of them and the design is                                 designed carefully to avoid in squared                                 sort of non-scale abilities it scales up                                 quite linearly and and produces very                                 very high performance so very very easy                                 to build a system around drill you can                                 do with a clustered system with map are                                 with ordinary hadoop distributions or                                 without on a single machine or on many                                 machines however you want to get to the                                 data however you want to execute it's                                 very easy to do now part of the                                 flexibility is not just                                 in deployment but in the data typing                                 itself you can imagine that data has                                 multiple axes you can talk about how                                 flexible the data is in terms of of                                 whether or not it has a schema that's                                 predefined or not at the schema end of                                 the world is relational data where you                                 have to have a schema in order to even                                 store the data at all on the completely                                 unskilled of the world you have JSON or                                 beasts on or things like this where the                                 data itself is the schema you can also                                 have data which has complex data types                                 or data which has only simple data types                                 in very regular fields and what drill                                 does is it takes us from the inflexible                                 non complex world of relational data                                 where the cod definitions of what                                 relational data are take us directly to                                 that definition to the completely                                 flexible end of the world the data layer                                 the data model inside drill is                                 essentially JSON the data representation                                 is not the data model is it's that                                 flexible now obviously it can handle                                 schema full data but it also hands                                 handles schema-less data even in the                                 same query even in the same table at the                                 same time so drill doesn't require                                 schema but it can use it the query can                                 be planned on files or tables with or                                 without early information and it can                                 bind late in the planning process and it                                 can generate code that's efficient that                                 way the guiding principle here is the                                 drill will operate at very high speed                                 with no metadata centrally stored and no                                 metadata until we read the data but it                                 will also use that data as soon as it                                 can get it so it will operate even                                 faster when you have more regular data                                 let's walk through some examples and                                 these examples are                                 are taken largely from the Yelp standard                                 data set which is a dump of the Yelp                                 database that's intended for use as                                 research or educational purposes we                                 can't redistribute it but we can show                                 you examples from that it's nice because                                 it has a couple of properties that make                                 it very difficult to work with from the                                 standpoint of a standard database for                                 one thing all the data is stored in JSON                                 in files well that that's incredibly                                 inconvenient from a database point of                                 view because there is no data model it's                                 not defined ahead of time you have to                                 kind of look at the data and understand                                 it and here's some of the data there                                 this is a business description of                                 something that is on Las Vegas Boulevard                                 in Las Vegas it sells French Steakhouse                                 kind of food whatever that could mean                                 it's good for dessert late nights and so                                 on that's the example of it and you can                                 find data in it like here are the votes                                 that a particular business has had                                 that's a different file in there and you                                 can understand it because the data is                                 well labeled and it's easy to see when                                 the schema is integrated in there you                                 can go from nothing to getting real                                 query results in about two minutes with                                 drill so installing it is nothing but                                 untiring a tarball so running it is                                 running a standard utility called sequel                                 line which connects via JDBC to an                                 embedded drill bit and from there we can                                 directly query data in the file system                                 this is literally all you have to do to                                 get started with drill you can do more                                 things if you want a large parallel                                 instance but if you just want to try it                                 this is how much it takes and notice                                 here what we would normally call a table                                 name says it's on the distributed file                                 system in a workspace directory of                                 predefined directory called Yelp and                                 then there's this file called business                                 JSON                                 now the structure of that has state and                                 city we're going to order it by the                                 number of businesses and we're going to                                 count the number of businesses there the                                 standard sort of reporting query you'd                                 expect except that these were from a                                 JSON data file not from a conventional                                 relational table we get a conventional                                 looking relational result even though                                 we're not querying relational data                                 here's another example this is more                                 complex notice that the references here                                 have multiple dots in them so we can say                                 well it's come down here we're going to                                 query from said this file business JSON                                 called be and we're going to say B dot                                 hours dot Friday two levels of                                 indirection because that's nested data                                 is less than                                                             is greater than                                                         what's open right now late night in Las                                 Vegas and we can also say and there's a                                 list that contains Mediterranean a list                                 is a data structure but it's not a data                                 structure that you normally query inside                                 a sequel query so there we get olives                                 and it's open right now as Mediterranean                                 and restaurant there's another one which                                 is the marrakech moroccan restaurant                                 both of them open at ten o'clock at                                 night in las vegas we can do all of the                                 sort of things like we can say the                                 business idea is in the results of                                 another query we're going to group that                                 by business ID having a sum of votes                                 greater than a certain amount does this                                 sort of correlated sub-queries works                                 just fine all of the sort of sequel                                 structures still apply here we get the                                 ability to build a view now views I'm                                 going to talk about more in a little bit                                 but views are essentially a file that                                 has a snippet of sequel in it so they                                 are not materialized views their virtual                                 views but they can be chained one view                                 can refer to another and another                                 the exciting thing about views is not                                 the fact that you can chain them I could                                 have just written the query but the fact                                 that permissions can be delegated                                 according to that chain one person might                                 have permission on a table they could                                 create a view which access to the table                                 and they could open up the permissions                                 on their view somebody else could have                                 permissions to read the view but not the                                 table well they could recreate a view                                 the references to view this allows us to                                 do things like on-the-fly decryption it                                 lets us do things like line by line                                 masking of data if we have a column                                 which says this data has been withdrawn                                 from public view we can do filtering                                 that way we can also build materialized                                 views or new tables this sort of thing                                 we can work with repeated values that's                                 a raise of things and this begins to be                                 pretty exciting we can do things like                                 flattened so before the data has these                                 lists of categories and when we flatten                                 it these become rows which let us do                                 queries on contents of arrays as if they                                 were already data that had been                                 flattened another good use of this is if                                 you want to compact many many many                                 samples or many many data points into a                                 single row in a database to make                                 insertion and retrieval very efficient                                 or even compact them into a binary                                 representation flattened then can turn                                 that back into what looks like                                 relational data if you have a time                                 series database you might insert a                                 thousand samples and time Sam timestamps                                 into the data but then when you retrieve                                 it you've only retrieved one view they                                 use flatten you get a thousand samples                                 but the data is not changed is not moved                                 is not transformed by flattened it's                                 merely how drill thinks about the data                                 the data is still stored in a flat                                 primitive column based representation                                 and so this goes at very very high                                 speeds much faster sometimes a thousand                                 times faster than the underlying                                 database would allow you to operate on                                 it if you were operating on a row by row                                 basis so flattened is a new capability                                 that's very exciting it's essentially a                                 lateral view now we can do recursive                                 things against flattened treating that                                 as of you or as a as a sub query we can                                 even do some interesting things one of                                 the one of the I don't want to say                                 beautiful I don't want to say bad i                                 don't want to say perverse but it's all                                 of these things one of the things that                                 happens in no sequel databases is people                                 put data into the column names so they                                 because you could have sparsely                                 populated columns you could do something                                 like if you have a web page you could                                 say the domains that that web pages                                 refers to are have the columns with the                                 name of the web domain that they refer                                 to and then the anchor text could                                 actually be the value in there so the                                 column name itself is from an open set                                 something we don't have predefined and                                 it contains useful information itself                                 the domain name here we have an array                                 here's a magnified view and this is an                                 object and the keys are time ranges so                                                                                                       are values associated with those ranges                                 this is a very peculiar thing where the                                 column has a name which is the                                 combination of two sorts of data                                 elements doing a query against that we                                 can use something called kv gen that                                 converts this one object into a list of                                 key value pairs and then we can flatten                                 that kv jen and we could do a query so                                 here's the result of the kv gen so                                 before this was the name of the column                                 and so now we have one object with a key                                 at a value and so now we can do queries                                 against that one value like here                                 flattened kv gin and we get rose with                                 those things and that means that we can                                 then do queries flatten that and now                                 check in key is                                                          how many restaurants have had a check-in                                 from eleven o'clock to midnight a very                                 very flexible sort of query but nothing                                 like what you could do in a normal                                 traditional sequel database but                                 something you have to do if you want to                                 manipulate very very flexible no sequel                                 data these data are the things that they                                 come to you as they come to you it's not                                 something you can change it's not                                 something that you can manipulate I                                 talked to one customer they have                                 something like                                                         items that might be for sale they come                                 from a hundred and twenty different                                 business units and each of the business                                 users changes their schema on average                                 twice a year that means almost every day                                 the schema of the collective data                                 changes and the IT department is                                 supposed to deal with it how are they                                 going to deal with it they can't control                                 it they can't even document it they have                                 to query it even so well techniques like                                 this would allow them to succeed drill                                 would allow them to live in the very                                 dynamic out of control world that they                                 have to live in and traditional sequel                                 systems which assume that you have a                                 curatorial rule and control over the                                 shape of your data would necessarily                                 fail drill also looks at this                                 decentralization problem and tries to                                 apply a decentralized but still secure                                 security model the idea is that you will                                 have perimeter security tools like                                 tableau or you might have sentry or                                 something like that                                 those are controlling which kinds of                                 queries you can do they're trying to say                                 you can't select against this sort of                                 thing but they are inherently holy they                                 have holes in them because it's very                                 difficult to parse and understand what                                 queries are doing and whether or not                                 it's dangerous things but then drill                                 also has distributed security at the                                 file system level and the way that that                                 works is based on these views and the                                 ways that views allow you to delegate                                 your read permission in controlled ways                                 to file system cysts assets you can have                                 a business analyst who has certain                                 permissions and you can see a certain                                 view of the data you can have a data                                 scientist who has other permissions and                                 they might be able to see the entire                                 content of a credit card number but                                 these are not physical copies of the                                 data they are virtual what happens is                                 the original data here at the bottom has                                 rights for the DBA the DBA can build a                                 view and on top of that Cindy might                                 build a further view that could be                                 accessed via queries that Frank rights                                 by controlling who owns which things and                                 who is allowed to read those things and                                 what the owner is allowed to query we                                 can provide controlled security that is                                 rooted in the file system semantics                                 rather than in our attempt to write a                                 pattern of which queries Arctic will be                                 allowed to succeed or not file system                                 permissions are of course much simpler                                 for people to understand and manipulate                                 especially if they come from a security                                 or a computer programmer sort of                                 environment as opposed to a database                                 administrator point of view so if we                                 think back now this has been a lot of                                 slides that are kind of slid by but we                                 have logical logical views that provide                                 security it's highly granular because                                 you have all of the power of sequel                                 to define what person can see or not see                                 it's completely decentralized there's no                                 single metadata resource that has to be                                 up that has to be performant that you                                 have to reload the cache of and the                                 security allows self-service while still                                 maintaining tight governance now as I                                 said drills goal is to go fast with                                 little information and do even better                                 with lots of information so here's a                                 little bit about how it actually works                                 we get fully pipelined execution and so                                 what we've got here is a execution plan                                 and you can actually see here this this                                 dotted line expresses where parallelism                                 is marked in this logical plan the                                 initial step of drill is to convert                                 sequel into so-called logical plan which                                 hellz logical operators here starts at                                 the bottom with the scan then there's a                                 producer consumer and a union exchange                                 there is no parallel ISM expressed here                                 and there's only as many types as we                                 understand no and no exists at this                                 point but what then happens is that's                                 translated into a physical plan and then                                 an execution plan and at this point we                                 begin to see fragments that are executed                                 across the cluster and part of it that's                                 not executed across the cluster and we                                 get data flow from the parallel bits                                 into the non-parallel bits this process                                 of converting from a logical plan to a                                 physical plan and then to actual                                 physical code occurs using the calcite                                 query optimizer but it also uses special                                 rules that are part of drill itself                                 which understand the parallel execution                                 environment drill then can generate code                                 on the fly data streams in from the                                 bottom of that flow data streams in in                                 what are called row batches                                 when they're stored in a columnar format                                 in memory each row batch comes with                                 what's called the empirical schema this                                 is the schema of data types as observed                                 in this data that we have this batch for                                 if that schema is the same as the                                 previous schema drill just execute the                                 query but if it's different drill will                                 Rio mais in this fragment regenerate the                                 code and compile it so that we get full                                 operational speed and you can imagine                                 this has applicability if we have old                                 data that has column one and at some                                 point we decided to convert to column                                 two with a different data type now new                                 data will have column two so most of the                                 data will have either column one or                                 column two and will probably have a view                                 which will say if column one exists then                                 we we give you this this synthesized                                 view if column two exists we just give                                 you that value on the first half of the                                 data the optimizer will throw away the                                 case statement turn it into straight                                 line code and always just give us column                                 one because the empirical schema says                                 column two is always null so we could                                 obviously do that in the second half of                                 the data the optimizer will throw away                                 the fork of the case that talks about                                 column one and again we get full speed                                 we have no conditionals in the data                                 processing itself and furthermore all of                                 the object creations in all of the                                 object references compile away as well                                 so what's left is direct pointer access                                 to an array of primitive objects even                                 though we have a dynamic language which                                 was not type specific when we originally                                 wrote the code it's only when it's                                 presented with a block of data that it                                 finally compiles down to primitive code                                 and at that point all the types are                                 known all the types are stored in                                 contiguous arrays so that we get full                                 speed of the machine running on a very                                 tight loop or even in a vectorized loop                                 so that we get                                 these incredibly high speeds even in a                                 dynamic situation this optimization                                 extends into the RPC layer which is                                 optimized for that same vectorized sort                                 of transfer we have optimized readers                                 for formats like parquet where drill can                                 do push downs into the data reader and                                 it's a fully pipelined reader that runs                                 much faster than the normal parquet                                 readers that are part of the park a                                 distribution itself drill has specific                                 readers for its own needs that increase                                 and accelerate that the value vector is                                 the way that drill stores data                                 internally this is an in-memory columnar                                 representation that allows full                                 vectorization of operations because it's                                 a pure array of primitive types that are                                 packed into congenial units that allows                                 random access to any materialized view                                 without any copying it allows us to                                 fully memory shred all operations so                                 that we get very very high access speeds                                 it also allows us to do and interesting                                 things all operators in drill are                                 expressed as udfs the same that you                                 might write what happens is you compile                                 that the annotations that you put on                                 your code are used to lift the source                                 code out and put it into a generated set                                 of code so that your code sits directly                                 in the data path and it's encapsulated                                 safely because you can only see the                                 references that you have declared but                                 the optimizer can now see across all of                                 the code that's generated in the data                                 path and so the objects that you think                                 you're accessing get optimized away and                                 turn into pure data pointer accesses                                 this is a unique property of drill and                                 it allows drill to run at full memory                                 bandwidth speeds on things that look                                 like dynamic code this runtime                                 compilation is part of the planning                                 process of drill which delays decisions                                 as late as necessary to get that dynamic                                 typing information fully resolved this                                 is another unusual characteristic of                                 this the ability to preserve uncertainty                                 as long as necessary so we never                                 generate conditional code where it's                                 conditional on type and it wouldn't                                 necessarily be true so the way that this                                 actually happens is your udfs get                                 transformed into synthetic code where we                                 in line your code into the main query                                 code the code model is a generated                                 template into which your code is                                 inserted based on the query itself                                 Janine o is used to compile this on the                                 fly Janine o is an on-the-fly in-memory                                 compiler for Java and we also have byte                                 code templates we do bytecode                                 engineering so that we can directly                                 insert the compiled code without                                 recompiling the entire template that                                 that accelerates this on the fly                                 compiling and we generate custom byte                                 codes as necessary to adjust the data                                 accesses that then is optimized by the                                 after loading by the standard Java chip                                 the optimizations that we do in these                                 things are done both at query time at                                 dynamic code generation time and then at                                 execution time we have special machine                                 code compilation optimizations that are                                 injected into the JIT so that we get                                 very very high performance this removes                                 for instance all bounds checking from                                 safe accesses even though your code                                 didn't even know was referencing vectors                                 it's still able to remove those bounds                                 checks and drive into machine native                                 pointers the vectorization does the                                 equivalent of running more than one row                                 of operations at the same time by the                                 use of vectorized instructions again you                                 don't have to a note                                 that in your UDF at all the simdi is                                 brought in as possible and accelerate                                 your code you also get advanced                                 telemetry out of this because the entire                                 process is instrumented and you get                                 profiles that are written in JSON and of                                 course you can use drill to analyze                                 those profiles and here's the sort of                                 thing you get out of that first of all                                 you get a full dag which this is in the                                 web interface for drill which expresses                                 all of the parallel chunks you get a                                 different color for each what's each                                 called a fragment a fragment is a bit of                                 code that's generated as one unit and                                 executed in one threat of control here a                                 query has been colored up and then on                                 the right what you get is what's called                                 a swim lane visualization so this light                                 blue section here you can see is                                 executed in multiple threads roughly a                                 dozen threads of control across the                                 cluster and it precedes for the most                                 part but not entirely the light red                                 which is the hash join here we have in                                 parallel dark red which is a scan and                                 project and filter section which drives                                 ultimately into the dark green the dark                                 green is able to start execution as data                                 becomes available in from the light red                                 the dark red but it's can only complete                                 after all of its data is presented to it                                 the last step of projection and hash                                 aggregation the result of a group by                                 occurs here in the light green and the                                 final output step which doesn't show                                 here is right there so you can see in                                 very very fine detail what's happening                                 when you run a drill query you can tell                                 exactly which parts are parallelizing                                 well and which parts are not the                                 in-memory representation is designed to                                 be efficient but also efficiently                                 accessible all vectors are stored as a                                 raise of uniform primitives and                                 therefore can be accessed in a fully                                 vectorized way                                 we get column where data structures you                                 don't see them when you write a UDF but                                 that's what's actually being executed                                 against and you get some support support                                 is on is ongoing implementation is                                 ongoing for fully compressed in memory                                 column to representation we've tested                                 this is a little bit of an old slide                                 that talks about testing up to                                           the ga is out and we've tested in our                                 labs up to roughly a thousand nodes and                                 again no scaling issues have been found                                 at that scale so this in a very                                 accelerated                                                             what drill is useful for and how it                                 works it's probably a bit much unless                                 I'd like to hire you it's probably a bit                                 much to digest in that                                                  think you can see already just little                                 sparks hopefully sparks that will lead                                 to fires sparks thats a data that you                                 have now could be analyzed conveniently                                 here either on the small scale kind of                                 as a grandpa knock replacement or on the                                 large scale as a data warehouse                                 alternative data which as it is rather                                 than after an ETL process can be                                 processed I was talking about Neil's                                 just the other night how we'd like to be                                 able to pull it his luck parser in that                                 actually drives real logs directly into                                 their there's no reason necessarily to                                 do ETL out of these data as long as you                                 have a scanner and the scanner can do                                 arbitrary levels of push down all of the                                 data sources and drill can expose                                 optimizer rules so that you could pull                                 down anything you're like I just heard a                                 guy in Sweden has put together a JDBC                                 sub-query unit so that it accepts                                 arbitrary pushed down into the JDBC data                                 source so drill is flexible enough to do                                 that so you have a data source which is                                 in fact an entire different query engine                                 that can look at certain tables and                                 under                                 stand how to process those somebody I                                 don't even know where they are produced                                 a query engine that does the same                                 sort of thing took them about two weeks                                 it's not hard there's some foreign                                 concepts here but it's not hard to get                                 started doing some very very cool things                                 in drill and I'd very much like to talk                                 to anybody who'd like to do that my name                                 is Ted dawning I work for map are we're                                 doing a lot with drill our customers are                                 beginning to put it into production but                                 I'd love to hear how you guys might use                                 it any questions that you might have                                 about it
YouTube URL: https://www.youtube.com/watch?v=llEcwGCLHWA


