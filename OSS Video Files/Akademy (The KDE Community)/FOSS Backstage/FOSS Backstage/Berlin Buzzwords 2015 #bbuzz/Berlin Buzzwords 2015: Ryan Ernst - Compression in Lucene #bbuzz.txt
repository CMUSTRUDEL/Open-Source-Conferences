Title: Berlin Buzzwords 2015: Ryan Ernst - Compression in Lucene #bbuzz
Publication date: 2015-06-03
Playlist: Berlin Buzzwords 2015 #bbuzz
Description: 
	Modern search engines can store billions of records containing both text and structured data, but as the amount of data being searched grows, so do the requirements for disk space and memory. Various compression techniques are used to decrease the necessary storage, but still allow fast access for search.  

While Lucene has always used compression for its inverted index, compression techniques have improved and been generalized to other parts of the index, like the built-in document and column-oriented data stores. In this presentation, Ryan Ernst will give an introduction to how compression is used in Lucene, including recent improvements for Lucene 5.0.

Read more:
https://2015.berlinbuzzwords.de/session/compression-lucene

About Ryan Ernst:
https://2015.berlinbuzzwords.de/users/ryan-ernst

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              I                               great hi thanks my name is Ryan I work                               for last search as he said I'm also an                               apache Lucene committer and so I'm going                               to be talking about a lot of the                               different ways that compression                               techniques are used within leucine to                               make indexes smaller so first of all to                               talk about compression we need to kind                                of understand what's in an index so we                                start out with documents right so you're                                going to take a bunch of documents here                                we have a document that is Berlin                                buzzwords and this is the year                                         we have like a unique identifier for be                                buzzed                                                              that so first thing we're going to do is                                we're going to generate this posting                                lists probably familiar with these                                essentially it's inverting the terms so                                we're going to for each term that exists                                in all the documents we're going to keep                                a list of which documents contain that                                term right and then we're also going to                                have something doc values this is                                something you've a talked about earlier                                and this is just a the inverted look up                                there so it's from document I want to                                look up for field what value did this                                originally have when I indexed it and                                then we also have this stored field                                sword fields which is just like a string                                that I want to keep with my document                                like the original value of the field for                                instance so I can get it back out later                                this isn't everything that's in like                                Lucien index but it's kind of general                                gist so understand compression we first                                what we're going to do is go through an                                example query so when we're running a                                query we're going to have to decompress                                some things and so let's go through this                                query part right so we pass a query                                through we're going to get documents                                back what does that look like so first                                of all we need to find the terms right                                so our query is going to be brilliant                                buzzwords and so we need to find what                                documents contain the word Berlin and                                what documents contain learn buzzwords                                so first we gotta remember back to an                                FST I'm not going to explain how an FST                                works here but in general it is                                a structured that is going to allow very                                quickly iterating through and getting a                                match and the T part transforms it's                                going to spit something back out and so                                that's something we're going to use to                                find some information in the index so                                the way that looks is we have this thing                                called a prefix FST it's called a block                                terms reader in lucene the idea is that                                we have an FST with prefixes of our                                terms and the output when we get to the                                end of when we find a prefix that                                matched in there is going to be the                                block within this terms blocks where                                that term actually exists or where it                                should exist and so we can traverse the                                FST and the output is going to be this                                offset into the file and now we get to                                to these suffixes right that are going                                to be tacked on to the prefix that we                                matched so now if we zoom in on this one                                block so we looked for                                                  we went through we got to the block and                                now we have five there right so we                                matched a                                                            this postings so this is yet again                                another offset that we're going to store                                in there and this is the offset into the                                postings file so for where we're going                                to find that list of document IDs so how                                are we going to encode this posting this                                number here so for many years since the                                                                                                        vient the bite warrant it has a lot of                                different names if you look in I our                                textbooks but the general idea is that                                we are going to take an integer and                                we're going to find the significant bits                                right so extra zeros that are up at the                                top we don't care about those so we want                                to find out the minimum number of bits                                that an integer is going to take and                                then we're going to split it up into                                groups of seven bits and the way we're                                going to encode this is                                taking the first seven bits and putting                                 it into a bite and then adding a one                                 there and that means there are more                                 bites to read and so then we can keep                                 doing this until we've run out of bits                                 that we need to encode and then when                                 we're decoding we can read through until                                 there is no longer a one in that most                                 significant bit and then we can use bit                                 masks and shifting to rejoin all the                                 bits together and we have our original                                 integer so let's look at some examples                                 of that so we start out with a very                                 simple number right one notice that                                 there's a lot of zeros these gray zeros                                 or things that don't matter right so the                                 only meaningful bits when we have this                                 one bite is the zero at the top because                                 that means this is the last bite so it's                                 just a one-bite integer and then we have                                 the lower one bit there then we go to a                                 value                                                                  take up                                                                 a single bite if we wanted right but in                                 this case because we only have seven                                 bits per byte we have to spill it over                                 to another bite so again we have a bunch                                 of zeros but if you a bunch of wasted                                 zeros but if you look at the total                                 number of of relevant bits across our                                    that we have there it's it's still a                                 pretty good compression ratio and then                                 then we get to yet an art another larger                                 number notice here that in the third                                 bite it looks exactly the same as that                                 first bite that we had before so context                                 really matters right we need to start at                                 the beginning of where the integer is                                 because there's no way for us to know                                 like this isn't the beginning of an                                 integer right so things like utf-                                       have special special values that they                                 have in these significant bits that tell                                 them to tell a decoder where it is so                                 you can actually jump back to the                                 beginning of a utf-                                                    but with v bite you camped and then                                 finally this number all one's a central                                 right we                                 we have                                                             biggest number that we can fit in                                   bytes so if we had a larger value in                                 this stash going to take more than if we                                 hadn't covered in int that's okay                                 because we don't have very very large                                 numbers like that very often so now that                                 we've found this offset into the                                 postings list we're going to go try to                                 decode our postings so how are we going                                 to do that so if we have our original                                 document IDs that we had encoded in the                                 postings list usually these are going to                                 first of all there going to be                                 sequential Rex we're going to do them in                                 order so that we can do like fast                                 intersections so we can keep track as                                 we're intersecting and know that we                                 don't have to go back and look for a                                 different number but most of the time or                                 all the time we're going to want to                                 store deltas because usually if you have                                 a term that is very common then the                                 difference between the two numbers is                                 going to be much smaller than the actual                                 number right so if I have document ten                                 thousand five and document ten thousand                                 and six I don't really need to store you                                 know three bytes for each of those I                                 just need to store one bite the                                 difference between them and so we can                                 use vient the bite right for that as we                                 saw earlier notice again there's a bunch                                 of grey zeros that means they're they're                                 wasted space right even though we were                                 much better than four by tints we're                                 still not as good as we could be and so                                 that leads us to how we're going to take                                 care of very large postings list so this                                 vient that i described this was that                                 like i said has been around since                                                                                                                 it's used much less and so when we have                                 a very large postings list we actually                                 we want to use less than one byte per                                 value to encode it because for instance                                 if we have a lot of just one two one two                                 one two right these are very small                                 numbers that we can code in a couple                                 bits we don't need a whole bite to                                 encode them so in order to do this pact                                 ince we're going to go                                 all the values within a block that we                                 want to encode and we're going to find                                 what's the minimum number of bits that                                 is required to encode all of the values                                 that we saw on this block once we do                                 that then we can put each of the                                 integers into a bit field so and we know                                 that it'll fit inside those bits so we                                 are going to have any leftover bits that                                 don't fit and then when we're trying to                                 decode because we know ahead of time how                                 many bits are the size of that bit field                                 we can use simple offset math right so                                 you see here if we would just have to                                 multiply and divide right and then and                                 again shifts and masks similar to V bite                                 it's a little bit a little different                                 shift right so V bite we're shifting by                                 seven bits here we're shifting by some                                 number and so an example with the with                                 patents postings so our original numbers                                 we have                                                              deltas these were Delta's that we had in                                 our last example for vient so the first                                 thing we're going to store this number                                 that's all in red this is like a                                 description this all this says is this                                 is the number of bits per value that                                 we're going to store and so we have a                                   there right in binary and then each                                 integer is only going to take up four                                 bytes so notice that we are taking the                                 same number bytes in this little toy                                 example that we did with vient but if we                                 kept going if we had a lot more values                                 obviously still that fit within those                                 four bits then we'd be able to store                                 them more compressed than if we had                                 vient at a slightly potentially extra                                 klaustreichs we have to shift and mask a                                 little bit more so that's what we use to                                 decode our postings lists and so we're                                 going through our query we found the                                 terms we found the postings lists that                                 were associated with those terms we did                                 let's say an intersection                                 so we wanted to find the documents it                                 had Berlin and buzzwords and now we want                                 to figure out what are the most relevant                                 documents what are what do we want to                                 actually return to our user and so for                                 that we're going to score the documents                                 so loosing has this a complicated                                 scoring function you don't have to                                 understand anything about it just know                                 that we're going to talk about this part                                 here so this is these are the norms                                 these are an index time scoring factor                                 that we're going to store in the index                                 and we're going to load or have loaded                                 that query time available for us so that                                 we can tack that into the score so it's                                 all the indexed time scoring factors the                                 main portion of it is what's called a                                 length normalization what that means is                                 if I have a match for instance in a                                 title field that is very short it's                                 probably a more relevant than if I have                                 a match in a very long field like a body                                 field or something like that and so this                                 normalization factor is basically a                                 weight that's going to be applied into                                 that score that's going to boost up                                 documents that that matched in a shorter                                 field this value is actually eight an                                 eight bit floating point integer or                                 floating point value and coded as an                                 integer the reason for that is only big                                 differences matter so if I have a                                 document that matches that that matched                                 in field that was a million in length                                 and won the match in a field that was                                 two million in length they're basically                                 the same right what matters is if I have                                 a field that matched with the head to                                 our length of to write or three or four                                 but all those three two three and four                                 those are also essentially the same so                                 we have small and large and so we're                                 actually going to truncate this value                                 essentially we're going to its lossy but                                 it doesn't matter it it's only these big                                 difference is the matter however the API                                 does allow for Long's so the encoding                                 has to                                 who has to support that this is more for                                 ir researchers and stuff like that but                                 by default we use an                                                 norms right now actually have seven                                 different techniques and talk about most                                 of them so the first one is simple this                                 is just an array so remember we have an                                                                                                     values are                                                        distort an array of the values                                 essentially next is going to be constant                                 this is for cases where let's say I                                 accidentally enabled norms for a field                                 and I didn't mean to and all the values                                 are there just length                                                  single of these documents is going to                                 have the exact same norms value and so                                 for this edge case we have a special                                 encoding which just the first detects                                 but all the values are the same and then                                 just says hey this is the number values                                 we had and here's that one value then we                                 have C Delta so Delta is like the pack                                 tints that we saw earlier with one                                 additional thing we're going to store                                 this this minimum value and then once we                                 store that minimum value every time we                                 decode one of these packed integers we                                 just add it to that minimum value so in                                 this case you can see that                                             just like we expect but                                             would have taken five bits but because                                 we offset it by that minimum value we                                 were able to store it in just four bits                                 and so if that minimum value is large                                 then it can help us compare compression                                 rate the next one is going to be table                                 compression this is again going to use                                 packed in stepped that the values that                                 we're going to                                 right out here so like                                                 right so these values are actually                                 officer indexes into a table of the real                                 values and the reason we do this is the                                 real values were larger right                                          we wouldn't have been able to fit those                                 into the tube it's that way encoded here                                 and so if we had lots of other values                                 but they were all the same essential if                                 we just had a bunch of tens and fives                                 and tens and fives then we're able to                                 encode this much more compactly by                                 storing the values separately and we saw                                 them sorted to indirect is a little bit                                 more complicated so the idea here is                                 that if we have a value that's very                                 common but it's not the only value we                                 see so here we have two tens and a five                                 right imagine we had tons more tense in                                 fact it's the trigger that we use is                                 ninety-seven percent it was kind of                                 randomly chosen but but tested so if we                                 see ninety-seven percent of the values                                 have one or ninety-seven percent of the                                 documents have one value then we'll just                                 encode that common value and then this                                 indirect is going to have two arrays so                                 we start out first of all the array of                                 the doc IDs that had a value that wasn't                                 the common value and so here our                                 document one right this is zero based                                 indexing so our second document had did                                 not have ten so this is the uncommon                                 value and so that was our first document                                 that we encoded in our array of rarities                                 and then we have a corresponding array                                 that matches it element for element with                                 the actual value that that document had                                 and because this first array is sorted                                 by document ID we're able to do binary                                 search over it so we first do a binary                                 search and see does the document that                                 we're trying to find a value for does it                                 exist if it doesn't exist we know it's                                 the comment about                                 you and if it does exist then we take                                 that index where we found it in our                                 array of rare doc IDs and we go over to                                 the values array and we look it up there                                 and the final one that we're going to                                 talk about for norms is patched so                                 they're actually two patched but they're                                 basically the same the idea here is to                                 combine a couple techniques that we've                                 already seen so we again have common                                 values but this this time it's multiple                                 common value so if we have a small                                 number of common values then we can                                 again use a an array of sorted values or                                 and use the index into there as an                                 ordinal meaning an identifier for that                                 value and then we reserve one special                                 value in our bit field here and we say                                 that special value means it's not                                 actually in the array and for that                                 special value we're actually going to go                                 back to a table just like we saw so                                 those special values remember they're                                 three percent or less of the documents                                 are going to happen so it's very rare                                 that we're actually going to go to this                                 exception table so it's a little bit                                 more expensive when we have to do this                                 extra in direction but the very very                                 common values are going to be faster now                                 with that scoring that we saw sometimes                                 we want to add some extra stuff to that                                 score maybe                                                             every document and we're going to store                                 this popularity and this is where we                                 need this way to look up for every                                 document every doc ID what's the value                                 that was associated with that when we                                 index the document and so that's where                                 we get doc values so doc value is there                                 are a couple different types of doc                                 values we start with numerics we have                                 support for both single and multi valued                                 binary and in binary enumeration so                                 binary murmuration is more like the                                 ordinals so we have like                                 a fixed number of values that we're                                 going to see and we're going to refer to                                 them by this ordinal this unique ID for                                 that value notice that binary doesn't                                 have multi valued that's because if you                                 want to have multi valued support for                                 binary you could encode whatever you                                 want into bio right it's just a bunch of                                 binary so some of the ways that we're                                 going to encode doc values we've already                                 seen a bunch of these so a delta                                 compressed table compressed const these                                 are things that we saw before with norms                                 we also have prefix compression which is                                 very similar to what we're doing in in                                 the prefix tree right that prefix FST                                 but there are two different two extra                                 techniques that are kind of a base for a                                 bunch of the different doc values                                 formats that we're going to talk about                                 now so the first one is GCD so imagine                                 that I have a bunch of values and let's                                 take a time stamp for instance so let's                                 say I'm storing milliseconds since epic                                 and I'm actually passing in seconds I                                 for whatever reason i'm passing seconds                                 but i'm storing milliseconds so that                                 means every one of my values it's going                                 to be a thousand or a multiple of                                 thousand so what i can do is divide all                                 of those values by this common multiple                                 right the greatest common denominator                                 and then i can just store the greatest                                 common denominator i'm going to store                                 that                                                                     be just the smaller value the divided by                                 the GCD and so we're able to again use                                 this pact ince right we're able to pack                                 the bits in to a small values we can fit                                 the next one is monotonic so the idea of                                 monotonic is we have values that each                                 document is going to have a bigger and                                 bigger value                                 and we often see that with like offsets                                 for instance right so like those offsets                                 that we saw in postings or two to refer                                 to the postings lists these are going to                                 be bigger and bigger right because the                                 first term is going to be written for                                 the second term is going to be looking                                 for the third term these values are                                 going to grow and what we can do is fit                                 in this case a linear model it's what we                                 use to the growth and then we can just                                 encode the difference from the model and                                 so here we can see that our first two                                 values right those are the ones we fit                                 the model to and they fall right on it                                 so all we need to encode is a zero for                                 those and then we have this one value                                 five there it's a little bit off but we                                 just need to encode a one right so this                                 is what it's going to look like and                                 obviously we have to encode what the                                 model is right so we're going to store                                 the M and the B and then for each value                                 we're able to take that apply which                                 document number we're on and add the                                 deviation from that so finally we have                                 got all of the documents now that are                                 most relevant right we've known a sort                                 on this score that we we took our                                 relevancy score from the scene we                                 multiplied it by popularity that we                                 pulled from doc values and now we want                                 to return something to the user so what                                 are we going to return so we want to                                 look up in the stored fields API in the                                 scene stuff that we stored along with                                 documents so this could be like unique                                 identifiers that we originally had maybe                                 it's the the full text of the document                                 like underscore source in elastic search                                 and so we want to look that up so how                                 are we compressing that so there's two                                 different techniques that are used we                                 first have lz                                                         lz                                                                   this is not going to be very exact but                                 so we're going to start by writing out                                 we have bee buzz                                                        riding out we write out be buzzed                                      and then we get to the second be buzzed                                 and we notice hey this is the same thing                                 that I saw earlier so instead of writing                                 out all of those be buzzed characters                                 again we're going to write out a couple                                 bites that actually refer back to where                                 that be buds originally occurred right                                 so it's a reverse offset and a length                                 that we're going to store and so                                 essentially we have two different types                                 of entities within the encoded stream of                                 LZ for lz                                                              like these are the things that I                                 actually wrote out and then we have back                                 references write these things that are                                 referring back to previous text that we                                 found now in in recent versions of                                 leucine we added the ability to have a                                 higher compression ratio and what we do                                 this with is deflate so deflate is very                                 similar to lz                                                           same family of of encodings the idea is                                 to take lz                                                             over it and in that second pass we're                                 going to find these common things that                                 are still common and we're going to                                 actually compress them down even more                                 using Huffman codes so we're going to                                 build a dictionary you know keeping                                 track of how many times we saw                                 particular tokens and then we're going                                 to rewrite it with these smaller binary                                 values okay so that is my talk and for                                 questions if anyone has any questions I                                 have beers in honor of Robert mirror and                                 yeah thank you                                 you
YouTube URL: https://www.youtube.com/watch?v=kCQbFxqusN4


