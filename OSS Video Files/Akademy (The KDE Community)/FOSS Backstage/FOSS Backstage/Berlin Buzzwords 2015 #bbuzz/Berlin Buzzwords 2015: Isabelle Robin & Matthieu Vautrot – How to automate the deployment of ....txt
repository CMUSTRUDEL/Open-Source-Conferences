Title: Berlin Buzzwords 2015: Isabelle Robin & Matthieu Vautrot â€“ How to automate the deployment of ...
Publication date: 2015-06-04
Playlist: Berlin Buzzwords 2015 #bbuzz
Description: 
	Data Science has enabled companies to establish predictive models about their sales, forecast their needs in human resources, enhance their customer knowledge and so much more. But what is the afterlife of these models? Are they doomed to perform one-shot predictions and then fade away? After the training and testing steps, the final part of an end-to-end Data Science project should be deploying the constructed model in a production environment in order to reuse easily its results.

Today this step can be quite time-consuming when it involves rewriting completely the machine learning model in another language or combining specific skills in machine learning and production coding. In this talk, we will present several techniques to automate the deployment of any R model in two complementary production environments: a big data cluster and a web service.

Read more:
https://2015.berlinbuzzwords.de/session/model-code-how-automate-deployment-r-models-production-environment

About Isabelle Robin:
https://2015.berlinbuzzwords.de/users/isabelle-robin

About Matthieu Vautrot:
https://2015.berlinbuzzwords.de/users/matthieu-vautrot

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              all right thank you for the introduction                               hello everyone                               we are really happy to be here today and                               thank you for attending our our little                               talk this is the first time we're going                               to do this talk so please be be a                               patient if it's a bit rusty sometime                               today we're going to talk about a                               project we've done for one of our client                                which we called Model S code                                the aim of this project was to to                                develop an approach on how to automate                                the deployment of our models in two                                different production environments so                                first a little bit a little word about                                canary canary is a young startup we are                                three years old now and we consulting in                                data science in based in Paris so we                                help our client with the analytical data                                project and so on this is going to be a                                two-person speech so first it was going                                to be Isabel another scientist at khatma                                tree and and me was also working for                                Coventry other as a consultant so just                                for an introduction and for for having a                                little bit of context usually what we                                see at our clients and how they deal                                their data science project and they're                                all the analytical project is that they                                have to separately the environment for                                form for doing there are the different                                and each core projects usually they have                                their projection environment where they                                have all their businesses use cases this                                is where they have their typical a DBMS                                sometimes they use Hadoop in production                                and different different system for                                further and further production and then                                on the other end as a sandbox there's a                                development platform sometimes we call                                it data lab this is where we are going                                to gather and centralize all the data                                and this is where all the scientists                                will will develop their model develop                                the insight from from there from the                                data they have in there                                penny so I'm just going to present a                                little bit faster the different steps of                                this kind of project though are actually                                quite typical steps the first step is                                usually to first collect the different                                data sources sources can be external                                sources or internal sources then you                                have to store the data source into a                                DBMS into almost akin no sequel engine                                or even sometime the file system of your                                development environment and sometimes                                even Hadoop then you do you the                                pre-processing the different first big                                analysis and this is where we're going                                to make your data talk with each other                                and get rid of any duplicates if needed                                and so on and then there's a predict                                step this is where usually the data                                scientist use this use R or Python to                                produce                                                                they just gathered and organized the                                little bit and finally usually in those                                kind of project the real-life test on                                the model they just developed to say                                okay if is the model good enough for                                business does it meet the requirement we                                we want heed so if not we'll go back to                                a full cycle and if yes what happens                                what what do we do now that we are happy                                with our model this is going to be the                                question at this talk today                                what about deployment usually we still a                                lot of time that struggle with this                                question and say okay my data scientist                                are ready they know are they know Hadoop                                and so on but what next they are afraid                                of of the model being stay expect in in                                those kind of sandbox so usually the                                classical approach is to tackle this                                problem is to first recode the model                                once it's trained on our Python then you                                can get your results back and say okay I                                will implement the different coefficient                                of my my linear regression and so on                                into sequel C++ and Java is this fine                                works well of course it takes a bit of                                time and you can also use the PMML                                stand-down                                which is a                                kind of norm which allows you to write                                any a lot of models into a XML file and                                then you can use a API such open scoring                                to read back your XML file and do the                                 scoring scoring on your different                                 platforms but PMML is fine but what                                 about non-sporting models but all models                                 are as per be are able to be written                                 into into xml and some prediction                                 environment doesn't speak PMML so it's                                 not really a general general approach so                                 was for this problem who who didn't                                 bother our client was a player in the                                 French energy industry and he did he                                 built model for predicting consumption                                 consumption into aggregates part of                                 France and they were really happy with                                 the model and they didn't want us to                                 improve it which which which is fine for                                 some time and as you can see you the                                 model they play they use in orange line                                 that the predicted consumption and in                                 blue the realized realized the observed                                 consumption so this works well but and                                 this is a well the problem is they                                 really happy with the model and they                                 don't want to change anything and what                                 they do is that they use for this kind                                 of use case what we call gum model gum                                 is a generalized additive model                                 it's another model that's trying to fit                                 on any variables of your data data set                                 one's going to try to fit a function                                 that it has into its its library so they                                 use a very specific R package called MDC                                 which is not supported by PMML and that                                 the results are not easily read really                                 reco recordable you cannot record that                                 is those kind of function that our model                                 uses so so they has test is there any                                 way for for                                 less to get less struggle with this step                                 and we had an approach in in our mind                                 for for quite a while and so and it was                                 the time to actually say okay we we have                                 an idea which would should work on paper                                 and we will try we'll try with you and                                 see how it goes                                 so the model of code the name of the                                 project the the goal is to directly                                 deploy and use the AR object for                                 projection so the goal is to say okay                                 I'm going to use my our object which                                 does the prediction fire the prediction                                 step quite seriously and I'm going to                                 deploy it directly into my production                                 environment and use it for my for my                                 day-to-day analysis to do so we set                                 three goalie in our mind first of course                                 we want you to drastically reduce the                                 deployment time so that it was really                                 take less time than recording the the                                 model we wanted the approach to be as                                 general as possible so for one                                 environment we wanted to put just one                                 code for any model we will submit after                                 that and we wanted of course to be                                 stable in performance we didn't know if                                 stacking up the production size                                 prediction service plus R plus the                                 different layer of this approach would                                 be really stable in performance so we                                 want you to be careful with that so to                                 go into a little bit more details I'm                                 going to let Isabel talk to for the next                                 part of this page okay so I explained                                 you first the few details about our                                 approach how we designed it so first                                 thing first you've got your development                                 platform where R is installed on it and                                 you've got your production platform                                 where a service is running let's say                                 it's your data or Hadoop so the first                                 step is to serialize your your model                                 this step is quite straightforward                                 because in our for example you've got                                 some functions to save RDS save in RDS                                 or save in RDA format which are                                 serialized format so once you've got                                 your binary objects                                 you should deploy it to the production                                 platform                                 the step is also quite easy because you                                 just have to copy it to the production                                 platform but the most tricky step is                                 when you want to do the prediction in                                 fact you want to enable the                                 communication between our and your                                 service so first step is to install our                                 in your production platform and then to                                 enable the communication between the two                                 so you for this you will need some                                 specific tools fortunately in Java there                                 is a library that exists it's called our                                 Java and in merges two projects gyri                                 which enables to open our session in                                 Java                                 that's the library we will use in this                                 project and our Java which enables to                                 use Java in your art session so the name                                 our Java bundles both are Java and                                 j'irai then to use j'irai you just need                                 to use to start to start your are in                                 join to to open your our session in Java                                 so what you will just use the function                                 get main in join and then after that you                                 just have to use you can use any our                                 command in fact with with the function                                 eval and you just have to pass your our                                 code into a parameter of your function                                 evil so that's quite simple in fact but                                 on our way to prediction we realize that                                 our objects our modal's                                 the circularized modules were really                                 quite heavy in fact it's because they                                 carry everything for for training so                                 they really have to in fact we just                                 wanted to do the prediction with this                                 modal's so we didn't need at all all the                                 data for all training so we decided to                                 remove all this metadata all this hidden                                 metadata by iteratively iteratively                                 removing the the data that was not used                                 for that was not necessary for a                                 prediction                                 so you                                 so you you can you can develop this                                 approach in several prediction                                 environments it could be a web service                                 Hadoop relational database complex even                                 processing in fact the approach is quite                                 general so you could imagine a lot of                                 prediction environments but we decided                                 in the time we had for this short                                 project to develop two of these                                 approaches the web service and the                                 Hadoop approach so now I will talk about                                 the web service approach which was known                                 as the REST API so how how will that                                 work first you build your predictive                                 model on R then you serialize and deploy                                 it to your API things to put request                                 then you have to launch your prediction                                 on new data thanks to a post request                                 okay so the architecture is quite simple                                 so you just it's just classic                                 architecture the only new thing is in                                 the communication between R and Java as                                 we said earlier so in your request you                                 have to enter three parameters the type                                 of the modal let's say it's a linear                                 regression logistic regression again                                 modal you have to enter the ID of the                                 modal in fact it's the name of the RDS                                 site of the RDS file and you have to                                 enter the daysand data so this requests                                 goes to the modal exposure class which                                 translates every HTTP request into a                                 call to a function which will deploy the                                 our code then this deploy your class                                 gathers old functions that can call the                                 our code thanks to joy and your data                                 goes to the input/output class that                                 modal's your JSON data you will have                                 free type off requests the first the get                                 request which enables you to return old                                 available modal's                                 in your API the put request which                                 enables you to deploy a modal and load                                 in a serial libraries of                                 your API and then the post request which                                 enables you to have the prediction of                                 your new data this the design of this                                 freeway request was I kind of inspired                                 of another projects open scoring which                                 is an API for deploying PMML modal you                                 can check the rig a job and there are                                 quite a bunch of interesting information                                 that so how about the post requests how                                 do you do the prediction first you've                                 got to read the data so we chose to do                                 this kind of structure of JSON so you                                 can have several features possible and                                 you can your multiple rows are are in an                                 array in fact and in this API we were                                 able to score on double values features                                 then you have to convert this JSON data                                 to data that are will understand so                                 thanks to RJ's entire library you will                                 convert that to our data frame and then                                 you have to load the modal so loads of                                 specific libraries according to type of                                 your modal in our case it was mg CV                                 library and then you have to use our                                 function to read the modal to read the                                 RDS file ok so now you're ready for                                 prediction you only have to to use this                                 our code thanks to the gyri library so                                 you only have to deploy this code and in                                 our there are there is only a function                                 that is called predict and that enables                                 you to to predict on several several                                 modal's any any any type of modal in                                 fact so you only have to pass the                                 parameter the idea of the modal and the                                 new data ok so once you've done that you                                 do the same work and convert this data                                 frame to JSON output ok so I'll hand                                 over to metaphor it's a big data                                 approach                                 yeah so the rest API was really working                                 really great                                 we had good results because it was quite                                 dynamic we will see we'll talk a little                                 bit about the result later to fold as a                                 conclusion and so the the second                                 approach was more for a batch approach                                 where if you have a lot a lot of data to                                 to score what if you don't just want to                                 score aggregate part of friends but                                 maybe every single household maybe every                                 single who knows with the internet of in                                 every single fridge who knows what they                                 want to do next so maybe they have an                                 Hadoop cluster so we we thought it was a                                 good idea to try to develop the the                                 approach and on a Hadoop cluster so                                 first as to there's a lot of ways to                                 implement to make a communication                                 between R and Hadoop in if you are if                                 you want to split into two big families                                 there's a streaming streaming approach                                 which works really well it doesn't take                                 a lot to set up it's easy to implement                                 but it uses standard input and output                                 communicate between your your Hadoop                                 jobs and the r the r functions and                                 actually we made it work in just a                                 couple hours just to find the right the                                 right line right here and find the right                                 jar in the right type of streaming job                                 so what we want you to do to do                                 something a little bit more complex                                 because we were quite concerned about                                 the performance and the stability of the                                 approach even though it's probably                                 working really fine so another big                                 family of approaches that you can do on                                 Hadoop's                                 were thinking about encapsulation you                                 can rewrite a specific MapReduce jobs or                                 a specific UDF UDF just just as a                                 reminder it's user-defined function it's                                 usually something you find in our DBMS                                 databases where you can implement your                                 your specific function in in your code                                 or C++ code and hive and pig actually                                 gives you the ability to write UDF so it                                 is without better because we have a                                 better control on input data                                 we'll be sent between our different                                 different HDFS file it's going to be                                 Java code that runs on Java codes we                                 like that and it uses GRI as Isabelle                                 told you to communicate between R and                                 Java and we started to turn a little bit                                 about this library so we went for the                                 UDF ugf approach and just really quick                                 we won't go too much into too much                                 detail about this one the general cycle                                 we wanted to achieve was to having hive                                 to launch the query with the UDF that                                 will be will develop and then the UDF                                 first will have to deal with the input                                 data see if there's null value see how                                 many parameters will be sent into the                                 ugf and organize the work for GRI for                                 the next step                                 ugh GRI part will deal the oscillation                                 and send instruction to our for the                                 photo scoring and scoring phase then a                                 score score the the line that hit                                 receives it as an input and then                                 although the result goes back undone to                                 hive that aggregates hold the result for                                 a lot different file on HDFS HDFS to be                                 going to more details about ugf on hive                                 you can develop two types of UTF a                                 simple one which is good well it's also                                 a bit more complex one a bit more                                 complex to write but which offers you a                                 bit more control of your of your data                                 they're called generic UDF read some                                 somewhere that it's supposed to be even                                 better in performance than regular UDF                                 so might be even better so it deals                                 dynamic number of parameters because                                 once again we wanted the approach to be                                 general so what if we change the model                                 and the number of parameter inside the                                 model changes then we we have we want                                 the UDF to adapt in this kind of                                 situation a better with nil value and                                 also it deals constant parameter better                                 as well as we will see we we are sending                                 constant a constant string in in our ugf                                 to organize a little bit of the of the                                 work so yeah we recommend to use a                                 generic ugf as                                 praxis first things first                                 setup stays so far if you have a cluster                                 with multiple nodes then then then you                                 have to set up your environment for each                                 each Hadoop node and so first you have                                 to install our and our Java and every                                 single node you have to install the                                 model required libraries which in our                                 case would be mg CV so of course if you                                 change your model then you'll have to                                 reinstall a specific library I guess and                                 then yeah I won't go into too much                                 detail but there were quite some                                 vironment enviable to set up so I down                                 the slide if you want to try the                                 approach some file that you have to move                                 into the Hadoop some Hadoop short file                                 some shell library yeah you slugs                                 it helped us to save you a lot of time                                 because in the setup phase we did a lot                                 of try and an error and actually Hadoop                                 slugs are great and we there everything                                 there and the message are quite explicit                                 when there's something that goes wrong                                 or any variable that are not set quirky                                 if we take the workflows walk through                                 classical workflow of the approach back                                 then what about this serialize and                                 deploy the model of phase as a new                                 station                                 if we save your model or our model and                                 we call it a new model then there's a                                 various phase which will put you give                                 you the RTS file and we will use the                                 Hadoop distributed cache function to                                 deploy the model and every single node                                 of our cluster and on hive it's it's                                 written like that is just a ID file                                 model dot add yes and then your file is                                 is been distributed into the memory of                                 every single node on your cluster for                                 the prediction step so just for an                                 illustration if you could into much more                                 detail about the the model when you                                 train the model in our case it was a gap                                 model you say okay my model                                 formula and I want to predict the                                 y-variable as an expression of my first                                 variable r                                                             and when you want to call prediction you                                 want to build a data data frame from                                 your from your new data and then you                                 call the predict function this is a                                 specific on any model prediction model                                 any predictive model implements a                                 predictor function so it's quite generic                                 so you called your project function on                                 new model and with your new new data so                                 what does EDF do is actually it's going                                 just to do the mapping between the                                 expected value the expected parameters                                 in the our model and the different                                 fields from from our hive table we want                                 to score                                 so basically pretty much it we just take                                 the the concentric constant string we                                 sent to the to the UDF and we do the                                 mapping between f                                                        we built a string r                                                     a hive hive : and our - we go equals f                                  which is another hive : and then with gr                                 I will send the prediction to R so the                                 whole hive probably looks like that you                                 had with your different libraries are                                 you using your in your you gf+ the UDF                                 the add file and then the Select query                                 looks like looks like that it's very                                 simple so now we are going to try to                                 have a little demo which will it's not                                 going to be as impressive as the other                                 one we won't tap in our hand but we will                                 still try to do something ok so yeah I                                 reduced on my microphone so that I don't                                 mess up if we this is a query I'm going                                 to stand to hive to to actually query                                 the data we have in our our table we                                 want to score let's say we want to                                 scroll this this table so it's hive so                                 hopefully we have a little bit of time                                 and if it it's not that fast then I'll                                 skip the demo part                                 okay so this is how it does take quite a                                 long time so it's real high and we did                                 developed it on a on a VM and we had                                 quite good results or origin so if let's                                 say we have we want to call this table                                 and our data set being used to train the                                 model looked like that we had a time                                 which are it's like a time theory the                                 idea is the time value and then a second                                 other variables that helped us to build                                 our model then if we want to watch the                                 the query just to just to show you I                                 didn't lie to you it looked like that                                 then you add your jaw you and you use                                 the query for this specific specific                                 case so once again the constant string                                 indicates which which model you want to                                 use which are your data scientist we                                 produced a new model which is called a                                 zone                                                                     then you just have to say ok my ugf i                                 call it with this string function and                                 the corresponding variable from my hash                                 table so if we send it to hive                                 also take a couple seconds but                                 apparently it doesn't lunch is MapReduce                                 jobs from the father knew new version of                                 height when we developed it it wasn't a                                 bit holder version of hive so every                                 single time we did some some tests we we                                 had the typical MapReduce failed with a                                 different percentage going on but there                                 you go                                 I called the the projection on my menu                                 data set and the results here shown in a                                 projection concern consumption is the                                 results of our using the RDS RDS file as                                 a other model so that's it for the demo                                 so I'm going to make the he that will do                                 the conclusion and perspective and                                 finish the job                                 okay so in fact we realized that these                                 two approaches were two complementary                                 approaches in fact the rest API is quite                                 agile and you score quite fast on a                                 small amount of rows but once you you've                                 reached the limit of the API the UDF                                 i've approach is is really more relevant                                 so because it scores really fast on a                                 big amount of rows so both both                                 approaches are complementary so that is                                 what satisfied the client in this this                                 project okay so for perspectives we sell                                 a lot of quite cool tools in the Berlin                                 buzzwords conference so that could be                                 really realized to test the general                                 models code approach in these tools and                                 what about Python so earlier for the                                 data science data science process step                                 we talked about Python a lot of data                                 scientists love Titan there's not only                                 are that that enables you to to do                                 analytical models so what about it could                                 our approach be generalizable to to buy                                 them we think that yes because RDS files                                 the serialization in our could be done                                 with pickle file in Python and for jri                                 library we could use our two pi and for                                 example for the web service we could                                 have used Jango so we we were to think                                 that this this approach would would fit                                 also in in Python so thank you for your                                 attention and now it's time for lunch or                                 questions if you have some
YouTube URL: https://www.youtube.com/watch?v=GLpvHDVdHSE


