Title: Berlin Buzzwords 2015: Michal Rutkowski - Cassandra at Yammer #bbuzz
Publication date: 2015-06-04
Playlist: Berlin Buzzwords 2015 #bbuzz
Description: 
	The Apache Cassandra database is the right choice when you need scalability and high availability without compromising performance.

That may very well be true, but it took us a year to get there and that road was dotted with site’s lowered availability and compromised performance.

In this talk I am going to share the insights we gained when we migrated parts of the Yammer’s messaging pipeline from a custom storage solution backed by Java BDB to Cassandra. Covering topics like:

- Why we decided to move to Cassandra from our proprietary Berkeley DB backed database 
- Modeling data and capacity supported by metrics
- Zero downtime production rollout
- How things started falling apart after three months of seamless operation 
- How to diagnose and fix things later on

Read more:
https://2015.berlinbuzzwords.de/session/cassandra-yammer

About Michal Rutkowski:
https://2015.berlinbuzzwords.de/users/michal-rutkowski

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              I                               hello I'm Mohammad kofsky i work at                               Yammer                               engineer sorry I just need                               the tools okay so this is the rough                               outline what we're going to talk about I                               will briefly tell you about Yammer for                               those who don't know then I will explain                               what we wanted to change and why then                                how we roll out Cassandra and what                                problems we encountered and finally I                                will conclude with by talking of what                                we've learned and what worked well for                                us on that project so as you can see                                Yammer the enterprise social network                                whose goal is to facilitate better and                                faster communication within an                                organization and here are two                                screenshots one shows a view of Yammer                                which probably looks familiar quite like                                Facebook I guess it shows a group view                                we are looking at one of the groups in                                the amur where teams collaborate and and                                the right hand side screenshot is the                                inbox which covers the conversations                                which the user follows which are address                                to them or the user was mentioned in but                                both these are considered to be                                something we call the message feed there                                are quite a few other message feeds and                                i would say that Yammer is all about                                messaging and a concept of a message                                feed is quite central to the product but                                the talk will be about extracting this                                inbox message feed to a separate service                                and why we did it and how we approached                                it so this is a quite outdated diagram                                of service dependencies in the amur but                                that's the best I could find as you can                                see in the center there's this purple                                box called work feat that's our Ruby on                                Rails mana up it's a bit of a legacy and                                the green things around it our drop                                wizard java based services for those I                                don't know how many people know what                                drop wizard is okay quite a few well                                drop wizard is not so much a framework                                is more like a glue which brings                                together several convenient to use                                libraries and which allows that it's                                open source and it allows us to quickly                                develop services it kind of removes the                                boilerplate anyway so the problem we are                                facing we are trying to extract features                                from work feed into separate services so                                we can iterate on them independently but                                it is a bit of a legacy to work with                                that and it                                see as well that this is pretty complex                                so this requires us to have quite a bit                                of operational a decent operational to                                link so we obviously have some sort of                                we have continuous integrations to ship                                with confidence we have a tool for the                                deployment it's also a drop wizard                                service would we build we have quite a                                bit of analytics we even built a system                                for cui requiring multiple databases and                                we have a whole team devoted to                                analyzing data data scientists I guess                                and drop was it comes with a library                                called metrics which makes it very easy                                to capture and measure behavior of the                                service or characteristics of the data                                which is processes and it's also also an                                open source project we piped it to Kafka                                and there is being transformed a bit and                                it's pushed to add a tool like a third                                party tool called white front allows us                                to visualize and create quite complex                                queries actually both for past behavior                                analysis as well as kind of on call                                real-time monitoring of the system so                                this is more or less real time it's I                                think it updates in                                                    and and we of course we use log                                aggregation work session cabana and so                                just to give you an example this is how                                the screen for our deployment tool right                                on my list name of the service it was                                developed in London it dealt with inbox                                and sound like a sensible                                              you can create a package you can deploy                                it to a different environments you can                                see the recent history you can also see                                a more wider view of what's happening in                                the Oregon terms of deployments and here                                to give you an example of the our                                monitoring this is reading the inbox                                 graph you can see latency and you can                                 see frequency there's this pearl this                                 one graph which is a bit out of line it                                 turned out this was a faulty node                                 basically we had RAM errors so it's                                 getting a bit out of sync and it also                                 affected our p                                                          inbox but is the kind of things we use                                 so more to the point so what did we want                                 to change in Y in this project so we                                 undertake extract the inbox feature from                                 an existing service it was already                                 extracted after the menore app because                                 it powered all the messaging feeds and                                 we wanted to be able to iterate faster                                 on the in box itself we didn't want to                                 be constrained by the kind of common                                 denominator of feeds and we also were                                 facing a different problem we were                                 working across DC we needed this for                                 data recovery and our existing database                                 based on Berkeley DB was in process it                                 was expensive to scale I had a bad                                 support story and it didn't particularly                                 well work with cross DC setup so there's                                 a like a product require motivation but                                 we also had an infrastructural                                 motivation for this project so to give                                 you an overview of like from that more                                 complex diagram that's roughly how our                                 message pipeline looks likes we have the                                 rails man up we have which is powered                                 mostly by them as postgres but we also                                 have an icing job to update to talk to                                 the drop user service which powers are                                 messaging feeds it serves as a bit as I                                 like an index service if you like for                                 the database there because the core the                                 range course on this SQL database are                                 too expensive for us so just to give you                                 an overview either post a message dab                                 does some logic to decide if they are                                 allowed etc etc it's stored in the                                 database and then a task is put on                                 rabbit a worker picks it up besides who                                 is to be delivered to and then sent off                                 to the drop wizard service which updates                                 is there's database and when the user                                 wants to read an inbox it adds a rails                                 app that rails up delegates to the drop                                 wizard service to tell us was the                                 content of that user's inbox what                                 threads are in it and then it hydrates                                 it with the contents of the messages                                 sorted the postgres TB which is actually                                 fronted with memcache yeah and its                                 result is returned to the user so what                                 part of that system we wanted to change                                 and how well again the view so we wanted                                 to train that back-end and yeah so sorry                                 it was a quick term so we wanted to                                 break down that drop wizard service and                                 this is what I'm gonna be actually                                 talking about we didn't want to break it                                 into two pieces and we wanted to remove                                 the bdb back end so now we came up with                                 a rather complex diagram I would say                                 it's probably too complex but                                 kingstown in our overall overhauling of                                 posting architecture but basically what                                 happens here is the old stuff happens                                 and one this once it happens we spin off                                 a second task to update our new service                                 basically it would probably be like long                                 term we want it to be more a pops up                                 model and actually I'm going to talk                                 about this component of the picture                                 today so I drop with your service backed                                 by a Cassandra so okay we had that we                                 know why we wanted to do it and how                                 roughly but a few words about our                                 methodology so we decided so we wanted                                 to capture our API and the semantics of                                 that API as early on integration tests                                 so that we kind of knew that what we are                                 delivering actually satisfies the                                 requirements we also wanted to use                                 production traffic for both capacity                                 planning and load testing to that end we                                 pretty early on we did a shadow deploy                                 of the surface and we use double                                 dispatch for both verizon reads just to                                 see how it performed etc we migrated the                                 data early serve either to work on the                                 realistic data set and we're running                                 verification tasks to pick up any data                                 and consistencies between the two                                 databases because as I said this is a it                                 was a also a legacy integration so we're                                 bound to miss fangs and we wanted to                                 detect that and we kind of also assume                                 we're going to make mistakes both in                                 data modeling so we'll come up with bad                                 design but will also miss use cases                                 something I mentioned as before so you                                 wanted to make the backfill or migration                                 task from the old to the new database as                                 cheap as possible and easy to do                                 effectively so that if needed we could                                 do it few times a day effectively so to                                 give you some context new is that inbox                                 is very heavy it receives about half a                                 billion read queries a day it's not so                                 right heavy is one tenth of that is                                 actually right for two individual in                                 boxes but what we have is we have                                 massive spikes so a single message                                 sometimes can fan out to                                            boxes and we do fan out on right so we                                 needed a technology                                 that will be good for reads but could                                 also provide like real time delivery in                                 the face of these massive fan outs we                                 wanted that the rights to be actually                                 also pretty fast so the first step was                                 to choose the database to back our new                                 service and we considered react and                                 Cassandra we actually have run react in                                 the Yammer we were running really am                                 already but we weren't sure if it will                                 be a good fit for data model so but both                                 of them are chardon and replicated so                                 that provides us horizontal scaling so                                 the address is they're expensive to                                 scale aspect and it replicators should                                 be highly available at both work well                                 across across disease and have a support                                 story time which is important for us we                                 have paying customers and we don't                                 necessarily have the in-house expertise                                 so we chose in the end which was                                 Cassandra because we did not want to be                                 forced to the read-modify-write on a                                 message delivery basically by that I                                 mean we've react unless we went to super                                 complex difficult to understand data in                                 terms of performance data model we would                                 have to read the whole inbox updated and                                 write it back now that would might could                                 provide natural saturation problems but                                 it was also bad from performance                                 perspective and if specially if the                                 inbox was contended because there were                                 multiple right stood at the same time                                 this kind of optimistic transaction                                 system the setup wasn't great for us so                                 that's why we went with Cassandra                                 because it allowed us to update exactly                                 what we wanted without reading the inbox                                 which would give us like very fast rides                                 so the second phase was to get something                                 working out so we've proven the hardware                                 with this week I'm okay now came up with                                 a decent restful api by decent I mean                                 some people captured probably eighteen                                 ninety percent of the use cases the rest                                 we discovered in the due process and we                                 build tests around that API and we set                                 up a build process that runs those tests                                 and actually starts the service and hits                                 Cassandra this is a Travis orvis                                 Cassandra's in Java so we used my event                                 basically but it takes two minutes to                                 run the whole suite of I think                                           and we started implementing against that                                 basically so                                 to go deeper into the design of like the                                 principles by which the inbox works so                                 it stores as I said before it starts to                                 Fred's address and watered by the user                                 and those threads when we view them are                                 ordered by most recently replied to so                                 we want the most recently updated                                 threads to be at the top of the inbox                                 but the Fred contents as I've said                                 before isn't actually stored in the                                 service that's the hydration happens                                 outside this is like a index basically                                 so on a message post we would deliver                                 the message to every inbox and this                                 effectively amounts to updating the last                                 message ID so that we can maintain the                                 or drink and some little pieces of                                 metadata and on read will paginate will                                 just read the most recent Fred's let's                                 say top true the most recent                                          and quite a fan would filter mostly by                                 red on red there are some other cases                                 but they don't matter that much so we                                 can remove the first design mmm it was                                 pretty simple we had a table for in                                 boxes it was partitioned by inbox ID it                                 was primary keyed my inbox ID last                                 message ID for those not familiar of                                 Cassandra if you have stuff in the                                 primary key basically data as ordered by                                 a primary key so that allowed so that                                 affect Lee meant but data is ordered                                 from the most recent to less recent                                 stuff and we use secondary indices for                                 filtering the filters were of small like                                 there were like three values for a                                 filtered mouse so there were a good fit                                 and we had a threat table because we                                 needed to maintain some metadata for                                 updating the                                                             updates in the past it was the last                                 message because in order to update that                                 table we need to know what was in it                                 before so we had to check it by thread                                 and yeah we this was partitioned by Fred                                 ID and it used the set crdt so and of                                 eventual consistent data structure                                 provided by Cassandra which was very                                 convenient so it was great that's we're                                 passing it fitted perfectly with our                                 usage patterns                                 and was self-healing the presence of out                                 of order deliveries or system partitions                                 it was also an important more or less so                                 except that it brought our migration                                 task to a halt it just wasn't working                                 with our migration traffic so what went                                 wrong well we discover that the                                 secondary indices are slow as hell                                 actually and crd teas are ok for small                                 and if in frequently updated data sets                                 but not something like our subscription                                 lists which we try to keep very                                 consistent and and secondly the cost of                                 our conveniently stored data was the                                 heavy the reliance of Dilys because the                                 message ID which was in the primary key                                 could not be modified it had to be                                 deleted and reinserted so that that's a                                 no-no in Cassandra world but so what do                                 we do now well we actually expected this                                 kind of thing because we're only just                                 learning Cassandra nobody of us from                                 from our team use Cassandra before and                                 we actually wanted to push something out                                 as quickly as possible so we could get                                 like realistic data to benchmark our                                 solutions but what was important is that                                 this did whatever our API was already                                 established and we had the semantics of                                 that API well captured in tests and we                                 had this migration task available so we                                 could use our metrics our tests and the                                 migration to quickly iterate on                                 implementation so this was a bit of a                                 setback but it wasn't and the world so                                 we came up with a second design which                                 was a bit dramatically different we                                 forgot all the cassandra extras and                                 designed around what is good at and and                                 in order to come up with this design we                                 had we kind of went deeper into our                                 product requirements or future                                 requirements and try to leverage that to                                 make sure that our design actually works                                 and we use analytics metrics etc so we                                 discover what we do not need to keep all                                 the data we just need to keep recent                                 stuff because content is searchable                                 otherwise and for instance                                              only take                                                      relatively little and it covers four                                 years for an active user so if we kept                                 on that would be                                 you pretty much doing everything we need                                 two or more and will be probably we will                                 be able to do what we I will explain in                                 the next slide and then importantly                                 since this is an index which basically                                 tries to show you what content isn't                                 available and in what order we actually                                 don't have to be that exact about what                                 data we store so let me explain so just                                 to give you an example what we came up                                 with we decided well partition still bi                                 inbox ID this was important because it                                 meant that hot and less hot networks of                                 in the amur would be evenly distributed                                 so there would be no hot spots but this                                 time we decided to partition by inbox ID                                 and thread ID and made last message ID                                 that our mutable metadata stored in the                                 inbox and the secondary table basically                                 dropped that that Fred table basically                                 dropped the use of CR DTS we just use                                 this as a controlled manual maintain                                 secondary index so this was clearly                                 going to be mutation heavy so we decided                                 to use level compaction and I don't know                                 how many people are familiar of                                 compaction Cassandra okay so Cassandra                                 uses like Asif has a two tiers to it it                                 has a append only log that's why it's                                 very facet writing but then that log is                                 consumed and materialized in Samuel is                                 called an SS typist assorted table I                                 guess and that's only then it's                                 conceivable for reading and these tables                                 are immutable so when there's let's say                                 you have an entry in the table and you                                 mutate it what effect will happen is                                 better a new table will create it with                                 an override so a query for a value will                                 have to go through all the SS tables                                 which contain it and take the most                                 recent one and and typically typically                                 cassandra is called sighs tiered                                 compaction so where to SS tables become                                 of similar size or four I don't remember                                 the exact details they're compacting to                                 one so this way next time you query that                                 as a stable you only have to read one                                 not four but level compaction is a bit                                 different it uses                                                       that at mouse you have to consult                                       the stables but expected number of SS                                 tables around one so that makes it                                 of constant low I oh it makes reads much                                 faster without clear okay sorry and so                                 we knew that there will be racist race                                 conditions on updates but that's what I                                 meant by not being exact on active                                 threat it doesn't matter we just order                                 and filter so if two Fred's are                                 competing for the high top position or                                 they have like the last message right is                                 not they actually last message ID but                                 the one before it doesn't matter because                                 we still can serve relevant content and                                 unless active ones races will be                                 negligible and users will correct it                                 effectively and this the second                                 especially the second one was a bit of a                                 hypothesis which we had to verify but it                                 proved correct there are no complaints                                 about ordering in inbox and on read we                                 read the whole inbox now not just the                                 top twenty first but we have to read                                 everything we sorted and filter but it                                 is small this is actually ok even better                                 because of how we represent Fred's where                                 the Fred ID is there actually the idea                                 of the first message in that thread this                                 data is semi sorted so sorting of                                      entries isn't particularly expensive and                                 we trim access data to make sure that                                 inbox stays in check so we read I think                                 fifteen thousand records trim it to five                                 thousand after it exceeded five and a                                 half that so we don't trim every time                                 but give ourselves a bit of a buffer and                                 so this time we go to production and                                 pass the migration step it was even                                 working in the shadow mode module some                                 data and consistencies from Miss ed use                                 cases which I mentioned but the rich                                 performance was very varied and below                                 our expectations so what now so again                                 usage metrics zoom in even further get                                 more detail out of it so out of the half                                 a million queries we see every day                                 ninety percent are for an unread count                                 actually it's important to visualization                                 in the amur                                 because the reason that is the case is                                 because we use we heavily rely on real                                 time delivery fruit like free comedy so                                 actually the actual load of an inbox                                 happens relatively rarely but we                                 referred unread can't quite often and                                 forever more than read the number of                                 unread messages is usually very small so                                 users have ninety five percent of the                                 users have less than                                                                                                                           entry was totally excessive and exposes                                 too much too much I oh so we decided to                                 materialise that worried and basically                                 maintain a second table for those two                                 cups it was a separate table just on red                                 stuff so it was trivial to query will                                 just query the thousand get select                                 thousand entries from the table and we                                 return the count so where did it get us                                 so our write latency is less than                                 hundred milliseconds for regular                                 messages I mean this is not like this                                 this is a whole service latency if you                                 like and it's less than                                                massive spiky announcements which is                                 okay announcements don't have to be that                                 real-time but overall system is pretty                                 robust in terms of rights and and read                                 latency is pretty good RP                                           milliseconds triple line is less than                                                                                                       inbox again that happens relatively                                 rarely so having a bit of a spinner for                                 that that is okay and the critical query                                 is less than                                                          gives us what we want so yeah we shipped                                 and yeah I should ask you for questions                                 now but actually not so fast because                                 three months later on my first day of my                                 summer holiday Yammer is down and the                                 site is down our services on fire and                                 free Cassandra knows are on fire I've                                 spent few hours on a call with San Jose                                 and San Francisco from war so when my                                 family expected me to be with them so                                 that was in fun so what went wrong                                 so first of all it turns out but having                                 an hae service we had free nose with a                                 load balancer NetScaler all that cool                                 stuff and highly available database                                 doesn't make your site AJ the route is                                 up it's never one thing but that the                                 root of it was a massive inbox which was                                 receiving tons of updates but was never                                 read this method was never trimmed so it                                 grew to like                                                    effectively now level compaction didn't                                 like it because level compaction has to                                 rewrite an inbox on every compaction so                                 each ride would effectively trigger a                                 compaction that compaction had to deal                                 with loads and loads of data so our                                 training strategy so what were the                                 deeper problem so our trimming strategy                                 didn't account for over growing in boxes                                 impact on compaction we just again new                                 technology we didn't fully understand it                                 so yeah we overlooked it but there are                                 more fundamental problem our brain app                                 didn't have circuit breakers so when the                                 service was slow and respond it was                                 waiting for quite a long time and the                                 first poll was exhausted so we couldn't                                 serve traffic the service itself didn't                                 control its resources as in when a                                 request was running long it would run                                 for however long it had to maybe ten                                 seconds I think that's how much by                                 default Cassandra it's probably about                                                                                                          second try the data sex driver so yeah                                 so it was easy with                                                request it was easy to exhaust its Fred                                 polls and backup requests in their                                 request queues so what did we do to fix                                 it so first of all we added                                 probabilistic and asynchronous trimming                                 on delivery this was to deal with those                                 in boxes which brought us down initially                                 but we also went a bit further and we                                 decided to keep stoom stands for shorter                                 so but actually even if we because after                                 they delete the tombstone stays until it                                 expires so even though we delete data it                                 would still have an impact on compaction                                 until it's collected you need to keep a                                 tombstone because if you delete and the                                 data hasn't been replicated to all nodes                                 it will resurrect                                 tombstones like a marker for this has                                 been deleted and you keep it until you                                 make sure that your delete has been                                 propagated everywhere and the second bit                                 so the repair job this is something you                                 run a independently across the whole                                 cluster this makes sure that all the                                 rice were propagated this allows you                                 time on actual right because you can                                 just pry to a quorum and then you run a                                 repair job would make sure that this                                 right has been propagated to everybody                                 and this took so we decreased the grace                                 period so terms of state for shorter so                                 we had lower impact and we had frequent                                 more frequent repairs for that reason                                 because you need to repair more                                 frequently even your tombstone lives so                                 if you tube the lives for                                             should repair every                                                   forever eight days so what your                                 tombstone doesn't go away before you've                                 propagated the delete so we shorten that                                 period to favor smaller but more                                 frequent repairs this kind of made that                                 our trimming also much more effective we                                 also looked at the service and we use                                 something which i guess is referred to                                 as a bulkhead pattern so it was not it                                 it's effectively time outs but its                                 surface side so basically the service                                 controls how much it longs runs the                                 operation so it this way we for each                                 logical operations I deliver a message                                 read a read and read count read the                                 inbox we allocated a separate thread                                 pool for that operation and we guarded                                 it with a timeout so that if that                                 operation is running for too long we                                 basically cancel it regardless of the                                 time out on the client and on the                                 application we're all out circuit                                 breakers so i will go explain this in                                 more detail so is this fixed now is this                                 the end of all of that well more or less                                 we still have some problems with running                                 repairs successfully all the time and we                                 also seen things like a single button                                 owed by bad i don't mean dead because                                 that's relatively easy I mean slow so if                                 one which things is ok but it isn't                                 taking down the whole cluster we think                                 it's basically because the it is                                 basically replay of what we saw with our                                 system                                 basically the nodes the node response so                                 the other nodes wait for it but it                                 responds so slowly but they basically                                 exhaust their resources but it's not so                                 trivial to tweak and it's actually not                                 very easy to reproduce because it                                 theoretically shouldn't happen but we                                 saw it happen so we have an ongoing task                                 to inject latency and reproduce it to                                 actually figure out how to solve it but                                 what's important is that even if we                                 haven't solved all the problems with the                                 backend a problem of Cassandra order                                 service doesn't take the side down                                 because it's me has decent kind of                                 failure isolation and the bulk heading                                 pattern means but only the users whose                                 data lies on the problematic nodes are                                 actually affected so what do I mean by                                 this and how this is achieved well let's                                 say we have a                                                           nodes for some reason are in a bad state                                 so if you have so that's roughly                                 accounts for thirty percent of the data                                 so in the past that would effectively                                 mean because of faults in the service                                 design that took me that's still all                                 users would be affected now we actually                                 observe that if that happens if that can                                 happen but three notes for some reason                                 bad memory etc we run our own DC so we                                 deal with that and we have only thirty                                 percent of the users affected so the                                 ones whose data isn't good nose don't                                 see it because they're thanks to the                                 bulkhead they're isolated and the bucket                                 works the following way so we know                                 what's our peak traffic we know what's                                 the worst-case latency we're willing to                                 accept so we sighs the fred pohl in a                                 such a way but we have enough threads in                                 during the worst latency so even if some                                 requests are timing out it doesn't                                 exhaust the Fred bulb and there are                                 threads available for the other requests                                 roughly speaking so what worked well for                                 us with this rollout our methodology                                 proved to be quite successful I guess so                                 we have the integration test which                                 allowed us to ship to prod with                                 confidence we had data modification                                 model modifications which we took about                                 an hour two hours to implement and then                                 we two and a half hours later it was in                                 product                                 so that's pretty good I guess the                                 shuttle deploy and double dispatch gave                                 has a great feedback on design from the                                 actual traffic's migration gave us peak                                 performance feedback and existing                                 metrics and analytics were very informal                                 like give us loads of information to                                 inform our design choices and I don't                                 want that's actually I think that was                                 really important the easy to run                                 migration really allowed us to move from                                 one model to another very quickly and                                 this was not a problem effectively so                                 that's it so instead of trying to avoid                                 problems we kind of allowed ourselves to                                 quickly deal with them I guess this was                                 the the trade-off we did and that pretty                                 successful what we've learned however is                                 that even for big my organization                                 Yammer's I guess now                                                probably about a hundred engineers                                 introducing a new technology has a high                                 cost getting a working model in                                 production to Conley three months even                                 though the actual product last a bit                                 longer but this was due to dealing with                                 legacy integration but getting like the                                 technology out was relatively quick but                                 what takes a lot of time and you have to                                 commit your resources to it if you're a                                 serious organization is ironing out the                                 operations and that will take at least a                                 air because you need to understand your                                 system much in much more deeper detail                                 than just rather feature and if you like                                 if like yammering heaven s light would                                 you commit you really to have a much                                 deeper understanding than just throw up                                 your feature you'll have a lot of                                 firefighting and fixing and you also                                 have to train people because even if you                                 do it as we did in a cross-functional                                 team there was a person from every bit                                 of the stack including operations                                 engineer involved well we don't have too                                 many we have all the necessary resources                                 then you have to propagate the knowledge                                 across small operations engineers of                                 small services people etc etc and that                                 takes time as well and support helps and                                 they did help data facts we support                                 helped us a lot but above still holds so                                 if you don't have support you will be in                                 a worse situation I guess it kind of                                 echo                                 the stuff mentioned by a guy who gave a                                 talk today from etsy but let's try to                                 solve school problems using boring                                 technology rather than introduce                                 unnecessarily in this case we had a                                 reason so but any technology introduced                                 to your org will effectively mean this I                                 would argue yeah so thank you this time                                 it's for real so any questions
YouTube URL: https://www.youtube.com/watch?v=yS3BiBsS6K4


