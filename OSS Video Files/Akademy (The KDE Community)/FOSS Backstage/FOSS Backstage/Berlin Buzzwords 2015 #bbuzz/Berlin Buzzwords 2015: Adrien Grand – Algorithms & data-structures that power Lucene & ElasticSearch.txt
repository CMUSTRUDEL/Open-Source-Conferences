Title: Berlin Buzzwords 2015: Adrien Grand – Algorithms & data-structures that power Lucene & ElasticSearch
Publication date: 2015-06-02
Playlist: Berlin Buzzwords 2015 #bbuzz
Description: 
	When you want to make search fast, 80% of the job involves organizing your data so that it can be accessed with as little work as possible. This is the exact reason why Lucene is based on an inverted index. But there are some very interesting algorithms and data structures involved in that last 20% of the job. 

In this talk, you will gain insights into some internals of Lucene and ElasticSearch, and see how priority queues, finite state machines, bit twiddling hacks and several other algorithms and data structures help make them fast.

Read more:
https://2015.berlinbuzzwords.de/session/algorithms-and-data-structures-power-lucene-and-elasticsearch

About Adrien Grand:
https://2015.berlinbuzzwords.de/users/jpountz

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              okay so thank you so has it just said                               I'm working at elastic where I'm looking                               that I have the chance twerkin                               elasticsearch and listen as part of my                               job and today we are going to some time                               diving into some internals of Huson                               elasticsearch but instead of focusing on                               how loosen elasticsearch we are a very                               high level perspective we are doing to                                do we are going to do something which is                                very different which is that we are                                going to focus on four very specific                                features and try to understand the way                                that they work this four specific                                features that we are going to talk about                                are first how does routine execute                                conjunctions for instant if you've built                                an index and you want to search on                                document that contain both foo and bar                                how does it work then we are going to                                spend some time trying to understand how                                loosen execute queries which are based                                on regular expressions then we are going                                to spend some time understanding how                                loosen compresses the red dot values if                                you don't know what the values are don't                                worry I'm going to introduce it and                                finally we are going to spend some time                                explaining how elastic search                                giardiniera to cardinality aggregation                                works the cutting edge aggregation                                beings the aggregation that you can use                                if you want to compute counts of                                distinct elements in a set so let's                                start but before we start actually                                explaining how conjunctions work I need                                to explain how an inverted index works                                and so here it is so an inverted index                                is composed of three main parts on the                                one hand on the left you have the term                                dictionary this term dictionary is                                composed of all the terms that have been                                found in your content sorted in                                alphabetical order so for instance that                                case we have elastic index Gaussian shot                                then to every term in his term                                dictionary we are going to welcome to                                map these terms to two things first a                                document frequency which is the normal                                the number of documents that contain                                this term and then pushing list on the                                right which is the list of document IDs                                in sorted order for all document that                                contain this term so for instance you                                can read this inverted index as                                Bluestein is contained in                                five documents and these five documents                                identified by de cádiz -                                              fifty - okay this is how an inverted                                index works and then there are two basic                                operations that you can apply an                                inadvertent Dex so disadvantage index                                supports iterators and you could create                                in Terry iterator halls on the terms                                dictionary and of the posting list and                                the first operation which is spotted on                                this traitor is called next and next is                                just about reading the next element so                                for instance if you cure create an                                iterator on the term dictionary first                                it's going to be and positioned then you                                connect is going to go to elastic then                                you connect again it's going to index                                etc and you can do exactly the same with                                posting lists when you create an                                iterator its first going to be on                                positioned then you connect for instance                                in the case of elastic is going to be                                positioned on                                                        then there is a second operation which                                is spotted which we call advance advance                                is useful because it allows you to not                                consume all the terms which are your                                posting list or transitionary and to                                directly skip to the term a posting that                                you're interested in for instance in                                this term dictionary if you create an                                iterator and immediately call advanced                                on the term which is search it's going                                to jump to the first term which is                                greater than or equal to search in that                                case it would be shot okay                                and you can do exactly the same with                                posting list if you are positioned under                                kd                                                                 going to go to the further KD which is                                greater than or equal to                                                it would default                                                        almost completely stored on disk and the                                 reason how we managed to make it                                 efficient that the term dictionary is                                 going to have very tiny in memory index                                 which is going to contain prefixes of                                 the term of the terms which is going to                                 be used in order to do this skipping and                                 for the posting list as part of the NDC                                 encoding we store everything into blocks                                 and in front of if it's block we storm                                 escape list and the skip list allows you                                 to know things like the first dock ID                                 which is greater than                                                after a offset maybe                                                    is a way that we can manage to keep                                 efficiently as posting lists so now that                                 we know basics about inverted index we                                 can start to understand how we can                                 execute queries and the first way I'm                                 going to talk about this to you an                                 example is the term crane you want to                                 search unsealer term for instance let's                                 imagine that you want to search                                 everything that contain the term which                                 would be search you would call advanced                                 in the term dictionary on search and                                 then they are two k                                                     term that you reach is the one that we                                 are searching for which means that this                                 term it exists it's not going to be the                                 case for search so you know that this                                 term doesn't match any document but on                                 the other hand if you were calling                                 advanced or losing for instance you                                 would find a term this term dictionary                                 and then in order to find all the                                 matching documents everything that you                                 would need to do would be to create an                                 iterator on the posting list which is                                 associated to listen and to call next                                 and redock ad in order to find all the                                 matches all documents that contain this                                 term so this would be for a very simple                                 term query now let's see what happens                                 with conventions so we have three                                 posting list in that case and we would                                 like to figure out which documents are                                 contained in all this posting list this                                 is what we call conjunction and again we                                 have a need to operation that we can                                 execute one is next which is going to                                 move to the next document in the posting                                 list and the other one is advance which                                 is going to help us keep of our document                                 that we don't need in this posting list                                 the first thing that we are going to do                                 with this posting list is that we are                                 going to solve them by by cost and in                                 the case of posting list the cost is                                 simply the document frequency which is                                 the number of documents that match this                                 term and then we are going to use the                                 posting list which has the fewer                                 documents as a lead for the iteration                                 and we are only going to use user                                 posting list as checks in order to make                                 sure that document that we have found in                                 the lead iterator can also                                 we found in the other posting list so                                 let's go through a concrete example we                                 want to merge this posting list first we                                 are going to advance the iterator of the                                 first question is which is going to move                                 to document                                                           document                                                            other person list so we are going to go                                 to this other passing list and to call                                 advance document - the first one doesn't                                 have to come in - and as we said before                                 advance is going to move to the first                                 document which is greater than or equal                                 to two so in that case it would move to                                 searching so we know that the first                                 match for the conjunction can actually                                 not be less than                                                       first iterator that we use as a leader                                 and we tell that we tell it that it                                 should advance to document                                                                                                                       we cap we can keep going with the other                                 person list we go back to the second we                                 are going to notice that we are already                                 on                                                                    one and advance to                                                  match which checked all our posting list                                 and the all contain                                                                                                                          conjunction and then we just need to                                 keep this process going until we consume                                 posting list entirely so we are going to                                 connect on the first post in week again                                 which gives us                                                         second posting list which gives us                                       we know that                                                         match we need to go back to first                                 posting list and advanced                                            next match is                                                          to second passing list it's also                                 contains                                                                know that                                                           which is going to be matched by our                                 conjunction and when we call next again                                 on the first posting list leucine is                                 going to give us a dark ID which is                                 maximum value which we use as place                                 holder in order to know that the posting                                 list has been exhausted and since since                                 we don't have any more documents in this                                 first person is we know that there can't                                 be any more matches for the conjunction                                 so this is a way that loosing runs                                 consumption and we                                 Curly's algorithms a leapfrog so I hope                                 it was clear so now we are moving to a                                 signal algorithm which is even more                                 interesting in my opinion which is how                                 does routine execute regular expression                                 queries okay so we still have our                                 inverted index this time I omitted the                                 document frequencies we have terms on                                 the left and postings on the right so                                 again postings are the list of documents                                 document IDs that contain this term so                                 the challenge when it comes to running a                                 Korean regular expression is to find all                                 the terms that match this regular                                 expression and to merge the posting list                                 a knave way to execute such a career                                 would be to go over every term in the                                 term dictionary to evaluate direct the                                 regular expression against it and then                                 if it matches to merge the posting list                                 together when issue with this approach                                 is that is going to be very strong so                                 when I said very slow it's a bit of an                                 acceleration because if you compare it                                 to the knave way of doing it which would                                 be to run it on every term of every                                 document the benefit of having an                                 inverted index is that we are only going                                 to evaluate the regular expression once                                 per unit term okay so if you have one                                 term that appears millions of time                                 across your document collection we would                                 only evaluate the regular expression                                 once again its term but still it's very                                 slow for the reason that we still need                                 to evaluate the rigor expression against                                 every term and it's very frequent to                                 have millions or tens of millions of                                 unique terms in an inverted index so can                                 we do better for instance let's imagine                                 that this is a regular expression that                                 you would like to evaluate against your                                 index it matches everything that starts                                 with elastic with either an upper s                                 or lowers and actually regular                                 expressions internally when you want to                                 evaluate them they are going to be                                 translated into an automaton that can                                 match this content for instance if you                                 look at the beautiful we have a start                                 date on the left and then we have some                                 transitions                                 you know that a term can only match this                                 regular expression if you can be                                 evaluated against this automaton so                                 first you are going to read the first                                 character if it is an upper e it matches                                 then L a etc then come something                                 interesting which is that you have two                                 transitions at this stage where you can                                 read either an upper s all over s and                                 then you reach the final state which has                                 a loop which means that as soon as you                                 consume the elastic with an upper s or                                 low s we accept any content and would                                 still match the regular expression so                                 when you look at this regular expression                                 as human and still sing about the term                                 dictionary that we saw on the previous                                 slide you know that there is no purpose                                 to evaluate this regular expression                                 against every term because all the terms                                 that we are interested in are actually                                 going to start either with elastic with                                 an upper s or with elastic with the low                                 s okay and although other terms are not                                 going to match anyway and so the way it                                 works that Lucene has some very specific                                 logic which allows it to intersect a                                 term dictionary and an ultimatum if you                                 look at this automaton actually it could                                 be considered an infinite iterator of                                 our set of string the first one which is                                 going to match is elastic with an upper                                 s then you are going to have also fixes                                 of elastic with a number s then you are                                 going to have elastic with a lower s and                                 then you are going to have elastic sorry                                 every term that starts with elastic with                                 the low F so this return is interesting                                 because it's something that you can also                                 iterate on and so the logic that we just                                 saw when it comes to merging personally                                 posting lists using a leapfrog approach                                 we are going to reuse this logic in                                 order to intersect a term dictionary                                 with an ultimatum and again it works                                 because we support exactly the same                                 operation next which allows us to go to                                 the next term that is contain either in                                 the term dictionary or which matches its                                 automaton in advance which refers you to                                 skip efficiently and so this regular                                 expression that you would as human have                                 a very legit efficiently by first going                                 to elastic and                                 everything that start with something                                 different we are also going to be able                                 to evaluate to run this query this way                                 with Racine okay                                 and when it becomes interesting is that                                 ultimate turns are not something that                                 you can only generate from regular                                 expressions if you look at physicalism                                 you can also generate ultimate sense                                 from physique race and for instance if                                 you look at this query which would find                                 all document that contain the term which                                 is add an added distance of                                            which is es this particular physique or                                 e can be represented with this automaton                                 you can have a look you can check if you                                 want                                 but this automaton is going to accept                                 every term and just every term which is                                 at a distance of                                                        way that we see neurons fuzzy queries                                 and so fuzzy grades actually very close                                 to queries on regular expressions                                 because they are actually evaluated the                                 same way using animation as an                                 intermediate representation ok so that's                                 it for regular expressions now we are                                 going to go to our third algorithm which                                 is numeric values compression before I                                 start I need to introduce what the                                 values are might not be abuse and so if                                 you are using elastic search let's                                 imagine that you won't run an                                 aggregation which is going to compute                                 the average price of all your green                                 documents what I mean with green                                 documents is all documented match green                                 as a term the way it works at first we                                 are going to go to the inverting index                                 in order to figure out which the KDS                                 contain green as a term so for instance                                 in that case we can see that documents                                 one four and five match green and then                                 the daka diese are going to be reusable                                 in a different data structure that we                                 called fill data elastic search fill                                 data is nothing more than a column store                                 that is built into your index and for                                 instance in that case we are mapping                                 every duck ID to the value of a price                                 field and for instance we can see that                                 I D                                                                      us to enter the price it ran and then we                                 are going to reuse this information                                 which comes from the inverted index in                                 order to read the values of the price                                 for all the doc IDs that match Karina                                 and we are going to sum up these prices                                 divide it by the number of matches                                      that case and this is going to give us                                 the average price for all documents that                                 contain green this this is exactly the                                 way that elastic search would run an                                 average aggregation on your data okay                                 Sophie later is what we are interested                                 in and I feel you have two options when                                 it comes to storing fill data in less                                 exchange the first one is a default one                                 when this field data column no entered                                 store is going to be computed on demand                                 and loading into memory and I create                                 something that would not fake match and                                 this is a reason why as a                                             going to in for that it's going to be                                 stored in the index and it's going to                                 use a feature which is called dock                                 values in missing dock values in Lucian                                 are just about storing Phil later                                 so first computing it at the next time                                 and then storing it as part of the index                                 which means that in your index you are                                 going to have a column oriented store a                                 common origin view of hadiza stored in                                 the index and just a map that you read                                 directly from disk so you can leverage                                 as five same cache in the page in and                                 out of out of the Faxon cash depending                                 on how hard are called your data are and                                 also the fact that it is computed I had                                 indexing time is so interesting for us                                 because it means that we have more                                 opportunities for compression because we                                 know that we are only going to do this                                 operation once so we can potentially                                 spend more time trying to compress data                                 and we are going to see what kind of                                 cool things we can do one sec                                 so first not all data is really                                 compressible and by the way by default                                 we are going to use a default strategy                                 which is very simple for instance let's                                 imagine that we have the following                                 values for Vedic ad by default we are                                 only going to use simple Delta encoding                                 a bit backing the way it works that we                                 would go over all the values and compute                                 the minimum value and the maximum value                                 in that case it would be                                                                                                                        the maximum value and the difference                                 between Max and min would give us the                                 maximum Delta in our dataset                                 ok and this maximum Delta can be used in                                 a lot to figure out how many bits we                                 need to use per value in order to store                                 our values it happens that                                          stored on only                                                        all the values in our dog values can be                                 stored on                                                                are going to do first we are going to                                 encode the minimum value for all our                                 values in that case it would be                                       then we are going to encode all the                                 deltas with                                                       document                                                            minus                                                                 one we would encode                                                 would be                                                                between                                                            encoded on                                                             how we save space by default and then if                                 you want to read a value so this needs                                 to support random access ok                                 and so if you want to read the value for                                 document                                                                offset                                                                   you would read exactly                                                minimum value and this would give you                                 exactly the value that you provide it at                                 next time this is a default strategy                                 that we apply when it comes to storing                                 numeric fields in the same but not as                                 special cases and not all details I call                                 and sometimes you can make better                                 decisions for instance sometimes it                                 might happen that                                 you have a few values which are very                                 different for instance let's imagine                                 that you're stirring some kind of daruka                                 liza and for everything you just you're                                 stirring a zip code but you happen to                                 only store zip codes about a couple of                                 places so you have very different values                                 but only maybe                                                         that case we're going to use something                                 that we call table and coding in table                                 encoding is just about using one level                                 of indirection in your data so we are                                 going to something which is very similar                                 but instead of using the values as they                                 are we are first going to build a table                                 of unique values and this table can be                                 built by just computing a set of all                                 unique values and sorting them so for                                 instance that case we have as unique                                 values to for document                                                 document                                                                document for this would be our table of                                 unique values and then for every                                 document everything that we need to                                 store is not the value itself but its                                 index in the table of unique values and                                 what is interesting is that as you can                                 see we have exactly the same values a                                 bit as before but we only have a couple                                 of unique values we only have four of                                 them which means that we don't need                                 three bits anymore in order to store                                 information per document we only need                                 two bits because                                                         IDS in our table only need                                             stalled so compared to the previous                                 strategy which is a default we would                                 save                                                                  works when it comes to decoding it very                                 similar you would go to offset D times                                 to beat which is the number of bits per                                 value that you are using then you would                                 read two bits which would give you and                                 your index in the table and then                                 everything that you need to do is to                                 read the value at this offset in your                                 table this gives you the original value                                 so this is a second tragedy that we use                                 for numeric values and cunning and then                                 I would like to talk about the third one                                 which is even more interesting and it's                                 typically something which is going to be                                 useful when you stole time stamps                                 illusion elasticsearch                                 when issue is time stamps that is very                                 common to store them using                                 Michigan precision but yet from your                                 application perspective you might not be                                 interested in Michigan precision and                                 maybe are only interested in second or                                 daily precision which means that you                                 have a lot of beads and at the end of                                 your values which are going to be unique                                 and used and to use space in your index                                 for nursing and for this kind of data we                                 are using something which is one tracing                                 that we call DC decompression and the                                 way it works that we are going to try to                                 compute a common TVs or across all our                                 values and to use it in order to convert                                 them for instance let's assume that you                                 have the following values the first                                 thing we are going to do is to compute                                 the minimum value in that case it is e                                 the minimum value is reached for                                 document                                                                 to compute the deltas so we are going to                                 subtract the minimum value to all the                                 values that we have in the price field                                 and so for instance it would give                                        the command                                                           the common two etc so then we have this                                 list of deltas and we are going to try                                 to see if we can find a common divisor                                 across these values then two options is                                 that the common divisor is                                            have any actual common divisor and we                                 are going to fall back to the first rata                                 G that I was describing when we just use                                 bit packing in all to encode values but                                 then that can be something more                                 interesting happening and for instance                                 we can find a common divisor this would                                 be typically the case if you store                                 timestamps with second precision Lusine                                 would detect a common Divis of                                      since one second is                                                   okay and then in the header of your dock                                 values routine is going to store first                                 the minimum value then the common                                 divisor and then instead of using the                                 values directly it's going to only incur                                 the questions so for instance for value                                                                                                       and                                                                   going to use exactly the opposite                                 approach and first for the KD D so these                                 values are between                                                      encoded on three bits per value we are                                 going to use exactly what we did before                                 and and code values and exactly the                                 number of bits that we need                                 then a decoding time for the kdd we just                                 need to go to a set D times the number                                 of ID's that we used at the next time                                   in that case read                                                    going to give us a quotient then we can                                 multiply it again with the GCD that we                                 computed in the first phase add the                                 minimum value and that's it we just                                 recomputed the original value which has                                 been provided at the next time so this                                 is how loosin performs compression of                                 numeric values and finally I'd like to                                 talk about a force algorithm which is a                                 bit more sophisticated and which is                                 behind the way that the CalNet                                 irrigation works so the cardia                                 terra-tory is the cardinality                                 aggregation elasticsearch is just about                                 computing unique counts of distinct                                 elements so if you run a sequel query it                                 could be equivalent to running a count                                 distinct on some field on a table and a                                 nave approach to run this aggregation                                 would be to use a set a hash set for                                 instance to put all the values in there                                 the set by its nature is going to                                 duplicate all the values and then you'll                                 figure out how many unique values you                                 have you just need to compute the size                                 of this set this works and this even                                 gives you an accurate answer but one                                 first issue that you are going to                                 encounter is that if you want to count                                 tens of millions of unique values it                                 might require a lot of memory for                                 instance imagine that you have hundreds                                 of millions of unique values and each                                 value requires                                                           require more than one gigabyte of memory                                 ok which is a lot and this is ignoring                                 the overhead of the set data structure                                 that we use in order to store these                                 unique values and even worse if you are                                 working in a distributed environment                                 like elastic sub - it's even worse                                 because all these sets of unique values                                 need to be propagated propagating                                 propagated three to a single node which                                 is going to merge them in order to know                                 how many unique values you have not                                 prasada but for your entire debt data                                 set for your entire collection and so it                                 makes the problem even worse                                 so what can we do so the way that let's                                 search service this issue is by using                                 and it's really an interesting algorithm                                 which is called hyper log log plus plus                                 which is an improvement of another                                 algorithm which is called a hyper log                                 log and a hyper log log is what I'm                                 going to talk right now                                 hyper log log is about using information                                 is about using her she's in a lot of                                 computing accounts and it does it in a                                 very smart way which I'm going to                                 describe so why is it useful                                 it's useful because hyper log log only                                 uses a few kilobytes of memory even if                                 you want to evaluate cardinalities of                                 billions of elements it's fast and you                                 can perform lossless engines which is                                 very important in the case of a                                 distributed system which means that you                                 would get exactly the same result if you                                 ran a hyper log log on a single shard                                 that holds all your data or on                                          that each hold one-tenth of the data you                                 would get exactly the same result there                                 is no I crazy loves when you run this                                 algorithm in the distributed fashion                                 which is one of the major arguments were                                 why we decided to use it so before                                 talking about how it works let's talk                                 about flipping coins in order to give                                 you an ID height works let's imagine                                 that you are flipping a coin okay and                                 your goal is to make a run of head which                                 is as long as possible                                 okay the probability of going your run                                 of n heads in a row is                                                which means that if you want to get                                   head in a row the likelihood of reaching                                 that state is                                                        want to get                                                      probability to get to                                                  is                                                              interesting it's interesting because if                                 someone goes to a room tries to do how                                 many heads in a row is possible and then                                 comes back to you and tells you how many                                 runs it managed to do you can use this                                 information in order to know how much                                 time is spent flipping the coin for                                 instance if someone comes back to me and                                 tells me                                 I managed to only do three heads in a                                 row it probably means that it didn't try                                 for a very long time on the other hand                                 if someone comes back to me and tells me                                 hey I managed to run to do a run for                                 instant of                                                              means that you spent the day flipping                                 the coins in order to reach this result                                 which is very important because it means                                 that given the number of heads that you                                 can do in a row you can almost know how                                 long someone has spent flipping the coin                                 so this is exactly the keen side of this                                 experiment to power the length of the                                 round is proportional to the duration of                                 the coin flipping and we can reuse this                                 information a lot to count unique values                                 how does it work                                 let's imagine that we have a hash                                 function which for any value is going to                                 give us a hash on                                                     important property in the binary                                 representation of this hash once and                                 DeRosa are as likely thank you                                                                                                         something which is not very hard to                                 treat and you don't need to use a                                 cryptographic hash function something as                                 simple as member                                                      work in memory's actually what we are                                 using elasticsearch and then you have                                 this hash in binary representation and                                 you are going to count how many zeros                                 you have in the end for instant here we                                 have                                                                value of the register to                                                                                                                         of the register to                                                      only have                                                              to the register the register is going to                                 store the maximum number of zeros that                                 we can see at the end of a hash value                                 and actually the number of zeros that                                 you can see at the end of a hash is very                                 comparable to the number of head that                                 you can do in a row and so we are going                                 to reuse this inside that we have for                                 con flipping to Kearney artist                                 estimation and to power the length of                                 the run so the number of zeros that you                                 can count the maximum number of zeros                                 that you can have in the end                                 is proportional to cattle ADT of a field                                 okay but now this approach has an issue                                 okay                                 so first example let's imagine that                                 someone runs has a set of value computes                                 hashes on every value and said that the                                 maximum number of zeros that it could                                 find was                                    sorry it was five it probably means that                                 the number of unique values was about to                                 power                                                                    power log log works but not even there                                 is an issue because it's not that                                 unlikely that even if you have a single                                 value you can get unlucky and get a hash                                 value which in that case                                                 end and this case is bad because I only                                 had one value as an input but yet the                                 estimator computed to power                                                                                                                    wrong and there are nice tricks that you                                 can use in order to improve accuracy of                                 this hyper log logarithm and this                                 solution is to keep multiple controls so                                 for instance will we still have this bad                                 value okay we were still only collecting                                 one value but counting                                              instead of using only one counter we are                                 going to use several ones and the                                 contact use is going to be computed                                 depending on the first bits of the hash                                 in that case we decided to use four                                 counters so we only need to have a look                                 at the first two bit of the hash in                                 order to decide in which register the                                 number of zeros that we can compute in                                 the end should go for instance in that                                 case the last bits sorry the first bits                                 are                                                                   which would be ablated is register                                   then we have another value this one for                                 instance which is hacked at this value                                 at the two first bits are                                               means that the register that should be                                 updated it register                                                     the representation of                                                  so we are going to update we just                                 register                                                        essentially we are going to split a                                 stream of value into different counters                                 and we are going to get different                                 estimates then so for instance we reach                                 this for count this four registers                                                                                                            because again we gotten lucky and fell                                 upon a heart value which just happened                                 to have lot of zeros in                                 even if we only collected one one or two                                 of them so each register is going to                                 give us a calculate e so the first one                                 is going to give us                                                      been to you for third one eight and four                                 one two and as we said we had an                                 independent stream of values going to                                 each read register so the naive approach                                 would be to get overall cardinality to                                 sum up the calculate is for each                                 register which gives us                                             doesn't help with the issue that we had                                 which is that we just get unlucky on the                                 first value so what hypergolic does                                 instead is that is going to apply a                                 mnemonic mean on all the cardinalities                                 which have been computed and to multiply                                 it by the number of registers this                                 approach is interesting because the                                 harmonic mean has the property of giving                                 less weight to outliers and for instance                                 in that case we are multiplying                                         is a number of registers times harmonic                                 mean which is computed based on this                                 formula and using this approach we                                 compute another whole cardinality of                                                                                                      almost means that we disregarded the                                 value which was given by the first                                 counter again that we only got because                                 we were unlucky and this is how we can                                 improve the accuracy of the epilogue                                 lock algorithm there are other nice                                 tribute of these algorithms like we said                                 it needs very little memory in all                                 Trenholm and for instance each register                                 is only going to store a number of bits                                 in a longer and there are only                                         in a long which means that you can store                                 a register only                                                                                                              another important feature is that unions                                 are lossless like I said as an                                 introduction to this algorithm you can                                 run the same algorithm on one shot that                                 holds all your data on on                                             then merging the results and you're                                 going to give exactly the same value and                                 the reason is that the reason for that                                 is that the maximum of two set is a                                 maximum of the maximum                                 sets so when you compute a union of two                                 hyper log-log instances you don't have                                 to take the maximum of the more bits                                 that you could count in gender and it's                                 going to give you the number of bits                                 that could be counted on your overall                                 data set okay and this is a way that you                                 can merge hyper log-log instances that                                 have been built on different charts                                 together and going to give you an answer                                 for your whole data set and this is                                 perfect for this ribbet environments                                 such as elasticsearch                                 so this was just about hyper log log so                                 just for your information                                 yeah algorithm at elasticsearch is based                                 on it's a critical hyper log log press                                 press which is an improvement of a Bella                                 glug we try to correct even more bias of                                 hyper log log and to improve accuracy                                 especially when you have small                                 communities and if you'd like to hear                                 more about it there are papers which                                 have been published and that's                                 everything I wanted to say today before                                 before I stop I just want to say that if                                 you found his talk interesting I took a                                 bit about compression and ran out who is                                 here is going to have a deeper talk                                 about compression loosing tomorrow                                 afternoon and I talked about hyper log                                 log and there is actually an algorithm                                 which can be used for computing                                 percentiles which has very similar                                 features which is called to digest and                                 there is going to be a talk about today                                 just this afternoon by deadening                                 so if you found this interesting I would                                 recommend you to attend this talks so                                 that's it for me thank you very much                                 you
YouTube URL: https://www.youtube.com/watch?v=eQ-rXP-D80U


