Title: Berlin Buzzwords 2015: Stephan Ewen - Apache Flink deep-dive #bbuzz
Publication date: 2015-06-03
Playlist: Berlin Buzzwords 2015 #bbuzz
Description: 
	Apache Flink is a distributed engine for batch and streaming data analysis. Flink offers familiar programming APIs based on parallel collections that represent data streams and transformations and window definitions on these collections.

Flink supports these APIs with a robust execution backend. Both batch and streaming APIs are backed by the same execution engine that has true streaming capabilities, resulting in true real-time stream processing and latency reduction in many batch programs. 

Flink implements its own memory manager and custom data processing algorithms inside the JVM, which makes the system behave very robustly both in-memory and under memory pressure. Flink has iterative processing built-in, implementing native iteration operators that create dataflows with feedback. Finally, Flink contains its own cost-based optimizer, type extraction, and data serialization stack.

Read more:
https://2015.berlinbuzzwords.de/session/apache-flink-deep-dive

About Stephan Ewen:
https://2015.berlinbuzzwords.de/users/sewen

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              I                               thank you hello everybody my name is                               Stefan I'm                               out Apache chief link today basically                               been here a year ago talking about a                               project called stratosphere that was                               back then a project coming out of half                               of university research collaborative                               research here in the in the in the area                                of Berlin and it was just very very very                                recently last year that it had been                                accepted at the Apache Incubator as a                                project was um sorry I just to the                                button here yeah it was that it had been                                accepted as a as an Apache project into                                the incubator was still called                                stratosphere back then I think we we                                knew that we were renaming it to flink                                so now now a year later I'm actually                                here to to show you what what what this                                has become what actually fling has                                become within a year um a little about                                myself maybe I think the by when the                                program is wrong we just accidentally                                forward copied the the one from last                                year so the the core the core team                                behind flink that I'm net that started                                this this effort giving it to the Apache                                Incubator and so honest it's no longer                                with the university even though it says                                it in the program um a bunch of us                                actually started a company here in                                Berlin courted artisans and where yeah                                we're we're developing flink we are                                supporting people who who want to use it                                okay um let me let me try and calibrate                                a little bit to the                                                    may have heard her may have not heard                                about fling before so I can I can't                                briefly ask you who of you has actually                                heard of link before this talk but it's                                a good bunch who of you has actually                                tried it out in terms of downloaded it                                and try to try to write write a few jobs                                that's actually a few hands at least I                                was actually cool to see okay to get a                                little bit the notion who have used                                familiar with storm spark                                all right I don't ask what my produce I                                assume that this is I see a lot of hands                                there okay that's actually good so let                                me try and use this to put things into                                perspective a little bit okay so um what                                what would you can see here is that                                since the last last time we were here at                                bill in buzz words we started out with                                the system called called stratosphere it                                was a pretty pretty small thing it had a                                bit of a rant I'm core a betch API and a                                few modes to run your jobs and within                                this one year actually the the fling                                community has grown this                                           quite a fat stack which can which can do                                do a few very interesting things and um                                I'd like to introduce you to those and                                in this talk so um first of all in the                                heart what is fling fling is a it's a                                net it's an entire like software stack                                but the very heart of link is a                                streaming data flow engine an engine                                that that thing's of programs in terms                                of operators and data streams that                                connect them it is in some sense like                                like a storm topology only that these                                operators may be extremely rich and that                                the that the way of communication and                                thinking about state and snapshotting                                and so on works differently but yeah I                                mean the notion of having a data flow is                                a bit like a streaming streaming                                topology this streaming data flow engine                                can present itself actually as a as a                                batch processor and as a stream                                processor both at the same boat at the                                at the same time and I mean it seems                                natural that the streaming data flow can                                present itself as a stream processors as                                a batch processor I'm going to talk a                                little bit about those those details                                later a streaming engine turns out to be                                actually quite a good fit for fit for                                for a batch processor and while this may                                initially sound a little esoteric for                                those of you that have read a little bit                                into let's say the the the blog posts                                from yeah from Martin clapman or Jay                                crafts well they might be familiar with                                the term cup architecture was a which is                                exactly that concept                                 running batch programs as special cases                                 of swimming programs so this is sort of                                 what the concept that fling follows here                                 as well I'll share a few details in a                                 bit so on top of these these AP is the                                 bed api we call that the data set API                                 thinking of a set as a yeah as a fixed                                 size data set distributed on the data                                 stream API speaking of data streams as                                 infinite streams of data on top of this                                 data set the badge API we've we've added                                 a couple of libraries so there's support                                 to to run Hadoop you had to your dupe                                 functions map functions reduce functions                                 input formats or performance there's                                 something called it the table API which                                 is a way of exposing these data sets in                                 in a more logical way so not not with a                                 physical schema in the sense of there's                                 a class or at a pole or so in Java or                                 Scala that defines what this is but                                 something like a sequel schema that                                 defines the data that goes that goes                                 through there a graph api called jelly                                 and machine learning library called yeah                                 we haven't come up with a very good                                 neighbor right now it's just called ml                                 at this point in time we're still open                                 to good suggestions there and there's                                 this quite a bit of work in progress so                                 this there's an adapter to run where                                 Google Cloud dataflow on on fling kits                                 it's google's hosted service so if you                                 want to run it um if you want to run it                                 on your own data infrastructure you need                                 you need another runner and flink is one                                 of the candidates there I'm RQL this                                 another Apache project for I think of a                                 very very principled functional our API                                 based on list comprehensions and kiss                                 skating I would assume a lot of your                                 only heard of cascading is was one of                                 the first um first tools on top of on                                 top of MapReduce to build more elaborate                                 programs so there's also work on running                                 his kidding at above link on the other                                 side of data streams we have                                 interestingly even though the runtime is                                 streaming we've spent initially a lot of                                 work on the bedside because that seemed                                 to be what people were familiar with                                 what they were using they were looking                                 at and we've recently started exposing                                 the the streaming capabilities of link a                                 lot more with this data stream API                                 and started putting a few libraries on                                 top of that as well so this table API is                                 in I say in a restricted form also are                                 running on the data stream abstraction                                 some more as a very recent project that                                 that entered Apache but it's very very                                 interesting it's actually a machine                                 learning library on top of streaming so                                 it's um it thinks in terms of streams                                 with asynchronous feedback and tries to                                 do online learners are based on these                                 streams and yeah cloud dataflow also has                                 streaming streaming component and we're                                 porting this actually to to the data                                 stream API as well so with these                                 abstractions sitting on top of on top of                                 the fling stack there's a oh yeah a                                 couple of ways to run this I'm in your                                 infrastructure so you can you can always                                 run it locally in your IDE for debugging                                 you can run it where we call remote is                                 we should probably call the spare metal                                 also it is if you just run the flink                                 demon on every machine it can run it                                 there you can run it through yarn you                                 can run it just embed it in in other                                 java applications where it uses a very                                 very lightweight non-parallel way of                                 executing programs and and what is very                                 interesting is that actually tells is on                                 this on this slide here apache tez is a                                 project that started out as um as part                                 of the of the initiative to to make high                                 faster i think it's mainly use actually                                 as the distributed scheduling and                                 communication back end of the newer                                 versions of yeah hi fountas and the the                                 architecture of link is layered in such                                 a way that we can swap a lot of                                 components at individual parts so we can                                 actually say that we execute a streaming                                 data flow if these streams happen to be                                 file streams or so so they're actually                                 not infinite streams but finite streams                                 they happen to represent batch programs                                 we can actually run those on on this tez                                 engine which which is yeah which is for                                 it's pretty good for a fuse cases                                 because it implements them very very                                 neat characteristics with yarn                                 elasticity and auto scaling and so on ok                                 but this talks about the core of link                                 so um in order to understand why why we                                 push the design of the architecture of                                 link the way the way it is we we should                                 have a look at what we're actually the                                 use cases that were that we were trying                                 to solve with flink and the set of use                                 cases has become extremely diverse in                                 that field so we're we're definitely not                                 looking at any any use cases from the                                 area of the same or transactional                                 processing the field where you would use                                 no sequel style databases HBase MongoDB                                 Cassandra so no flink is purely in the                                 analytical side but the analytical side                                 in itself has become very very diverse                                 and here here are four for use cases                                 that I think sort of categorized a                                 pretty broad spectrum of things that we                                 were trying to do trying to run                                 streaming topologies real-time streaming                                 topologies with with complex operations                                 and low low latency complex operations                                 in terms of um being able to flexibly                                 window with the events dreams and and                                 detect patterns on the event streams at                                 the same time of course match pipelines                                 are probably the most the most                                 established use case of those                                 large-scale machine learning is becoming                                 increasingly interesting and people are                                 still are still showing quite an                                 interesting graph analysis as well so                                 we're trying to to look okay but how can                                 you build an engine that is that is a                                 good match for these diverse use cases                                 and in the terms of that that the engine                                 really has primitives to execute the                                 operations that are are needed for these                                 for these use cases natively natively in                                 the in the sense of that we don't try to                                 work around certain characteristics and                                 buildings around that sort of yeah sort                                 of mitigate these effects but have have                                 proper primitives in there so in order                                 to make this a bit more concrete what                                 does what is a native support mean have                                 two examples of what what is not native                                 this may actually have to explain it so                                 if you look at machine learning in in                                 this field one of the first things that                                 that people did                                 was yeah mapreduce came out it was was                                 great and in the way it gave you a very                                 very nice and easy way of our scaling                                 computation and in order to run the                                    to do the patterns that a lot of the                                 machine learning algorithms have the                                 iterative computations where you're                                 repeatedly loop of the training data in                                 order to refine the parameters the way                                 people were implementing this is they                                 were saying okay let me let me ask yet                                 me right one step as a MapReduce program                                 and then repeatedly execute this um so                                 in some sense you were just running                                 multiple jobs one after another and it                                 and it's very interesting in some sense                                 this space is to a large extent still                                 actually still at this model I think I                                 mean people have come up with better                                 things in terms of caching invariant                                 stuff in memory representing data                                 structures in a way they can be reused                                 across steps but for a lot of systems                                 that still pretty much what they're                                 doing and another another use case is if                                 you want to do streaming and what you                                 have is a batch processor you can you                                 can do a lot of very very small batch                                 jobs right you can discretize the stream                                 you can you can chunk it up and to into                                 little parts and then run run a batch                                 part for each for each of those and how                                 are large these parts are sort of                                 depends on what you want to do right                                 there can be daily they can be now they                                 can be a second um this does work for                                 for a good set of use cases it is put                                 restrictions on on what you can do                                 across these these mini these matches                                 these yeah stream dis chris discretized                                 units so when whenever you you have                                 something that's not a that's not an                                 easy aggregation that you carry across                                 this is this may be a bit tricky to work                                 with so what we're looking at what is it                                 what is a good abstraction actually to                                 to work with these use cases and and and                                 that sort of gave is the the corner                                 points for for the design of link and                                 well what those are not all of them I                                 think those are the four most important                                 ones                                 that have sort of surfaced over the over                                 the last year and not number one is I've                                 already hinted at that is the big at the                                 beginning is well actually execute                                 execute things are streams the batch                                 paradigm made a very very strong move                                 into this into this field with MapReduce                                 but we're actually seeing that the                                 streaming code is sort of a sort of                                 coming back because done done the right                                 way it has it has a few interesting                                 advantages over the batch paradigm and                                 interestingly if subsumes it because you                                 can always think of patches a special                                 case of streaming the the second corner                                 point is um if you want to do a turret                                 of computation it's actually a good                                 thing to sort of have this in the system                                 in the runtime in the data flow allow                                 for a certain controlled way of feeding                                 back data because if if you're doing                                 this if you're actually making the                                 system aware of these iterations it can                                 do a few a few nice things and is also                                 not bound to to to certain barriers so                                 um as you are if you execute things in                                 in luck in super steps and synchronous                                 super steps in the the bulk synchronous                                 parallelism paradigm the third point and                                 this is this is probably more                                 interesting for streaming but we see it                                 also for graph analysis is to to to                                 really think about what to do with state                                 during computation the the paradigm of                                 the of the batch processing is has been                                 pretty much you have an input you have                                 an output the function produces its                                 output based on the input this no state                                 they concern themselves with this is                                 good for a lot of cases statelessness                                 makes things extremely easy to reason                                 about easy to paralyze and so on um for                                 certain operations having a way of of                                 having state is extremely beneficial and                                 there are ways of actually abstracting                                 states such that it isn't break that                                 paradigm in some cases it does in some                                 cases you also have to worry them                                 exactly what how does this affect my                                 fault tolerance and we're going to talk                                 about that in a bit and the fourth point                                 is something that has come up also in                                 the the last year's more and more um it                                 seems the Java ecosystem is really                                 dominating                                 this this field at least in the in the                                 open source the open source space with a                                 few exceptions and if you want to do                                 something data intensive just relying on                                 the garbage collector to handle all the                                 data is it's not always working out the                                 way you would want it to work out so at                                 some point you have to start worrying                                 about how to use memory if you're going                                 to get intensive and you can do this in                                 the JVM more and more projects or doing                                 this it sits in the very core of link i                                 think the spark patrick is starting to                                 pick this up as well projects in c++                                 like and Paula are doing it anyways                                 because you have to I think drill is                                 doing this quite quite enough quite                                 sophisticated way so yeah it's one of                                 our corner points as well okay um I'll                                 show you the the stack a little bit                                 before you remember the libraries on top                                 of the badge and the streaming API and                                 then the runtime if you write a program                                 against such a system what what is what                                 does it look like what how does the the                                 execution the life size of such a                                 program behave so here's here's an                                 example of such a program it's it's a                                 toy program it's a very naive way of                                 computing transitive closure but yeah                                 you're just for the sake of example um                                 what flink does to every program is it                                 transforms it into a parallel data flow                                 it does that in an interface that we                                 call pre-flight which runs on on the                                 flink line side so you're running a set                                 of server processes in your cluster our                                 master process worker processes and                                 there's a client process when you when                                 you give your program to fling so what                                 it is it takes this Java or scholar                                 program and it goes through it a bit it                                 builds a data flow graph and it sends it                                 through these two components type                                 extraction and optimizer tab extraction                                 is it's just a way to say okay we want                                 to know at every function exactly what                                 is the data types that go in and out                                 because we can use that to to our to                                 generate schema to generate serializers                                 and compare this to do some some                                 checking that you know if you happen to                                 manage to to use some casts and cheat                                 around the type and Friends of the                                 compile                                 to figure out that this doesn't crash at                                 runtime and and also if we can pre                                 generate this utilizes we can we can do                                 our are managed memory operations much                                 much better a bit more about this in a                                 few set and a few minutes another part                                 that's I think freely interesting is the                                 is the optimizer that we added to the                                 system which which looks at these                                 programs and and and determines a few                                 things where you like join algorithms                                 where to where to catch data in the loop                                 invariant way the result of all that is                                 this streaming data flow which is then                                 given to the master and the master                                 starts deploying these operators into                                 the cluster they make handshakes on the                                 streams that they produce and consume                                 and the master checks yeah the the                                 dataflow topology and the metadata so                                 let me let me talk to you about the                                 remainder this is very very high level                                 very coarse-grained the remainder based                                 on on on use case by use case yeah so                                 the first use case I would like to talk                                 about this is data streaming analysis                                 because that if you started with a                                 streaming engine that's a that's but                                 that's sort of what what naturally comes                                 and to give a bit of a of a context and                                 and how we always see this this field                                 and where we see where wavelength fits                                 in um I've come to think of of streaming                                 infrastructure is in in the way that                                 that the companies use it of how they                                 usually have these three parts they have                                 a part that that gathers data that yeah                                 produces the initial streams these are                                 adapters to database logs to two sensors                                 to just to have a logs and so on and                                 then there's a second component it's                                 often called the broker where I think                                 Apache Kafka's is sort of becoming the                                 technology of choice in this area it's a                                 combination of a it's a it's a message                                 queue slash log that yeah that is a just                                 a very good system for for gathering                                 data buffering it                                 making it available for multiple                                 consumers in a reliable way and then                                 there's the third part which is taking                                 these dreams consuming them and                                 enriching them computing things over                                 these streams and this may naturally                                 feed back into the program and be                                 available available for further                                 consumption so for for streaming let me                                 let me actually skip this slide and into                                 it to it figure by figure um one of the                                 one of the core parts of link is that                                 really everything is executed in a                                 streaming way so whenever you have a                                 shuffle or or yeah and an alter all                                 connection between operators in the                                 graph travels and broadcasts the data                                 communication is actually can actually                                 be streamed it is in many cases so that                                 means that when elements come in here                                 and they pass the one Operator pass to                                 the next this let's say this computes a                                 hash a hash code to determine to which                                 target is ended and it sends it over to                                 this to this machine there's there's no                                 synchronization boundaries where this                                 really has to has to stop there's a                                 there's a bit of buffering with a with                                 an upper latency bound if the buffers                                 are not full by that time they're gonna                                 get flushed in order to guarantee that                                 the that we do not want infinitely keep                                 infinitely keep data on and hold it back                                 so so there is one thing that yeah this                                 is um you find the same thing in storm I                                 would say or in yeah and some somewhere                                 the connections are implemented as Kafka                                 cues but they make sense if you want to                                 go for low latency and another thing is                                 that we spent a lot of time and we're                                 still spending a lot of time and in                                 trying to to see what what are the what                                 are the api's that you need to to                                 flexibly define define the streaming                                 workflows and the the current state that                                 that we have in in the data stream API                                 on fling is as long as you can see here                                 this is the word count example I                                 apologize for that you've probably seen                                 it a million times is this a slight                                 variation of it so this is you know this                                 is where data showed you again here's                                 the the best variant the data set                                 variant is the data stream very entered                                 has a lot differences and it has this                                 this                                 version of defining windows of a certain                                 length and a certain trigger interval                                 this is just an example here so                                 windowing in flink in the current and                                 the current windowing implementation are                                 defined exactly like this you have a you                                 define how long are elements staying in                                 Windows and how often are these windows                                 evaluated so with these two policies you                                 can sort of quite flexibly define what                                 you can do you can use counts you can                                 use time you can use some user-defined                                 characteristics although as the the more                                 the more in transparent that gets to the                                 system it becomes yeah it becomes a                                 little a little trickier to define but                                 in in general de the this syntax is a                                 bit like that so we try to follow this                                 this this fluent functional better                                 syntax md and the windowing mechanism                                 the way it it's being it's still under                                 heavy evolution right now but it has it                                 has evolved into something that is less                                 that's quite interesting and flexible so                                 some an interesting example that one of                                 the committee's likes to give us you can                                 actually use this windowing system to                                 define to define and analysis where you                                 say if I have a trace of GPS coordinate                                 second I can use this to say every time                                 this thing moved more than a mile give                                 me the average speed over the last five                                 minutes or so so this this is up as                                 possible with that okay um yeah this is                                 just a follow-up here's a little longer                                 coat excerpt from an example that                                 implements a pipeline of processing of                                 stock prices and and is actually                                 monitoring for price variations of more                                 than more than five percent and yeah you                                 can you can basically see how it works I                                 mean these AP is or I think what people                                 nowadays are are used to and we try to                                 follow this familiar pattern on the on                                 the streaming site again you can see the                                 window definitions here operations like                                 mapping over a window you can group by                                 by Fiat names of the case classes and so                                 on so this should give you a pretty                                 pretty high level on our way of doing of                                 doing things                                 something that that we're actually kind                                 of proud of the way we we are we figured                                 out how to do this is how we do the the                                 the fault tolerance and the                                 checkpointing because we were aiming for                                 for guarantees for exactly once                                 guarantees in such a system without the                                 necessity to chop the stream up into                                 many batches and and the way that we                                 that we realized this in the end was                                 actually implementing a variant of the                                 Chandi lampard algorithm on top of the                                 streaming system um it's up it's not                                 important that you're familiar with this                                 algorithm it's it's actually fairly                                 fairly old algorithm like a lot of the                                 fundamental algorithms in this field                                 getting a bit of renaissance right now                                 I'm with our with systems also like to                                 see blood GraphLab I'm building on top                                 of that and so on and the the basic idea                                 of of this algorithm on top of the                                 streaming topology is the following we                                 have we have data streams here and                                 periodically we start pushing so-called                                 barriers to these streams and these                                 barriers logically chopped the stream                                 into into different generations a pre                                 checkpoint in a post checkpoint                                 generation and this barriers just it                                 slides through the streaming topology it                                 can slide through each stream at its own                                 speed when M it flushes the stream and                                 pushes elements before it and whenever                                 it reaches an operator it triggers a                                 checkpoint of the state and that                                 operator that checkpoint can actually be                                 written asynchronously the when when as                                 soon as the as soon as the s the rioting                                 starts the barrier can can can leave                                 that operator and flow on and as soon as                                 they have reached the end and the                                 sources acknowledge that they've                                 received all of these all of these                                 barriers that slide through and all                                 operators acknowledge that they're done                                 with their with a checkpoint you know                                 that actually this part is persistent                                 and the interesting thing is why these                                 barriers are sliding through with the                                 operations can actually continue                                 continue working so you start emitting                                 this and you start pushing it through                                 and data immediately comes comes after                                 that it requires a little bit of finesse                                 how you how you do the state handling                                 and make sure that while you're writing                                 you're not you're not overriding what                                 you're doing in your data structures but                                 these are things you can                                 yeah these are these are things where                                 data so I just exists that figure this                                 out so this is a very um yet it can give                                 you sort of it can give you the benefit                                 of mini batching in terms of exactly                                 once without without interrupting the                                 streams this is this is basically what                                 comes out of that okay um so much about                                 streaming I have a few minutes that's                                 good so I can talk about batch pipelines                                 so um when we talk about batch pipelines                                 this is um this is just running one                                 example it's a visualization from from a                                 batch program since flink things in of                                 everything data flows there's it's sort                                 of a against a natural thing to to just                                 render these data flows as as such                                 graphs this is basically this is using                                 the d                                                                JavaScript to take a fling program and                                 render it as a graph so these things                                 your data sources these are our                                 functions like maps map will reduce                                 there are binary functions like joins                                 unions and so on and they're this                                 program for example I think it computes                                 candidate flights based on on on                                 schedules from from various airlines um                                 if you run this on top of link and I                                 said it's a streaming data flow how what                                 is the is there anything you have to                                 look look out for a few if you execute                                 batch programs almost on a streaming                                 data flow engine what does that actually                                 mean executing batch flows on streaming                                 data flow so there are few dualities                                 between batch processing and stream                                 processing where you can actually view                                 batch processing as a special case of                                 off stream processing so if we think of                                 stream streaming programs as processing                                 infinite streams then a batch process a                                 batch program is a program that runs                                 over a finite stream and this this is                                 actually not very surprising right i                                 mean every time you write a MapReduce                                 program and you read a file from HDFS                                 usually don't get the file as a batch                                 copy into memory right you get a fine                                 stream so in some sets right running a                                 batch program as a streaming program is                                 just following this                                 paradigm conclusively to the end in                                 streaming programs you can and whatever                                 you do grouping you can never group the                                 entire stream because it's never going                                 to end there's never a point when when                                 you say okay I have all elements for                                 this group I can now do whatever                                 aggregation or reduce function I want to                                 run over that group so you have to                                 introduce windows in beds you don't have                                 to do that but you can think of it of                                 just having one window that spans                                 everything if you if you have this                                 notion in there then again this is just                                 a special case of a streaming program                                 with a very large window and streaming                                 programs need pipeline data exchange                                 always otherwise you're not staying                                 within your latency for for a batch                                 program you can also use pipeline data                                 exchange there's really no no                                 fundamental reason why not to do that                                 unless you have scheduling constraints                                 that lets say the producers of data                                 think mappers and the consumers of data                                 i think reducers cannot be brought                                 online at the same time in which case                                 you may want to fall back to a to a                                 blocking data exchange which you can                                 think of as a stream that is fully                                 buffered so I producer can just produce                                 stuff push it into the stream and the                                 stream has a buffer that is large enough                                 that it can just push all its data into                                 the stream go away and then the route                                 the consumer comes up and and drains                                 this stream so this is really internally                                 halfling things about these programs so                                 what does this mean if you run a batch                                 program it means that yeah you cannot                                 really read any of these texts there but                                 I hope you can it can sort of see there                                 see that the data flow graph structure                                 here which is important so just to tell                                 you their little boxes here which if you                                 could read them they would tell some                                 characteristics of the stream that's                                 flowing from here to here something like                                 okay this is just a one to one forward                                 stream this is a stream that is using a                                 hash petitioners to split itself up this                                 is a stream that is um yeah that is that                                 is yeah that is materialized as a                                 pipeline breakers or produce first then                                 consumed or it's a stream that is really                                 pipelined and in a lot of these programs                                 you can you can run a lot of them as                                 truly pipeline streams interestingly the                                 operators or                                 are often blocking if you have a sword                                 you do consume your entire input stream                                 then sort and then produce so the the                                 operators are blocking and your you're                                 not running everything at the same time                                 as you would with a lot of streaming                                 programs but mainly because of the                                 characteristics of the of the operations                                 running the operators and if you have                                 that it's actually good thing to exploit                                 this so if you run this on flink if you                                 throw out this program it's not going to                                 just bring up everything at the same                                 time and try to run it it's going to                                 start streaming the data and as soon as                                 it streams are available for the                                 successors the successes are brought up                                 and the predecessors are torn down as as                                 they are streams are produced and                                 consumed so it the net effect of                                 including this and scheduling is                                 actually that the execution doesn't                                 behave all that different then then a                                 batch execution with the exception that                                 the lifetime of the operators overlays                                 so if you if you look at the at the at                                 the breakdown of a flink a flink job                                 this is from the from the web front-end                                 after after has been executed it's                                 pretty short job actually one second or                                 so it's just an example on toy data but                                 it gives you an impression you can see a                                 lot of these things may start at the                                 same time if you have enough resources                                 they may run and even though this it                                 says drawing here and this has co group                                 even though this joint produces input to                                 this Co group the co group starts very                                 very soon after the join because as the                                 joint is producing data the co groups                                 already consuming it putting it into its                                 sort buffers starting to to sort the                                 first parts maybe even attempting to                                 start merges in the background and and                                 all of that it's a bit like when when                                 map produced introduced that the                                 reducers can come up early well then                                 while the mappers are still producing                                 things it's sort of similar like that                                 but in a more general paradigm all right                                 one thing that we learned this is really                                 important if you want to do such a thing                                 is managing the operators properly                                 because if these operators are up at the                                 same time there may be parts of this                                 coke running in the same JVMs disjoin                                 and if you don't make these operators                                 behave they're going to kill each other                                 they're just going to one                                 one is allocating its data structured                                 make some assumption the other one makes                                 a similar assumption and then they're                                 going to eat a wage others memory and                                 the jvm and just boom that's the way so                                 one one of the really important things                                 that we we learned you have to do for                                 this is actually manage memory and I                                 don't have time to go into this into                                 detail there's been a recent blog post                                 about that from from one of my fellow                                 yeah fling committers far beyond has                                 written an awesome blog post about this                                 so whoever's interested i recommend go                                 to the fling clock and check it out just                                 going to give you the very basics um                                 with fling test is at startup it                                 reserves a certain portion of the heap                                 by allocating byte arrays think of it as                                 as memory segments that are a memory                                 manager in an EOC program owns and that                                 that logically divides the heap in to                                 enter the data structures that the that                                 the workers need for themselves the                                 memory that we use for sorting hashing                                 caching data and then some remaining                                 heap space that is for for the user code                                 I think the races don't quite match we                                 leave a little more than they have to                                 the user code and it looks like that on                                 this figure but this is the conceptual                                 thing and then all operations place data                                 always in binary forms into the manage                                 memory buffers and try to walk their                                 work on this binary data as much as                                 possible so they really built the hash                                 tables in binary form the even sort                                 binary data to a large extent without                                 ever deserialising it into objects by by                                 means of of this special serialization                                 framework that we that we built for                                 flink and that that uses this type                                 information and extraction that runs in                                 the client that I showed you on on some                                 very early early arm slide while you                                 submit the program you look at the tabs                                 you pre generate the co lasers and you                                 pre generate utilities and analyze                                 classes to figure out okay how do                                 actually compare data in its binary                                 encoding for some data types it's                                 trivial for others you have to sort of                                 compose and change and coatings for                                 certain situations and then you can do                                 that and yeah the net effect is that                                 even if you run out of memory so it                                 gives you very robust behavior in memory                                 but also if memory runs out you get a                                 very graceful behavior because you can                                 just take bunches of these pages and                                 move them between memory and disk and                                 yeah especially this is for her for the                                 house join it behaves pretty gracefully                                 okay let me skip over this a little bit                                 and just give you one last yeah one last                                 thing before for my time runs out I                                 mentioned earlier on that I'll most                                 systems if you're running iterative jobs                                 actually doing that by submitting                                 multiple programs into the cluster I'm                                 the same thing again and again and try                                 and keep intermediate data in memory                                 what we've built in fling is a way of                                 having a feedback in there in the data                                 flow so you have a function that                                 encapsulate seeeeee this step that you                                 are repeatedly executing and rather than                                 deploying it again and again consuming                                 the previous deployments data you deploy                                 it once and you allow it to feed back                                 its results to the beginning um this has                                 the nice characteristic of first of all                                 you're only deploying this once you can                                 actually keep data structures around                                 across iterations it has because we                                 surface this in the API the nice                                 characteristic that the system knows                                 about that it can figure out for you                                 what is loop invariant and and cash that                                 in memory and yeah and with all that we                                 were actually able to to get some some                                 pretty good results on on on very heavy                                 algorithms in the machine learning space                                 so this is a slight you may have seen                                 this if you went to the talk from till                                 yesterday he was talking about machine                                 learning with flink this is um the                                 matrix factorization use case that they                                 were scared to some I think a pretty                                 good scheme actually on are not true                                 large heart whistled factorizing a                                 matrix of                                                           you're dealing with many terabytes of                                 intermediate data sets although the                                 input data is is only yeah it's a bit                                 below below a terabyte of input data the                                 algorithm has the notion of of blowing                                 up intermediate results vastly and doing                                 doing complex computations and and the                                 way of having this very well-behaved                                 feedback loop is very well-behaved                                 memory management actually allows you to                                 UM to scale this pretty far on a decent                                 on a not overly large cluster actually                                 and the next thing that you can actually                                 do if you do closed loop iterations is                                 and I was sitting at that earlier is you                                 can you can keep state across individual                                 invocations of of whatever you do inside                                 your generations I'm not going to going                                 to go into details this concept called                                 Delta iterations is is a way we try to                                 abstract mutable state in a way that it                                 doesn't break the functional abstraction                                 um the one net effect that you can get                                 off keeping state around let me only                                 give you that on before ever for a                                 rabbit up is that you can write                                 algorithms such that they work on                                 whatever I need to work on whatever work                                 needs to be done depending on the                                 changes in the previous step and you can                                 you can not look at the let's say                                 they're already converged parameters                                 that you that you can exclude because                                 they haven't changed for a while and                                 none of their of their training points                                 give you any new inside so why should                                 they change now you can write the                                 algorithm to ignore those points and and                                 selectively compute what is what you                                 discover as a new update in in a                                 subsequent iteration and just update                                 that into into the mutable state if you                                 go back to to that abstraction which you                                 have in most systems the result is                                 whatever comes out of the step function                                 so you have to implicitly carry forward                                 all all your results every time                                 otherwise they're going to be lost the                                 net effect of keeping status you can                                 just compute on what needs to change you                                 have some stage that actually holds                                 whatever your result should be and this                                 is going to be the result of your                                 computation so with that you can                                 actually implement a lot of graph                                 algorithms or machine learning                                 algorithms which have this structure of                                 being of having graphical dependencies                                 between their parameters in a in a in a                                 very efficient way so this is this is a                                 slot comparison if you go from Hadoop to                                 the Latina Eve way of iterating with                                 fling you get all these caching and                                 memory deployment benefits and so on and                                 if you then say okay let me switch to                                 the Delta model you can actually again                                 shrink this computation and you can see                                 that especially                                 if you if you do more iterations the                                 later the iteration is the less work it                                 usually does so if you you can you can                                 get to too much lower error rates which                                 is focusing on the parameters that                                 that's still that's still relevant                                 adding few durations doesn't cost all                                 that much in that model okay there was a                                 lot of contents thanks for for for                                 staying with me um I'm going to put up                                 this slide for the for the end it's what                                 what we're working on right now um                                 streaming is on a heavy development I                                 think it has some some very interesting                                 characteristics right now in the way it                                 it allows you to do low latency with                                 states with state with user-defined                                 state without ya without sacrificing the                                 the nice cities of a of a proper stream                                 processor we're working on master                                 failover e because in highly available                                 we don't have this in the we don't have                                 this in the system as of yet but it's                                 pretty crucial for highly available                                 streaming setups monitoring integration                                  with other apache projects and well of                                  course i like like usual the library is                                  actually the most active part static                                  most people because they have a they                                  have a comparatively low bearing of                                  entry if you want to work on something                                  and if you find this interesting and our                                  excited i want to learn more will                                  actually have a link conference called                                  fling forward later this year in October                                  here in Berlin actually in co                                           where buzzwords was yet last year so if                                  you're a frequent buzzwords guest you                                  will know this place it's um it's also                                  nice location and where we're putting                                  together the the program so if you are                                  playing around with fling if you're                                  working with that if you have prototypes                                  experiences suggestions for four                                  features and improvements that you want                                  to share the call for for participation                                  is still open so feel ya feel free to to                                  also yeah share your share your ideas                                  and results with us thank you                                  you
YouTube URL: https://www.youtube.com/watch?v=zHF4EoXf7Hk


