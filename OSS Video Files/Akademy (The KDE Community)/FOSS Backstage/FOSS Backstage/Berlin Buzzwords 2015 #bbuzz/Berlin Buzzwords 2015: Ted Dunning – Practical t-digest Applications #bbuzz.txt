Title: Berlin Buzzwords 2015: Ted Dunning â€“ Practical t-digest Applications #bbuzz
Publication date: 2015-06-02
Playlist: Berlin Buzzwords 2015 #bbuzz
Description: 
	The t-digest is a state-of-the-art algorithm for computing approximate quantiles with adjustable accuracy limits and very few limitations.

Implementations of t-digest algorithm are easy to use and have been integrated in all kinds of software from ElasticSearch to Apache Mahout. Certain kinds of queries such as finding the top 99.999th %-ile can be accelerated by several orders of magnitude by using t-digest.

I will describe the basic algorithm and demonstrate the effect of some variations of the algorithm. I will also show how to use the algorithm in your code or your queries.

Read more:
https://2015.berlinbuzzwords.de/session/practical-t-digest-applications

About Ted Dunning:
https://2015.berlinbuzzwords.de/users/ted-dunning

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              I'm gonna talk about T ty jest here                               which is a pretty geeky sort of talk                               I'm Ted Dunning I'm chief application                               architected map R but I'm also an Apache                               member and committer on many projects in                               this particular project that I'm talking                               today is completely open source so no                               need for the map R hat this is with                               Apache hat on so I'd also like to                                mention before I go this is appropriate                                for anomaly detection we have a book a                                small book that describes anomaly                                detection techniques that Ellen and I                                wrote we also have a book on time series                                for very high performance ingest for                                Internet of Things and most recently we                                have a book about real world Hadoop this                                whole series is about practical things                                that can be put into a small book that                                you might be able to use in a day or a                                week well let's talk about tea digest                                here so I want to talk first about                                quantiles and why we should care why we                                should do this online and then I'm going                                to talk a little bit about how to digest                                actually works and then how you can get                                it how you can use it so the first                                question is why do we need online                                algorithms and an online algorithm is                                one that always gives you the answer                                right now it takes data it has a finite                                amount of memory and you can get the                                answer from it at any given time well                                here's an example of why you need online                                algorithms this is from a very popular                                URL shortening site I won't say their                                name because it's probably too                                embarrassing but if you notice there are                                seven clicks today on ten different                                links seven clicks ten different links                                sixteen of the seven clicks came from                                buzzwords okay seven clicks ten links                                all nonzero apparently I think we have a                                problem from the beginning and then                                sixteen nine five and four                                so what's happening here is they're not                                using online algorithms so the counts                                are delayed by different amounts for                                different things and I can ask you do                                you believe them no because it's a silly                                will you ever believe what they say                                maybe in a year or so after they do                                perfect jobs for that full year you                                might believe them again when will their                                investors believe them never so this is                                a great example where they should have                                been using good online algorithms or at                                least delaying everything the same                                amount and getting decent out answers so                                that's one answer online gives you                                answers now hopefully good answers so                                the next question is why quantiles why                                percentiles well suppose we have a whole                                bunch of users and they talk to a whole                                bunch of websites and we would like to                                characterize how well the universe of                                websites is doing we do that by a high                                percentile response time the the average                                response time is very very uninformative                                because it's dominated by the average by                                most things what we want to find out                                about is what's bad things have happened                                with very low frequency say the                                       IEL latency but we have a hundred                                million people and say each one of those                                goes to a thousand websites per day so                                we've got billions of measurements per                                day we want to have online results for                                any kind of subsets like how did people                                in Kansas get results how about the                                people who complained yesterday but not                                the day before how can we compare what                                they saw what were the worst cases that                                they saw keeping that sort of thing in                                log files is a good idea but it's not                                going to give us instantaneous responses                                to this sort of problem suppose we have                                a cluster a single cluster a thousand                                machines every machine talks to every                                other machine with with remote procedure                                calls every machine has its own storage                                devices                                and we want to know all of the response                                time characteristics the entire spectrum                                of how fast things happens what                                percentage of the time well we also need                                of course to minimize the overhead be                                able to tandel every sample in no more                                than a few nanoseconds maybe a few                                 hundred nanoseconds and we want to only                                 use a few megabytes of memory on each                                 machine to do this we're not going to do                                 this with a log file we couldn't write                                 to the log file in                                                    couldn't even probably format the                                 message for a log file in that much time                                 so how are we going to get these answers                                 about what's working what's not well the                                 thing we need is high-end qui tiles the                                 the high end or the low end near zero                                 near one sort of quantiles those are the                                 things that we need to understand and be                                 able to measure in an online and                                 efficient way now of course the question                                 in any sort of thing like this when we                                 start going to these online algorithms                                 of certain quantities we cannot do them                                 exactly quantiles the top hitters the                                 the number of uniques all of these                                 things can only be approximated in                                 online algorithms and so the question is                                 how accurate do we want to be here's                                 some examples if we want to compete the                                 median which is the                                                      or minus half a percent so the answer we                                 get is somewhere between forty nine                                 point five percent aisle and the fifty                                 point fifth percentile that's probably                                 okay but the                                                             half percent makes no sense whatsoever                                 because you know we were talking about                                 something that's point O one away from                                 the end and yet we talk about errors                                 that are much larger than that so that                                 makes no sense so it makes a lot of                                 sense to have a                                                      high accuracy but then if we apply that                                 same accuracy to the median                                 we're probably over killing it                                 so we need variable accuracy loose                                 accuracy in the middle tight accuracy at                                 the ends in fact what we would like is                                 constant relative accuracy relative to                                 how far we are to either of the ends                                 very accurate at the ends less accurate                                 in the middle the tea digest does                                 exactly that and that is the one                                 characteristic that changes when we go                                 from other algorithms to approximate                                 quantiles                                 to the tea digest it has exactly this                                 property of variable accuracy and                                 constant relative accuracy now the way                                 it does this is it keeps something like                                 clusters in fact the original versions                                 of the algorithm were almost exactly a                                 k-means algorithm in one dimension with                                 the big difference that the size of the                                 cluster was allowed to be large in the                                 middle where Q equals about                                             to be small at the                                                      end now choosing exactly how that size                                 is is done we could choose it to say be                                 equal to Q times                                                      that near the ends the size of the                                 cluster gets smaller in direct                                 proportion to the distance to that end                                 of the scale that we're talking about                                 and that means our accuracy has exactly                                 that same relative accuracy property                                 that we want that that one idea of                                 restricting the size of the clusters                                 gives us all of the accuracy properties                                 that we want now originally we'd use                                 this form but it was pointed out to me                                 by a guy he's in the credits at the end                                 that we could do better by having                                 something that grows a bit faster                                 something that has the square root of Q                                 times Q                                                                 can do linear interpolation here is the                                 cumulative distribution of just a normal                                 distribution doesn't really matter but                                 you can see that if we have                                 these lines are close together at the                                 ends and further apart in the middle we                                 can do linear interpolation of this                                 cumulative distribution and we get                                 errors that are quadratic in the size of                                 the cluster that we have and since the                                 errors only are the curvature not the                                 linear part because they're quadratic in                                 that size we can therefore decrease the                                 the the power on the limit that gives us                                 a very important property that the size                                 of the tea digest is bounded for the                                 same accuracy no matter how much data                                 you give it it has a finite amount of                                 size and we do that by using I'm going                                 to go quickly through these parts and                                 then give some more examples these are                                 going to be for reference more there's                                 the the fundamental concept is that we                                 have a mapping from the Q space the                                 quantile to the centroid index this                                 mapping is nonlinear it's steep at the                                 ends so that we have small clusters at                                 the ends flat in the middle relatively                                 flat and we allow every cluster to have                                 a bound in case Kayle of at most one                                 that means that we can build this                                 algorithm with a very simple algorithm                                 what we do is we just order the points                                 and we collect points together as long                                 as that scaled version of the quantile                                 come on there we go as long as this                                 scaled size of the Quan tire of the                                 centroid of the cluster is less than                                   then we continue to merge things in once                                 it's too big then we commit that one                                 centroid and we start on the next one                                 and we keep accumulating points until                                 its size is too big we set it aside so                                 that's the algorithm we walk through                                 sorted data collecting them together in                                 that way in that scaled size so that                                 things that the ends are small                                 things in the middle are big that's sort                                 of scaling that's sort of merging gives                                 us this scaling we want it's a very                                 simple thing and what we can do is we                                 can have a short buffer for new points                                 when that fills up we sort it and merge                                 it with the old centroids we then fill                                 up the buffer sort it and merge it with                                 the old centroids because of the size                                 bound these can all be statically                                 allocated at the beginning of the                                 algorithm and there's no allocation in                                 the process we can improve it a little                                 bit by using an in-place merge that gets                                 rid of half of the space almost we can                                 use an approximate for that curve the                                 compression curve that will improve the                                 speed because we don't have a                                 trigonometric function and we can get                                 the cost per point down around or                                 possibly below a hundred nanoseconds per                                 point because we have no allocations all                                 of the code is straight line very very                                 simple sort of code this is really fast                                 this is really truly online we can get                                 any of the quantiles out that we'd like                                 we can do alerting on large values we                                 can do all of the things that we marked                                 as necessary early on and it's easy to                                 integrate you can use tea digest as an                                 aggregator directly in elastic search a                                 guy who was on this stage earlier adrian                                 ground integrated this into elastic                                 search you can use it from stream live                                 which is a collection of algorithms for                                 doing approximate counting and things                                 like that you can use it as a UDF for                                 drill that's not quite released yet but                                 it will be soon it's already in Apache                                 mahute and it's in maven central so the                                 API is also trivial you ask it for a tea                                 digest it's a data structure you feed it                                 points and you can ask for quantiles at                                 any time so the upshot is that we can                                 now build streaming applications to do                                 percentiles accurately and conveniently                                 and quickly                                 we can use it to do anomaly detection                                 what is higher than the                                               had a speaker earlier who was building a                                 maximum detector for instance she was                                 asked after the talk how do you decide                                 what maximum really is what if some                                 there's one sample that's very large we                                 possibly do to noise there's no good                                 answer other than ranked based                                 statistics you could say a maximum is                                 something that's larger then the                                      percentile of samples within a minute so                                 you could get that sort of thing there                                 and you can use this almost anywhere I'm                                 going to move to the questions quickly                                 but before I do here's a few credits                                 unmoral gave the idea of that Kate to Q                                 curve on during grand did the fastest                                 current implementation in a tree sort of                                 thing Hoss from the the the solar                                 community he has provided some API                                 improvements cam Davidson pretty nice                                 some very very interesting public                                 discussions of this and of course                                 there's one name missing here that's                                 your name you can contribute to this to                                 if you have any needs have any                                 suggestions or anything you'd like to                                 add to this
YouTube URL: https://www.youtube.com/watch?v=CR4-aVvjE6A


