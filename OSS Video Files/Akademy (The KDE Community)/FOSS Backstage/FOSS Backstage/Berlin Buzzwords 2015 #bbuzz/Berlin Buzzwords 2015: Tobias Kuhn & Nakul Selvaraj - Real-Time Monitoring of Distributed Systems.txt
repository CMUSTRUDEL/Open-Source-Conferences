Title: Berlin Buzzwords 2015: Tobias Kuhn & Nakul Selvaraj - Real-Time Monitoring of Distributed Systems
Publication date: 2015-06-04
Playlist: Berlin Buzzwords 2015 #bbuzz
Description: 
	Instrumentation has seen explosive adoption on the cloud in recent years. With the rise of micro-services we are now in an era where we measure the most trivial events in our systems. At Trademob, a mobile DSP with upwards of 125k requests per second across +700 instances, we generate and collect millions of time-series data points. Gaining key insights from this data has proven to be a huge challenge.

Outlier and Anomaly detection are two techniques that help us comprehend the behavior of our systems and allow us to take actionable decisions with little or no human intervention. Outlier Detection is the identification of misbehavior across multiple subsystems and/or aggregation layers on a machine level, whereas Anomaly Detection lets us identify issues by detecting deviations against normal behavior on a temporal level.

Read more:
https://2015.berlinbuzzwords.de/session/real-time-monitoring-distributed-systems

About Tobias Kuhn:
https://2015.berlinbuzzwords.de/users/tobias-kuhn

About Nakul Selvaraj:
https://2015.berlinbuzzwords.de/users/nakul-selvaraj

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              Oh                               so I'm Nicole and together with                               today we would like to talk to you about                               how we do monitoring of distributed                               systems at trade mob and just to give                               you a bit of context and trade mob is a                               Berlin based startup and we are a mobile                               app marketing platform we do things like                               user acquisition retargeting and we're                                also mobile only demand-side platform                                for in the real-time bidding ecosystem                                so I think a lot of people already heard                                about RTB systems a lot a bit earlier in                                one of the other talks this talk is                                going to be a two part talk and the                                first but I will talk about how the                                architecture and the engineering part of                                how we monitor a trade mob while so we                                will then talk about the science and the                                techniques behind them how we do these                                things as a DSP and we are used to                                handling about nearly from                                       requests a second to a hundred or                                hundred                                                                daily basis so this naturally means that                                the infrastructure costs that we have                                trade mob are fairly high so our                                infrastructure has to actually scale                                organically based on on our traffic as                                Etsy already popularized and measure                                anything and measure everything no                                matter what scale you are I think                                collecting metrics gives you visibility                                into what you're actually doing helps                                you increase your mean time to detect or                                respond to the issues that you are                                facing in production but at the scale                                that we are operating in it quickly                                became an issue for us we were                                collecting about                                                        or on average and are on                                      individual metrics per second at peak                                the number of dashboards to developers                                that we had was too much for us to deal                                with so when you can't really gain                                insights from your metrics you're                                collecting you no longer have control                                over the services that you're building                                we needed something a lot more smarter                                at this point of time a trending topic                                that was in the community of monitoring                                again as popularized by Etsy and Twitter                                was anomaly detection so with that okay                                now we have all this data and we can                                apply                                normally detection or outlier analysis                                on top of this data to actually gain                                insights to when things go wrong but we                                quickly realized that the number of                                false positives that we actually got                                with anomaly detection was way too much                                the number of alerts we were getting                                were insane our email was being                                bombarded by false positives all the                                time clearly this wasn't helping us so                                of course what this meant was anomaly                                detection was an extension to your                                toolkit and it was not just the solution                                itself we need to apply something                                additional to normal detection                                techniques such as convergent cross                                mapping or seasonality or trend                                decomposition to actually combine the                                signals from multiple algorithms and                                then collectively make decisions based                                on this but unfortunately there wasn't a                                system or a project that let us do this                                fairly easily so we decided we wanted to                                build something on our own and we had a                                set of requirements for that it had to                                be real-time or at least knurled                                real-time I have the Opera just because                                the doc said real-time monitoring of                                distributed systems but this is not                                possible because of various aggregation                                or policies that you have in terms of                                how you're collecting the metrics                                themselves now also we wanted an                                isolated point of view from different                                algorithms or techniques based on the on                                the metrics that we were collecting we                                wanted anomaly detection or seasonality                                to look at the same data points at                                different perspectives in an isolated                                manner so that we could gain signals                                from these and then use them naturally                                the reason for us to actually build this                                so that was for us to optimize our                                infrastructure costs so the system had                                 have a really low footprint in terms of                                 maintenance and operation costs and                                 finally it had to be extensible                                 extensible in terms of adding new                                 algorithms or new techniques and also                                 the data sources that we were actually                                 integrating with so we wanted to be able                                 to even measure and collect metrics from                                 the dashboards or our business KPIs or                                 others and this actually led us to build                                 animali as we call it                                 animala is a Python project and that                                 lets you collect metrics and then                                 analyze these bits multiple techniques                                 with dynamic which allows you to have                                 dynamic monitoring at the end of the day                                 the main focus here for us was to                                 abstract the complexity for developers                                 so that you could just plug in your                                 metrics and then have multiple                                 algorithms looking at this now before we                                 actually did this we had to deal with                                 another issue we had to scope the                                 metrics we are talking about distributed                                 systems here which means that we have a                                 large number of instances running across                                 in different groups or different                                 clusters and all of these instances or                                 services are actually sending two kinds                                 of metrics one is application metrics                                 which come from your services and the                                 second being the system metrics such as                                 CPU or disk or whatever now what do I                                 mean by scoping we want you to look at                                 these metrics not just at an individual                                 or an instance level but at different                                 levels of granularity we wanted to have                                 a micro and a macro view on the services                                 and the metrics that we were collecting                                 in terms of an instance level or a                                 cluster or any sort of logical grouping                                 at different levels so we need to build                                 a solution for this which means meant                                 that we had to hack certain paths to                                 give you a small example this is one of                                 a metric and that comes from one of our                                 services as you can see it has a number                                 of different tags including the host                                 name the region or the auto scaling                                 group or the availability zone and so on                                 now when such a metric is pushed by the                                 service we then multiplex it by                                 multiplexing it what we get is a count                                 number of possible combinations of that                                 individual metric based on the tag so                                 one metric when we actually receive at                                 animali it becomes multiple log that is                                 split at different levels of granularity                                 these are then aggregated based on                                 different policies with different                                 interval periods                                 so that when the data is actually                                 received at animali it's ready for the                                 algorithms to begin and start doing                                 their magic to give a short overview of                                 the architecture itself again this is                                 what animali looks like we have a number                                 of services sending metrics to our                                 multiplexer or aggregator layer where                                 each metric is multiplexed aggregated                                 and computed and then it is pushed to                                 the collector layer the collector layer                                 is a piping implementation on top of                                 libuv                                 that performs really well and it pushes                                 data to what we call a metric store                                 which in our case happens to be Redis                                 for sake of simplicity but you could                                 easily plug in any other key value store                                 based on your scenario                                 now once the metrics are in the metrics                                 store we have the task runner which is                                 basically a celery instance that is                                 running behind the scenes and                                 selectively runs different algorithms on                                 the data in the metric store and then                                 this triggers further data or events                                 which are then handled by event handlers                                 now in our case this event handlers                                 could be something as simple as sending                                 Orton alert or it could also be                                 something like killing an instance                                 because it is performing sub-optimally                                 so anomaly to summarize the architecture                                 it's just lets you selectively collect                                 metrics and then analyze this and                                 trigger events upon which you can then                                 react to leading developers to have a                                 hands-free approach on top of this now                                 Toby will talk about the algorithms                                 which we actually use to help in                                 production yeah thanks I cool um                                 I said I will going to present the                                 algorithmic part of anomaly and first of                                 all I would like to start with a short                                 introduction how anomalies could look                                 like in our case and what possibly ways                                 they are to solve those issues or to                                 detect those issues so the first example                                 is rather simple it's just showing the                                 CPU utilization of a single box which                                 suddenly peaks at                                      and because it's also quite easy to                                 detect if you just apply a static                                 threshold we can catch this and react on                                 that the second example also shows CPU                                 utilization in this case it looks rather                                 okay so it's quite stable over time but                                 it is not okay if we put this into                                 context                                 this means the green band what we see                                 here will come to this in a bit more                                 detail the next slides                                 define something like expected state of                                 this box in this particular service and                                 yeah the shown box obviously does not                                 fit into that so depending on the time                                 you can give define this also as an                                 outlier and the third example is a time                                 series plot of incoming system requests                                 a trade map we see three consecutive                                 days and I guess you all see also the                                 outages what we have here and well the                                 problem here is more of having an                                 algorithm which is capable of detecting                                 outages like this and if we all put this                                 together it leads to the point that                                 before we can do proper anomaly                                 detection we need idea of what we expect                                 from our system so we need a definition                                 of normality before we can charge                                 certain situations as okay or as false                                 in the end and in the following I would                                 like to go into two different directions                                 the first one is the behavior of the                                 distributed system so as mentioned we                                 are running up to a thousand boxes at                                 time and yeah the question is how two                                 single boxes within specific services do                                 behave and the approach we are using is                                 rather simple it's the so called tackies                                 outlier filter and again it's using                                 statistics so what we see here is a                                 distribution of different instances of                                 again CPU utilization I don't know if                                 you see it it's from                                                  and we see the most most of the boxes at                                 around                                                                   also some which are far below or above                                 those values and the question is                                 how can we chat so how can we get them                                 as outliers and one way of doing this is                                 applying quantize to that so what we see                                 here quantile                                                           quick reminder quantity                                                  off from the lower                                                      of the distribution and both values                                 together define the so-called inter                                 quartile range which is a quite robust                                 statistics containing the middle                                        the values and if we now take a constant                                 of that and add it to the quantized                                    and subtract it from the quantity                                       get a screen band which in the end                                 defines something like an healthy range                                 or as a stage of the system which we                                 accept and everything above or below we                                 would then define as an outlier this is                                 only a snapshot if you look at it over                                 time                                 it looks of course more like this so we                                 have again the evolving green band and                                 most of the instances are okay but                                 there's one box which is only using a                                 fraction of possible CPU and yet you can                                 say we are essentially losing money                                 there because we are not using what we                                 could use so if this dates persists for                                 a fixed period of time like one hour two                                 hours we can react on that by just                                 restarting the service or kill the box                                 in this case to sum this up the idea                                 here is that we want to identify                                 auto-scaling issues not only about CP                                 you can also think about memory leaks or                                 business values also so incoming                                 requests per box for instance of course                                 the approach has restrictions so in the                                 end what we always need is kind of a                                 help arrange so if you think about a                                 service group and all boxes have issues                                 you would still define something like an                                 interquartile range and you will find a                                 lot of boxes as healthy even though the                                 whole service has problems and to tackle                                 this it's not sufficient to just use                                 this of course so we also need to look                                 at the overall system performance                                 and yeah one last word to CPU usage you                                 can just think about taking the average                                 CPU usage of all the boxes within that                                 service and if this metric has issues                                 then well the whole service has issues                                 and not only a couple of boxes and yeah                                 also here there are several ways to go                                 on this problem and you approach what we                                 are using it's mostly the so-called                                 season a                                                               using the fact that most of our metrics                                 have this inherit seasonality which is                                 one day usually so what we see here are                                 seven consecutive days and yeah I guess                                 the pattern is quite obvious that those                                 are daily metrics and by the use of this                                 you can then apply this algorithm and                                 decompose this time series into two                                 parts so we have first the seasonal part                                 which is then more or less identical                                 over the days and we also have the trend                                 part it's also looking quite constant in                                 the end we see a slight increase and                                 that is for a good reason so if you look                                 into the days you see that this was                                 Saturday Sunday and we have a slight                                 increase of traffic and yet this is                                 reflected by this trend component how do                                 we use that another example to that so                                 incoming requests for two days again and                                 we now apply the algorithm and decompose                                 this time series and generate those two                                 time series seasonal and trend part and                                 you can just add them up now so you can                                 create a model of your incoming service                                 metric so that defines something like                                 the expected state and if you put this                                 together to the raw input and just take                                 the difference it's yeah quite obvious                                 that you get the distribution of error                                 or deviation and in this case with                                 simple thresholds you could detect those                                 outages unfortunately it doesn't look                                 that nice all the time so this final                                 error distribution                                 in reality looks more like this so we                                 have Peaks drops longer one stronger                                 ones and the question is of course when                                 do we want to react so the overall goal                                 is for sure that we have a very small                                 number of false positive but we want                                 also to detect as much as issues we have                                 so there are several ways and this is                                 actually a big part of the whole problem                                 how to judge those distributions and                                 just to name a few you can use                                 normalization for instance if you have a                                 broad range of your incoming metric you                                 can set minimum deviations you can use                                 static thresholds and the way we are                                 going for usually our dynamic thresholds                                 by the use of t digests which was i                                 guess some of you heard it yesterday                                 presented by deadening also so a quite                                 nice online clustering to estimate                                 quantize of your distribution and if                                 you've applied this or combination of                                 that then essentially we want to build                                 something like that so we have a flag                                 now which is just saying everything is                                 fine or we are in an arrow state and                                 yeah pending on the length of this arrow                                 state we then can take actions like                                 restarting services or boxes or                                 alert some responsible person social and                                 yeah that's it about animali last but                                 not least we decided that we want to                                 optimize this project so actually it's                                 already open sourced since today so you                                 will find that on github the whole                                 animali project and additionally also on                                 python implementation of t digest which                                 is heavily used within animali                                 and well that's it from my side thank                                 you very much
YouTube URL: https://www.youtube.com/watch?v=iSxlLW-CUNw


