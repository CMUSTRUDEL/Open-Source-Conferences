Title: Berlin Buzzwords 2015: Andre Lynum - Beyond Significant Terms #bbuzz
Publication date: 2015-06-03
Playlist: Berlin Buzzwords 2015 #bbuzz
Description: 
	In Comperio we work on projects that aim to learn from the documents and social activity published on the web. The challenge is to summarize recent activity, drawing together current events with past activity and finding new sources to learn from. We base our on ElasticSearch and its significant terms technology. 

In this presentation we show how we expand on the base functionality provided in ElasticSearch to focus on areas such as immediate trends, entity identification and topic building using additional techniques from Information Retrieval (IR) and Natural Language Processing (NLP).

Reda more:
https://2015.berlinbuzzwords.de/session/beyond-significant-terms

About Andre Lynum:
https://2015.berlinbuzzwords.de/users/andre-lynum

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              so I call this talk be honest                               maybe a bit ambitious title but                               it's about using elastic search for                               level analysis so first briefly about me                               and the company i work in compare you                               comparison a region consulting company                               specializing in search and related                               technologies it focuses and search                               recommendations analytics and other                                related technologies aside from that I'm                                a current                                                        language processing focusing in machine                                learning and statistical techniques and                                I work in compare our work on projects                                using elastic certainty for J among                                other things so looking a bit into the                                talk we're going to look at how search                                it's a great metaphor for making robust                                analysis we'd have it right a few                                sources and hold aggregations and                                especially the significant turf                                segregation gives us a real-time                                retrieval of key lexical statistics and                                considering time you know as time allows                                we're going to look at some examples of                                lexical analysis so this is perhaps not                                a new for anybody here but elastic                                search is a search engine free text                                search engine and it focuses on data                                aggregation and analytics and has kind                                of moved on from research roots into                                this kind of areas it's also a highly                                scalable kind of no SQL data store so                                what I'm going to talk about here is                                based on a current project I'm sad to                                say that I can't really say that much                                about it this is kind of a one of the                                screens and but it's an open source                                intelligence collection and analysis                                tool that is open source as in public                                information                                and the project places them rather tight                                constraints on how we solve problems the                                scope is rather ambitious and and we                                don't really have too much money to                                spend and also we had to handle some                                languages that are not really commonly                                available in in tools and also we have                                to handle some sources and document                                types that are also kind of a bit                                different from what you might normally a                                normal meet so just briefly gonna to                                introduce the setting this is a kind of                                detective pipeline it has RabbitMQ                                ingestion pipeline there's an enrichment                                step and it populates elastic search                                indexes in near forge a graph database                                there's a application layer which feeds                                an analysis front and then and and                                various external feeds so and in this                                context to this application significant                                terms and other aggregations                                elasticsearch are kind of fit perfectly                                with our philosophy of making kind of                                dump systems act intelligent so kind of                                you might want to call this the beauty                                of not having to be correct all the time                                actually and what does this mean well it                                means that you might want to simplify                                the system scope so what does that mean                                exactly you know can we predict more                                coarsely for example instead of                                predicting precise positions might we                                just kind of predict that something is                                there at all and if you're doing                                something over time window we might want                                to do it over lots of time innocent                                aggregate results and this kind of                                thinking brings in more relevant data                                and kind of adds robustness and it and                                it decreases the degrees of freedom in                                the system in the model                                if you're going to put it in the kind of                                machine learning statistical terms also                                another aspect is can be elicit from the                                users knowledge that we can use so this                                is going to time-honored old artificial                                intelligence technique goes back to                                Eliza and and I think it's a pretty much                                popular today big thing with the systems                                like Google is kind of an expert kind of                                shoving you very even to go based on                                where you bin so back to a significant                                term segregation I don't know if                                everyone is familiar with it but its                                presentation in elasticsearch                                documentation and in the presentation is                                going to find it a bit disappointing                                because going to present this as magic                                 it's like ten commonly commonly common                                 in common and and the details is kind of                                 glossed over a bit this is not really                                 very magic at all so it's mostly our                                 comparison the term frequencies you get                                 our background set of documents with                                 that you take the frequency of term vid                                 this is kind of like the dark green line                                 air you take your four grams of the                                 documents which is what you're gonna do                                 focus and that's kind of like the light                                 green line and any kind of positive                                 deviation as you see indicates of the                                 term is significant so so why do we come                                 here with this well person i like the                                 significant turns a creation quite a lot                                 it's and this comes from a background i                                 find it the kind of bear kind of lexical                                 signal is very powerful and if you hear                                 talks or get presented an LP it is you                                 get part of speech tag you get parsing                                 but often the kind of the bear lexical                                 signal the relationships in verse in the                                 context are in is kind of very hard to                                 beat for a lot of tasks if you're going                                 to produce system that behaves in robust                                 manner so it's also its kind of                                 relationship to search                                 which is kind of arguably the most                                 successful a traditional artificial                                 intelligence or information retrieval                                 application of all time so we retrieve a                                 large set of documents and we use an                                 algorithm to rank them and then we kind                                 of just present the top that kind of                                 most most sure about so we take the part                                 of result that we like the best                                 represent it the thing you're not sure                                 about it is going to put down below the                                 fold and and the youth doesn't get a                                 seat that's kind of nice space to be in                                 I think when you're making a system so                                 i'm going to talk briefly about using                                 significant returns on single documents                                 so elastic CEO doesn't really support                                 this they have come like a minimum                                 document count and you can go below it                                 but as it turns out it never kind of                                 does anything useful and that is because                                 basically wasn't is slide is the JLH                                 it's kind of the default scoring                                 algorithm used for significant terms                                 it's not really very well-documented if                                 you look at it it's the frequent fork is                                 the first equation different of                                 frequency so                                                        multiplied by the ratio or the                                 foreground and background so it turns                                 out it's kind of line your term it's                                 kind of pretty relevance with basically                                 boils down to this the rate of between                                 the square the foreground in the                                 background it turns out that the term                                 segregation kind of lies behind there so                                 it retrieves document frequencies and if                                 you have a single document that is one                                 and you end up with a inverse document                                 frequency of the background set which                                 means what you see from the significant                                 terms are gregation is actually a list                                 of the various storm in the background                                 said in your document to add insult to                                 injury it's awesome at alphabetically so                                 we kind of get all the rare terms of the                                 background set that begins with a so                                 often it doesn't isn't real very useful                                 so what we had to do is to create a new                                 score that will let us do significant                                 terms style AG scoring on a single                                 document                                 so we want to keep these that we want to                                 bring things the four things that are in                                 common but still are common enough                                 basically and much summarized that is                                 kind of like a Pareto style scoring this                                 kind of                                                                  this kind of curve here where we want is                                 juicy kind of informative it's not too                                 common not to be a common the scoring                                 used is the one you see here it's                                 basically a tf-idf in an additional                                 factor and and so the TF kind of it                                 brings the document in the IDF kind of                                 brings out common things and the middle                                 term is actually a bit kind of                                 situational because if you have a very                                 noisy data set this is kind of an in                                 domain IDF IDF which will kind of                                 suppress noise that is in the domain if                                 you're very clean data you might want to                                 bring for structure from the domains                                 then you use a document frequency                                 instead this time to works a lot better                                 than kind of just running this internal                                 single document it's also nice to                                 mention that in elasticsearch I can do                                 very kind of you can actually do the TF                                 IDF calculations in arbitrary terms as a                                 query so tf-idf is kind of the central                                 waiting term for a lot of relevant                                 scoring for text and it's kind of a                                 basic building blocks for a lot of                                 machinery that you might want to build                                 it's really nice to have it you can                                 actually use the term segregation                                 actively has all the things you need to                                 build an IDF score and then you can give                                 it three of the term frequency from the                                 term vectors and you can do that it is                                 in batches so everything you need is                                 there and it's very very fast it's a lot                                 faster than the original significant                                 term square actually so so these are the                                 building licks building blocks we use                                 and then we're going to see a bit odd                                 kind of some applications the first is                                 entered extraction and end of the                                 extraction is kind of coming from                                 academic standpoint and in practice it's                                 a pretty difficult problem so just a                                 brief introduction is basically finding                                 things in text and as you see this kind                                 of example there's you're not only                                 finding the things you're saying what it                                 is and where they are in the text and as                                 I said it was difficult but there aren't                                 very good solutions for English and                                 other major languages for us it's that's                                 kind of a not very comforting because we                                 have languages besides English that is                                 not really covered there's no training                                 data of this sort to build a machine                                 learning solution and also you need a                                 lot of kind of entities list guess it                                 tears public indexes or whatever these                                 these can be hard to obtain actually so                                 in addition we will kind of cheat a bit                                 doing our cheap as a person so instead                                 of just kind of pinpointing exactly                                 dissing what it is we're just going to                                 list them it's gonna list the top entity                                 see no text you're knotting and even                                 going to do what I going to indicate in                                 the figure is not going to say what it                                 is even it just can state as an entity                                 and that's it and ranked entities by                                 relevance and this gives us a lot of                                 flexibility in terms of cloudiest                                 experience what he used to because the                                 he just get to see the part of the                                 result is we kind of pretty confident                                 about and and if we even showed him to                                 him which which we do and and it's not                                 going to laugh that we didn't find                                 anything or that we you did something                                 wrong                                 oh yeah some air is there ok so the to                                 summarize the idea we're going to use                                 high recall heuristics to to scoop up as                                 many entities as possible along we had a                                 lot of other stuff actually and it's                                 going to collect potential entities with                                 a very wide net they hope to kind of                                 have hundred percent disposal and                                 written recall and then we're going to                                 score these entities with significant                                 terms and finally if you're actually                                 gonna we're going to use whatever kind                                 of cheap knowledge resources we got to                                 kind of accept the result we get so for                                 a kind of concrete example we're going                                 to scoop up a lot of capitalized terms                                 it's like a                                                            it's not a big piece of engineering                                 we're going to using different terms                                 based on IDF domain IDF I subscribe to                                 earlier so this is very noisy me only                                 this data it contains a lot of crap you                                 can hold these things a lot of kind of                                 sightings and all miners signatures and                                 ugly stuff which makes actually makes it                                 difficult and then we're gonna I say                                 reranking but we're going to pick up                                 entries from the dbpedia that are                                 actually relevant and and bring out                                 those so this is kind of how it looks                                 it's a bit small but so it takes the top                                 is is the text i'm gonna i highlighted                                 the virus results and the first line is                                 the                                                                     what i ate that i consider good it's                                 machine types technical technical staff                                 it's it's got a nickname and a name it's                                 also got two entities that are kind of                                 just capitalized springs in the                                 beginning of sentences which are not                                 useful and the blue ones in their                                 offerings in the middle of the ranking                                 the                                                                      the rankings which is which is mostly                                 annoying stuff but it's got to kind of                                 mrs. here it's got into which is common                                 which is common in in                                 in general domain and gets dragged down                                 by IDF there and gets not this actually                                 is not mailing list so it gets dragged                                 down but in the main IDF thankfully                                 these can be carried out from wikipedia                                 and and because this is software                                 entities software companies is produced                                 to get out and and along with red hat                                 enterprise linux at the top which is                                 also which is also kind of log in the                                 ranking and we can bring those out and                                 kind of save these from being forgotten                                 at the bottom on the ranking list                                 exactly help is that very common things                                 it's actually more likely to be listed                                 in such resources as dbpedia then the                                 most common things that we bring up                                 dbpedia and wikipedia something i should                                 be a bit careful about actually because                                 there's a lot of kind of odd structure                                 and collections of stuff if you start                                 just matching of things you'll get                                 you'll match up in there anything so it                                 tends to be a bit careful there and just                                 kind of pick out the domains that you                                 know very interesting you're doing                                 training keywords this is kind of our                                 key function out and applications it's                                 kind of discovering new topics of                                 interest that kind of having an getting                                 a lot of interest in our sources and                                 it's kind of like a short introduction                                 and basically you look for increasing                                 recent usage and it's actually a very                                 important functionality because this is                                 something that makes a system living                                 self-developing system for analysts to                                 kind of pick up on new things add new                                 sources and and an add new functionality                                 they need to know what's developing and                                 this is one function allottees help them                                 do that and the problem is that we often                                 have very few documents to base or                                 transcend and some trends some source is                                 kind of vary in topics so it kind of                                 degenerates to just the last top last                                 message the last posting over                                 being the trend which would you know                                 isn't helpful to us so they kind of                                 trying to tend to spike on there just                                 the immediate activity all the time so                                 what we do is that we smooth the time                                 interval so do significant terms they                                 can drag this out but we smooth them out                                 to do that we need a relevant term                                 scoring that significant term scoring                                 that is actually comparable between                                 different foreground and background sets                                 which the original scoring doesn't do                                 and then we want to compare different                                 sources there's also key step to kind of                                 make nice robust so I as a third                                 increasing windows and we have inverse                                 Pavel of sellers all of the rank which                                 is comparable and then we look for                                 things that are trending along different                                 different sources is a small example I                                 think we don't really have a lot of time                                 to look about it you see like in mid                                 month there's a exploit kind of we have                                 two sources here and the correlated                                 terms and and you see there's it is for                                 example an exploit trending mid mid                                 October in                                                              time so I'm know if you get to talk a                                 lot about the guide is filtering it's                                 about creating filters for specialized                                 topics this is our analyst he has an                                 internal entity called Hal and he gets a                                 lot of messages that are not about what                                 you want actually the rage of this can                                 be from one to ten thousand to one two                                 hundred thousand for over relevant                                 documents relevant documents so it makes                                 the source completely useless to the                                 analyst and we like to use significant                                 terms to highlight the terms that makes                                 up the first stage of forgetting the                                 filter that is actually based on the                                 data and it's actually relevant to the                                 data sources it also describes a topic                                 and for analyst be doing something like                                 this that                                 the significant term looks for exploits                                 at the background that that is Linux it                                 is need interns query you get a lot of                                 terms and we group them by topic mode                                 for example the circles we also given by                                 a lexical similarity model where for                                 things where we can't find as seinem                                 topics so these are for example plugins                                 plug-in modules are similar exploit and                                 scrapes an nmap similar threat and                                 attacks are similar and you kind of                                 colored the things that we think that                                 the for this kind of topic that animals                                 will pick up in red hair there's also a                                 lot of kind of and sordid kind of                                 entities on the bottom so so i can't                                 really hard time to explain it fully but                                 i think we based our time yeah so thank                                 you for watching we're going to thank                                 you for watching you're going to present                                 a lot of the technical it's about it's                                 on a blog in the coming weeks so if                                 you're interesting this you might want                                 to google that and check it out thank                                 you
YouTube URL: https://www.youtube.com/watch?v=yYFFlyHPGlg


