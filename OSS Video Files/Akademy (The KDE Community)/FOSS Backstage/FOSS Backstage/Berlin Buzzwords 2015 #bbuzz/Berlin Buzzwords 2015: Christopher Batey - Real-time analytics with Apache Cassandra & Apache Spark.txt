Title: Berlin Buzzwords 2015: Christopher Batey - Real-time analytics with Apache Cassandra & Apache Spark
Publication date: 2015-06-03
Playlist: Berlin Buzzwords 2015 #bbuzz
Description: 
	Time series data is everywhere: IoT, sensor data, financial transactions. The industry has moved to databases like Cassandra to handle the high velocity and high volume of data that is now common place. However data is pointless without being able to process it in near real time. That's where Spark combined with Cassandra comes in, what was one just your storage system can be transformed into your analytics system, and you'll be surprised how easy it is!

So, join me for a whirl wind tour of how to use these two awesome open source projects for time series data. We'll cover:
- An overview of Cassandra - Why is it so good for time series?
- An introduction to Spark 
- What canâ€™t be done in Cassandra and how Spark can fill in the gaps
- How to build analytics on top of your operational data without the typical Extract-Transform-Load
- Specific use cases: sensor data, customer event data, financial transactions

Read more:
https://2015.berlinbuzzwords.de/session/real-time-analytics-apache-cassandra-and-apache-spark

About Christopher Batey:
https://2015.berlinbuzzwords.de/users/christopher-batey

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              okay so thanks everyone for                               for coming along to this talk the the                               keynote introduced it really well                               because it is essentially a technical                               deep dive into the stock that was                               mentioned in the in the keynote you                               might have to bear with me a little bit                               we've had to have an emergency laptop                               change so my normally animated slides                                are now coming out on a PDF so sometimes                                if there's lots of things on the slide I                                have to pretend there's only a few                                things and i'll tell you when they need                                to appear okay little introduction                                before i get going so you know who knew                                who I am so I work for datastax there's                                a company the kind of like the Cassandra                                company we contribute an awful amount of                                code to Apache Cassandra and we build a                                product on top of Cassandra that                                includes spark and we open source the                                connection between the two so the the                                spark Cassandra connector that was                                mentioned in the keynote was open                                sourced by datastax my background before                                datastax is building lots of systems                                with Cassandra okay so I've spent a                                couple of years basically deprecating                                large Oracle systems with Cassandra                                systems and then we started using                                sparkle on top of that so the warning                                sign would normally would normally fly                                up afterward but this is what we're                                hoping to get through in                                               it is quite a lot so if you're new to                                Cassandra spark stream processing this                                is going to be a very large brain dump                                okay it's going to be quite hard to                                assimilate if you're familiar with one                                or two of them then hopefully you can                                you can follow quite well but it's it's                                a lot to cover in                                                     to do it by that of an example ok so                                it's an example it's another type of IOT                                walnut about the processing of weather                                data going to introduce that then we're                                going to go through a bit of a technical                                deep dive into cassandra and how the                                spark connector works into cassandra and                                then show you some code ok and show you                                a spark streaming application so these                                do fly up in some crazy order which I've                                now forgotten but essentially it was                                like roughly to work out who in the room                                does what ok and I come from the left                                hand side of this ok so the online                                transaction processing which you can use                                to any kind of relational database any                                of these no sequel ones like coming                                there's a new database every week on                                Apache I think we've also got things                                like online transaction processing okay                                so that's being able to do things in the                                semi near real-time report generation                                learning from your customer data but                                eventually that gets too large and then                                we throw our data to Hadoop okay today                                we're going to talk about Cassandra for                                your operational database spark to do                                but jobs directly on top of Cassandra to                                do your kind of batch work but then                                we're going to talk about spark                                streaming for kind of keeping these                                botched things up to date throughout the                                course of the day okay and this is the                                this is the use case so this is a                                reference application you can go and                                play around with it uses Kafka for the                                ingestion of messages and I'm not going                                to talk about apache Kafka that's too                                many Apache projects in one day you just                                got just as if you don't know what it is                                just assume it's a but it's just a piece                                of queuing software think of it like                                that we're going to look about Cassandra                                for the straw storage of the data and                                we're gonna look at spark streaming for                                ingesting the data storing the raw data                                and also building up kind of on the fly                                like aggregates the kind of things which                                you you could do inside say my sequel or                                postgres if the data size was small                                enough but if your data size is big                                enough you can't do but we can we can we                                can replace that with them spike                                streaming and essentially the end goal                                is to build up a set of materialized                                views that we constantly keep up to date                                that an application could read directly                                 out of Cassandra so though I'm by no                                 means a UI developer i'm pretty much i'm                                 pretty much terrible at front ends but i                                 did just put a little front end on it so                                 you can visualize it we can get all the                                 weather stations out we can get their                                 longitude and latitude and you can get                                 you can actually get software for a                                 Raspberry Pi which can collect all the                                 data we're talking about but we want to                                 keep things like this up to date so for                                 an individual weather station can we                                 keep things up to date like the daily                                 monthly precipitation even when we're                                 talking about things at very very large                                 scale okay hi low temperature for the                                 year for the month you know is it likely                                 to rain is it likely to                                 windy should I wear a coat you can go                                 and play around with this Tom github                                 like everything is these days so now i'm                                 just going to introduce the stack which                                 this thing which this application uses                                 so to build something like that you                                 first need a database and if you've got                                 a small amount of data which fits on a                                 single server so hundreds of gigabytes                                 it doesn't really matter what database                                 you pick right take anyone who cares as                                 soon as you start building like systems                                 at a larger scale then we need something                                 different from a database and because                                 I've built lots of things with Cassandra                                 and work for datastax we're going to use                                 Cassandra okay who here has used off                                 because who's used Cassandra heard of it                                 wow that's changed over the last when I                                 start first I using Cassandra I went to                                 a meet up in London those about six                                 people and now it seems to be that it's                                 it's almost becoming mainstream so if                                 you're even if you haven't heard of it                                 you're probably using it today there's                                 up we'll have tens of thousands of nodes                                 in production it's heavily used in                                 finance because if we're going to see                                 why it's good for time series data so                                 but we're going to talk about this top                                 left corner which is kind of you know                                 storing sensor data because Sandra                                 allows you to store things on order in                                 disk you get to pick that as an                                 application developer which is why                                 people tend to use it for time series                                 and it's why it's a good pic foot for                                 this problem ok however when I start                                 using Cassandra a few years ago I didn't                                 think who I've got some order data I'm                                 gonna look for a database which is                                 really good at order data it was for the                                 non-functional requirements so Cassandra                                 can run across many many servers it can                                 run across multi data centers and it can                                 scale to pretty large volumes and you                                 can always be assured that whatever your                                 problem is someone like Apple are doing                                 something bigger so they'll fall they'll                                 fall into the problems before you um it                                 comes about two different papers one                                 came from google one came from amazon                                 we're really only going to talk about                                 the first one which is the dynamo paper                                 because what that talks about is how to                                 distribute data and that's really                                 important if we're going to run spark on                                 top of it because if we've got a large                                 quantity of data we know we need to move                                 the computation to the                                 later not the other way around and if                                 there's a if there's a technology which                                 can move you know our computation and be                                 aware of how Cassandra distributes data                                 then we can we can do some really cool                                 things okay dynamo actually describes a                                 key value store cassandra is not a key                                 value stores what we call it a wide                                 columnstore that's the that's where a                                 big table comes from but that's not too                                 relevant for today what we're going to                                 use extensively when we're running when                                 we're trying to do analytics on top of                                 cassandra with spark is the fact that                                 cassandra is data central where every                                 single node is like hey i'm in the USA                                 data center or I'm in the Europe data                                 center these don't have to be real data                                 centers okay in the example that we're                                 going to go through it's actually going                                 to be going to lie to Cassandra we're                                 going to use this data center where                                 feature to do workload isolation okay so                                 we're going to we're going to segregate                                 a few nodes of our operational database                                 we're not going to let our customers go                                 to those we're not going to and we're                                 going to run analytics directly on top                                 of them okay so this is where you can                                 avoid that you know take all the data                                 shove it somewhere else like HDFS and                                 then do the analytics there with                                 workload isolation on Cassandra we can                                 actually you know do it directly on top                                 of our operational database sounds crazy                                 doesn't it so the dynamo paper is pretty                                 awesome who's read it yeah you should go                                 and read it it's very it's an awesome                                 domain because we all understand it                                 because it's about amazon shopping                                 baskets ok and they concluded that                                 that's a pretty large scale service and                                 they don't make a great deal of money                                 when the shopping basket service goes                                 down okay you know shock horror so they                                 third we need to design a debate                                 database differently and it was very                                 much around fault tolerance we're not                                 going to talk about fault tolerance                                 today but the fact that we end up just                                 two buting our data across many servers                                 and across you know many data centers                                 allowed us to basically you know get the                                 the data distribution we need for doing                                 this distributed computation the two                                 features of that paper that are really                                 important for this topic are the                                 consistent hashing and how cassandra                                 replicates data okay every every                                 database every no sequel store in the                                 world is jumping up and shouting please                                 run spark on top of me                                 okay it's it's the hot topic right                                 there's no way people haven't heard of                                 spark at the moment unless you're hiding                                 under a rock what's really important is                                 that you can get workload isolation and                                 that you can make spark and build spark                                 partitions that are aware of how the                                 underlying data store you know                                 partitions data so what is consistent                                 hashing how does Cassandra do it why is                                 it useful for running spark on top of it                                 so if you do a full table scan on a                                 single node database okay so big large                                 relational a few million rows it's going                                 to be slow okay if you've got a thousand                                 no database it's going to be really slow                                 so Cassandra kind of like removes                                 features that you'd expect in a normal                                 like relational database system to stop                                 you ever having to do something like a                                 full table scan all right and it does                                 this just by taking part of your data                                 hashing it and that working out which                                 node it's on right and spark can do the                                 same so if we have some data here like                                 Jim and Carol and Johnny and Susie what                                 we do or what Cassandra does is it                                 hashes that part the key and then you                                 know that goes to a value now the real                                 value is very big and I can't say the                                 numbers so we're going to pretend it's                                 between zero and a thousand so we'll                                 pretend that Jim goes to                                                to                                                                   node in a Cassandra cluster owns a range                                 of this this dish hush range so zero to                                 a thousand we're going to say that node                                 a owns you know                                                        to                                                                      and other nodes in the Cassandra cluster                                 can always work where your work out                                 where your data is okay so not only do                                 Cassandra nodes do this but drivers do                                 so the driver says oh I'm going to store                                 Jim I'm going to go directly to note a                                 okay this is why you often see the funky                                 picture whenever you look at a dynamo                                 based system you often see your ring                                 okay and you often see people talking                                 about hush rings and things and it's                                 simply because you can put the nodes in                                 a nice ring and say you know know a owns                                 this section no be owns this section the                                 reality is in Cassandra that it actually                                 split up vastly more into lots of little                                 sections and that's so we can                                 easily scale out and scale in without                                 having to it to rebalance that's so this                                 this basically result in a table which                                 can easily be worked out it's a                                 deterministic algorithm for working out                                 where data is and this is exactly how                                 the spark connector is going to build                                 putts park partitions too much Cassandra                                 partitions but that's not it it would be                                 a pretty terrible database if it told                                 you to separate your data across many                                 nodes and then it didn't replicate                                 because poor old Johnny if no day went                                 down would vanish and we'd cease to ever                                 have Johnny again so the next thing that                                 we do in Cassandra is we replicate it                                 and replication has to be clever if you                                 want to build an AP system from say                                 savior of your cup literate you need to                                 be topology aware ok there's no point                                 putting three replicas all on the same                                 rack inside your data center ok then if                                 you have a network partition between the                                 rocks then you know it's all hidden over                                 here so what Cassandra does is because                                 it's its data center where it's also                                 Rock aware and if you ask for say a                                 replication factor of three inside the                                 data center what it will do is it will                                 put you a copy of your data on each rack                                 so that you can handle Network                                 partitions between the rack so you can                                 handle four trucks going down without                                 losing data ok this is where the real                                 animation has gone away because I'm no                                 longer in keynote and I'm on a PDF so                                 I'll just have to tell you the order at                                 which these lines would have come in                                 because this is kind of explaining the                                 whole system of how a right works in                                 Cassandra so we start off with the line                                 up at the top here and this is the you                                 can just about see that so this line                                 here ok a client decides to do a write                                 for a particular key or a read it goes                                 directly to a node in a data into its                                 local data center which owns that data                                 ok so it knows that it needs to store it                                 here and it know that if it was reading                                 it would read it from there when you do                                 a writing Cassandra you pick a                                 consistency level we'll talk about                                 consistency levels brief briefly later                                 but essentially if we do it a                                 consistency level one then the cut this                                 guy here the first note that we                                 up to can return right away and say yep                                 I've got your data what Cassandra then                                 does is this coordinator as we call it                                 just it's a coordinated just for this                                 request what it does is it replicates                                 the data out to the other two replicas                                 and it can always work out what these                                 are two deterministic algorithm for                                 looking for particular nodes on                                 different racks at the same time I'm the                                 coordinator selects what I call a remote                                 coordinator in a different data center                                 okay so here we've got two data centers                                 and we want to replicate our data three                                 times in each we only copy it over once                                 because we're exhuming that the the                                 network link between data centers is                                 slow or expensive etc I mean is more                                 likely to go down its then the remote                                 coordinators responsibility to replicate                                 it to two other nodes in the other data                                 center okay and at that point you pretty                                 much you have replication the tunable                                 consistency comes about about when you                                 want to say that request is complete so                                 if you want to be really super if you                                 want to be really quick then you say one                                 and it returns right away if you want to                                 wait until more replicas have the data                                 you can say say all right that would be                                 all six nodes this is a terrible idea                                 and you should never do it because                                 you're losing all of the availability                                 suddenly right as soon as a single node                                 goes down queries queries are going to                                 start failing more likely you're going                                 to start using things like quorums which                                 is a majority so in this previous                                 example we have six replicas a quorum                                 would be four okay to majority you often                                 don't want to do that though because if                                 we were going to do a get it                                           previous example we would have to go                                 across the one before completing that                                 request that could be slow so you have                                 these local local quorums or local one                                 well that basically says it's in the                                 local data center get it to a majority                                 there and that means that in this                                 previous example we get it to two out of                                 the three nodes in the local data center                                 and let the rest happen asynchronously                                 when we're doing when sparks doing                                 writing writing to Cassandra it's always                                 doing things locally okay because what                                 you don't want is for your spark jobs to                                 suddenly start hammering your remote                                 data center which would be your                                 operational the one that say your                                 customers are facing                                 so by default you can override it the                                 spark connected as everything at local                                 one which basically means i want                                 acknowledgments from one mode in my                                 local data center at that point I                                 normally like wave my hands and say                                 Cassandra doesn't have any masters okay                                 Cassandra is a there's no single point                                 of failure because my first I'd learning                                 about Cassandra I started like seeing is                                 that a single point of failure is that a                                 master etc so that coordinator in the in                                 the previous example it is not special                                 okay if that coordinator were to go down                                 then the driver has a load-balancing                                 policy and it would just use a different                                 coordinator the other node which                                 sometimes people think is that one                                 special does that one need to be up is                                 the node which your data hashes to okay                                 rather than the other two selected                                 replicas and the answer is no that one's                                 down it's a deterministic algorithm for                                 working out the other two replicas and                                 clients can just use those two so that's                                 my quickest fire introduction to                                 Cassandra to make so we make sure that                                 people understand how the spark                                 connectors going to work does anyone                                 have any questions I realize that we've                                 only spent                                                           anyone have any questions on Cassandra                                 before we before we jump into the spark                                 lund yeah what if all de nodes which                                 handles one single hash are down so                                 you've got two options so the question                                 was basically yeah what if what if                                 selectively one replica in each of my                                 racks was down and like I can't right                                 you are you have two options and it's                                 you as an application developer that's                                 got to decide you either fail or there's                                 a special consistency level called any                                 what any is is it gives it to another                                 node to look after until those nodes                                 come back up okay and it keeps it for a                                 configured amount of time by default I                                 believe it's three hours so it's like                                 you change the hashing power to                                 temporary right the hashing remains the                                 same it's still those are the three                                 replicas but what you're telling the                                 driver is give it to another node and it                                 that node will keep it in what's called                                 as hinted hand off just for a short                                 amount of time                                 you pick the time the reason it's not                                 unlimited is because let's say this is                                 happening right it's bad times you've                                 got a third of your data center down or                                 however many if you do this for too long                                 what you end up doing is giving the                                 existing node so you're half of your                                 data center you're giving them more work                                 so you're almost overloading the node                                 when there's half the number of nodes                                 available so you can you can turn this                                 off and you can configure the amount of                                 time did it happens for any more                                 Cassandra questions before we get into                                 spark nope let's move on to to move on                                 to spark then so given the keynote and                                 the fact that no one put their hand up                                 that could have been because she asked                                 who doesn't know spark who does no spark                                 will do the positive question instead so                                 most people have heard of it well I will                                 do will spend a few minutes on it and                                 then we'll go on to the integration                                 between between spark and Cassandra will                                 start looking at spark streaming so if                                 we were to store our data in cassandra                                 cassandra is very good at storing vast                                 quantities of immutable kind of like                                 time series data if we were going to                                 store data for say all the weather                                 updates of all the weather stations in                                 the world it might look a bit like this                                 okay weather station ID I'm sometime so                                 this is where your month day hour so                                 we're assuming one event per weather                                 station per hour and then some cool                                 stuff pressure precipitation arm                                 temperature alright this gets very big                                 very quickly how do we actually how to                                 actually do some more like like                                 aggregate queries on this okay how do we                                 how do we work out the daily how do we                                 work out the daily I can get                                 precipitation right Cassandra doesn't                                 support a lot of the features that you                                 might expect of a relational database                                 because they typically don't scale okay                                 so the next section is going to be how                                 do we use spark to turn the data into                                 something more useful and it's at that                                 point where I get into my defense of                                 Cassandra like why doesn't it support                                 this stuff okay as I said if you if                                 you're your problem fits on a computer                                 don't use a distributed database like                                 Cassandra use a relational database okay                                 use something                                 with lots of fancy secondary indexing if                                 you've got this if you've got a scale                                 problem and you know you want it you                                 want to run across many servers and if a                                 technology offers you behavior like                                 secondary indexing and aggregates and                                 all these types of things think to                                 yourself how they scale on a                                           how do you think a secondary index works                                 on a hundred node cluster okay wherever                                 that secondary index might be it has to                                 go to that that's going to link to many                                 hundreds of rows in your database across                                 many hundreds of computers it's going to                                 be slow if you take Cassandra actually                                 does have secondary indexing I just tell                                 people not to use them because it works                                 on a three node cluster it might work on                                 a six node cluster it doesn't work on a                                 hundred note or a thousand node cluster                                 and that's simply because you end up                                 having to go to too many computers to                                 answer your query so if you denormalize                                 and you duplicate your data inside                                 cassandra you get it you get the                                 advantages of the fact that you get to                                 hash it you get to go directly to a                                 single node this works if it's three                                 nodes or a thousand nodes and then you                                 get to do as few seeks on disk as                                 possible so it's irrelevant whether                                 you've got you know a petabyte or a                                 gigabyte the actual satisfying your                                 query is the same it goes to a single                                 node it does a couple of disk seeks it                                 brings your data back it's called what I                                 call this a scale of be performant query                                 if that's if that's a real thing however                                 when we want to do things like                                 aggregates and analysis and things we                                 don't really care about a                                                response time we're may be happy to you                                 know to actually you know it take you                                 know seconds or maybe minutes or hours                                 or days if you're if you're using Hadoop                                 also we want to come up with a solution                                 to keep these things up to date we want                                 to maybe do stream processing so we're                                 not doing a query what is the daily                                 aggregate you know precipitation we just                                 constantly kept it up to date okay                                 that's where spark on top of Cassandra                                 comes in this is one of my favorite                                 cartoons so we've all kind of heard of                                 it most people put their hands up but                                 it's pretty much you can think of it as                                 a replacement for Hadoop MapReduce right                                 it's a distributed computation engine                                 with some really cool fancy bits which                                 make it slightly nicer my favorite part                                 of spark is essential                                 you get the same API for stream                                 analytics micro batch analytics and that                                 you do for you know proper batch                                 analytics okay that makes my life as a                                 developer vastly easier okay and it's                                 obviously hit the world by storm no pun                                 intended because it's already in all of                                 the Hadoop distributions but it's not                                 just making its way into Hadoop                                 distributions it made its way into                                 datastax this product which is very much                                 an operational database okay so it's                                 useful on for both the kind of business                                 intelligence the data warehousing and                                 it's also useful for augmenting your                                 kind of operational system okay that's                                 what we're going to talk about today                                 we're going to talk about a couple of                                 the components so obviously spark is not                                 a database right it needs somewhere to                                 get its data that's typically HDFS today                                 we're going to be talking about                                 Cassandra all right the general compute                                 engine which is the scholar API or                                 there's Python and Java as well we saw                                 some of the Java compute engine in the                                 in the keynote we're going to look at                                 going to look a scholar today okay                                 that's a nice Skylar API it makes you                                 look like you're just dealing with a                                 collection which is very nice okay we're                                 also going to look at streaming and                                 maybe some SQL if we've got time so our                                 DD you've already seen an RDD you've                                 seen like three of them already today so                                 I don't need to tell you what on our DD                                 is okay it's just an abstraction across                                 a vast quantity of data think of it as a                                 list it just happens to be an entire                                 cassandra table with a petabyte of data                                 in it or a very large file on some on a                                 distributed file system okay you get to                                 do if you're used to any of say the                                 scholar API sort of collection its kind                                 of immutable collections in general                                 you're going to get things very familiar                                 like filter and map and flat map etc                                 okay we're going to see quite a few of                                 those today what spark does when you do                                 all of these things is precisely nothing                                 okay you do all these things ignores you                                 until you do an action something which                                 has a side effect and the side effects                                 could be writing it to a file but today                                 it's going to be writing it to a                                 cassandra table so we're going to see                                 data coming in to Cassandra from Spike                                 streaming but also out of Cassandra                                 everyone has to have a word count                                 example I'm amazed at the Kino doesn't                                 have a boil count example stolen                                 directly from the spark docks so this                                 not the most exciting spark example in                                 the world but it gives you an idea if                                 you haven't seen sparco before of                                 exactly what we mean by an RDD right you                                 take a file you turn it into an idd                                 we're going to see the same but                                 Cassandra table very shortly you can do                                 operations like split you know split                                 each line you know by white space map it                                 to a tuple with the word and then one                                 and then your typical reduce right and                                 if you've seen must produce hadoop                                 mapreduce of the equivalent program                                 you'll realize this is pretty quite nice                                 there's also a ripple right I spent most                                 of yesterday on a rebel working on                                 gigabytes of data inside Cassandra and I                                 was just hacking away doing ad-hoc                                 queries things I wouldn't normally be                                 able to do with pure Cassandra because                                 it kind of restricts you to do only the                                 performant queries that example was what                                 we call a batch job right you're dealing                                 with a huge file right we could we're                                 going to show you how to go across that                                 entire weather data and do a bad job on                                 it that might take minutes it's going to                                 take proportional to how much data you                                 have that might be you know quick for                                                                                                      down for a spark streaming comes in so                                 we get to actually keep things up to                                 date you know in the near real time                                 because we can't say real time because                                 that word means that something very                                 strongly to set certain people it's not                                 the full functionality of you're                                 familiar with something like storm it's                                 not going to process things as they come                                 on the queue it does what's called micro                                 batches so we're going to see a for our                                 example with them weather data will show                                 you a                                                                   as small as it gets so if you need                                 results to be updated more often than                                 every                                                                 okay this is quick but not the                                 single-digit milliseconds the kind of                                 response times we'd want directly from                                 from a cassandra database so let's get                                 on to some example code okay we've got a                                 database we've got a stream processing                                 platform and batch processing platform                                 inspired let's start looking at some                                 code so a note on deployment first so if                                 you in deploy a system like this in                                 production what you're actually going to                                 do is co-locate spark and cassandra on                                 the same servers right if you're if                                 you're an open-source junkie you have to                                 do this yours                                 self if you're using datastax enterprise                                 you get this bye-bye free you just click                                 I want a spark work or inside that                                 Cassandra node and you get it okay                                 little shameless plug ler the reason for                                 this is the spark connector now if your                                 spark literate you know that spark has                                 thing called partition and a partition                                 is the level at which concurrency and                                 tasks are executed on what the cassandra                                 spark connector does is build spark                                 partitions of cassandra partitions same                                 terminology gets confusing that live on                                 the same node so the connector is never                                 going to build a spark partition which                                 is made up of cassandra partitions that                                 live on two different nodes why so we                                 can send the computation to the worker                                 that's local to that node right that's                                 the important thing if the driver did                                 not do that it would be useless I back                                 to the example so we'll do batch                                 processing first so we're going to                                 assume that the data is ingested into                                 Cassandra by some other means and we're                                 going to have a look at maybe what kind                                 of batch jobs we could run on top okay                                 remember the data look like it was a big                                 wall of numbers this is the table so                                 this is the schema for the table i                                 showed you before um when we introduced                                 Cassandra i didn't actually talk about                                 exactly how you model data in Cassandra                                 cuz i thought we might as well wait to                                 the example so the only hard thing you                                 have to worry about when you do things                                 with cassandra is selecting the primary                                 key alright so this looks a bit like an                                 SQL table we've got things like wind                                 direction and speed and temperature and                                 all these types of things the really                                 important thing is the primary key well                                 then I spend a couple of minutes on this                                 the primary key is exactly how you tell                                 Cassandra how to distribute your data                                 and how to order it on disk now because                                 you're picking how data is ordered on                                 disk in Cassandra you obviously can't                                 query it in different orders which is                                 very important that's why you end up                                 duplicating so primary key looks simple                                 enough the primary key is made up of two                                 parts and I've already kind of explained                                 this a little bit but we're going to see                                 a concrete example here the primary key                                 is made up of the first bit that's the                                 part which is called the partition key                                 which is hushed everything with the same                                 partition ki will                                 end up on the same node in your                                 Cassandra cluster so anything any                                 queries in the same partition are going                                 to be quick even without spark right the                                 next thing you pick is all the other                                 fields which you want to be part of your                                 your primary key they actually dictate                                 your order on disk right so if you do a                                 range query which is for like a time                                 period it's going to be really quick so                                 if we actually look if we this this                                 primary key here ends up looking on disk                                 a bit like this so we have a partition                                 key and then I'm only showing                                         temperature otherwise the slide will get                                 too big and then we have everything in                                 order so if you have a look here the                                 temperature has actually been prefixed                                 with all of the members of the                                 clustering column all of these guys okay                                 so we can see that the tenth ninth                                 eighth seventh so if we did a limit                                 query on this table get me the last five                                 events from this weather station it's                                 going to be really quick okay if we                                 asked for a if we didn't include the                                 cost the partition key in our query your                                 query or you'd have to go to every                                 single node in the cluster for that                                 reason Cassandra doesn't really let you                                 do it there are some cases where it will                                 but that's when we're going to use spark                                 to do it all okay I'm say yeah thousand                                 no cluster we just go directly to one                                 node all right this also had fancy                                 animation but it's all gone so                                 essentially if we want to do a query                                 which is say greater than                                            than                                                                   what Cassandra do is go directly in you                                 know start at ten over the indexing on                                 file and then slice it and it gives you                                 it back is a nice table so you don't                                 have to worry about that format when                                 you're programming you know if you're                                 used to tabular data from a relational                                 database the programming API is the same                                 just know is that's why it performs                                 really quite quickly you can also just                                 store you don't have to use the                                 clustering column functionality you can                                 just have a primary key which you know                                 with just the partition key so an                                 example of that is your reference data                                 or you know something where you're                                 always you're not going to be wanting to                                 do range queries so the weather station                                 data is like that in Cassandra where we                                 would start to lose out in Cassandra is                                 when we wanted to build                                 table like this now I've already told                                 you can't do an aggregate style query in                                 Cassandra so what we want to be able to                                 do is keep a materialized view like a                                 materialized view up to date in                                 Cassandra this one here is for the daily                                 aggregate temperature so it has the same                                 primary key but we're actually going to                                 keep things like the day high the day                                 low you know averages things like that                                 and there's no real way to to build this                                 okay there's um you could obviously do                                 some fancy things inside your                                 application but we're going to use spark                                 for that another example might be of a                                 materialized view in an air quotes is a                                 daily aggregate precipitation to                                 constantly keep up to date with how much                                 rain has happened every day okay and we                                 could do a batch job for that and I'm                                 going to show you an example we are                                 going to show you about example of a                                 batch job for that where but in reality                                 what we want to do was with spike                                 streaming to just keep them up to date                                 forever so we're building like a                                 dashboard application it can just go                                 directly to Cassandra do one of the                                 performant queries and the spark                                 streaming job is going to is going to                                 keep it up to date so I'm sorry if you                                 don't like Scala I realize some people                                 don't but that's quickly just to raise                                 free some of the code so this is                                 actually what the raw data looks like                                 this is real data by the way its                                 historic data from from the US you can                                 you can go and get years worth of it                                 this is a case class in Scala so you can                                 take a this basis it's like a POJO ver                                 in Java it's or a struct it's just how                                 we're going to represent the data inside                                 their application so it's in the same                                 order as the comma separated value you                                 create a streaming context this is where                                 you select how often you want your code                                 to run right ignore the kafka bit okays                                 are just connecting to Kafka everything                                 after the cuff cab it is would be the                                 same whether you're getting it from MQTT                                 or zeromq or from a soccer or whatever                                 we can split this comma separated values                                 you know by x comma and then shove it                                 into the case class this is actually                                 going to give us a stream overall                                 weather data artifacts and then it's our                                 responsibility to do something funky                                 with that one really cool feature of the                                 connector is that you can just if you're                                 case class or if you've got a couple                                 matches your cassandra table directly                                 all you do is save to Cassandra with the                                 key space and a table name and it will                                 just put it in there so even though i'm                                 talking about stream analytics here we                                 are leaving the raw data so now actually                                 spy extreme and has taken over our                                 ingestion of raw data the next thing I'm                                 going to do is keep a date the daily                                 aggregate table up to date all right so                                 what we can do is we we take that stream                                 so remember that's a stream of raw                                 weather data case classes we can then                                 map it to look exactly like this table                                 right so it's got the weather station                                 the year month and date and it's got the                                 same primary key now there's a really                                 funky con type in Cassandra which used                                 to be quite buggy but works really well                                 in                                                                       a counter is is like imagine it as a                                 normal integer field inside a database                                 but every time you update it just adds                                 on the value okay so what were acts what                                 this code is actually going to do is                                 every                                                                save all of the raw data it's going to                                 map all of the raw data into this it's                                 going to take out the one hour                                 precipitation and it's going to put it                                 in Cassandra and the Cassandra counter                                 is going to keep it up to date it's                                 going to add them all up for you so you                                 can go to this table at any time and it                                 will be at most five hundred                                 milliseconds behind behind real time                                 what about a batch job okay so this is                                 what good old this isn't streaming this                                 is just a regular loading a cassandra                                 table from into an RDD we're selecting                                 the temperature now the important bit                                 here is we're selecting the weather                                 station ID in this query okay what does                                 that mean in Cassandra means it's going                                 to be performance it's going to go to a                                 single node right so this batch job will                                 actually collect will actually be really                                 quickly you could make this as part of                                 your operational system because it's                                 going to a single Cassandra partition                                 all right and here's an example in spark                                 you have the stats counter which can                                 then you know keep give you a you know                                 account and means and things so we could                                 then save this back to Cassandra if we                                 don't include the weather station I do                                 in this example here right this is going                                 to scan the entire cassandra table this                                 one is going to take time depending on                                 how big your partition is how many                                 weather events do you have for a                                 particular station this one is going to                                 get slower you know as you add more                                 weather stations and as you add more                                 data all right you would not make this                                 one part of your operational system                                 because it has to scan an entire                                 Cassandra cluster this would be a bad                                 job which you might run at night this                                 would be the one that you could run                                 whenever if you wanted to keep this one                                 up to date which would be the mean and                                 the high-low for all of your weather                                 stations you would then write a spark                                 streaming job so you in the same                                 application you can have a mixture of a                                 spark streaming and spark batch jobs                                 okay so that's pretty much it you can                                 see here that you can easily include                                 both okay so when you're building a                                 streaming system with arm kafka the                                 story with um spike streaming you need                                 somewhere to put the data okay i'm                                 saying cassandra is a pretty good                                 solution especially if you have vast                                 quantities for the raw data for the                                 materialized views you can put those in                                 Cassandra as well or you could put them                                 in a caching system or you know because                                 you know there's not the data size there                                 isn't as great and don't forget that if                                 you're doing spy extremely you can also                                 combine it with running batch jobs on                                 the fly some batch job as if they're in                                 a single Cassandra partition are going                                 to be quick however the jobs which are                                 going to be over your entire cassandra                                 tables with you know gigabytes terabytes                                 petabytes of data remember they're going                                 to be the ones you're going to run at                                 the end of a day and the whole goal is                                 essentially to keep a bunch of views                                 ready so this guy can just pop in here                                 whenever he likes and see an up-to-date                                 version of these aggregates even if                                 you're talking about you know many tens                                 of thousands of events per second and                                 you know many terabytes of data under                                 the covers so on that note I am done if                                 you've got any questions I think we have                                 a couple of minutes for questions so                                 thanks for listening and hopefully you                                  have a good idea now of whether things                                  something like spark and Cassandra's                                  would be a good text up for your next                                  your next product project
YouTube URL: https://www.youtube.com/watch?v=rvaRH0LgzJU


