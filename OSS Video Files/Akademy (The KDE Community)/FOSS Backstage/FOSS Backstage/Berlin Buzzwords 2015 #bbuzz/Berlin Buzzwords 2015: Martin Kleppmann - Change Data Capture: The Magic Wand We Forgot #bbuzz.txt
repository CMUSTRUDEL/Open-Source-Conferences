Title: Berlin Buzzwords 2015: Martin Kleppmann - Change Data Capture: The Magic Wand We Forgot #bbuzz
Publication date: 2015-06-04
Playlist: Berlin Buzzwords 2015 #bbuzz
Description: 
	A simple application may start out with one database, but as you scale and add features, it usually turns into a tangled mess of datastores, replicas, caches, search indexes, analytics systems and message queues. When new data is written, how do you make sure it ends up in all the right places? If something goes wrong, how do you recover?

Change Data Capture (CDC) is an old idea: let the application subscribe to a stream of everything that is written to a database â€” a feed of data changes. You can use that feed to update search indexes, invalidate caches, create snapshots, generate recommendations, copy data into another database, and so on. For example, LinkedIn's Databus and Facebook's Wormhole do this. But the idea is not as widely known as it should be.

In this talk, I will explain why change data capture is so useful, and how it prevents race conditions and other ugly problems. Then I'll go into the practical details of implementing CDC with PostgreSQL and Apache Kafka, and discuss the approaches you can use to do the same with various other databases.

Read more:
https://2015.berlinbuzzwords.de/session/change-data-capture-magic-wand-we-forgot

About Martin Kleppmann:
https://2015.berlinbuzzwords.de/users/martin-kleppmann

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              Oh                               before I get started if you want to                               follow along the slides already online I                               put them at Martin KL comm /b buzz at                               that URL on the slide so you can follow                               along on your own screen if you like as                               well at your own pace so anyone here                               know about change data capture already                               could you just raise your hand okay so                                maybe                                                                 actually really simple and it's really                                powerful if you build systems based on                                this idea and when I first came across                                it I felt it was like a kind of magic                                wand where you could simply make                                magically certain problems in building                                systems go away and maybe that's over                                selling it a bit but I'm still quite                                excited about it and find it                                surprisingly not that well known so                                hopefully I can bring across some of the                                excitement for that and also showed the                                details of a practical concrete                                implementation that I open-source last                                month anyway so I I'm Martin clapman I                                used to work at LinkedIn doing love                                scale data stuff I was working on a                                stream processing framework they're an                                open source one called Samsa I'm now                                actually doing a sabbatical at the                                moment and writing this book for                                O'Reilly the book is called designing                                data intensive applications and it it's                                not about any one particular tool or                                system that you can use it's really                                trying to be an overview over all the                                different fundamental algorithms and                                techniques and trade-offs that feed into                                building data systems especially                                distributed ones so its focuses on                                replication and partitioning and the                                internals of databases data models data                                storage engines and so on very broad                                it's still work-in-progress but you can                                find the first seven chapters online ups                                data intensive dotnet if if you are so                                inclined anyway let's talk about just an                                example application for the sake of                                argument the kind of thing we're talking                                about is assume you're building some                                kind of website so you've probably got                                clients talking from a mobile app or web                                browser to some kind of server where                                your application code lives                                and whenever that server wants to store                                something for the future it writes into                                a database                                whenever it wants to look up something                                that was stored previously it queries                                the database obviously and in very                                simple systems one database is enough                                but as we find as systems get more                                complex we start needing different tools                                for different purposes so maybe you need                                a cache to speed up reads maybe you need                                a full-text search index in order to                                allow full text queries and so the                                problem is what we end up now is we have                                these different systems and data lives                                in all of these systems and we somehow                                need to keep them in sync                                so we have data the same data just                                stored in different forms if you think                                about a full-text search index a at an                                elastic search for example you probably                                want your you want all of your documents                                in there but you also want them in some                                other database which is your source of                                truth your transactional database so now                                you have to make sure that all of the                                same data ends up in both places and                                indexing is just one example of that so                                you can see caching as being just                                another way of saying well I've got this                                different data and I want to represent                                it in different data stores in different                                forms but it's still the same underlying                                data when the underlying data changes it                                has to be changed in all these different                                places which leads us to the problem of                                really integrating these different                                systems where you need to make sure that                                the same that the right data ends up in                                the right systems at the right time this                                is not that straightforward actually                                because if you implement this the naive                                way you end up with something like this                                so this is a diagram with time going                                from left to right and we've got here                                 two databases for two data stores maybe                                 one is a database one is search index                                 and you've got two users writing to                                 these and so say the the first user                                 wants to write some document a under a                                 key X and it writes it first to the                                 first data store and it's okay and then                                 writes into the second data store and                                 that's okay and now concurrently with                                 this we've got a second user the red one                                 who wants to write some document B and                                 they've first write B under the same key                                 X to the first datastore and and write                                 it to the second data                                 and now it's happened well if you look                                 at the evolution of the value in each                                 data store in the first data store it                                 first got set to a and then got set to B                                 so the final value is B in the other one                                 it's the other way round you it was                                 first B then a and this is not eventual                                 consistency right this is perpetual                                 inconsistency because unless somebody                                 comes and overwrites this thing again                                 these two data stores are now going to                                 be forever out of sync                                 due to this race condition and there are                                 other problems that can occur like for                                 example you can imagine one of the right                                 succeeding and the other one failing and                                 now what do you do like okay you can                                 retry for a while but that may not                                 eventually succeed the process that is                                 retrying may itself crash so there are                                 all sorts of different ways of how these                                 if you're writing the same data to                                 multiple systems how they can go out of                                 sync and it really gets rather messy so                                 how do we solve this problem of getting                                 the right data into the right places                                 which I call fancy words data                                 integration but there's not that much to                                 it really so this approach of simply the                                 application doing dual writes or triple                                 writes to different systems doesn't work                                 very well because of these race                                 conditions so change capture what I want                                 to talk about here is really a super                                 solution to this problem and the                                 approach is very simple rather than                                 writing to several different data stores                                 just write to one database pick one                                 database doesn't really matter what                                 technology that is but that is now your                                 source of truth or your system of record                                 you call it and now whenever some data                                 gets modified in that database through a                                 write we take those changes that are                                 happening in the database and extract                                 them in some way and move them out into                                 a separate system and so I'm suggesting                                 Kafka here we've heard Kafka a few times                                 already of this conference I'll mention                                 it a bit more later                                 the basic idea here being that you can                                 get every time somebody writes to the                                 database that is actually a message like                                 a message on a message queue and you can                                 simply stick it at the end of this log                                 and other people can then come and                                 consume that so if for example you want                                 to keep your search index up-to-date                                 whenever a document get                                 written in the database it should also                                 be written in the search index it can                                 just consume this log of changes and it                                 will apply the changes in the same order                                 as they were applied in the database so                                 this now you can start building this                                 index and just keep it up to date by                                 keep consuming and so it may lag                                 slightly behind depending on what your                                 latency endured this pipeline is but it                                 won't go perpetually out of sync but                                 it's now not just the search index you                                 can actually have you can have multiple                                 different systems which are feeding off                                 that same that same log they can all                                 subscribe to the same list of changes                                 happening in the database for example                                 you can do cache invalidation this way                                 or you can take the data and archive it                                 to HDFS and run some kind of batch                                 analysis on it offline or another this                                 all works because all of these consumers                                 are independent from each other they                                 independently just subscribe to to                                 change and do whatever they need to do                                 with it that different consumers are not                                 dependent on each other it's not just                                 data stores that you can feed this into                                 you can feed it into string for extreme                                 processing frameworks as well so                                 anything like Sam saw or flingers Bach                                 streaming or strong you can use to now                                 analyze any data that's that's any                                 rights that are happening to the                                 databases you can use it for monitoring                                 anomaly detection etc so I wanted to                                 implement a practical example of this                                 and did so using Postgres as the the                                 source database now in principle you                                 could do exactly the same ideas with any                                 database Postgres is interesting here                                 because it's launched a new API in                                 Postgres                                                         December so it's a it's the basis on                                 which it's building is fairly new and                                 this this new API is called logical                                 decoding which I'll explain in a moment                                 and it allows change data capture to be                                 implemented in a nice way so it actually                                 makes quite a nice like example proof                                 point of what this kind of system can                                 look like and I'm hoping that this will                                 kind of encourage people to do the same                                 thing for lots of other databases as                                 well this it's open source in this open                                 source work was sponsored by these nice                                 folks at confluence there                                 a couple of people who left LinkedIn to                                 start the startup they're the original                                 authors of Kafka they're very smart very                                 nice people and they're building now a                                 kind of commercial distribution around                                 Kafka so they allowed me to work on this                                 which is very nice Thank You confluent                                 the basic idea of how the system works                                 which oh I call it bottled water for                                 example by the way I forgot to mention                                 that which is kind of a stupid pun on                                 taking data streams and packaging them                                 up in a format that's kind of nice to                                 transport around so the way this works                                 is you have a Postgres database and                                 bottled water consists of two main parts                                 one part is a little plugin which sits                                 inside the database server so that may                                 sound a little bit scary because you're                                 actually running code inside the your                                 main database server at the moment                                 there's no way around that                                 maybe at some point in future there will                                 be but for now the way this works is                                 this plug-in hooks into an API in the                                 database which receives callbacks every                                 time a row is inserted a rows updated a                                 row is deleted whenever transaction                                 starts in the transaction commits and                                 that's all you need this plug-in can now                                 take this and you can use all of the                                 internal Postgres functions to format                                 the data nicely and format the data into                                 some way that you can send it over a                                 network connection and now the second                                 part is a client daemon which connects                                 to the Postgres database it gets a                                 snapshot of the entire contents of the                                 database and then it also hooks into                                 this stream of writes happening live                                 every write that happens every insert                                 update or delete at a row level gets                                 encoded it's using Avro as the encoding                                 format here at the moment and all of                                 those rights that are happening are then                                 pushed out to Kafka and so it's using                                 Kafka as the mechanism for other parties                                 then to subscribe to the stream of                                 activity happening so anyone who wants                                 to consume this content is consumed from                                 Kafka you've just got this bottled water                                 kind of acts as this bridge between the                                 Postgres world and Kafka                                 it would be possible to instead of using                                 Kafka just use it as a kind of embedded                                 library that you build into your process                                 that you load into your process like                                 through J&I or something like that that                                 would work just as well and the other                                 thing that does actually it integrates                                 with an avro schema registry there's a                                 bit of a fine detail about how a Frode                                 the serialization format works but it                                 has a statically typed schema which can                                 be evolved so if the if you add a column                                 to a table for example this can be                                 represented as an avro schema evolution                                 it's not it can maintain backwards                                 compatibility that way nice thing about                                 schemas here well afro is a very nice                                 format like I like it much more than                                 JSON for example because say it                                 distinguishes between integers and                                 floating point numbers properly which                                 Jason doesn't and various other issues                                 with binary strings and such like so                                 Avro works very well it's very compact                                 very fast language neutral and so on                                 and the this client daemon here                                 registers the schemas so the schemas are                                 actually derived from the Postgres                                 database it simply looks at the tables                                 that are there and automatically                                 translates Postgres table into an avro                                 schema so it's very low if I had to set                                 this thing up it it just works                                 maybe I should demo what it actually                                 looks like so that you can get a feel                                 for this this is a I don't know how well                                 this will work but we'll see so I have                                 here at the top is a Postgres database                                 with a massive three rows in it and you                                 can see it's got three columns in this                                 table and it's quite straightforward and                                 now what I'll do in this other window                                 here is I will start a take Africa                                 consumer and so this caf-co consumer it                                 also integrates with the avro schema                                 registry so it does a nice thing of the                                 Avro encoded data is actually binary and                                 so it's a bit of a pain to read but it                                 can be transformed into a sort of semi                                 human readable Jason kind of right at                                 the end of the output stage and for that                                 it takes the schema from the registry                                 parses the Avro data and prints it out                                 here as JSON so right now                                 as nothing happening yet so I will now                                 switch over to bottled water so bottled                                 water is this is the client here the                                 little plug-in is really installed in                                 the database server and this is just a                                 command-line tool which you run and I'm                                 going to run it and simply give it my                                 Postgres on localhost into the database                                 name so I run that and you can see here                                 it has captured a consistent snapshot of                                 the database and it is registered some                                 schemas and then it says odor snapshot                                 is complete and now it's streaming                                 changes so if I switch over back to here                                 oh look at the Kafka consumers has found                                 some stuff in particular you can see                                 that it's taken a dump of the database                                 contents here so it's got a key and                                 value and it's exported the                                         years word                                                               message in Kafka which is taken from one                                 row in the original database and I can I                                 don't know let's try inserted a row for                                 example here                                                             appears at the bottom so this is                                 actually gone from Postgres through                                 bottled water through Kafka and back out                                 into the Kafka client and I can update                                 stuff search foo equals                                                where ID equals                                                         I've now got this                                                   hearing down here so it works on a very                                 small database at least so a couple of                                 interesting things to talk about in the                                 internals of how this works so maybe I                                 can just convey some of the interesting                                 details there one question is this                                 consistent snapshot and stream of                                 changes how how do we make this work so                                 imagine you've got this is an                                 operational database so you've got                                 people writing to it all the time and of                                 course reading from it all the time as                                 well but the rights of the difficult bit                                 so people are constantly writing to this                                 database and we somehow want to build a                                 new search index on that for example how                                 do we do that first thing is we take a                                 consistent snapshot of the entire                                 database                                 and now in a dance database this might                                 take a few hours so we don't want to                                 lock the database for updates during                                 that time we certainly have to be able                                 to continue processing rights while                                 that's nap shot is happening but                                 fortunately Postgres is MVC MVCC                                 concurrency control can handle that with                                 no problems so it can present a snapshot                                 of the database as of one point in time                                 to one transaction and that can                                 transaction can go on for a while and in                                 the meantime others are happily reading                                 and writing latest stuff and that's that                                 one transaction is it's basically doing                                 a select staff from every single table                                 it sees the database as of that                                 particular point in time and it doesn't                                 see like later updates coming in so that                                 snapshot will will dump the entire                                 database and write all of that out every                                 single row becomes a message to Kafka so                                 you've dumped a whole lot of messages                                 into Kafka and one go and now in the                                 meantime of course there are more rights                                 happening while while this is going on                                 so in this diagram we've got here the                                 the users at the top then postgrads on                                 the second level bottled-water on the                                 third level and Kafka at the bottom or                                 or the full text index at the bottom so                                 fortunately the way Postgres implements                                 this is actually very nice this is not                                 really bottled waters work it's just the                                 way that Postgres provides this API it                                 actually queues up all of these changes                                 that have happened here so even if the                                 snapshot takes a couple of hours that's                                 not a problem it's not going to miss any                                 rights because any rights will simply                                 remain there in the log and then when                                 the snapshot completes its going to                                 catch up on all of those rights that had                                 missed in the meantime though it's not                                 actually going to miss any rights                                 it'll just catch up on them when the                                 snapshot is done so then quickly turns                                 through that backlog of Rights that                                 happened and when that is done it can                                 relax so from that point onwards simply                                 every right to the database it gets                                 picked up by bottled water gets sent out                                 to cafeteria gets picked up by the                                 subscribers and does the whole thing                                 within fairly low latency like just now                                 in that example you saw it's about a                                 second or so you could probably tuned up                                 down lower if you fiddle with the with                                 the settings a bit should talk a bit                                 about transactions and concurrency                                 because that's that's                                 kind of an interesting area so that's                                 where you know the a race condition that                                 I mentioned at the beginning that's the                                 problem we had yet another time diagram                                 here so we've in this case we've got two                                 users they are concurrently writing to a                                 database                                 what does bottled-water do of this in                                 this case we've got the first user which                                 is writing a equals one and B equals two                                 and then committing and it's doing that                                 within one Postgres transaction so this                                 happens atomically from the point of                                 view of everyone involved in particular                                 we have to make sure that it remains                                 atomic from the point of view of anyone                                 consuming these streams downstream as                                 well so now the a second user                                 concurrently is writing x equals                                     equals                                                                   doing that with in some other                                 transaction now again Postgres actually                                 does does this wonderfully well what it                                 provides us here in this API it is                                 plug-in API is that these rights are                                 only seen by bottled water at the point                                 when the transactions commit so even if                                 here the X equal                                                     before any of the transaction                                          right actually won't be visible to                                 bottled water right until that transit                                 that red transaction here commits and at                                 the point where it commits then it gives                                 us all of the rights from that                                 transaction in one nice convenient                                 package so what Postgres is actually                                 doing here is reordering the rights to                                 be consistent with the commit ordering                                 it's got an internal thing called a                                 reorder buffer to do exactly this as                                 it's super handy because it means that                                 for example if a transaction aborts                                 halfway through then actually bottled                                 water hasn't even seen any rights from                                 that transaction so there's nothing to                                 rollback or anything it just hasn't even                                 happened                                 so this ordering by commits is also                                 super important for any downstream                                 systems of course because you have to                                 apply the rights in the same order as                                 they were committed to the original                                 database otherwise we get                                 inconsistencies I'll maybe introduced                                 Kafka briefly so we've had it mentioned                                 in quite a few talks already so I don't                                 need to say very much basically you can                                 think of it as a message broker there                                 are various processes which can dump                                 data into its                                 quite often used for kind of clickstream                                 events for example where each event is                                 the fact that a particular user at a                                 particular time loaded a particular URL                                 something like that and you just dump                                 all of these things into Kafka and then                                 anyone who wants to subscribe to those                                 streams can do so and so you can have                                 various different subscribers which are                                 all independent from each other and they                                 can do whatever they need to do build                                 recommender systems or do analytics or                                 monitoring and so on more interesting                                 than this is actually how Kafka is                                 implemented internally which is actually                                 very different from say the AMQP based                                 message brokers like like rabbitmq or                                 ActiveMQ or JMS those types Kafka is                                 quite a different model here which I                                 which actually works really nicely in                                 conjunction with change data capture and                                 it was a Kafka was actually designed                                 with this kind of thing in mind so it's                                 not a coincidence you can think of Kafka                                 as basically append-only files so                                 whenever you publish a message to Kafka                                 when a producer publishes a message it                                 simply gets appended to the end of one                                 of these files indeed these are                                 literally memory mapped files on disk                                 replicated across multiple broker nodes                                 and now if you want to consume these                                 streams as a subscriber what you have as                                 a consumer is an offset a position into                                 one of these append-only files and you                                 only ever read messages in these files                                 sequentially so it's it's very much like                                 you know just opening a file pointer to                                 a file on your local disk and                                 sequentially reading byte by byte except                                 this is message by message and because                                 you're reading sequentially and these                                 this offset into the the file is                                 monotonically increasing it means that a                                 consumer can keep track of its own                                 offset the brokers don't need to keep                                 track of which consumer has seen which                                 message and acknowledgments and stuff                                 like that instead each consumer just has                                 its current position and because it                                 reads sequentially it knows that all of                                 the messages before its current position                                 it's already processed and all of the                                 messages after its current position have                                 not yet been processed it's a super                                 simple model actually                                 and in this case where we want the                                 message is actually be to be totally                                 ordered it works super nicely so let's                                 talk a little bit back to bottled water                                 and how it translates some of the                                 Postgres world over interview the Kafka                                 world so there's some flexibility of                                 changing these discs these decisions but                                 this is kind of the way it's working                                 right now in the open source release                                 whenever you have a table in Postgres                                 that turns into a topic in Cathcart                                 which is kind of the unit at which you                                 can subscribe to messages the DDL the                                 like the form of a table in Postgres                                 first class uses schema to use something                                 means something completely different                                 confusingly but a DDL in Postgres the                                 definition of what the tables look like                                 is translated by bottled water                                 automatically into an avro schema as I                                 mentioned and now every time a row is                                 inserted or updated in a table that                                 turns into a message in Kafka where it                                 takes the primary key of that row as the                                 message key and Kafka in tables which                                 don't have a primary key there's some                                 extra contortions but assume for now                                 that everything has a primary key                                 similar thing goes for an update what it                                 simply does is it logs the new new                                 entire row for a particular primary key                                 so the the message in Kafka is simply                                 the the primary key and the value is the                                 new all of the columns of the new row                                 new version of that row that was written                                 what about deletes in this case that's                                 kind of interesting so we also have the                                 primary key for a delete and in this                                 case what bottled-water does is it sets                                 the value of the message and Kafka to                                 know this is actually a special no null                                 is a special message value that's has                                 special handling in Kafka the way it's                                 special is in the context of log                                 compaction so log compaction is this                                 interesting feature of Kafka that's many                                 have not actually heard of but it it                                 works if you have messages and this kind                                 of key value model so if you just have                                 clickstream event later than compaction                                 doesn't make any sense but if you have                                 this kind of key value model where in                                 this case you can think of this as the                                 the key here is a B or C and the key                                 indicates the primary key of the road                                 that was written and the value is the                                 the new value of the road that was                                 written so in this case here a was set                                 to                                                                    set to                                                                 that you only really need the most                                 recent value for a given row it's like a                                 key value store updating a particular                                 key in place overwriting it with a new                                 value you only really need the latest                                 value so for a given key here in the                                 sequence you actually only need the                                 latest message with a given key and so                                 if you turn on Log compaction in Kafka                                 it's actually allowed to throw away some                                 of the messages if Kafka has seen a                                 later message with the same key it's                                 allowed to throw away that message so it                                 keeps only the most recent message for a                                 particular key but the most recent                                 message for a particular key is kept                                 indefinitely it doesn't get thrown away                                 so unlike the standard Kafka thing where                                 it retains messages for a week or                                 something like that and then just throws                                 them away with a lot compaction it                                 doesn't do that it keeps them forever if                                 there's no new message with a certain                                 key except if you send a message with a                                 key that exists and a null value and                                 that no value that I mentioned earlier                                 that deletes get translated into that no                                 value is a scientifica that it's allowed                                 to completely forget about that                                 particular key so when the next round of                                 log compaction happens it will simply                                 forget about that and the very nice                                 thing with this is now that because if                                 you keep updating the same key or if you                                 insert some stuff and then delete it                                 again it'll eventually disappear that                                 history will disappear from Kafka as                                 well so even though every single write                                 was replicated up to Kafka at the end                                 the size of the Kafka log is only the                                 size of the database so if you can keep                                 the database on disk without running out                                 of disk space you can also keep to keep                                 it                                 Kafka without running out of disk this                                 allows a very nice thing which means                                 which is that if you want to build say a                                 new search index you can actually you                                 know in the search index it it has to be                                 complete you want every single row in                                 your database to be included in your                                 search index so if you want to build a                                 completely fresh index you need to have                                 some kind of dump of the entire database                                 and with LOC compaction in Kafka this is                                 actually possible because a consumer can                                 simply start at offset zero at the very                                 oldest offset in a Kafka stream and it                                 just sequentially churns through slowly                                 consuming message by message and every                                 single message is applied as a writes to                                 say the search index or whatever it's                                 updating and you know maybe there are                                 multiple messages with the same key in                                 which case the later ones overwrite the                                 earlier ones no problem and eventually                                 after turning through this for month for                                 a while it reaches the head and then                                 it's got an up-to-date copy of the                                 entire data set because because we have                                 this log compaction here we know that                                 the most recent message for a given key                                 is still there in the log we actually                                 have a complete history you complete in                                 in the sense of there no missing rows so                                 if something got updated multiple times                                 then those intermediate states might                                 have got log compacted away but the most                                 recent version of every row is there so                                 you can simply turn through this entire                                 thing and at the end you've got an                                 up-to-date copy of your data in some                                 replicated system but now you just                                 continue consuming and it keeps itself                                 up-to-date so there's no kind of                                 switchover between like building an                                 index in a batch process and then                                 switching over to a stream process to                                 maintain that index and keep it                                 up-to-date that's just one way of                                 building this which is when you want a                                 completely fresh index you start at the                                 front and it may take a while but that's                                 alright and then thereafter just keeps                                 itself up to date and it just seamlessly                                 transitions from one to the other if you                                 want to know a bit about some of the                                 internals of how or Postgres and bottled                                 water actually do this this you know I                                 mentioned this is a logical                                 decoding plug-in I will just briefly                                 explain how that works so the recent                                 Postgres has managed to only launch this                                 last year even though post credit of                                 course has been around for a very long                                 time the reason for that has to do with                                 the internal data structures that                                 Postgres uses for storing its data and                                 for replicating its data so you probably                                 know what a b-tree is it's the most                                 common type of index in a lot of sudden                                 mood relational databases so you've got                                 this kind of tree structure and you                                 could by following the pointers from one                                 block to an X you can find the                                 particular record that you're looking                                 for now what happens if you need to                                 update an index if you're inserting some                                 value for example if it doesn't fit in                                 an existing block or page of this index                                 you might have to split it in two and so                                 now you have to write several several of                                 these pages as kind of one atomic unit                                 if you think about what what happens if                                 the database crashes after it's updated                                 only some of these pages of the b-tree                                 well you've got a corrupted b-tree so                                 that's not a nice situation to be in but                                 of course people figure this out decades                                 ago and they introduced the writer                                 headlock and the rule of the writer head                                 log is that before you make any changes                                 to your disk structures of the you know                                 the actual database contents if you want                                 to change a page of b-tree for example                                 you have to first write it to the right                                 head log the right head log is basically                                 this log of changes saying indicating                                 your intent of what you're about to                                 update in some of these internal data                                 structures and only once the right head                                 log has hit disk and it's been F synced                                 only then are you allowed to go and                                 modify the other disk pages and this                                 means now if the database crashes while                                 writing the log then your actual data                                 structures haven't yet been modified so                                 it's fine and if the database crashes                                 while modifying the b-tree then the log                                 contains the information of what changes                                 were about to happen and so you can                                 recover and so databases have been doing                                 this for a very long time and Postgres                                 uses the same writer header log for                                 replication                                 so if you've got a standby or leader                                 follower master/slave whatever you want                                 to call it                                 replication setup                                 the way it does that is actually by                                 shipping these chunks of log and/or                                 streaming an ongoing stream of writer                                 head log from the leader which accepts                                 the right down to the follower this is                                 different                                 compared to what's my sequel does for                                 example so my sequels replication log is                                 a logical log which actually contains                                 the kind of row level inserts updates                                 deletes whereas in Postgres this writer                                 head log is describing modifications to                                 internal data structures on a kind of                                 byte level so it's basically impossible                                 for anyone but Postgres to decode this                                 right head lock because it depends                                 entirely on all of the data structures                                 that Postgres internally has and for                                 that reason also you can't have for                                 example a leader and follower on two                                 different versions of Postgres - like                                 major versions difference because they                                 can't actually parse each other's writer                                 head log because it's so implementation                                 detail ii so if you want to do something                                 like hook into this log and move it out                                 to separate systems well you need some                                 way of kind of consuming that log that's                                 okay the writer head log and you process                                 it sequentially and it's very much very                                 similar actually to Kafka a Kafka                                 consumer in that you know it just                                 sequentially works its way through it                                 but additionally you have to somehow                                 take these internal data structures that                                 are represented in the writer head log                                 and expose them to applications as                                 inserts updates and deletes which is the                                 kind of stuff we can understand at an                                 application level and that's really                                 where this this little plugin here comes                                 in so the what logical decoding here                                 means is take the right head log which                                 describes the physical modifications to                                 the data structures on disk and decode                                 it into these logical ie insert update                                 delete type events which and which make                                 sense to an application and then there's                                 a plugin which can take those events                                 further which are callbacks and turned                                 them into bytes on a wire and that's                                 then what's the bottled water demon can                                 subscribe to and so on so this is how                                 the                                 system fits together in the end so                                 hopefully that's giving you a bit of an                                 idea of of what's going on here here's                                 here's a whole list of other things that                                 might be interesting so this idea of                                 change data capture I actually ripped                                 off shamelessly from LinkedIn's                                 implementation of change data capture                                 called data bus which is the second one                                 on this list Facebook has a kind of                                 similar issue                                 looking system called wormhole which is                                 the third one on this list so they're                                 they're described in papers and they're                                 they're very nice to read actually and                                 also give a bit of background on the                                 motivation of why these systems were                                 built and some of the details of how                                 they were implemented so for example                                 it's possible to do similar things with                                 triggers rather than parsing the log                                 which has pros and cons don't really                                 have time to go into it too much detail                                 and a few other things kind of on the                                 general topic if you're interested so if                                 you want to take a look at bottled water                                 it's it's very much alpha stage software                                 I wouldn't recommend it for production                                 use yet but it's kind of that I think                                 the code is not too shabby it's well you                                 saw it working just now it's open source                                 at this URL on github there's also again                                 the link to the book if you're                                 interested in that kind of thing and I                                 believe we still have a few minutes for                                 questions                                 you
YouTube URL: https://www.youtube.com/watch?v=ZAZJqEKUl3U


