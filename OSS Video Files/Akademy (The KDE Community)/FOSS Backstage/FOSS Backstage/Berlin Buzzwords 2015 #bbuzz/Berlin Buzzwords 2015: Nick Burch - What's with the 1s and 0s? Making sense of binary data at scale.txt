Title: Berlin Buzzwords 2015: Nick Burch - What's with the 1s and 0s? Making sense of binary data at scale
Publication date: 2015-06-03
Playlist: Berlin Buzzwords 2015 #bbuzz
Description: 
	If you have one or two files, you can take the time to manually work out what they are, what they contain, and how to get the useful bits out (probably....). However, this approach really doesn't scale, mechanical turks or no! Luckily, there are open source projects and libraries out there which can help, and which can scale!

In this talk, we'll first look at how we can work out what a given blob of 1s and 0s actually is, be it textual or binary. We'll then see how to extract common metadata from it, along with text, embedded resources, images, and maybe even the kitchen sink! 

We'll see how to use things like Apache Tika to do this, along with some other libraries to complement it. Once that part's all sorted, we'll look at how to roll this all out for a large-scale Search or Big Data setup, helping you turn those 1s and 0s into useful content at scale!

Read more:
https://2015.berlinbuzzwords.de/session/whats-1s-and-0s-making-sense-binary-data-scale-0

About Nick Burch:
https://2015.berlinbuzzwords.de/users/gagravarr

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              so ones and zeroes so what I'm going to                               talk a little bit about first                               is how we can work out what kind of file                               we've got their little bit about the                               problems and challenges of detecting                               text the languages I'll talk a bit about                               apache tika and how we can use that to                               solve some of those problems then i'll                               talk about how it can all go wrong and                                scale and what you need to start                                thinking about if you've got more than a                                handful of documents and also one of the                                interesting things are doing in secret                                at the moment which is automatically                                trying to work out if things are getting                                better or worse as we make changes so I                                can get a sense of what people are                                interested in and know which things to                                focus on and who here is mostly                                interested in search stuff who here is                                more interested in the Hadoop for me                                sauce the storm that kind of thing and                                doing a bit of data binary stuff on the                                side okay who here has used tika before                                okay right that's fine so detecting file                                types should should be easy right so if                                the files on your computer surely surely                                you know what what it's going to be and                                it's only for you so it's easy and if                                it's not on your computer well you know                                people never rename things do they the                                internet everything on the internet is                                right isn't it and web servers would                                never lie they've never get confused a                                few percent of the internet it's not                                like that's worth worrying about                                operating systems they never help they                                never hide extensions rename extensions                                launch the wrong program so yeah it's                                easy isn't it so file names normally but                                not always have extensions but there                                aren't very many extension combinations                                there's a lot more file formats than                                there are extensions                                there's no place that you can go to                                register or bank see your extension so                                everyone just picks a sensible one and                                hopes that there aren't too many clashes                                who here uses windows isn't it fun when                                you've got three different programs that                                all want the same extension and isn't it                                fun when you rename something and all                                sorts of weird stuff happens and so the                                farm names are a really quick way to                                guess what a file type is but it's                                pretty dirty and it's not always right                                so if you've got ten files it's going to                                be close enough if you've got ten                                million files it's it's going to cause                                issues and my magic most file formats                                will have a well-known structure and                                most of them will have a nearly unique                                pattern at the start sometimes these are                                called my magic numbers sometimes they                                are actual numbers often it's just bite                                patterns or Oh bite masks and things now                                ideally every file would have a couple                                of unique bites at the start that would                                let us work out what they are or at                                least a couple of bites at a known                                position that will give us a hint so the                                PDF should start with percent PDF dash                                or sometimes though it starts with a                                byte order mark and then that or                                sometimes even a few random bytes of                                crap and then that only two documents                                should start with that a fun little                                number there zip files PK                                               it's pk-                                                              structure here p executables probably                                going to have penal lull at one of those                                two upsets except when it doesn't and if                                your file start                                                        utf-                                                                   it could be an mp                                                      camera image file or it could be                                something else so often but not always                                handy another challenge is if you see a                                file that starts PK x                                                  a zip file but it could also be an                                   XML file a dot docx xlsx which is                                wrapped in a zip file could be and I                                works file it could be an audio file it                                could be a whole bunch of other things                                so just because we've got a magic number                                it doesn't always uniquely identify the                                file format and then if it's a                                text-based one an interesting ticker bug                                this last week which was how do i tell                                 if this is a matlab file or a see macro                                 file because they're both text-based                                 file formats there's no header on the                                 front the comments aren't always great                                 so it can help especially on binary                                 files but it's no magic bullet so I                                 mentioned already container formats if                                 it's a zip that's a challenge could be a                                 no xml docx or something open document I                                 works pub all that sort of thing if you                                 get one of the media container formats                                 like an org like an mp                                                   could be video could be audio could be                                 combinations of them just because you                                 found the mp                                                            you can say all that's music or that's                                 video immediately so it gets gets tricky                                 to work out what's going on and there's                                 also some fun stuff you can do if you                                 like confusing people you can build a                                 microsoft binary file early to structure                                 that has multiple streams in it and then                                 if you rename it from say xls to doc you                                 get a different document that opens it's                                 a sort of fun little practical joke for                                 stena graphically hiding stuff but kind                                 of throws up some interesting problems                                 we're here the file extension on windows                                 will cause it to open in different                                 applications and show different                                 information                                 so what can we do with containers if                                 we're trying to make sense them well the                                 easiest one is that we can use a mind                                 magic to work out what container is and                                 then we have to open it up and start                                 looking through so if it was a zip file                                 and it contains an underscore rails rels                                 it's probably our XML if it's an OGG                                 file then we can look for a CMM meld                                 stream and then pass that and start                                 looking for descriptions that tell us                                 what's going on but this isn't quick so                                 when we had the filename stuff if we                                 found that it was doc txt we can very                                 quickly say it's probably txt if we                                 found that it's it's an August ream we                                 now have to stop passing all the old                                 packets going through the page structure                                 finding the streams finding the sea FML                                 stream pass that look in it see where                                 that tells us about whether this is                                 going to be a video or an audio and                                 hence what parts are you going to use to                                 get the text out which is probably what                                 you wanted in the first place because                                 I'm assuming that most of you here what                                 you've got is a whole load of files and                                 what you want is a whole load of use of                                 information so you're going to have to                                 choose how much work you put in and                                 that's going to be different for all of                                 you it's can be different based on your                                 needs but sadly it's not a quick thing                                 you've got to tune the dial based on the                                 amount of memory the amount of                                 processing time to come up with the                                 answer of what this thing is and only                                 once we know what it is can we start                                 getting the information out another one                                 that you can sometimes do is look for                                 patterns inside the file and there are                                 some scientific data formats that don't                                 appear to have any kind of known                                 structure on them so there's some work                                 that's going on at the moment where                                 people are using histogram patterns and                                 they analyze the the whole file and they                                 look for certain common repeating                                 patterns and repeating structures and                                 use that to work out what the file might                                 be that's also of use if you've maybe                                 got corrupted files or truncated files                                 so if there's anyone in the audience is                                 doing any stuff around digital forensics                                 where you may have eighty percent of the                                 file if you've missed the first hundred                                 and twenty eight bytes of it                                 missed all that lovely magic number                                 stuff so you won't have those there and                                 then you're looking at this date renew                                 going well is this most of a PDF or is                                 this most of a text file or it's just                                 the hard disk is dead so you can do some                                 stuff around that finally what you're                                 probably going to want to do is combine                                 all of them you're going to take the                                 filename and use that as a hint you'll                                 take the my magic use that as a hint if                                 you looked inside the file that's                                 another hint it's going to have impacts                                 on the speed and accuracy and another                                 thing that's you maybe want to do is do                                 a bit of machine learning and find out                                 what's the right waiting for your target                                 data set that means a bit of work up                                 front because you're going to have to go                                 through and say yes that really is a                                 Word document whereas that thing that                                 claims this a text file is actually a                                 PDF and then learn the right combination                                 so that your system will predict that                                 from the given inputs which can I mean                                 yes it's more work but it can increase                                 your accuracy so it just depends on what                                 you're going to care about okay so that                                 was that was for our binary stuff what                                 about text text is easy isn't it who                                 thinks text is easy good the cynicism is                                 working all you've fallen asleep so in                                 coatings                                                               your file or I'm sorry if you've got a                                 letter A that you're going to be                                 encoding you could write it as                                                                                                                   all your kind of typical pc ascii style                                 formats or you could have a main frame                                 in which case it's been written as                                                                                                           you're dealing with a lot of old file                                 formats they're all going to be using                                 one byte per character which just remaps                                 it all sorts of different ways so if                                 we've finder the first character in our                                 file text valley                                                        the most common characters that that                                 could actually be from the most common                                 single byte encoding things                                 some file formats are helpful and                                 they'll tell you what encoding the next                                 stream of text is in but if it's a plain                                 text file or a lot of the email file                                 formats and stuff it's just a bunch of                                 ones and zeros and you've got to work                                 out what that's going to be so you can't                                 actually extract the text from your txt                                 file until you know what encoding it's                                 in so how are we going to work out                                 whether our file is going to be an                                 Arabic or Hebrew or Greek or French the                                 good news is that you can often find the                                 language and the encoding at the same                                 time so if we see some accents like that                                 it's probably not going to be English if                                 we see lots of words starting with s                                 it's probably not going to be Spanish on                                 the other hand if we see lots of words                                 that start es it's much more likely to                                 be Spanish so what we can do if we've                                 got enough of a corpus of text available                                 to us we can go through and find these                                 common patterns for certain languages                                 and we can find these common patterns                                 for different encodings of the same                                 language and then we can use that but we                                 need a lot of text if I have one                                 sentence it's going to be very hard to                                 work out definitively what that is if I                                 have a lot of text it's going to be                                 easier if I have mixtures so if I've got                                 this slide here and I save that as a                                 text file some simple analysis tools                                 might get confused and they'll go oh                                 look there's all these accents in it it                                 can't be English but it's mostly English                                 but I put some accents in but if I've                                 got the whole presentation saved as a                                 text file then you can ignore those odd                                 little rounding errors from the Act the                                 odd accent I've put in and still work                                 out most of what we've got okay who                                 knows what an n-gram is Oh excellent                                 everyone okay so all I'll say then is if                                 you've got enough text you can build up                                 the common in grams from each language                                 you can use tools like I convey and I                                 to you for J and that kind of thing to                                 take your sample engrams translate them                                 into all the different encodings that                                 you might be dealing with and then you                                 can do an n-gram match and say aha this                                 is spanish in iso                                                  that's the highest likelihood for these                                 given Engram patterns and so then what                                 we can end up with when we started with                                 something with a dot txt extension that                                 may or may not have been text we can                                 apply the engrams on it and go ah this                                 is French and this encoding this is                                 German in this encoding this is Korean                                 in this to light eastern encoding which                                 there means we can read it in because we                                 know what encoding it since we know how                                 to get the characters out and we know                                 what language it's in which means if                                 we're going to be indexing and searching                                 based on it we know what kind of                                 analyzers and stem is to apply to it                                 because if it's in French you're going                                 to need to use different techniques                                 breaking it down into words than if it                                 was in Korean so that's handy because we                                 can tackle by at the same time it's a                                 few things that it can go wrong with if                                 you've only got that tiny little bit of                                 text it's going to be hard to work out                                 what it is if we've got a load of                                 control characters and garbage at the                                 start of the file you may mistakenly                                 think it's not text so again that's for                                 any of you in the room who are doing any                                 forensic stuff or dealing with any weird                                 files where a few random control                                 characters end up on the front you might                                 not you might think oh this is some                                 random binary file when it's actually                                 text some encodings are very similar so                                 the difference between iso                                               is I believe the euro symbol so if                                 you've not got any pricing information                                 in there you can't tell the difference                                 between them but on the other hand if                                 you've not got any pricing information                                 there does it matter if you pick one or                                 the other because you're not using that                                 Co point so                                 those are the techniques for working out                                 what our binary files are and what our                                 text files are what what about if you                                 don't want to coat all those up for                                 yourself so Apache tikka is like a Babel                                 Fish for content it helps you work out                                 what your file is then it helps you                                 extract the metadata from it in a                                 consistent way so that you don't need to                                 know that a word document calls it                                 summary and a PDF calls it short title                                 and an mp                                                             tikka find out what this file is and                                 then tell me the thing that most closely                                 matches the dublin core subject and the                                 teacher can go off and do that for you                                 and give the information back and for a                                 lot of you in the room you're interested                                 in search it will give you back nice                                 simple plain text which is handy so                                 these are some of the the file formats                                 that it's going to support out of the                                 box what tikka does on the whole is wrap                                 other libraries so that you don't need                                 to know that the best library from PDF                                 is PDF box and for a word document its                                 POI on the whole tikka doesn't have its                                 own code for handling given file formats                                 what it tends to have is calls out to                                 all the appropriate libraries that will                                 do it for you so it's got all your usual                                 office file formats and an HTML and PDF                                 and all that sort of stuff but also                                 increasingly it's got a lot of                                 scientific file formats supported in it                                 and then audio and video and image file                                 formats as well and it's got hooks into                                 the tesseract OCR tool that if you've                                 got an image file you can then hand that                                 off to tesseract which will give you the                                 text and then you can give it to exit                                 tool or something like that and get back                                 the EXIF information that'll mean that                                 you can work out that that particular                                 photo was taken in front of brandenburg                                 tour with someone wearing an i love                                 Berlin t-shirt on it and so you can pull                                 out the metadata and you can also get                                 the text from it                                 which is great because before you have                                 that you just had an image file and you                                 couldn't index it and you can do                                 anything with it and now all of a sudden                                 you can say give me all the images taken                                 in Berlin that mentioned the word Berlin                                 in them and get it that way so few                                 different ways of cooling tikka there's                                 a command-line tool you can call it from                                 Java it's got osgi bundles as a network                                 server and it has a solar plugin who                                 here uses the solar plug-in fatiguer ok                                 don't put too much data through it                                 because it's in the same jvm so we're                                 going to come on a bit later to the ways                                 it can go horribly wrong I was having a                                 chat with some of the the solar                                 committers last night and they say that                                 they absolutely love it for demos and                                 they absolutely hate it when it gets put                                 into production because then it falls                                 over and wrecks everything so and you                                 hit right stuff in Java you hear right                                 stuff in a JVM language not java who                                 here uses some other language ok so for                                 those of you not using Java the main two                                 ways I'd suggest you use our batik a nap                                 and the network server for those of you                                 in Java you can pick any of them                                 whichever one you you prefer so the                                 ticker app is a single runnable jar it's                                 getting quite fat these days because                                 it's got all of the dependencies bundled                                 into it but you just take it you run it                                 and it can give you the detection so you                                 can say Tico what is this file if it's a                                 text file you can also do the language a                                 texture and say you know what white                                 coating is this what language is it it                                 will give you all the metadata back it                                 will give you the plain text and the                                 XHTML it's really good for testing it's                                 really good for demos but you're                                 spawning a new JVM each time so if                                 you're going to be running this                                 thousands of times an hour on a handful                                 of machines that's maybe not going to                                 work out so well because you're going to                                 be paying                                 yeah maybe a second to spawn a JVM and                                 then                                                                  that's not necessarily going to be a                                 great thing but on the other hand if                                 your application is mostly dealing with                                 simple plain text and only very rarely                                 you're going to get these binary files                                 maybe it is okay to spawn a new JVM once                                 in a blue moon to pass the old file and                                 increase your coverage that way it's a                                 bit of a trade-off going on there the                                 tikka server is a restful server you                                 just put files to it and then it will                                 give you back the results be that the                                 detection or the text or the metadata so                                 if you're running in a non Java language                                 and you're going to want to put a lot of                                 stuff through or even if you're running                                 in a java language and you want it in a                                 separate JVM so if it falls over it                                 won't take out your main one it's worth                                 running the server its standalone single                                 jar you run it up comes the server you                                 start putting resources to it but it's                                 very easy to have a little watchdog that                                 spots when that falls over and respawns                                 it and then that's completely separate                                 from your main application so if you are                                 doing solar and you're doing                                 elasticsearch at any kind of scale the                                 generally recommended way is you find                                 that you've got a binary file you post                                 it to the tikka server get back the                                 response hand that off to your search                                 engine and then if it all goes horribly                                 wrong you've not just wiped out a third                                 of your elastic search cluster which is                                 not ideal there's a fair bit of                                 documentation available on the wiki                                 about it or alternatively if you just                                 launch it and go to it in your web                                 browser it'll tell you in summary what                                 all the endpoints are and what they do                                 for those of you who are running in Java                                 and want something nice and simple the                                 tikka facade is the easiest way to call                                 it it has methods there that say you                                 know I have an input stream and I want                                 plain text I have a file and I want                                 XHTML so you're just very simple method                                 calls the only downside of the tikka                                 facade because it's all nicely wrapped                                 up in for you is there's no easy way to                                 say did I forget to give you half the                                 jars                                 what will happen is that you'll give                                 tika an inputstream and get back nothing                                 and the reason you got back nothing is                                 you didn't have any of the required                                 Giles there to handle that file format                                 so TQ will have worked out what it was                                 and then said well this is a PDF and                                 I've got no PDF library so I can't help                                 sorry the direct calls calling all to                                 detect parser with ticker input streams                                 with the Tiki config gives you full                                 control lets you do exactly what you                                 want to do and also you can ask the                                 config object what passes did you find                                 what mimetypes did you find and if he                                 comes back and says I know about three                                 mine types and you'd be like well where                                 did the other                                                          we stuff something up in the deployment                                 or you ask you about what passes it                                 supports and it comes back saying three                                 not                                                                      that something went wrong in their                                 deployment and you can then fix that as                                 a mentioned because tikka wraps a lot of                                 these other libraries for you there's a                                 lot of these dependencies that need to                                 be present at runtime for it to work and                                 so a lot of the things that go wrong for                                 people seem to be when their deployment                                 system deploys ticker and mrs. all the                                 rest now another thing you might be                                 interested in is the forked parser which                                 fires off a separate JVM to do the                                 passing so that if something goes wrong                                 with tika it's a set of a JVM that falls                                 over not your main one okay another                                 thing to flag off about tika is it tries                                 to be consistent across all the file                                 formats mapping all the different                                 metadata onto their common things giving                                 you simple HTML back simple plain text                                 back so the word parser will tell you                                 that it's a table it will tell you that                                 this is a heading                                                        style save as HTML all the bold italic                                 random styling things which is probably                                 a good thing for most of you because                                 you're mostly interested in search but                                 if you want to do a rich preview of the                                 search results be aware that some of                                 that information will have been thrown                                 away by default and if you really need                                 to show that that bit of text was in red                                 then you might need to use a different                                 different parser I won't mention too                                 much on extending ticker just to mention                                 that if you have your own mimetypes that                                 aren't supported out the box you can                                 just drop an extra file onto the glass                                 path and take your pick that up and add                                 in your Union mine types and the tikka                                 has                                                                  available out of the box and if you need                                 to add a new one it's only                                             code plus whatever is required for your                                 file format which is going to be more                                 work but if there's a particular file                                 format that you're dealing with a lot in                                 your environment where you've may be                                 licensed in or written your own parser                                 your own library for dealing that file                                 format it's fairly easy to then plug                                 that into tikka and start indexing okay                                 quick note on embedded resources it's                                 not just containers if you've got an                                 office document you can put images in it                                 you can put other office documents                                 inside it teka will try and tell you                                 where it found an embedded resource and                                 give you a way to get that embedded                                 resource and it's then up to you how you                                 handle it maybe you'll want to inline                                 all the text from that embedded office                                 document in another office document or                                 maybe you want to treat them as separate                                 resources and index them separately or                                 stall them separately that's that's a                                 choice that you can make okay how does                                 it all go wrong ok so at scale you're                                 going to crumb across a lot of junk                                 documents or the corrupt documents a lot                                 of messed up documents you're going to                                 hit a lot of edge cases and one percent                                 of the internet is still a very large                                 amount of data so you're going to have                                 to plan for files that are going to be                                 misidentified files that are going to                                 trigger weird bugs in the libraries just                                 going to have to plan for failure so as                                 you're passing your particular set of                                 data you're going to get a lot of files                                 that are not supported                                 that haven't actually got any mind type                                 defined for them maybe a some new type                                 of file so one thing I'd suggest is if                                 tika can't work out what your file is                                 log the first few bites there's a                                 reasonable chance that they'll contain                                 something a bit like a magic number and                                 then maybe once a month once every six                                 months go through and identify the most                                 common bite patterns and use that to                                 work out where to spend your time don't                                 just say the first file that we couldn't                                 identify was food up in so let's work on                                 that you want to look at it and go three                                 percent of the files couldn't be indexed                                 but they all have the same white patent                                 at the start so maybe that's a whole new                                 file format that we want to go off and                                 add supporting so then we can index it                                 better search it better if Deacon knows                                 what the file is but has no passes and                                 that's easy you know just lock the mime                                 type and then go on adding a parcel                                 later but a lot of what you'll come                                 across you'd be like I don't actually                                 know what this is but I've got ten                                 million of them so maybe it's worth                                 spending some time finding out what's                                 going on so sometimes teka will miss                                 identify your file and will hand it off                                 to the wrong passer and so some of those                                 files can then cause the the passes to                                 do silly things if you manage to break                                 teka with a file you can share please                                 tell us and we want to work on it the                                 problem comes is we sometimes get bug                                 reports sometimes a document you can't                                 have that I can't even find sometimes                                 breaks things not really an ideal bug                                 report but if there's any of you out                                 there who are crawling the internet if                                 you can come to us and say sometimes                                 this file here at this URL that's                                 publicly available breaks that's awesome                                 because we can use that in a test case                                 we can build on top of that but do be                                 aware that every so often you will get                                 an out memory you will get a hang and if                                 the file fails once because of one of                                 these errors and you're running on top                                 of a clustering system something like                                 Hadoop or storm that's going to keep                                 retrying if the problem is with the file                                 then every time you give that file to                                 another no                                 you're going to wipe out that node as                                 well so maybe you want to say for these                                 kinds of errors just give up but look it                                 because maybe when you upgrade to the                                 new version of the library and we fix                                 that bug then you want to retry all of                                 those failed files so I know some                                 content management systems that build on                                 top of tika have a special marker where                                 if they can't index the file they put a                                 marker in the database that says there's                                 stuff in this file that we can't handle                                 so that when you do an upgrade you can                                 then retry all of those files and see if                                 you can now start indexing some more of                                 them so don't think of it as a permanent                                 failure be aware that mere maybe in six                                 months time you can you get a search                                 those files and especially if it was the                                 file format that you couldn't identify                                 before and you've now under support for                                 you're going to want to go through re                                 add them into your index and expand your                                 index coverage and because occasionally                                 t he will break your jvm seriously                                 consider putting it in a in a separate                                 and separate JVM separate process but                                 then if it's a very rare thing maybe                                 have a pool of tika servers if it's a                                 really really rare thing that you're                                 calling teak and maybe it's not the end                                 of the world if every so often your jvm                                 dies but if you're making heavy use of                                 it if you're doing a web-scale cruel and                                 a lot of the files you're coming across                                 they're going to be binary that you                                 going to be passing through tika think                                 about separating an outfit when it all                                 goes horribly wrong okay this is a bit                                 of an extreme example the top is what it                                 looks like in Adobe Acrobat the bottom                                 is what it looked like when sent through                                 Apache pdfbox when there was a                                 particular nasty bug that was causing                                 corruption and one of the things that                                 PDF file format does to improve                                 efficiency is it will remap some of the                                 characters on to more common short forms                                 so that it's a moral compressed encoding                                 and then at extraction time you have to                                 go bite position to actually maps onto                                 this file this this character here and                                 if that goes wrong then you end up with                                 garbage coming out rare but not unheard                                 of and it's going to be hard if you're                                 just blindly indexing it to them make                                 sense of it another problem don't know                                 if you can see in the background here                                 this is from someone CV so that there is                                 their statement of work and then this is                                 all of their job experience another PDF                                 bug here the bit in bold is all that we                                 were able to pull out of this person CV                                 it's very hard to search for missing                                 text searching for a negative is hard so                                 if you were just running searches to                                 check the data was coming through it's                                 going to be a really tough problem to                                 spot that something's gone wrong and you                                 didn't get that piece of data in the                                 first place obviously you can't look at                                 every single document your indexing by I                                 and compare it side-by-side but you need                                 to be aware that if its information has                                 been lost it's very hard to spot that so                                 some of the things that will go wrong                                 already mention all the out memory the                                 infinite hangs memory leaks running out                                 for our handles and stuff exceptions are                                 easy because you can catch them and lock                                 them if there is missing text or missing                                 method ater that's hard to spot                                 sometimes you'll get extra text and                                 there was a bug recently in the power                                 point extraction where we got bit                                 overzealous and started pulling through                                 all of the placeholder information from                                 the slide masters it's not that bad                                 really if you pull out extra information                                 but it can then mess up some of your                                 counts so if you say oh well the most                                 common word in this PowerPoint used to                                 be buzz words but now it's insert text                                 here yeah it's going to mess with                                 statistics that's not too bad the gobble                                 text is a pretty bad one rare but does                                 happen                                 within tikka and hopefully within a lot                                 of your applications as well you can                                 have unit tests and they're going to                                 tell you if anything really obvious has                                 broken but probably you're earning on                                 how handful of files available that you                                 can test against really you're going to                                 want to run testing on a much larger                                 scale but if you've got                                                  can check them I i if you've got                                        you can't if you've got terabytes                                 there's no way you've ever going to be                                 able to check that except with another                                 computer process you need to be aware                                 that file distributions are uneven the                                 kind of files that you are dealing with                                 are not necessarily the kind of files                                 that they're dealing with are not                                 necessarily the kind of files that are                                 in my unit test suite so something that                                 will pass and go green for me is not                                 necessarily going to be awesome for you                                 so you can't completely share that the                                 test systems and you are going to need                                 to check even though teka have released                                 it and it's all gone green and then                                 notch included it and that was all grain                                 that doesn't necessarily mean it's going                                 to be error free so you can need to do                                 automation of the running and of the                                 analysis so if it's just a number of                                 exceptions that's pretty easy because                                 you can log them metadata is relatively                                 easy because there's not much metadata                                 right each file so you can maybe log                                 that for a subset of your files and then                                 check between two versions that you're                                 still getting about the same amount of                                 stuff out attachment counts easy this                                 word document used to have three                                 attachments now it's got none probably                                 that's a bad thing or something's gone                                 wrong and you need to look at it junk                                 text is relatively easy because you can                                 look at the entropy and you can look at                                 the common words and if the common word                                 for your document used to be forests and                                 meadows and now it's Eirik                                 work probably that's that's a sign that                                 something's gone horribly wrong missing                                 text is a real tough one if you look at                                 it by eye you can spot that but                                 otherwise and how are you going to work                                 it out and at the moment for testing                                 tikka we're using Gulf docks one which                                 is from                                                                 lot of the new file formats and we're                                 using some stuff in common cruel but if                                 any of you have large data sets of                                 binary files that you can share with us                                 we do really like them so that we can                                 run the kind of automated analysis on                                 them one of the things that um nope                                 where are we one of the things we're                                 doing is with the tikka eval tool is we                                 run for hours and hours and hours of                                 machine time tikka at different versions                                 and we different proposed patches in                                 against large corpuses and then we                                 report the number of exceptions found                                 and the occurrences of them and we                                 report the most common words for certain                                 file formats and then we check that                                 between two different runs so if someone                                 proposes a patch we can then see whether                                 or not it is on the whole got better or                                 worse which tends to mean ideally that                                 we can catch a lot of these bugs which                                 would cause the PDF to come out like                                 that before we apply it and before you                                 start deploying onto your cluster but if                                 you've got a lot of custom passes that                                 you're adding in for a custom file                                 formats you might want to look at                                 running this and just look at the                                 results and say did it get better did it                                 get worse how did it get rid of how did                                 it get worse it's not going to tell you                                 for certain if it's better but it's                                 going to give you a very narrow set of                                 things that you as a human can go and                                 check the top ten most common problems                                 the top ten most common differences                                 between these two things that's great                                 because you can look at it and go that's                                 fine that's fine that's not                                 end of the world or we need to solve                                 that one you couldn't look at a million                                 documents but you can look at                                           repeating failures and analyze that and                                 then decide what to work on so whatever                                 you're doing with your kind of analysis                                 pipelines and so on eating is worth                                 trying to summarize them as you're going                                 to have too many errors to look at but                                 if you can get a common subset together                                 and then once a month or once a week or                                 once every six months you spend some                                 time and look at that you can tackle the                                 most important things and you can                                 improve the quality that way rather than                                 picking one at random teka batch is                                 what's powering teka eval at the moment                                 it's only running on a single machine it                                 spawns off a whole bunch of different                                 ones there is work on going at the                                 moment we were hoping it was going to                                 finish now but it hasn't been to get it                                 working on Hadoop so that you could say                                 tikka here is an HDFS area I want you to                                 run do all the analysis and give me a                                 nice summary out afterwards and there's                                 also some work going on to make it work                                 on storm the same kind of thing here is                                 a million documents go off and process                                 them and give me the text so I can then                                 feed into and index and give me the                                 metadata that can go into the index and                                 tell me what went wrong if you're                                 wanting to do that now and you wanting                                 to run on top of Hadoop have a look at                                 this wiki page here it's got a whole lot                                 of pointers or alternatively just use                                 BMS which is a text indexing and                                 processing suite that can also do some                                 natural language processing at the same                                 time that starts off with very large                                 collections documents and outputs very                                 large collections of useful information                                 um got mount a minute left so I'm just                                 going to mention these are some of the                                 other Apache projects you might be                                 interested in notch is the Hadoop                                 powered web crawler and then indexing                                 analysis storm crawl there's a talk on                                 that a bit later on that's looking to                                 reimplement notch on top of storm ne                                   is great if you've got a lot of triples                                 a lot of semantic data open NLP and you                                 EEMA a great for the natural language                                 processing                                 so once you've taken your word document                                 and you've pulled out some useful text                                 you might then want to do some sentiment                                 analysis and find what sort of things                                 they're talking about in this document                                 sea takes is great for anyone in the                                 medical or farm of space that takes                                 things like open NLP and you eema and                                 has a training data set already                                 available so that you can then just use                                 it batteries included then if there's                                 any other ones people want to suggest                                 for non Apache stuff there's the                                 hardened tikka project there's bmus if                                 you're doing any of the language                                 detection and encoding detection those                                 three lengths the tools may be slightly                                 better as well
YouTube URL: https://www.youtube.com/watch?v=oxiXxUtvgnQ


