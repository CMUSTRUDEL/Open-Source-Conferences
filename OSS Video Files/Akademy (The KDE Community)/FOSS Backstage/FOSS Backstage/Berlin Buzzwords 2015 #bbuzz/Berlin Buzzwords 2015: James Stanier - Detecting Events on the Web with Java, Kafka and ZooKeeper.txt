Title: Berlin Buzzwords 2015: James Stanier - Detecting Events on the Web with Java, Kafka and ZooKeeper
Publication date: 2015-06-03
Playlist: Berlin Buzzwords 2015 #bbuzz
Description: 
	Brandwatch is a world-leading social media monitoring tool. We find, process and store nearly 70M mentions from the Web every day from our crawlers and firehoses. As the amount of data we find continually increases, the harder it becomes to separate the signals from the noise for our customers.

Over the last year, we have been building, experimenting and scaling a distributed cluster of JVMs that process the data from our client’s queries and detect influential mentions of their brands online and alert on unusual and important trends. We used Apache Kafka, ZooKeeper, and Spring to achieve this.

This talk explains the architecture that we built, how it performs, scales and some of the difficulties we’ve faced along the way. We’ve learned a lot in the process. We hope you'll learn from us too!

Read more:
https://2015.berlinbuzzwords.de/session/detecting-events-web-java-kafka-and-zookeeper

About James Stanier:
https://2015.berlinbuzzwords.de/users/james-stanier

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              yeah thanks this is the point where                               whether my colleagues have actually come                               but I can't see because the light                               I hope they're here yeah so I'm going to                               talk to you today about how we built a                               real-time event detection system and I'm                               going to start by going through a bit                               about what the company that I work for                               does and show you how the platform's                                transform me away from a kind of                                historical data analysis platform into                                something with people want more                                real-time stuff and our first iteration                                towards doing that if I go any further                                I'd seen team to speak quite loud is                                that is that okay thumbs up yeah cool I                                don't to shatter your eardrums so yeah                                whoa this little clicker is going crazy                                there we go and so we're going to spend                                                                                                       more seconds talking about brand watcher                                which is a company that I work for and                                the problems that we're facing we're                                going to talk about how we move to calf                                go because Kafka's awesome how we                                process data in this event detection                                system and then how we scaled up and we                                distributed this work into a cluster and                                then from this event detection system we                                talked about how we found some meaning                                from the things we were detecting in                                order to deliver interesting emails to                                our clients and I'm not going to use his                                clicker anymore so Who am I and it                                appears to be the laptop actually okay                                I'm James and that's about the smartest                                that I've looked in a very long time I                                was at a wedding and I usually look like                                this every day I do have different                                clothes I not wear the same thing I'm vp                                engineering at brandwatch and we're a                                start-up and a bit bigger than a regular                                starts at now about                                                   our post series be around going in Serie                                C and we're based in the UK we've got                                office in Berlin and should have gotten                                San Francisco and New York and various                                other places and what do we do I pletely                                having some AV issues                                got a very sticky arrow on the keyboard                                it seems living right for every four or                                five steps forward will have to go a few                                steps back and so what do we do and we                                do effectively social media and web                                listening so we have lots of web                                crawlers that are crawling data on the                                web all the time and we're processing                                analyzing and storing that data so that                                our customers can look into their                                dashboards and find out interesting                                stuff that's going on let's have a look                                at what the app looks like it's easiest                                way to explain it it's a web app when                                you log in if you're paying it's lots of                                your nice money to use our software then                                you can set up your dashboards which                                show you things that you're interested                                about your set up queries and then from                                your query will process the web for data                                will analyze it and will present it then                                there's these flexible dashboards where                                you can drop the data up you can tag it                                categorize it chart things like things                                and find out things that happening so                                we've built the platform in such a way                                that we allow the users to and analyze                                the data themselves I mean this is a                                custom kind of rules in categories that                                they've set up based on particular                                tweets and forum blog post coming in and                                taking them with particular car brands                                and you can chart it by sort of the two                                pages where things came from over the                                days that those things happened and                                there's a lot of flexibility in this                                platform that's what we've done really                                well over the last few years we've built                                this great framework for storing and                                processing data and we do various                                different types of classification for                                example we do demographics                                classifications based on people's                                Twitter profiles so for example if                                they're male or female what things they                                talk about the topics of conversation                                they talk about and what we try and                                guess what their professions are and                                their interests based on what they're                                talking about online and we use rules                                 base classifiers and machine learning in                                 various different techniques for this                                 and it's not just the top level metrics                                 that you get in the app and we've                                 indexed everything that's matched our                                 customers queries back to                                             have quite a large solar index                                 use a lot of age base and post res and                                 various other things which I won't be                                 covering in this talk because this is                                 about this is a real time thing that we                                 built on top of our platform but we've                                 given talks before about various things                                 that we use so just some numbers if I                                 fight with this there we go and we've                                 got about                                                          crawlers and this was kind of the first                                 thing that was in our product we wrote                                 them mostly in house at the time many                                 years ago and we've extended them ever                                 since and we're probably crawling about                                 hun tens of hundreds or other hundreds                                 of millions of web pages a day which we                                 then matching in memory to our customers                                 queries we also have historical queries                                 where when you can set it up we can                                 match against our historical index that                                 we've got the goes back all those years                                 you can get in some context in your data                                 so you're not starting with a blank                                 canvas when you select people searches                                 before we go any further is the                                 microphone distorted slightly just a                                 little bit is that okay is that better                                 or not yeah cool I think that's okay                                 okay I'll carry on we process and                                 twisted data directly from Twitter and                                 we get the deck of hose we use power                                 track for all the terms that match our                                 customers queries we get dates from                                 Weibo and discuss in many different                                 places we've got lots and lots of fire                                 hoses and different things coming into                                 our system and there's about                                            plus matches to customer queries every                                 day so that's data going into those                                 dashboards I shows you about does that                                 all make sense so far some nodding good                                 so the platform itself was used for a                                 lot of historical and analysis and I'm                                 extremely sorry about this but this                                 clicker is infuriating                                 it's okay I'll try and fight with it and                                 so we release this product the other                                 year and called vizia so rather than                                 just sitting in front of your computer                                 and digging into your dashboards we now                                 have this really cool infographics                                 products where if you're a company you                                 can come along and you can have us                                 install these awesome infographics in                                 your lobby in your bank in your office                                 in your command center in your business                                 and they're a great kind of point of                                 conversation and they were driven by the                                 platform that we've written over the                                 last few years and that's great this is                                 this is the first step of people getting                                 away from their computers to use our                                 data and as time goes on you know you've                                 got people accessing things on their                                 phones on their tablets so increasingly                                 people are kind of away from the                                 computer when they're looking at our                                 data and people want to find out what's                                 happening right now really and here's an                                 interesting picture from this is jag you                                 were the car company does anyone own a                                 Jaguar anyone like to earn the jugular                                 German cars a cool head so this was                                 there at their war room during the World                                 War during the super bowl halftime so                                 you've got the super bowl adverts going                                 up at the halftime in the middle there                                 being projected and they're using our                                 video screens to display the data and                                 they're on the laptop in the brand watch                                 app and looking at all the data coming                                 in in order to sort of see how their                                 adverts are going down and increasingly                                 this is happening in many different                                 organizations and people want to be able                                 to react to crises when they happen so                                 for example if there's a big PR crisis                                 with your brand or your organization you                                 want to be able to react first before                                 you know the press pick it up before and                                 anybody else gets there and you can                                 prepare your response and you can be                                 composed when these things happen so we                                 have a new challenge as well as this                                 clicker and we have a new challenge so                                 what this is is that in that picture                                 that we had before there was a spike and                                 people if they're logging into the app                                 to see their data they might not have                                 seen that when it happened so is there                                 any way that we can build a system that                                 can tell people when they're away from                                 the app who maybe that was a Sunday                                 maybe it was a Saturday people weren't                                 actually logged in using the application                                 tell them that something interesting is                                 happening so                                 can then log in and deal with a crisis                                 so this is sort of signal from the noise                                 problem so this is the challenge we have                                 about it's working now that's wonderful                                 oh it's not such a shame we've got about                                                                                                       those are lots of streams of data that                                 we're processing and matching to                                 different peoples queries and we've got                                 about                                                                 into those if we were to use our data                                 stores that drive our platform in order                                 to do say for example a traditional kind                                 of SQL style breakdown query of what's                                 happening right now what's happening                                 right now it takes over eight hours to                                 go over each one individually so using                                 the platform that we've built just won't                                 really handle this kind of thing so we                                 need to build a new system that will be                                 able to do it and that's what I'll talk                                 you through today so this is a rough                                 overview of what we built and on the                                 left-hand side you've got our web                                 crawlers                                                               the middle of this because it did a                                 wonderful job of piping our data around                                 in this and all of the mentions that                                 were matching people's queries we kind                                 of liberated from the crawlers and we                                 just put them into Kafka and then we                                 built this self-organizing cluster which                                 we called our signal processing cluster                                 that's processing the data from all                                 these queries and finding out when                                 something interesting is happening when                                 it's found something interesting it                                 pipes out on a nuke Africa topic of                                 events and then we have other processes                                 pulling on those events looking at them                                 analyzing them trying to see whether                                 there's something relevant to show our                                 customers and then we also store them                                 today to stores at the end it's a first                                 think Africa quick show of hands who has                                 used kafka who uses it in production                                 unless it's still a fair amount and                                 who's had good experiences with it yeah                                 I mean we've we've had a really good                                 time where you pretty much depend on CAF                                 canal for all the things that we do so                                 the step one was get the matches to                                 people's queries and put them into Kafka                                 so what is Kafka it is who doesn't know                                 what categories it's cool if you don't                                 that's cool so there's this bit of the                                 talk and those of you who know lots                                 about it do bear with and it's a                                 publisher subscribed messaging system                                 and there's been so many talks today                                 that talk about it and                                 but unlike a traditional message broker                                 it's a distributed commit log so I've                                 seen it also described as the sort of                                 the UNIX pipes of streams and it became                                 an Apache top level projects in november                                                                                                       behind calf grove done a really amazing                                 job of providing great documentation is                                 really easy to install and get it set up                                 and just try it out and and that's the                                 kind of thing that drew us to the                                 project because when there's lots of                                 care putting around the edges you can                                 usually tell that it's a good project                                 underneath it's pretty fast hundreds of                                 megabytes of rewrites per second from                                 thousands of clients it's scalable and                                 you have your Kafka nodes in a cluster                                 and it's partitioned over many machines                                 it's replica replicated rather and you                                 can expand it without downtown you can                                 add new nodes into the category Custer                                 and it just gets on with this thing and                                 it's durable and so messages that you                                 send to a Kafka topic or persisted to                                 disk which sounds a bit weird and then                                 they replicated in the cluster and                                 written to disk is a bit strange but                                 this is a graph from a cmq from a couple                                 years ago and the speed of sequential                                 reads from disk is actually really good                                 I mean it's comparable if not better                                 than the random access in memory and                                 because the Linux kernel does lots of                                 clever stuff with paging so if you know                                 where you are on disk with your                                 particular message you're consuming and                                 you know that everything's stored                                 sequentially contiguously then then                                 streaming the rest of them is actually                                 quite a fast operation and storing to                                 disk is really cheap and it's persistent                                 which is great and this is a graph from                                 Twitter themselves so they in their                                 infrastructure that sends out data to                                 people like us they're using cash core a                                 lot and the blue lines on here are a                                 load test of sending a whole other                                 tweets at Kafka that red line is a                                 consumer that's pulling them and you see                                 that in their load test there gets to                                 this point where the red line doesn't                                 keep up with the blue line because it                                 can't keep up anymore well that's not                                 actually a problem because the consumer                                 is reading from disk and messages are                                 being pushed to disk they're just                                 getting put in a log so the consumer                                 can't keep up but it's okay because                                 after a while it does catch up and you                                 with an infrastructure that's much more                                 flexible and bendy you don't have                                 problems like we had originally when we                                 started doing this without Khafre when                                 we were quite heavily dependent on                                 hornik you wear if Hornick you got too                                 much back pressure it would just                                 effectively fall over which is really                                 bad so many thumbs up to cap for Kafka                                 and if you're using it and for example                                 in our crawlers we just take the message                                 in the mention that matches the query                                 right at the end before we persist it to                                 our usual stores we create a new message                                 on Caprica we send it to a new topic and                                 we send it asynchronously in the                                 background and it's done and that's                                 about it for the first part and you can                                 keep your messages in CAF go for as long                                 as you like really you can specify in                                 the properties if you'd like to keep                                 them for say some amount of time for                                 example a day or a week a month even                                 depends how much disk space you've got                                 or you can keep them up to a point of                                 number of megabytes or gigabytes of disk                                 space or both whichever happens first so                                 you can then replay messages on the                                 topic a week later if you want which is                                 a really neat feature so step one is                                 done excellent we now move on to                                 processing so just to recap because I                                 went off on a kafka diversion there what                                 we're going to be talking about is how                                 do we detect that thing when it happens                                 so that we can tell our customers that                                 something's happened that they should                                 log in and have a look we're going to                                 start off by talking about just one JVM                                 doing this and also tracking just one                                 type of thing so the example that we're                                 going to use his hash tags and let's say                                 that someone just tweeted probably about                                 the crazy AV for example right now that                                 is amazing actually check your watches I                                 time this it's currently five pass for                                 clever and someone tweeted for example                                 about me and I'm here and I'm giving a                                 talk and I'll crawlers do all the                                 various things to these mention as they                                 come in and they split up who the author                                 is what the hashtags are in array who's                                 being mentioned that kind of thing then                                 the most basic level really we can just                                 start counting these these things when                                 they're happening so let's just think                                 about maybe a map for                                 poor and we can start off with a map and                                 the key is update which can just be                                 flawed to the hour so you can have                                    hours of keys you could just have twelve                                 o'clock one o'clock two o'clock three                                 o'clock and then the value to that key                                 is a multiset and you can just                                 initialize it with the last                                              example then when this tweet that we                                 just saw came in you can have a look at                                 the time and then you can floor it to                                 the current hours you say four o'clock                                 the four o'clock bucket and then you can                                 just put those hashtags in the multiset                                 it's a really really simple way of doing                                 this and then you just have a count of                                 one next to those in the multiset so                                 that's that's the most basic level what                                 you can be doing and okay let's say                                 we've got these buckets we can then                                 cycle these buckets so if using spring                                 you can use a scheduled annotation which                                 on the hour every hour we'll get the                                 oldest key in our map remove it and then                                 create a new one the the newest hour and                                 that way you've got this sort of                                         cycle just going in memory in a map                                 really simple stuff so then we've got                                 these hashtag counts being made which is                                 pretty straightforward so far how do we                                 detect that something interesting has                                 happened well we can think about it like                                 this maybe at scheduled intervals so                                 every minute every every hour whatever                                 we think is necessary for each hashtag                                 that we have we can transform that data                                 structure into an equivalent one which                                 is effectively time series so for each                                 hashtag at each data point how many                                 counts that they've been we can compare                                 that our to our history that we've got                                 in memory we give it a score based on                                 the algorithm that we decide to use to                                 detect the spike eNOS and then we can                                 have some presets so for a given average                                 volume of a query and one of our                                 customers query if that score is over a                                 threshold than we think it's probably                                 interesting and then let's keep the work                                 is really done so let's just send that                                 event out on a nuke Africa topic because                                 then we can write other applications and                                 then stream those events and do stuff                                 with it and and in the code itself I                                 mean it's very very similar to this                                 we've done the fair bit of work around                                 and keeping the heaps down and keeping                                 the history compacted but it's not too                                 far off this what we                                 doing but the issue that comes with this                                 simple processing is not so much about                                 the processing it's just countered like                                 the previous talk was saying it's just                                 counting things but the scale is the                                 interesting bit so what we just did here                                 is that we had a data model with                                 counting hashtags not that exciting but                                 there are many many different things                                 that when we sat down with our customers                                 before we did this project we said when                                 you're looking at real time data what                                 are you actually looking at in the app                                 and what are you using to see if                                 something interesting is happening loads                                 of other things maybe the the links                                 being shared any particular moment the                                 general volume of the query the the net                                 sentiment whether things are going                                 positive or negative particular authors                                 I mean there's lots of things that we                                 have and if each of these has a                                 different data structure you end up with                                 just one query having these so buzzard                                 Berlin buzzwords query but we have a lot                                 of queries so we have a hundred thousand                                 queries so the interesting bit is                                 scaling and how do we distribute this                                 out into a cluster in such a way that it                                 continues to scale and it works so we                                 need more JVMs so how do we share the                                 workload so distribution of work let's                                 move on to the next bit we looked at one                                 JVM doing hashtags there let's think                                 about a cluster so that box at the                                 bottom has now become a little cloud                                 many Jovians what are we actually                                 distributing well it makes sense that                                 one query one of our customers queries                                 with a data stream is a good thing to                                 distribute but how do we take that one                                 query and give it to one JVM and none of                                 the other ones just one of them where                                 this case we use leader elections so                                 who's familiar with leader election few                                 people and leader election is a                                 methodology for finding the leader for a                                 task in a group of distribution notes                                 you've got some number of nodes you want                                 one of them to do it some methodology                                 needs to pick one of those nodes so                                 that's what we do if you want to do                                 something like this then zookeepers                                 really good zookeeper users cool lots of                                 zookeeper users whose kind of like a                                 zookeeper I guess I don't try to think                                 the way to say not use it but you also                                 develop with it so you may be written a                                 feature that uses zookeeper to do stuff                                 ok less people but it's good it's good                                 and zookeepers a way of coordinating and                                 managing distributed applications if                                 you've installed stuff like HBase or                                 HDFS or storm it always made Queen                                 install with zookeeper as well that's                                 because it's doing clever stuff which                                 will be similar to what I'll just show                                 you here and what we use for our system                                 so zookeepers kind of like a file system                                 or kind of like a tree depending on how                                 you look at it and in our zookeeper                                 cluster for example all the features                                 that use it we have underneath                                 brandwatch now because that's the name                                 of the company and feature one feature                                 two at the top level in zookeeper you'd                                 have other things like the controllers                                 and brokers thing for Kafka and has                                 various things for each base if you're                                 using it and you can use the command                                 line interface as well and and you can                                 do sort of LS where we are right now it                                 will say zookeeper blah blah blah and                                 then show me the things are underneath                                 brokers running LS brokers and it will                                 show you the thing so you can imagine it                                 like that and if you're going to be                                 doing programming using zookeeper to                                 build your feature then we highly                                 recommend using apache curator it's a                                 library that netflix released the builds                                 on top of the lower level zookeeper api                                 and it gives you a whole bunch of                                 recipes to work with so for example if                                 you want to do some kind of shared                                 reentrant lock across a cluster then                                 they have some code that you can use to                                 do it and likewise if you want to do                                 some kind of leader election like we're                                 going to be doing then they have some                                 api's that you can use for that and it's                                 pretty straightforward to use and the                                 documentation is very good as well so                                 thanks Netflix so let's think about what                                 we're doing so in in the app customers                                 have queries they say i want to receive                                 emails when something interesting                                 happens and they press a button and                                 that's what we want this feature to be                                 as simple as just press a button so when                                 they press that button we're inserting a                                 row into our postcards relational                                 database to say that this user wants                                 this thing so we're going to write a JVM                                 that is then listening for what's                                 happening in that database and when a                                 query is enabled then it will put a                                 query ID into zookeeper so brandwatch                                 signals queries and put the query ID in                                 there and likewise if they                                 able the feature and they don't want to                                 receive the emails anymore then that JVM                                 will look it up find the node and just                                 delete it does that make sense so far                                 blank faces yeah cool so in order to get                                 stuff out of the database so trying to                                 work out when a thing happens and we                                 used pgq for that and I'm not completely                                 sold on PG q it's pretty good it was                                 part of the londa strep location suite                                 that skype wrote which is pretty good                                 and it's part of that we wrote the Java                                 consumer for it if this thing will stay                                 still and it's on github so if you want                                 to use it with your java apps then we                                 did the hard work so just just use it                                 away and you put a trigger on the table                                 basically and every time that thing                                 happens for example a row gets inserted                                 into the into the table that says this                                 user has requested the feature it sends                                 an event on PG q and the JVM picks it up                                 so                                                                      are not initiated let's imagine that                                 we've got this set up on zookeeper we                                 have one query that's currently enabled                                 we've got one that just gets enabled                                 right now and we've got three JVMs                                 listening to it for using curator then                                 you can use the watching facility to say                                 watch that queries node and tell me when                                 something happens and you get a call                                 back so this one                                                      got added right now the JVM will be                                 watching the queries mode and when that                                 gets added they all get a call back to                                 say hey there's this you know what are                                 you going to do about it and at that                                 point in your code you say start leader                                 election then it's a race what happens                                 is each JVM will try and write an                                 ephemeral sequential node underneath                                 that ID so it's ephemeral because if the                                 connection between zookeeper and the JVM                                 is lost then that node will disappear                                 when the acts and timeout it's                                 sequential because the one that gets                                 there first gets number one the one this                                 they're saving this number two so the                                 one who wins wins the job that's leader                                 election basically so then what happens                                 if you want to deal with failover so the                                 let's say the leader is dead at this                                 point then because it's an ephemeral                                 node that node disappears it drops off                                 and then it initiates the next stage of                                 leader election where the second one in                                 line gets the lead                                 ship and then it processes the job and                                 then if that one comes back it joins the                                 back of the queue does that make sense                                 so that's roughly how leader election                                 works and we used it for this system in                                 order to distribute the queries in                                 different JVMs so there we go each time                                 someone turns on the feature the note                                 gets added we do leader election one of                                 the jvm is in the cluster will get that                                 they'll create the data structures and                                 then you'll start consuming the buckets                                 of data from Kafka so are we almost                                 there well yes yes I know and the thing                                 is is that we're processing long-running                                 jobs there are some queries of been in                                 our system for years and so we're                                 distributing these jobs that aren't just                                 a quick fire and forget they persist for                                 a very long time potentially                                 indefinitely so the workers that are                                 distributing the load can they get                                 overloaded and the answer is yes they                                 can so one way to get around this is                                 that we did load testing on these                                 workers that were keeping stuff in                                 memory and doing the spike detection and                                 we profile the JVMs under the worst                                 cases to find out the maximum heap they                                 use for certain numbers of numbers of                                 queries and we set that as a constant                                 and it's configurable and we altered the                                 leader election algorithm slightly in                                 that when they win leadership they check                                 to see whether they're already                                 processing too many queries and if they                                 are then they back out of leader                                 election they pass it on to the next                                 person and that works pretty well it                                 means that no one node in that system                                 can take on too much work but we're not                                 actually all the way there yet because                                 there's still an educator in that case                                 where if all of your workers in your                                 cluster all at max capacity then you can                                 get this kind of infinite election                                 problem where each of them says I can't                                 do the job give it to the next one then                                 that one says no I can't do it either                                 and it gives it gives it on and then                                 they just keep spinning and spinning and                                 spinning I mean you should never really                                 get into this situation hopefully you                                 have enough resources in your cluster                                 but it's still an educator you have to                                 deal with potentially in case you ever                                 did and the way that we we programmed                                 around that edge case was that the                                 worker code has a very short-lived cash                                 just like a guava loading cash that just                                 keeps a note if it refuses a particular                                 query keeps it in memory and then we                                 know that because all those know is that                                 if everyone                                 sure that everyone else in the                                 leadership election process will get a                                 fair Turner trying to process the query                                 and then if it comes back around within                                 the time period that it's in the cache                                 then we know that nobody can do it and                                 that's a really big problem and we've                                 never gotten that situation because we                                 always have enough workers spun up                                 however at this point you might want to                                 message your team send them a horrible                                 text message maybe if you're really                                 clever use a patchy meses to spin up                                 some new workers or some kind of other                                 elastic arrangement that you're using                                 but that's a neat way that we got round                                 of that a problem I thought it was quite                                 proud of that so stay if we're having                                 these workers deal with failover then                                 they're keeping like                                                     memory at any given time so there needs                                 to be some way that they can recover if                                 leader drops from one to another they've                                 lost that                                                                it come from and and that's why we do                                 snapshotting so I'm not going to go into                                 huge detail here however each of the                                 workers are regularly snapshotting their                                 data into HBase all the time and using a                                 combination of some serialization and                                 cry oh we managed to get it down to sort                                 of naught point three not point four                                 megabytes per query from a really large                                 amount of JVM heap which is great and so                                 we just regularly snapshot out if a                                 worker dies maybe within five seconds                                 another one's picked it back up we might                                 have lost five seconds worth of mentions                                 but if you were just trying to find                                 events on a rough curve that's probably                                 okay to deal with that so we've sort of                                 done at this point to here where we have                                 all the mentions coming out of our                                 crawlers they're going into our cluster                                 of nodes that are computing that when                                 events are happening they're piping it                                 out on a nuke Africa topic cool so the                                 last bit of the talk is about finding                                 meaning in these events so we want to                                 have all these events that are happening                                 and we want to deliver something concise                                 and interesting into our customers in                                 boxes there isn't spammy that hopefully                                 means something so for example this is a                                 real email from the system and when we                                 were tracking our own launch of the                                 product that I'm talking to you about we                                 receive this email when a particular                                 blog posted about it and lots of people                                 started talking about it so we want to                                 send them that for example the United                                 States is trending for this particular                                 query and it's that many mentions                                        the last                                            some topics of roughly the conversation                                 what it's about and then also some                                 influential mentions that we found at                                 the time so we want to gather all this                                 stuff up and then send it to the                                 customer so firstly just to get some                                 topics for example if there was a trend                                 going on at the moment for the biebers                                 hashtag then there might be a whole load                                 of noisy text knocking around in this in                                 this data song once we go off to our                                 data stores and we say give me a random                                 sample of data but it might turn out                                 that if you do topic our rhythm on it                                 like tf-idf or something similar then                                 you'll get some fairly concise topics so                                 that's that's fairly so overall with                                 existing techniques not too interesting                                 but it's a neat way of seeing at a                                 glance what's going on but we want to                                 send them one email we don't want to                                 send them for example if bebas was                                 trending and also the brand watch query                                 had a general volume increase and also                                 berlin was trending at the same time in                                 the same query and the number of tweets                                 from Germany had gone up we don't want                                 to send four emails to our customers                                 that were about the same thing because                                 that would be really rubbish and so we                                 want some way of being able to group                                 them together to give them one email                                 from all these different events what did                                 we do firstly we look at granularity so                                 for example a hashtag like bebas is more                                 granular than a general increase in                                 volume so we have a decision tree of                                 granularity so that people will                                 hopefully get something that is a                                 granular event that describes what's                                 going on also text similarity so if                                 there are four different events at one                                 given time we'll look at the similarity                                 of the text to work out roughly if                                 they're the same thing or not using some                                 heuristics and we'll also look at the                                 shape of the volume spotlighting in                                 these events to see are they or are they                                 not roughly happening at the same time                                 with the same volumes and do a heuristic                                 comparison between the two and if you                                 combine all those three things then you                                 can sort of plot them on a imaginary                                 graph like this where we have volume up                                 the side and then the granularity lon                                 the bottom and we want to pick the thing                                 that's the highest volume and the                                 highest granularity and that becomes                                 what we sort of called the hero of the                                 email which is the thing that you get                                 along the top that says this is                                 happening and you get the volume on that                                 spot line for it and then everything                                 else gets relegated to sort of beneath                                 the fold on the email for example during                                 the                                 debate in the UK elections and this                                 particular tweet came out from the drum                                 we released some data about the                                 different things going on during the                                 debate the drum posted it and we                                 detected that was the most granular most                                 interesting thing that was happening                                 right now and then all these other                                 things were happening for example                                 hashtag leaders debate all these other                                 things being pointed to we kind of                                 relegated to beneath the fold so our                                 customers didn't get loads of emails I                                 just get one so hopefully that makes                                 sense about that grouping that we were                                 doing there so just some some closing                                 remarks here I mean this is our first                                 kind of foray into a sort of streaming                                 in memory sort of real-time pipeline                                 system and because we spent a lot of                                 time over the last few years building                                 out a really good at analytics platform                                 and we're trying to build more things on                                 top of this now for example for people                                 to request custom filters for their                                 streams before they get the detected                                 signals so that we can then put another                                 JVM in the pipeline which is doing                                 filtering and then piping things out on                                 another kafka topic and then you end up                                 kind of composing these different                                 streams of data as they become more                                 enriched more interesting and then other                                 people can build other apps based on it                                 as well so our infographics panel are                                 visiting can start consuming these                                 events as they come out of Kafka and                                 then for example if in their query and                                 event happens you can then change the                                 configuration of those displays to say                                 hey this thing is happening right now                                 and the architecture solves the problem                                 anyway if you want to laugh at me                                 because of AV trolls and that's my                                 Twitter handle I like talking to                                 like-minded people and please do say                                 hello and that's about it so any                                 questions and answers whoa that was                                 really really quick
YouTube URL: https://www.youtube.com/watch?v=TpSVh5MbJL8


