Title: Berlin Buzzwords 2015: Szehon Ho - Hive on Spark #bbuzz
Publication date: 2015-06-03
Playlist: Berlin Buzzwords 2015 #bbuzz
Description: 
	Apache Hive is a popular SQL interface for batch processing and ETL using Apache Hadoop.  Until recently, MapReduce was the only Hadoop execution engine for Hive queries. But today, alternative execution engines are available â€” such as Apache Spark and Apache Tez. The Hive and Spark communities are joining forces to introduce Spark as a new execution engine option for Hive.

In this talk we'll discuss the Hive on Spark project. Topics include the motivations, such as improving Hive user experience and streamlining operational management for Spark shops, some background and comparisons of MapReduce and Spark, and the technical process of porting a complex real-world application from MapReduce to Spark. 

Read more:
https://2015.berlinbuzzwords.de/session/hive-spark

About Szehon Ho:
https://2015.berlinbuzzwords.de/users/szehon-ho

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              Oh                               thank you good morning everyone my name                               is Sehun ho the topic of my                               sparked and first I will try to                               introduce myself and my company that I                               work for                               so I come from cloud era yeah so I work                               in the Palo Alto office in cloud era and                               our company has a distribution of Hadoop                                called CDH and basically we bundle                                Hadoop along with dozens of other                                open-source projects that work together                                very well with Hadoop and on top of that                                we also sell some enterprise and                                management security tools for people to                                use with their Hadoop cluster in cloud                                era I am a member of the hive team so we                                have a lot of people working on                                different projects I work on project                                called hive as part of my job I'm also                                working with the hive community which is                                a very big worldwide community of people                                working on the product so I'm a                                committer and pmc on the project and a                                little thing about myself I'm very                                excited to be here in Berlin                                I visited Germany last year I really                                liked it and I'm very grateful for                                Berlin buzzwords to give me the                                opportunity to come back here and                                hopefully this talk will also be                                interesting to some people here so the                                talk is gonna be split into three parts                                first is a background hive what is hive                                what is spark and why we think hive on                                spark is a good project then some                                technical deep dive into the project                                some of the details and finally end with                                a user view like how this project will                                help users of hive get better                                performance out of the their jobs so so                                first is about hive what is hive hive                                really is a project that came with the                                whole Hadoop kind of revolution in                                     when the open source version of Google's                                MapReduce was first introduced to the                                world as an Apache project at that time                                it was quite a ground groundbreaker                                because a lot of organizations had this                                need to process huge amounts of data and                                the open source version of MapReduce                                allowed them to do so to scale to                                thousands of nodes and terabytes of data                                so a lot of web company                                like Yahoo Facebook LinkedIn they all                                took this up but really there was a need                                as part of this project to have a sequel                                access layer because a lot of people                                still I mean MapReduce you can write                                programs against but in order to bring                                MapReduce to power to ordinary people                                who might not program or even people who                                can program but don't want to write a                                complicated program you need a SQL                                access layer on top of this so that's                                really the story of hive and hive the                                main use case                                I mean hive has been in production right                                now for almost eight years I guess so                                it's a very feature-rich very mature it                                can be used in a lot of different areas                                but the main use case is really the                                online analytics where you know Hadoop                                strong suit is processing a lots of data                                and doing analysis across the data set                                kind of like what some data warehouse                                tools do and because it's really been                                around for so long hive has become kind                                of the de facto standard for sicko and                                Hadoop meaning that all you know sequin                                jhin's that coming out who wants to work                                on how to really look to hive and try to                                make sure they support the hive features                                and it's the popularity is very amazing                                like in cloud era about                                                of our enterprise customers use hive                                just again because of the ease it allows                                ordinary people to be able to run these                                huge MapReduce jobs so a little diagram                                of where hive sits it's on the top of                                the stack so map reduces the processing                                engine backing hive and then HDFS of                                course is the storage for the Hadoop                                file system so the second part of the                                story is spark so spark really came as a                                second wave of big data innovation the                                problem with MapReduce it was like it's                                very powerful but it wasn't it wasn't                                designed with the general purpose in                                 mind it was more designed for googles it                                 was an internal Google project to help                                 them do the search indices so a lot of                                 projects like tears like fling like                                 spark a lot of these new buzz projects                                 kind of have a hindsight                                 of hindsight to look at Map Reduce and                                 be able to do things a lot better than                                 what is being done in Map Reduce so in                                 cloud era we have a lot of time kept our                                 eyes on spark and we think it's really                                 the kind of the front-runner to be the                                 next MapReduce and the industry has kind                                 of come to that as well because if you                                 look at the activity among the Apache                                 projects you know Apache Software                                 Foundation has probably hundreds of                                 projects under the umbrella and by now                                 spark is the most active one and we I                                 mean the success of a project in Apache                                 really is the bigness the largeness of                                 the community so in that sense I think                                 spark really is the front-runner and                                 also around the community other Apache                                 projects are all moving a lot of them                                 are moving to spark I know a lot of                                 people and this Berlin buzzwords gave                                 talks about how they're moving their                                 projects over to spark as well they're                                 not on this list so it's very catching a                                 lot of ground very quickly and the                                 reason spark is so popular I think is                                 just because it was designed with very                                 powerful API is very expressive api's                                 and abstractions really meant to make it                                 very easy to program a distributed                                 processing pipeline just the way that                                 you want it so some details of that so                                 three things that I think that spark can                                 do better than that produce and that you                                 know users of spark also steal our first                                 is the the file the data format so in                                 Map Reduce                                 if you remember everything is HDFS files                                 it's the HDFS files that are fed into                                 the job and then output of a job whereas                                 in SPARC they made a new concept called                                 RTD which you treated almost like a file                                 in that you've passed it around between                                 jobs but the point is our T DS is                                 abstraction because a lot of times they                                 can be kept into memory so spark you                                 know being built in around                                              benefit of being around at a time when                                 the systems today have a lot more memory                                 than they did when MapReduce was there                                 so by having an RDD concept and hiding                                 the fact that you're dealing with a file                                 or in memory you're really making more                                 Bennett                                 of the resources on your cluster another                                 advantage of SPARC is just the fact that                                 in the programming in SPARC there's a                                 lot of more transformations you can do                                 so if you ever see a spark program to do                                 a word count for example it's much                                 easier to write then in MapReduce and                                 also MapReduce has a kind of a design                                 where you have to write a mapper or                                 reducer and then the framework execute                                 it in that order map the shuffle reduce                                 whereas in SPARC you can run things in                                 any order that you want so you can get a                                 lot more efficient program that way and                                 we use these two quite heavily on hive                                 on SPARC to make hive run a lot faster                                 and the last one is something maybe                                 people don't realize is that in                                 MapReduce every tasks so like a map task                                 or reduce tasks are these Java processes                                 and they all come up when they need to                                 be run and then the idea shut down once                                 the job is over so if you have a                                 thousand you know mappers and                                     reducers you're spawning and killing one                                 thousand five hundred processes every                                 time so this is if you add it up you                                 know the JVM overhead can be kind of                                 substantial for SPARC they again                                 abstract it out so that you have these                                 things called executors which are                                 long-lived processes and they can be                                 there and run multiple tasks so you kind                                 of don't have to pay the cost every time                                 of when you run a spark job of spawning                                 all these processes and then destroying                                 them so these three things really are                                 just I mean this is just kind of the tip                                 but these three things we really take                                 advantage of in hi one spark to make                                 hive much more efficient so it brings us                                 to hive on spark which is the project                                 that we've been working on for probably                                 half a year or two a year and the goal                                 is well hive is a sequel access layer                                 and as I mentioned there's probably you                                 know a lot of people there's probably                                 thousands of hive deployments around the                                 world solving real world problems today                                 and as an access layer we want the user                                 to not care underneath hive what the                                 processing engine is so by switch                                 swapping out MapReduce with spark we're                                 hoping that they can keep all the                                 functionality that they have today and                                 also be take advantage of the new spark                                 concepts like you know better                                 in memory processing better process                                 latency and so forth so the goal is to                                 reiterate is one we want hive to be able                                 to run seamlessly across execution                                 engines so the same hive instance if you                                 point it to a MapReduce cluster or a                                 spark cluster or even test clusters                                 should be able to run the same jobs and                                 secondly obviously I've on spark we will                                 need to support the full range of                                 existing hive features so that's kind of                                 a background about why we decided to do                                 the project and the next section is                                 going to be about some technical details                                 about how we achieve hive and spark                                 project so the Highland spark as I                                 mentioned before the real challenge I                                 mean it's one thing to kind of build a                                 new system that's obviously very                                 challenging but I think for this project                                 the challenge was to powder pour a very                                 mature system that again is solving                                 real-life problems today a huge scale                                 onto a new processing engine without any                                 loss of functionality and there's a lot                                 of functionality built into hive if you                                 ever work with the database you know                                 that the power of the database is how                                 much of the sequel syntaxes supports and                                 sequel syntax well it's deceptively                                 simple it can get you know very                                 complicated very quickly and also a lot                                 of times databases they they have things                                 that are not specified in the sequel                                 syntax called                                 things called dialects sequel dialects                                 and so people have gotten really used to                                 you know hive sequel dialects for                                 example because sequels syntax you know                                 sequel standard only supports up to some                                 specification beyond that you need your                                 database itself to have this                                 functionality and other things like data                                 types so how many how hive treats data                                 types when you'd want to define a int or                                 a string or a you know carve our car                                 binary things like that how hives                                 implicitly cast things from one data                                 type to another these things people have                                 gotten really used to that we want don't                                 want to break when we do have on spark                                 people might know hive has really a real                                 big library of functions that you can                                 you can define yourself and then use in                                 your sequel like for example of some                                 mathematical standard deviation function                                 or linear functions                                 thing you can define and hive has a very                                 big library of those and also file                                 formats the fact that hive has these sir                                 days that you can swap out your favorite                                 file format like RC file part K file etc                                 so all these things in order to keep the                                 same what we ended up doing is we try to                                 keep most of the execution code in the                                 hive operators to be the same across                                 different processing engines we don't                                 want to rewrite every single operator                                 just for SPARC so the idea is that in                                 general we will run the same code that                                 we used to run in MapReduce on - on the                                 equivalent thing in spark which is a                                 spark transformation so for example in                                 the mapper if we used to run a filter                                 operator that dealt with a filter                                 function in sequel now we run the same                                 operator in SPARC similar in reducer we                                 run the same in a spark transformation                                 so it's by doing this we integrate a                                 spark at a low level and managed to keep                                 all the functionality of hive now people                                 might be wondering then like if we're                                 just running the same code in spark                                 where's the benefit come from and so I'm                                 gonna spend the next few slides talking                                 about different ways that just by doing                                 this we can still get a lot of savings                                 in terms of performance for hive queries                                 written in spark and I think this might                                 be also interesting to people who are                                 you know not hive users but just in                                 general want to migrate MapReduce jobs                                 on to spark or people who you know once                                 you write spark jobs for the first time                                 this might also give you some insight                                 about how you can optimize things in                                 when when doing in spark so the first                                 improvement is that we can eliminate a                                 lot of redundant phases that we have in                                 MapReduce as I mentioned before in                                 MapReduce if you know you have a mapper                                 you have to have a reducer and in                                 between there's a shuffle phase so if                                 that doesn't work for your job and a lot                                 of hive queries actually go away beyond                                 just one job you have to have another                                 job so if you want another shuffle for                                 example you have to have another mapper                                 another shuffle another reducer and you                                 know typical hive queries that we've                                 seen from customers are probably                                 averaged                                                                know some really big                                 whereas maybe got                                                      MapReduce jobs so a lot of this is kind                                 of wasted because like between the                                 reducer and mapper there's really not                                 much there so if we can in spark because                                 we can run the transformations in any                                 order we can go ahead and eliminate the                                 redundant phase and just on the bottom                                 have three kind of phases of spark                                 transformations and run the same code                                 that we used to do in MapReduce so the                                 rule of thumb is for every stage that                                 used to have every job you used to have                                 in MapReduce we can eliminate one phase                                 in the spark transformation and so to                                 give kind of a maybe an example of this                                 so one query in highest that will boil                                 down to more than one MapReduce job is                                 if you have apologized some of this cut                                 off but if you have a joint statement                                 like in the inside I have select key                                 from source one joint source two and                                 after that I feed that into an order by                                 statement you're gonna get two MapReduce                                 jobs the first one to do the join the                                 second one to do the order by but in                                 spark we can just do it in one big spark                                 job and also combine the second the                                 first reducer and second mapper into the                                 same the same transformation there to                                 get so we actually run the same                                 operators that we used to run but now we                                 eliminate a whole stage in the data                                 pipeline using just by using spark so                                 that's some savings pretty significant                                 savings and it actually gets better than                                 that because as I mentioned before in                                 MapReduce there's this concept of files                                 and every MapReduce job will write out                                 this huge file and then the next                                 MapReduce job has to read back in this                                 huge file just because that's the only                                 way for the two jobs to communicate with                                 each other and in fact in hive if you                                 ever used hive you have to define a                                 temporary directory where what it really                                 is is like the the partial results from                                 one MapReduce job are kept there and                                 then they're read back in when the next                                 MapReduce job runs so in spark again                                 this problem doesn't exist anymore one                                 because we've combined the reducer and                                 mapper so                                 really there's no need to kind of you                                 know channel data from one job to                                 another but even when we do like for                                 example from transform to the shuffle to                                 the transform where we do need to pass                                 data along we use the spark RTD concept                                 so spark already anyway keeps that into                                 the memory so the way RTD works is                                 unless you explicitly ask to spill to                                 disk or if you run out of memory across                                 all your processes only then does it                                 spill to this so if you configure enough                                 memory for your cluster a lot of this                                 stuff can be kept in in the memory and                                 at very low low low latency when you                                 move from one transformation to another                                 so that's one another improvement that                                 we can achieve by doing the high bond                                 spark running the same operators but at                                 a more efficient transformations and                                 kind of the next one is the shuffle                                 phase so this is the last thing I'm                                 going to talk about for the execution                                 part of the of the problem I've coded                                 so inside so that was talking about if                                 you have more than one MapReduce job                                 even if you have one MapReduce job                                 there's a shuffle phase that we can                                 improve on and if you know a little bit                                 about MapReduce you know that shuffle is                                 really after the mapper is done the                                 output is kind of shuffled to different                                 nodes and then sent to the reducers so a                                 lot of MapReduce jobs this is one of the                                 more expensive parts because shuffling I                                 mean it's not file i/o but it's still a                                 lot of network i/o you're still moving                                 around potentially you know gigabytes or                                 terabytes of data across the network                                 so in SPARC you can't get around                                 shuffling but you can do sometimes you                                 can get more efficient shuffling because                                 MapReduce in my next slide so MapReduce                                 has only one thing called a shuffle sort                                 so when the shuffle phase happens what                                 it means is that if it does a hash                                 partitioning of all the data and then it                                 sorts each of the partition so this                                 works for all the problems but then it                                 sometimes in fact a lot of times it's                                 overkill for what you want to do so to                                 give some examples to help illustrate                                 that point                                 so for a query like select so it's                                 aggregation query if you know SQL so the                                 point is you group by a key and then you                                 select average from each key group so                                 basically in this example we don't                                 really need to sort each key group after                                 you have all the rows of the same key in                                 one group all you need to do is take the                                 average of them but in MapReduce it will                                 do secondary sorting anyway which is a                                 little unnecessary and not able to turn                                 off so in spark you can call just by                                 calling a different transformation                                 shuffle transformation like group by you                                 can get the exact behavior you want                                 which is just to hatch partition by key                                 just make sure the data rows with the                                 same key go to the same partition and                                 stop stop at that so we get some freedom                                 that way another one is the ordered by                                 so another example is select key from                                 table order by key so this query means                                 you just have you know you have this                                 huge table and you just want to sort get                                 a total order sorting of it inspark you                                 can call an order by transformation that                                 does that in parallel so spark has                                 already you know just by calling this                                 API has a lot of logic already to do so                                 what it does is it takes samplings of                                 all your data makes appropriate range                                 partitions for example                                                  and then have the data partition in each                                 range and then parallel sort those to                                 get a total sorted result in MapReduce                                 the traditional way if you wanted to                                 write a sword in MapReduce is you just                                 Park it's a kind of a you know a stupid                                 way is like you actually just partition                                 everything to one partition and then the                                 MapReduce anyway sorts all the                                 partitions so you get it sorted by                                 default but it's in serial instead of                                 parallel so again by running this                                 transformation in spark you achieve a                                 more savings for the shuffle phase so                                 that was that was all I had about the                                 execution part just the fact that                                 between MapReduce jobs and inside                                 MapReduce jobs just by using spark were                                 able to achieve a lot of savings that                                 way as I mentioned before the second                                 part is this process life cycle so if                                 you remember MapReduce all the mappers                                 and all the reducers                                 are actually one process and then when                                 they're done they just shut down so                                 they're a lot of the MapReduce job is                                 just spawning and killing processes so                                 in spark again the executors are what                                 they call processes and they're quite                                 long-lived and they can run one or more                                 tasks and another terminology in spark                                 is called an application so a set of                                 executors for a given user is called a                                 spark application and a user can choose                                 you know how many executors he wants                                 Tunas so that he can increase or                                 decrease it and so what we do in haiwan                                 spark is that for every hive user                                 session so every time you log into hive                                 and you run a spark query we're gonna                                 start a spark application for that user                                 and based on what you configure we're                                 going to pre start that many executors                                 for you so that only the first query                                 will incur the cost of starting all                                 these processes subsequent queries will                                 just reuse the processes and not pay the                                 cost of spawning all these processes so                                 just to give kind of a graphical thing                                 about what I talked about in MapReduce                                 every stage will have a lot of process                                 kind of life going up and down and but                                 in spark what you do is you define for a                                 given application like initial number of                                 executors the minimum and the maximum                                 and then or you can even give us up just                                 you know everything is the same number                                 and then the per number of processes                                 stays more constant throughout the whole                                 spark jobs in fact across the whole                                 session in multiple spark jobs processes                                 don't need to be spawned anymore so                                 that's kind of for the technical part so                                 for the user part so I'm just gonna                                 spend a little time talking about you                                 know after that what kind of results we                                 got and how users might be able to                                 leverage hyewon spark if you are a hive                                 user so to leverage this thing again we                                 try to make the installation not as more                                 complicated than it is today so anyway                                 when you install hive today you have to                                 install first hadoop which is the file                                 system another thing is                                 spark can actually run on different                                 resource managers so you know there's                                 yarn there's mesos and these things what                                 they do is they make sure that users of                                 spark different users of spark                                 don't you know collide with each other                                 and that their jobs are scheduled                                 properly so we we recommend in Qatar to                                 use the yarn mode in and in fact we                                 shipped with the yarn mode as default so                                 the reason is it gives you a little more                                 control over the number of processes                                 that an application can have so all this                                 stuff of dynamically increasing or                                 decreasing the number of processes is                                 only in spark on yarn mode and so the                                 only extra step here is you have to                                 install spark and then after that you                                 install hive which is so if your install                                 hives before you know that if you                                 install hive on a MapReduce node and                                 that Hadoop node has you know                                 configuration so just pick up those to                                 find out where the MapReduce master is                                 so similarly when you install hive on a                                 spark node it will just pick up from the                                 spark configurations where the spark                                 master is what the spark sterilizer is                                 so forth so you really don't need to                                 configure anything the versions you have                                 to run with I mean these are the ones                                 that we laid the latest ones that we                                 shipped with is these versions here hive                                 one point one spark one point three                                 hundred two point six if you download                                 the latest cloud era five point four in                                 fact we actually shipped with all these                                 versions so you don't have to worry                                 about compatibility issue and then the                                 hive client all you have to do is set                                 the execution engine to spark as I                                 mentioned for one of our goals is to                                 have the same hive be able to run jobs                                 in MapReduce and spark so you can switch                                 that dynamically what cluster you want                                 you to run your job with then you just                                 run the query and keep in mind the first                                 query may take a little more time to                                 execute because we're again we're                                 pre-warming the cluster and starting up                                 these executors but subsequent jobs will                                 be able to reuse the executors and get a                                 lot of performance benefit so when you                                 run the job if you ever run a hive job                                 you run the job and then a lot of logs                                 come and basically this they used to                                 show the MapReduce job status like how                                 many mappers are running                                 how many reducers are running it looks a                                 little different in spark but the idea                                 is the same just how many spark tests                                 are running at any time so not too much                                 different there then in the spark UI                                 sorry the yarn UI you can actually find                                 the spark application that you started                                 click on that and then you're able to                                 drill down into the spark UI which is                                 again it's a little cut off but the                                 spark you I will show it kind of shows                                 the same as what the console prints out                                 in terms of job progress but you can                                 click to see more details about what                                 tasks are running how many are there and                                 also like environment variables                                 executors what whatever one is doing and                                 so forth so pretty easy to monitor your                                 job                                 also on spark and things you might want                                 to tune so again I talked about a spark                                 application how it's pre starts a number                                 of executors for you and the idea is you                                 can set like a constant number which is                                 static allocation or on yarn mode you                                 can set a dynamic number so the two                                 differences is if you set a static                                 number obviously you get everyone gets                                 ten executors okay well then the                                 performance is gonna be more stable                                 because you're always gonna run on those                                 nodes but then you might end up with                                 some users not being able to get in                                 because if                                                                                                                                      next guy will have to he will not be                                 able to get any resource so dynamic                                 allocation is a better way there's                                 slightly more unpredictability for                                 performance but then what you can do is                                 you can specify a range like I start at                                                                                                        are running it will go back down to the                                 minimum number so these are two options                                 you can use in hive to to and spark in                                 fact even if you don't use hive you're                                 just running spark jobs you can you can                                 use these two options - - and how many                                 executors given that application has and                                 then again spark executors you can                                 another thing to tune is like the memory                                 so the memory will affect obviously how                                 much can cash into                                 memory how much of the data of the RTD                                 can be kept into memory so this affects                                 the performance so spot has kind of a                                 lot of settings for memory one is the                                 cores but also has memory settings and                                 then another one is the driver which is                                 the spark like the master executor I                                 guess and you can tune that one maybe it                                 has a little more so you can tune those                                 so so that's kind of it for the user                                 view if you want to play around with                                 Highland spark now I like to share some                                 performance benchmarks that we made so                                 we have people working from Intel China                                 and they helped us do these benchmarks                                 so I'm just taking their numbers so they                                 played it around with a cluster of eight                                 nodes where every node has                                             gigs of ram                                                             for network so kind of a little large                                 but typical like a data center                                 configuration and these are the versions                                 that we use for the different components                                 and we played around with the TPC DS                                 data sets with two data sizes                                          which is kind of a average data size and                                 four terabytes which is a little on the                                 larger side and TPC TS is if you don't                                 know is the standard kind of                                 benchmarking for all the sequel like                                 databases use it quite heavily and now                                 some of the big data on how sequel                                 solutions are also using it so and we                                 also have some of these configurations                                 that we enable to make sure all the                                 processing engines get equal kind of                                 play in fact and maybe as interesting                                 side like all these optimizations that                                 are in hive code is able to be run                                 across different processing engines                                 without any loss of functionality memory                                 vectorization for example is just to                                 make sure hive operators when they do                                 loops that there's in such a way that                                 they can benefit from the multi                                 instruction of a CPU CBO is the fact                                 that you're so in all the execution                                 engines we're running the operators in                                 the same order but CBO makes sure that                                 the operators are run in optimal order                                 so it does things like maybe reorder                                 some join or                                 push down some predicate to the front                                 and then finally the last one is kind of                                 a strangely named flag but what it is is                                 make sure some joints get turned into                                 map joints if there's enough memory it's                                 a different joint implementation that's                                 a lot faster but you have to have enough                                 memory to do it and so not gonna spend                                 too much time but these are the kind of                                 what we the Intel guys set to make sure                                 that both these tears and spark are                                 tuned very properly and again we because                                 the first query is the one that kind of                                 pre allocates a lot of these executors                                 what we do is run each quarry twice so                                 the first is to warm up the cluster and                                 then the second is the one that we                                 actually measure the results and so not                                 only spark has this issue but even tez                                 some tests has something called pre warm                                 containers which is basically the first                                 query will spawn a lot of containers up                                 and then the second one will be able to                                 use those so for                                                       some nice results I mean we don't beat                                 so we beat map spark - spark is faster                                 than have on MapReduce for all the                                 queries but I mean and then we're at                                 faster than high vantes on a lot of                                 queries but there are some queries that                                 we're still working on - to catch up to                                 two days and if you look at four                                 terabytes the results is more pronounced                                 so for I mean the the ratios is most                                 mostly the same but then the the gains                                 is more pronounced and that's just                                 because again I mentioned a lot of the                                 savings we have in hi one spark is you                                 know not flushing everything to disk or                                 not shuffling everything in this kind of                                 a inefficient way so if the data is more                                 you actually get more savings from some                                 of this stuff so and tazed so some of                                 the I think what they did is some of the                                 range is a little small so they blew it                                 up and then just have another comparison                                 for spark versus tazed so so for most of                                 the queries were we're faster than the                                 tazed but some of them were slower than                                 tazed and all I'll talk about that in                                 the next slide                                 which is so spark is faster at most                                 queries we're working on some                                 optimizations to make spark faster for                                 some queries that are slower than                                 today's stem things like dynamic                                 partition pruning which is kind of a                                 hive concept means when you join two                                 tables you can prune away some                                 partitions of another table based on the                                 fact that the first table you know only                                 has these join keys so you don't                                 actually need to read these partitions                                 or the second table to join it's kind of                                 a more advanced optimization that we're                                 working on for a headline spark in fact                                 one of my colleagues is already working                                 on this and has a patch ready and spark                                 is also kind of a newer project so a lot                                 of some of the shuffle that they do                                 shuffle sort that they do is not as fast                                 as it could be so but the the real                                 benefit of spark not really is is not                                 like that spark as fast today but also                                 the promise that spark can be faster in                                 the future just because it's such a                                 healthy and a vibrant community and they                                 already you know have a lot of things in                                 the works like shuffle start                                 improvements already in the works and                                 then they have things like project                                 tungsten that can you know promise to do                                 a lot of more improvements for spark so                                 we're very confident that just by doing                                 this project in the future we're gonna                                 get even more benefits for hive users on                                 top of spark so so actually I leave some                                 time for questions so actually this is                                 perfect because I the last slides just                                 to say that hive on spark is available                                 in these versions hive                                                 and if you want to look at the code or                                 any of the work we did                                 there's hive uses the JIRA system so                                 high of                                                                 for all the hive on spark work and then                                 it's not just a one company effort in                                 fact we had many companies like Intel                                 people from China                                 IBM app are some help from data brings                                 and from our company too so it's kind of                                 a joint effort and yeah so I think                                 that's pretty much all I had so I'll                                 leave time for some questions                                 you
YouTube URL: https://www.youtube.com/watch?v=U8xQnGslkFI


