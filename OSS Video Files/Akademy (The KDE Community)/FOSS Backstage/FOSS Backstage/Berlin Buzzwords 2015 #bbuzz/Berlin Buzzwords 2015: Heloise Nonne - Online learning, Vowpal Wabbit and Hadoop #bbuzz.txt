Title: Berlin Buzzwords 2015: Heloise Nonne - Online learning, Vowpal Wabbit and Hadoop #bbuzz
Publication date: 2015-06-03
Playlist: Berlin Buzzwords 2015 #bbuzz
Description: 
	Online learning has recently caught a lot of attention, following some competitions, and especially after Criteo released 11GB for the training set of a Kaggle contest. Online learning allows to process massive data as the learner processes data in a sequential way using up a low amount of memory and limited CPU ressources. It is also particularly suited for handling time-evolving date.

Vowpal Wabbit has become quite popular: it is a handy, light and efficient command line tool allowing to do online learning on GB of data, even on a standard laptop with standard memory. After a brief reminder of the online learning principles, we present how to run Vowpal Wabbit on Hadoop in a distributed fashion.

Read more:
https://2015.berlinbuzzwords.de/session/online-learning-vowpal-wabbit-and-hadoop

About Heloise Nonne:
https://2015.berlinbuzzwords.de/users/heloise-nonne

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              Oh                               so hi everyone I'm many ways I am data                               scientist at Konami tree this is our                               team you've already seen it this morning                               by at mature and Isabela presentation                               and I am to tell you about an                               experimental project that we we did kind                               of a fan in addition to our consulting                               activities but I am to tell you about                                some some work involving online earning                                baba rabbits and Hadoop I think is that                                everyone is tired this is the end of the                                conference config back is coming so I'm                                gonna start with the story so a long                                time ago in the country far far away                                after one of the most terrible conflict                                the world has ever known a few inspired                                men inventing one of the greatest                                inventions of all time the Turing                                machine actually wasn't really built but                                anyway a few years later some guys came                                up for the first computer and by by                                   they were able to store three megabytes                                in big machines like that I kind of cool                                so people thought we've got artificial                                intelligence solved we're almost done we                                people invented neural networks and                                people thought ok we're going to we're                                going to build intelligent system clever                                more clever than men and things are                                going to be quite easy in the end didn't                                work that well so next episodes reality                                strikes back and you don't have enough                                computing power you don't have enough                                storage capacity we don't have enough                                data and your algorithm are not                                efficient enough so in the end we are                                still stuck with the regressions                                people are doing regressions today okay                                we still we do some more some more fancy                                stuff right now right now but still many                                people do regression and point is now we                                can do it fast we can do it really fast                                and in a very efficient way so that's                                what I want to talk today about so you                                have a regression what you do it's                                simple you have n samples you have many                                features and you want to predict the                                number so you fit a linear model so you                                have a model with some parameters cold                                weights and this model gives you a                                prediction and you know reality so                                you're going to to see how much error we                                do so you can do this on a simple                                regression or you can you can classify                                this you can classify you have                                        have red blue you have if you're at                                retail you're doing click or not click                                Shakespeare to be or not to be anyway                                when you have a regression you need to                                choose a loss function that's going to                                account for how much error your model                                does so according to use case you choose                                different type of loss function that                                accounts for this error you can choose a                                least square for if you're doing simple                                regression of you if you're doing                                classification it choose logistic                                regression or hinge regression                                there are many type of loss function you                                can choose but in the end this is a                                function that accounts for the error and                                you want to minimize this so you're                                going to change your your weights are                                going to according to this traditionally                                you have your doing batch learning so                                with batch learning you have n samples                                you have a model with some parameters to                                weights you initialize this and when you                                initiate this you loads all your                                datasets into memory                                and you you take a sample you predict                                what your model says about this sample                                and you compare it with reality and that                                gives you the individual last function                                of your sample then you do this follow                                your sample and you accumulate                                everything into a global loss function                                that gives you the global error all of                                your model on the entire data set having                                computed this global loss on the entire                                data set you have some simple update                                rule that tells you how to change your                                weights in order to get your model to be                                closer to reality to give better                                prediction and you do this you do one                                iteration at a time but computing and                                computing each time on the entire data                                set and then you iterate many times each                                 iteration involves the entire data set                                 you have many of these and the                                 complexity of each each iteration is o                                 of n so it's very very expensive in                                 terms of computation so to give you a                                 picture of what happens this is the                                 parameter space so you have you have two                                 weights here but you may have a higher                                 dimensional space and for each iteration                                 you're trying to find which is the                                 direction that is going to bring you to                                 the minimum of this last function                                 because the minimum of the last function                                 here you have the contour of your global                                 loss function and you're trying to reach                                 this place where this is where your                                 error is minimal but you can't do it                                 straight because otherwise you start at                                 some point with your weights that are                                 initiated and and then you cannot choose                                 a direction because you're not going to                                 go straight to the minimum so you need                                 to do many it rash iterations one a one                                 after the other each time with a                                 complexity of oh of n in order to find                                 bit by bit to to go down the slope of                                 your loss function and this is really                                 expensive and the problem is that you're                                 working with your entire data set into                                 in-memory so what if your data doesn't                                 fit in memory then you you're stuck you                                 don't want to subsample because you have                                 may have many parameters so you need all                                 the data you can get to have a good                                 predictive model so I don't want to                                 subsample my dear                                 what if also you have sample data said                                 that fits in memory but you have many                                 features and you want to combine them                                 together maybe if you combine two                                 features it's going to give you a lot                                 more information that just just one each                                 each of them or if you're doing natural                                 language processing you're going to want                                 to combine words together so you're                                 doing in grams and this inflates your                                 data set and that may not fit in memory                                 anymore what can happen is also maybe                                 some time you you trying on a death set                                 and at some point someone gives you a                                 new column and you'd like to learn this                                 new column but you don't want to lose                                 all of you what you have learned on the                                 on the on the data set before so you'd                                 like to be able to end to handle this                                 kind of new features you have a so                                 phenomenon such as when your model                                 drifts with time you're trying to model                                 something for instance the stock market                                 is going to change with with time and                                 you cannot learn the model long before                                 and then predict later because things                                 have changed in the in in the mean time                                 so you'd like to do some kind of                                 learning that can evolve evolve with                                 your phenomena and so on and learning is                                 one of the solution to to disable to                                 deal with that in a very efficient way                                 so it works the same way you have n                                 samples and and you have a predictive                                 model that you initiate with some                                 weights and then you actually load just                                 one sample and you work on one sample at                                 the time in memory                                 so you take this sample you computes                                 with prediction you can compute in the                                 individual loss function on this sample                                 but just this one and have an update                                 rule that just take the individual loss                                 so and a new iterates a lot of time so                                 complexity is just all of one because                                 kind of all one because you just have                                 one sample at a time and to picture                                 what's what's going on there                                 on the global loss because this is the                                 global last you want to minimize if you                                 work one sample at the time the sample                                 you're minimizing the loss function of                                 this sample so you may not go in the                                 direction to the minimum of your global                                 loss function but you on average                                 actually you're going you're going in                                 the right direction maybe your sample is                                 not representative enough of your entire                                 data set so you you may wander around                                 your global loss function but in the end                                 you get there and the thing is that you                                 do it very often you iterate many times                                 on your data set and you're going fast                                 to the minimum so you you you stabilize                                 this kind of stochastic behavior you                                 stabilize this by doing many updates one                                 on one sample at a time                                 so you approach the minimum very quickly                                 problem with online learning is that you                                 approach this minimum very quickly but                                 one when you're there you're still                                 processing samples so you get one sample                                 and this sample is going to tell you ok                                 you're there but my minimum is is not                                 there is there and then you move there                                 by with your add a true and then you                                 process another sample these samples is                                 you know my minimum is not there is                                 there so that's why you wander around                                 your minimum because what you're                                 minimizing is the loss function of an                                 individual sample at a time and not the                                 global loss function so that can be a                                 problem in principle but in practice                                 it's not it's not so bad so to give you                                 an idea of a comparison between online                                 earning and batch learning here in blue                                 you have online earning and in red you                                 have batch running here it's the                                 accuracy of your model so when you go                                 write your error is going to to diminish                                 and on y-axis you have the training time                                 so online learning to reach a given                                 accuracy online learning is a lot faster                                 than budget availing at least at the                                 beginning because it's going very fast                                 to the minimum and at some point things                                 cross batch learning becomes more                                 becomes much more efficient to find the                                 minimum because it's noiseless online                                 learning is going to continue wander                                 around a minimum while batch learning is                                 slow to get there but once it's there it                                 knows where it is and nice idea is to                                 combine those two approach start with                                 online earning and then do a batch                                 learning for the last few bits of the                                 last few steps to reach the minimum so                                 what I want to tell you about is a                                 library that implements online earning                                 and that we we started to use a few                                 months ago which is called and so it's                                 called wobba wobba wobba wobba - it's a                                 straight name but it's it's an open                                 source project that's been developed at                                 Yahoo by John Lankford it's written in                                 C++ and it's implements online learning                                 very very efficient way and as you can                                 see on you can see maybe you can't see                                 you so well for some people but last                                 year there was a really increase of                                 interest in vibhava bits that's                                 translating to the number of comments                                 you can see on the project so people                                 really got excited about that I'm gonna                                 tell you a bit why why is it so                                 interesting to use                                 so Papa rabbit has many algorithm                                 implemented in its system so you have                                 regression you have fancy regression you                                 have k-means we even have nonlinear                                 models such as neural networks or SVM so                                 you've got basically you've got                                 everything you've got more than the                                 basics of machine learning in there and                                 it's implemented in a very efficient way                                 with clever algorithms such at vs GS                                 conjugate gradients anyway                                 the clever algorithm plus some tricks to                                 to do the learning in a very efficient                                 way it can also it has the ability to                                 combine features do engrams process text                                 and everything and the other thing is                                 that you                                 it has implemented by default some                                 automatic tuning of your hyper                                 parameters the learning rates for                                 instance is how fast you go down to your                                 minimum and this is very critical and in                                 practice to to train another Gordon but                                 it's very difficult to tune it depends                                 on your use case and everything so                                 actually the default settings of Papa                                 rabbits allow you to reach already an                                 efficiency without much work and that's                                 very valuable when you start to discover                                 datasets you want to be efficient very                                 fast so you can tune the last bit and                                 low-power a bit is good for that because                                 it's it's kind of efficient by default                                 in most cases and it has some more                                 features that are pretty nice another                                 thing is the input formats it looks a                                 bit weird at the beginning but so so                                 that's that's the kind of input you have                                 to to to give to the power bit so here                                 you have a label it's                                                  may have some weight doesn't really                                 matter and then your features that are                                 separated separated by pipes so you can                                 bubble your features together give it                                 numbers you can process binary in                                 numerical features categorical features                                 and it can process text very easily                                 it can also handle missing values like                                 here you see you have two two features                                 height and length and here just have                                 length doesn't matter for what pal                                 because it doesn't expect to have all                                 the features at all time so it handles                                 this sparse features a set and missing                                 values very well and that's quite                                 valuable when you have the dates that                                 you want to discover because it allows                                 you to just avoid some data preparation                                 step you want to avoid when you start to                                 try to understand what's what's going on                                 in your when you dead set and what                                 you're trying to model so summarize                                 low-power a bit it's an efficient                                 leverage you can do fast turning in                                 itself for scoring that's what it was                                 designed for but also the other feature                                 can allow you to not spend too much time                                 to prepare your data to start to                                 discover phenomenon so it's also a very                                 good tool for experimentation maybe even                                 if you plan to use a more complex model                                 later it's it's quite nice so you're                                 fast you're doing online learning with a                                 fast fast library but you would like to                                 you'd like to paralyze this and so you'd                                 like to paralyze the power a bit you can                                 do it actually it's been implemented in                                 football but why would you do this so                                 first you you're fast but you want to                                 speed up you want to spread your jobs on                                 different nodes so you'd like to be                                 faster or maybe your data isn't fits in                                 your in a single machine storage maybe                                 you will have one terabyte of data that                                 doesn't fit in your laptop and you don't                                 want to subsample or you have data you                                 want to take advantage of this rich                                 storage so you don't move that that data                                 around your clusters and the other the                                 other reason you would want to paralyze                                 is to take advantage of this distributed                                 memory because on signal mushing you're                                 having a memory to maybe inflate your                                 data set                                 by combining features together so how to                                 do this so the idea is as I told you                                 nice ideas to combine online learning                                 and batch learning so you would send a                                 job too many notes ask them to do an                                 online pass to start learning on each                                 one on its on its own data and then you                                 would like to send back the weights that                                 it has learned somewhere to average the                                 knowledge that's each node has                                 accumulated so you combine this together                                 you average this and you Brooke you                                 would broadcast down this average back                                 to the notes and maybe start other                                 running so doing batch learning and                                 maybe do some iteration so maybe so you                                 really reach your minimum of your loss                                 function and it's been implemented in                                 verbal a bit projects by John Langford                                 and some collaborators and I will                                 present this approach that they use they                                 implemented and gives you a little                                 account for our experience using it so                                 what I wanted to do is to capitalize to                                 use both all reduce which I'm going to                                 tell you about later or reduce and Map                                 Reduce                                 so what you need to do when you paralyze                                 this this thing is you you want an                                 effective communication infrastructure                                 because you need to communicate do                                 weights between nodes and and everything                                 you need that that's what that eccentric                                 platform so MapReduce is no good for                                 that because you have full knowledge of                                 the data allocation and handles handles                                 things well you want a fault-tolerant                                 system and you you want some way to not                                 to recode everything because you've got                                 a nice implementation in c plus with                                 with wapa rabbit and you'd want to                                 recode all your implementation or your                                 optimization to java or some other                                 language so you'd like to be able to                                 avoid rewriting everything                                 and I'll reduce so the combination of                                 all reduce and Map Reduce they use                                 allows to to do this paralyzation all                                 reduce is based on a communication                                 structure entry so you have a tree you                                 build a tree on over your notes and each                                 node has a number it's like the result                                 of the online pass it's the weights so                                 here you have one weight but you have                                 actually a vector of weights and first                                 type each one starts with a number and                                 the first step is to this reduce step so                                 you want to sum up your weights all the                                 way up your tree in order to do the                                 average so you're going to sum up these                                 two children are going to send that                                 number to the parents and the parent is                                 going to make the addition so                                            is                                                                      then you do you do the sum up to the                                 parents and you end up with                                           top and then the second step of our                                 reduce is to broadcast down the result                                 of your addition down to all your tree                                 and every node ends up with the sum                                 everybody and it just need to divide by                                 the number of nodes to have the average                                 of the number so that's quite a simple                                 idea and that's why they when they in                                 turn did this they thought it was good                                 an efficient communication                                 infrastructure to build them and                                 combining with MapReduce that gives you                                 the the implementation of                                 parallelization of Apple a bit on Hadoop                                 so you start your daemon that's your                                 communication system and then you send                                 your your job with MapReduce actually                                 send just a mapper you don't have a                                 reducer you just send a mapper to each                                 node and each node is going to make an                                 online passwords data then you                                 initialize your tree you bind                                 three that allows you to to send back                                 your your weights to the parents to the                                 masternode you use all reduce algorithm                                 to average the weights over all the                                 notes that's the result of the ulna pass                                 and then you send it back to all the                                 notes so that they can do their batch                                 steps so there are a few bad steps to do                                 in a row and so they do their bad step                                 they sent back the weight its average                                 and then it's sent down again to the                                 notes so they can do another bad step                                 and you iterate so the advantage of this                                 implementation is that you have a                                 minimal additional programming effort in                                 the sense that you're using MapReduce                                 you hacking somehow MapReduce just to                                 send a mapper would pop all wabbits                                 comments asking for power that's                                 installed on your all your all your all                                 your nodes to do the job so my produce                                 is just kind of a way to send job and                                 and that's it and you capitalize on the                                 data allocation knowledge of MapReduce                                 with this implementation all reduce                                 allows you to have a small                                 synchronization of our heads because you                                 have a tree so you're in log of number                                 of tree so the time spent in all this                                 operation is much lower than the                                 computation time on each node so you'll                                 find there are also tricks to use to                                 capitalize on Hadoop speculative                                 execution to handle the problem with                                 snow nodes and dead nodes that's quite                                 clever but I could go I could discuss                                 that with you later I don't want to go                                 into details and combining online                                 learning and batch learning gives you                                 rapid convergence because with online                                 learning paths you're reaching the                                 surrounding of your minimum and then                                 batch learning helps you to do the last                                 steps of the running                                 so that's the theory and we thought okay                                 it's cool it's implemented it's gonna                                 work so you download the code on github                                 and in principle log work so you know                                 the theory you've read paper you                                 understand how it works and in principle                                 you start reading and there is a code in                                 the github called the spanning tree it's                                 set up to your communication system you                                 launch your MapReduce job                                 you kill your spanning tree and it works                                 and your MapReduce job is using Hadoop                                 streaming it's very simple you want                                 speculative execution you don't have any                                 register input output that's simple you                                 load a few libraries that verbal web it                                 needs to work and you give it a mapper                                 and that's it but it doesn't work that                                 well so first we decided to use Amazon                                 Web Services to do this but honestly it                                 was the first time we with we used it                                 it's a bit different that the usual                                 Hadoop cluster we were used to with with                                 Amazon what you do is you use transient                                 Kuster so instead of having you cluster                                 up and running and signing job and                                 monitoring what's what's going on what                                 you have is your data on s                                              you start an EMF coaster you start your                                 cluster you maybe strap some actions                                 like installing libraries such as low                                 power E's and other things you need some                                 configuration so you can just do it with                                 the simple scripts and then you run your                                 job                                 that's called step in in Amazon Web                                 Services and when the job is done you're                                 shut down you cluster so actually your                                 cluster is only live during it is it's                                 live only during the time of your                                 MapReduce job and then it's killed                                 afterwards so it's a bit weird when                                 you're not used to it it has some                                 advantages and it has some drawbacks                                 advantages is that it's easy to say that                                 it works you don't have to do                                 maintenance it's low cost problem is                                 that we really struggle to find the logs                                 the logs are easy to find on how to                                 cluster because you know where they are                                 what they are and it took us some time                                 to figure out what to find them and                                 there is some configuration that's not                                 by defaults you need to set up debugging                                 debugging option in Hamas on so really                                 read the dark of Amazon because                                 otherwise it's difficult to find so once                                 we we got this Amazon cluster up and                                 running you you need to make it work                                 so Papa rabbits needs to work it needs                                 MapReduce environment variables it needs                                 to know the number and the total number                                 of of tasks because it just needs to                                 divide the sum over all nodes by the                                 number of nodes so it's not nice to know                                 that it needs to know in some IDs it                                 needs to know the private DNS of the                                 parents and the children in order to                                 communicate with all reduce and so the                                 problem is that this implementation of                                 paralyzation of a puppet was done for                                 Hadoop one and the name of name of the                                 environment variable changed so you have                                 to change then you have to find out                                 change a bit the code that you can find                                 on github and the MapReduce the mapper                                 that you have is written in shell in                                 bash and getting the environment                                 variables were not possible so we had to                                 hacker a bit with Python to to reget                                 this thing and took us some time to                                 figure out what what was the problem                                 another problem that you have when you                                 try to run this is the number of splits                                 you need to brute-force the MapReduce                                 job to to do the number of plaits you                                 you want that means the number you know                                 you need the number of splits to be                                 equal to the number of nodes you want to                                 do your job on                                 and we got some hints from jong-hong                                 ford codes because he was kind of                                 computing this by by inside the code and                                 using this option but it doesn't work                                 anymore so we kind of did we were we                                 were shorting time so we do dirty work                                 around to split the data and put                                 everything into a gzip file because GC                                 files are not are not spit able so my                                 producer won't try to split it more than                                 you want and there was a last thing if                                 you try to run it there is an option                                 called called that involves the number                                 how much RAM you want to allocate to                                 vibhava bits and this I haven't figured                                 out what's the problem with it yet but                                 you have to be aware of this Ram                                 allocation                                 also that doesn't work very well on a                                 single machine even if you paralyze on a                                 single machine but doesn't work anymore                                 on Hadoop being aware of that we solved                                 all this and as you can see it's it                                 works so here this is the log from the                                 node so this is this comes from the                                 bagua bit and here you can see it's the                                 IP number of the nodes it starts raining                                 and does some stuff so it gives you some                                 parameters it's going to use and there                                 it starts online learning and when it's                                 done when it's done when it's completed                                 so the first thing is the last thing of                                 all the all the Passover dataset it                                 connects to its parents send its waits                                 waits for the answer from parents                                 because apparently going to send back                                 the the average and then it tells you                                 some help on how much examples it it has                                 been working and then you do the batch                                 the batch part so BFGS is a kind of an                                 implementation of batch learning it runs                                 it connects to its parents it waits for                                 the answer with the sum over all nodes                                 and when you reach the                                 of steps you've set you it stops and                                 sends the results back to back to                                 masternode so I tried to do a benchmark                                 actually this was done yesterday at                                      a.m. so a bit short but so I've tried I                                 haven't savvy an eyeball with a hotel to                                 upload a lot of data on Hadoop but just                                 with six gigabytes that means                                       million example running on                                             features means that I've done many                                 combination of all this feature on a                                 single machine means on my laptop was                                 able to run this with just online                                 earning in                                                               worked in                                                                a lot and you have to take into account                                 that this was quite a small data set and                                 on that that I said it should really                                 speed up your online earning pass for                                 similar similar accuracy so to conclude                                 about online learning online learning is                                 is fine when whenever you your                                 computation time is the bottleneck of                                 your job then you should use the online                                 learning even if your laptop even if you                                 have enough RAM it's always good to to                                 use online learning because you can then                                 you can process more data and you can                                 include more feature more combination in                                 your analysis and that's very useful for                                 scoring but also very useful for                                 research and experimentation and data                                 set and working this projects we got to                                 really understand what was going on so                                 understand the basic of optimization                                 algorithm and understand a lot on how to                                 put this on a Hadoop so we really were                                 very interesting and those guys long for                                 number two I got one they                                 did a lot of work and in implementing                                 this and speaking speeding up online                                 learning so the last episode is coming                                 soon there are still things to debug on                                 the codes but we were willing to do some                                 pushes on get up so if you are                                 interested just it's going to come soon                                 when we solve the last bit of problems                                 and what we would like to do is of                                 course benchmark on very large data set                                 more more just more large data set and                                 they are available there are solutions                                 such as graphlab and ml leap that                                 implements online learning and we would                                 like to know how how about what it                                 compares with with this dissolution and                                 so if you have any question um I'm done                                 and thank you for listening                                 with questions
YouTube URL: https://www.youtube.com/watch?v=anZa5gxJPjQ


