Title: Berlin Buzzwords 2015: Ema Iancuta & Radu Chilom â€“ In-memory data pipeline and warehouse at scale
Publication date: 2015-06-05
Playlist: Berlin Buzzwords 2015 #bbuzz
Description: 
	Live demo of building an in-memory data pipeline and data warehouse from a web console with architectural guidelines and lessons learned. The tools and APIs behind are built on top of Spark, Tachyon, Mesos/YARN, SparkSQL and are using our own open-sourced Spark Job Server and Http Spark SQL Rest Service.

Last year Spark emerged as a strong technology candidate for distributed computing at scale, as a Hadoop MapReduce and Hive successor. Tachyon is a very promising young project that provides an in-memory distributed file system that serves a caching layer for sharing datasets across multiple Spark/Hadoop applications. Mesos and YARN are resource managers that ensure a more efficient utilization of the resources in a distributed cluster. Parquet is a columnar storage format that enables storing data and schema in the same file, without the need for an external metastore.

In this talk we will showcase the strengths of all these open source technologies and will share the lessons learned while using them for building an in-memory data pipeline (data ingestion and transformation) and a data warehouse for interactive querying of the in-memory datasets (Spark JVM and Tachyon) using a familiar SQL interface. 

Read more:
https://2015.berlinbuzzwords.de/session/memory-data-pipeline-and-warehouse-scale-using-spark-spark-sql-tachyon-and-parquet

About Ema Iancuta:
https://2015.berlinbuzzwords.de/users/ema-iancuta

About Radu Chilom:
https://2015.berlinbuzzwords.de/users/radu-chilom

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              um hi my name is a Mayan kusa and                               together with my colleague radical on we                               will present in memory data pipeline and                               warehouse at scale using spark spark                               sequel tachyon and parkette both of us                               we are software engineers at a teacher a                               teacher is a company that provides big                               data analytics and machine learning                               solutions a teacher worked with Hadoop                                for more than six years and then switch                                to spark and so we have been working                                with it for the past two years in our                                free time we build up a big data                                research community that focuses sorry I                                don't know how to that focuses on the                                technical problems that this field has                                and offers of open source solutions                                after this quick intro will describe our                                use case we will speak about how to                                build a data pipeline with spark we will                                speak about our to open source projects                                specialized service and the sparks                                Aquarius service code named Jose there                                will be a few words about Poquette                                briefly go through the benefits of using                                tachyon there will be a too-short ademas                                also so our use case is to build a                                memory data pipeline that will process                                millions of financial transactions that                                would be yours downstream by the data                                scientists in order to detect fraud                                first we will need to ingest the data                                from my stream into tachyon cluster                                backed up by a cluster of HDFS HFS will                                be the underlayer file system for                                tachyon then we will have to transform                                the data in order to fit the                                requirements of our data scientists and                                in the end we will want to build a                                warehouse that will allow to run                                interactive queries with sparks equal in                                the next part of our presentation I will                                letter I do deep dive into the first                                challenges this use case brings up high                                next time going to present to the first                                part of the in-memory data pipeline                                which                                assists in two steps downloading and                                transforming a large amount of data as                                in memory distributed engine we've                                chosen apache spark a few words about it                                spark is a fast in general engine for                                large-scale data processing it was build                                around the concept of our DD which is an                                a beautiful collection of elements                                partitioned across the clusters nodes                                spark has a rich epi consisting in                                eighty high level operators and on top                                of it there is a stack of high-level                                tools including ml lib and spark sequel                                for our use case we want to download the                                data from an external source public s                                 bucket named public financial                                transactions even if we are downloading                                for s                                                                 that I'm going to present apply to other                                external sources so as you can see the                                structure for this bucket is very simple                                the scheme is in the scheme folder and                                the data files are in CSV format and are                                divided into multiple data folders the                                easiest way to get the data into our                                cluster would be to use the method whole                                text file let's park on text offers as                                input for this method we have a regular                                expression containing two wild cards the                                first wild card will match all the folds                                inside all the folders inside the bucket                                and the second wild card will match all                                the files inside the folders in order to                                transform from wild cards into file                                names behind scenes sparklies the files                                metadata this is a problem because                                listing the metadata for a large number                                of files in an external source can take                                a long time so in order to improve this                                we can list the metadata distributed so                                instead of listing all the files on the                                driver as we did before we can now get                                the folders list there paralyzed it                                and get the metadata for the files                                inside each folder in a distributed way                                on multiple cores let's see the code for                                this so as you can see in the first part                                or obtaining the folder list with the                                use of an s                                                        paralyzing the list by the way it's                                worth paralyzing only if the folder list                                is greater than                                                       only one folder then it will only move                                 the computation from the driver to one                                 of the workers ok and then with a flat                                 map we're going to get to go to list all                                 the files from the folders by default                                 the Paralyzed method creates a number of                                 partitions in our cluster that depends                                 on your cluster manager however in order                                 to fine-tune your jobs you can specify                                 the number of partitions by adding an                                 extra integral parameter to the method                                 call as a tip you will probably want to                                 have the number of partitions as least                                 as the number of available course in                                 order to fully benefit from our                                 computation power ok the dell the                                 download part is pretty straightforward                                 we're doing a map in which were                                 initializing an s                                                   downloading the file the problem that                                 might arise here is if you have                                 unbalanced partitions so we can end up                                 in a situation similar to this where the                                 data is not evenly distributed among                                 partitions this happens because the                                 default partitioner is a hash petitioner                                 and for the string data type it doesn't                                 always provide an even distribution as                                 you may know each part the elements in                                 each partition in spark are processed by                                 a separate task so the real problem is                                 unbalanced partitions are that while                                 some of the tasks finish very very                                 quickly the rest of the                                 of the tasks struggle to finish this                                 leads to an increased overall time for                                 your stage okay a solution for this                                 would be to rebalance our partitions by                                 hashing on an integral index we can                                 achieve this by transforming the                                 filename rdd into a parody be that would                                 have as a key a unique index attached                                 okay so now let's see the code for this                                 as you can see we can achieve this with                                 the Zipit index operator and what this                                 does is that it adds to each element of                                 our rdd a unique index then we have to                                 use a map operator in order to                                 interchange the key and the value and so                                 the index ends up being the pair ski                                 after that we want to repartition our                                 rdd in order to shuffle the data around                                 and balance the partitions please keep                                 in mind that repartitioning your data is                                 a fairly expensive operation because it                                 shuffles data across the cluster however                                 in some use cases the benefit of                                 repartitioning is much more greater than                                 the penalty it inflicts now let's move                                 to the data transformation step data                                 cleaning is the first step in any data                                 science project in our use case we have                                 to remove the lines that don't match the                                 structure and some of the columns that                                 are useless for our analysis another                                 thing we want to do is transform the                                 data in order to met to have a                                 consistent format so we have two columns                                 both representing country codes the                                 problem is that one is in numeric format                                 and the other one is in alpha numeric                                 format so in order to be consistent we                                 can join the transactions RDD with                                 country's my part d which is this and                                 change the value from the transaction                                 from numeric to alphanumeric the                                 problems with joints in spark and also i                                 think when doing hive arise when you                                 have asked you in your key distribution                                 so as you might know all the values for                                 a for a key he'll have to be processed                                 in the same task so if a key is very                                 frequent much more frequent than the                                 others the task that will process that                                 key will take much more time to finish                                 you can spot the SKU key distributions                                 by looking at the summary metrics when                                 running an app an action on top of your                                 joint transformation so as you can see                                 here while seventy percent of the task                                 finish in under two seconds the maximum                                 task took                                                              to look into the shuffle redfield and                                 you can see that there are very large                                 differences so when you can see this                                 kind of differences in a joint or in a                                 group by it's very probable that you                                 have a skewed key distribution we can                                 achieve this transformation by also by                                 shipping our country code map to each                                 worker in our cluster we can do this                                 with a broadcast operator which takes                                 your value and send it to each node of                                 the cluster this method is suitable only                                 if the data you are shipping to the                                 cluster it's not that big because it                                 inflicts serious IL penalties as you can                                 see in the code after broadcasting the                                 country's map we can achieve our                                 transformation with a simple map or flat                                 map in which change the value now let's                                 take a look at the matrix from running                                 an action on top of this transformation                                 as you can see the running times are now                                 very close in range and we can also say                                 that about the input                                 so yeah when each task process is the                                 same amount the duration is only seven                                 seconds per task instead of having                                 multiple tasks that finish in two                                 seconds and one that runs in                                            now I want to show your chart with the                                 running times from run from running with                                 the running times of the both presented                                 transformation versions as you can see                                 the broadcasted Mac version is much more                                 faster and it looks linear while the                                 joint version looks exponential however                                 this only applies when you have skilled                                 key distribution now that we have                                 implemented a downloading job and                                 transformation one we have to run them                                 we can run them on top of our job rest                                 which is a restful server that provides                                 context management and job submission                                 services spur job rest is an open source                                 server which we created in order to fix                                 the inability to run multiple spark                                 context in the same jvm this is a spark                                 or bug that still hasn't been fixed and                                 that also affects we all as per job                                 server from each by the way we've                                 started but facing the urge to run on                                 multiple contexts we came up with this                                 server that as a solution spins up a new                                 process for each context it creates i'm                                 going to show you the quick demo now                                 okay so this is the UI for spar job rest                                 it context in it has three sections                                 context jobs and jars in the context tab                                 you can see all the running context on                                 your cluster you can create one or you                                 can delete one a very nice feature is                                 that we have managed to spark you I port                                 in order to show it here and also in                                 order to give you a quick link to the                                 spark you I page where you can see all                                 the tasks that have run on your context                                 in order to create a context you have to                                 provide a jar with your classes so you                                 can go to the jar section and upload                                 your jar as you can see here I have to                                 upload it jars one of them is s                                  download job I'm going to create a                                 context and run that job okay so in                                 order to create the context you should                                 specify a context name let's say data                                 download context a jar a nice cool                                 feature is that here you can add                                 multiple jars and also you can provide                                 HDFS path so if you have your jar in                                 HDFS you don't need to download it and                                 upload it to the server in the parameter                                 section you can specify any spark                                 parameters that you want to be set on                                 your context so for example we can set                                 the spark executor memory to be four                                 gigabytes so now okay an error                                 yeah I misspelled park executor here                                 four gigabytes okay so when the server                                 received is request it will spin on you                                 processed and on that process it will                                 initialize a spark context as you can                                 see the context was created now we can                                 go to the job section here you can see                                 all the jobs that were finished or that                                 are still running on your context a nice                                 cool feature is that you can see the                                 error directly here if it failed so you                                 don't have to go to the log files also                                 you can see the output of the jobs this                                 is the output for the normalization job                                 and now let's run the download job so                                 you have to specify the full name of                                 your class you have to choose your                                 context and here you can enter any                                 parameter that you want in order to                                 customize your job so the s                                           job was created with three parameters                                 the first parameter is the pocket we                                 want to clone so yes three bucket it's                                 public the second parameter would be the                                 number of partitions in which we want                                 our data to be paralyzed and the last                                 parameter would be the output for this                                 job but I didn't mention anything about                                 this until now because we want to stay                                 in memory we want to put our files in                                 memory so i will write to tachyon                                 tachyon is the in-memory file system                                 about which my colleague Emma we'll talk                                 later                                 so I have to provide the valiant valid                                 tachyon path this is similar to HDFS you                                 have to specify the master the port and                                 the path where to write ok now let's run                                 the job as you can see the job have                                 started its running we can go in the                                 context tab and into the spark you I and                                 here we can see the stages that I run on                                 top of our our context as I've said you                                 can go here on a stage and see the                                 summary metrics for your for your stages                                 ok now if you don't like to use the UI                                 spur job rest also offers an HTTP client                                 which provides an abstraction on top of                                 the of the rest for later so you can go                                 and write code in order to execute our                                 pipelines oh now that the first part of                                 the pipeline is over I will let Emma                                 continue it creating an in-memory data                                 warehouse                                 okay so there are a lot of sparks or a                                 lot of sequel engines that can run on                                 top of data stored on ETFs I could have                                 could have used them all but I choose                                 Park sequel i chose particle because                                 we're great fans of spark and because we                                 invested in an open source project that                                 submits queries on top of it for those                                 of you who don't know spark sequel is                                 one of sparks modules that is used for                                 processing structured data spark                                 sequence supports several input formats                                 like files stored in Burkett or JSON or                                 the data stored in hive tables where the                                 actual data laid resides on a GF s and                                 the schema is in the hive meta store it                                 also supports as an input already                                 existing rd DS or data stored in                                 external tables like Cassandra for                                 example data that it's retrieved through                                 JT be seen sparks equal organizes its                                 the data into distributed collections                                 called data frames or the former schema                                 LEDs for those of you who are more                                 familiar with previous versions of                                 scoffs part we can work with it either                                 writing code in Java in scale our Python                                 or we can execute sequel statements on                                 top of the sparks equal context okay so                                 the data of your of our pipeline until                                 now was exported in CSV format in                                 tachyon in in memory so in order to                                 query it we will have to create that                                 obstruct ization that data frame that i                                 mentioned before and in order to do that                                 we first need to have we first need to                                 have a spark sequel context and we will                                 use this context in order to read the                                 schema schema I said that it's comma                                 separated so we will read the file from                                 memory split it after coma and then map                                 each field into a struct field that will                                 create the struct type containing our                                 schema                                 the data will be writing the same in the                                 same way we read the data file split it                                 after coma and then map Ichiro inside                                 your data file into a spark sequel at                                 all that will be stored in the in an RDD                                 containing rose so now that we have the                                 schema created and we have the robot DD                                 created we can create the data frame                                 that will use to query to have a look                                 inside our data so in order to explore                                 it we can perform simple queries either                                 directly using the methods built-in upon                                 a data frame here is an example of a                                 group by or you can register your data                                 frame into a temporary table and then                                 execute sequels sequel commands on top                                 of it however this it's not is not that                                 easy to do so you won't want to map your                                 data into a data frame and then register                                 it as a table each time you query each                                 time your spark sequel context dies all                                 you have when you you create a new                                 session so what would you like to do is                                 to persist your tables somehow in order                                 to share them across sessions you have                                 several ways to do that either creating                                 a hive table executing a sequel                                 statement on top of hive context as you                                 can see here I have a hive context this                                 actually extends the sparks equal                                 context and adds the functionality of                                 operating with tables with the data                                 stored in hive tables another way to to                                 share the tables between sessions is to                                 save your data frame as a table like                                 here what does this mean you're that                                 you're a table will be created the                                 sparks equal table will be created                                 having the schema stored in the hive                                 meta store okay but we're building                                 in-memory data pipeline so the ingestion                                 was in memory the transformations were                                 in memory so what I would like to do is                                 that my warehouse works with files                                 stored in memory what I like to do is                                 save the data frame into                                 a pocket format inside tachyon if                                 possible so I will be tempted to do this                                 in the following way however this won't                                 work it won't work if the driver the                                 spark driver it's on a machine that                                 doesn't have a tachyon worker it will                                 fail during the schema during writing                                 the schema I into into the pocket of the                                 packet file a colleague of ours open                                 sourced an API that fixes this issue by                                 sending the code that executes the                                 railroad that writes the ski mind to                                 park at file in tachyon to the workers                                 so in the job I have created for                                 exporting the files to talk you and I                                 have used his library you can take it                                 from github or you can download it from                                 from the maple may want public reckons                                 it's published okay there are several                                 file formats that we have used for                                 different pipelines however we favor                                 parkett because being a columnar data                                 representation format speeds up the                                 aggregation queries so for the                                 long-running query also if so for query                                 queries that required only some columns                                 but there will be read only the required                                 ones and not the entire row what's cool                                 about it is that it supports nested                                 sorry nested data types and the schema                                 resides with the data so you won't need                                 an external meta store to store the                                 schema for the files also working with                                 parkett and will spark sequel it's nice                                 because it supports schema evolution so                                 if you have some files in park at format                                 having a schema and then you add another                                 file that adds a new field then spark                                 sequel will know to merge those schemas                                 together the most important feature                                 though that I like about parkette is                                 that supports efficient compression and                                 encoding schemes as they can be                                 specified per column and if there is                                 something that I have learned while                                 working with files in                                 is that the footprint of your data it's                                 more important than the compression and                                 the compression speed because spilling                                 files to disc it will always be more                                 expensive than a slower the compression                                 algorithm of course it depends on how                                 large your data set is and how many                                 memory you have on your machines it's                                 something you will have to balance this                                 battle between the disk i/o and CPUs ha                                 ok I have mentioned tachyon tachyon it's                                 a memory file system and what it's                                 really cool about it it's that is not                                 going it's not going to disk each time                                 you want to read some data that you                                 frequently use it avoids this by keeping                                 it keeping the data in memory and the                                 fact that the data is in memory makes it                                 possible for tachyon to allow different                                 frameworks running on the cluster to                                 access the same data set with the                                 in-memory speed tae-hyun spills the data                                 also to disk into a pluggable underlayer                                 file system that could be either HDFS or                                 s                                                                       fits you best there is another way to                                 keep your data in memory you could use                                 Park cash for that you could cash or                                 tables either calling the cash method on                                 top of your sequel context either                                 calling the cash method on your data                                 frame or executing on standard sequel                                 cash table comment on your context it                                 doesn't matter which of these methods                                 you choose because Park will catch your                                 data in a columnar format in memory and                                 will fine-tune the compression in order                                 to to minimize the pressure on your                                 memory so if we have spark cash why                                 using tachyon well there are a set of                                 reason why we like it it's because                                 sometimes power context my crash and it                                 does if it does and if it does you will                                 lose your data from memory and                                 this means that you will have to upload                                 to cash once again your your table or                                 you will have to trigger once again your                                 transformations and for large data sets                                 it's something that you won't want to do                                 so using tae-hyun you avoid this because                                 the data will be outside or jvm and                                 sorry outsider jvn so you won't lose it                                 when the spark JVM will die generally                                 speaking spark cash will outperform                                 tachyon and this is because while                                 working with with tyonne there is a time                                 penalty inflicted by i don't know the                                 serialization or the serialization or                                 the calls between the two je viens but                                 for long running queries spark cash will                                 encounter GC and if it does your queries                                 will be slower this is something that                                 you won't you won't face if you're using                                 tachyon because you're our DD we will be                                 stored of hip it will be in there in RAM                                 disk so in these situations we have seen                                 tae-hyun outperform spark cash but only                                 I have to to say it once again only when                                 garbage collection kicks in and this                                 usually happens when your data is too                                 large and there is a pressure on your                                 memory the most important thing why why                                 we use tae-hyun is what I said before                                 that the data can be shared between                                 applications with the in-memory speed so                                 you won't have to build a huge                                 application to do them all you can have                                 micro services or micro apps to do                                 several things and share the same data                                 with the memory speed okay next I                                 represent one contribution we have made                                 to the spark echo system it sparked your                                 breasts it's a data warehouse that                                 offers the possibility to concurrently                                 and I synchronously submit spark sequel                                 queries actually it's an it's an                                 alternative to spark sequel jdbc but it                                 has an interactive UI and it has a set                                 of features that i will i will demo                                 later on                                 Jose has quite a history I have been                                 working on it since Park                                               it worked on on top of shark now its                                 support sparks equal and you also can                                 run hive queries on hadoop mapreduce in                                 future we plan to support some other                                 sequel engines that run on on top of                                 data stored on HDFS this is the github                                 link ok for resilience reasons you might                                 want to deploy Jose on several instances                                 and put it under the same load balancer                                 and thanks to the ARCA actor system                                 these instances can communicate through                                 each other which makes possible a nice                                 feature that we have like query                                 cancellation meaning that it doesn't                                 matter on which of these instances the                                 cancel called ends up the I correct I                                 characters will send this message                                 through to all the instances so that the                                 context that triggered the query at the                                 beginning will be the one that cancels                                 it ok next I will I will show the rest                                 of the features directly in the UI to                                 have a look to see how it how it is the                                 code that we used for this use case                                 meaning the download job the                                 transformation job and the job that run                                 that writes the files to park it is                                 available on github and the following                                 address I will put this here too shortly                                 ok do you hear me ok ok                                 okay this is how Joe's looks like this                                 is how it looks like we have here                                 Aquarion history we can see the results                                 this is a quite cool feature that sparks                                 equal jdbc doesn't support is the                                 retrieval of large results what do we do                                 we take the results led with zip it with                                 index and after that store it into                                 tachyon or now or HDFS if you want and                                 while retrieving the results paginate it                                 we filter for we filter with for that                                 index in order to provide samples from                                 the for the results for a certain                                 queries you can see the logs here you                                 can take a look into the hive warehouse                                 you can see all the databases stored in                                 hive for the dead database you can see                                 all the tables and of course or the                                 columns for a table and another nice                                 feature that we have is the pocket                                 tables management what does this mean we                                 map pocket files from tachyon or from a                                 gfs into temporary tables in hive and                                 persist them across sessions I know that                                 this is the feature that the latest                                 version of sparks equal support but we                                 needed it before so we had to implement                                 it this means that if you are using                                 spark within another version and you                                 need it you can use jaws okay what do we                                 have next there is a file browser that                                 browse the files on tachyon this folder                                 contains the output of the job that                                 wrote the trans financial transactions                                 into pocket files okay we can map this                                 folder to a table let's say it's buzz                                 oops                                 ok now the table was registered so we                                 can we should see her see it under the                                 park at tables tab it is here and now we                                 can query it so let's say we want to                                 select I will write it with one hand                                 select I let's say count all the                                 transactions let's say that group by                                 side from buzz group by side awesome how                                 hope it works ok you can see the logs                                 and then the results so this is jose                                 let's see how tachyon looks like for the                                 ones who does who don't know it looks                                 just like a normal file system but the                                 files are in memory here is the packet                                 format that i showed you before if you                                 go inside it you can see that all the                                 files are one hundred percent in memory                                 here you can see the underlayer file                                 system it's HDFS and for each file each                                 market file you see the location the                                 watched ip where it's stored ok also not                                 another nice thing if you go in my sauce                                 we are using muscles as a resource                                 manager if you go in vessels you could                                 see that the sparks equal context the                                 Joe's particle context it's alive can be                                 queried and if we look in the terminated                                 frameworks you will see that there is                                 the context that wrote the data in park                                 it in tachyon and this shows a show us                                 that even if the context sparks equal                                 context is dead and buried we can steal                                 query its output with the in-memory                                 speed and this is only because tachyon                                 ok so here is the github having the                                 projects that we used for the use case                                 and here is the github for Joe's it                                 split between between not into                                 microservices one that triggers hive                                 queries and one that recurs spark sequel                                 query                                 you have a readme file explaining all                                 the api's and also you have a                                 configuration file to configure all the                                 spark settings you want in order to                                 improve your queries okay so thanks if                                 you have any questions for both of us                                 more than welcome
YouTube URL: https://www.youtube.com/watch?v=acjMiWoFHcA


