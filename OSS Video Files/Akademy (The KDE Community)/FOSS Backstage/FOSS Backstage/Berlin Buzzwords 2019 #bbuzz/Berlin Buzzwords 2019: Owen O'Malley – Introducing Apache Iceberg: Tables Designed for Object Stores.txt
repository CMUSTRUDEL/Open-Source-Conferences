Title: Berlin Buzzwords 2019: Owen O'Malley – Introducing Apache Iceberg: Tables Designed for Object Stores
Publication date: 2019-06-18
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	Hive tables are an integral part of the big data ecosystem, but the simple directory-based design that made them ubiquitous is increasingly problematic. Netflix uses tables backed by S3 that, like other object stores, don’t fit this directory-based model: listings are much slower, renames are not atomic, and results are eventually consistent. Even tables in HDFS are problematic at scale, and reliable query behavior requires readers to acquire locks and wait.

I will present an overview of Apache Iceberg, a new open source project that defines a new table layout addresses the challenges of current Apache Hive tables, with properties specifically designed for cloud object stores, such as S3. Iceberg is joined Apache Incubator last year. It specifies the portable table format and standardizes many important features, including:

* All reads use snapshot isolation without locking.
* No directory listings are required for query planning.
* Files can be added, removed, or replaced atomically.
* Full schema evolution supports changes in the table over time.
* Partitioning evolution enables changes to the physical layout without breaking existing queries.
* Data files are stored as Avro, ORC, or Parquet.
* Support for Spark, Hive, and Presto.

Read more:
https://2019.berlinbuzzwords.de/19/session/introducing-apache-iceberg-tables-designed-object-stores

About Owen O'Malley:
https://2019.berlinbuzzwords.de/users/owen-omalley

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              hello as he said I'm Owen O'Malley I'm a                               co-founder of cloud heiress strange is                               that                               me to say but I'm very happy to be here                               I apologize if I'm a little jet-lagged                               at least I got him on Saturday instead                               of lost yesterday so iceberg is a new                               pod length that came into the Apache                               Incubator this year and it came out of                                Netflix and it came out of their                                production use case so what are we going                                to talk about I'm going to talk about a                                use case and wide Netflix came up with                                iceberg to start with then I'm gonna                                talk about some of the restrictions on                                the hive cables that everyone uses today                                what iceberg does instead and then how                                to get started so iceberg really came                                out of the performance out of s                                     there's some other characteristics of s                                 that are also very important but                                fundamentally came out of performance                                and so they have huge amounts of data a                                Netflix of course they are continually                                keeping data on everything that you                                watch and so they need to analyze that                                because Netflix is a data-driven                                business they need to know what people                                are watching and who's watching what so                                they run these queries a lot and this is                                a time series they've got one month of                                data is                                                           couldn't process more than a few days so                                then the query it looks like that where                                you are selecting distinct tags from a                                range of dates so when they ran it on                                hive they got                                                           are individual units of work and they                                explained query just doing the planning                                took nine and a half minutes that's a                                long time but it's not unheard of and                                when they replaced it with iceberg they                                got down to                                                            thing ran in                                                                                                     cumulative task time but the wall time                                was                                                               seconds of planning now with the file                                formats you can do more work upfront and                                figure out that you don't need to run as                                many tasks and so if they turn that on                                an iceberg they could spend more time                                planning and as matter of fact                                   seconds planning but then the wall clock                                time got down to                                                      why they wanted iceberg they wanted to                                get the performance so what is the table                                format the first question that everyone                                thinks is that is it a file format and                                the answer is absolutely no you actually                                even when you're using iceberg are still                                storing your data in your Avro park' or                                orc files as you are now and what the                                table format is is actually a layer of                                abstraction in-between it's how you are                                organizing those files so that you get                                them grouped up into a table so that you                                can do planning quickly and run your job                                quickly so what would a good table                                format be first of all it'd be specified                                because you really want to be able to                                layout your data once and then use it                                for a lot of different execution engines                                you want to use the same data with hive                                with SPARC with presto right with all of                                them together and you want it to be well                                defined what goes where it should                                support atomic changes now part of the                                no sequel world has always been we don't                                need transactions well we found out that                                wasn't actually true we actually do want                                to be able to change the data and be                                able to mutate it with getting                                consistent results with people seeing                                the same results based on and consistent                                results based on when they start their                                query we also need schema evolution if                                you've got your data that's been                                accumulating over years                                almost guaranteed that you're going to                                need to change the columns that you have                                in that data you're going to add columns                                you're going to delete columns sometimes                                you don't even know that types you're                                going to end up with but in any case you                                want to make sure that it supports the                                evolution of those types and moves                                forward and you finally need to enable                                efficient access there's all the storage                                 formats at this point support predicate                                 push down that means that the query                                 execution engines actually tell the the                                 loader when it's reading a file ok I                                 only need rows that satisfy particular                                 predicates so for example if you're                                 searching for a particular time range                                 you actually push those predicates down                                 into the reader so that it can just read                                 the parts of the file that are needed                                 for that particular query additional                                 features it's really really good if it's                                 a solid abstraction and doesn't leak the                                 details out to the upper levels and you                                 want to be able to evolve that layout                                 over time ok now let me talk about where                                 the current state of affairs when hive                                 was first getting written by Facebook                                 they actually came up with some really                                 nice structures that were similar to                                 some ones that actually the guy who                                 started it saw a yahoo and so basically                                 they create directories based on                                 particular values of particular columns                                 so for example this is a table that's                                 sorted first are broken down first by                                 date and then by our within the date in                                 general I wouldn't recommend breaking                                 down by hour because if you do the math                                 you quickly end up with a lot of                                 partitions very quickly I've doesn't do                                 well if you do that but certainly by day                                 makes a lot of sense                                 and then within down at the bottom you                                 have the files at the individual                                 Pass wrote so here you can see that the                                 important characteristics are you've got                                 a directory structure you kind of needed                                 to traverse the directory structure to                                 see what data you have and then the data                                 is actually done with the leaves now                                 when you want to do a filter hi                                 factually does the first level of                                 filtering by only reading the                                 directories that can match the predicate                                 so for example if you only needed the                                                                                                     could just go to that directory enesta                                 directory and pull out the data that you                                 need no that works but now you need this                                 know where are the partitions so I've                                 keeps a meta store and it tracks                                 information about the the partitions                                 that you have it also tracks the schema                                 information and how your types have                                 changed over time and it tracks some                                 table statistics now it allows you to                                 filter by partition values but there's a                                 really big caveat there it'll filter on                                 the client side and it won't filter on                                 the database side unless your predicate                                 your partition column is the string type                                 it uses that external sequel database                                 which is kind of annoying the cloud and                                 the file system is the only place that                                 the individual files are tracked and                                 there's no way to add the profile                                 statistics into the system now a few                                 years ago those of us that Hortonworks                                 were like okay                                 hives structure is okay that that works                                 for us but we want to be able to support                                 acid transactions on our data and so we                                 started implementing a and implemented a                                 hive acid layout as its called                                 and so we left the partition structure                                 the same and in we added bucket files                                 and Delta files so that you could                                 do insert updates and deletes all with                                 snapshot isolation from each other that                                 was really really handy for our users                                 and it's been very well received however                                 now everything is a happy thing in hi                                 flange first the partitions are only in                                 the meta store and the files are on the                                 file system so now you have to look to                                 places in order to plan your query the                                 bucketing is defined by java's hive                                 implementation or sorry harvest Java                                 implementation of the hash and so if you                                 don't lay things out using that hash                                 function now all of a sudden when hive                                 tries to read it it will make bad                                 assumptions and won't find your data                                 right you can get incorrect results and                                 so that leads to people doing things                                 like saying ok spark can read hive data                                 but it can't write to it and that blows                                 away one of Hadoop's ecosystems really                                 big strengths which is you can read your                                 data at the way that you want to read it                                 for this particular query another                                 addition or problem until we added the                                 acid layout was that the only atomic                                 operation you had was adding a new                                 partition that's why you often see                                 people doing hourly partitions is                                 because that was the only operation that                                 you could do atomically you could write                                 all the files and then add into the meta                                 store and that worked any other                                 combination didn't work deleting a                                 partition adding files to a partition                                 all of those would lead to race                                 conditions on reading the data and it                                 required a directory listing per                                 partition so that is pretty painful in                                 HDFS although it's ok where that's                                 really problematic is the cloud object                                 stores that's exactly why the hive query                                 planning was taking so long it's because                                 it's trying to do those directory                                 listings in s                                                         actually it's got two other problems                                 that didn't matter here                                 another the second one is that it's                                 directories are eventually consistent                                 your tasks can write a file in destory                                 get back the response saying it's done                                 if you look you'll see it but some other                                 tasks on a different computer can look                                 and sometimes it'll see it sometimes it                                 won't see it now eventually it'll always                                 see it but it's that eventually part                                 that becomes really problematic because                                 if your table is missing some of its                                 data that's not just unfortunate that's                                 wrong results and you can get in big                                 trouble for your business by missing                                 data the third problem actually is that                                 the directory layout in s                                          hotspots that are really hard to work                                 around there are also some less obvious                                 problems the first is that because all                                 the partition values are being stringify                                 some of the cases like null get                                 translated to this crazy hive default                                 partition thing if you ever seen that                                 that's because your partition value had                                 a null value in it another one is that                                 the file or the statistics becomes stale                                 so it's possible to get incorrect                                 results based on the stale information                                 and the hive meta store and not even                                 know it the other an additional concern                                 is that the hive table layouts are                                 continually evolving and they're not                                 documented anywhere right if you need to                                 find out the details of the hive acid                                 format you end up looking at code or                                 asking someone who knows but mostly you                                 end up looking at code and finally the                                 bucket definition is tied the hive code                                 other annoyances you need to know how                                 that data is laid out right you need to                                 know that if you just say time stamp                                 greater than X you're gonna be doing it                                 table skin if you end up wanting to do                                 work instead of that you probably meant                                 to say the temp temp is bigger than X                                 and                                 the condition on the day once you do                                 that then you can get partition pruning                                 but until you do that you aren't going                                 to get it you're just going to do a                                 whole table skin another annoyance is                                 that the schema evolution in hive is                                 defined by the file format so CSV it                                 doesn't do anything everything is                                 partition is position based parque and                                 work will do it by name and so you get                                 more flexibility actually everyone work                                 for case mostly does it by position                                 still then you also have additional                                 problems like which format support                                 decimal which ones support maps to a                                 struct keys and so on so now as I said                                 iceberg came out of Netflix and so I've                                 done some work on iceberg but mostly I                                 was their champion getting into Apache                                 Incubator now icebergs design was                                 basically that ok instead of using the                                 hive meta store we're going to keep a                                 track of all the files in s                                            keeps track of the list of files over                                 time and every write produces a new                                 snapshot so that gives us the ability to                                 do this where readers use the current                                 snapshot writers are writing a new                                 snapshot and as time goes forward you                                 get a series of these histories of                                 showing which files are being included                                 in the table and which ones aren't so                                 any change is an atomic operation                                 whether you're appending new data or                                 merging or rewriting files in reality it                                 gets a little more complicated than that                                 so in particular iceberg implements it                                 tracks the schema the partition layout                                 and the properties of each of your your                                 files and it tracks the old snapshots                                 for garbage collection because you need                                 to know when you can delete those old                                 ones                                 each metadata file is itself immutable                                 but it allows us to keep track of more                                 metadata about each of those files and                                 so it's a copy-on-write system so you                                 never can rewrite the old data you just                                 make a new copy with the updated                                 information and move it forward and of                                 course you can roll back if things go                                 badly now you don't want to rewrite                                 those snapshots every single time and so                                 iceberg doesn't make you do that it                                 divides snapshots into manifest files                                 and those manifest files contain the                                 directory listings themselves and so                                 they can store data across many                                 partitions and you can reuse them across                                 snapshots so in this example m                                         or manifest files version                                        snapshot one is using M                                               sorry M                                                                                                                                           you can use those two to reuse parts of                                 their table if you're just adding new                                 stuff it's easy to add a new manifest                                 you don't have to rewrite the old                                 manifests you just need to write a new                                 manifest and add in to your snapshot so                                 it cuts down on the write amplification                                 that you're having to do so what goes                                 into them all the list of all the files                                 and iceburg tracking data it also lets                                 you track the partition values and the                                 per column upper and lower bounds so                                 that you can do partition pruning and                                 actually even bucket pruning based on                                 just the contents of the manifest and it                                 also gives you row count sizes null                                 counts a lot of extra detail that the                                 optimizer so want in order to be able to                                 operate efficiently on the data itself                                 so that you can get much better query                                 optimization without actually looking at                                 the files you just look need to look at                                 the manifests okay so how do you update                                 these things the you start by doing a                                 commit and the commit will need to take                                 the current version create a new                                 metadata version and manifest files and                                 atomically swap them so how do you do it                                 do that the easiest way is to use the                                 database to actually point to where the                                 root is or you can actually do atomic                                 rename if you're in HDFS the atomic swap                                 actually guarantees linear history and                                 yeah so iceberg goes on the assumption                                 that everything will work by default and                                 that no one else is writing to the table                                 that's true in the vast majority of                                 cases now if you get unlucky and someone                                 else is operating on the table it                                 actually will detect the conflict and                                 force the operation to retry with the                                 newer metadata so you basically need to                                 keep your the code keeps the assumptions                                 about what state the query was in what                                 it updated and then looks for conflicts                                 assuming everything is good so for                                 example if your input if you were just                                 trying to merge several small files you                                 could be merging two small Avro files                                 and replacing them with a parquet file                                 that one would be fine as long as none                                 of those three files were getting                                 deleted now if you someone did delete it                                 one of those files and you would need to                                 retry again and redo the operation with                                 the updated state so what does that mean                                 that means that now we are in a state                                 where we don't need to do the directory                                 listings right if you're in s                                          can operate without accessing any of the                                 data at all                                 all and a rather sorry this iceberg can                                 operate without doing any s                                            listings at all that really is a huge                                 speed up and means that you're                                 guaranteed to get consistent operations                                 you also get to avoid the prefix up                                 problem where the the path operators try                                 to open the files directly in place and                                 so they get overloaded this s                                          so you never are renaming things                                 everything's are in place that makes                                 life much much better and you get faster                                 planning right you do one man one set of                                 manifest reads and you don't have to do                                 any of the directory listings now one of                                 the advantage of that is that because                                 now your partitions can be smaller you                                 can actually get more pruning right                                 because the hive meta store was your                                 limit on how much scale out you could                                 have on this data now you can actually                                 go through and have smaller partitions                                 and get away with it another hidden                                 feature is that because as part of                                 icebergs design you can treat the                                 bucketing as very similar to partitions                                 so you can actually get bucket pruning                                 and only look at the buckets that are                                 relevant for your particular query so                                 that gives you a whole nother set of                                 operations that you can push down that                                 let you read fewer and fewer files which                                 is exactly how you get the speed up                                 finally you when you're using high                                 formats you have your choice if you do a                                 blind split you're just randomly cutting                                 files at HDFS boundaries or pick                                 random spots in an s                                                    you actually record where the logical                                 divisions are in the file so you can                                 actually make specific splits and say                                 I'm gonna cut it this offset that offset                                 because I know how much data is there                                 and where those points are so you don't                                 need to to do the random probing you can                                 use exactly the correct split and find                                 out where those cut points are another                                 characteristic is that it actually                                 defines what the valid scheme evolution                                 is right so you can add drop rename or                                 reorder columns it actually does that by                                 assigning IDs for the columns inside so                                 you don't need to worry about the IDs                                 but that lets you rename things and keep                                 this scheme evolution alone I recently                                 had a customer that in hive actually                                 they were in the middle switching from                                 hive to spark which is a whole nother                                 set of issues but but that's what they                                 were doing and in hive had actually                                 worked because they basically took some                                 work files and just dumped them in to a                                 table it wasn't great it wasn't good but                                 it worked                                 now the unfortunate thing is they used                                 the wrong names for the columns again                                 hive they got lucky and it did the right                                 thing because of the version of hives                                 that they were using when they switch to                                 spark now because spark had it a                                 different code path it said oh the                                 columns that I'm looking for aren't                                 there I'll give you nulls for everything                                 so they basically were getting moles out                                 of their data and that all came about                                 because the different execution engines                                 hive and spark we're doing different                                 kinds of schema evolution and so it                                 didn't play well together with iceberg                                 they defined one set of rules and so now                                 all the execution engines                                 the same scheme evolution across the                                 different platforms                                 it also standardizes the date and time                                 characteristics the timestamp okay how                                 many of you guys know what a mess Hadoop                                 is with timestamps only a few of you                                 more of you should be aware of just how                                 messed up they are my favorite is that I                                 was talking to the park' team and the                                 park' depending on whether you're                                 running from spark Impala or hive has                                 completely different semantics for                                 timestamp and they can't tell which one                                 wrote it so they can't tell how to undo                                 the semantics it's awesome and then they                                 were trying to get hive to change its                                 semantics so that the semantics of time                                 stamp in hive would change depending on                                 which file format they were storing in                                 and like no no bad idea don't do that                                 so we talked them off that cliff but but                                 iceberg standardizing the timestamp                                 stuff is really good stuff we need that                                 and actually it even pushed orc to do a                                 little bit better getting consistent                                 support for decimals also a really big                                 deal and of course it's got to support                                 them the complicated types because of                                 course you guys are all denormalizing                                 your data as you should for the Big Data                                 stuff                                 it also supports hidden partitioning                                 another feature of icebergs partitioning                                 that's really handy is it's not                                 hierarchical so if you have things split                                 up by product and split up by country in                                 hive you'd have to pick one of those to                                 be the first level of the directory and                                 the other one to be subordinate to it in                                 iceberg they actually can be independent                                 so you can choose whichever order you                                 want and they'll both be available for                                 previck you push down regardless of                                 which order they would be an ordered in                                 hi                                 and finally get the the mixed file                                 support and reliable metrics because the                                 metrics are reliable in iceburg                                 precisely because they're getting                                 updated along with writing the data in                                 diceberg so it's not two steps like it                                 is in hive you write the data and part                                 of that update is exactly updating the                                 statistics okay so what have we done                                 into other projects                                 Ryan blue from Netflix has been working                                 a lot on the data source v                                            trying to get more logical plans and                                 behaviors in work we needed to add                                 additional statistics and we actually                                 are adding timestamp with local timezone                                 because we only had timestamp instant or                                 timestamp the local timestamp and                                 partying Avro improvements again calm                                 resolution by ID and a new                                 materialization API so how do you get                                 started with iceberg about six months                                 ago at the end of the year I talked the                                 Facebook guy or sorry the Netflix guys                                 into contributing it to Apache so that's                                 now an Apache paddling so you can go to                                 Apache Incubator and pick up the code                                 you contribute it just like any other                                 Apache project complete with github                                 issues and pull requests by the way it's                                 been great having Apache actually                                 support github issues and pull requests                                 I don't know about you guys but whenever                                 I go back to hive and have to deal with                                 JIRA and downloading the issues off of                                 there I'm like really so painful but oh                                 well so the supported engines spark is                                 supported pressed is supported someone                                 did read-only support for pig and                                 you can go to the dev list and ask lots                                 of questions so what's some of the                                 future work some of the future work is                                 integration with hive Netflix doesn't                                 use hai very much so they didn't start                                 with that there's a Python library                                 there's arrow support so they actually                                 that's part of the Python support and                                 actually one of the big pieces that has                                 been lighting up the the dev list                                 recently is dealing with Delta files now                                 when I first read the the the spec for                                 iceberg I was like oh it's pretty clear                                 you want Delta files here right because                                 we'd already done hive acid and we're                                 like of course you want Delta files to                                 be in here and so now we're going                                 through and formalizing that actually                                 there are some big companies that very                                 much want that and so it'll take the                                 form of probably two pieces one with the                                 most important one is to have natural                                 keys so if you've got a table that                                 sorted on your primary key you want to                                 take updates based on that primary key                                 and then guarantee uniqueness in that                                 primary key so you basically will record                                 new versions saying hey that key out of                                 that file is no longer there and it gets                                 replaced with this new version from the                                 the updated file there also be support                                 for artificial keys so that if you get                                 updates you can delete them if your                                 table doesn't have natural keys okay so                                 that's my presentation any questions yes                                 queuing let me head over the mic if it's                                 just wait a second                                 are we recorded - um quick question                                 performance-wise for writing and                                 mentioned lot about reading mm-hmm                                 it sounds like it's gonna do a bit more                                 computation than for example maybe spark                                 on high for the less intelligent                                 bucketing and things is it comparable                                 was the sort of situation there it's                                 actually pretty comparable it's it's not                                 that much more on the right load it's a                                 little bit more because you're updating                                 that's three blob with the results but                                 assuming you don't have contention for                                 writing the same files it's pretty                                 comparable and on that note does it suit                                 larger writes or lots of small writes or                                 is it disease a favor one or the other                                 um wait I'm sorry if you if it was                                 trickling in data row by row is it gonna                                 suit okay so this is absolutely not                                 going to be super great for the case                                 where you're trickling data and a few                                 rows at a time right this isn't going to                                 be the way you implement your online                                 database that's not the intent here the                                 intent here is yes you can delete                                 records you know for gdpr it's great if                                 you want to insert new rack new                                 partitions that's great but you're much                                 better off inserting things a million                                 rows at a time than a few rows at a time                                 so you can do a few rows but not                                 millions of times in a second Thanks                                 sure thanks for a great talk                                 do you have an opinion on the recently                                 open source Delta Lake and how would you                                 compare it to iceberg yeah the first                                 thing about Delta is that it's Apache                                 licensed but it's a day to bricks thing                                 and so what that means is that just like                                 Isabel was talking about                                 it's not open governance if you want to                                 contribute to Delta it is you're going                                 to have to contribute your code the data                                 breaks right it's not like you're giving                                 it to a open source Apache project                                 you're giving it to data bricks if they                                 decide to relicense it that's up to them                                 so you're basically giving work to data                                 bricks that doesn't make give me warm                                 fuzzies but it depends what you're                                 looking for other interesting bits out                                 of data bricks Delta is it looks like                                 according to their own documentation                                 that you only get atomicity if you're                                 using HDFS you don't get it out of cloud                                 stores so I've only looked a bit at it                                 I've I'll give a disclaimer but those                                 are the two things that bother me about                                 and I can see that I suspect that Delta                                 got released that way precisely because                                 they don't want the cloud providers                                 shipping it Thanks presentation actually                                 that was my question but oh so our own                                 hoody that's iPod she so I actually                                 haven't looked as much at hoodie hoodie                                 you kind of sorry hoodie is an Apache                                 project that came out of over uber                                 definitely seems to be developing their                                 own ecosystem of a bunch of different                                 pieces and I just haven't looked at it                                 as much I know Ryan has Ryan Ryan blue                                 is the one the guy who started iceberg                                 so he he's talked to them a fair amount                                 but I haven't I'm sorry okay I have a                                 complementary question sorry it was not                                 clear for me how can i integrate this                                 into a different frame were like let's                                 think I want to write a connector for                                 Frank is this Dave babies are stable                                 because this is like a wall protocol                                 it's absolutely possible I mean the the                                 new ones are getting added                                 for example the big one was contributed                                 as well as the presto one the API is a                                 reasonably stable it's still a young                                 puzzling so it's they're not super                                 stable but absolutely adapters for flink                                 would be awesome wait I have a question                                 again a follow-on an OD meat on the                                 bones there so hoody was kind of came                                 from a different background he'll to                                 support machine learning pipelines as                                 well so this notion of being able to do                                 what they call time travel to make a                                 query on on what was the value of this                                 roam with this primary key at this point                                 in time and also data validation so as                                 you're ingesting data into the system to                                 be able to have a scheme or some rules                                 in which to validate it is that on the                                 roadmap or sir are you time travel is is                                 on the roadmap because the snapshots                                 give that to you very very easily right                                 it's easy to travel through time I guess                                 what are you looking for in terms of the                                 validation so your data's a schema but                                 you don't know so in machine learning                                 all data is numeric pretty much so your                                 schema is F P                                                  categorical variables will be F P                                        you want to know range is valid ranges                                 so typically that's a problem in                                 pipelines because you have to know if                                 the value is outside the range it's an                                 anomaly and you might want to write it                                 you know you could write them up filter                                 you know but you know having support for                                 doing that so all so the min and Max of                                 each column is automatically computed so                                 and stored in the manifest so it'd be                                 easy to get ranges out this is in t FX                                 so                                                                    also an amazon project called doopy i                                 think it was dalton berlin                                 okay okay oh just a note for the time                                 troll I guess that's just the absence of                                 garbage collection if you don't alright                                 sure well let's take one last question                                 here all right thanks for the talk                                 my question is about migration so let's                                 say you have a system now where you                                 Sparky writing hive layout so how would                                 you migrate to using both producing and                                 consuming using iceberg that's a good                                 question I don't have a good answer I                                 suspect that the way that I mean like I                                 said the hive integration isn't very                                 plugged together yet it's it's in                                 progress but but it's not plugged                                 together what I suspect will end up                                 happening is that you'll end up saying                                 this is an iceberg table that's a hive                                 table and then migrating I don't think                                 you're going to be able to have one                                 table that's both iceberg and hive at                                 the same time right okay thanks                                 all right okay so thank you speak again                                 [Applause]                                 [Music]                                 [Applause]
YouTube URL: https://www.youtube.com/watch?v=z7p_m17BXs8


