Title: Berlin Buzzwords 2019: David Moravekâ€“Apache Beam pipelines at 100TB+ scale using Apache Spark #bbuzz
Publication date: 2019-06-20
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	At Seznam.cz, we are building a successful search engine, that is used and loved by millions. Selecting the best possible content from the infinite internet, that satisfies our users needs, requires processing of massive data volumes every single day.

This talk will focus on our long-term journey of scaling Apache Beam to handle 100TB+ scale data pipeline with exponential data skew, using Apache Spark runner.

Read more:
https://2019.berlinbuzzwords.de/19/session/apache-beam-pipelines-100tb-scale-using-apache-spark

About David Moravek:
https://2019.berlinbuzzwords.de/users/david-moravek

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              hi I'm David I'm lead engineer at says                               Nam                               does anyone know                               the company except the guys from system                               No okay so says nom is a search engine                               which is local to Czech Republic like                               for those who are not familiar with the                               search engine market like it's pretty                               much dominated by Google all over the                                world and there are very few countries                                where there is actually some competition                                and the Czech Republic is one of them                                and we're very proud to that profit of                                that                                thanks we still maintain like                                           market share which is pretty cool okay                                so as I said I work on a search engine                                as well like especially on the crawler                                part and the most important thing that                                crawler needs is to process like                                incredible amount of data like we yeah                                we just need to like score them select                                all of the like most relevant documents                                to the user and we just send it to index                                and that's where our job ends ok so                                here's some timeline we started using                                MapReduce in                                                           on pretty decent scale we have like                                   billions rows in database which is HBase                                it has like about four hundred terabytes                                without replicas so it's like pretty big                                we download                                                           single day the Iran on bare metal                                servers over to data centers we have                                like over                                                            over                                                                 computation on so it's pretty decent                                scale and it comes with a price so we                                were able to use MapReduce for years and                                we found out that it's really slow and                                expensive it's in bold                                enough terminals of like development                                price because it's really inflexible if                                you want to express what you really want                                to do and if you have like more complex                                business logic you need some like job                                chain of MapReduce jobs and like between                                each jobs you need to write everything                                on HDFS you need to replicate it three                                times you need to send it over the                                network and it's really really slow so                                we were seeking for a replacement so in                                                                                                        euphoria API which was dislike engine                                independent programming model that could                                do both batch and streaming pipelines                                because like we had to maintain                                basically two code bases one for the                                like slow slow path which had really                                really high throughput and one for low                                latency one so we kind of wanted to                                write all the business logic just once                                and run it on both batch and stream so                                it was basically a Java SDK that then                                translated to either spark or fling does                                this sound familiar to anyone it should                                so in                                     Apache Bean was open sourced and you are                                like okay maybe they do very similar                                thing as we're trying to do so it's                                basically the same as I described except                                there they went a little bit further                                like they want you to be able to write                                your pipelines in any SDK in any                                language that you can imagine and then                                it just translates to the B model and                                you can run it anywhere you can run it                                on spark you can run it on flame on data                                flow and there are so many emerging                                runners right now that you can you'll be                                able to use in near future                                so if you're interested in like running                                pipelines in any language there would be                                really great talk tomorrow by Max and                                Ismail                                it's called peyten Java or go it's your                                choice but Apache being so you guys                                should definitely go there I'll                                definitely go there okay so then we were                                kind of like struggling what to do next                                and we finally realize it doesn't make                                any sense to maintain our project                                anymore because like we needed to write                                our own runners and it had like really                                large development cost for the company                                so in                                                                  into being there is a JIRA issue if                                anyone is interested and we have like                                really great documentation on the bean                                 website and like if anyone wants to                                 contribute to being it's like really                                 great community and they are very                                 welcoming so it's really easy to start                                 with it okay so our largest pipeline                                 that we have is basically doing that                                 it's turning the internet upside down                                 because like anytime you download a new                                 document document in terms of search                                 engine is it can be either like HTML                                 page it can be an image it can be a                                 video video PDF like many different                                 things and you only see like the links                                 that are like going forward from the                                 document so you just like you know only                                 forward Network and what you read the                                 question you really want to answer is                                 what are the documents that I that are                                 pointing and to me and what I know about                                 them like for example if they are all in                                 English there is a really high                                 probability that that the document it's                                 pointing to is also in English or if all                                 the documents pointing at it are link                                 farms it's really high probability                                 that it would be a span so like we                                 basically use this to like calculate                                 some really really good signals that can                                 be then used by machine learning models                                 for like search relevance so it's really                                 important and these jobs this pipeline                                 it runs incremental every single day and                                 the inputs it needs to process it ranges                                 from                                                              terabytes each day so it we actually                                 tried to run it on spark and we                                 struggled a lot for like two years so                                 this is what we're going to go through                                 so you guys don't make the same mistake                                 as we did okay so the biggest issue we                                 encountered was the exponential data                                 skew is anyone familiar with it there                                 are a few people okay so an example                                 would be joining documents that's what                                 we already talked about and it would be                                 like joining it with some metadata about                                 domains that it's located on so if you                                 look at the distribution how it looks                                 like probably most of the websites would                                 have like around                                                     those are like mostly like blocks and                                 those kind of small websites and then                                 there is for example YouTube which has                                 like hundreds of millions of documents                                 and if you want to join it to a single                                 key you'll struggle a lot so what are                                 the solutions yeah this is what will                                 happen on spark you can see that like                                 most of the splits would finish in like                                 five minutes and then there would be one                                 split which would finish in                                            so it would like really make your job                                 competition very longer than it needs to                                 be so what we want is to evenly                                 distribute                                 data data amongst plates so one solution                                 would be not to shuffle it at all so if                                 in does is anyone familiar with maps I                                 join like three four people                                 okay so Maps I joined basically if you                                 want to join two sides left and right                                 and if one side fits in memory you can                                 just take it you can broadcast it to all                                 the executors and you can just like map                                 through the left side and just do in                                 memory lookup to the hash table that you                                 have in memory so you just collect it                                 broadcast it and then when you're going                                 through like single elements you just                                 look it up and you have your result and                                 you didn't have to send like hundreds of                                 terabytes over the network just to do                                 join with like                                                          this is usually not the case so another                                 solution would be you could split the                                 large keys for example if you have                                 youtube you can just like copy the key                                 which is on left side you can partition                                 it and then you can evenly distribute                                 the data among those partitions this is                                 heart if you have an exponential data SQ                                 because because you have to like                                 calculate first like vitamin domains you                                 want to split and how much you want to                                 split them but it's definitely worth it                                 ok so another huge issue with SPARC all                                 values for a single key must fit in                                 memory who is familiar with the                                 difference of guru by key and reduce by                                 key calls in spark ok ok so I hope that                                 everyone is familiar with MapReduce                                 right so if you want to do a word count                                 for example you just like need to send                                 all the same words with a value                                        the network and if you use                                 in memory combiner you can combine it                                 mops eye on mop site and then you can                                 just like send the combined residual                                 results of the reducer and you saved a                                 lot of time a lot of network traffic and                                 a lot of iOS this is basically the                                 difference between reduce by kyun grew                                 by key Reggie's bike he just takes                                 combiner and it does in memory combined                                 but the issue is that grew by key is                                 implemented using reduce by key which                                 means you get like list combiner and any                                 value that goes to group by key you just                                 add it to the list                                 so it means like at the end of the day                                 you'll just need to load the whole list                                 in memory at once which is obviously                                 problems sometimes okay so there is a                                 little quiz about group by key does                                 anyone know what's wrong with this code                                 because like we really struggled with                                 this okay so so what happens if you take                                 a byte array and new shop and you use it                                 as a shuffle key like in Java it just                                 like defaults to object hash code so it                                 will be completely random so you don't                                 have any guarantees where the key will                                 end up so this is something to be aware                                 of also if you're using composite keys                                 that contains a enum is the same thing                                 should be really careful about that okay                                 so can we do more efficient of course we                                 can we can just like repartition and                                 sort everything it's kind of like going                                 back to the MapReduce but it works so                                 you just sort everything by key and you                                 just go key by key and only think you                                 need to load in memory is a single key                                 at a time which is perfect so now                                 everything should be okay right                                 and we don't have any more problems                                 yeah then we get exception like this you                                 can see that like right now the                                 distribution of data is pretty even but                                 there is still like one one task that                                 completed in                                                          order of magnitude worse than the rest                                 and we can also see there is something                                 called Shaffer read blocked time and we                                 we can see that shuffle was blocked for                                                                                                    biggest scaling issues with spark there                                 is great paper from Facebook                                 it's called riffle optimized shuffle                                 service for large-scale data analytics                                 and it basically says that within with                                 an increasing number of tasks shuffle                                 time and i/o requests you need to do                                 they grow quadratically and the size of                                 Io requests they decrease quadratically                                 it basically means that like every task                                 that completes just ends up with one one                                 shuffle file on disk so if you have like                                 I don't know                                                             end up with a hundred thousands shuffle                                 files and you you are doing a lot of                                 random i/o seeks and most of the expired                                 clusters are just like on the yarn and                                 on hdds so it's not really random access                                 friendly and it will just kill the                                 performance of the whole cluster                                 completely and it also has really bad                                 impact on other jobs that are running                                 there so what you can do you can kind of                                 decrease the number of map tasks but how                                 you do it you just like have larger                                 spreads for a single task which means it                                 will run but it will run way slower                                 because you'll need to do like lot of                                 disk spells so it's a trade-off but it's                                 runs I think like Facebook has                                 some ongoing work on this and hopefully                                 they will contribute it back to spark it                                 would be really awesome because                                 otherwise yeah it doesn't really scale                                 then like one really huge performance                                 gain we got from being was something                                 called bite based shuffle because being                                 kind of like have an abstract concept of                                 serialization so only data that spark                                 sees are actually bytes so like during                                 shuffle spark just doesn't need to                                 deserialize it or all the time and                                 doesn't need to use any like custom                                 comparators so this was for like really                                 really huge performance game and what                                 you can do if you want to debug spark                                 pipelines there is this really great                                 tool it's called Babar it's from                                 righty-o I don't know much about the                                 company but it gives you like all of the                                 information about like containers that                                 were allocated about memory your job was                                 using about about CPU about garbage                                 collection and the best thing it gives                                 you it gives you a flame graph of your                                 job this is really great tool if you                                 want to profile it and if you were like                                 trying to find out why the job                                 competition takes so long for example we                                 were able to find that there was like                                 really nasty issue with combiners in                                 being that like every single time you                                 added something to combined accumulator                                 you just need to serialize and                                 deserialize it and it was really easy                                 fix but it's really too hard to know                                 that you have to fix it without like                                 properly profiling the pipeline because                                 this is something that tests just cannot                                 catch and of course Hipp dumps are very                                 useful so like right now we were able to                                 run our like biggest pipeline we had on                                 park runners so I would say it's                                 production-ready right now for batch for                                 streaming there are some ongoing works                                 and if you're more interested in Apache                                 beam there will be another great talk                                 the next session it will be by Tomas is                                 about streaming your share right so it's                                 like and then there is a beam summit                                 Europe happening this Wednesday and                                 Thursday                                 it's actually at the same place as                                 barreling buzzwords so and it's free so                                 you're free to like register and you                                 will be very welcome to show up ok so                                 that's all from me and if you if you'll                                 have any questions I'll just be around                                 and you can ask me anytime thank you                                 [Applause]
YouTube URL: https://www.youtube.com/watch?v=rJIpva0tD0g


