Title: Berlin Buzzwords 2019: Michael Sokolov & Mike McCandlessâ€“E-Commerce search at scale on Apache Lucene
Publication date: 2019-06-20
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	After many years running its own in-house C++ search engine, Amazon is exploring moving its customer facing e-commerce product search to Apache Lucene, serving millions of customers each day worldwide. Apache Solr, Elasticsearch and other Lucene derivatives have been used widely for many years at Amazon, but until now the .com product search has been powered by a proprietary in-house engine. 

We'll discuss why we decided to adopt open source for this vital technology and dive deep into the technical challenges we faced in replicating our legacy engine's behavior, pointing out novel uses of Lucene along the way. 

Read more:
https://2019.berlinbuzzwords.de/19/session/e-commerce-search-scale-apache-lucene-tm

About Michael Sokolov:
https://2019.berlinbuzzwords.de/users/michael-sokolov

About Mike McCandless:
https://2019.berlinbuzzwords.de/users/mike-mccandless

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              so                               I am Mike McCandless I'm a longtime                               committer on the attack                               open-source project I                               should've seen in action Mike Sokolov                               search veteran worked many years with                               Lucene and just recently became a                               committer and we are the two lucky souls                               up here today but there is a large team                                at Amazon a large distributed team                                including people in Seattle                                Tokyo Dublin San Francisco Palo Alto who                                have contributed to this effort some of                                them are in the audience here so here's                                a quick outline of our talk first I'll                                give an overview of why we chose we've                                seen and what we're trying to solve at                                Amazon with Lucene then I'll describe                                our production architecture I'll                                describe how we measure performance the                                other mic will come up and describe some                                text analysis challenges we have                                indexing our catalogue he'll describe                                how we optimize for query latency                                because we're very concerned with the                                long pole query latencies p                                           we do ranking and then I will wrap up so                                I think most people are probably                                familiar with Amazon you're probably                                customers of Amazon you probably                                appreciate how difficult a search                                application this particular search                                application is Amazon has a very large                                catalogue it is a high velocity catalog                                it's constantly products are being added                                prices are changing with high frequency                                we have to keep up with that indexing                                rate adding new documents and deleting                                documents or replacing them but at the                                same time we have to handle a very high                                query rate all of the amazon shoppers                                and this is the search box you get at                                amazon.com when you go to the top and                                and type in a few search terms that's                                hitting our service when you search on                                the Amazon mobile app that's hitting our                                service so that query rate is very high                                we have very strong latency requirements                                we don't want customers to wait even at                                the the long pole query Layton sees we                                want to keep those very low and then to                                make matters even more challenging the                                query rate is extremely peaky there's                                there's a daily pattern there's a weekly                                pattern a monthly pattern and then                                there's prime day which is coming up                                very soon and that's a completely                                different thing that's very challenging                                to handle so and people were unsure when                                we started this project whether Lucene                                is up to the task so why did we pick                                we've seen well the scene is an amazing                                search engine it is                                                  in                                                                  visionary source code for this new                                search engine to source forage at the                                time and it just developed a massive                                community very passionate people                                constantly iterating it's hard to find a                                software project that is still                                successful after                                                      innovation that Lucene has every major                                releases adding incredible new features                                block max wand of codec impacts showed                                up in the latest major release and it's                                a high performance search engine this                                isn't just a toy research engine that                                people like to just play with but never                                really use in practice it's used in                                practice all over the place if you look                                for a ride on uber if you do a search on                                Twitter or LinkedIn if you are searching                                for a date on tinder you're using                                leucine leucine is everywhere and now                                some searches on amazon.com are using                                Bluestein and it's because of this                                incredibly passionate community and the                                                                                                  leucine is such an incredible search                                engine leucine is a pure Java                                architecture and at the time that was                                really shocking in                                                   newfangled language but today that's a                                really great property you don't have                                memory leaks in the traditional sense of                                memory leaks anymore leucine is an on                                disk index so it can scale to indices                                much larger than the free RAM on a                                computer and still have good latency it                                 has amazing concurrency so as computers                                 in                                                                      concurrency these days they have amazing                                 concurrency you get                                                      box the solid state disks that are in                                 machines they are really fast they are                                 also incredibly concurrent under the                                 hood they have many different channels                                 that they can handle saturated i/o so                                 leucine taps into all of that of that                                 capability it handles that concurrency                                 very well and it has a segmented                                 architecture and if you're curious how                                 leucine deals with merging segments                                 because new segments are written and                                 they're small and over time you get too                                 many little segments it merges them                                 together in the background into bigger                                 segments there's a fun YouTube video                                 that shows you how that process works                                 and it has a very nice near real-time                                 architecture used so you can search all                                 the documents you indexed up until a                                 point and you can continue indexing                                 while you're searching and then every                                 few seconds you can just open a new                                 reader and it will search the point in                                 time view of that index as of when you                                 refreshed it so that and that that                                 refresh operation is not that costly                                 it's in proportion to how many documents                                 were indexed since you last refreshed                                 and that right once designed those                                 segments are written once and they never                                 change and that gives very good index                                 compression leucine is able to very                                 compactly write the values because it                                 knows the values that just wrote won't                                 be updated instead another segment will                                 be written in the future so we in                                 building this this search application on                                 top of leucine we are using a huge                                 number of leucine features and so this                                 is one of the important characteristics                                 that leucine had for our search problem                                 I mentioned the near-real-time                                 capability we're actually using a near                                 real-time segment replication to                                 distribute the index across many                                 replicas and this is different from how                                 elastic search and solar work today we                                 are using a feature called concurrent                                 searching so modern computers that have                                 massive concurrency we have hard queries                                 to answer they take a lot of computation                                 so we now use multiple threads to answer                                 that that query and that's a feature                                 that's available in busine but it hasn't                                 been exposed in elasticsearch or solar                                 yet                                 we use index sorting so we have this                                 notion of whether a product is a good                                 product lots of people are looking at it                                 and purchasing it or a not good product                                 it's not really getting much engagement                                 we sort the index by that criteria and                                 it allows us to do early termination on                                 difficult queries we're using index time                                 joins to handle offer informations                                 because every every product has multiple                                 offers and that's an index time join for                                 us it's very efficient dimensional                                 points was added in leucine                                          think dimensional points is the                                 geospatial filter and it does a very                                 good job at that it's also a very                                 generic multi-dimensional index and                                 we're using that to handle the kinds of                                 specials that we get on prime day                                 Lucien's extensibility API is a                                 collectors doc value source queries                                 leucine has a it's a very modular                                 architecture and it's very easy to build                                 your own custom implementations of these                                 classes those are great touch points to                                 customize the behavior custom term                                 frequencies for representing how how                                 custom how engaged in Aysen is so that                                 was a feature that was not available in                                 leucine in the beginning and we                                 that we contributed that feature back                                 upstream and people have since built on                                 top of it and done all sorts of                                 interesting things                                 faceting multi-phase ranking expression                                 so the list goes on and on and                                 especially because those two top two                                 features were not available in                                 elasticsearch and solar but also because                                 we're heavily customizing many different                                 parts of Lucene we are building directly                                 on top of leucine we're not using                                 elasticsearch or solar today and                                 thinking back to Isabelle's keynote this                                 morning she mentioned she had a point in                                 her talk where she said contributions                                 back upstream are dependent on your                                 situation and where you are at your                                 company and that's what happened at                                 Amazon too early on as we were building                                 this search engine on top of leucine we                                 hit a bunch of rush corners a few small                                 bugs you know some performance issues                                 and early on we pushed a lot of good                                 stuff back upstream and that benefits                                 the whole community benefits everyone is                                 using the scene elasticsearch and solar                                 so that's a healthy process that works                                 well inside our team and less so lately                                 because leucine is working really well                                 for us right now okay I'm going to                                 switch gears and talk about our                                 production architectures sort of at a                                 high level first I wanted to dive into                                 that first feature that I listed near                                 real-time segment replication so when                                 you have a search index that's too big                                 for one computer to handle even with all                                 of its concurrency and solid state disks                                 you shard it you just take the index and                                 you break it up evenly into n pieces                                 then if each if one of those n pieces is                                 still too hard to handle the query load                                 how many queries per seconds you have to                                 handle then you replicate that index so                                 you have a matrix structure for search                                 clusters and that's how elastic search                                 and solar and our production                                 architecture work as well however when                                 you add documents to that index you have                                 to have a way to get the new documents                                 to all of the replicas so you had when                                 the document comes in you have to figure                                 out what shard it goes to and then in                                 the case of elastic search and solar                                 cloud the document is reindex across all                                 the replicas that it wouldn't really                                 scale very well for us because we have                                 very deep replicas the search traffic is                                 really peaky especially on prime days                                 and Black Friday's so we use a feature                                 called near real-time segment                                 replication which takes advantage of                                 leucine write-once architecture so loose                                 one indexer node will write these new                                 segments and                                 then we copy those segments out to all                                 of the replicas and we take advantage of                                 AWS is as three it's fast networks it's                                 a very fast operation for us to                                 distribute those new checkpoints out to                                 all the replicas and then we rely on                                 Kinesis to make sure we don't lose any                                 documents so if an indexer goes down we                                 can fall back and the Kinesis queue and                                 replay the updates so in addition to                                 Kinesis we're using a ton of AWS                                 features it's a great time to build a                                 search application because you can just                                 stand on the shoulders of giants all of                                 these amazing services that are                                 available in AWS you don't have to                                 reinvent anymore so we use elastic                                 container service to run each component                                 of our architecture DynamoDB is used to                                 hold the entire catalog that we're                                 indexing so we can sort of pull from                                 that to reindex the whole dock the whole                                 index Kinesis queues are bringing in the                                 real-time changes we use girfs EAP is to                                 tell the nodes when they should refresh                                 and and save snapshots to s                                          unique property of our infrastructure is                                 we rebuild the entire search index every                                 time we push any version any software                                 change it could be a trivial change it                                 could just be a Java doc change or                                 readme file or it could be a drastic                                 change introducing a different approach                                 for synonyms so because it's kind of                                 risky to determine which software                                 changes might have impacted the index in                                 which which haven't we just rebuild on                                 every on every push and it's Lucene is                                 incredibly fast a tree indexing so that                                 architecture works fine for us and                                 because we're doing near real-time                                 second replication we can afford to just                                 do that on one node and copy the                                 segment's out to the replicas so this is                                 a picture that shows roughly how it                                 works I'll go through it really quickly                                 a query comes in at the top that's one                                 request into a component we call the                                 blender the blender fans out that                                 request to multiple departments in                                 Amazon's catalog each department has a                                 collator in front and the collator                                 I think is kind of like the coordinating                                 node and elasticsearch it receives a                                 query it figures out which charge in                                 which replicas it's going to talk to it                                 waits for them to reply it might retry                                 if something goes wrong and then it                                 merges the results back so that's a                                 collator coming down here                                 picking a replica in every column each                                 shard is one column here and then                                 sending the response back to the                                 customer these are the indexing nodes so                                 on top of each column we have an indexer                                 who's taking consuming the Kinesis cues                                 and DynamoDB pushing checkpoints or                                 snapshots into s                                                        are copying from s                                                     new segments for searching so when Lucy                                 needs to search its index                                 it's a segmented architecture the query                                 comes in the query checks at each                                 segment gets the best hits from every                                 segment merges those results together                                 and sends it back it's just like a shard                                 at architecture but it's happening with                                 happening within one Lucene index but if                                 you do that sequentially which is the                                 default for for leucine and and is how                                 elasticsearch and solar work you pay a                                 latency cost for all of those segments                                 and if your query is costly and your                                 shard is big enough that latency starts                                 to push into uncomfortable numbers of                                 milliseconds so we use the the leucine                                 feature that allows us you just pass an                                 executives executor service to index                                 searcher and when that when you pass                                 that index searcher will take the query                                 and dispatch the query across multiple                                 segments concurrently the small segments                                 are coalesced into one work unit so one                                 thread will do the small segments and                                 then when those threads have all                                 finished it does does it Joe in on all                                 the results and merges them and returns                                 them back this is really really good                                 useful for us this allows us to make our                                 shards larger and keep our query                                 latencies lower until you we approach                                 capacity so as we get close to red line                                 QPS this feature hurts us because of the                                 thread switching the overhead of                                 switching between threads adds some cost                                 and the red line QPS the red line                                 capacity is a little worse as a result                                 but it's a good trade-off from our                                 standpoint we don't try to run our                                 clusters at red line and we have other                                 tools we can use to deal with a red line                                 situation so it's far more important                                 that we bring down our query Layton sees                                 below red line we noticed one strange                                 thing when we first launched our service                                 we have all sorts of wonderful metrics                                 this is a chart showing you two metrics                                 the                                 blue line which was the first metric we                                 looked at we didn't know about the Green                                 Line yet is our p                                                    latency and we had to erase the axes I'm                                 sorry we weren't allowed to show exact                                 numbers here but the sawtooth pattern is                                 what was very strange about this we all                                 sort of scratched our heads and looked                                 at our service and why would it have                                 such a sharp behavior where suddenly the                                 latency jumps and then kind of goes down                                 again so then we realized we had a                                 theory that it was it was related to                                 leucine segmented architecture and could                                 are concurrent and how we take advantage                                 of concurrent search and what is                                 happening is the Green Line is showing                                 how many bytes were just copied out to                                 the replicas so the spikes in the Green                                 Line are large segment merges so                                 normally when leucine merges small                                 segments into a big one that's a good                                 thing because because you've taken ten                                 small segments and made a big one you                                 don't have to you don't open so many                                 files you don't have to visit all these                                 little segments it's supposed to be a                                 good thing and overall it is a good                                 thing it brings you it brings up your                                 capacity but in our case because we do                                 concurrent search across segments those                                 large segments were caused causing us                                 more latency because now we lost some                                 concurrency previously when we have the                                 ted segments we would allow ten threads                                 to search but now we have only one                                 thread searching so I think there are                                 some improvements we can do here and                                 we're looking at whether we can use                                 multiple threads even inside one large                                 segment and then that would sort of                                 smooth away those those sawtooth                                 patterns okay                                 changing gears to how we measure                                 performance so we all have wonderful                                 unit tests integration tests all sorts                                 of Approval steps that we have in our                                 pipelines there's all these things we                                 have to catch functional errors but                                 catching performance regressions is a                                 lot harder so to help us with that and                                 it's not a perfect solution but it works                                 really well we have a set of internal                                 benchmarks that are similar to the                                 public Lucene benchmarks that run every                                 night and these benchmarks they wake up                                 they take up a recent snapshot of the                                 catalog they take a recent snapshot of                                 actual customer queries they index the                                 catalog they send the queries to that                                 index and they measure all sorts of                                 metrics our long toll query latency our                                 throughput memory usage number of                                 garbage collection                                 all sorts of metrics and chart those in                                 nightly graphs which we go and look at                                 every so often to catch accidental                                 regressions they have caught a lot of                                 accidental regressions it's easy to make                                 a wonderful-looking change that passes                                 all tests and everything's fine and you                                 notice it had a                                                     we also measure functionality so it's                                 not just performance but whether a                                 change that should have just been an                                 optimization altered the search results                                 so that's a very bad situation if we if                                 we change the search order and we didn't                                 expect to our benchmarks will help us                                 catch that so these same benchmarks that                                 run in a nightly machine they also are                                 available for developers to run so that                                 if they made a performance change they                                 want to see what the impact was they can                                 do that in the privacy of their                                 workspace they can iterate and fix it                                 and makes it a lot easier and lot safer                                 to make exciting changes so I mentioned                                 that long pole query latency is                                 something we really pay attention to                                 when we measure our long pole latency we                                 use a load testing client it's an                                 in-house load testing client that uses                                 it's an open loop client so an open loop                                 client is one that sends the request and                                 doesn't wait further response it goes                                 back to its thread pool and it sends                                 another request when it's time to send                                 it there are quite a few frustratingly                                 high number of load testing clients that                                 don't do this they use a closed loop                                 tests to measure query latencies and                                 that will lie you will get rosy looking                                 results when in fact you have horrible                                 results if you if you look up gilt na                                 did a great talk                                 describing why your load testing for the                                 client is probably lying to you so if                                 your load testing client is not using an                                 open loop go back and look at it watch                                 that talk and and find a better load                                 testing client redline QPS we measure                                 with a closed loop client so a closed to                                 the client will just send the request                                 and wait for the response and then when                                 it gets it send another request and if                                 you if you run with enough clients that                                 the the reports the metrics measured by                                 that close the plant will be close to                                 your red line capacity when we measure                                 latency there's a we sort of struggle                                 with what at what load should we measure                                 latency because if you measure latency                                 at red line that's not so interesting                                 there's just a lot of waiting happening                                 if you measure at                                                        Poisson process which is a mathematical                                 model of how queries arrive in practice                                 then you're going to also measure a lot                                 of contention so we've sort of changed                                 where we measure latency we measure at a                                 fairly low rate because we really want                                 to measure whether the software got any                                 slower at replying at computing the                                 answer to a query when we looked at our                                 benchmarks this was a very interesting                                 thing we noticed                                 Lusine had absolutely horrible refresh                                 times and which was very surprising                                 because I know from Lu Singh's nightly                                 benchmarks it doesn't have horrible                                 refresh times so we realized that the                                 way we were using the scene and how Lu                                 seems concurrency model works weren't                                 wasn't a good match so with Lu Singh's                                 indexing if you have lots of threads                                 doing indexing it's wonderfully                                 concurrent you're saturating CPU or i/o                                 you're getting amazing throughput but                                 then if you stop if you pause your                                 indexing threads and wait for them to                                 all finish which is a common case you                                 know you indexed all of your catalogue                                 and once it's all indexed then you ask                                 who seemed to commit that is a single                                 threaded operation in Lucene today which                                 is really weird because the indexing was                                 nice and concurrent and now you do a                                 refresh and it's really slow so we in                                 our benchmarks we noticed that was                                 incredibly slow we opened an issue                                 Simon replied on the issue and said this                                 is how you can fix it with an existing                                 Lucienne api we made that fix and it's                                 much faster going from a single thread                                 to                                                                     improvement in your refresh times the                                 benchmarks that we use are able to tap                                 into some amazing metrics that would                                 normally be quite hard to get except for                                 the fact that leucine has really good                                 abstractions directory reader is the                                 abstraction for reading an index from a                                 disk directory we use that to count how                                 many term dictionary lookups we're doing                                 because that's a fairly costly operation                                 in Liu's team we have a bunch of custom                                 code that does terms dictionary lookups                                 and that that's something we chart is                                 how many times a single query had to                                 look up terms we wrap directory and                                 index input to gather IO counters so we                                 have a metric that tells us how many I                                 ops did the query do each query we have                                 this metric how many bytes did it read                                 and if we push a change that double the                                 number of bytes then we want to go back                                 and understand why that happened so                                 those metrics make those abstractions                                 which Doug cutting a long time ago had                                 the four                                 site to create most of his abstractions                                 are still in loosing today those are                                 incredible tools for doing gathering                                 custom metrics for for search engine we                                 learned the lesson that everyone else                                 has learned already that full garbage                                 collection is bad                                 Lusine is an amazingly lightweight                                 search engine it does not allocate much                                 memory to handle a query and and many                                 concurrent queries in flight really                                 don't allocate that much memory we made                                 that we broke that because we had some                                 places that we're using a lot of heap                                 places we didn't think would be so bad                                 but under load it turned out to be quite                                 bad and and we don't trust G                                          although Wei told me I should start                                 trusting it we still use the depth the                                 now deprecated concurrent collector                                 because we want low pause times and we                                 don't mind trading off some throughput                                 to get that Azul has an awesome tool                                 called J hiccup this is a way to monitor                                 the pauses you see in production not                                 just from garbage collection but from                                 the operating system scheduler your i/o                                 devices if there are any unexpected                                 pauses that will tell you the best you                                 can do on your long pull query latencies                                 because if the JVM is pausing the                                 machine is pausing you you can't do any                                 better than that no matter how fast you                                 make your code so when we when we had                                 problems with garbage collector we were                                 hitting full stuff the world events we                                 fixed a few places that we're holding on                                 to too much heap we increased our heap                                 size and we post a few parameters from                                 elasticsearch that caused the concurrent                                 collector to work a little harder to                                 kick in a little sooner and that was a                                 big improvement for us and this chart is                                 not our benchmark chart this is from the                                 scenes nightly benchmarks just an                                 illustration of how much you have to pay                                 attention to your garbage collector this                                 is what's showing the performance of a                                 sloppy phrase query on a full Wikipedia                                 index so it's kind of a costly query for                                 Lucene to run it was sort of find you                                 know through months that it was fine for                                 quite a while back before this but then                                 suddenly we have created lou singh's                                 nightly benchmarks to JDK                                              substantial in                                                        QPS performance of this one query there                                 was a discussion on the dev list                                 huwway said it must be the garbage                                 collector and so I went back and fixed                                 the Java invocation to go back to the                                 the throughput collector parallel GC and                                 that restored quite a bit of our                                 performance not all of it so there's                                 still something missing there but                                 garbage collection                                 is difficult okay I'm gonna hand it over                                 to Sparky you can still call me Mike                                 it's okay                                 okay well Michael come back at the end                                 and we'll try to leave some room for                                 questions but I'm gonna take the rest of                                 this so um                                 analysis I mean everybody has to deal                                 with text text is you know one of the                                 great things about Lucene is it gives                                 you so many tools for dealing with text                                 you know there's an analyzer for every                                 language there are many many tools you                                 can apply however every application is                                 unique and it's in its own special way                                 and we're no different we found we had                                 some kind of special challenges to deal                                 with part of it part of this also comes                                 from trying to maintain the history of                                 an engine that was already built                                 in-house over                                                       compatibility because we we had a kind                                 of a goal not to change the user search                                 results too much during this port                                 because we don't want to test too many                                 things all at once so how do we maintain                                 that while completely shifting you know                                 the way that the analysis is being done                                 underneath we committed to to doing                                 basically a leucine native analyzer I                                 mean early on we said well we could just                                 port the existing code wrap it all up as                                 a tokenizer and say well that's our                                 that's our analysis chain instead we                                 decided to break it up into pieces and                                 that was more challenging I'm gonna give                                 you a few examples here's one what does                                 plain mean to you                                 well obviously like many words it has                                 many meanings you know even in the                                 context of shopping could mean many                                 different things you know it could be it                                 could be an airplane Amazon doesn't sell                                 airplanes but it does sell toy airplanes                                 and airplane keychains I guess could                                 also mean you know in English a tool a                                 bench plane or a plane so when someone's                                 searching for for that we want to give                                 them the right the right answer well                                 what is it well the thing is what we'd                                 like to be able to do is apply some                                 synonyms if someone searches for                                 airplane but that document only said                                 plane you know we'd like it to match so                                 we want to apply an airplane to plane                                 synonym but if we do that we don't want                                 when they search for airplane for them                                 to be finding bench planes so this is                                 kind of a challenging problem                                 the good news is we're doing our                                 synonyms at index time we know at the                                 document                                 I mean we know for example that this is                                 not a toy we know it's a tool so we can                                 apply different synonyms in different                                 contexts                                 that's that's the sort of the theme here                                 is context-sensitive analysis we want to                                 be able to apply different analysis                                 chains depending on maybe the value that                                 comes from another field in this case                                 the category that the document is in                                 that's kind of challenging to do or has                                 been challenging to do in lieu seems API                                 is because you the way that you                                 typically apply an analyzer is by                                 specifying which field it applies to so                                 you say ok I'm gonna use this analyzer                                 for the title this one for the full-body                                 text but it doesn't really give you a                                 way of combining information that come                                 from other fields so we had to write                                 some custom code around that and then at                                 some point discussion on the mailing                                 list led to some entirely new way of                                 handling this which is a conditional                                 token filter which we didn't have time                                 to use because came out later but I                                 think that'll be a nice new way that you                                 can do this kind of thing yeah I think                                 that's all I have to say about synonyms                                 another thing that you know was kind of                                 unique in our use case in the analysis                                 chain is the way we handle numbers like                                 searching for products on in a catalog                                 people really care about the sizes the                                 age that the products are for the prices                                 you know the the voltage that the tool                                 works at and so on so the numbers are                                 really important in a way that they                                 aren't so much in a traditional you know                                 let's say full-text search so some of                                 the things we do are like twenty four                                 three year old we wanted to match this                                 age range two to four years but nowhere                                 in the text of that document does it say                                 three so you know we expand the range to                                 include all the intermediate numbers you                                 know we want to do number unit                                 translations so that you can you can                                 search for you know                                                   don't match things that are about                                     volts or what have you and then handling                                 decimals and fractions and other kinds                                 of numbers with punctuation you know is                                 is is a challenge I asked people to ask                                 me about tires today only only a few                                 people did but we you know we had a bug                                 with tire sizes because                                 our sizes have slashes in them they're                                 often written as several numbers with                                 slashes in them and you know we confuse                                 them with fractions so there's just all                                 these subtleties and and it's really                                 tricky building analysis right because                                 you you um change one thing when one                                 place and it changes everything                                 somewhere else even though it's a nice                                 modular chain of filters right there                                 they impact each other I think we all                                 know that if we fiddled with with                                 analysis one of the things the                                 consequences of having all this                                 specialized punctuation handling is that                                 we can't really use standard tokenizer                                 standard tokenizer is what most people                                 use it it's works great unless you're                                 very fussy about punctuation but then                                 you can't use it because it drops all                                 your punctuation so instead we use you                                 know basic a basic tokenizer with                                 whitespace and then later on we apply                                 word delimiter graph filter where                                 delimiter graph filters like the Swiss                                 Army knife of an analysis chains I mean                                 it handles a special case of sort of                                 secondary tokenization right you can do                                 it if you didn't tokenize on those                                 things early the punctuation or the non                                 letter number characters or whatever                                 it'll split them up later but it leads                                 it leads to trouble like if it's it's                                 useful but if you don't have to use it I                                 would say don't use it because it causes                                 Lucene gets a little bit unhappy if you                                 have multiple things that split your                                 text into tokens anyway but you can use                                 it and then we do it's pretty good but                                 one takeaway from all this analysis                                 stuff there's there's not really a theme                                 here because it's just a lot of little                                 problems one after another right and you                                 have to solve them and keep working at                                 it until you get it right and we did                                 that but I would love it if we could                                 like isn't machine learning the sauce                                 that's supposed to solve all our                                 problems like with somebody please apply                                 it to tokenization I would love I would                                 love that I think we need that there's                                 an opportunity here I mean this is like                                 the oldest part of search but it needs                                 some work okay so that's enough about                                 analysis query optimization so I'm gonna                                 Mike refer to a few of these things at                                 all before sorry and I'll just dive a                                 little deeper                                 you know latency is important for us our                                 customers are humans we don't want them                                 to have to wait if they wait too long                                 they'll go away they'll buy things                                 somewhere else                                 speed is super important you know it's                                 important in in every dimension delivery                                 has always been one of Amazon's like                                 prime                                 promises we'll get it to you fast but                                 you know it's also part of the search                                 experience and like if you have to wait                                 for three seconds you know it's just not                                 good so you know one of the tricks this                                 was this was a cool idea that I think                                 many people can use and you know I'm                                 it's called index queries here there's a                                 couple of different ideas here they both                                 come out of like what I would call                                 adaptive indexing so the basic idea is                                 look at your queries see what people are                                 searching for commonly and then use that                                 to inform your indexing process and it's                                 a circular thing and it it's tricky but                                 you can get some nice gains from it so                                 what we did here is we went to our query                                 logs you know printed the Lucene query                                 that it was we were parsing out of after                                 doing all our processing and we thought                                 well hey like all our queries have the                                 same you know set of filters they're all                                 filtering on you know these are some                                 very common things that say this                                 document is a product it was not                                 suppressed by some rule that some                                 business rules someone applied it's not                                 an adult product most queries are like                                 this right and there are there are more                                 filters than that I didn't show you all                                 of them that are repeated over and over                                 and you know leucine is really fast at                                 matching these very simple term queries                                 and then conjoining them together                                 finding the documents that match all of                                 them but it still wasted effort like if                                 we can replace all of those with a                                 single term hey we can we can go faster                                 so basically what we do is we treat the                                 queries as documents we look for                                 commonly occurring sub queries we factor                                 them out of the queries and the                                 documents so when we're indexing we find                                 documents that match these commonly                                 occurring sub queries and we index a                                 single term to represent that and then a                                 query time we do the same thing replace                                 all those terms that whole term thing                                 with a single term query and we got a                                 nice speed up from that this is a little                                 more about how that works I mean in                                 general the problem is hard because                                 boolean expressions are trees you know                                 you them it's kind of a challenging                                 problem but our queries are simpler you                                 know we have a process that generates                                 them that doesn't create these very deep                                 trees so that helped also I'll just give                                 a shout-out to this FB growth algorithm                                 I don't know if anybody's encountered it                                 I learned about it during the course of                                 this work and it's pretty cool it                                 commonly occurring sub patterns you know                                 inquiries it's commonly applied like in                                 database world and it sped things up on                                 our index inside so yeah this is kind of                                 the results that we saw this was early                                 on you know but we saw a nice boost at                                 that put time thirty percent in                                 improvement to our QP s you know by                                 avoiding all this repetitive work and                                 latency went way down I think maybe a                                 little different now but it's it's a                                 it's a nice boost we did a similar thing                                 with full text so the thing I showed you                                 before was really just for kind of                                 filters you know on very simple terms                                 but you can also apply similar                                 optimization to you know text it's a                                 little different here because the                                 cardinalities of the terms are different                                 you have to kind of set different                                 thresholds about how many of these                                 things you index so you don't blow out                                 various limits but I wanted to show                                 these partly just so it's a cool idea                                 but also because the terms are kind of                                 interesting I think and it highlights a                                 few things about our system one of them                                 is that hey this data is changing all                                 the time I mean you can see Valentine's                                 Day down there at the bottom it's not                                 all that interesting for us to index                                 Valentine's Day most of the year so if                                 we build this index you know this this                                 kind of set of indexed tuples and                                 queries once and leave it then we're our                                 performance is going to slowly drop and                                 that's kind of a dangerous thing to have                                 in a production system you know you                                 thought your performance was this but if                                 you just leave it alone and don't do                                 anything it's slowly going to degrade                                 right oh that's not a good situation to                                 be in                                 so so you know you really need if you do                                 this kind of circular kind of indexing                                 of you know dynamic indexing you have to                                 have an automated process that's                                 continually updating it and I think the                                 the faster that loop works the better                                 off you are because you know we reach                                 patterns change very quickly new product                                 drops suddenly there all these queries                                 for a thing you never saw before anyway                                 yeah that's kind of a oh yeah another                                 optimization this is this is cool I mean                                 Mike talked a little bit about this but                                 just to elaborate so lightning deals                                 with what are they basically we want to                                 sell you know at a discount on for two                                 hours you know buy it now get it for                                     less or whatever so they have all these                                 deals                                 but searching for those deals                                 traditionally was very expensive they                                 were implemented as a post filter you'd                                 find all the matching documents and then                                 after the fact you'd search them all to                                 see if they were if you know they                                 matched your deal                                 well we've replaced that using the                                 dimensional points and leucine we saw it                                 very nice speed up the funny thing about                                 these dimensional points as as you know                                 maybe or as Mike said before they they                                 were originally designed for geo queries                                 so like for example in that region there                                 you see a red you know region the query                                 might find anything in that region by                                 essentially breaking it up into little                                 rectangular sub regions and then                                 subdividing them to match to match the                                 boundary using this tree like structure                                 in multiple dimensions to here but you                                 can use it for other kinds of data so                                 for this example the Lightning Deals we                                 have really three dimensions we've got                                 the start time the end time and an                                 identifier you know which is just which                                 deal is it and we actually like the                                 weird idea was well that identifier                                 isn't a number but we can throw it in                                 there you know and it doesn't have any                                 ordering properties but it's still a                                 dimension and it actually works out                                 great so we're hoping that we see that                                 we you know this saves us from a prime                                 day disaster no it's we've already seen                                 good speed-up so that'll be good okay                                 Wow am i doing it in time forty minutes                                 in I want to save some time for                                 questions six minutes left so I'm                                 probably not going to get all the way                                 into this last thing because it's kind                                 of complicated but I'll give it just a                                 gloss on it basically the story here is                                 we have some very expensive machine                                 learned models that we want to apply but                                 you know we also have a lot of documents                                 we can't score those those models across                                 all our documents we have these tight                                 latency guarantees so how do we do it                                 basically we do multi-phase ranking                                 first phase let's see what did I this                                 isn't really what the slide says but why                                 are they expensive well basically they                                 just do a lot of stuff a one funny story                                 is we initially tried to model these                                 these scoring models as leucine                                 expressions I mean a lot of people do                                 that right you take you take your                                 recency time and your relevance score                                  from the text and you kind of fold them                                  together into a function and leucine                                  will pretty efficiently come                                  shoot that by compiling you know this                                  javascript looking expression into java                                  and then it's very fast but we                                  discovered that you can your expression                                  has a limit a function a Java function                                  cannot have more than                                                     in it so you know otherwise it runs into                                  JVM limits and these are really big                                  these things so it just wasn't it just                                  wasn't working you know so we had to                                  write custom code for that part of it                                  but but anyway it just gives illustrates                                  that they're kind of expensive so what                                  do we do is we do this multi-phase                                  ranking thing the first phase we do                                  index you know static rank that's that's                                  done at indexing time we just pick the                                  top end documents from that and we say                                  well we'll just rescore those with our                                  less or more expensive model we actually                                  do two phases of that we have a kind of                                  intermediate expense model and I think                                  that in the interests of saving time for                                  questions I'm not gonna tell you about                                  this really cool thing that we did but                                  I'll just I'll just leave you with a                                  problem to think about you may remember                                  and this illustrates a little bit when                                  Mike talked about concurrent searching                                  across multiple segments before that he                                  showed that picture which may not be in                                  your head anymore but there were you                                  know how do we collect all these                                  documents across all the segments then                                  we merge sort them right well when we                                  did that we collected let's say we want                                  an end documents we collected n                                  documents per segment and then we threw                                  most of them away well we probably                                  didn't need to do that I mean most of                                  them came out of the big segments fewer                                  of them came out of the small segments                                  so anyway we did some stuff to prorate                                  that and got a little speed-up                                  so that was kind of a whirlwind tour                                  this is more about showing you how the                                  collection works when it's proportional                                  and you know you run the risk of not                                  getting quite the same results you would                                  have done otherwise but you've got a                                  speed-up from it so that was a kind of a                                  neat thing yeah I think I'll leave that                                  there and ask Mike to come up and wrap                                  us up ok real quick so yeah it turns out                                  Lucene can handle this search                                  application it's been really challenging                                  we're using all kinds of features we                                  push changes back upstream but it's                                  working really well if you go and search                                  on amazon.com today you're using Lu                                  seems sometimes it's not a hundred                                  percent out there yet                                  segment replication is incredibly                                  efficient if you have deep replicas                                  count so that's something I hope someday                                  elasticsearch will offer it as an option                                  if you have really deep replicas it                                  would be a big win we're using multiple                                  threads to handle one query is                                  incredibly incredibly important if you                                  care about latency and you're not always                                  running at redline and come join us if                                  you like working on open source software                                  on really challenging high scale search                                  problems we are hiring so unfortunately                                  we probably only have time for one                                  question and this was an amazing talk so                                  show of hands who wants to ask a                                  question and Mike and Mike will pick                                  from the audience one two okay you can                                  come to our booth we have a booth                                  downstairs so if you want to ask us more                                  challenging questions come and ask us                                  thank you for the talk I just have one                                  question related to synonyms handling so                                  you mentioned you do synonyms during                                  indexing time do you think you are                                  missing the context of the user or and                                  the user query or do you also like a                                  different type of synonyms during query                                  time so yeah probably I think it's done                                  in the interest of efficiency for the                                  most part you know although this this                                  document context is something we                                  wouldn't have a query time so there's a                                  trade-off I think there's a there's a                                  role for both there are other processes                                  that we didn't talk about which run kind                                  of prior to the search engine as well                                  and they do some of that work too                                  there's some kind of query rewriting                                  that happens before we see it but but I                                  think there's room for more yeah okay in                                  theory we have one minute left so any                                  other question Tuesday thanks for a nice                                  talk so you said it leucine powers most                                  of the searches what else what else do                                  used for powering the rest on searches                                  well so so far                                  Lusine is used all over the place the                                  Amazon for                                  situations that aren't product search                                  busine elasticsearch and solar so we're                                  just talking about in the product search                                  situation Amazon started with an                                  in-house search engine they developed                                  years ago it's highly tuned to what                                  Amazon customers need for search and it                                  was highly challenging to replicate its                                  behavior on top of leucine so that's                                  what handles the rest of the queries
YouTube URL: https://www.youtube.com/watch?v=EkkzSLstSAE


