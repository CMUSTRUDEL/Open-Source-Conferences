Title: Berlin Buzzwords 2019: Lars Albertsson - Eventually, time will kill your data pipeline #bbuzz
Publication date: 2019-06-20
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	Race conditions and intermittent failures, daylight saving time, time zones, leap seconds, and overload conditions - time is a factor in many of the most annoying problems in computer systems. Data engineering is not exempt from problems caused by time, but also has a slew of unique problems. 

In this presentation, we will enumerate the time-related problems that we have seen cause trouble in data processing system components, including data collection, batch processing, workflow orchestration, and stream processing. We will provide examples of time-related incidents, and also tools and tricks to avoid timing issues in data processing systems.

Read more:
https://2019.berlinbuzzwords.de/19/session/eventually-time-will-kill-your-data-pipeline

About Lars Albertsson:
https://2019.berlinbuzzwords.de/users/lars-albertsson

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              thank you everyone                               and a little time that we have here on                               earth we spend writing data pipelines                               one of my favorite things to do on in a                               professional aspect just to come here to                               this conference because it's such a                               technical ordinance you can really D                               keen on something and go deep and this                               year I've chosen to geek in on the                                troubles that time I caused and how to                                deal with them so at support I saw a                                couple of slide or a couple of graphs                                that look like this these graphs tend to                                freak people out in particular if you're                                at the dip and somebody had launched a                                campaign or launched in a new country or                                something I'll also send a graph that                                looks like this there's a mysterious dip                                I kept midnight every day doesn't seem                                to match what we expect or this                                mysterious bump in the middle of the                                night one about that what that might be                                or this one only premium users are                                supposed to access premium services but                                apparently some free users do that but                                then they stop at midnight and then they                                start again that's weird these are                                examples of things that do not really                                match what the users did but are caused                                by our mismanagement of time and data                                processing pipelines so I will mention a                                bunch of issues that I've seen a bunch                                of and patterns there were we have dealt                                with things badly and a couple of                                patterns that I have come to conclude to                                use in order to mitigate the problems                                these are quite I'm quite opinionated in                                my pattern so take it with a grain of                                salt my hope is that you will become                                more aware of the issues that might                                occur and that I will share some of the                                tools that I have in my toolbox first                                looking at what from a time perspective                                what different types of data we have we                                have facts these are either or events                                that happen or like observations                                measurements out in the wild these have                                a time stamp they happen at a particular                                time and they communicate like a                                continuous stream we don't really                                control the time something else does we                                also have state that we have on our                                system and we have usually dump to the                                I can explain why later and these are                                typically dumped that like regular                                intervals daily or hourly or something                                and then we have claims which is a                                statement about something that has                                happened previously I'll give a couple                                of examples anymore and these claims                                always have a windows a time window                                scope like their claims about a                                particular time period these events and                                state dumps and and claims have                                different time scopes it's more mostly                                complicated for for events you have the                                event time the time at which the event                                occurred and they have some registration                                time when you see the event coming into                                your back-end systems where you can                                trust the clocks and then you have the                                ingestion time where which is when you                                put it into your data like for for                                eternal preservation in a single DC                                environment the registration ingest this                                usually the same time in a multi                                disinvite meant you might have a                                propagation delay and then later we                                process the data so we have a processing                                time as well for for state we only have                                the ingest time and then for claims we                                have the word I call the domain time the                                time scope which the claim refers to for                                example these were the users that we                                thought were fraudulent during this                                particular time we know we use clocks to                                measure time they don't actually really                                tell us what time it is they tell us how                                much time has elapsed and but in order                                to deal with them we first tell them                                that more time it is and then we get a                                new machine it back we have clocks that                                our high quality usually our back and                                stuff clocks that are of low quality                                like in IOT devices mobile devices and                                so forth we also have wrong clocks which                                is when we look at the different time                                scope than the one we actually care                                about sometimes this is okay we used the                                 wrong time scope as a proxy and some                                 clocks we control some clocks we can                                 trust in their value evaluation their                                 measurement of time but we don't really                                 control the how they are set in terms of                                 time zones and so forth                                 clock time maps the calendars which is                                 like your more a human invention that                                 maps to the astronomical and social                                 domains and if I were to ask my son how                                 the definition of a calendar this is                                 what he might come up with this is also                                 what a computer comes up with if you if                                 you ask it Maeve Lee now reality is more                                 difficult in a naive calendar you might                                 think that these properties hold like is                                 a day has a certain number of seconds if                                 you go                                                                   in what year you are at plus                                           start at the beginning of the year and                                 you go                                                                                                        end up in December if you have                                    minutes forward you might end up in the                                 next hour you think and if you shift                                 timezone you would perhaps think that                                 you at most shift today and that the                                 number of minutes is preserved and if                                 you are at December                                                      that the next day is December                                         that January or February has like                                                                                                             obviously since I put them up these are                                 at some point in time wrong how many                                 people can find at least one                                 counterexample for these things Oh                                 how many people can find at least two                                 keep your hands up at least three at                                 least for at least five six oh it's not                                 so easy I had to Google a bit on this                                 one it's performance the leap seconds                                 second one is year zero after year minus                                 one comes year plus one in historical                                 time counting there's an astronomical                                 just to confuse things where there is a                                 year zero there was a switch between                                 Julian and Gregorian calendars when you                                 might ask well it stretched over                                     years in different countries and it                                 stretched your way into the                                              daylight savings time you're familiar                                 with time zones are not                                 not really on a full hour and they span                                 more than                                          some countries jump back and forth                                 between on the date line so they                                 sometimes skip a day and I proudly                                 present the only country to have had the                                                                                                          confusion in the Gregorian Jolanda                                 gorian calendars which fortunately you                                 don't have to know most of these things                                 they rarely come up unless you're doing                                 real historical data leap seconds might                                 go up it took down a couple of big sites                                 a few years ago for data processing it's                                 probably not a huge deal you might have                                 to prepare that that the timestamps                                 might have the second number                                           than that you should are probably fine                                 if you're in Google they smear the leap                                 seconds out very elegant solution I                                 don't know what l other cloudforest do                                 daylight savings town time on the other                                 hand that for sure one will bite you it                                 even kills people are you aware that the                                 in spring when you lose an hour there's                                 an elevated number of heart attacks and                                 there's a lower number of heart attacks                                 in autumn but they don't add up                                 so daylight savings time killed people                                 and of course they'll attend his time is                                 the explanation for this counter graph                                 where suddenly the hour the day has like                                                                                                          in the awesome now how might this affect                                 technical systems this is a scenario we                                 had recently                                 this is a typical ingestion pattern                                 called the loading dock you have a                                 legacy system it pushes data on a                                 regular basis and this gains hourly to a                                 neutral ground a file system a Google                                 cloud bucket in this case and they                                 ingest your mechanism pulls it and                                 compensate to the data like usually the                                 legacy systems are more difficult to                                 change so you adapt to whatever                                 conventions they haven't just accept                                 them in this case by the way I included                                 some Luigi workflow code because I've                                 I've seen people stumble with these                                 types of patterns so here's an example                                 you can get sliced later if you want in                                 this case it turned out that the time                                 stamps for a local time would including                                 local timezone which doesn't matter all                                 that much until there's a shift and then                                 in spring suddenly there's one hour                                 missing and then in autumn if we're at                                 the code the work local this way it will                                 pick up the first data set it will                                 ignore the second data set and we have a                                 silent data loss you can compensate                                 these either on the source systems by                                 not including the daylight savings time                                 or the time zone or by with some hacker                                 of course in the receiving system so one                                 principle that I try to push to to all                                 the clients and people that I worked                                 with is to separate the online world                                 from the offline world in the online                                 world processing time and event time are                                 tightly connected you process things                                 right away after they happened in the                                 offline world you process them later                                 with streaming just a little later with                                 batch perhaps much later and it can be                                 tricky when you mix these world due to                                 the different time scopes when you're                                 batch processing there are a number of                                 principles essentially inherited from                                 functional programming that I found to                                 be really valuable and to adhere to to                                 make things simpler I will explain why                                 these are so important in the very end                                 essentially you want your batch process                                 to be a pure function it should be the                                 output should be a function only of the                                 input and of the code of nothing else                                 not state and databases no randomness no                                 wall clock time don't look at the                                 processing time and so forth and you                                 want the amount of input data to be                                 known and bounded this is a common like                                 beginner mistake and data processing                                 violence that I see often we process                                 whatever data happens to be in this                                 bucket or in this bigquery data set and                                 so forth                                 in order to achieve this if you want                                 state from a database                                 we cannot query the database so                                 therefore we dumped the database to like                                 on a regular basis it seems simple I've                                 seen lots of ways to fumble with this                                 one is to direct your cluster to dump                                 from the database which is fine if the                                 data is small but if the data is large                                 this is essentially a denial of service                                 attack this is one of my oldest slides                                 as you can tell from the references to                                 scoop and MapReduce some of you might                                 have seen it before I've seen a couple                                 of systems go down this way and you're                                 essentially mixing up the offline world                                 separation of processing time was                                 processing time and event time are                                 separated into the online world where                                 they are the same and the offline world                                 wants all of the events at once and                                 process it as soon as possible                                 so here's a a pattern corresponding                                 pattern that you can do you are very                                 careful taking the data to the offline                                 world                                 and in and I recommend using some kind                                 of replica you can have a library liquor                                 in this case I suggest taken the backups                                 from your databases restoring in an                                 offline database and you have separated                                 the online and the offline world now                                 what do you want to do with your                                 snapshots in many cases you want to                                 decorate your events you have your users                                 in the database and your all of your                                 events have a user ID so you want to                                 decorate them and throw some remote                                 demographic information in there and so                                 forth but the events are a continuous                                 stream continues over time and this the                                 snapshots that you have come at regular                                 intervals so when you join you will                                 always join information from two                                 different times so this you can usually                                 live with this this is acceptable but                                 you need to be aware and it's often                                 easier to have the mismatch only in one                                 direction it's common to to join with a                                 previous snapshot or the user database                                 for example otherwise you have to wait                                 for an hour or                                                   mismatch is what causes graphs like this                                 because here the user we was free a free                                 user here upgraded to premium here and                                 then started using premium services here                                 but when you join you have the mismatch                                 and you have this weakness as long as                                 you where it's usually fine if you're                                 doing batch you your events or bucket                                 and in two batches and if that batch                                 doesn't really align with the dump time                                 of the database you might end up with                                 having wrong information in two time                                 directions usually okay but if you care                                 about being the data being wrong in only                                 one direction you might want to shift                                 this window and a couple of occasions I                                 try to understand a complex system where                                 and the different different situations                                 with how we bucket and where the data is                                 copied from and the other data and                                 perhaps we can align to make the error                                 smaller and the result has always been                                 no because of some reason we're screwed                                 anyway we just live with it with the                                 information loss you can avoid this by                                 adopting a paradigm called event                                 sourcing where you say that the the                                 state and data basis is not the truth                                 the only truth out there is the history                                 of events that has happened and we do                                 use databases but they are just the                                 cached view of a certain aggregation of                                 the history of events now if you only                                 look at if you join now you can choose                                 to play as many events as you want to                                 match the time where you want to join so                                 that you don't have these mismatches                                 anymore unfortunately this makes your                                 code significantly more significantly                                 more complex you don't you no longer                                 just look up and a table and undo a                                 plane join this point any streaming fans                                 might say well this these are problem                                 specific to batch right if we stream we                                 can update a table all the time and join                                 with that table so great we have one                                 stream with it with the user events here                                 perhaps and we have some other stream                                 with a service events or playing of the                                 films or whatever so we just update this                                 table in the middle and join with that                                 whatever a state is in that table and                                 this will give you the right data under                                 the naive assumption that these streams                                 are in sync and practice it's difficult                                 to make streams being sync so every join                                 here will be a race condition to see                                 which event goes first to the table                                 let's continue on the streaming track in                                 order to avoid these types of race                                 conditions you usually look at Windows                                 when you're doing stream processing                                 whether you do in aggregations on the                                 left or whether you're joining here on                                 the right in stream you kind of look at                                 larger windows of time on both the                                 streams and try to match them you have                                 some choices when when making the                                 windows you have to choice whether to do                                 sliding windows or tumbling windows and                                 so forth in addition to this you have to                                 Cho choose what to window over which                                 time or in the upper case here the                                 number of events is also an option the                                 early stream processing systems were                                 only supporting like number of events                                 and processing time now those are                                 easiest to implement but often not                                 necessarily what you need because you                                 have a trade-off here with the size of                                 the window and the accuracy and if you                                 window over a number of events for                                 example then if there's a spike in                                 events your effective time window will                                 shrink or if you window over processing                                 time you lose the nice                                 property of reproducibility if you ever                                 try to replay again you will get a                                 different result which breaks these                                 functional principles that I happen to                                 like a lot                                 so lately we've seen support for joining                                 or for windowing over event time instead                                 which is usually what you want from a                                 business logic perspective                                 unfortunately you no longer know how                                 much resources you need if you have a                                 spike then your resource your memory                                 consumption will advise and if you have                                 all the events times like some bad clock                                 stating that it is now next year then                                 you've certainly flush all of your                                 windows and not do what you want so you                                 have to do some time to check in and so                                 forth                                 so no matter what you do you have some                                 interesting trade-offs and this is not                                 so easy if anybody still thinks that                                 this might be easy I suggest you watch                                 hawk anomalous talk from from last year                                 which could have been titled it is                                 surprisingly difficult to join two                                 streams but it wasn't it had another                                 title lots of interesting learnings so                                 let's try and see if we can sort of do                                 better with batch instead and and see                                 how many problems we can avoid in the                                 batch world we take the events in we put                                 them into like our buckets for example                                 or daily buckets so we need to again we                                 need to decide to window over what                                 because these these buckets are also                                 windows and also we need to decide when                                 we when these windows these buckets are                                 complete when can we start the                                 processing downstream so this is a data                                 collection system I was working with                                 from many years ago we had logs going                                 events going down of files in the                                 different services and we partitioned in                                 my hour and we copied them with with or                                 sync or SSH this even predates the                                 existence of Kafka hence the ancient                                 tools and then we put them over there in                                 hourly buckets and then we decided to                                 start the processing when all of the                                 hosts had reported their data now as the                                 number of hosts grew this got more and                                 more fragile and we were processing                                 later and later later                                 not scalable and we were trying to add                                 add on various hacks with looking at the                                 monitoring system to figure which shows                                 were up or down and so forth and just                                 get more and more and more complex and                                 we all like the feeling this is not                                 supposed to be this complex and it isn't                                 but we didn't figure that out at the                                 time so another Swedish company had                                 another trick they they optimistically                                 started the batch processing downstream                                 when the hours through and then they                                 measured how much new data has arrived                                 and I mean if you went over a certain                                 threshold they triggered a reprocessing                                 downstream which is also not great                                 because then you have a data collection                                 determined or decided for every use case                                 downstream what the quality requirements                                 were which worked for one or two use                                 cases but it didn't work in the end for                                 for many use cases there are tools and                                 structures where you can do reprocessing                                 googily is is doing something along                                 these lines but they have very good                                 internal tools we they've exposed some                                 of that tooling in turn in terms of a                                 patchy beam where you can say hey now I                                 now have an updated data set so forth                                 the I have so far                                 not used or deselecting those tools                                 because it turns your batch processing                                 operationally into a stream and batch is                                 much easier from relational perspectives                                 I try to try to stay with the batches                                 master much as I can                                 they're also tools in terms of our                                 versioning you you generate new versions                                 of datasets downstream and so forth or                                 or in provenance keeping really good                                 track of what was used for what and so                                 forth I again try to avoid those tools I                                 because I feel that the with the tooling                                 that exists today the the cure is worse                                 than worse than the disease and I think                                 there are simpler patterns so here's                                 what I try to do these days we have some                                 different times scopes that we can                                 choose to bucket from and just choose                                 the closest one the ingestion time                                 because if you do that it's a local                                 clock in the                                 cool system closest to the data lake and                                 that means that you can close and seal                                 the datasets very quickly after the hour                                 is through so you can start processing                                 at that point so you don't have these                                 situations where we're waiting for more                                 data done before processing                                 unfortunately if you're doing it                                 analytics it leads to graphs like                                 therefore one and then per use case you                                 decide if that dip comes what do you                                 want do you want to see the dip with the                                 data that you have or do you want to                                 wait until you have better data you can                                 decide whether you have better or not                                 based on the event time this should be a                                 decision per use case and in the inner                                 dashboard where you might want both but                                 but for machinery downstream we have to                                 make a choice and this is what it might                                 look like Luigi code to the left bar                                 code on the right we if we want to wait                                 and get better data we delay the                                 processing with n number of hours and                                 then in the in the spot code we just we                                 just take all of the wide window shuffle                                 and spit out the the hour that were                                 actually interested but since we waited                                 for a while we have some more better                                 data what if we're wrong we might be                                 wrong and that's ok as long as we know                                 how wrong we are so we count whenever                                 our assumptions fail then and the data                                 goes at the event time falls out of the                                 window that we're looking at we count                                 those as cameras so after the fact if we                                 have a report we might know whether it                                 was a good report or not and if it                                 wasn't we can increase the delay and we                                 compute different delay hours for to                                 cater for all the use cases that we have                                 so we usually do a fast one without                                 delay delay our say equals zero and then                                 some more and further for the really for                                 the cases where quality is really                                 important like financial reporting and                                 so forth we wait for really long time                                 so then we have some kind of land                                 architecture in batch usually the lambo                                 architecture is is you have you assumed                                 that your streaming is unreliable so you                                 redo the same thing about here we assume                                 that the first batch processing is                                 unreliable because it doesn't have all                                 the data so we add another one or two or                                 more pipelines on the side                                 this might seem complex it's actually                                 not because you can essentially use all                                 the code it just at which parameter I've                                 been in cases where we are dealt with                                 like financial reporting and also some                                 new data sets come in afterwards like                                 that like the fraudulent users or                                 something that we want to include in the                                 high quality reporting so this and                                 that's shown in this example here so                                 those fraudulent users are examples of                                 data that is delayed not by machinery                                 but by humans claims is also a form of                                 data that is delayed by humans the                                 humans on inside your company or outside                                 your company have not yet figured out                                 what they are supposed to claim like you                                 compensate economical compensation or                                 whatever and this might convert late                                 I've had I've had scenarios where                                 according to the contrast contracts                                 external parties might two years after                                 the fact to come and claim things and we                                 have to deal with that so you can't wait                                 two years and until you spit out reports                                 so you want preliminary reports but at                                 the end after all claims you want                                 accurate reports and as I've explained                                 already reprocessing no good instead                                 what you do is to explicitly model the                                 time domain or the domain time and the                                 registration or arrival time so for each                                 domain time scope in the past like like                                 the March this year you have each day                                 build-a-report                                 data set that states what you knew about                                 that window and time on this particular                                 day and then tomorrow we do another                                 computation if new things have arrived                                 about what we knew at what we knew on                                 this day about that time that this turns                                 out to make the whole thing                                 deterministic and explicit and it also                                 allows us to go back and all it things                                 afterwards the dependency tree might                                 blow up in these cases so so your your                                 workflow Orchestrator might have a bit                                 difficult time then there are you can do                                 hacks to make the the tree spar since                                 then and so forth in some cases it turns                                 out to be convenient to depend on                                 yesterday's data set we had this is also                                 one of recent examples where we were                                 looking at stocking in a store and we                                 are getting updates with you know new                                 stocks but only for a portion of the                                 stock so we aggregate by just taking the                                 latest information for a particular                                 stock item if you depend on on                                 yesterday's data then you have a                                 recursive infinite dependency to to some                                 kind of starting point this is usually                                 okay but it represents an operational                                 risk if you decide at some point that                                 you need these will these calculations                                 wrong you need to go and backfill then                                 you need to go back all the time this                                 has often you can get away with this                                 I've had number of occasions where this                                 was actually a practical problem and                                 cost days of delay for for financial                                 reporting and so forth so there is a                                 mitigation here that I used at some                                 point which is to do the recursion in                                 jumps so for every first of the month                                 I wrote to a job that depended on the                                 first of the last last month and then                                 everything in between and then so forth                                 which made you stride quicker forwards                                 in time and then for the other day so                                 the managers just depend on the first                                 and whatever was incremental so it's                                 kind of like MPEG encoding where you're                                 a liar frames and P frames and things                                 and this cuts down the the operational                                 risk here in some cases your business                                 logic the texts that you need to look                                 back in time in order to figure out what                                 the output should be and the simplest                                 example is                                 where you had users to click on your                                 webpage or you use a mobile phone and so                                 forth and you need to you want to plump                                 lump that into sessions with of a                                 certain length and so forth this is an                                 example I taken I hold the course of                                 several months and while this is an                                 example I take in order to make people                                 think about dependencies so let's say                                 let's say there you have a different                                 definition of sessions so which hours of                                 data do you need in order to figure out                                 the sessions here well if for this                                 definition you need infinite history                                 because the sessions might be very long                                 well long sessions might not make sense                                 they're bots anyway so you might cut off                                 here and say that no more than three                                 hours good excellent excellent now you                                 you might think that you need only four                                 hours or data and the sessions will look                                 something like this                                 it turns out if you go to the border                                 here you have a session in the beginning                                 but you nevertheless don't know well                                 that session started at that point or                                 not so depending on the history back you                                 you might have a cutoff date or not and                                 so forth it might be a contrived example                                 but it illustrates that it is often                                 difficult to figure out how much data                                 are you actually what time window you                                 actually need in order to do a                                 particular job and this graph that we                                 saw where there was like a ten percent                                 dip at midnight was a failure to realize                                 that we needed more data than we                                 actually thought we needed what to do                                 about it where you can do the recursive                                 strides as mentioned previously and just                                 take all of the data and but express it                                 in a different way or you can introduce                                 like limits we're going to cut all the                                 sessions at midnight to something that                                 align you to your artificial batch                                 boundaries and then you are losing some                                 information but you are making it more                                 practical and less realistic it's                                 passing a use case why do I care about                                 so much about these properties of repres                                 ability and functional properties like                                 immutability and so forth these are some                                 aspects that I have found that not so                                 many people follow                                 but turn out to be really to really make                                 a difference in whether you get value                                 from your data engineering or not I've                                 learned to gravitate heavily towards                                 simplicity and towards slow data so I                                 always use batch processing if I can get                                 away with it if the use cases can handle                                 it or have the latency because the cost                                 of operations is much smaller and the                                 smaller cost of operations mean that you                                 can innovate faster move faster well in                                 this presentations I've focused on                                 techniques for defending these                                 functional architecture principles and                                 I've solved some of the problems that                                 are bumped up with workflow                                 orchestration why are these principles                                 so important because they support high                                 team concurrency we have learned that                                 immutable data and work and expressing                                 things in in functional pipelines is                                 good for thread concurrency and for                                 computational concurrency it turns out                                 that immutable data sets and data                                 pipelines as the means of collaboration                                 allows for high team concurrency                                 different people can work with the data                                 without applying organized operational                                 risk on the teams that own the data                                 because the data is immutable and so                                 forth and reproduce ability it cuts down                                 your operational risk because if you                                 need to rerun things you know that you                                 get the same thing so you can remove                                 things and rerun them as soon as you                                 want and there's some sometimes talk                                 about the reproducibility crisis and so                                 forth and this this is this crisis is                                 aggregated by or increased by failing to                                 comply with these principles in order to                                 do machine learning on a repeatable in                                 order to get lots of value from machine                                 learning in your products you need to be                                 able to run many experiments and if you                                 run experiments and they are not                                 reproducible they                                 know whether your change made a                                 difference or whether it was different                                 data or just some some of the face of                                 the moon or something that affected the                                 results and the democratization of data                                 and a ability to collaborate is the real                                 value of big data it's not the size of                                 the data it's not the machine learning                                 and shining things this is what in all                                 of the organizations that I've seen is                                 what make the big difference the that                                 you're breaking down silos you don't                                 have to coordinate with lots of people                                 because the data is readily available                                 and that's why I soo-min focus on these                                 things some credits some good timing                                 libraries that I usually use and if you                                 are having questions about the the batch                                 versus streaming and the operational                                 trade-offs and so on I have a                                 presentation about that that you might                                 wanna watch it seems that we do have a                                 few minutes for questions                                 [Applause]                                 who has got some questions please put                                 your hand up as long as I cannot see you                                 no come on I was really exciting talk so                                 yes how do you deal with them like when                                 the members of a team wants to do                                 screaming because it's cool hands oh I'm                                 very much down-to-earth and I have I                                 used to love complexity right but I                                 started in big date with big negative                                 stuff fairly early and now I hate the                                 complexity and some of course some in                                 there are cases where we're streaming is                                 the right trade-off right you need the                                 low latency and that's fine and also you                                 usually have some kind of streaming in                                 your in your data collection pipeline                                 okay so forth I find that you need to                                 have a cultural balance and sufficient                                 focus on value delivery and product                                 ownership and business value focus                                 represented in the teams if you throw a                                 bunch of engineers a way to do something                                 they're going to do really cool things                                 so the successful teams that I've worked                                 with have had a balance of engineering                                 domain expertise and product ownership                                 and in the case of some machine learning                                 a data scientist I've never been in a                                 successful team that delivers feature                                 that was not cross-functional anybody                                 else yes                                 you talk about workflow orchestration                                 but I mean it's still quite problematic                                 to find a scheduler that does all the                                 workflow orchestration for you do you                                 have any recommendations there are two                                 reasonable options there is the examples                                 that I showed here it is Luigi from                                 Spotify the other reasonable option is                                 r                                                                     reasonable options those are both                                 implemented in Python you need a real                                 language in order to express the things                                 that are non-trivial so many of the                                 examples that I put up here if you go to                                 something like OC you cannot express                                 them which one should you take well I am                                 biased I used Luigi since long before                                 our flow existed the it kind of depends                                 on whether you are a buy things or build                                 things organization airflow is has a                                 wider scope it gives you some monitoring                                 it gives you the the scheduling                                 mechanism and so forth Luigi has a                                 narrow scope it's a UNIX philosophy tool                                 do think one thing well but it also it's                                 more versatile in sense that it allows                                 you to express more complex things so if                                 if you were good at our flow you                                 probably able to express some of the                                 things I showed you but some of the                                 things would be difficult I am are doing                                 user flow son expert so those are your                                 two options and thanks anybody for the                                 last question No perfect thank you very                                 much miss amazing                                 [Applause]
YouTube URL: https://www.youtube.com/watch?v=1spKXX2W7Eo


