Title: Berlin Buzzwords 2019: Thomas Weise â€“ Streaming your shared ride #bbuzz
Publication date: 2019-06-20
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	Fast data processing is essential for making Lyft rides a good experience for passengers and drivers. Our systems need to track and react to event streams in real-time, to update locations, compute routes and estimates, balance prices and more. These use cases are powered by our streaming platform that is based on Apache Flink.

Enablement of data science and machine learning friendly development tooling is a key requirement for our users. Learn how we enable streaming SQL for feature generation and development with Python via Apache Beam to provide the development framework most suitable for the use case on top of a robust deployment stack.

Topics covered in this talk include:
-Overview of use cases and platform architecture
-Streaming source and event storage with Apache Kafka and S3; why both are needed for replay, backfill, bootstrapping
-Stateful streaming computation with scalability, high availability and low latency processing on Apache Flink
-Development frameworks for varying abstraction levels and the language to use case fit for Java, SQL and Python
-Python with Apache Beam as the bridge from data science and machine learning friendly environment to distributed execution on Flink
-Kubernetes based deployment to abstract infrastructure and simplify operations of stateful Flink applications

Read more:
https://2019.berlinbuzzwords.de/19/session/streaming-your-shared-ride

About Thomas Weise:
https://2019.berlinbuzzwords.de/users/thomas-weise

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              so the week                               of all then what we do in                               area an example for an application                               integrations connect us and finally also                               how we deploy things in our                               infrastructure so first of all data at                               lift is essential for the business to                               operate and specifically because if                               you're familiar is right sharing you                                have an application and you expect                                information to be up-to-date the goal is                                to match passengers with drivers and we                                want to ensure that our users both                                drivers and passengers have a good                                experience which means the ETA                                information that you see for your ride                                should be accurate and up-to-date the                                pricing should be fair and drivers                                should receive fair earnings and                                notifications should be timely so for                                also for the users we want to make sure                                that they are getting notified in a                                timely manner when there are changes in                                the conditions like traffic delays route                                changes and so on here you see a few                                examples for example pricing pricing can                                change and I'm going to say more about                                that use case later but also fraud                                detection is one area and then just the                                general area of notifications for users                                and the overall experience in this                                application so looking at the data we                                have as different user audiences so                                their data modelers analysts product                                managers and so on and in this talk                                we're going to focus more on two of                                these audiences data scientists and                                engineers the overall platform                                architecture it starts on the left side                                with the data that is being produced and                                being ingested stored in a pops-up                                platform in the form of streams then                                processed by a stream processing engine                                eventually landed in a permanent storage                                for offline analytics we also have you                                can see here tools like a spark and app                                flow and also flight which is another                                scheduler for ETL processing in                                and then finally to make the data                                accessible interactively to use us with                                query engines which presto is most                                workers into presto nowadays and then                                the front-end to is finally to make this                                accessible to users and our focus is on                                the streaming site so for the pops up                                system so the transport of messages from                                producer to consumer we are going from                                Kinesis to Kafka lyft traditionally had                                everything running on Kinesis and there                                have been challenges as business is                                growing and the data volumes are crowing                                also the requirements for faster                                processing also growing so one key issue                                is latency where Kinesis has a very high                                tail latency and when you compare that                                to Kafka with Kafka we can have much                                fast or durable writes and the emphasis                                is on durability we want to achieve a                                good latency SLA but also reliable                                system that we don't lose messages and                                with Kinesis it's hard you have to trade                                one for the other                                essentially with Kafka we don't have to                                make that choice the second part is the                                fan-out limitation fan-out limitation                                means that with how kindnesses is a set                                up and exposed to the users there is                                restrictions how much data can be taken                                out of a stream or a shot in the stream                                and because of that it's not possible to                                do a fan-out on a heist report stream                                because there can only be one consumer                                essentially with the limits that it has                                even though Kinesis makes some                                improvements with the new                                             still limited to five consumers but what                                you see in a Kafka world very often is                                that you have many consumers for the                                same stream processing the data in                                different ways so that's something that                                we need to and finally the scalability                                limitation there are restrictions on the                                number of shots and I believe the                                default is                                                      processing we have it lived we are in                                the thousands of shots you cannot easily                                increase those and it requires manual                                intervention it requires approvals also                                for the account by AWS so and at the end                                 of the day when you increase the number                                 of shots you also increase the cost so                                 those are all problems that we can                                 overcome with Kafka so for the streaming                                 compute we are using fling and fling as                                 different API it has a sequel of fling                                 sequel API you can write Java flick jobs                                 we are also using beam to make fling                                 available to other languages I will say                                 more about that later so with that we                                 have a platform to define processing                                 workloads in various ways and flink                                    needs to be deployed in the                                 infrastructure and that follows pretty                                 much the tooling that's already there at                                 lift currently for observability and                                 other aspects and our deployment of link                                 is currently standalone cluster that                                 means we have instances on ec                                           we put the flink processes on those the                                 task managers and job managers but                                 that's not the future the future of                                 deployment is kubernetes which we will                                 take a closer look in a bit so the flink                                 abstraction levels that were shown there                                 you can write jobs using sequel and a                                 lot of effort has gone into sequel                                 lately and this is going to continue and                                 not just in fling but in other systems                                 to a level below that is your Java                                 program and now you write your data                                 stream or data set in the case of batch                                 flink application with that and requires                                 programming right and then even more                                 flexibility and access to the state and                                 time and and auto details                                 flink provides those abstraction levels                                 too so depending on the use case you can                                 go down and the level of abstraction and                                 gain more flexibility for more work and                                 a higher skill set that is obviously                                 needed to work with it as well                                 about categorizing the use cases that we                                 have at lift broadly we can divide it                                 into analytics steering analytics and                                 then streaming applications and in                                 analytics there are many use cases that                                 have simple needs think of aggregation                                 simple ETL simple ETL jobs where you                                 just want to compute some aggregates and                                 then store them somewhere so that can be                                 expressed actually whistling sequel                                 there's no need to write a Java program                                 for it and then the fling table API also                                 gains a lot of attention now and so                                 let's equate to a set to define a large                                 number of simple flink jobs without                                 going into the programming realm many                                 things are automatic if you use those                                 API optimizations or done by the system                                 state and triggers and time are managed                                 automatically what you get as the user                                 is a faster time implementation time and                                 in the case of lift we have combined a                                 fling sequel with a management platform                                 that also takes care of all the                                 deployment of linked jobs so that to the                                 user                                 there's no fleeing it's just a                                 declaration for the job on the other                                 side our applications that need                                 programming complex use cases that are                                 special that require special logic and                                 more flexibility in the form of finer                                 grained control and those would be                                 written in Java the problem we have a                                 lift is that most teams don't work with                                 Java traditionally a lot of it was done                                 this Python so and so beam is then the                                 route to enable other languages on top                                 of link so let's look at the analytics                                 first the system we call it internally                                 drift the goal of the system is to                                 enable future generation broadly in a                                 unified way for machine learning we want                                 to have one system that can do that for                                 bulk processing how to generate features                                 for for training mostly                                 but also for the real-time processing                                 for the current events for scoring or                                 model inference we are using sequel to                                 the user is using sequel rather to                                 define what is what those pipelines                                 should do and then they give this job                                 declaration to a fully managed service                                 that will ensure that it runs gets                                 deployed in the right way and runs in in                                 a cluster so this is what you would see                                 as a user in that case so there's a                                 configuration file that describes where                                 the sequel query is what the sources are                                 and what the features are that are being                                 created on the right side this is                                 something that is flink sequel                                 essentially so then there are two modes                                 of execution one is batch quote/unquote                                 because everything runs in streaming in                                 this case but it's semantically patched                                 because it's about the data set of past                                 data or current data and in the in the                                 case of processing all day that the data                                 would come from s                                                        different from streaming it could also                                 be s                                                                   sequel part in the middle is exactly the                                 same now looking at the real-time pass                                 the sources Kafka topic or Kinesis                                 stream sequel statement remains same but                                 the sync is something different maybe                                 it's a new stream in Kafka or it's a                                 dynamo DP or multiple options but                                 there's also an interesting case where                                 our historic data processing and                                 real-time processing intersect and when                                 you think of the scenario let's say you                                 want to go back                                                         compute a feature that needs                                            data to do that you need                                                 now to get an answer immediately all                                 right because we have to go back and in                                 the past to even get an answer right now                                 that cannot be done with the current                                 state of pops up at least how we have it                                 not always kinesin always Kafka because                                 of data retention so what we need to do                                 is we need to have a hybrid source which                                 you see here part of the data comes from                                 s                                      then at some point it cuts over to the                                 head data that comes through the pop sub                                 system and computes the features there                                 are more details and the slides are                                 shared so you can take a look at the                                 presentation that is linked here our                                 benefits of doing things this way is                                 that we enable low latency computation                                 and streaming data the teams that use it                                 internally they have a faster onboarding                                 they don't need to be Java programmers                                 the development time is small and no                                 awareness of deployment to the user they                                 don't need to know how flink runs on the                                 machines on on ec                                                      the goal is to be reliable of course on                                 the other hand application use case and                                 this is now specialized because when we                                 look at it we understand why I mean                                 dynamic pricing is one use case dynamic                                 pricing means that the price can change                                 at a given location and time depending                                 on the contextual information supply and                                 demand which are continuously evaluated                                 so if the market isn't balanced then                                 it's not good for traverse and not good                                 for passengers either because you may                                 not get a car if there's too much demand                                 there long wait times and drivers have                                 no reason to fill the demand shortage                                 because they get paid the same price so                                 price is the level to bring this into                                 balance so primetime we call this                                 mechanism primetime primetime is a                                 multiplier on the base price for a given                                 location and time and to determine that                                 multiplier we need to look at millions                                 of geohashes so here it's the location                                 is identified with the geo hash and we                                 need to look at millions of geohashes to                                 update this for every location because                                 the conditions are different at every                                 location and they change fast                                 so it's scale and it's also latency that                                 matters here                                 in the legacy architecture this is a                                 series of cron jobs that do stage                                 processing so in the stage one                                 some aggregation is performed then the                                 results get stuck into Redis then                                 another cron job at fixed interval kicks                                 off takes those does some model                                 computation when an already trained                                 model again store some results into                                 Redis a third phase kicks off computes                                 the information for the pricing service                                 and passes that on of course this means                                 latency in every stage it's not data                                 driven it's driven by a cron schedule                                 and the system architecture is fairly                                 brittle because it's it cannot be easily                                 changed so and in addition to that                                 adding new features is also difficult                                 code complexity comes from the fact that                                 we have to write code for all this                                 scheduling and coordination that                                 normally you would not find in a                                 streaming pipeline so why not use                                 streaming when the team looked at at                                 first there was no pattern for flink                                 flink has a Python API but it's tight                                 and it's not really pi - no we need                                 something where we can use the full C                                 pite and ecosystem of libraries just as                                 it existed already so what can we do                                 about that is Apache beam source                                 promises to solve that problem its                                 promises to have language portability                                 which means that you can choose the                                 programming language that is appropriate                                 for your use case now something like the                                 dynamic pricing we don't want to write                                 in Java because we already have the                                 machine learning models and the people                                 working on it don't even know Java so                                 beam can help you the multi-language                                 support became a reality over the past                                 couple of years                                 end of last year it was possible to run                                 Python for the first time which is what                                 we need at lift                                 there is another SDK which is                                 experimental write notes for go but                                 in theory you can learn or what SDK at                                 at the beam summit in a couple of days                                 from the gentleman with the blue shirt                                 in the back and so there are some                                 choices in language right you pick what                                 is best for your use case maybe you have                                 already written code maybe you have some                                 libraries and then you run it on your                                 run off choice even if the runners like                                 flink Java so that's the beam the                                 promise of beam and we are making                                 progress on that path if you write                                 something in part and it looks like this                                 so simple example reading from a text                                 decide what windowing we want if you do                                 an aggregation define how we want to                                 trigger results with different types of                                 triggers there are three examples here                                 and the computation is just a summation                                 and then write the results to text so                                 just to give a feel of what you would                                 see as a Python program and you can of                                 course then plug your user code in                                 instead of doing a summation you can                                 perform some out of function call models                                 use any library that you would like                                 running this on fling looks on the right                                 side you see a fling cluster but before                                 we are ready to go to the flink lust or                                 the client-side is a Titan program that                                 you run like any other pattern program                                 one parameter that identifies job server                                 now this is a language boundary so                                 you're going from the Python world to                                 the Java world in the case of link and                                 the way this works is that the pipeline                                 that was written in Titan gets                                 translated into a language agnostic                                 format and photograph then it's being                                 sent over that endpoint to the run to                                 the job server which contains the run                                 out of link run all the flink Runner                                 then takes this portable pipeline                                 definition and translates it into its                                 own job graph becomes a flink job in the                                 case of streaming a data stream job and                                 this is being passed over to fling like                                 a lot of link anything application would                                 run but there is a pieces of Python in                                 that job definition and they need to                                 executed somehow they cannot run in the                                 task manager so we have these SDK                                 workers the separate processes that not                                 just to run the Python code and they                                 need to communicate with the flink side                                 which is Java which runs in the task                                 manager and that API is called FN                                 services with various planes to drive                                 the execution as records arrive they are                                 being passed over to the SDK via our                                 results go back from SDK worker to fling                                 chrono fling chrono is responsible for                                 the distribution it's also responsible                                 for the state management for timer                                 management and everything the worker is                                 really just a like a stateless service                                 that is disposable it just executes                                 records or bundles in beam so after the                                 pricing system switch to beam it looks                                 conceptually like this it's simplified                                 but still reading data from Kinesis and                                 now for every record it passes that on                                 to the next function is data driven is a                                 pipeline filtering aggregation all of                                 that can happen in incremental way                                 there's no more cron scheduler here                                 involved it's just the data processing                                 graph and no intermediate state storage                                 is required because the data flows                                 through the pipeline                                 instead of using Redis as a intermediate                                 hop past data so with that they managed                                 to get the latency reduction and the                                 interesting part about the latency still                                 looks high right                                                     mostly bounded by the model execution                                 now not by the system around it and not                                 by the Krone scheduler or not by the                                 streaming that is happening now it's the                                 model execution which would need to be                                 worked on to get the latency further                                 down but it's possible the model code                                 could be reused want no change to it but                                 lines of code reduce because we got rid                                 of all this boilerplate code just to                                 stitch multiple jobs together and it was                                 also with a reduction in the resources                                 that are needed                                 mr. AWS instances so now integrations                                 integrations means the pieces that help                                 you to get the data into your streaming                                 application and all                                 right at Howard sources and sinks in                                 beam so we are using Flinx connect us                                 even in beam they are using Flinx                                 connectors because we did the work once                                 to make those really work inside lift                                 and now we try to capitalize on that                                 effort so we have exposed the flink                                 Kinesis consumer also as a beam source                                 in our beam fork we are planning to do                                 the same for consumer and producer but                                 this is not only for beam this is for                                 also for all Java pipelines we use the                                 same set of connectors we need to read                                 from cough-cough we need to write to                                 Kafka we need to reach from s                                          to write to s                                                           elasticsearch we need to consume from                                 DynamoDB streams which is special flink                                 and is consumer and then very important                                 in the flink is checkpointing the check                                 pointed                                 state needs to be written somewhere                                 which is s                                                             had a lot of interesting experiences and                                 a lot of learning with writing                                 checkpoints to s                                                         like a filesystem to fling but it's not                                 a filesystem when it really works so                                 here are some of the challenges                                 generally connect us and flink are                                 pretty good but usually you hit some                                 bumps when it comes to certain                                 production readiness aspects we found                                 issues with observability which means                                 that the metrics and the locks are                                 helpful diagnosing issues also that we                                 are able to configure everything that we                                 need or the underlying client api                                 configuration parameters are exposed                                 sufficiently and then also performance                                 when you run it at scale you find things                                 that nobody has hit when you're the                                 first one so in the case of Kinesis i                                 think we were probably the first one                                 running it at that scale level AWS                                 integration is also a little bit                                 interesting because it behaves in                                 unexpected ways sometimes they are                                 transient service errors that are                                 bubbling up and if those are not handled                                 correctly with retries then it means                                 that your pipeline is interrupted every                                 time such a thing happens it will                                 basically fail recover from it                                 point that means in the mean time does                                 no processing which is of course not                                 what we want so what we want is                                 identified those retrial exceptions and                                 then just continue with a with a retry                                 and a small time out and dealing with s                                  is also interesting for checkpointing                                 with large state because if you hit the                                 same as three shot for many subtasks                                 then you get the hot spotting symptom                                 and there is a way to work around that                                 with modifying augmenting the file path                                 just to make it go over multiple shots                                 that changed went into the last link                                 release then also we need event time                                 processing it's really important and so                                 source water marking we added that to                                 the kindnesses consumer in flink the                                 kafka consumer has that already it                                 wasn't there and the Kinesis consumer                                 also that water marking is done                                 correctly while reading from multiple                                 shots merging data for multiple shots                                 the correct computation of the watermark                                 is actually can lead to surprising                                 effects if that's not done in the source                                 and watermarks q I'm going to say                                 something about that problem because                                 that's a seems to be a quite generic                                 issue also I said at the beginning that                                 was Kafka you can you can process a lot                                 of data very fast which is true now you                                 get the opposite issue with Kafka the                                 bandwidth so can be so high to an                                 individual consumer that the other                                 consumers get stopped and then you have                                 to think about rate controls on the                                 consumer so the bottom oxq is the issue                                 of the watermark of individual                                 partitions and Kafka or shots in Chinese                                 is not being Alliant that means one of                                 those in this example gets consumed or                                 gets consumed slower than the others and                                 so you can see the red data is still                                 being accumulated if this is a windowing                                 use case because the watermark has not                                 advanced but in partition two and three                                 we are also reading newer data so what                                 is happening here is that you are                                 accumulating state in the flink                                 application                                 and if that is large scale and that skew                                 becomes very large then it becomes a                                 problem because the checkpoint state                                 becomes so large the application might                                 come to a grinding halt on checkpoint                                 fail checkpointing and at some point not                                 pauses at all and then there's also no                                 way out of that because the consumer                                 offsets or checkpoint and even if you                                 restart you will still have the same                                 issue so it is important that there is                                 some synchronization mechanism and this                                 queue can be caused by variety of                                 factors it could be a difference in the                                 speed of the consumers it could be a                                 difference in the speed of the serving                                 system like kafka protocols or Kinesis                                 shots or it could also be the density of                                 the data so it's it's important to have                                 this ability to align so we implemented                                 this mechanism of based on a global                                 watermark that all subtasks can flink                                 all consumers are aware of by knowing                                 where everybody else is so the minimum                                 of all the watermarks consumers can                                 adjust the speed so consumer that runs                                 ahead and say hey I need to slow down                                 you know I need to pause I'm going to do                                 this until others have caught up so                                 there's a little bit of a stage sharing                                 mechanism that is now part of link that                                 we utilize this and then we have built                                 this into it the Kinesis consumer to                                 illustrate this on the left side you see                                 the skew the effect of the skew so in                                 the middle you see checkpoint size and                                 you see as the application is running                                 the checkpoint size is climbing and the                                 exact same execution on the right side                                 doesn't have that problem you see this                                 is fairly flat the right side is with                                 synchronization the left side is without                                 synchronization and when you look at the                                 iterator age in Kinesis you see that it                                 the band widens for for the processing                                 without synchronization because they are                                 being consumed at different speeds and                                 it over time it wide it widens and this                                 is a replay scenario where there are a                                 lot of data is read in a short amount of                                 time                                 and once it returns to the head read                                 then it's flat but on the right side you                                 can see that this band is narrow the                                 checkpoint size is has a ceiling and                                 that's what we want to see so that we                                 can also replay large amounts of data                                 without running into memory issues so we                                 contributed that the flink the next                                 release will have T synchronization in                                 the Kinesis consumer we will built it                                 also for the Kafka consumer it's a bit                                 more work because Kafka consumer of link                                 after consumer has a different consumer                                 model compared to Kinesis consumer but                                 in the future there's also flip                                          will provide a new framework for sources                                 in flink that will account for all of                                 these requirements have a correct event                                 time consumption                                 so now deployment for deployment we are                                 moving from our standalone flink                                 deployment on ec                                               kubernetes so everything it lifts will                                 run on kubernetes eventually which                                 provides more stability more flexibility                                 shorter wait times when it comes to                                 starting up a new fling deployment                                 because if Lync deployment is always two                                 things even if there's just one job                                 running on it it's always a fling                                 cluster and then a job that is running                                 in the fling cluster for us it mean                                 always means you're only interested in                                 one thing we don't want to run multiple                                 flink jobs in one flink cluster which                                 has other issues with isolation so the                                 we are building a flink operator that is                                 open source you can go and check it out                                 and maybe even contribute to it                                 kubernetes flink operator that                                 understands how to manage Lync                                 applications and supports its own                                 deployment descriptor that describes the                                 flink application so you can see an                                 example here this is very simple of                                 course but it's a custom resource                                 descriptor that is understood by the                                 flink Kinesis operator only which is                                 always active in the company this                                 cluster                                 it describes a single job the all the                                 applet                                 code is packaged in a docker image that                                 token which contains all dependencies                                 and we are using source to image                                 internally to produce this from there's                                 a base image and a user's add the                                 application code to it and to get an                                 application image that is then being                                 specified in the customer sauce                                 descriptor and any time this descriptor                                 changes the operator has to update so it                                 could be any change it could be a change                                 in the parallelism but it also could be                                 a change in the code and this operator                                 needs to understand how the update of                                 think job fling shop is different from a                                 micro service micro service you can just                                 change the replicas and everything will                                 happen in the right way if link job is a                                 stateful application so we need to do                                 some more work here there's an example                                 so the operator the kubernetes operator                                 will detect a change in the CID it will                                 now go into nap updating State and that                                 means it will have to take a safe point                                 because we want to carry over the state                                 from the current running application to                                 the new job that will be launched in                                 parallel it will create already the new                                 fling cluster which is the new set of                                 task manager and job manager processes                                 because we don't want to wait later                                 while it's safe finding it can already                                 create this new flink deployment when                                 the safe pointing completes then we have                                 everything that we needed to start the                                 new version of the job so the old one is                                 now canceled the safe point was written                                 the new fling shop comes up from that                                 safe point on the new set of processes                                 and so that would be a stateful update                                 the user should not be aware of the                                 different phases that this that is                                 contains we also currently have                                 deployment tooling that is doing similar                                 things but with the kubernetes operator                                 this is nicely encapsulated and more                                 predictable so finally last beam submit                                 is coming up in day after tomorrow                                 so if you want to learn more about beam                                 then please stop by and any of the beam                                 things that I quickly run through here                                 you will get great amount of detail                                 there's also a beam summit coming up in                                 North America                                 in September in Las Vegas at Apache con                                 or the                                                                 came over here from North America or                                 those that want to go there this might                                 be a good thing to bookmark and we have                                 a few minutes left for questions right                                 that's pink the speaker other questions                                 so I have a question regarding your                                 connectors you mentioned you might you                                 use fling connectors on top of beam how                                 do you enable developers like to test                                 and debug their code to do usually have                                 a locally instance running but you have                                 like another connector that they can use                                 with dileep rima wanna yeah this is a                                 good question so the way it is currently                                 works the team that is working with beam                                 they do indeed run the application they                                 have for unit testing they have a way to                                 stop this and replace it with something                                 else for example you can read from a                                 file file can also be like a stream and                                 you can read some test data so this way                                 you can test the logic without actually                                 having to have that connector that's one                                 way of doing it                                 beam gives you other options you can                                 replace you've got to learn more at beam                                 summit about it you can replace any                                 transform with something else of your                                 choice by just redefining how your end                                 gets translated that requires surgery on                                 the runner this is what we do when we                                 see is lift Kinesis consumer then we                                 know what to put in we plug in the flink                                 chineses consumer so for if you have a                                 logo that's an interesting path because                                 you can do pretty much anything but the                                 route forward for beam itself would be                                 cross language transform where you can                                 use you write your pipeline your                                 transformation code in Python and then                                 for sources you have the option to use                                 an existing beam Java IO their quotes                                 and there's an example that we will show                                 day after tomorrow for Kafka for reading                                 from Kafka and for writing to Kafka                                 there's no questions                                 I don't wanna talk oh yeah so you were                                 talking about how you want to unify have                                 the feature engineering for both serving                                 and training that sounded very much like                                 you're building a feature story you                                 didn't use that term and tf-x                                 which is kind of the as you know the the                                 layer on top of beam link for building                                 machine learning pipelines doesn't have                                 a feature store can you say anything                                 about your roadmap and what way you                                 think it's going is it going the                                 features or way or that P FX pipeline                                 way well we are not doing that because                                 you're already doing it now I'm just                                 joking so we do have a feature service                                 which is a separate system and lift                                 so the flink jobs that are running there                                 and using sequel as a specification they                                 will eventually output also one of the                                 things is actually feature store well                                 these things are being stored and then                                 consumed but you're right I mean this                                 could be an interesting path in the                                 future to do something like this based                                 on existing tools but this was created                                 years ago and this current feature                                 service we call it internally probably                                 at some point it will be touched when                                 the systems that are coming up or made                                 sure enough yeah more questions there's                                 money in the center                                 I'm about the analytics parts you said                                 some people can write Taipan only using                                 sequel so that's great because more                                 people can write pipelines do you have                                 any safeguards in case they don't fully                                 understand what's being behind or it                                 just like the trap is going to be super                                 long so they're going to figure out it's                                 not the ideal way well it's really the                                 question is whether you can express what                                 you want to do with sequel really then                                 that's rather that's basically                                 independent of fleeing can i express the                                 computation that I want to do there with                                 a sequel and in many cases the answer is                                 yes at least for us what what was                                 written previously was Python for                                 example doing just simple aggregations                                 summations with no special trigger                                 conditions and so on all of that can be                                 expressed with sequel and most of the                                 flink jobs are actually that we have at                                 live today or in that category and the                                 cloister is much faster because when you                                 write a full-fledged beam with beam                                 that's the case too but a blink Java job                                 there's a quite a hurdle you have to                                 have somebody very familiar with Java in                                 flink first of all and also you have to                                 have somebody who understands the                                 deployment when you open this wide range                                 of options how you write the code and                                 what you have access to the flipside of                                 that is that there are many more things                                 that can go wrong and so somebody has to                                 support that so yes I mean the use cases                                 that can be served to sequel or many and                                 the ones where you want to go down the                                 programming route or hopefully few                                 because it requires way more work and                                 time all right very good                                 don't know that's I think the speak                                 again                                 thank you                                 [Applause]
YouTube URL: https://www.youtube.com/watch?v=2R0RXRH2eD4


