Title: Berlin Buzzwords 2019: Niels Basjes – How to handle 100k events per second #bbuzz
Publication date: 2019-06-20
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	In the last decades many systems have been used that were described as "queues" (AQ, ActiveMQ, RabbitMQ, etc.), yet from a computer science perspective these are not queues at all. Many of us have learned to work quite effectively with these messaging systems and we all understand that we cannot expect to receive the messages in any particular order and that we get all messages exactly once (which we can expect with a queue). With the arrival of Apache Kafka and Flink a new class of applications became possible. 

In this talk I will go into several real applications from the bol.com context that all revolve around low latency behavioral analytics. I will talk about the entire end-to-end pipeline from the webbrowser and application server to application and discuss many of the things to think about when creating your analysis application. I will also touch upon using state machines as a way of doing this type of behavioral analysis using very simple software and show example algorithms from our context.

Read more:
https://2019.berlinbuzzwords.de/19/session/measuring-20-how-handle-100k-events-second

About Niels Basjes:
https://2019.berlinbuzzwords.de/users/niels-basjes

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              hi good afternoon so the reason I'm not                               on the schedule is because yesterday I                               found out somebody dropped out and I was                               asked to talk so I'm going to talk about                               measuring                                                            we have at Bolcom in this talk I'm going                               to talk a bit about the context why we                               have this project it's about measuring                               interactions and intere in essence the                                whole chain of using that data I have a                                background in both computer science and                                business and have been doing software                                development algorithm research and doing                                open source software for a long time and                                actually Isabel mentioned me this                                morning in her keynote I have talked                                about this project before at Borland                                buzzwords so you can look it up on                                YouTube and my colleague Yvonne also                                talked about it when we were just in                                production because we're in Germany I                                assume only fusion you know what Bolcom                                is well                                simply put comparable to Amazon we're an                                online marketplace where lots of                                companies sell stuff we have over                                   million available products things like                                chrome costs and we have a lot of                                additional information on what we put on                                the page and that's exactly where the                                problem starts the key requirement we                                have is that we want to measure what is                                happening on the page                                what are we showing why are we showing                                it and how are people responding and the                                use cases are all over the place we want                                to do straight on dashboards but also                                personalization site improvement data                                science for all kinds of purposes and                                fraud prevention and the main question                                I've been asked is why did you start in                                new projects we already have Google                                Analytics on mature and stuff like that                                the main reason is data quality and                                details so if you look at a page most of                                these JavaScript based solutions like                                Omniture and Google they measure the                                page                                and that's it and what we want is we                                want to know exactly all of the parts we                                put on the page and why we put them                                there now it's not uncommon that our                                website does three four thousand pages                                per second and if we measure everything                                on all pages should yield something like                                                                                                      which is about                                                        project has been partially implemented                                not all aspects of the website have been                                tagged yet but around this time we're                                already part passing the                                          measurements per day now the alternative                                systems like Omniture and Google                                Analytics are all JavaScript based and                                in my opinion they are broken                                fundamentally broken primarily because                                you're measuring a side effect you're                                not measuring the page you're measuring                                the effect of a script that is on the                                page and that means that stuff goes                                missing stuff goes duplicate and in fact                                the measuring of orders is on the order                                confirmation page which is a side effect                                of a side effect these tags are                                blockable ITP that is built into the                                Safari browsers and into Firefox ad                                blockers robots hackers all those who                                things cannot be measured using                                javascript and many of those are boxed                                so in all mature you have something                                called an Ivar and that's a variable you                                can put in but they're fixed in number                                so you always have not enough of them                                and in Google Analytics we at some point                                saw this you know number of users and                                then we saw a bump and then a systematic                                uplift so I started digging what                                happened there turns out that if you                                zoom in by the hour we are loading                                additional labels that came out of our                                data science to tag users the tagging of                                the users actually changed the                                measurements in number of visitors or                                users on the site                                so in my opinion javascript-based                                measurements are something like this                                then people still say we're data-driven                                and I go like yes seriously an                                additional problem of the JavaScript                                 measurements is that in general for                                 doing kind of personalization they're                                 too old so because they're loaded every                                                                                                        at this product maybe these are                                 interesting you know that's that's not                                 what I want and the key thing here is                                 that the data quality the value of the                                 day that drops rapidly by time and for                                 those situations where you want to                                 personalize the website you really want                                 to be here in the low seconds area to be                                 up to speed with a visitor at on the                                 side now do note that not all systems                                 require such a low latency if you're                                 optimizing if you're doing analysis for                                 future visitors like building a                                 recommender data set batch is fine but                                 if you're doing something that analyzes                                 the behavior of the visitor on the site                                 you want to be as fast as possible so                                 the project supports not only the very                                 old-style Hadoop type of processing but                                 we also ship the data into bigquery                                 because we use both Hadoop and the                                 Google cloud and we want to support                                 stream processing things like flink or                                 beam so the project m                                                    making better measurements to allow                                 better processing and make the site and                                 all the personalization applications                                 more relevant and of course like I said                                 we want to measure everything all                                 interactions all visitors even hackers                                 in Google bot make the site data the                                 measurements more reliable and reducing                                 the load on the client because although                                 javascript is really heavy and drop the                                 latency basically we want everything and                                 of course we want it easier for                                 developer to build because that's nasty                                 the business or data science people they                                 want to ask different questions they                                 didn't know beforehand and we took into                                 account the whole privacy and security                                 discussions which at one and says no                                 long-term profiling no security problems                                 and the business sells it needs                                 long-term profile which is a bit of a                                 catch but we figure that out simply put                                 the goal is to make the best possible                                 interaction data stream knowing what                                 happened on the website and to do that                                 we had to make a lot of choices over the                                 years let's start with the start of the                                 flow measuring the fundamental choice we                                 made is that we measure where it happens                                 so where we are absolutely sure the                                 thing happened that the user did or that                                 we did that's where we do it that means                                 usually that most of the measurements                                 are done in some kind of front-end close                                 to the user surface either the web shop                                 or the API that services our app or very                                 close to a system that owns the event                                 that actually happens like our basket                                 server or ordering service it's                                 important to realize that usually this                                 means we are not measuring these things                                 in the browser because in the browser                                 it's always a side effect now for                                 example measuring a page in the data                                 center in our servers we measure what we                                 actually put in the page we do have                                 measurements at the browser end but                                 those are things that actually happen in                                 the browser which part was actually in                                 view and what was the screen resolution                                 for example orders are now measured in                                 our ordering service and the order                                 confirmation page that in the JavaScript                                 based solutions are classified as the                                 order are now just classified as the                                 viewing of an order that was just placed                                 so this is not anymore the order but                                 viewing of an existing order what we                                 also do and that is has to do                                 with the volume and denormalizing                                 everything is that when the event occurs                                 we record as many attributes as we can                                 so we record the product number things                                 like the product type the title etc at                                 the moment of creating the page also if                                 we have an offer from a seller we record                                 the price in the condition and seller ID                                 now people have said to me that yeah but                                 you only need the offer ID because all                                 of these others can be joined later well                                 there are two very important reasons why                                 you can't our webshop our ordering                                 system has some caching built in so even                                 though the cache may be seconds even                                 within that second you can have a                                 situation where you are saying I showed                                 you five euros when the reality was it                                 was six euros because you were still                                 looking at a slightly older price now                                 looking at the general pattern of what                                 we want to do with the data in our                                 situation is cause and effect if you                                 don't have cause and effect if all the                                 measurements are single things that are                                 completely independent and unrelated                                 it's easy processing is easy and none of                                 the things that I'm going to show you                                 matter but our use cases are things like                                 banner optimization or a bee testing or                                 search suggestions or attribution                                 modeling and in all of these cases it's                                 we do something we show something and                                 then the user either responds or doesn't                                 so it's always a cause and effect an                                 action and a reaction and it's that                                 causality that we're really interested                                 in and in those chains of causality                                 ordering of events matters so if you                                 click a banner and then buy a product                                 then the banner is most likely part of                                 the reason why you bought it if the                                 order of these two events is reverse it                                 was definitely not a reason for buying                                 it so the was or was not is based on the                                 order of the events                                 now nice thing about having ordered                                 events is that analyzing them figuring                                 out what happened can be done with                                 something like a finite state machine                                 which in general is a very very suitable                                 for low latency analysis and finding of                                 specific patterns in streams in general                                 I have found that you need to go one                                 step more advanced and that is a                                 pushdown automata Moton                                 which is essentially a state machine                                 with a little bit of memory and I'll                                 show you an example later on so why is                                 this event ordering so important well                                 let's assume I have an IOT situation so                                 not a webshop at an IOT situation where                                 I'm measuring the temperature of                                 something and a fast temperature change                                 is dangerous it may explode or something                                 and if that happens I want to be alerted                                 immediately and I do that by                                 implementing a very simple state machine                                 that calculates the delta from the                                 previous measurement a very simple push                                 down automata automaton and the memory                                 is just the current the previous                                 measurement so I take the Delta from                                 these two and if it stays within bounds                                 all is fine but if I introduce ordering                                 problems and all I did here is reshuffle                                 the orders you get lots and lots of                                 problems you get false positives and you                                 get false negatives now do you realize                                 that repairing event ordering is hard it                                 is needless complexity but also it takes                                 time because the general pattern you do                                 when you want to repair ordering in an                                 event stream is that you create a                                 time-based buffer of the estimated                                 maximum out of ordinates and that slides                                 over time with your events and then                                 within that buffer you really reshuffle                                 them and when you're certain enough or                                 an event is ordered correctly you output                                 it to the next step but that can be                                 several minutes later and that is too                                 long                                 for a use case in our use case we would                                 also really like to have exactly once                                 also their deduplication knowing what                                 happened before is pretty hard to                                 maintain and and hard to build and in                                 our volumes would mean a pretty large                                 memory buffer - to guarantee that so                                 simply put we need ordering guarantees                                 per session because the event stream                                 we're looking at is a session a single                                 visitor that visits the website and                                 that's the cause and effect relationship                                 we're looking at the only way you can do                                 that if you can guarantee end-to-end                                 ordering so from where the user does                                 something you measure it you transport                                 the data into your processing stack and                                 the first phases of the processing must                                 be able to support these ordering                                 guarantees now the measuring point I                                 strongly recommend using a single entity                                 and then using a single measurement                                 system for that single entity because if                                 you have multiple instances for example                                 you do a round-robin load balancing each                                 instance will have a measuring output                                 buffer that will retain measurements for                                 a short while whatever that time is and                                 because they will be flushed independent                                 of the actual events you will get race                                 conditions and does ordering problems so                                 in IOT you see something like this you                                 know you have a measuring device and a                                 sensor that are tightly coupled and                                 eyeball become we are saying one visitor                                 should be on one single instance of our                                 webshop we have a lot of them but you                                 know so we have a session routing is for                                 the data quality and must-have now do                                 you note that there that it is not                                 perfect but the impact of this                                 imperfection is negligible because it                                 only affects the view measurements that                                 are which parts of the page are actually                                 shown on the screen and the orders so                                 now we have                                 neatly ordered measurements and we need                                 to transport them transport them in such                                 a way that our processing stack can                                 handle them so we need ordering first-in                                 first-out people call that a queue or in                                 some cases I would call it a partition                                 queue and then you would pin a specific                                 session to a specific partition to                                 maintain the ordering so Wikipedia says                                 quite nicely the entities are kept in                                 order that's what the queue defines and                                 and what I call partition queue is                                 essentially a single thing that collects                                 a couple of queues together the big                                 problem in our IT land is that there are                                 so many systems that call themselves                                 queue that are not JMS SS subsea thing                                 called queue which does not maintain                                 order there is a thing called active MQ                                 which does not maintain order                                 google has pops-up they have an                                 elaborate marketing page explaining that                                 you do not need order because they can't                                 can't deliver it within Bolcom while ago                                 people's implemented a new queueing                                 system they call a human system but                                 people ran into that it doesn't maintain                                 order now luckily there are a couple of                                 systems that do maintain order and the                                 most well-known that we also use in                                 production is apache Kafka                                 but do you note that there is also a                                 patchy pulsar that sports that maintains                                 ordering few months ago I was made aware                                 of a pretty new project called Pro Vega                                 it's not ready yet it's not a patchy                                 light also but it is Apache License                                 which also is set to maintain ordering                                 but I haven't tried it yet now we as a                                 company are doing data center and Google                                 Cloud amazon has kinases which is the                                 wrong clouds                                 Microsoft has event ups which is the                                 wrong cloud so I'm telling my colleagues                                 ok go ahead and use Google pops up                                 because                                 it's a good system as long as you                                 remember that the ordering is messed up                                 and yet you get at least once so some                                 events come in twice treat it as a high                                 io distributed set and you're fine if                                 you need ordering use Apache Kafka                                 because then you have something where                                 ordering is maintained and in                                 combination with Apache flink you can                                 guarantee exactly once for your                                 processing Kafka in my words is an high                                 i/o partition queue so now we have the                                 data at our processing and how do we                                 process well we have of course the same                                 requirements for the processing stack                                 low latency exactly once                                 ordering guarantees and support for a                                 push them down automaton per session you                                 know Keit stateful processing where the                                 key would then be something like a                                 session ID we had a good look at Apache                                 beam primarily because it runs natively                                 in the Google pod it's essentially the                                 open source version of the Google API it                                 does support low latency except it does                                 at least once exactly once is done by                                 additional deduplication and there are                                 no ordering guarantees and there's no                                 natural key stateful processing and the                                 primary reason for that is that in my                                 opinion all of the Google tools have a                                 very clear requirement and that is that                                 they support dynamic scaling when you                                 increase the load the system can                                 automatically use more resources if you                                 decrease the load it can scale down                                 again this dynamic behavior is as far as                                 I can tell the primary reason why these                                 features are not there in addition to                                 that I personally have a dislike for the                                 Java API but that's a personal thing the                                 nice thing about bean bill is that it                                 runs both on dataflow and on other                                 execution engines like flink so you can                                 run a beam job on fling on a Hadoop                                 stack I've tried that actually works                                 now my preference is Apache fling for                                 this type of processing because fling                                 internally requires ordering guarantees                                 and low latency and that has all to do                                 with that it does the exactly ones                                 processing by the chain D Lamport                                 checkpointing system so internally it                                 has the low lay to see exactly once with                                 ordering guarantees and as a consequence                                 something like keyed stateful processing                                 is just there and the recovery of heat                                 stateful processing with a recover in a                                 disaster scenario also works but you get                                 the quote unquote fixed scaling they're                                 working on making that more dynamic but                                 still it the underlying design model is                                 there the java api is in my opinion much                                 better if you say i have a data stream                                 and then i do a key by you get a key to                                 data stream if you say I'm doing a                                 window on the data stream you get a                                 window to data stream so that makes                                 makes it for the developer a lot easier                                 and it runs on both the Hadoop stack and                                 on cubed Andy's colleagues of mine are                                 running a fling in the Google cloud                                 works fine so now we have the whole                                 stack from measuring to processing and                                 everything in between and then something                                 changes people have improved insights                                 new business models new wishes things go                                 away so the records we get new fields                                 fields get dropped etc and in general                                 you see that a streaming scenario you                                 have a system that produce data you have                                 a streaming interface like Kafka but                                 essentially all of them need a byte                                 array as their payload and you remember                                 that in addition to the adding of new                                 fields and new features you have                                 multiple applications you do rolling                                 upgrades you don't do canary releases so                                 the reality is that you have the same at                                 the consumer                                 and the reality is that at every moment                                 in time your topics will contain a                                 mixture of versions of the data and so                                 we need something that supports that we                                 need something that allows us to define                                 a record to fill in the fields convert                                 that into a set of bytes and back the                                 things that support data types data                                 structures and bi-directional schema                                 evolution two ways now being a PMC for                                 any committer for Apache Avro I had a                                 look at what people around the world are                                 already doing and we decided to just put                                 it into Avro as a part of the stack so                                 you can now do a schema definition in                                 for example the IDL the form it looks                                 like this it's actually quite easy to do                                 and then the code generation creates                                 codes with builders and getters and                                 setters that allows filling in the data                                 structures and finding out what is                                 mandatory and what not very easy it also                                 creates Javadoc that includes the                                 comments that you put in there as part                                 of the Javadoc for the downstream                                 consumers to read so it also makes                                 documenting the fields for the consumers                                 easier in order to do this I added the                                 Avro message format which is essentially                                 a serialization of a single records into                                 a bunch of bytes and it was designed for                                 this use guys or this type of use case                                 there is however one thing you really                                 need and that is a schema database of                                 some sort a very simple key value thing                                                                                                        fingerprint of the schema and the string                                 representation of the schema which is in                                 a JSON format if I then use this to fill                                 in data to shove it into Kafka from                                 Apache flink I first of all need to                                 create something called a serializer for                                 that data type which has a method and                                 you could do something like person to                                 byte buffer and then pull out the                                 the actual bytearray I was recently told                                 that this is the wrong call to do that                                 so be aware don't copy it directly and                                 then in your your application you have a                                 data stream of the generated class and                                 then you say hey add a sink for Kafka                                 and use that serialize ER and from there                                 the data your records will be serialized                                 in the correct format shift into Avro a                                 shift into Kafka and then you can                                 consume them and the consume code also                                 has something in the serialization area                                 but now it's a DC réaliser                                 which is one additional step because the                                 the message decoder from Avro for your                                 class needs to be able to retrieve the                                 fingerprint from your data store if it                                 receives a message for which it doesn't                                 have the schema yet if once it retrieves                                 it it caches it and keeps it in memory                                 in a compiled form so it's really                                 efficient and then you create the D                                 serialization method that just does this                                 and then your application you say hey I                                 want to get a data stream from person                                 from the source in Flynn Kafka with that                                 D serializer and in the event that D                                 serialization problem occurs you get                                 garbage you get a nil value back so be                                 sure to drop the nil values otherwise                                 the rest of the chain may fail now to                                 give you a bit of an overview of what                                 the project looks like today we have our                                 web shop where we built the HTML there                                 we have included a library attached to                                 that that creates the measurements and                                 puts them serializes them and puts them                                 in Kafka that is in our webshop then the                                 HTML is tagged with IDs and those get                                 put in the HTML and the browser then                                 sends the in view measurements to                                 separate endpoints that also puts them                                 in Kafka things like orders are from our                                 checkout system into our ordering system                                 and those are also firing measurement                                 endpoint into Kafka so there we have a                                 very complete stream of what is actually                                 being done by our visitor                                 the next step is something we call the                                 session Iser because it detects the                                 visit pattern as in after                                            being idle you're attached a new visit                                 ID which makes sense to make things more                                 palatable we add geoip information so we                                 know the country and the ISP or came                                 from and we use the user agent analyzer                                 so we can pull out all the fields from                                 the user agent that we're really                                 interested in and drop the actual user                                 agent string because the it's so unique                                 it's a PII thing now that data is shoved                                 into Kafka files and in bigquery for                                 analysis by our people but because this                                 is a full detail with everything in it                                 it's a PII think so only our security                                 and fraud teams have access to this to                                 allow personalization we have added an                                 anonymized step that drops a lot of                                 fields keeps the customer number so we                                 can still recognize the person on the                                 website but only the number no not too                                 many other things and that is available                                 for all kinds of personalization                                 campaigning etc so now that we have a                                 bit of an overview of the entire stack                                 how about showing a way of using the                                 data the way it was intended now this is                                 just a sketch of how it can be used we                                 have a search suggestion application on                                 our website and was originally written                                 by me about                                                            yes that long ago and essentially what                                 we're saying is that if you're doing a                                 search and right after that you're going                                 to a product page then we say                                 essentially an attribution model this                                 page is caused by this search term and                                 if you then do an add to cart and a                                 purchase hey those are all caused by                                 that search term because then we can say                                 hey the further you go the more valuable                                 it is the more relevant it is now at an                                 high level                                 full m                                                                   do some stateful analysis you know is                                 this a valuable event how valuable is                                 this event and then if it's relevant                                 ship out something relevant and then do                                 a very simple scoring and aggregating to                                 convert the search term into a                                 suggestion and then store it somewhere                                 where the website can consume it again                                 like I said before ordering is important                                 up to a certain point and that's about                                 here in this example after that the                                 ordering of the events is not that                                 relevant anymore you just put them in                                 the database and if one overtakes the                                 other it's it's not much of a thing                                 because you're just adding scores                                 together so how would such a state                                 machine how could that look well                                 actually it's a pushdown automaton                                 because I need to remember the search                                 term so I have an initial state where                                 I'm essentially not doing search and                                 then if I do a search for Forex I go                                 into the state searched and I'm                                 outputting an event that okay apparently                                 a search something was searched for X                                 then I go to the product page and was                                 found I owe a product page caused by X                                 now our product page is an overview of                                 recommendations of alternative prices                                 etc so you can argue that if you stay                                 within that you stay within found and                                 after that if you do an Add to Cart you                                 know a we have an Add to Cart for the                                 word X if in any of these stages you do                                 something different you go back to the                                 start and we forget X so using this it                                 should be possible to build a very very                                 low latency search suggesting                                 application and make the site more                                 relevant now this is just an example we                                 see a lot of situations where this may                                 be useful now if this is the kind of                                 thing that you really like these are a                                 few books I highly recommend                                 I think I saw the author of this one in                                 the audience                                 yep he's there so and like I said if                                 you're interested in these are highly                                 recommended especially this one because                                 it goes into the foundations we're                                 always looking for good people willing                                 to do things like this doing machine                                 learning on this type of thing we have a                                 website on that and I assume you have                                 questions put your hand up if you have a                                 question                                 guess I'm curious how you deal with                                 tabbed browsing in cases where the exact                                 ordering events doesn't necessarily                                 indicate they're like causal ordering so                                 for example if a pager user has two                                 pages open clicks on one of them it does                                 something then goes back to the other                                 one and does something yeah then it                                 might be that the intermediate clicks                                 don't really have any causal ordering                                 but yes yeah you're right one of the                                 things we've designed into the data                                 structure is that every event get a                                 globally and for every unique ID and the                                 next event gets the idea of what caused                                 it as the cause ID so although we                                 haven't built that and it's clearly not                                 shown in my simplified example you can                                 see what was the previous what was the                                 cause of the event you're seeing now so                                 if you maintain a couple of events in                                 your state you can actually detect this                                 but it will take quite a bit of memory                                 to do that reliably for all visitors and                                 then it becomes also a trade-off                                 okay how many people actually do that                                 and how bad is it to be wrong for this                                 use case but yeah we actually thought                                 about that yes after ordering is done                                 what sort of algorithms do you use to                                 find relevant events sorry I couldn't                                 understand question after the ordering                                 is done like what what sort of                                 algorithms do you use to find the                                 relevant events like when once the                                 ordering is done once the when people                                 order something yeah what sort of like                                 relevant relevant algorithms to use for                                 oh that depends on what we want to do                                 with it you know if we'd want to do                                 attribution modeling for advertising we                                 use different models than for example                                 the attribution of search keywords so                                 that really varies per use case and it's                                 up to the team because we have a scrum                                 organization so every team that                                 needs to build something of course they                                 talked to each other about how do you                                 approach this and can I reduce reuse                                 stuff                                 it varies and a lot of lot of situations                                 is a very simple rule based thing                                 because those usually perform better                                 right and the second question is that                                 you said during these relevants such you                                 the ordering does not matter if like I                                 mean even even then like let me check I                                 think you mean this one yes exactly                                 because what rolls out here is                                 essentially that this word was seen on                                 the PDP on the product page and from                                 there on it's just a time assigning a                                 score and figuring out for which letters                                 this is a recommendation and then all                                 that happens here is that in the                                 existing set these are incremented and                                 if two measurements for the same thing                                 swap places the effect is negligible in                                 fact I'm even willing to say that while                                 here you are doing in flink terms stream                                 processing based on the time in the                                 events here you can do it in the time of                                 processing and just win do it together                                 for a few seconds and then batch it in                                 in the data in the data store there                                 every few seconds or a minute or                                 something like that                                 we good yes yeah so so like the key                                 themed I think you emphasize this                                 maintaining order in events and I think                                 one of the things that you brought up is                                 that in order to do that you have to                                 route sessions to the same node in order                                 to have a single clock right it's right                                 the events or our captains and also                                 single output buffer that's actually the                                 bigger impact right okay I'm not sure I                                 completely understood that so maybe you                                 can expand than that but but the other                                 quiet but the question is really what's                                 the cost of do see is it difficult to do                                 the routing I mean you have issues right                                 when you want to roll out updates to                                 your servers okay you'd like to be able                                 to send someone to a new node Onegin yes                                 yeah in those scenarios you will                                 undoubtedly have some damage to the data                                 one of the things we're doing there is                                 that we are using a small number of                                 instances and do session replication                                 within the small number of instances and                                 then switch the user to the other two                                 another one                                 and because it's a small number you can                                 still have the session replication and                                 you know don't break stuff too much but                                 in general yes you would there's a high                                 probability in those scenarios that you                                 break stuff but we try to keep it as                                 limited as possible because that makes                                 the assumptions you can make in the                                 downstream processing a lot easier and a                                 lot time layer and go much lower in your                                 latency then it's possibly if you do if                                 you don't do this my question is how do                                 you handle scaling so for example on                                 Black Friday do you maintain always                                 fixed amount of for example clusters for                                 Kafka or do you scale them like                                 dynamically well Kafka is similar to                                 what the approach if link has is stuck                                 in a fixed number of parallelism so we                                 have created such a number of partitions                                 in Kafka that we can handle the Black                                 Friday                                 and it works yeah                                 any more questions                                 No so could everyone give a copy of the                                 hands for Niels in his great                                 presentation
YouTube URL: https://www.youtube.com/watch?v=xY-pdiIrSCA


