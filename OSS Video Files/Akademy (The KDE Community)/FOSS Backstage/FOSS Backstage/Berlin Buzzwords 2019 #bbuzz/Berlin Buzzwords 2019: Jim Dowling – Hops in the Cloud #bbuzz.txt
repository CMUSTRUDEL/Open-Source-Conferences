Title: Berlin Buzzwords 2019: Jim Dowling â€“ Hops in the Cloud #bbuzz
Publication date: 2019-06-28
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	Hops is a European open-source, next-generation distribution of Apache Hadoop that is being repurposed for the cloud. In this talk, we will walk through some of recent technical developments in Hops, including solving the small files problem by stuffing them in metadata using NVMe disks, free-text search of file system with extended metadata (this is great for automated annotation of millions of images and then finding them in milliseconds with consistent), and most interestingly data-center level HA for HopsFS with millions of filesystem operations per second on real industrial workloads. 

So yes, we will tell you why a POSIX-style hierarchical filesystem with indexed extensible metadata is superior to an object store. Finally, we can show you what else you can do with Hops, and how we built Hopsworks, a horizontally scalable secure platform for Data and AI, using Hops' extended metadata.

Read more:
https://2019.berlinbuzzwords.de/19/session/hops-cloud

About Jim Dowling:
https://2019.berlinbuzzwords.de/users/jim-dowling

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              so my name is Jim I'm coming from stock                               and I represent a company                               and we build an open-source                               called hops or hops works and let's go                               get going so you may or may not have                               heard of hops works                               I'm just curious show of hands how many                               have heard of hops a few okay so we're                               European firstly and we started out life                                as a next generation of distribution of                                Hadoop so we actually made Hadoop and in                                particular HDFS                                                       not work with Spotify and we won some                                prizes for us and because we were so                                good at Hadoop and we'd built up a team                                we added GPUs to Hadoop before anybody                                else did and we've worked on the file                                system since then we've added support                                for small files in the metadata layer                                using nvme disks and at the end of last                                year we released the world's first                                feature store for machine learning it's                                a data warehouse for machine learning                                features so we've done lots of new cool                                things we come from a research                                background but we're a company in our                                vc-backed company and I guess the big                                thing here is that you know we're not in                                the cloud this is Hadoop this is                                primarily on-premise but I'm gonna talk                                today a bit about the next part of our                                journey it's actually a world first it's                                the first hierarchical file system that                                is multi data center h-a that means you                                can run a POSIX like file system in the                                cloud across availability zones and you                                don't necessarily need to make                                compromises in terms of using an object                                store but it's gonna be a bit of a                                journey to get there now firstly I just                                tell you a bit about the platform we                                don't actually sell hops as a platform                                hops was our Hadoop distribution our                                marketing people say do not mention                                Hadoop you are not allowed mention                                Hadoop it's not cool so what I'll talk                                about are properties of the front of our                                platform that are unique so nobody else                                has them in an open source platform so                                the first one is that if you have                                sensitive data and you want to put that                                data into a cluster let's say a Hadoop                                cluster you cannot prevent people from                                reading it and maybe writing it to some                                other place in the cluster so you can't                                sandbox your data allow people to                                process it in place but in our platform                                you can you can do that with something                                called projects it's a new abstraction                                we'll see later on that we need                                distributed metadata                                and TLS certificates to do that the                                second thing is if you want to have a                                platform today that's on premise that                                has more GPUs than run on a single host                                ours is the only platform globally to do                                that on if you want to have Python per                                project so each project that you have                                you'd like to have different versions of                                scikit-learn or tensorflow you can do                                that the feature store I'll talk a bit                                about that later                                we're the only enterprise features store                                out there you can write your code as                                Jupiter notebooks and run them as jobs                                orchestrated by airflow so you can write                                full pipelines in Python if you if                                that's your language of choice which                                isn't mostly here but I talk a lot to                                pipe some people and they like that                                and another thing that we do is free                                text search you can you can search for                                any file in the file system and you can                                have hundreds of millions of files and                                find them in less than a second or                                directories or tags on those files so                                this is something you're probably used                                to at Dropbox and think it's normal                                right but try doing that in s                                       it's not gonna work and then finally                                we're the only only distributed file                                system to store small files in the                                metadata layer with nvme discs that's                                some of our unique things that we do and                                you know we know we're not that                                well-known we're European so we're                                trying to get the message out there so I                                thought to get the message out there I'd                                say well what can you do on your                                 platform that's kind of cool so we have                                 a customer a large automotive customer                                 through a partner and what the partner                                 is doing with us is that we're trying to                                 take lots and lots of images do image                                 classification on them so you can think                                 self-driving cars but what kind of                                 platform would you need to do that well                                 in our platform you can take like a                                 million images insert them into the file                                 system in just seconds you can train                                 models on all of that data again in                                 minutes instead of hours by using                                 hundreds of GPUs if you have them you                                 can then run a spark job on the model                                 that you've trained and what you can do                                 is you can take all of the images the                                 files as they currently are and you can                                 annotate them by attaching a JSON object                                 to them in the file system and that JSON                                 object will magically appear over in                                 lastik and then you can search an                                 elastic and say please show me all of                                 the images here that have more than                                 three bicycles or two                                 hours or something like that and get                                 that response in sub-second now if                                 you're an ops person you say well I                                 could kind of do this I could put                                 together a key value store and a file                                 system and sort of elastic and hack it                                 all together and but we've done the hard                                 work for that we have the eventual                                 consistency protocols between the method                                 extended metadata and elastic and ops FS                                 and what that means is if you take your                                 million images and just remove the                                 directory all the metadata is cleaned up                                 it's all consistent if you're a data                                 scientist the thing that's cool for you                                 is that you can do all this in Python                                 you can write all this in Python and and                                 they like that okay so this sounds kind                                 of cool but the problem is the elephant                                 in the room is kubernetes like we are                                 not kubernetes today my answer to that                                 is kubernetes is just an implementation                                 details whether you're yarn or                                 kubernetes doesn't really matter but the                                 one thing about kubernetes that people                                 don't talk about is that if you're going                                 to the cloud where is the data everyone                                 assumes that data has to be basically in                                 s                                                                       contents of the top primarily canwe is                                 there is there future beyond s                                         some of my issues with s                                               is s                                                                   insert a file in the directory and then                                 list the directory the file may not be                                 there but Google Cloud we've actually                                 solved that problem but Amazon haven't                                 what's the same API so what is the                                 behavior of the s                                                     amazon says so that's one of the                                 challenges that I think everyone is                                 having going to the cloud are you going                                 to rewrite all our applications against                                 s                                                                    won't work                                 lots of applications assume implicitly                                 that you can do atomic renaming of files                                 that you can list the directory contents                                 the directory and a file will be there                                 so let's have a look we're gonna start                                 in a bit of a journey so I am a                                 professor also a kth associate professor                                 so I'll do a little bit of I'm gonna try                                 and convince you that this is a journey                                 that we will all make at some stage with                                 an analogy from databases so let's go                                 back in time all the way back where was                                 our data stored at the beginning well as                                 on punch cards and you know they had                                 MapReduce                                 it's actually true to process that data                                 I wasn't particularly fault-tolerant to                                 so it wasn't so cool and but magnetic                                 hard disks appeared in                                              problem with the magnetic hard disks at                                 that time was that you needed to know                                 when you wanted to read a file what part                                 of the disk was it on what sector was it                                 on and in fact the block size in your                                 file system was tightly coupled to the                                 sector so you would change the box size                                 if you're on a different sector of the                                 disk now this is you know it is alien to                                 us at this point in time we have fixed                                 block sizes now for our file systems and                                 but when the first databases came out                                 what you have to do is you have to say                                 well I want to read this record and you                                 would have to tell the data but if it's                                 a hierarchical and network database you                                 have to say go to this particular sector                                 of the disk on this cylinder and Vario                                 find the record and this was done for                                 efficiency if you didn't do it this way                                 you just wouldn't have an efficient                                 database so the nice thing was that when                                 sequel came out sequel was originally                                 the relational model introduced by card                                 but also the work done by Jim Gray on                                 transactions and indexing and with those                                 two were put together you now had a                                 sequel database which we know about                                 today so you would specify your queries                                 in a relational algebra and then the                                 system or platform by IBM showed how you                                 could take that abstract language sequel                                 and converted into efficient disk                                 accesses using indexes so now you didn't                                 have to go look through the disk and                                 know about where in the distal to find                                 your data the problem of course was that                                 that as data volumes grew these                                 databases just couldn't cope right so a                                 single service SQL database just                                 couldn't cope and we know what kind of                                 happened there at that point you know we                                 had this evolution where we start out                                 with called sequel and system or and we                                 still have those systems today but no                                 sequel is born but they need to scale we                                 need to scale these databases but we                                 have to compromise so what we have to do                                 is we have to basically give up notions                                 of consistent data we would we would                                 embrace eventual consistency I insert                                 something I read something it may not be                                 exactly what I think it was now the                                 application has to be rewritten to                                 handler                                 so many of you will know that in recent                                 years there's been a new class of                                 database systems that have basically                                 said hang on we can scale would still be                                 consistent and they're called new sequel                                 databases so spanner by Google is very                                 well known cockroach mem sequel I'd even                                 put in there and then NDB which is an                                 open source again European database                                 built by Ericsson originally now called                                 my sequel cluster it's part of my sequel                                 family but it's an in-memory distributed                                 database I actually worked on that team                                 so that's part of our of our product so                                 yeah that's the analogy right that we                                 went from single server we gave away                                 notions of consistency to scale and then                                 we went back to consistent databases                                 when we could solve that problem but in                                 file systems where are we today we had                                 POSIX is the father of api's and and                                 semantics for file systems you have this                                 POSIX standard you know you insert a                                 file into a directory your list it                                 should be there and we've managed to                                 scale distributed file systems to single                                 data centers quite well so you have NFS                                 HDFS is POSIX like they're you know                                 missing a little bit of politics but                                 mostly it's there but s                                                and s                                                                    basically to help us scale our file                                 systems to multiple data centers but we                                 gave up the notion of consistent                                 metadata we said we're good enough with                                 eventually consistent metadata they                                 probably know what's going to happen now                                 I'm gonna say well hang on we're gonna                                 can we go back to POSIX like or POSIX                                 file systems and still scale across data                                 centers and the answer is yes our file                                 system hops FS is a hierarchical file                                 system so it's suppose X like file                                 system with HTS API Google compute                                 Google Cloud store is going that way                                 they're adding bits because they're                                 building on spanner so they're using it                                 this consistent metadata layered spanner                                 to build at their file system and we're                                 using NDB is not consistent metadata                                 layer okay so why is strongly consistent                                 metadata for file systems important well                                 I mentioned already even certified you'd                                 like it to be there POSIX like semantics                                 are important applications expect this                                 behavior and if you try and port an                                 application from an                                 premise or a legacy application to the                                 cloud to work with s                                                    may encounter issues and then you end up                                 either rewriting your application or                                 looking to some file system that like                                 NFS or something like that that Amazon                                 has I think one code was called Amazon                                 file system I can remember the neighbors                                 the other issue is atomic rename which                                 if it seems like a very small thing it's                                 supported in HDFS but not in s                                         means all of the sequel databases you                                 know the the snowflake had to be                                 rewritten entirely to work with s                                      the hive's and the impalas of this world                                 and spark sequel for that matter didn't                                 work on s                                                             lot of work to make them rewrite them to                                 work with s                                                           work as as well and then finally when                                 you have strongly consistent metadata                                 there's something really cool you can do                                 that you may not be aware of which is                                 you can get a consistent changelog from                                 the database of what's happening in your                                 filesystem metadata so that means you                                 can you can stream that with some work                                 to a system like elastic and then you                                 can actually search for through your                                 entire file system and you can do that                                 in a consistent manner and the other                                 thing that we're doing for machine                                 learning is that you can basically also                                 capture all of the file system events so                                 if I write a PB file to a models data                                 set I kind of know it's it's a protocol                                 buffers file                                 it's a tensor flow model a train tensor                                 model so I can actually tag that and say                                 that's a tense flow model and I can say                                 ok it was run by this application and                                 this application read this particular                                 file that's that's a TF records file ok                                 well then then I can see now that this                                 model was trained by these TF records                                 and so on so the file system helps you                                 do that implicitly without you having to                                 explicitly rewrite higher-level                                 frameworks to work with data provenance                                 and that's what's happening in machine                                 learning we have T effects by Google we                                 have ml flow by data breaks the                                 rewriting the higher levels to put in                                 the data provenance but we're doing it                                 implicitly ok a little bit about the                                 file system it's gonna be a little I'm                                 not going to get too technical but just                                 that you know what top surface is if                                 there                                 file system we basically have data nodes                                 that store the the file data the block                                 data we have named notes and we have a                                 leader elected amongst them there's a                                 leader election algorithm vendor and                                 then we have this in memory database                                 back in for the metadata and we also can                                 store small files and nvme discs there                                 that's kind of the architecture nets                                 horizontally scalable at all layers and                                 that's fine but what we need to do is                                 make the top two layers data center AJ                                 that's what we've been working on and                                 going from working on a single data                                 center to working on multiple data                                 centers introduce a few problems so the                                 obvious things are that there's going to                                 be higher latency between the data                                 centers and then you have issues related                                 to you know network throughput bandwidth                                 and things like that so we had to redo                                 everything with the whole stack from the                                 database level all the way up to the                                 name nodes and this is a basic way it                                 looks when you're finished it can say                                 well we we can have this arbitrator node                                 in zone one it's kind of like zookeeper                                 you know make sure that if there's a                                 split brain which which side will win                                 and then if one of these data centers                                 goes down so on - or zone                                            system is still available and that's                                 great we did a lot of work on mitigating                                 for the fact that file system operations                                 may originate in this zone and go to                                 this zone so I might write read a file                                 across here and we did some                                 optimizations to get it                                     performance improvement so we introduced                                 data locality into the database and also                                 into the - the name node layer and                                 that's nice but what a lot of people                                 would run it is because they you know                                 you won't hate real h.a you just run a                                 almost erratic of the file system at                                 each each zone and they have tripled                                 replication and even if you know two                                 zones go down if you have enough                                 replicas of your database you need three                                 rapid your database then the thing will                                 still be highly available so I mentioned                                 already that the taking a consistent                                 change log of the file system is is is                                 is a fundamentally new and enabling                                 technology so if you're interested we                                 had a paper release recently at CC grid                                 on us and the overhead of introducing                                 this was about                                                         it's not not going to kill you and but                                 what we can do is we can take any                                 changes in the file system metadata and                                 push them to downstream systems so we're                                 already doing two of the most important                                 ones are elastic search for free text                                 search and the other one is hive so what                                 we've done is we put the metadata for                                 hive into the same distributed metadata                                 layer so now if you go to hive and                                 record say it from the file system and                                 you say remove this directory which is a                                 hive database hives cleaned up the                                 metadata cleaned up everything's cleaned                                 up we actually do most of that with                                 foreign keys but the hive has a few                                 tables that are kind of orphaned so                                 that's why we we take these cleaning                                 events and just clean a pipe if that                                 happens but this is fully extensible and                                 we will extend it over time to add                                 support to different systems                                 so to summarize kind of where we are at                                 the file system my view is that that                                 like POSIX as the Empire the ancient                                 empire is striking back right we're kind                                 of saying hang on s                                                    cut it what if I have a file system that                                 could be highly available across data                                 centers it can hit                                                     second on a Hadoop workload by Spotify                                 and this was done these experiments were                                 done on Google Cloud and you can                                 introduce nvme discs to store small                                 files and that will help with the HT of                                 a small files problem and all of this is                                 done with TLS security which is the real                                 killer                                 going from on prem if you go from                                 Kerberos to TLS that's going to kill you                                 so we do it with TLS and you have HTS                                 API this is ready you can use it now                                 today in the cloud but the big thought                                 is it's gonna cost you isn't it                                 I mean it's gonna cost a little bit more                                 like the estimates I read is that you                                 know it's four times more expensive to                                 have your data in HDFS than s                                           doing a parallel project right now that                                 we've started we don't have any good                                 results yet to put our block data in                                 test three so we'll have the combination                                 of the scalar metadata layer from how                                 suffice with the block data in in s                                      then                                 the cost will be comparative with s                                     that's kind of where we're going we're                                 not quite there yet but why do you think                                 hops is interesting because that's just                                 the kind of enabling layer at the bottom                                 so our company logical clocks we sell a                                 platform cold helps works it's open                                 source you can go and grab it and use it                                 but you know we're a vendor we provide                                 support and licensing and and all those                                 things and what we market it as we say                                 it's it's a a data intensive AI platform                                 right which is completely buzzword                                 compliance which is totally fine right                                 you have to have a buzzword and I hadn't                                 seen anyone else write data intensive a                                 and I so you cannot get a corner of the                                 market on that but what data intensive                                 AI means is that you know people think                                 AI is about resource allocation how to                                 get my GPUs and things like that but                                 really it's about managing data and how                                 do you manage the pipeline's of data                                 from your data lake or wherever it is                                 into training the models and that's the                                 challenge that I guess the industry is                                 facing as a whole so if you have many of                                 you will not have a machine learning                                 background that this this slide you may                                 not have seen before but if you have                                 done machine learning I apologize                                 because everyone shows us what this site                                 is from a Google paper but it got called                                 skully and some other co-authors it                                 basically says that what people think is                                 machine learning or deep learning is                                 taking some data training a model of                                 making predictions and if you do a                                 course in deep learning and there's a                                 course call fast it's really good fast                                 AI they give you                                 ready-made datasets you train them you                                 feel really good and go I'm an expert                                 now but when you go to the real world                                 your data is not in a nice clean CSV                                 file with no missing values it's it's                                 all over the place it's in you know it's                                 in your hive you're Cassandra your                                 objects store or wherever and you need                                 to integrate and pull that all together                                 you need to make sure that you can                                 collect the data you need to engineer it                                 you need to validate the data make sure                                 there's no missing values impute values                                 you need to do a lot of work in terms of                                 managing your data and that's where the                                 feature store comes in so the feature                                 store is an abstraction that we provide                                 to data scientists and you say they just                                 go to the features store and say                                 I want these features give me some                                 training data and they can assume that                                 the data that comes out is clean and                                 it's usable in their models and on the                                 other side the data engineers need to                                 make sure that the features that are in                                 the feature store do fulfill those                                 requirements so we provide tooling for                                 through the talk last night at the if                                 any went to the after afters about                                 Amazon of a framework for doing this                                 called that we actually use and we also                                 supporting tf-x                                 which is support for some of that but                                 the feature stores you're the only                                 vendor who provide this on the other                                 side once you have your training data                                 data scientists want to use lots of GPUs                                 to train the models distributed training                                 they want to check lots of hyper                                 parameters again using lots of GPUs they                                 want to write pipelines to run this                                 stuff and they need to serve their                                 models so we provide api's not just in                                 general rest but Python api's and even                                 Scala if you guys to do some of these                                 things so most of the Skylar stuff is on                                 the left and we have Python on the left                                 as well or Java and then on the right                                 mostly it's Python is one way of looking                                 at it so the platform hops works if                                 you're to kind of just break it down                                 it's a platform it's not a product so                                 you know Hadoop vendors release products                                 which are                                             ours is a REST API so it's a platform                                 with a single REST API and you can do                                 scalable deep learning and pipelines on                                 it and to do that if you want to do it                                 on more than one machine you'll need to                                 support some form of data parallel                                 processing so we support both fashion                                 streaming and we support distributed                                 machine learning and also serving of                                 models so in the one platform you can do                                 all these things and that's really the                                 whole kind of cycle of machine learning                                 pipelines so most of you will be                                 familiar with ETL pipelines which is                                 kinda classic data engineering concept                                 machine learning pipelines are pretty                                 similar                                 we'll see one later on so the other                                 thing that we introduced it's novel is                                 the feature store is a kind of an                                 intermediate step between the data                                 engineering on the left-hand side and                                 then the training of models done by data                                 scientists on the right-hand side and                                 then we have our files                                 I'm supporting all of this underneath it                                 we actually do support criminals for                                 model serving as well and because that's                                 become the de facto standard and then if                                 you're training models it's not just                                 deep learning you know a lot of people                                 still use scikit-learn and other                                 frameworks h                                                     scientists really want are jupiter                                 notebooks they want to do everything in                                 a joopa notebook and never leave it and                                 that's kind of what we're really                                 focusing on and and they want tools for                                 visualizing their models like tensor                                 board so a couple of bigger properties                                 of the system are that we do TLS                                 everywhere we have this multi-tenancy                                 property that i mentioned already we                                 call it secure collaboration and then                                 this thing this platform can be a daily                                 so we have customers who don't have data                                 lakes and they put this in and say daily                                 and we've customers who do have data                                 lakes and this sits beside us because                                 the existing vendors of data X data                                 lakes do not provide this functionality                                 so just to sort of the overview of the                                 platform opsworks what kind of things                                 are in it well we have a feature store                                 distributed deep learning up so fast                                 bah-bah-bah                                 you can see a lot of features up there                                 and on the board and you know no one's                                 gonna read these the one point I wanted                                 to make here was that if you're building                                 a platform like this and you'll probably                                 see                                                                     the same features as this right so why                                 are we different what is underlying this                                 is different                                 the thing we do that's different is                                 distributed consistent metadata in our                                 file system and in fact that propagates                                 all the way up to stack right so we get                                 things like secure multi-tenancy from it                                 we get this data provenance from it we                                 got our nice file system from it and the                                 feature store which is also in the                                 distribute metadata layer it comes from                                 that many of the other things you'll                                 find in other platforms                                 I guess the AI asset governance is also                                 enabled by our platform as well but you                                 know from the from it as a tech product                                 or a platform this is how we differ from                                 the others so I'm just going to go                                 through an end-to-end pipeline in in                                 hops works because this is basically                                 what people use the platform for                                 primarily it looks like an ETL pipeline                                 you have a number of steps and the the                                 lifecycle of day                                 in machine learning pipelines is                                 basically that you ingest the data you                                 do feature engineering you train your                                 models you validate your models you                                 validate the data before you train them                                 as well and then you serve your models                                 and when they're served you then want to                                 monitor them and if everything is going                                 okay you continue but new data will                                 always come in and you always need to                                 then retrain your models to make them of                                 today's so this pipeline if it's big                                 data you will need user to like spark or                                 or beam and they're effectively the two                                 ecosystems that are out there right now                                 so we do have like full support for                                 spark and I would say alpha support for                                 beam right now we have a talk in the                                 beam somewhat later later this week and                                 but the thing I guess that to take home                                 from this is that in the in the deep                                 learning community we just finished the                                 Battle of the frameworks you know we had                                 lots of more frameworks we had MX nests                                 and we had C NT k and Tiano and a bunch                                 of other frameworks chainer                                 they all just disappeared we're now down                                 to two frameworks one is PI torch and                                 one is tensorflow and the battle that we                                 see appearing right now as a vendor is                                 who will control the pipeline because                                 whether it's tensorflow or pie charts                                 doesn't really matter right from the                                 pipeline perspective it's just gonna say                                 train here's the data go train so data                                 breaks of course are pushing spark and                                 Google are pushing beam and we're trying                                 to keep our options open on both so that                                 will be interesting in terms of seeing                                 how the community of elif's because the                                 one thing that the community has agreed                                 on is that the pipeline is the unit of                                 abstraction for building a AI                                 application pipelines are what we do to                                 take data and spit out models okay so                                 just to get technical on the challenges                                 in the pipeline and these are some of                                 the things that the different                                 communities are working on that if                                 you're taking in large amounts of data                                 and you're doing feature engineering on                                 that you need to do it with data                                 parallel processing and typically that's                                 done with sea views it's not done with                                 GPUs alright and when you're training                                 models you can use lots of GPUs not CPUs                                 so                                 the spark community in particular are                                 working at saying well how can we have                                 this as one pipeline an in-memory                                 pipeline from start to finish and the                                 kind of there's a project called                                 hydrogen and they're looking at it but                                 they're really it's difficult right it's                                 extremely difficult to take because the                                 the you can see it as kind of like a                                 funnel you start with large volume of                                 data and typically it will reduce but                                 sometimes it may actually get back                                 bigger and you have some we had we had a                                 data set of                                                             data and we were doing and deep learning                                 to predict fraud or money laundering and                                 it turned into                                                           we added we took we took windows of lots                                 of number of transactions how many                                 transactions in the last day the last                                 week the last month you know network                                 embeddings lots of things you know a lot                                 of information that you can extract from                                 that raw data so you you have large                                 firms of data on the left it which is                                 typically spark and then we have lots of                                 GPUs tensorflow or pi torch so how do                                 you make a big pipeline out of this so                                 the one thing that we're we're actually                                 arguing for now is that you need                                 distributed storage you need to have a                                 layer here and break up this pipeline                                 there's no point in trying to do the                                 whole thing                                 so spark haven't managed to do the whole                                 thing yet but it's doubtful anything                                 reasonable will come out in the near                                 future and with this is the model where                                 we're predicating upon and in fact that                                 distributed storage layer we're calling                                 it the feature store alright so this is                                 the place where you will have pipelines                                 that publish features so they'll run                                 every day every hour the feature data                                 will be updated sometimes you won't want                                 to have the feature data cached in the                                 feature story just want the how do i                                 compute the feature in there but this                                 needs to be horizontally scalable the                                 feature store and then the users are                                 data scientists who want to train models                                 that just write training pipelines                                 they'll say ok I haven't I want to take                                 these features generate some training                                 data train the model I'll do some                                 experimentation find good hyper                                 parameters now I'm happy with that now                                 we can have a distributed training                                 notebook and then I could have another                                 notebook to validate the model and                                 service so those three then stages                                 become three different notebooks that                                 you can turn into an airflow job and now                                 it's productionize so I've mentioned the                                 feature store a few few times so I'll                                 just tell you some                                 the properties of it because you may not                                 have heard them before it's it's a data                                 warehouse for features so features are                                 not the same as columns in data                                 warehouses so an example of a feature                                 might be let's say I have a new app                                 that's released and I want to check the                                 adoption of the app and I want to build                                 a model to predict the adoption of the                                 app so I might look at when the user                                 installed the app that's in my data                                 warehouse I might look when the user                                 first used the app that's in the data                                 warehouse or registered but the time                                 between them is not in there in the data                                 warehouse so I have to compute that                                 that's a feature right you could compute                                 it live or you could store that value in                                 the feature store it's up to you and                                 that's a really simple feature a complex                                 feature would be if I have lots of                                 transactions and I want to take a                                 snapshot of the network around a given                                 customer who had made lots of                                 transactions with their neighbors in the                                 last                                                                    embedding for us that's a complicated                                 feature but we work with customers of up                                 to four hundred five hundred features in                                 the future store uber has said they have                                 twenty thousand features in their                                 features store and why it's different to                                 a data warehouse apart from the fact you                                 can have reusable features which is                                 great but we're using hodie to make sure                                 you can do incremental updates to the                                 feature story in hive is where                                 underneath visits of actually Apache                                 hive but instead of having to drop a                                 table and recreate the table when you                                 update data you can do incremental                                 updates with with hoody iceberg O's who                                 supports it and data breaks Delta also                                 support that functionality you want to                                 do data validation before the day is                                 published into the feature store tf-x                                 the google stands for extended model has                                 support for this there was a talk                                 yesterday by Amazon they have a                                 framework for this we're using the                                 Amazon right now and then you want to                                 make sure that you can look at your                                 features which ones are being used                                 governance so you know which ones are                                 widely used which ones aren't used when                                 do we allow users to publish them or not                                 and then the final one time-traveled                                 you'll hear this mentioned more and more                                 in the next period of time because it's                                 a really tough one it's a really                                 interesting new property we need from                                 data stores if for example we identify                                 in a case of money laundering at the end                                 of the year but the money laundering                                 took place today I want to go back in                                 time                                 to the                                                               many transactions did this user do in                                 the previous                                                             June before and in the last week before                                 that in the last month before that in a                                 typical data warehouse you'd overwrite                                 that value those windows will be                                 overwritten every every time you run                                 your pipeline but now we need to                                 actually store all of that those those                                 those updates and that's what we're                                 doing with with hoodie and with hoodie                                 you can basically then say I want to go                                 back in time tell me that the value of                                 this feature at this particular point in                                 time and the reason why we need to do                                 this is because I want to generate new                                 training data I got my fraud case at the                                 end of the year I want to see what the                                 features were that were used and what                                 the prediction was made at that time                                 because at that time I may have                                 predicted no fraud so but I need to                                 change the label to make it proper                                 trained it and say actually it was fraud                                 let's retrain the model with this new                                 feature vector so the type of                                 applications that use the feature store                                 it's online apps and batch apps so a                                 batch app might take a pre trained model                                 train model and just read it up and use                                 it but online apps will maybe use those                                 models that are served over over the                                 network from there you'll typically do                                 your training from we actually use spark                                 to distribute training on tensor phone                                 by torch now the the users view of the                                 platform is very different if you're a                                 developer your data scientists you might                                 say well this is another team they do                                 the feature engineering they don't even                                 like PI spark they want to do it in                                 Scala fine and you start from here you                                 basically say well I want to generate                                 some training data and this is where the                                 future store comes into its own because                                 someone might like PI torch someone                                 might like tensorflow                                 and someone may say well once scaleable                                 and there's different file formats for                                 those so you have TF records for                                 tensorflow numpy for pi torch and uber                                 at least the framework called peda storm                                 for really scale it's basically parkade                                 with metadata and then finally I'll have                                 worked a notebook to validate your model                                 and you write some more Python code and                                 airflow to orchestrate that into a                                 pipeline so they actually don't see                                 anything underneath this and at this                                 point I'll just kind of make our my view                                 on hops in the                                 which is kind of the title of the talk                                 if I'm a Python developer do I really                                 care if this thing is running on yarn or                                 don't care if it's running in Cleburne                                 edits or do I just want this managed                                 platform that gives me this UI and this                                 this this workflow so that's the                                 question we're kind of faced with at                                 some level and at that point I'll kind                                 of just say conclude the talk you know                                 that's our our our platform you can go                                 to hop start site register for an                                 account and try it out I didn't demo the                                 platform and but you can see it in                                 action there are some videos available                                 on YouTube to show you how to get                                 started if you want to try it out you                                 can go to edit those images for AWS in                                 Google Cloud and VirtualBox and you know                                 we ran new projects and we'd like we                                 need support and you know support us in                                 whatever way you can tweet about us                                 stars and github or whatever so with                                 that thank you                                 [Applause]                                 if you move back to slides for the                                 features and the time travels the future                                 store yes and I mean one of the key                                 problems with training data is that they                                 need to well basically the model gets                                 evaluated at all these points in time                                 and the data volume of the features just                                 explode I mean okay example firing you                                 know what you want to evaluate a model                                 at every game and and you want to                                 evaluate a model on every game and every                                 time you finish a level okay I think and                                 yeah and then where we say evaluate a                                 model you mean you you you say when you                                 send a feature vector the model                                 prediction prediction then if I'm                                 supposed to you know I don't give the                                 user a lollipop hammer or five extra                                 moves or yes you know whatever feature                                 in the game yeah I'm evaluating okay so                                 that's an that's an online model being                                 served over here yeah and then your                                 applications making a call then the                                 training but I'm training is not like                                 here yeah yeah and the problem is that                                 then the features you know they might be                                 different that have you know if a user                                 has                                                                because we're playing yeah that's a lot                                 of feature vectors so they I mean                                 there's there's yeah I mean okay I'll                                 try and handle that in a couple of parts                                 right so what you're saying is that you                                 will generate a lot of predictions and                                 should I store the predictions I guess                                 that's one point can I store all of                                 these predictions with a huge volume of                                 data so a guy called Steve Welsh I met                                 last week from Google he worked on Borg                                 he said on Borg which is the resource                                 manager for Google they store trillions                                 of data points and they don't care they                                 store all the predictions they just go                                 storages in Fitness is their assumption                                 now for the rest of us and the feature                                 store key so what you can do what we do                                 I didn't show here is that when we serve                                 models on tensorflow we actually can                                 store all of the logs of all the                                 predictions in Kafka and then you run a                                 spark streaming job to monitor your                                 model that way okay and your spark                                 streaming job could just append a table                                 in hive or a bigquery or whatever you                                 want and then that data can grow                                 infinitely right so the problem so this                                 is the problem this is the problem the                                 feature store addresses because if we                                 didn't you can do                                 and that's fine but if you use the                                 feature store the feature store will                                 store all the parkade updates as park'                                 files and it'll grow but there's an                                 assumption that this the growth in this                                 will not be infinite at some level you                                 know it'll be big but it won't you know                                 maybe double triple quadruple the size                                 but you'll be able to go back in time                                 and find out the values of the features                                  at that point I won't need to have the                                  store of all the predictions because the                                  store of all the predictions will be                                  orders of magnitude larger than having                                  the updates to the feature right I mean                                  the problem in the future value is                                  basically I need for the example of the                                  fraud prediction I would need to be able                                  to jump back in time to in between every                                  transaction basically to see what it was                                  you know before and after the actual                                  transaction no I mean so if you make                                  assumption that your transaction windows                                  are updated daily once a day okay not                                  every minute because you're gonna have a                                  job running here that runs every day                                  it'll update the windows and in the                                  feature store and that update will be a                                  park a file it'll be stored there and                                  integrated into the feature store it's                                  the latest value in the feature store                                  but the older valuably still stored as a                                  park a file so then when you want to                                  search back in time it will you'll use                                  that that time index to find the value                                  in that Park a file I don't know if that                                  makes it clear yeah I know but I was                                  also thinking like in terms of say a                                  credit transaction company that yes the                                  risk like you know clora which is close                                  to and it's used here like they probably                                  want the feature matrix you know for                                  every before every transaction for the                                  training of if they're gonna prove that                                  like I'm buying and buying a bunch of                                  things because I'm travelling to Germany                                  then I'm losing my credit card and then                                  someone else starts buying things and                                  then yeah I mean take it to kick in that                                  mean I mean I think what you're talking                                  about if if I'm correct here is that                                  models can use a lot more information                                  that changes a lot more frequently so we                                  could use all of this data to make                                  predictions which would cause                                  exponential growth of the feature store                                  is that the kind of point yes yeah so at                                  some point you have to make a trade-off                                  right and you can't use every potential                                  feature to make predictions because the                                  general rule if you have an online app                                  and you want to make predictions with a                                  model is that you can only use features                                  that you can actually access                                  and online in real time so we have a                                  real time part of the featured store so                                  you can make certain your millisecond                                  lookups on feature values there but you                                  can't make it infinitely large right you                                  can't you can only have things that will                                  you can insert there in reasonable time                                  so I mean you know there's trade-offs                                  need to be made we can take it offline                                  yeah thank you
YouTube URL: https://www.youtube.com/watch?v=vvUfgWvtqHE


