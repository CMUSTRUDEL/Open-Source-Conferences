Title: Berlin Buzzwords 2019: Paresh Paradkar â€“ Multilingual Search System - How to Build and Improve!?
Publication date: 2019-06-28
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	You are a multi-national company having customers in different countries where the business language is not necessarily English. How would you build a centralized search system catering for the needs of all your users? Apache Lucene/Elasticsearch/Apache Solr all provide different language-based text analyzers to analyze the text but which one should you use and when? We found overselves in exactly this situation. 

We are an email-security company having around 300 billion emails archived with us resulting in indices in petabytes of indexed data. Earlier we used whitespace analyzers from Lucene to be able to serve the searches in different languages but this approach although simplistic presented many limitations once we started to serve in different languages (e.g. German). I will explain how we overcome these problems by first identifying the language of the content through our own language detection model which in turn served as the guide for the selection of an analyzer to analyze the email in various languages. 

This talk will walk you through how to build multilingual search systems and explore different possible approaches. It will also discuss different problems one may run into when these language-based analyzers are used and what are the ways to improve the search results in these cases. 

In particular the talk will focus on the query-log analysis as an effective way to improve the multilingual search by providing the feedback to fine tune the analyzers used for stemming and lemmatization thereby increasing not only the recall but also the precision (relevance) of the search results.

Read more:
https://2019.berlinbuzzwords.de/19/session/multilingual-search-system-how-build-and-improve

About Paresh Paradkar:
https://2019.berlinbuzzwords.de/users/paresh-paradkar

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              good morning everyone welcome to my talk                               I'm going to speak about how to build                               multilingual search system and how to                               improve that of maybe to be precise how                               we build it and how we improved it just                               a brief intro about myself I'm a senior                               software engineer at main cast Services                               Limited based in London and Apache                               leucine and elasticsearch enthusiasts I                                did my Master's here in Germany from                                University of Freiburg that's my                                LinkedIn account if you want to connect                                to me and let's take a let's take a                                glance at mine cause let me introduce my                                company mine cause mine cost makes                                billions of users of the three thirty                                two thousand two hundred plus customers                                the emails and business data safer they                                restore around two hundred and eighty                                seven billion plus emails under our                                management in the size of more than                                forty petabytes we process around                                    million plus emails every day and our                                search volume is around three million                                messages or emails every week we have                                twelve data centers strategically placed                                around around the globe and that that's                                what we call basically grades - - so our                                customers in different geographical                                regions better and the story I'm going                                to tell you about today begins when mine                                cast open their data center started                                their German grid in early                                        although it's multilingual mostly we are                                going to focus on German and English                                because that's how we started serving                                customers nor the English first time                                yeah so that's the that's the famous                                joke about German Scrabble the fact that                                German companies were the first                                companies or customers that we serve                                whose data was non-english                                and our simplistic approach at the                                beginning laws that we will use just                                white face analyzer so you know we can                                we can break it on white space and then                                so whichever language it could be                                because most of the languages are broken                                down into white space but German in                                their unique or can say unmatched                                ability to create longest words by                                conjugating words after another it's                                really hard you know to break that word                                into sub words if you if you really                                tokenize only on white space so imagine                                a pain of a person for example searching                                for that work if you are if you are                                storing only on white space and                                searching on white space tokenizer                                so for example people might want to                                search that word just by searching for                                Gazette's isn't it going why can't they                                search why just guess it's an                                           containing that so we need to change                                something there and then maybe it's not                                that every email contains that many long                                words but some of the real you know real                                words from our real users data where                                like like these compound words and for                                example if we say people are finding                                words beystadium which means                                confirmation they are not going to find                                emails which contain for example you                                know term in which telugu means                                appointment confirmation or first and                                vegetable means delivery confirmation                                and so on so with system that tokenizer                                is just one white space we said okay we                                have to change something otherwise it's                                really not that user-friendly in terms                                of no searching email with containing                                compound words or big long words so what                                we and then another problem was actually                                that the character equivalences in                                german or you know this languages which                                which have like umlauts which can be                                represented by a character and in the                                II and specialist ed character which can                                be correct which can be also represented                                by SS so for example if a user searches                                on such as with use of character                                equivalences you will not be able to                                find email which is containing you know                                which is actually written with the                                umlauts or s set character so these were                                the two problems that basically started                                us to think that we cannot just have                                whitespace analyzer and we need some                                different strategy so we decided and and                                 whether we use Lucene directly so we                                 have to customize everything on our own                                 so we decided to write our own custom                                 analyzer which will sit on top of the                                 German analyzer that we have in the                                 scene so the analyzer basically does is                                 it starts with a compound word token                                 filter in the scene there are two types                                 of compound word token filters one is                                 dictionary compound word token filter                                 and another is a hyphenation compound                                 word and filter basically both use                                 dictionaries but the hyphenation one                                 uses high finishing gamma to identify                                 where it can break basically your                                 compound word into its of words and then                                 those sub words could be looked into the                                 dictionary to find if that really                                 matches so that one is the -                                 action-filled filter is basically faster                                 so we chose to use that one to break our                                 compounding words into the sub words and                                 then those sub words would then be                                 passed to german analyzer from the scene                                 so that we can process the umlauts or                                 estate characters into the character                                 equivalences and then at the end we                                 store it into the index again so that                                 means we have the we have the                                 possibility of breaking down compounding                                 words into sub words and then you know                                 from processing umlauts and estate                                 characters etc so let's take an example                                 maybe                                 from our previous real data let's say we                                 have user who is who is having an email                                 with term invested ego which means                                 appointment confirmation and we want to                                 store it with our analyzer so what it                                 will do is first it will using                                 hyphenation grammar it will break down                                 this word into termine best and which                                 they do best rest is basically coming                                 out of the static again because it's                                 recursive so that means we have now word                                 for appointment best and which data gong                                 which means confirmation so we have the                                 we have the broken down sub words and                                 then we pass them to our german analyzer                                 which will then you know process this                                 umlaut channel and make them as                                 lowercase and term invest and starting                                 on so that's how basically the analyzer                                 will break down the compounding words                                 into the sub word so that we can match                                 for example the longest word we had seen                                 before you can also search it by                                 matching visits for example so that's                                 fine so our user is saying okay thanks I                                 can't find sub words now but can i                                 really search my both emails in German                                 and English together now just an                                 analyzer won't help it we need some more                                 you know techniques to identify how we                                 are going to store our emails in                                 different languages so that's a criteria                                 to to decide and then we need something                                 which will understand what language our                                 email basically is in so first would be                                 the language detection model which will                                 you know this beside which language the                                 document belongs to so we decide we                                 develop one in the logistic regression                                 model based on Wikipedia data set to                                 identify the language it detects                                 languages with precision but the                                 criteria here was not just precision but                                 also high recall for English because as                                 though we are serving now multilingual                                 the more or the most customers are still                                 having English emails in our                                 corpus for our data so we cannot give                                 the possibility that you know English                                 email being recognized as non English so                                 the idea for the language detection                                 model was that model which will give the                                 highest recall for English so that it                                 will identify the email from different                                 languages as them as but it will mostly                                 focus on not identifying English as non                                 English so so that part is is done there                                 but we still need to know how we are                                 going to store awareness in the scene                                 index so there are three so there are                                 three generally as accepted ways to to                                 you know put multilingual documents                                 inside your Lucene index one is                                 basically you create a separate index                                 for every language and then you just                                 store your documents in every language                                 in that respective index another is that                                 you create a separate field for every                                 language so your index remains only one                                 but it has multiple fields and then                                 another would be you just insert all the                                 documents in one index and just have a                                 field which tells you basically what                                 language it belongs to so let's look at                                 them one by one so first one is a                                 separate index this is basically nothing                                 but you create create a separate index                                 whenever you want to you know support a                                 language so new language comes in you                                 want to support that then you just                                 create an index and store start storing                                 your documents in that language into                                 this separate index this provides you                                 know clearer structure because every                                 language it has got their own index it                                 also doesn't match up in your own                                 frequency is considering every document                                 is in its own data set and basically it                                 also gives you better handling for                                 linked particular language where you                                 know the languor query regarding a                                 particular language so that's that and                                 then second one is the separate field                                 for language in this approach you will                                 have to create for every language a                                 separate fear so for example if your                                 content is your field is content and                                 every email has a different language                                 then you have to have separate field                                 like English content German content                                 French content and so on and now with                                 this approach the problem is that if you                                 start now saying okay I am going to                                 support it tallien as well from tomorrow                                 then you have to change your index                                 schema you have to change your fields                                 because you have to add new field there                                 so to index all your documents in that                                 index and it's very hard for for you                                 know systems where the data is really                                 large but it could be so it could be for                                 easier for I don't know with some                                 systems where your data your indexes are                                 small and volatile maybe you can just                                 reindex those it really depends on on                                 your use cases basically but it brings                                 overall overhead of you know extra                                 fields into index while indexing as well                                 as query and the third approach is you                                 create separate documents per language                                 so that means you basically nothing but                                 you create one index there will be a                                 field which will tell you what language                                 that a document belongs to and you just                                 reuse your fields basically there is no                                 separate field for any separate language                                 this also this approach also scales well                                 with you know increasing number of                                 language supports example you you have a                                 new language you want to support you                                 just store your document inside your                                 index and just create you specify which                                 language it is in your language field                                 but since all languages are mixed up it                                 could be a wrong tone frequencies                                 problem and yeah I mean it depends again                                 your your project but what we did what                                 we decided is to better keep it clean so                                 we have every language its own its own                                 index basically so so we have now                                 language detection model we have now a                                 way to this way to you know store all                                 your emails in the separate language                                 separate index but our friend is saying                                 that okay I can find my German                                 English humans now because I we are                                 storing it separately but your system is                                 returning email containing word best a                                 legume a stop result when I search for                                 best so he wants to have the ability to                                 search the sub words but he says okay                                 you cannot just search it you just have                                 to improve it in terms of relevance and                                 precision so the what what's the what                                 problem here is that we have you know                                 balance we have to balance our precision                                 and recall basically we increase recall                                 by adding new terms in our index                                 well we D compounded it but we lost                                 precision because now as as our friend                                 was saying that we can we can match more                                 emails or more documents to our queries                                 because we have matching terms so there                                 could be there could be two ways to to                                 handle this one is to one is to do query                                 log analysis so what what you can do is                                 basically you analyze your quail logs                                 and see what users are searching or how                                 they are searching and can feed back                                 that basically back to your analyzer                                 saying you know to fine-tune your                                 parameters where to cut your sub words                                 how much longer you want to go for your                                 sub words or how small they should be                                 because if you if you cut for a small                                 length of sub words it's going to create                                 more terms and then it's going to be the                                 situation that you are matching more                                 emails so basically you can say okay I'm                                 going to let's say I'm going to create                                 sub words only if they are length of                                 five or four and then you will not have                                 that many that many terms created out of                                 your analyzer and basically not matching                                 everything so you minimize number of                                 terms that are getting stored and number                                 of emails that are matching there                                 another way was to rewrite your queries                                 so what what we can do is we store the                                 output of our analyzer not basically                                 into the same faith                                 to a separate field so you can do is you                                 create another field called I don't know                                 sub words and store your context or your                                 sub words into that field and whenever                                 your user is searching you boost your                                 actual field over your sub verse field                                 so that docks containing you know the                                 actual word that will be ranked higher                                 than they than they were matched just                                 due to being in the sub words so what                                 I'm saying is for example a term in                                 which lady goon created best best lady                                 goon enter men so you save those Tulsa                                 words into your another field so that                                 you only boost when when somebody is                                 searching for best you only only boost                                 the words best in the content field and                                 not from the sub words field so the guy                                 was you know our user was complaining                                 why I get emails with Bastida goon as a                                 top result when I'm searching for best                                 we can solve it by that way that we have                                 only emails with best at the top of our                                 result so in a nutshell I would say we                                 build we build a language detection so                                 basically scales for all languages                                 German and English were the most                                 interesting one because of our use case                                 as well as the the ability to you know                                 compound compound the words but in the                                 nutshell we can say that we we need a                                 language detection model which                                 identifies which language document                                 belongs to and we need to decide on a                                 structure how we are going to store our                                 emails or documents whether in one index                                 or separate indexes then in case of                                 languages where there is a compound in                                 word there are compounding words or                                 something like that we need an analyzer                                 which analyzes correctly and you know                                 brings the sub words out of it and then                                 in terms of improvement we can always                                 rewrite queries to boost the documents                                 to boost the fields correct field so                                 that the the relevancy matches better                                 and there is no loss of precision so                                 that's that's all from my side actually                                 also and we are hiring smart engineers                                 like all you guys                                 is joining our music our careers page                                 and yeah Wow                                 question from like new candles that's a                                 multilingual search is incredibly hard                                 and incredibly important so you built a                                 language detector which is a missing                                 piece of infrastructure in the open                                 source world some two questions do you                                 know about Google's compact language                                 detector which is an open source                                 language detector factored out of the                                 chromium open source web browser mm-hmm                                 first question second question do you                                 have plans to open-source your bottle                                 sorry I did not get a second question                                 what else do you have plans to open                                 source your language detect oh okay                                 there's a piece of infrastructure so                                 many people need it really ought to be a                                 solved problem by now in the open source                                 world but it's not yeah yet so we build                                 our own custom model because we are you                                 know use it as a part of our email                                 processing system basically to identify                                 which which documents they are belonging                                 to and we have a separate data analytics                                 team which which created that that                                 language detection models we are                                 actually also quite involved in open                                 source we are starting to involve as I                                 can say but I don't really have any                                 answer for that if we actually language                                 detection models but I can check sure                                 sure did you encounter a situation where                                 a document could have multiple languages                                 yes of course so how did you address                                 that problem so in that case what we do                                 is we rely on users query order you know                                 corpus so for example if we contain an                                 example of World Museum which is same in                                 German and English basically in that                                 case there will be English documents                                 containing Museum as well as German                                 documents containing Museum so in that                                 case what we do is basically we search                                 on all languages the user has because he                                 has all languages he has documents in                                 all languages so we have to show them                                 but if his his corpus is more of German                                 then we                                 boost those that do those records                                 basically so that it's a higher precise                                 records of German emails
YouTube URL: https://www.youtube.com/watch?v=otRdUhHqJjI


