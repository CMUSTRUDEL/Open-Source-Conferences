Title: Berlin Buzzwords 2019: Abhishek Kumar Singh – Managing Distributed Workflows at Scale #bbuzz
Publication date: 2019-06-28
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	Abhishek Kumar Singh talking about "Managing Distributed Workflows at Scale - Kubernetes Jobs in Action".

Kubernetes has been widely accepted as the de-facto deployment orchestrator for managing and scaling containers in the cloud. While it's capabilities have been well acknowledged in the deployment space, its ability to manage business-specific workflows is yet to see a wide application.

This talk gives an insight into how we at Unbxd used 'Kubernetes Jobs' to build a Workflow Orchestration Engine that helps to configure and manage complex sequence of processes where the output of each step is used as an input for the next node. This architecture can also use existing microservices running on any platform as a node in the workflow and the data routing intelligence remains with the workflow orchestration layer (in the form of fault-tolerant DAGs).   

This dynamic and configurable workflow also helps in scaling the architecture well, as the inter-node data flow is controlled by the Orchestrator from within a Kubernetes pod (using a distributed message queue).

Read more:
https://2019.berlinbuzzwords.de/19/session/managing-distributed-workflows-scale-kubernetes-jobs-action

About Abhishek Kumar Singh:
https://2019.berlinbuzzwords.de/users/abhishek-kumar-singh

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              good morning everyone good afternoon                               everyone thank you for joining me today                               for my talk so I'll be talking about                               his jobs and distributed you know                               workflow orchestration engine that we                               built ad unboxed so so starting off so I                               work at unboxed which is an e-commerce                               discovery platform so so basically you                               know towards our approach to solving                                these complex ecommerce problems we have                                come a long way with these products so                                Site Search browse recommendations and                                product information management I work in                                the enterprise products team where we                                bring Big Data information retrieval                                automation and intuitive interfaces                                together to help our clients experience                                the power of scale unbox PIM is a recent                                development is a recent product that we                                recently launched so you know Maj PIM is                                a one-stop solution to collect manage                                and enrich ecommerce data and                                distributed out to e-commerce channels                                and partners an important aspect of                                inbox PIM is its workflow automation                                engine and today's that presentation is                                going to be all about that so this is                                how my agenda looks like I'll give an                                overview of the workflows and then we'll                                talk about the objectives in the                                components of the workflow castration                                engine that we have built and the final                                architecture of course and then we'll                                move on to the kubernetes and its                                features and which features have been                                used in which components of the                                orchestration engine so beginning with                                in with the overview of the workflow so                                a typical workflow in the e-commerce                                ingestion pipeline looks something like                                this here I have taken the example from                                an e-commerce brand aggregator so say                                there's this guy who has his catalog                                coming from a lot of brands and then he                                and his end goal is to basically you                                know publish it out to one or more                                ecommerce platforms so so this is how                                his flow actually looks like he gets his                                data from s                                                             filters it out based on a particular                                brand based on the filters it branches                                out into two different kind of                                operations that he decides                                and you know as we can see on one kind                                of a product he applies this operation                                called modify pricing and then                                watermarks images and so on so you know                                the extent to which these workflows can                                expand can fan-out is unlimited and it                                has to be highly configurable so all                                that we need here is an automation                                engine and engine that lets you                                configure your workflow and and lets you                                automate it no matter what the scale is                                so so what we have to build is a                                workflow automation engine so these are                                the primary objectives we'll be looking                                at it should be scalable and fault                                tolerant it should enable time or even                                based triggers and should also expose                                rest api is for workflow configuration                                the components of the engine are going                                to be a node data streams the workflow                                itself event listeners scheduler and the                                orchestrator will have a look at each                                these each of these components in detail                                in the coming slides so starting off a                                node can be seen as the most basic                                entity in the workflow every node                                contains a special business logic that                                has been applied on a finite stream of                                data it receives a node constitutes of                                four logical steps internally so                                configure takes the takes care of                                converting the configurations it                                receives into a finite stream of data                                and passes it on to the execute node                                execute has a business logic which is to                                be applied on this data and then and                                yeah and and the business logic which is                                there inside execute can either be an                                in-memory processing of the data or it                                can simply be hitting external API is                                the end goal is to transform the set of                                data that it receives and again the kind                                of data that is being moved across these                                stages of a node is finite this is this                                is bounded data basically it is some                                kind of batch processing is going on                                 over here the third step is init output                                 metadata Varian you know the results                                 which have been achieved in the execute                                 state is being converted into some                                 metadata I'll explain in the next slide                                 why metadata and the finalized step is                                 where you                                 all kind of cleanup happens and this is                                 how if a workflow has you know a                                 particular sequence this is how the                                 dataflow looks like the node from one                                 gets transferred to the next one talking                                 about the i/o data streams so the                                 communication between two nodes happens                                 in the form of metadata so as we know in                                 a work flow the data which flows between                                 two nodes has to be you know one node                                 may filter or simply process on a                                 particular set of data and that gets                                 passed on to the next node so here what                                 we do is we checkpoint this data which                                 you know gets output from a particular                                 node and this checkpoint thing if you                                 start doing it for millions of data it                                 gets you know it doesn't really scale so                                 what what we have designed is that you                                 know every node should output a set of                                 metadata and every node will have a                                 logic to basically convert the metadata                                 into the actual set of data that it                                 should actually be processing on there                                 is another advantage with parsing on                                 metadata is that it can be check pointed                                 as well in case you know some kind of                                 hardware failure or something happen and                                 you know your workflow just wakes up it                                 can simply start up from the last node                                 that had executed so the output data                                 streams is encoded in the third step                                 which is the init output metadata steps                                 of a node and it is decoded back in the                                 configure step now so after we have the                                 grasp of the basic concepts involved in                                 this workflow the node and the streams                                 we can configure overflow so when we                                 configure a workflow these are the                                 things which are needed a directed                                 acyclic graph which is you know we                                 configure several nodes and then we                                 define a particular sequence it there                                 can be fan outs and fan ins there so                                 that's what a dag structure is all about                                 a workflow should have some trigger                                 details based on what kind of a trigger                                 should a workflow act should actually                                 start it should you know it can be an                                 event listener or it can be a scheduler                                 and these specific node configurations                                 so you know it can be something like                                 there is a node which works on the                                 pricing scales the pricing out there you                                 just                                 on the thresholds these can be static                                 configurations so each node operates on                                 some data there can be two kind of data                                 the static data is the configurations                                 that you pass while configuring the                                 workflow and the dynamic data is the                                 data that has been passed on by the                                 previous node onto it and this is the                                 last in the most important component of                                 the workflow orchestration engine is the                                 orchestrator itself so the orchestrator                                 has some responsibilities it has to                                 react whenever a time trigger or a event                                 listener lets it know that a particular                                 workflow has to be started it goes to                                 the directed acyclic graph which has                                 been configured inside the workflow and                                 starts the particular nodes it also                                 exposes some API is out through which                                 you know this triggers can be informed                                 and when the nodes are executing they                                 can checkpoint their information we'll                                 be looking at that information that                                 detail in the coming slides a workflow                                 Orchestrator is complemented by a                                 workflow meta store which can be an                                 outside service which has all the                                 configurations related to all the                                 workflows we have ever configured and a                                 persistence layer of course where all                                 this you know all the check pointed                                 States and all these information is kept                                 well after having discussed all the                                 components all the building blocks                                 involved in this architecture let's move                                 on to the actual architecture there so                                 the orchestrator receives an event                                 through one of the triggers and goes                                 through the dag structure of the                                 workflow and starts a particular node                                 the node executes each of its internal                                 steps and keeps on checkpointing the                                 data back to the workflow Orchestrator                                 which persists it's in                                                 of its steps the orchestrator basically                                 keep the orchestrator here only stands                                 as a particular service which starts a                                 particular node and keeps on receiving                                 events back from the node after the                                 finalized step of any node it knows what                                 next node has to be started and one                                 important aspect that I hadn't discussed                                 till now here are the external micro                                 services so then the execute step of the                                 node which actually is supposed to have                                 all the business logic in it may simply                                 leverage all the external API is you                                 already have out there all the business                                 logic or the processing logic you have                                 in your system the workflow will help                                 you in automating this so you know so                                 this micro-services find a very                                 important place in this architecture as                                 well now you know when when I have                                 actually described the particular flow                                 and the architecture of this system here                                 I'll go on to the technology which has                                 been used to implement this so yes                                 kubernetes so kubernetes as we all know                                 is a widely used container Orchestrator                                 it's open source and it believes in                                 managing applications and not machines                                 deployment is a very common use case for                                 kubernetes with kubernetes deployment                                 rockets and controllers keep monitoring                                 your application and in case it goes                                 down or something                                 kubernetes does everything it can to                                 auto heal so it will spin another                                 container up and recover for itself                                 here deployment is not just about the                                 initial launching of the container but                                 it's something much bigger in kubernetes                                 apart from deployment                                 it also lets us do much more that we'll                                 look up in the next slides kubernetes                                 jobs so kubernetes jobs is a special                                 feature which lets us run batch jobs in                                 covenant is cluster it differs from                                 other controller objects like that of                                 deployment because in equipment                                 kubernetes environment because it is                                 managed as a task that has to run till                                 completion not like a deployment where                                 it has to be continuously running                                 so here kubernetes make sure that the                                 kubernetes job goes till the completion                                 state in case some kind of restarts                                 happen or some kind of hardware failure                                 happens kubernetes make sure that all                                 the jobs which were running and have not                                 attained of completed or a failed state                                 or he started again so it gets our                                 responsibility to make sure that the                                 jobs know that it can be restarted in it                                 has to you know check start resume from                                 a last check pointed place so it's                                 useful for large computations and batch                                 oriented tasks and also you know it                                 falls it's fall torrance because                                 kubernetes meant make sure that it goes                                 till the completion state another                                 feature that we have used in this                                 architecture our kubernetes cron jobs                                 these are essentially kubernetes jobs                                 with the same fault tolerance but with                                 scheduler configurations so basically                                 you can specify the same job                                 configurations with with the specific                                 scheduler configuration which is of the                                 format of a Linux crontab expression and                                 kubernetes will make sure that that                                 point of time this job is started so and                                 everything else is exactly similar as                                 covenant is jobs and now we will see                                 which feature like the kubernetes jobs                                 in the cron jobs have been used in what                                 part of the architecture so the event                                 listener event listener is a managed                                 kubernetes pod it's a it's a deployment                                 object basically so here you know a set                                 of jobs are running long-running jobs                                 are running which have a Kafka consumer                                 running inside that and that is                                 listening on a particular topic which                                 has every event generated in your system                                 whatsoever to make this kind of a system                                 work we also need to have a particular                                 topic where we are ingesting every                                 special event in our system the Shuler                                 as you all must have guessed already                                 it's a covenant a scheduled job the                                 responsibility of this Shatila is to is                                 this kind of a job is to just wake up                                 whenever kubernetes lets it know that it                                 has to wake up and start a particular                                 workflow it lets it let's the                                 orchestrator know that this particular                                 workflow has to start and the                                 orchestrator knows everything else about                                 the workflow so it it will take care of                                 you know executing the nodes inside the                                 workflow in the same sequence as we                                 define anything that the node whenever                                 an Orchestrator starts a particular step                                 in the workflow that executes as a                                 kubernetes run to completion job the                                 kubernetes bad job so here it will be                                 our responsibility that each node each                                 new business logic that we add to this                                 architecture we have to deploy that as a                                 docker image and when we are configuring                                 we have to give the name of the docker                                 image that set and the orchestrator will                                 make sure that the docker image has                                 started as the kubernetes job and then                                 it will be our cube and it is the                                 responsibility of the kubernetes to make                                 sure that it runs till completion and                                 yes the orchestrator and every other                                 micro services are running                                 as covenant esports so they are highly                                 available so as I explained if anything                                 happens to them covenants will make sure                                 that they get up the orchestrator is                                 special in one aspect that all the                                 triggers that it receives from the                                 scheduler or                                 event listener they all go to a queue                                 rather than you know being served then                                 in there for all obvious reasons that                                 even if it's you know just in case down                                 for even a second or something no                                 trigger should be missed out so that's                                 our is now as you all must have guessed                                 by now the orchestrator is a code and                                 that controls kubernetes so there is                                 some library that we have used the                                 fabricate kubernetes client this lets                                 you control kubernetes through your code                                 so you can do everything that you can do                                 with kubernetes rest api is using this                                 library so this is nothing but a wrapper                                 over the kubernetes rest lance where                                 it's works really well and that's the                                 dependency out there and yes                                 instead of yml files it uses a well                                 structured strictly type java objects to                                 configure your jobs it also exposes into                                 F interfaces to filter filter your jobs                                 and pods based on a particular label or                                 a metadata basically everything that you                                 could have done using kubernetes                                 command line or at rest EPA's so that's                                 how it is now what happens at scale so                                 this architecture was defined designed                                 for scale because we were using                                 something else previously and that                                 didn't use to scale that well and wasn't                                 this much configurable either so if the                                 number of workflows executing in                                 parallel                                 increases drastically it directly                                 impacts the number of nodes for which                                 resources should be allotted by                                 kubernetes scaling kubernetes cluster is                                 as easy as adding another instance to                                 the cluster now resource allocation can                                 be planned based on the expected number                                 of parallel workflows running at any                                 point of given time so it's pretty easy                                 for us another aspect that I would want                                 to highlight here is that you know                                 scaling by rate limiting so something                                 that we learned over the years that you                                 know                                 in case you experience a burst in                                 traffic or a high traffic all of a                                 sudden you might be tempted to scale                                 your processing units your metal weighs                                 out but you know what happens is this                                 also exerts a higher amount of first on                                 your database or your persistence layer                                 or the downstream micro-services now you                                 might say that you will also use kale                                 the databases out but in real primary                                 real-life practice what we have observed                                 is that scaling out a database might be                                 pretty easy scale it back scaling it                                 back down is not so easy at all so you                                 know in that case scaling my rate                                 limiting works really well so here what                                 you actually do is you make sure that                                 the middle a middleware through which                                 you know each request has to go to has                                 to be throttled at a particular                                 parallelism say that can be                                                                                                                      will ensure that even in the worst cases                                 in the highest cases of bursts the                                 actual number of parallel requests going                                 on to your final persistence layers will                                 be will be throttled at a given my                                 maximum so kubernetes helps you in rate                                 limiting the processing layer as we can                                 specify the upper cap on the number of                                 pods or the resources that can be                                 running parallely while every new node                                 will be queued for future when the                                 resource will be available thereby                                 limiting any kind of burst on the                                 underlying DB of the micro services                                 layers being used by the nodes and some                                 good practices that we learned while                                 implementing this the orchestration                                 engine should be made platform agnostic                                 I'm not really sure platform is the                                 right word here or not but what I mean                                 is that the orchestration engine should                                 be written in a way that today if it is                                 working with the kubernetes cluster                                 tomorrow it can be working with other                                 kind of other kind of container                                 orchestrators as well so so basically we                                 should keep in mind of this fact                                 cleaning up the pods in kubernetes so                                 you know so what happens is when a job                                 completes no more pods are created but                                 the pods are not deleted either                                 kubernetes keeps them around so that you                                 can still view the logs of completed                                 jobs and check for errors warnings or                                 other diagnostic outputs the job output                                 also remain                                 after it is completed so that you can                                 view the status but in our case we will                                 be anyways be managing the states of the                                 nodes in much more excruciating detail                                 and much more granular level so these go                                 states only take up resources inside the                                 kubernetes cluster it's better to delete                                 them so you can have a look at the                                 kubernetes TTL controllers you know                                 that's how the configuration goes inside                                 the job configuration that will you know                                 make sure that the jobs are cleaned up                                 and yes you'll have to back up your logs                                 before the pods are deleted otherwise                                 you'll be losing all the logs out there                                 and yes as I have described in the                                 architecture slide a node may simply                                 rely on other micro services or an                                 existing data pipeline for its data                                 processing this helps in offloading the                                 responsibility to an external system                                 well the node on the other hand make                                 sure that the synchronous process is                                 completed and let's the orchestrator                                 know about the same and the next nodes                                 can be triggered so just in case you                                 have some massively parallel jobs this                                 node the code that you've written is                                 inside the node just that is not going                                 to help you out you should you know take                                 help of some external engines and yes                                 the node can be you know checking out                                 and some on some metadata that you might                                 have devised in your system and it will                                 help you in sequential icing the process                                 that's my team at unboxed and thank you                                 very much you can get in touch with me                                 on my email id or in my Twitter handle                                 do we have time for questions                                 there is actually time one minute for                                 questions so questions in the room                                 there's one                                 hi thanks for your talk                                 one question I had what was what is the                                 benefit of your system against something                                 like Apache airflow or Luigi so as far                                 as I know airflow does not give you so                                 we have used air flows in the past for                                 managing you know a small scale or you                                 know a very hard coded workflows so in                                 that case you know air flow takes its                                 configurations only in the form of a                                 Python program here the software that we                                 have it exposes a UI to the clients and                                 the clients want to drag and drop stuff                                 and you know configure each node and                                 make their own workflows so there it                                 won't really help unless you actually                                 write another engine that converts this                                 UI data into a Python code that's                                 something second thing is I don't really                                 remember the name that air flow uses for                                 its parallelism we have seen that at                                 scale it starts trembling it doesn't                                 really work that well I don't remember                                 the name of that component but yes there                                 are some problems with that so that's                                 how we were motivated to use kubernetes                                 in itself the whole ecosystem would be                                 around kubernetes and the workflow                                 itself quickly do you guys plan to                                 donate it to me yes so yes I would love                                 to donate this but you know for as in                                 you know when something goes out to the                                 open source it has to have a minimal                                 standard of quality code out there so                                 you know there is a particular team in                                 my company that actually reviews the                                 code and all that it's actually under                                 review and if they approve it I would                                 definitely love to you know open sources                                 okay thank you very much Abhishek thank                                 you                                 [Applause]
YouTube URL: https://www.youtube.com/watch?v=LjVfRz_q8YA


