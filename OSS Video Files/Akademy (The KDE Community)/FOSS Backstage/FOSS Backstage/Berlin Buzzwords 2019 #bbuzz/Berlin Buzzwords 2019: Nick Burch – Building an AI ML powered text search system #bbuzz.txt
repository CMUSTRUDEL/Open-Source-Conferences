Title: Berlin Buzzwords 2019: Nick Burch â€“ Building an AI ML powered text search system #bbuzz
Publication date: 2019-06-28
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	There are some great Open Source text search / information retrieval systems, such as Apache SOLR and ElasticSearch. But could an AI / ML powered solution do better? And how would you even go about building one?

Based on our experiences building a knowledge base / Q&A system, we'll guide you through the process. Learn how to get your text into a format that AI / ML techniques can work on, and how to build a simple model and recommender. Then it's Deep Learning and Neural Networks, and finally updating the models with real user feedback. Oh, and comparing it to a traditional search engine, to see if it's actually any better ...

Read more:
https://2019.berlinbuzzwords.de/19/session/building-aiml-powered-text-search-system

About Nick Burch:
https://2019.berlinbuzzwords.de/users/nick-burch

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              so my name's Nick Burch                               CTO and head of AI development Quantic                                 which is a firm that helps out with                               clinical trials we've got a lot of                               mathematicians and statisticians doing                               analysis I'm gonna show some source code                               in the slides during the talk I've only                               put in little snippets that are relevant                               but I have the whole thing available as                                a ipython notebook so you can have a                                look at more of the context now if you'd                                like and then also later you can clone                                that notebook and try and customize it                                for your own needs so nice little QR                                code hopefully it can work for everyone                                ok so it's not gonna be your typical AI                                talk and can explain a little bit about                                the kind of problem that I've got to                                solve and then I'm gonna look at how we                                can do AI for text how we can make it                                better and then at the very end I'm                                gonna give a whole bunch of resources so                                that if you're interested in this sort                                of thing it's the sort of problem that                                you've got you'll have some pointers on                                how to go off and learn some more so                                what is an AI an ml and why is it the                                buzzword at burning buzzwords this year                                so AI artificial intelligence ml machine                                learning and Larry Tesla's theorem is                                that AI is whatever hasn't been done yet                                and ml is whatever we can do today some                                other people say that if you're raising                                VC funds it's AI if you're hiring staff                                its ml ok but why today so the first big                                AI bubble was in the mid                                                was over a billion US dollars raised in                                VC funding that was mostly AI for expert                                systems they had two big problems one of                                them is that wasn't enough training data                                available to build their AI is to train                                their AIS and the second problem was                                that in the                                                        employ an expert than it was to buy the                                computers we're back again today so                                Moore's law to the rescue                                             dollars worth of computing power in                                     it's less than $                                       what's more Amazon will rent you a                                machine with a whole terabyte of memory                                for about the cost of a latte an hour                                okay you can rent from them a full                                terabyte memory machine for                                            hour so a whole bunch of problems that                                used to need more computers than would                                fit in this room are now                                            hour yeah first thing and then we've got                                a lot more data available to train our                                models on whereas you used to have a few                                records we've now got terabytes and                                petabytes of records that we can train                                on and the other big thing is all the                                open source libraries and frameworks you                                used to have to hire computer scientists                                to write you're open to write your a I                                library so that you could then get your                                data scientists to train it it's just                                three lines of Python now five lines of                                Java it's amazing there's all these                                libraries out there that make it easy so                                you can focus on the data and on your                                model not on how the hell do I build one                                of these things okay machine learning                                xkcd everyone seen this one before                                be aware that some of it is just                                cheating with your hyper parameters                                until the right answer comes out you can                                do amazing things with AI nml and you                                can also do terrible terrible things                                just try and have some quality metrics                                that give you an idea of what's going on                                okay so in your typical AI ml demo you                                have images images a call make for good                                demos so let's do a little bit of                                audience participation labeling some                                data it's this dogs or cats dogs okay                                what about this one is it dogs or cats                                ah okay so the AI is only going to be as                                good as the questions and the training                                data so if you give an AI that's trained                                to recognize dogs and cats a picture of                                a koala it's not going to give you the                                right answer okay let's generalize what                                kind of animal is this one                                kangaroo okay this one this one bat okay                                but if you're not careful your AI will                                say that's a dog because you know four                                legs low to the ground and this one                                now this one is an echidna so if you're                                 getting a load of humans to classify                                 your images and they don't know what's                                 in the image they can't give you the                                 training data that your AO is going to                                 need so image classification is pretty                                 cool as long as your humans are able to                                 give you the right training data but my                                 data                                 doesn't look like that I mean cooler's                                 Australian animals are they they they                                 don't represent what I've got at work                                 what I've got at work is stuff like this                                 anyone recognising these kind of things                                 you've got a lot of these at work                                 you got spreadsheets you've got Word                                 documents you've got lots and lots of                                 data                                 you've got Q & A all that kind of thing                                 so this is the kind of stuff that I've                                 got a lot of and I'm hoping that most of                                 you also have a lot of and saying is                                 this a cat or a dog or a wombats not                                 really going to cut it so now if you did                                 want image classification there was a                                 really good one last year on satellite                                 imagery and if you really wanted one                                 that's just on the numeric model                                 fiddling numeric model fitting there was                                 one two years ago where they went                                 through all of it and again they've got                                 the code so if that was your sort of                                 thing                                 Berlin buzzwords has covered you before                                 but what we're interested in today is                                 text so I have lots and lots of                                 documents I've got policies I've got                                 procedures I've got training guides I've                                 got help information                                 I've got RFPs and all that sort of thing                                 I also have a whole load of people's                                 medical data trial data but for ethical                                 and compliance reasons I'm not allowed                                 to touch that have to get ethics                                 committee approval and so on so I'm not                                 going to touch on any of the work                                 medical data stuff I'm only looking at                                 the text that we've got but I think a                                 lot of us do have lots and lots of text                                 out there so I need to do a whole load                                 of in exact searching over these                                 documents so for example I've got a                                 whole bunch of project training and help                                 stuff and I've got someone who's new to                                 the project and they come in and they're                                 like okay so I need to do this thing                                 here's the general term and they look in                                 the training manual and it uses the                                 project specific naming                                 we've got a lot of abbreviations in our                                 company we've got a lot of weird naming                                 that we use I think a lot of you also                                 have that the size of your company goes                                 up the number of abbreviations and weird                                 for sources kind of goes up so at the                                 moment our workaround is that people use                                 the project just message their team lead                                 and say hey how do I do this there's                                 some definite scaling problems with that                                 another problem we're a kind of                                 consulting firm we deal with lots of big                                 customers and they keep sending us                                 questionnaires and those questionnaires                                 come in in the format of the customer                                 not in our format and they all use                                 different wording to describe the same                                 thing so at the moment lots of really                                 busy people have to keep reentering the                                 same answers into the RFP questionnaires                                 even though they altered it through                                 before because the different companies                                 use different naming and the sales guys                                 don't understand fully what we do so                                 they can't do the in exact matching so                                 can we can we do something now I can't                                 share with you all of my training                                 material and I can't share with you all                                 of the RFP responses if you want an RFP                                 response must you've got to come and                                 contract with us but what I do have is                                 all of the berlin buzzword talks so                                 we've got titles and we got abstracts                                 and we've got classifications so let's                                 see if we can use these as a proxy for                                 doing some text and somatic ok you've                                 all been on the website you've all seen                                 things like this so we've got speaker                                 we've got the track we've got the                                 abstract all sorts of text here so what                                 we're going to try and do is partly                                 clustering so what kind of talks are                                 similar to what other kind of talks                                 what words in those talks are similar to                                 what other words in those talks and                                 partly it's going to be a recommendation                                 problem so if you went to one talk and                                 you're interested in search then what                                 other talks are you going to be                                 interested in but it's not exact                                 matching I don't have classification                                 labels and I didn't go around last year                                 pigeonholing every single person and                                 saying which talks did you go to and                                 which exact keywords did they match I                                 haven't got that and it does have to                                 cope with people who give funny talk                                 titles and cool talk titles which                                 sometimes I must admit is myself                                 so we've got to deal with in exact                                 matching here but that maps onto my                                 problems at work where people use weird                                 naming and weird abbreviations and                                 people don't know those so first issue                                 my a IML frameworks don't like word                                 documents and they don't like                                 spreadsheets luckily Apache tikka its                                 project that hides all of that                                 complexity it's in Java you feed it a                                 thing you like I don't know what this is                                 figure it out and it gives you back some                                 nice clean HTML that we can then process                                 so at work that's what we do we feed in                                 all of our spreadsheets all of our                                 policy Word documents outcomes HTML                                 which hung it up based on the the slides                                 or the tables and then we turn now to                                 JSON I could cheat for this talk so I                                 just use beautifulsoup                                 scraped the whole of the Berlin                                 buzzwords website and spat out a load of                                 simple JSON so level track abstract                                 title URL all nice and easy so taking                                 all the Berlin buzzword talks put it in                                 JSON we set know just as the ML                                 frameworks don't work with spreadsheets                                 they also don't work with JSON of text                                 so what do we need well what we mostly                                 need is values between minus                                                                                                                    value for each feature of the thing that                                 we're going to learn on it could be a                                 really sparse one with a handful of                                 nonzero values or it could be a really                                 dense one we're almost everything set we                                 can have loads of features but that                                 means we need loads more memory and bit                                 more CPU but what we've got is text does                                 that look like a bunch of ones and                                 zeroes I mean yeah we could turn that                                 into the ascii hex codes and get a load                                 of ones and zeroes but that's probably                                 not going to be the best feature                                 representation so few wording things so                                 features are the input variables so one                                 specific aspect of the thing that we're                                 trying to predict yet could be someone's                                 height it could be someone's weight it                                 could be the first RGB channel in an                                 image so the                                 we're going to feed in multiple features                                 to our model and a lot of the work goes                                 on to selecting what is the right                                 feature now a label is a single value                                 that all of those features represent so                                 if we feed in all the image data all the                                 different RGB channels those are our                                 features and the label it should say                                 this is a dog this is a cat or we might                                 feed in a whole bunch information about                                 a house and say this is where it is this                                 is how many bedrooms it's got this is                                 what color its painted and I want you to                                 predict that this is how much the house                                 is worth okay                                 training is the process by which we                                 stuff a load of features into a model                                 and it builds model and then inference                                 is where we stuff a load of brand new                                 features in and it gives us an answer                                 that's all new to you have a look at the                                 Google crash course they've got some bit                                 more detail in there and so a little bit                                 more regression is all about predicting                                 continuous values here's the location                                 number of bedrooms give me a answer                                 between                                                                 house is worth classification is for                                 discrete values here's an image dog cat                                 wombat give me one specific answer and                                 then clustering is where you're not                                 actually sure what goes together see you                                 like here's a lot of images kind of                                 figure out which ones are like each                                 other and stuff them together please                                 so that's unsupervised machine learning                                 okay so we've got our text and we need a                                 load of ones and zeros the first thing                                 that we have to do is tokenization so                                 going to turn all of those sentences all                                 of those paragraphs down into little                                 chunks and the very simple way to do                                 that is just split on whitespace and                                 punctuation if that's new to you any                                 kind of Lucene intro talk will will help                                 you there we're going to take it and                                 split down into individual words okay                                 now we're going to build a term                                 dictionary so for each individual term                                 that we've broken up each token we're                                 going to give it an index so basically a                                 giant list dictionary lookup so we've                                 got mouse ran up the clock nice and easy                                 that's the first one so                                               then the mouse ran down                                 the one Mouse to ran three down six so                                 we've got from our text to a load of                                 term indexes we're getting there it's                                 not still ones and zeros but but we've                                 got some numbers so unique index unique                                 ID for each term and then figure out                                 which terms go where now the trick is to                                 invert that so the two main ways out                                 there are one hot encoding where                                 everything is either                                           continuous bag-of-words                                 so the number of times that each word                                 occurs so in the first sentence here the                                 mouse ran up the clock the occurs twice                                 so for the term for the it's gonna be                                   and then everything else is                                           almost there and then our final trick is                                 tf-idf so if a document is going to                                 contain a term a lot so if a document                                 keeps using the word cat then probably                                 when someone searching for cat we want                                 the document with cat in a lot to come                                 up and if we've got sort of phrase query                                 and we're looking for the black cat and                                 almost every document we have contains                                 the word the' then that's probably not                                 that relevant to our query but if almost                                 no documents contain the word cat then                                 that's probably going to be the most                                 important bit in our query so we want to                                 boost the terms that are rare and push                                 down the terms that are quite common ok                                 and then if our document is really                                 really long it's going to have loads of                                 words in it so it's not going to be                                 quite as relevant as a really short                                 document because the terms are going to                                 be more important in that so we want to                                 wait the rarer terms higher the common                                 terms lower want to wait the longer                                 documents lower the shorter documents                                 fire and tf-idf is the simplest way to                                 do this and it is the one that is most                                 commonly implemented in all of the                                 libraries that you'll be working with                                 it's not the best so BM                                               one there were a few more talks at                                 Berlin buzzwords as well in the past                                 about better ways of doing it                                 but it has the advantage of being                                 relatively simple and whatever language                                 whatever framework you are trying to                                 play with it's gonna be there so you                                 don't actually have to worry about the                                 maths behind this the implementation                                 details behind this you can just say                                 import tf-idf require tf-idf and it's                                 just gonna be there it's going to be                                 implemented it's gonna be unit tested                                 and we don't have to worry too much on                                 the details and what's going to happen                                 is we're going to feed it a load of text                                 it's going to tokenize it for us it's                                 going to build the term dictionary it's                                 going to calculate the scores and we're                                 going to get back values between                                                                                                                is really important in that document so                                 we've got our ones and zeros so we're                                 looking good but my next challenge                                 is that I don't actually know what the                                 right answer is I haven't gone round to                                 all of you and asked you which talks you                                 wanted to see and which search terms you                                 were looking for I haven't managed to go                                 round all the new starters on my                                 projects at work and ask them what they                                 were trying to find and see what they                                 needed you can feed that in later but                                 right at the moment I don't know the                                 right answer and so I can't do a simple                                 classification I can't train the model                                 and say here is my query here is the                                 talk that you should match because I                                 don't know it so I have to do a little                                 bit of fuzziness so what we're going to                                 start with is just simple text                                 similarity the same kind of way that it                                 works in Lucene in solar and elastic                                 we're going to say look for terms that                                 are similar to other terms and match                                 based on that so first a little bit of                                 AI we're going to build a classification                                 model for our talks                                 and we're going to feed in the title in                                 the abstract chuck it through a tf-idf                                 then we're gonna ask a model to classify                                 a query cluster it and say which talk is                                 most like our query and then finally                                 we're going to do a matching around and                                 say just based on simple text similarity                                 which are the talks are like that encode                                 it looks like this so build up our                                 tf-idf vector find out how many things                                 are in it so in this case I've got                                     talks and it came out as                                 mm different terms I'm going to                                 calculate the similarity between each                                 talk in each other talk I mean use a                                 multinomial naive bayes really simple                                 but quite powerful ai framework and                                 going to learn and build a model so this                                 probably would have taken in the                                         man year or two to do and we've done it                                 in less than ten lines of Python                                 it is wonderful living in the future and                                 this is all done with scikit-learn which                                 is a Python library for machine learning                                 other libraries exist other languages                                 exist pretty much whatever you're going                                 to want to do it in it's going to be                                 available the reason that we're picking                                 scikit-learn here is that we're                                 optimizing for developer productivity                                 it's a bit slower than tensorflow but                                 it's really easy to follow and it's                                 really easy for me to train my new data                                 scientists in how it works or a                                 tensorflow super fast but you could have                                 a lot of knowledge to start before all                                 the documentation make sense so I'd say                                 if you're new to all this                                 pick something simple later on when                                 you're models got a lot bigger you got a                                 lot more complexity then go down one of                                 the super fast routes but trying to                                 learn machine learning and tensorflow at                                 the same time is a bit more of a                                 challenge and especially if you're not                                 just doing cat or hot dog so yeah here's                                 our code and then finally to do a query                                 we feed in some text we ask the model to                                 predict the talk most like it then we                                 figure out which what the score is so                                 what the similarity is between each talk                                 sort it by their similarity to the talk                                 that I model has suggested and then                                 print those out not the best way but                                 surprisingly effective so let's try a                                 live demo this is hopefully going to                                 work so here is the ipython notebook                                 that we were having a look at earlier                                 and we built up our model already we try                                 and make that text a bit bigger for you                                 so as you see it's the same kind of code                                 that I've just shown on the screen                                 before and then                                 would someone like to suggest a talk                                 title or a query or something for us to                                 work on someone near the front maybe                                 okay let's try no I didn't think it                                 actually took that like demo yep I                                 sometimes get this wrong alright so here                                 we are here are some of the talks that                                 it's recommended for machine learning                                 and it's not that great actually                                 it's okay but it's not amazing so what                                 can we do so the next thing we can try                                 is clustering so a clustering is where                                 we want the machine learning system to                                 figure out what is similar to other                                 things without the help of a label we've                                 got the label that's classification                                 that's dog that's the hot dog easy                                 clustering is where we don't quite know                                 upfront what the right answer should be                                 so the best one to start with is usually                                 k-means so it comes from signal                                 processing so we're going to group end                                 things into K groups and we're going to                                 do that in a way where we're trying to                                 minimize the error so how many clusters                                 so I'm if we've got                                                    you make                                                                every talk is in its own cluster it's                                 not really actually helped us with the                                 grouping though if we have one single                                 cluster and put everything in together                                 then we're gonna have the maximum error                                 and again we haven't really caught                                 anything useful so we sort of need                                 something in between the two where most                                 things are grouped with other things                                 like them and the errors fairly low if                                 you know what your labels are there's a                                 whole bunch of techniques you can use                                 for figuring out which cluster size is                                 best for you                                 but I haven't got that because I don't                                 actually know the right answer so the                                 main two we've got available                                 our average silhouette and gap statistic                                 measures which try and give us an idea                                 of how effective things are fitting in                                 our cluster I'm now the next problem                                 with k-means is it's not completely                                 deterministic it sort of wanders down                                 the graph until it finds a nice low                                 point and stops now it might have                                 wandered a little way down and found a                                 little bump and got stuck in a sort of                                 ditch halfway down the hill or it might                                 make it all the way to the bottom so we                                 have to run it a whole bunch of                                 different time starting in different                                 places and then take the lowest point                                 that it's reached so this is the next                                 thing to know that your machine learning                                 journey is it's not always going to give                                 you the same answer each time you run it                                 and you have to run it a bunch of times                                 to be sure that you've got the best                                 answer possible when you had your nice                                 simple linear program that was going to                                 do some filtering and then give you an                                 answer you ran at once there's the                                 answer you run it again there's the same                                 answer machine learning doesn't always                                 work like that if you have exactly the                                 same seed each time and you're not using                                 the randomness and using the same size                                 steps you should be able to get the same                                 answer each time but if you're picking a                                 different random seed each time you run                                 it may be picking different parameters                                 you could get a different answer out on                                 the same input data using the same code                                 just based on these random seeds that                                 could mean that the next time you run it                                 it gets better                                 or it might get worse if you care about                                 that make a note of the seed and when                                 you save the model save all the seeds                                 and the parameters that we used to build                                 it so you can recreate it otherwise you                                 end up in situation where you build this                                 amazing recommenda model and then you've                                 also come to you and says oh that's                                 wonderful so now we've got a little bit                                 more data can we add that in and you go                                 hmm                                 no your models probably not let me stuck                                 in time so make sure that you've                                 captured everything that went into                                 building it which is not just the data                                 and not just the feature extraction                                 it's also all of the hyper parameters                                 that you fed it like the number of steps                                 and the random seats so that you can                                 recreate it so we're gonna have to run                                 our k-means clustering a few times at                                 each point and then try and see which                                 size cluster is going to work best                                 so encode we're gonna have a range of                                 fluster sizes that we're gonna try and                                 for each one we're going to build a                                 clustering and then we're going to take                                 the silhouette score to try and figure                                 out roughly how accurate it is and then                                 at the end we're going to sort and                                 figure out which one was best so we run                                 that and it's a lot slower than the                                 naive phase and we end up with something                                 like this so as you see start off with                                 only a few clusters the accuracy it's                                 not that great if we end up up in the                                 top right we've got a lot more clusters                                 but then we've got actually less                                 grouping together each cluster has fewer                                 things in and the schools going up but                                 as we move up there's local maxima                                 minima it's not just a nice smooth                                 straight line some cluster sizes are                                 better than others just based on our                                 input data so we're going to look at                                 this and say right well we want about                                    clusters based on finger in the air                                 guess we think that's about as many is                                 going to be the ideal so about                                         in each cluster so then we're looking                                 for somewhere around the                                               we've got a nice high value and then                                 we'll take that as the best one to go                                 for but again there's not a single                                 simple answer you're going to have to                                 start dealing with uncertainty dealing                                 with errors dealing with things that                                 don't move in a nice smooth fashion and                                 you're gonna have to get comfortable                                 with it                                 which for a lot of us used to nice                                 deterministic programs is a bit bit of a                                 change okay so we had a twenty to thirty                                 thousand different dimensions two                                 different terms and we're going to bring                                 that down to about fifty fifty clusters                                 now how do we see how that works on the                                 whole our brains like about a maximum of                                 three or four dimensions so you can have                                 a                                                                    have different colors and that is about                                 the limit that you can really cope with                                 and even that's advanced and two to                                 three dimensions is about what we can                                 cope with but we started off with                                        and we've just reduced it down to                                    we're not gonna be able to get a nice                                 pretty graph that we can look at and say                                 yes that one's awesome                                 one's worse we're going to have to throw                                 away some more information so techniques                                 like t-sne and pca let you do that throw                                 away a whole bunch of the dimensions                                 while keeping some of the information                                 some of the relationships but not all                                 just be aware that you might build a                                 model feed it into a TSN a look at it                                 and say oh that's beautiful we're all                                 set but because it's mushed all the                                 different dimensions down that might not                                 mean it's perfect                                 equally if it looks terrible in t-sne it                                 might still be a good model these are                                 kind of rough visual guides for us but                                 they're not perfect answers so this is a                                 TSN a plot and things are reasonably                                 spaced out so that's probably quite good                                 if everything was really close together                                 then it wouldn't actually have done much                                 clustering the fact that it's managed to                                 group the different talks into different                                 separate clusters seems to give us an                                 idea that it's worked ok and it's group                                 things together but I've got                                    dimensions and I've just squished them                                 into till on the page so I can't be                                 certain so the only thing really to do                                 is to test it now this was our raw data                                 so this is the fifty thousand                                     thousand different dimensions and then                                 I've just plotted it in two dimensions                                 and then the color is the cluster that                                 it put things in now as I've got fifty                                 different clusters and most of you are a                                 long way from the screen and it's a bit                                 hard to tell the different colors how                                 well that's worked because I've got                                    different yellows I've got                                              blues so again you can have lovely                                 pretty graphs but they don't necessarily                                 help you in telling whether or not this                                 has worked if you can get nice clean                                 training data get your humans out there                                 to go and classify things give you                                 proper answers and do proper statistics                                 trying to look at this and be like is                                 that okay hmm maybe so and next a I                                 approach is to identify the optimal                                 cluster size which in this case seems                                 b                                                                        size match the text of our query into a                                 cluster find the center of that cluster                                 and then do regular sort of loosing                                 style text similarity matching from                                 there here's the code so with the                                 k-means clustering it's going to group                                 the stalks together and then it's going                                 to tell us the center of that cluster                                 and that center is going to be in the                                 tf-idf space so it's going to be a                                 virtual talk that is the middle of all                                 of the other talks in the cluster so                                 we're going to match on to that and work                                 from there so does it work I'll get you                                 to shout out some more query terms and                                 we'll see if it's any better so who                                 wants to suggest a query machine                                 learning let's run it okay so here we                                 are so these are what it thinks are the                                 best talks and machine learning how to                                 start company based on machine learning                                 okay that one seems okay what makes                                 machine learning algorithms work data                                 preparation business intelligence yeah I                                 think that one's worked okay what's some                                 reasonable things and some of these                                 don't actually contain the word the                                 exact words machine learning so that's                                 also quite good because we want to do                                 similar ish sings and if you want to                                 have a play the the notebook is there                                 you can type in your very worst queries                                 later date have a play around play                                 around became in size and see how it                                 does okay so the next kind of question                                 is we've got the talk yes we know which                                 year each of the berlin bus web talks                                 are from equally on my RFP responses i                                 know which year we gave those answers to                                 a customer for my training material i                                 know which year that training was given                                 in so what what if i want to include                                 that                                 now we can't easily add it as a feature                                 because when we're doing the query we                                 don't have a year like the talk has a                                 year query doesn't um if we add it at                                 scoring time that's a bit harder because                                 we're not scoring the query we're                                 finding a similar talk to a query and                                 then scoring based on that so if we were                                 to actually had leucine we could do                                 better because of the way it's working                                 and so ideally we want to feed it into                                 the model before we do the scoring now                                 after but if we reduce the tf-idf                                 weights by some factor based on the year                                 we're changing where in the dimensional                                 space they are and we might actually end                                 up pushing talks into the wrong pluster                                 just by reducing the beasts so it's not                                 actually as easy as we'd hope for a next                                 problem is that our data isn't long-term                                 static all being well those can be                                 Berlin bells words                                                  we're going to be then adding in                                 additional talks likewise my training                                 people at work are still writing new                                 training and they keep feeding that in                                 so we're going to have to keep                                 rebuilding our model now let's say I was                                 to force all of you to stay in the room                                 not go to lunch get out a copy of the                                 current program and scribble down what                                 queries you'd have given for each of the                                 talks in the program you'd probably hate                                 me for that but we could get that data                                 and then you'll come back in a year's                                 time and it's not pretty classified                                 because I didn't you talks in and we                                 haven't got the classification so how do                                 we even if we know the right answer for                                 the query today how do we find out what                                 the answer is for tomorrow so we're                                 going to need to factor in some sort of                                 known correct versus brand new data and                                 make sure that the ml isn't prioritizing                                 too much the well-known existing data at                                 the expense of brand new data so the                                 novelty factor which I don't actually                                 have a good answer for right now but I'm                                 just saying if you're thinking about                                 your data and building your models think                                 about what happens when brand new data                                 gets fed in                                 okay a little bit more on the                                 tokenization and stop words you've come                                 from a leucine elastic background you                                 know all about this but if there's some                                 very common words like the a they those                                 kind of words that crop up loads we want                                 to throw them away because it's going to                                 shrink the size of our tf-idf matrix                                 smaller size of the matrix the less                                 memory we need and the faster our                                 calculations are going to run but be                                 aware that you need to have the right                                 stop words firstly for your language and                                 secondly for the domain of your text                                 it's no good taking one that is trained                                 on Wikipedia movie titles and then                                 feeding it into a load of data that's                                 talking about clinical trial processing                                 it's not necessary gonna work so well                                 next thing stemming if we've got a whole                                 bunch of different forms of a word like                                 talk talks talks talking they all                                 represent the same thing so if we've got                                 a language model we can trim all those                                 back to the same word and then we can do                                 an exact match on Nik is talking when we                                 query for talked another interesting                                 trick is engrams                                 so if we're doing word based engrams we                                 can just take every word as a single                                 thing there's word uni Graham's all we                                 can do word by Graham's so if we say                                 Nick is talking then we've got Nick is                                 and is talking those are at to you by                                 Graham's so that lets us do poor man's                                 phrase query it's important because we                                 haven't got a lovely query parser like                                 inducing and if we're doing stuff on                                 characters then we could have trigrams                                 so the program's of hello are h-e-l-l-o                                 ello so if someone has made a typo then                                 the use of these trigrams might allow us                                 to do a correction on their typos but                                 you do lose some of the information                                 about where those three letters come in                                 the word and you might end up matching                                 just on some common pattern like ist                                 that crops up in all sorts of places in                                 english-language words so you might say                                 oh well we're looking for something that                                 has the trigram ist and you                                 got holo two completely irrelevant words                                 coming through so there's some                                 trade-offs we made here final thing                                 whatever you do has to be the same for                                 query and model building if you have                                 different stop words if you've got                                 different tokenization different                                 stemming then your query won't match                                 anymore and you won't get sensible                                 answers okay next thing to wear of is                                 feature extraction and embeddings so                                 even from just                                                         terms come out and even the tf-idf                                 matrix most of the values were                                           a sparse matrix it's can take up a                                 reasonable amount of memory if we're                                 working with Bayesian and k-means there                                 are ok with                                                            I'm reasonably fast they don't mind and                                 neural networks not so much newer                                 networks like                                                           total yeah we can you stop words we can                                 use stemming that might get us down from                                                                                                         more than                                                              using your networks we need to think                                 about ways to reduce the dimensionality                                 now we've already seen a little bit of                                 that with the t-sne we're just so that                                 our eyes could cope with it we reduce                                 the dimensionality down to two but our                                 neural networks are going to need the                                 reduction there and so if this is all                                 new to you                                 I'm scikit-learn I've got a great piece                                 on it and then Google have a good piece                                 on it as well but the the way of turning                                 our Rohrer features into a much smaller                                 set for your network it's generally                                 known as embedding right ok one of the                                 most popular ways of doing the embedding                                 in text is word Tyvek was originally                                 developed by Google and it is actually                                 based on a neural network itself but you                                 tend to use it to pre-process your data                                 before you give it to your own your                                 network and if you give it enough text                                 it can make predictions based on a words                                 meaning so if you say figure out the                                 vector between man and boy okay now find                                 the point                                 in the embedding forewoman apply the                                 same vector distance and see what word                                 we get to and then you get a girl if you                                 trained it right                                 unfortunately there's this thing called                                 bias where if you fed it a lot of human                                 written text and you might say man is                                 too this job and woman is too and you                                 get a sexist answer out because if you                                 fed in sexist training data which                                 actually is most of the training data                                 out there because most of us have our                                 biases when we're writing language then                                 you can end up training a biased AI but                                 assuming that you've got the right kind                                 of thing you get lovely things like this                                 where it's figured out the relationships                                 between a whole bunch of words and how                                 they interact with each other and done                                 it in a vector space so that when we                                 feed that into the neural network and we                                 train and we do queries it can figure                                 out some parts of speech figure out some                                 parts of relationships and give us good                                 answers                                 so it's worth having a play with words                                 avec if you want to do anything around                                 this and there's a whole bunch of                                 implementations of it available and                                 Google have got quite a good intro to it                                 keep neural networks who hears come                                 across neural networks a little bit of                                 audience participation before you fall                                 asleep okay so the system is built up of                                 a whole bunch of layers you have an                                 input layer where you're feeding your                                 features in an output layer which is                                 where you want to get your answer out                                 and then a bunch of hidden layers in                                 between that the system has figured out                                 what they should be so you go to the                                 neural network system you say this is my                                 input that's the prediction I want you                                 to make you figure out how to glue all                                 of these different bits together for me                                 and then where we go I'm the more layers                                 you have potentially the better answer                                 you're going to get but the more work                                 that's going to go on in the training                                 and the more inputs and outputs you have                                 again the more work that it has to do in                                 figuring out the right layer think about                                 what kind of outputs you're going to                                 want if you need something that's going                                 to be a cat or a dog you need to                                 constrain your outputs                                 that the probability of cat or dog adds                                 up to                                                                what's in this image then something that                                 comes back and says                                                                                                                          but if what you really wanted to know is                                 is this the right talk you want a single                                 answer the more layers you put in the                                 longer it's going to take to train and                                 this is an iterative process you keep                                 running it suggesting that it adds some                                 extra layers in or add some extra                                 neurons in or throw away some neurons                                 and then urea ate the score and you keep                                 going until the score basically doesn't                                 change okay now if we know what the                                 model should be predicting then we can                                 train and we can evaluate the model and                                 let's say we've got                                                     got a load of answers now if we train                                 the model on all of that input data we                                 have no way of knowing how well it's                                 done because we've got nothing left to                                 test to it if we train it on three                                 pieces of input data and keep                                           to test with we can be really sure how                                 well it's done but it's not going to                                 have learnt much so we're going to have                                 to do this trade-off of our input data                                 which is nicely labeled into the                                 training part and the testing part if we                                 give the model too much data then it can                                 over fit and it can learn specific                                 attributes of our test data that's not                                 present in the rest of our real data and                                 another thing to look at is if you've                                 not got that much data you can split                                 shuffle it all up do a split for test                                 and train get an answer shuffle it                                 around again do the same test and train                                 and make sure that you're getting                                 roughly the same kind of accuracy each                                 time that gives you an idea that your                                 model is fairly stable and it's not                                 doing too much overfitting                                 so that's cross validation hyper                                 parameters the hyper parameter is                                 anything that we're tuning or selecting                                 and changing in the machine learning                                 that's not part of the data in the                                 features so for our k-means example one                                 of the type of rameters was the number                                 of clusters that we're breaking it down                                 into and quite often it's going to be                                  the seeds the                                  that's the number of layers how much to                                  change between iterations all those kind                                  of things if you pick the wrong                                  parameters your machine learning or get                                  stuck like half way down the hill in a                                  little little bump yeah it might just                                  take forever to complete so the hard                                  bits in machine learning tend to be                                  picking the appropriate features and                                  picking the appropriate hyper parameters                                  it's a lot of work a lot of                                  experimentation and Google did a study                                  last year on the hyper from to picking                                  and they said it is cheaper to have a                                  data scientist in California picking -                                  parameters for them to have a whole                                  extra set of machine learning trying to                                  optimize the hyper parameters of another                                  machine learning but that was a year ago                                  and probably in California now it's                                  betters have an AI tuning your X today                                  is but anywhere else in the world                                  probably be better off with humans maybe                                  in five years that will have changed                                  again but for now we have to have the                                  humans looking at it and saying or I                                  think I'm going to want roughly this                                  kind of many steps based on my past                                  experience and then my AI should                                  converge on the answer quite quickly                                  okay need to think about errors and if                                  we've got something that's supposed to                                  say cancer not cancer and we feed it the                                  the diagnosis and the health records of                                  a person and it could say they have                                  cancer and they do so that's a true                                  positive could say they're clear from                                  cancer and they don't that's a true                                  negative or I could say nope they're all                                  clear no cancer and they've really got                                  it so that's false negative or it might                                  say someone who's clear that they've got                                  cancer so that's a false positive now if                                  you're detecting cancer what happens if                                  we give someone the all-clear that's                                  actually got a tumor or what happens                                  someone who doesn't have a tumor and                                  we've just given them a load of                                  chemotherapy so you've got to think                                  about the impact of your model and the                                  impact on the errors of your model other                                  things think about is the precision the                                  recall and the confidence so the                                  precision is how much correct results                                  correct values in our results the recall                                  is how many is the correct results we                                  actually gave to you so if we said here                                  are five answers                                  and four of them were correct then                                  that's an                                                              answers and there are actually a million                                  possible answers then maybe our records                                  pretty terrible a lot of the models can                                  tell us how sure they are that they                                  could be knew the right answer so that's                                  the confidence so it might say I think                                  this is a dog but I'm only five percent                                  sure and you say or maybe we won't trust                                  that equally it says that's definitely a                                  dog                                                                                                                                probably okay okay already mentioned                                  biases a little bit your data sets are                                  probably gonna be biased certainly if                                  you're working with text written by                                  humans it's definitely gonna be biased                                  your a I can find extra features so you                                  might say oh I'm a bit worried about the                                  bias in my input data song at a high                                  gender but I'm gonna leave in the name                                  of the author and you're AI will be like                                  oh I'm gonna learn what female names                                  look like equally you say oh I'm a bit                                  worried about the racism inherent in my                                  data so I'm going to leave out the race                                  field but you've left in the postal code                                  or zip code or the first elementary                                  school and it turns out in your city                                  people of different races tend to live                                  in different places or you're a is just                                  learn that just figure out the race even                                  though you didn't do it to it so think                                  about the bias think about how people                                  are going to use your data how you can                                  use the model and make sure that you                                  correct for it or make them aware of it                                  and don't reuse the model there was a                                  really good intro yesterday it'll be up                                  on YouTube very soon about the bias and                                  text and the ways to correct for that                                  okay natural language processing I'll                                  skip over that we have really got time                                  that the slides are be up                                  dr. QA there was a really good talk                                  yesterday on this it's a thing developed                                  by Facebook for pulling out specific                                  answers from text trained on Wikipedia                                  looks really interesting haven't played                                  with it yet only heard of it yesterday                                  but that looks good final question would                                  leucine have done better probably for my                                  specific case probably a fully tuned                                  leucine elastic cluster something like                                  that would have done better it certainly                                  would have had a better ability to scale                                  up to really really large datasets                                  almost all the ML techniques need                                  everything a memory leucine doesn't but                                  as a way to teach a lot of my                                  statisticians about AI n ml using the                                  kind of data that they understand and as                                  a way to put some of the                                  ai nml stuff into production in my                                  company it was wonderful                                  so it was worth doing just not worth                                  doing as a pure search project so if                                  you're looking for a way to learn AI and                                  you've got text this is really great if                                  you think this is going to replace                                  elasticsearch it's probably not okay                                  both Microsoft Anna and Google let you                                  have a play with all of this stuff                                  batteries included there's some links                                  there and slides will be up on the                                  website soon but I've got a whole bunch                                  of resources in here if you want to                                  learn some more okay everyone is hungry                                  run away now anyone who wants to ask me                                  questions we can do any into a lunch                                  break okay thanks                                  [Applause]
YouTube URL: https://www.youtube.com/watch?v=uYVdm8t241o


