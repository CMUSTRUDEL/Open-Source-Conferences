Title: Berlin Buzzwords 2019: William Benton – Band-Aids don’t fix bullet holes #bbuzz
Publication date: 2019-06-19
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	William Benton talking about "Band-Aids don’t fix bullet holes: Repairing the broken promises of ubiquitous machine learning".

Buoyed by expensive industrial research efforts, amazing engineering breakthroughs, and an ever-increasing volume of training data, machine learning techniques have recently seen successes on problems that seemed largely intractable twenty years ago. However, beneath awe-inspiring demos and impressive real-world results, there are cracks in the foundation: ordinary organizations struggle to get real insight or value out of their data and wonder how they’ve missed out on the promised democratization of AI and machine learning.  

This talk will diagnose how we got to this point. You’ll see how the incentives and rhetoric of software and infrastructure vendors have led to inflated expectations. We’ll show how internal political pressures can encourage teams to aim for moonshots instead of realistic and meaningful goals. You’ll learn why contemporary frameworks that have enjoyed prominent successes on perception problems are almost certainly not the best fit for gleaning insights from structured business data. Finally, you’ll see why many of the solutions the industry has offered to real-world machine learning woes are essentially “bandages” that cover deep problems without addressing their causes.

This talk won’t merely offer a diagnosis without a prescription; we’ll conclude by showing that the way to avoid disappointing machine learning initiatives in the future isn’t a patchwork of superficial fixes to help us ignore that we’re solving the wrong problems. Instead, we need to radically simplify the way we approach learning from data by embracing broader definitions of “AI” and “machine learning.” 

Organizations should prioritize results over emulating research labs and practitioners should focus first on fundamental techniques including summaries, sketches, and straightforward models. These techniques are unlikely to attract acclaim on social media or in the technology press, but they are broadly applicable, allow practitioners to realize business value quickly, produce interpretable results, and truly democratize machine intelligence.

Read more:
https://2019.berlinbuzzwords.de/19/session/band-aids-dont-fix-bullet-holes-repairing-broken-promises-ubiquitous-machine-learning

About William Benton:
https://2019.berlinbuzzwords.de/users/william-benton

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              thank you so much good morning ever                               Benten it's an honor to be here at                               buzzwords and I'm really grateful for                               your time this morning after Isabelle's                               excellent keynote I feel like I should                               point out that I work for Red Hat but I                               don't speak for Red Hat and in                               particular I'm not speaking for Red Hat                               in these talk in this talk these are my                                own opinions the subtitle of this talk                                we have a colon and the title of this                                talk the subtitle is repairing the                                broken promises of Icarus machine                                learning I suspect maybe a couple of you                                have been to a talk or two where someone                                says hey we're sort of not getting what                                we expected out of machine learning it's                                that fair heard that once or twice so                                this is this is also such a talk right                                but I'm been to enough talks that have                                started with that premise that I hope I                                can offer you something new as a bonus                                since this talk is scheduled at the                                beginning of the day I'll be                                recommending some other talks you should                                go to today that are related to themes                                of this talk I feel like I owe it to all                                of you though to explain the title of                                the talk that comes before the colon                                know so I'll do that first quick show of                                hands who here has heard of Taylor Swift                                okay so so a couple of you just a                                level-set Taylor Swift is a relatively                                popular American pop star it's always                                difficult to distill in artists over as                                succinctly but from                                                  Swift deals with three themes the thrill                                of new love the heartbreak of lost love                                and unending rage for those who've                                wronged her a song that's in the latter                                category is called bad blood which deals                                in fairly general terms with an unnamed                                antagonist who has wronged MS Swift at                                some point in the past you might wonder                                what a hit single from four years ago                                has to do with the title of this talk or                                with machine learning but we're getting                                there                                shortly after bad blood was released I                                was at a truly excellent machine                                learning conference the details don't                                matter but as I was waiting for the                                keynotes to start I was looking at a                                slideshow of sponsor logos and bad blood                                was playing in the background the chorus                                includes these lines baby now we've got                                bad blood you know it used to be mad                                love but now we've got problems and I                                don't think we can solve them I was                                paying a lot of attention to the news                                that summer and it occurred to me that                                two of the biggest sponsors were                                mutually engaged in a really acrimonious                                lawsuit and one of the sponsors had                                recently rebuffed an acquisition offer                                from another                                and while every individual I knew at any                                of these companies was a friendly person                                technically excellent the public faces                                of their employers were all seriously at                                odds                                identifying inappropriate background                                music as a minor hobby of mine which                                makes wedding receptions disastrous as                                Miss Swift got to the bridge in which                                she points out that the relief her                                antagonist has offered is grossly                                insufficient band-aids don't fix bullet                                holes I reflected on whether or not this                                was inappropriate background music or                                actually just a perfect ironic                                commentary on the collection of                                technology industry frenemies that were                                on the big screen behind the stage I was                                amused enough to write this down in my                                notes to put in my trip report but the                                conference was starting so I stopped                                thinking about it                                the first keynote presenter opened by                                asking who in the audience was                                disappointed with how they're machine                                learning initiatives were working out I                                was fairly close to the front of the                                room but almost everyone I could see                                raised their hands he acknowledged this                                response and immediately launched into                                an extremely impressive and polished                                demo I'm fictionalizing the details                                because the details aren't important but                                 it was one of these things where you                                 have a bunch of vector embeddings of a                                 bunch of wildly different domains and                                 you combine them to make a parent magic                                 in a way that doesn't totally make a lot                                 of sense like we're gonna have separate                                 embeddings for clothing and books and                                 pastries and we have a way to combine                                 them so that if you tell me what your                                 favorite t-shirt is in the name of the                                 last book you read I can recommend a                                 doughnut to you the actual application                                 was sort of dubious but the results were                                 impressive and the really amazing part                                 was that they had library to support to                                 serve these kinds of queries in a very                                 small amount of code and so you see the                                 presenter concluded we're actually going                                 to be able to get value out of machine                                 learning after all and it hit me at that                                 point being able to easily deploy a                                 bunch of predefined models is actually                                 not what's keeping me from getting value                                 from machine learning it was sort of a                                 minor detail that seemed to be                                 addressing the wrong problem I again                                 thought of miss Swift what if the real                                 commentary of bad blood for that morning                                 was not about the technology companies                                 that appeared to hate each other even                                 though all of their employees were                                 friendly and excellent people but about                                 how we're not really addressing the deep                                 problems of using machine learning to                                 create business value and we're solving                                 the wrong things                                 so in the rest of the talk I want to                                 look at why machine learning systems are                                 hard and see how we're using the wrong                                 tools how we've created and are                                 responding to the wrong incentives and                                 how we're not really solving the deep                                 problems we should so practitioners know                                 that there's more to machine learning                                 than just training a model and launching                                 it into production                                 there's an entire workflow that                                 practitioners follow in order to solve                                 real problems and the end result isn't                                 just a model or a way to train a model                                 it's a whole pipeline we start by                                 formalizing the problem we're trying to                                 solve we collect label clean and                                 structure our data before evaluating                                 different approaches to make sense of it                                 we ensure that our results are                                 defensible that we haven't overfed our                                 training set and we're going to deploy                                 that model into production as part of an                                 application and monitor its performance                                 over time now a careful practitioner                                 could find a problem at any one of these                                 steps and realize that he or she had to                                 go back and revisit a decision made in                                 an earlier step that's why this workload                                 diagram has these arrows going back                                 because sometimes these are things we                                 need to fix some parts of this workflow                                 directly inform or even provide the code                                 that we're going to deploy into a                                 production system right like our feature                                 engineering step is going to sort of                                 directly inform a feature extraction                                 stage in a production pipeline our model                                 training and tuning approach is going to                                 represent another stage in a production                                 pipeline and so on some of these                                 components can we can turn more or less                                 directly into code and we can                                 incorporate these stages into a pipeline                                 that takes raw training data extracts                                 features trains a model and then                                 ultimately move with that model into                                 production we can reuse some components                                 this pipeline for scoring like the data                                 transformation and feature extraction                                 routines for example as well as the                                 model we trained in the training part                                 with the ultimate goal of putting them                                 all into production extracting feature                                 vectors from more or less raw data                                 making predictions tracking metrics                                 about our predictions so we have an idea                                 when we're going wrong and archiving the                                 data we saw in the predictions we made                                 so we can explain ourselves when                                 regulators or stakeholders want to know                                 why we did what we did in the future                                 however we put these pipelines into                                 production as parts of larger software                                 systems that use machine learning to                                 solve real business problems                                 these systems are even more complex than                                 the pipelines right if you're trying to                                 identify which products to recommend to                                 when customers are about to checkout in                                 an e-commerce site at massive scale if                                 you're trying to decide whether or not                                 to make a securities trade in a very                                 tiny window with someone else's money if                                 you're trying to decide whether or not                                 to decline an electronic payments                                 transaction because it's fraudulent you                                 have a complex system and machine                                 learning is a part of that system but                                 it's a small part that's a part that can                                 make every other part more difficult if                                 this diagram looks familiar it's because                                 I'm borrowing it from a great paper                                 hidden technical debt in machine                                 learning systems which like Taylor                                 Swift's bad blood was released in                                      starts with the premise that machine                                 learning techniques are easy to develop                                 but machine learning systems are hard to                                 maintain because they have many moving                                 parts all of which are sort of listed in                                 boxes on this size slide and those sort                                 of relative sizes are supposed to                                 indicate maybe how important they are                                 that tiny box in the middle is actually                                 the machine learning code all of the                                 other things ultimately represent more                                 engineering effort but the interesting                                 thing about the machine learning code in                                 the middle is that it can make                                 accidental dependencies or other bad                                 engineering properties of these systems                                 more important in ways that are hard to                                 discover for example your future                                 extraction routines are probably                                 uncomfortably tightly coupled to your                                 data ingestion pipeline if the way you                                 structure your raw data changes your                                 code might break but the more important                                 question is will you notice                                 practitioners are familiar with the                                 phenomenon of data drift where the                                 distribution of data that we observe in                                 production materially diverges from the                                 distribution of data that we trained on                                 so the model that's performing                                 reasonably well at one point might start                                 performing adequately and then poorly a                                 little while later the hidden technical                                 debt paper points out that data drift is                                 actually a special case of a general                                 phenomenon of entanglement in machine                                 learning systems if we add a future or                                 remove a feature we've potentially                                 created a change that impacts the whole                                 system similarly if we change anything                                 about the way we've trained the model                                 like hyper parameter settings or even a                                 random seed we could potentially                                 preserve some other component that was                                 accidentally depending on a detail we                                 didn't think was important about how we                                 were behaving                                 another problem the hidden technical                                 debt paper identifies is the problem of                                 glue code many times these pipeline                                 components are developed by teams other                                 than the teams who ultimately put them                                 into production and thus they're treated                                 as black boxes these boxes need to be                                 integrated together with so-called glue                                 code that orchestrates these stages                                 manipulates the output of one stage that                                 it's an appropriate input for the next                                 stage and so on so we know that the sort                                 of actual machine learning parts of this                                 the feature extraction and the model                                 training requires skill insight and care                                 but it's really easy to overlook how                                 difficult it can be to write and                                 maintain robust glue code I even did it                                 in this slide right there's a lot of                                 complexity hidden in those arrows as we                                 develop more and more complicated                                 pipelines maybe we have a bunch of                                 branching experimental paths and rarely                                 exercised branches not only we have more                                 of this difficult in Braille glue code                                 but we also have a much more difficult                                 system to test I mean if you think about                                 just writing a regular program if you                                 have a function that gets beyond a                                 certain length and has a lot of control                                 flow in it right it's difficult to read                                 it's nearly impossible to reason about                                 and almost everyone has had the                                 experience of sort of looking at a                                 program scratching your head making a                                 cup of coffee going for a walk in coming                                 back and still trying to figure out how                                 you got to some impossible state right a                                 pipeline in which data can flow down one                                 of exponentially many paths some of                                 which may only be exercised rarely in                                 the service of building several                                 complementary models is prone to many of                                 the same types of challenges failures                                 and maintenance headaches except that's                                 not just a bunch of ifs and a single                                 function it's a bunch of separate                                 programs written in different languages                                 and probably running on different                                 computers in any case this paper has                                 been widely read and even more widely                                 cited if you haven't read it you should                                 take a photo of this QR code with your                                 phone and then email the PDF to a device                                 that you can stand actually reading a                                 paper on I wanted to mention this paper                                 because it's a really concise                                 presentation of some serious problems                                 but I'm not gonna spend a lot more time                                 in the stock discussing this paper but I                                 wanted to introduce it and spend some                                 time on what I think is a really                                 interesting problem which is how people                                 have taken the message of this paper and                                 responded to it I've seen this diagram                                 cited in many contexts but the most                                 fascinating for me is when someone says                                 hey                                 remember that hidden technical debt                                 paper it argued that these systems are                                 complex and have a lot of subtle                                 dependencies which is why you need to                                 buy our product or adopt our open-source                                 project sometimes it would be a point                                 product that addressed one of these                                 responsibilities other times it would be                                 a system that addressed several of these                                 responsibilities but in every case this                                 frame ignores that one of the key points                                 to the paper which is that it's not the                                 components themselves necessarily that                                 add the complexity but the interactions                                 the couplings the entanglement between                                 these components so point solutions are                                 really important but they don't address                                 many of the problems of machine learning                                 systems I could have the best model                                 serving infrastructure in the world for                                 example I would love to have that and I                                 still not have a solution for continuous                                 data quality monitoring and point                                 solutions don't address the problem of                                 gluing these components together and                                 making the resulting system more robust                                 we'll come back to this concrete problem                                 later in the talk so we've talked about                                 why machine learning is hard but now I                                 just want to talk about what we're doing                                 wrong I want to start that with the fact                                 that many times we choose the wrong                                 tools first but let's get some                                 background on a particular class of                                 problems classical computer vision tasks                                 depend on building a database of image                                 features colors textures shapes and so                                 on and encoding them in a way so that                                 the thing we want to identify is                                 identifiable so here we have a face with                                 heart eyes emoji we want it to be                                 identifiable even if we rotate it up to                                 a certain point or scale it right so a                                 deep learning turns out to be really                                 great for problems like this sort of                                 computer vision problems speech and                                 language processing get really amazing                                 accuracy I remember taking computer                                 vision in grad school and being shocked                                 at the sort of accuracy you could get a                                 publishable result in computer vision                                 before the deep learning revolution with                                 but the promise of deep learning is that                                 we can to some extent elide this feature                                 engineering work that we do in our                                 pipeline because the network is sort of                                 forced to learn useful features in order                                 to generalize from more or less raw data                                 so if you get your objective right the                                 claim goes the features will sort of                                 fall out of the early layers in the                                 network                                 put another way at a high level what's                                 going on is that we have a neural                                 network with a bunch of layers each of                                 which is sort of like a separate model                                 they're all trained together                                 and because each layer can convey less                                 information than the previous one                                 there's this filtering effect so as a                                 consequence the earlier layers wind up                                 looking a lot like feature extractors                                 and the later layers wind up looking a                                 lot like a traditional model that uses                                 those features the last layer is where                                 you get our prediction which is what                                 kind of thing is this that we're looking                                 at so deep learning has had enormous                                 successes for vision speech recognition                                 and language processing and people have                                 assumed that these successes would be                                 easy to replicate and other problem                                 domains but the same properties that                                 make it possible to perform perceptual                                 tasks impressively well with less time                                 spent on feature engineering have really                                 bad software engineering consequences                                 for machine learning systems done                                 properly manual feature engineering                                 means that we've thrown away irrelevant                                 details and kept the things that                                 generalize well and provide some signal                                 ideally with some insight about the                                 problem space a technique that                                 encourages us to provide all available                                 information and let it algorithm sort it                                 out may make it easier to get results                                 quickly but it makes it much more likely                                 that our system has some accidental                                 dependencies on irrelevant details I'm                                 gonna mention another paper now called                                 intriguing properties of neural networks                                 which had a fascinating result the                                 authors showed that neural networks                                 don't necessarily maintain a smoothness                                 assumption what this means is that small                                 changes in the input model should result                                 in small changes to the output of the                                 model in particular that a small input                                 change shouldn't change how we classify                                 an image but this doesn't always hold                                 true for neural networks and deep                                 learning like most widely used classical                                 computer vision techniques these deep                                 learning models are robust in the face                                 of rotation and scaling I can't fool you                                 this is still a face with heart eyes                                 emoji but one of the very interesting                                 consequences of this paper is that                                 perturbing an image with almost                                 imperceptible noise can cause it to be                                 misclassified and even more                                 interestingly it's possible to construct                                 this noise in such a way that gets you                                 misclassified with a particular result                                 so in this case we've optimized noise to                                 turn this face with heart eyes emoji                                 into something that gets classified as a                                 stack of pancakes emoji and I hope no                                 one in the audience sees that on the                                 right as a stack of pancakes doesn't                                 look like a stack of pancakes                                 so I think I'm gonna make what I hope is                                 a non-controversial claim which is that                                 in no other neighborhood of software                                 engineering would we accept a technique                                 that worked with cooperative inputs but                                 failed inexplicably in the case of any                                 attempted subversion imagine a network                                 service that worked really well for                                 inputs it expected but failed                                 catastrophically on others if you're                                 thinking that this sounds like the setup                                 for any post mortem analysis of a                                 security bug ever I basically agree with                                 you I don't want to present this as an                                 insurmountable obstacle trying to work                                 around the brittleness of deep learning                                 networks in the face of adversarial                                 examples in many cases by constructing                                 and using these examples while training                                 the network's is a focus for researchers                                 and practitioners in this space but if                                 you're thinking of rolling up your                                 sleeves and becoming a deep learning                                 researcher to address these limitations                                 so you can get good results on a new                                 problem                                 you'll want to consider my next point                                 which is that you probably can't afford                                 to do a lot of innovation in this space                                 for many large problems identifying the                                 trade-offs between different hyper                                 parameter settings and architectures is                                 literally the sort of task that                                 governments build supercomputers to                                 solve not long ago the paper I'm linking                                 here reports hundreds of experiments to                                 evaluate network architectures and hyper                                 parameters for machine translation you                                 might assume someone had done this                                 already but they hadn't and the reason                                 is actually in the abstract because it                                 took                                                                   you're doing this kind of work in the                                 public cloud an hour of GPU time is one                                 to three dollars that quickly turns into                                 real money if you're doing this work on                                 your own GPU great take                                                  years and divide it by the number of                                 GPUs you have and you get to this get to                                 this number when I proposed this talk                                 I'd originally plan to discuss another                                 potential pitfall of deep learning which                                 is that classical machine learning                                 techniques actually outperform deep                                 neural networks on a range of really                                 interesting problems but when the                                 program came out I saw that szilárd                                 would be speaking on gradient boosting                                 and I said well I can just assert that                                 classical machine learning techniques                                 outperform deep neural networks on a                                 range of interesting problems and refer                                 you to his talk which is this afternoon                                 and the poly is I'll tell you a for a                                 more complete argument                                 but the other thing is really sort of                                 philosophical by focusing on deep                                 learning we're focusing on the sort of                                 smallest and easiest part to get right                                 of the whole system right we want to                                 move our focus to more of the system so                                 why do we use the wrong tools I think                                 part of it is that the wrong incentives                                 have led us to salt lead led us to solve                                 their own problems and the first bad                                 incentive is one that applies to us as                                 practitioners it's a social one right as                                 an industry we tend to over ordered                                 complexity and the esoteric it's much                                 cooler to say I trained this enormous                                 and incomprehensible neural network                                 while heating my house with exhaust from                                 the compute farm than it is to say hey I                                 was able to improve our overall business                                 metrics to exceed our goals by employing                                 a linear model and fitting the                                 parameters takes                                                         and we can explain why the model made                                 the decisions that made of stakeholders                                 or regulators asked us to justify                                 ourselves part of this incentive                                 structure is totally salutary right it's                                 that you know we're engineers we're                                 scientists we want to reward curiosity                                 and innovation we're excited about new                                 things we want to see how they fit into                                 our toolboxes we want to really                                 understand what the trade-offs are but                                 we can't privilege novelty and                                 innovation over elegance especially when                                 a simple solution works just as well or                                 has far more appealing engineering                                 trade-offs the second set of incentives                                 I want to discuss are the incentives of                                 vendors and even of open source                                 communities if you remember that diagram                                 from the hidden technical debt paper                                 showing how all these components fit                                 together and we discussed how focusing                                 on machine learning code that tiny box                                 in the middle leads to bad engineering                                 outcomes I also mentioned earlier in the                                 talk that if we focus on just any single                                 box we're really solving the wrong                                 problem right and a lot of people have                                 repurposed this diagram to say yes my                                 single box is actually the place where                                 we should be focusing our efforts and                                 you notice that the focus depends on                                 who's talking right it turns out to be                                 the areas that benefit particular                                 vendors or particular projects and this                                 has only gotten worse as machine                                 learning has become a hotter and hotter                                 area with more organizations putting an                                 AI spin on their technologies storage                                 vendors would love it if your machine                                 learning initiatives were successful but                                 they get paid if you buy more capacity                                 so their incentives are to increase data                                 gravity get you tied into an ecosystem                                 and machine learning is one way to do                                 this cloud vendors are happy when your                                 machine learning initiative succeed but                                 they get paid when you consume more of                                 their resources so their incentive is to                                 make it as easy as possible to consume                                 more of their resources specialized                                 hardware vendors delighted when your                                 machine learning incentives succeed but                                 they get paid when a larger swath of the                                 compute industry as a whole looks more                                 like the high-performance computing                                 market so their incentive is to                                 encourage techniques that require                                 massive computing power so in the case                                 of storage vendors and other platform                                 vendors in general they're incentivized                                 to help you build tooling to make and                                 manage increasingly complicated data                                 processing pipelines to the extent that                                 it makes it more attractive to use a                                 platform or a storage solution for that                                 data for technical and strategic reasons                                 this tooling often tightly couples the                                 pipelines you deploy to the details of a                                 particular storage solution or platform                                 and this kind of entanglement can lead                                 to maintenance difficulties and make                                 migrating your systems or applications                                 close to impossible cloud vendors almost                                 every cloud vendor is gonna offer you                                 tooling for hyper parameter tuning right                                 I need to run a bunch of experiments at                                 once I need to figure out which model is                                 the best I need to use two hundred and                                 fifty thousand hours of GPU time as                                 simply as possible right because if you                                 need to run all these experiments you'll                                 be leasing more capacity from them some                                 cloud vendors are even offering this                                 sort of automatic machine learning                                 tooling right where you evaluate a bunch                                 of different models and a bunch of                                 different hyper parameters at once for a                                 particular problem it's supposed to take                                 the human out of the loop right you                                 don't even have to think about what kind                                 of model you need it's an interesting                                 research problem but it also makes it                                 really easy to consume a lot more                                 compute resources in the public cloud                                 interestingly cloud vendors have an                                 incentive to make it easier to build                                 complete systems right tooling that                                 provides the solution in this space                                 makes their offering stickier tightly                                 coupled applications to a particular                                 cloud keeps their customers happy and                                 keeps them coming back it's too bad that                                 cloud providers don't also have an                                 incentive to make my bills easier to                                 monitor and understand                                 another innovation that's come from                                 cloud vendors and and also from a lot of                                 startups in this space is the idea of a                                 model marketplace a way to sort of                                 purchase pre-trained models or access to                                 model services to address particular                                 application concerns I don't want to                                 train an object recognition model I want                                 to pull one off the shelf and treat it                                 as a black box so there's a lot of                                 interesting work in this area too but we                                 need to take care that if we use these                                 kinds of solutions we aren't just making                                 it easier to deploy something that will                                 be hard to manage right if we buy a                                 model as a black box that makes implicit                                 assumptions about features we can't                                 really understand or debug what's going                                 on with it so specialized hardware                                 vendors have done a lot of research and                                 engineering to make training complex                                 models faster in many cases orders of                                 magnitude faster than without special                                 hardware support and this has led to a                                 tremendous amount of applied research                                 that's dramatically expanded the                                 applicability of for example GPUs but                                 it's also made our systems more complex                                 to configure more complex to program and                                 more expensive both in terms of hardware                                 cost and in terms of environmental cost                                 just so you know I'm not throwing stones                                 you might even see someone who works for                                 a system software vendor arguing that                                 Linux containers and container                                 orchestration wind up solving a lot of                                 problems for machine learning systems                                 everyone has a perspective and                                 everyone's perspective is informed to                                 some extent by their incentives right                                 even if they aren't speaking for their                                 employer and official capacity the title                                 of that talk was Gil scott-heron Taylor                                 Swift as a step in that direction I                                 don't know which one but it's a step so                                 this isn't to say that advances that are                                 aligned with vendor incentives aren't                                 valuable but far from it like all the                                 projects I've mentioned are valuable and                                 they represent awesome engineering                                 effort I'm just encouraging everyone to                                 really consider for any project what                                 needs it addresses how the incentives of                                 the organization or community they                                 created it aligned with the incentives                                 of our practitioner community and what                                 additional work we'll need to do to each                                 fill out a whole picture so we have                                 problems right but I don't think it's                                 that we can't solve them I think it's                                 that we're solving the wrong ones                                 the problem isn't that it takes too much                                 Python code too many lines of code to                                 deploy a deep learning model that has                                 bad engineering properties anyway the                                 problem isn't that we don't have enough                                 canned models to solve interesting                                 problems but that won't produce                                 defensible results or will lead to a                                 pipeline we can't debug and the problem                                 isn't necessarily that we don't have a                                 way to consume public cloud                                 capacity as quickly as possible so we                                 spent some time talking about why                                 machine learning systems are hard to                                 build and we diagnosed some places in                                 which the wrong tools are employed by                                 people and organizations to solve their                                 own problems in the last section I want                                 to discuss the problems we should be                                 solving as a community and look at how                                 we can make these systems easier to                                 maintain just as easy as area to build                                 so I don't have a lot of text on my                                 slides in general but I don't want to                                 read this to you so I'm gonna give                                 everyone a second to read this quotation                                 from my Edgar Dykstra got it okay so                                 this infamous quote we see it's almost                                 exactly                                                                  funny and true it's funny because I hope                                 we assume that the median pure                                 mathematician is a much better                                 mathematician than the median programmer                                 like like not even close right but I                                 think that's largely because better                                 abstractions better tools and better                                 engineering practices have democratized                                 access to programming in a way that we                                 have not democratized access to pure                                 mathematics there's a wide range of                                 people programming and creating valuable                                 things with computers because it's                                 easier to get started and it's easier to                                 figure out where things have gone wrong                                 so we can fix them but it's true because                                 programming is still actually really                                 hard like I wrote my first programs when                                 the clash we're still recording good                                 albums and I still occasionally get                                 confused or stumped Pat shame is Swift I                                 think we actually can solve the problems                                 of machine learning systems to make them                                 more robust and more maintainable and I                                 think we can take cues from how we've                                 begun to address these problems for                                 software development consider how the                                 dominant user interface for programming                                 has changed since Dijkstra wrote that                                 memo from decks of punch cards that ran                                 overnight and returned a print out of                                 either results or errors to compiled                                 languages in which we could build                                 programs interactively to Terminal two                                 languages that support immediate                                 interaction which can be compiled and                                 run phrase by phrase as we type with                                 live feedback for errors one lesson of                                 better and faster feedback in                                 programming language user interfaces                                 which is not going to be news to anyone                                 who's taught a programming class before                                 is that you don't need to be perfect in                                 advance if you can find out right away                                 that something is wrong                                 by making the dynamic behavior of our                                 machine learning systems easier to                                 observe we can make it easier to                                 understand which is an important first                                 step to making it possible to debug when                                 things go wrong this is a general                                 distributed systems problem right and                                 it's actually something that several                                 open source communities are working on                                 and it turns out that taking advantage                                 of general-purpose observability and                                 monitoring functionality turns out to be                                 a really great potential advantage of                                 putting your machine learning systems on                                 kubernetes we should also build better                                 abstractions for machine learning                                 systems overall another great sort of                                 computer science quote is Alan Perlis                                 saying that a programming language is                                 low-level when its programs require                                 attention to the irrelevant if you think                                 about programming think about the                                 spectrum between machine language                                 assembly language low-level languages                                 and high-level languages just whatever                                 definitions you have for those concepts                                 right now if you think of our machine                                 learning systems the last thing you                                 built with machine learning how many                                 your elleven are accidental details did                                 you have to think about with the tools                                 you used another way we can make things                                 simpler is to really focus on feature                                 engineering and get rid of useless                                 features you probably have an idea that                                 most of your features aren't containing                                 any information right like if you run                                 PCA on your features you could probably                                 get rid of                                                            will perform just as well but you also                                 ideally have some domain knowledge maybe                                 you know that some features are                                 correlated with the source maybe you                                 know that some are even causally related                                 by eliminating some of these before you                                 use them to make any decisions elsewhere                                 in your system you can make the overall                                 system easier to maintain and debug by                                 ensuring it's less likely to develop                                 accidental dependencies on redundant or                                 low information features let's put it                                 another way just because a lot of                                 algorithms can deal with high                                 dimensional data doesn't mean they                                 should by applying domain insight to                                 winnow down the feature set this also                                 gets to sort of what feature engineering                                 should be right extracting meaningful                                 general patterns from data to use the                                 signals for our model we can also make                                 things simpler by focusing on simpler                                 models instead of using our engineering                                 cleverness to figure out how to use                                 complex intellectual frameworks to solve                                 complex problems                                 we should be clever in how we make a                                 complex problem simple what's the most                                 straightforward technique that will get                                 us good results could a little extra                                 feature engineering enable us to use a                                 simpler model instead of optimizing                                 model parameters what if we identified                                 ways that a basic summary of our data                                 could solve a problem just as well there                                 are a lot of cases where a clever                                 application of a sketch could solve the                                 kind of problem that we might be                                 inclined to use model form with the                                 bonus that the trade-offs and                                 engineering properties of sketches are                                 typically easier to understand than                                 those of conventional machine learning                                 models for an example application of how                                 to use sketches to solve a machine                                 learning problem let me refer you to a                                 wonderful talk this afternoon                                 so if he Watson will be discussing some                                 real-world concerns and implementing                                 recommender systems and as part of her                                 talk she'll show how the min hash sketch                                 a probabilistic data structure for                                 identifying set similarity at scale can                                 support personalized recommendations                                 with much more attractive                                 maintainability properties than                                 conventional techniques based on matrix                                 factorization and if none of that makes                                 sense that's all explained in the talk                                 to talk about another problem we're                                 solving let's look at this hypothetical                                 line of code what kind of thing is X                                 it's a float what does it mean is it a                                 threshold I mean is it a probability is                                 it a scaling factor we don't know right                                 and not knowing what kind of thing X is                                 makes it harder to maintain whatever                                 code includes this line you could argue                                 that maybe we solved this problem by                                 choosing better variable names or                                 commenting our code or any number of                                 other ad-hoc ways to sort of have                                 documentation but these might or may not                                 buy us much in the long run write                                 comments and even variable names get                                 update when your code changes there is                                 one kind of code documentation that's                                 been proven useful over time though and                                 that's the type signature type                                 signatures are useful because you can                                 automatically check when they get out of                                 date now will I hear some of you                                 objecting the type systems I know about                                 only talk about the shape of the values                                 I'm dealing with they don't talk about                                 the kinds of things that we're putting                                 into those shapes right we just talked                                 about floats we don't talk about                                 thresholds                                 that's a reasonable objection I hope you                                 won't still have it at lunchtime because                                 I hope right after this talk ends you'll                                 head over to the front salon and see                                 Eric Aronson Justin scan encode and                                 check units that you're using in your                                 computations his talk will focus on how                                 using unit types will make your data                                 engineering work more reliable but I bet                                 you'll come out of it with some ideas                                 for how to use these types for machine                                 learning as well another place where                                 advanced type systems that have made                                 software engineering better could also                                 make machine learning systems better is                                 in encoding contracts above behavior for                                 a motivating example consider the noun                                 list how many Python programmers do we                                 have in here many people's something                                 other than Python anything other than                                 Python not Python okay so the Python                                 library uses the word list to mean a                                 data structure that has essentially                                 constant time access to a bunch of you                                 can assume they're contiguous elements                                 every other part of the computing                                 industry uses lists to mean a linked                                 list where if you actually want to                                 access something in that linked list you                                 have to step through one at a time so if                                 you want to access element five of a                                 list you have to do five steps in                                 general it takes the number of elements                                 in the list that's proportional to the                                 number of elements in the list taxes                                 anything right its linear in the number                                 of elements in list so I've seen this                                 assumption from you know that lists                                 should support constant time access                                 break code in the scala language in                                 particular or cause it to dramatically                                 misbehave in production in many contexts                                 and I've seen this everywhere from like                                 code developed by super-smart interns                                 who are Python programmers writing Scala                                 against their will and I've seen code                                 developed by Scala experts that depends                                 on lists being constant time access and                                 they really should know better                                 stub structural type systems are a                                 family of type systems that enable us to                                 reason about the resource usage and                                 complexity of code we write and applying                                 research in this area to the programming                                 languages that we use for machine                                 learning systems could make them more                                 reliable and robust by alerting us to                                 these problems before we put something                                 into production or publish a benchmark                                 or whatever of course you might not want                                 to experiment with type system research                                 or features that your favorite program                                 just directly support but you can still                                 benefit from types even simply talking                                 about the shapes of values you care                                 about buys you something for those of                                 you who are Python programmers I'm sure                                 you have a story about how some value                                 that you didn't expect to be someplace                                 where you didn't expect it to because                                 you're programmed to do something                                 unexpected in a way that you can't                                 explain right and sometimes this turns                                 into wasting a week of GPU time or                                 overfitting a model or you know any                                  number of awful things by leaning on the                                  type system that our languages provide                                  where we can we can make our systems                                  more reliable and easier to understand                                  and maintain so we're at the conclusion                                  we're in the homestretch thanks so much                                  for being here for sticking through the                                  talk I'd like to spend a few minutes                                  reviewing what we've discussed so far                                  today and those of you who are inclined                                  to photograph slides there will be                                  things you want to photograph in this                                  section so have your camera handy first                                  we reviewed why machine learning systems                                  are easy developed but can be hard to                                  maintain and we framed the beginning of                                  our discussion around some of the                                  concepts in this hidden technical debt                                  and machine learning space systems paper                                  we also saw how the way that people in                                  the community have responded to and used                                  this paper is actually sort of                                  interesting in itself                                  we saw how complicated models like the                                  deep neural networks trained by deep                                  learning techniques can be powerful for                                  vision speech and language problems but                                  can have some undesirable engineering                                  properties they encourage unnecessary                                  feature entanglement their predictions                                  can often be subverted by uncooperative                                  input and they're so expensive to train                                  that only the most well-funded                                  organizations can really conduct                                  exhaustive experiments about how to use                                  them in any domain we considered how the                                  incentives of individuals and                                  organizations aren't always aligned with                                  our incentives as people who want to                                  make applications of machine learning                                  successful and finally we looked at just                                  a few highlights of the last few decades                                  of programming in general software                                  engineering and to see if we can take                                  any of those ideas and use them to make                                  our machine learning systems better one                                  of the things I love the most about                                  buzzwords is that pretty much every slot                                  I want to go to three or four of the                                  four talks there are always fantastic                                  talks that explain                                  how to make something about data                                  processing better in the real world and                                  if I called out every talk you should                                  also go to if you care about the things                                  I talked about just now I'd still be                                  talking for longer than anyone stay in                                  the room                                  I did mention three talks though that go                                  into greater depth on some of the topics                                  we discussed in this session I hope                                  they're all on your calendar so it                                  thanks again I'm I'm here if we have we                                  have a little time for questions but                                  here's how you can get in touch with me                                  if you don't want to ask a question now                                  I answer emails or tweets and I'll be                                  happy to talk to anyone about any of                                  these things at the conference thanks so                                  much again ok thank you William
YouTube URL: https://www.youtube.com/watch?v=A6M9TsznUjg


