Title: Berlin Buzzwords 2019: Nicolas Frankel â€“ Fast log management for your apps #bbuzz
Publication date: 2019-06-20
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	So, you've migrated your application to Reactive Microservices to get the last ounce of performance from your servers. Still want more? Perhaps you forgot about the logs: logs can be one of the few roadblocks on the road to ultimate performance.

At Exoscale, we faced the same challenges as everyone: the application produces logs, and they need to be stored in our log storage - ElasticSearch, with the minimum of fuss and the fastest way possible.

In this talk, I'll show you some insider tips and tricks taken from our experience put you on the track toward fast(er) log management. Without revealing too much, it involves async loggers, JSON, Kafka and Logstash.

Read more:
https://2019.berlinbuzzwords.de/19/session/fast-log-management-your-apps

About Nicolas Frankel:
https://2019.berlinbuzzwords.de/users/nicolas-frankel

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              hi everyone things to be for this talk                               about rock management's and Nicola                               Frankel I've been for like more than                                  years                               like developer architects as a                               consultant in different context and for                               different companies and like since last                               September I decided to come to talk to                               conferences and be a developer advocate                                because you know where you have less                                kettles and less timelines to respect                                even when I was a developer and an                                architect I I was very interested in the                                other side of the wall in the ops world                                so monitoring and stuff like that I                                worked for a company called exascale we                                are European cloud provider our                                companies in Switzerland but we have                                data centers all around Europe actually                                that's one of the reason I am doing this                                talk is because it's some of the stuff                                that I'm going to describe is how we do                                it                                who here cannot understand that this is                                Java I am sorry I have been the Java                                developer but everybody understand that                                right okay can you tell me what's the                                problem with this statement it compiles                                and insurance but there is an issue know                                type check and the I'm using SLA for G                                so this like double square stuff double                                bracket will be replaced by code get                                price so now everything should work as                                as expected but sorry oh but it doesn't                                get price might be super expensive if                                you have done any e-commerce stuff I                                mean the price is not a value it's a                                computation the computation might take a                                long time so if I if I do it this is the                                codes and I want to run it and I want to                                light log the start time and the end                                time so I can like get the time it takes                                me to do that                                it's not because it's Java is just like                                super long it takes a long time and if                                we check the implementation of get price                                it's well it's very stupid but it means                                it means to simulate the fact that it                                takes a long time and it does takes a                                long time and and the problem with that                                is it happens a lot and if you want you                                might want to get the price in some                                context and you might not be interested                                in other contexts but the log statement                                must be there all the time so you might                                say okay normally this only happens when                                I like I'm in at the debug level and                                still every time I get this computation                                done even if I am at the info level so                                one of the way to bypass that is to say                                okay we can every time we check if we                                are at the right level so if we say                                every time we say each debug enabled and                                we are not debug then it's super fast                                again but the problem in that case is                                that it's super boring for the developer                                it's not one lock statement somewhere                                it's log statements everywhere that need                                to do that worse not only it's boring                                but it's error prone because it's super                                easy to D to do like that                                to add light and synchronization between                                the wall and the log statement itself                                this is bad because then you will again                                get faced with the same problem but this                                is also bad because it's the reverse in                                that case you you don't get what you                                want so this is all only marginally                                fixing the issue                                what could be a good way to do that to                                put the logic inside the logger yes and                                we can put the logic inside the logger                                and actually we can do it in a way that                                sorry it's not this one it's this one                                here I created what I call a lazy logger                                and this lazy logger actually gets not                                the log message itself but a wrapper                                around the message like in Java it's                                called a supplier which is like not a                                rapper a rapper function so when I call                                get I will get the result but so far                                it's lazy and in that case it's very                                easy for me to say ok and here it's also                                very fast not as fast as manual stuff                                because I need to create the object                                itself but it's it like nicely fast                                so one of the problem is it starts from                                the developer the developer must be                                aware that something is happening when                                 he writes a lock statement and someone                                 must provide him with the tools to                                 handle that second problem is                                 unfortunately log doesn't happen in like                                 the virtual world we are bounced to the                                 real world and so we must be aware of                                 where we are logging in which physical                                 drive we are logging it can be a hard                                 drive it can be a SSD it can be a                                 network file system and now we go to the                                 other side of the chain which means that                                 the administrator must know and must                                 like fix the fact where where we log the                                 stuff so it depending on on the physical                                 device you must be aware that you will                                 have different performances again no big                                 magic is just stuff to take care of the                                 writing process the the write log                                 writing process is your planned stream                                 you write the bytes you close the stream                                 and actually opening the stream and                                 closing the stream text a non-negligible                                 time compared to writing to bite                                 themselves so perhaps we could just open                                 the stream writes multiple chunks of                                 bytes and then close the stream and                                 opening and closing would be just done                                 once a that is possible in most                                 frameworks because by default logging is                                 synchronous but there is nothing                                 preventing you from doing a synchronous                                 logging this is exactly what I described                                 however when you start having this a                                 synchronous logging ideas then again                                 you've got other issues so this is how                                 you can do it with SLE                                                 Java world but I'm sure that in your own                                 tech stack you've got the same using log                                 back you've got configuration now first                                 configuration is what's the size of the                                 buffer of course because it's in                                 synchrony oh so you must buffer the                                 writes                                 the second is sometimes you will get a                                 lot of messages and some messages like                                 error and info are much more important                                 than other messages and when the queue                                 starts being full perhaps it's better to                                 discard those non very important                                 messages than doing something else                                 another configuration that you have is a                                 is it better to really be very very fast                                 or to drop messages is it better to have                                 every info every piece of that available                                 or to be slower and you must decide for                                 yourself in your context but again it's                                 a configuration that you must choose and                                 of course the log message itself is not                                 very important by itself what is                                 important is the context of the log                                 message so of course you've got the                                 timestamp                                 you've got the log level you've got to                                 thread them the class name the file name                                 whatever the metal name whatever there                                 are a lot of important metadata that you                                 might want to capture the thing is not                                 all of them are free I mean most of them                                 might be very expensive to compute again                                 so it's a trade-off between I want this                                 data and it slows me down some of them                                 are very very expensive to compute so                                 perhaps it's better to write them down                                 in the log message but then you get the                                 same problem as I mentioned before you                                 might not have the right information so                                 the line number you might decide to                                 write it by hand but then if you if                                 somebody adds like lines of code before                                 your statement then you've gotten                                 synchronised again and even if you have                                 everything then it's not very                                 insteresting by itself I mean how many                                 of you go to a server and reads the log                                 file anymore it it does                                 thanks in general what you also do is at                                 some point you like get everything into                                 a centralized place do you do that more                                 or less okay let's say more or less                                 because you want to correlate the events                                 that happen sometimes on the same                                 machine between like different nodes in                                 the cluster or stuff like that so you                                 might know that a elasticsearch you                                 might know but Splunk about Greylock                                 they are different like frameworks and                                 engines and tech stacks to do the ads I                                 tend to be more familiar with                                 elasticsearch so that will be what I                                 will be using in the rest of this talk                                 anyway in general you use additional                                 metadata because now it becomes even                                 more complex because you have like                                 different log files so you will need to                                 know which log file is a source and                                 perhaps you have like different IPS or                                 directly you use the AWS name you might                                 and probably should have different                                 environment so you must know that this                                 log file is like production this log                                 file is staging and again you might have                                 different clouds own if you are using                                 the cloud well this is additional                                 metadata that you probably need and if                                 you just lock that it's not very                                 interesting and it's just like if you                                 are doing backups but never try restore                                 and you know all those logs you want to                                 search I mean that's the point of the                                 logs and now comes the problem if you                                 want to be very very fast then you can                                 ingest your logs put everything into                                 elasticsearch and every time you search                                 it's going to take a long time or you                                 can decide that every time you send a                                 log message this log message is                                 structured so you must put the right                                 information into the right bucket and                                 everything will be fine and of course it                                 takes time to analyze too porous                                 and to put them into the right place so                                 if we are using the elastic stack you                                 can have file bit that scrap the log                                 files and send it to log stash to parse                                 the stuff that in turn will send it to                                 elasticsearch and here you can see that                                 I've been an architect because I can                                 draw nice UML diagrams so they are like                                 two phases creation so the application                                 create the log file and at some point                                 file bits text log file read the log                                 file and send the log to log stash and                                 log stash transform the log into JSON                                 and push J'son to elasticsearch and I                                 don't know if you have been using log                                 stash and the grog patterns it's not                                 super fun to do first and depending on                                 the log file that you will be parsing it                                 can be very complex and again complexity                                 means that it takes time also I think                                 that log stash is written in Ruby right                                 and let's say it's perhaps not the                                 fastest language ever so I have this                                 like super nice log message that gets                                 scrapped and in the end that is how I                                 want to interpret it to send it to                                 elasticsearch so that at this point I                                 can say okay find me everything that                                 happens on the info level and during                                 this class but if we think about it for                                 a second                                 actually that's not necessary think                                 about it we are creating like a string                                 and then we pour sit to create Jason why                                 don't we create jason just directly                                 so here I'm always my my stupid example                                 for the developer it doesn't change                                 anything it's just that now when I run                                 it I already create J'son the important                                 part is it must be on the same line so                                 again you don't need to say to a file                                 bit okay you must compute the fact that                                 two lines two consecutive lines might or                                 might not be part of the same log                                 message and if you are using SL f                                       log back it's pretty stupid I mean it's                                 just the pattern that it needs to be                                 like this it's not no black magic so                                 this is what I what I get again it's                                 from different lines but you shouldn't                                 do it                                 and your architectures is simpler your                                 performances will be better it's                                 everybody wins that also what you can                                 think about is I've talked about like                                 writing on the disk and then scrapping                                 from the disk and perhaps in some cases                                 you might think that it's not very good                                 ID so on one side at least your                                 persistence but on the other side it                                 takes more time what if we could if you                                 could just send the event logs to the                                 place that you want so you could                                 directly create an appender to send the                                 logs to elasticsearch in the first place                                 nothing prevents you from doing that in                                 that case however the problem becomes                                 that well everything will be in memory                                 and if something happens then you will                                 lose data also I mentioned a lot of time                                 that there is like sweet spots between                                 light consistency or persistence and and                                 being like fast for that I encourage you                                 to consider the fact that you might do                                 like hot reloading of configuration like                                 line numbers are not very important and                                 take a lot of time to compute sometimes                                 however they are become very important                                 for whatever reason so you might have                                 like hot reloading for saying a I have a                                 problem on this server for whatever                                 reason I need to get the line number I                                 don't want to switch off my GBM and                                 restart it again so author landing                                 configuration might be handy because you                                 change just change the configuration on                                 the server and then you've got the line                                 numbers finally how we do it at XO scale                                 because that was best picture I I showed                                 you and this is meant to like G mode the                                 fact that whatever is best of read in                                 your context might be completely                                 different so we are using syslog-ng                                 instead of five bits because for                                 whatever reason when we started it was                                 not available five bit was not available                                 and people at exhaustion are more like                                 sysadmin and they are very familiar with                                 syslog-ng and also we have Kafka in                                 between this is not super fast this is                                 not what we want however in our cases as                                 I said we are a cloud provider and we                                 are using logs to Bill our customer so                                 this is not only logs to the events and                                 we do a lot of the computation of the                                 events of the billing in Kafka so Kafka                                 is a place to go where we do a lot of                                 life business rules and so it's better                                 for us to send everything into Kafka and                                 only then to push it to elasticsearch                                 when when we need it so depending on                                 your context what I showed you might be                                 very different so summary it involves                                 everyone so the first is developers but                                 cent not the result of the computation                                 but the computation itself to the log so                                 that you don't compute something that is                                 not necessary you should consider the                                 physical side file system you should go                                 as synchronous if speed is more                                 important than reliability of                                 you shouldn't use expensive metadata but                                 if you do then you should consider                                 having a hot reloading so that when you                                 need it you have them of course just                                 like putting stuff into elasticsearch or                                 any data storage is not important the                                 real stuff that you want is to search so                                 you should consider perhaps not the fact                                 that the fact that the logs are super                                 fast but the fact also that the search                                 in the log is super fast so of course                                 use schema on writes and then don't care                                 about poor Singh and growing and having                                 a big growing pattern just send JSON                                 directly if you want to have like                                 performant logs everybody should be                                 involved it should go from the developer                                 to the cset min to the architect and of                                 course it's all a matter of contexts and                                 I've been a consultant the answer is                                 always it depends so don't take                                 everything that I told you here as with                                 a pinch of salt because in your context                                 might be completely different answer so                                 yeah you can with my blog you can read                                 our blog the blog of my company and you                                 can follow me on Twitter and perhaps now                                 I have like one minute for questions one                                 minute for questions like one question                                 thank you for the nice talk                                 [Applause]
YouTube URL: https://www.youtube.com/watch?v=U71Ju3y40fE


