Title: Berlin Buzzwords 2019: Conor Landry – Reindexing in record time (...) #bbuzz
Publication date: 2019-06-20
Playlist: Berlin Buzzwords 2019 #bbuzz
Description: 
	Conor Landry talking about "Reindexing in Record Time: How Shopify Indexes Over 800,000 Merchants' Data in Under 24 Hours".

Chances are, if you have shopped online, you’ve searched for the item you want to buy, placed that item in your digital cart, paid for that item, and had it delivered in record time. Each of those steps you took to enjoy your shiny, new item wouldn’t be possible without the help of search engines. Search engines help us find products, help merchants confirm your order, and ship it on time. How do we initially get all this data from slower, traditional databases into fast search engines?

In this talk, Conor Landry focuses on how Shopify indexes product, customer, order, and merchant data from MySQL to ElasticSearch in near real-time and how to reindex over 50 terabytes of data in less than 24 hours and the roadblocks we’ve encountered. Conor describes the challenges faced when handling data which is critical to the livelihoods of small business owners and well-known brands as well as strategies used by Shopify when scaling a search indexation system for the long term.

Read more:
https://2019.berlinbuzzwords.de/19/session/reindexing-record-time-how-shopify-indexes-over-800000-merchants-data-under-24-hours

About Conor Landry
https://2019.berlinbuzzwords.de/users/conor-landry

Website: https://berlinbuzzwords.de/
Twitter: https://twitter.com/berlinbuzzwords
LinkedIn: https://www.linkedin.com/showcase/berlin-buzzwords/ 
Reddit: https://www.reddit.com/r/berlinbuzzwords/
Captions: 
	                              thank you so as he said I'm Connor I'm                               going to talk about how weary and ex                               elastic                               and how we handle like mapping changes                               for good                               such and yeah so I work in Montreal                               Canada I almost missed this conference                               because my flight was canceled but I                               Here I am so so a little bit about                                Shopify who for those who don't know                                about us Shopify is an e-commerce                                platform that provides everything you                                need to sell online in social media or                                in person                                we have merchants that I saw a lot of                                products with us and bigger brands like                                Kylie cosmetics jeffree star all birds                                fashion OVA and                                                         collectively they have sold over a                                hundred billion dollars in gross                                merchantville um-- and we have about                                                                                                        a better sense of the scale of Shopify                                the last few years                                last Black Friday about ten percent of                                online sales were through us and five                                years ago that was below one percent so                                it's pretty crazy growing that quickly                                I've only been there two years so yeah                                so today we're gonna talk about first of                                all is how we run elasticsearch at                                Shopify for all of our development teams                                then a little bit about our Shopify                                Court's apology which is our main rails                                monolith if you aren't aware Shopify is                                really big in the rails community and we                                love monoliths fight me what R and X                                thing isn't like what it means to us how                                we're and X elasticsearch and then our                                future plans and improvements for this                                so yeah so a lot searchers Shopify I'm                                on a team of six engineers I shall have                                I called the search platform which means                                to be a common name for this team at                                other companies as well we have over                                   lots of search clusters at Shopify that                                I know of that use our tooling and so                                you run these allow search clusters we                                use a kubernetes controller we built for                                a search you can see all of our clusters                                there most are pretty small but then our                                largest cluster there which is for the                                Shopify core monolith and rails has                                   billion documents more or less and let's                                look a little bit more at that so this                                allows search cluster is set up in an                                active-active set up and fully                                replicated across two regions for                                resiliency purposes so if we have a DCP                                outage which happened a couple weeks ago                                we can simply failover to that the                                region and hopefully nothing is affected                                each region has about                                                  about                                                                   during the day we're out all documents                                or all queries by shop ID and we shard                                all shops by                                so a single shard and elasticsearch has                                all the documents for a single shop                                which is it's okay it could be done                                better and we run everything with one                                replicas per shard and zoned across                                three zones in each region so if we lose                                a region then we're not gonna lose                                access to the documents but they might                                not be replicated so looking at Java                                like Cornell there's hundreds of                                services at Shopify the largest at                                Shopify core it's a pretty conventional                                rails monolith and we shard the monolith                                by shop ID as well which is convenient                                and it's                                                            made the shift from data centers to                                kubernetes in                                                        left in the data center is is some load                                balancing tooling which as well as for                                resiliency against GCP outages so we run                                those applicators we run in three                                regions one is your central US east and                                then also north america north east one                                it's special because one it's in                                Montreal where I live and also it runs a                                lot a large portion of the Canadian                                legal cannabis sales so we're not gonna                                talk about the Kanto region anymore                                because of legal restrictions but we'll                                talk more about the structure of Shopify                                Court so what you're looking at here is                                pretty much the structure of a single                                 region of that shop of a core                                 application it powers all of our online                                 store functionality at Shopify and a pod                                 in this case is not the same as a                                 kubernetes pod we come up at the                                 terminology pause before we move to                                 Cooper Nettie's this is a Shopify pod                                 though a choppa fight for us Shopify pod                                 for us is the logical grouping of all                                 sharded components that's necessary to                                 run a Shopify and so this includes stuff                                 like my sequel Redis caches and some                                 cron utilities and notice that it like                                 the web servers and search                                 infrastructure is a shared resource web                                 because scaling web doesn't fit well at                                 this boundary architecture and search                                 because it's                                 prohibitively expensive to run like we                                 have hundreds of pods would be expensive                                 to run hundreds of elastic search                                 clusters compared to too large lots of                                 search clusters so just kind of like                                 seeing how a request flows through this                                 big rails application it goes through                                 these web workers which is then routed                                 to the correct pod and then                                 korie's we'll go to the my sequel query                                 and so like if a shop is on pod - I                                 can't query resources on pod                                            like the reason I'm going over this is                                 that it plays a big role in how we re                                 index quickly since we can horizontally                                 scale these pods and thus horizontally                                 scale or relaxation finally then when                                 search queries happen they go to the                                 shared search resource and back to the                                 user that's buying things online so what                                 is rien dexing now so we're indexing a                                 travel fight for us well first of all we                                 have a lot of developers at Shopify and                                 we have give them a lot of freedom for                                 when they're working with elastic search                                 so they can add new fields modify                                 analysers delete indices create indices                                 or change existing fields and when that                                 happens as you know the elastic search                                 you have to migrate all that data to a                                 new version of the index so especially                                 like if you add a new field you get to                                 backfill that data and so for indexing                                 for us is the process of migrating that                                 data from one index version the old                                 version to the new version                                 putting that new version into production                                 and serving that to the merchants and                                 buyers this is different from real-time                                 indexing which is what happens when like                                 you create a product in your online                                 store admin and then it gets into expert                                 search and then people can search for it                                 and buy it this is whenever we actually                                 changed the mapping and we want to move                                 all that data to a new version on the                                 index so I'm going to go through kind of                                 like a scenario of me being a product                                 developer and adding a new fancy                                 analyzer for autocompletes who like a                                 product title field and we want to get                                 there's a next mapping change into                                 production as quickly as possible so we                                 kind of start with writing the spy                                 commit this slack command we have this                                 slack chat bot called spy we have a                                 pretty like broad chat ops                                 infrastructure there's blog post on our                                 engineering blog if you want to read                                 more about that but we can do everything                                 from like failover is bought blocking                                 bots loadshedding traffic statistics                                 starting R and X's so here I am typing                                 spy es reindex start products and that                                 starts a R and X for the product                                 strategy we use the term strata strategy                                 instead of indices in terms of                                 relaxation because we have                                 multi-language                                 language search so when I reindex the                                 product strategy                                 indexes products products Japanese index                                 and so on so right after that reindex                                 starts we have to create a new alias and                                 a new and apply templates to that alias                                 so the new templates are they template                                 for this new mapping is already an                                 elastic search which was merged by                                 developer when they added that title                                 analyzer and the current index product                                   well first of all we we go through and                                 we use like a monotonically increasing                                 version number for indices in elastic                                 search so those products does                                            this case is the only index available in                                 elastic search right now and the                                 products alias points to it and then                                 whenever we wants to create this new                                 mapping with the new analyzer its                                 products dot one and has the products                                 new alias so when we index data for the                                 reindex we point to the products new                                 alias and search results are still                                 served from the products alias and real                                 time indexing is still done with the                                 products alias so now we're ready to                                 start moving documents from my sequel to                                 elastic search and start denormalizing                                 data from my sequel so that they can be                                 in this do normalize format for elastic                                 search so a Shopify all ring indexing                                 loads happen in the monolith and it's                                 done through a structure of like a very                                 concurrent structure of this coordinator                                 job and all these worker jobs that they                                 create all of this is done in after job                                 in Rex queue so if you know anything                                 about rails you're probably familiar                                 with active job which is like a                                 background job framework for Ruby on                                 Rails and rescue is the adapter for that                                 to actually work on those jobs so first                                 we create a coordinator job per pod                                 which remember one of these pods it runs                                 off of Redis with the rescue adapter on                                 top of Redis we create this coordinator                                 job then the coordinator job will create                                 a worker job for every single shop so                                 overall we're creating                                                 course those don't all run at the same                                 exact time we do throttles so we don't                                 take down Shopify every time we index so                                 to avoid scheduling too many jobs on too                                 many shops at one time first of all we                                 limit it to                                                              per pod which ends up being about                                        to                                                                       at once when we index                                 and we also read all data from the my                                 sequel read replicas so we don't take                                 down the writer likewise each                                 coordinator job has a keep track of its                                 progress so it will report back to the                                 so the worker job will tick that                                 progress bar for each coordinator job                                 once the rate index is complete we                                 should see that a hundred percent of                                 these jobs completed we'll see which                                 one's failed and we need to retry those                                 jobs if we need to and by doing that we                                 can set s ellos based on what percent of                                 a pawn shops are reindex so likewise                                 again if you're familiar with rescue an                                 active job this strategy won't work                                 because we'll create too many jobs even                                 with limiting it to                                                    and we'll take down rescue and other                                 jobs won't get time to run because it's                                 not rescues not a scheduling system it's                                 just a job working system and so this is                                 where the iteration API comes in hand so                                 the iteration API is a extension for                                 active job which makes jobs interrupts a                                 bulletin resumable and saves progress                                 that all the jobs have made so in this                                 case for us like a job can run for a                                 maximum of                                                           at which point they're paused the                                 current cursor position Andrian queued                                 and we'll continue with the next time                                 quanta they get and the way this looks                                 when you're writing a job in Shopify is                                 useful the job in the two sections the                                 collection you want to process and you                                 build an enumerator so in this case we                                 pull up our strategy get all documents                                 and batches and then you want to apply                                 an action which for us is producing the                                 build documents to kefka all jobs at                                 Shopify use this API and we also share                                 the same rescue workers with all other                                 jobs at Shopify so we have to make sure                                 that we don't take up their their fair                                 share of work as well because that                                 includes stuff like billing and                                 checkouts so looking here this is a time                                 series diagram of us running a Rand X                                 with all of these worker jobs you can                                 see like the darker yellow at near the                                 bottom is this reindex starting and                                 getting to run in not taking up the rest                                 like all of the available reindex                                 not all the available rescue worker                                 pulled workers at one time and but we                                 still get our good fair share of                                 we limited that                                                          have these jobs and we've started these                                 jobs and they're running and now I want                                 to index the data into elasticsearch                                 from within that job and he saw hints of                                 that with the produce akafuku line but                                 now we'll talk a little bit more about                                 that because it goes deeper than Kafka                                 so we have all the Shopify rescue                                 workers in this diagram on each host in                                 one GCP region and whenever we produce                                 two Kafka initially it actually produces                                 to a system five message queue which                                 runs on every single host and is shared                                 by all of the rescue workers running on                                 that host then we then we have a tool                                 called calf Cobra just runs and pulls                                 from that system                                                    Kafka regional which uses calf c'mere                                 maker to share it to the calf get                                 aggregate cluster which is not within a                                 single region it's shared among all                                 regions and then our last search                                 clusters will read from the Kafka                                 aggregate into elastic search so                                 thinking a little digging into that a                                 little bit more the reason we use this                                 calculator gate is it pretty much                                 guarantees that we have a full                                 replication across both regions since                                 Kafka has the concept of like consumer                                 offsets and committing offsets if we                                 fail to consume a certain batch of                                 documents and we're not going to lose                                 track of those documents and we'll know                                 that like the East region is you know                                 it's indexed all these offsets and so as                                 a central region and thus we can believe                                 that both regions are fully replicated                                 and then also the reason we use the                                 system five message queue is kind of                                 like a resiliency tool that helps the on                                 call a little bit so if for example if                                 caf-co were to go down which it does                                 occasionally then the job of I rescue                                 workers can continue working and                                 producing documents to kafka however                                 they're going to the system five message                                 queue so this means that jobs don't get                                 backed up and jobs don't get delayed in                                 core and they can continue working as if                                 they don't know about the incident                                 that's happening and then when Kafka                                 comes back up it'll just drain the queue                                 and we'll continue working                                 of course this queue is of limited size                                 of the system                                                         the incident within like several hours                                 then there is the chance that we can                                 lose messages in that case for                                 re-indexing it's not a big deal we can                                 restart the re-index                                 and we just lose time but that is a                                 drawback of this system                                 so I already kind of talked about this                                 but so wow this is some fine message                                 Hughes won the jobs can be simpler                                 because they don't have to know about                                 Kafka they're persistent against                                 container restarts as well and why we                                 use Kafka like I said the resiliency of                                 Kafka is nice because once we produced a                                 Kafka it is persistent and we know if                                 Kafka goes down at least we don't lose                                 Kafka data the implicit replication by                                 using the Kafka aggregate the offset                                 committing x' and of course the order                                 guarantees of Kafka because if you write                                 to a single partition and Kafka topic                                 then you're guaranteed to have the order                                 there so we write all the documents for                                 a single shop into one partition and                                 like use a modulus to write to each                                 partition in that cafetalk and then we                                 can guarantee that the order is there                                 and we're not overwriting documents so                                 bringing it together then looking at                                 like the whole view of shopify we can                                 see like the u.s. east region you a                                 central region in our last historic                                 regions the Kafka regional in each                                 region the Kafka aggregate in our                                 elasticsearch clusters and the flow then                                 from each of those charted pods to the                                 regional Kafka and elasticsearch so                                 after everything is R and X a way                                 started that reindex the documents were                                 produced at Kafka they've been read from                                 our consumer index we verified that                                 everything completed successfully we                                 need to put this new index under                                 production it's pretty simple we just                                 changed the alias name and now the                                 products index is products dot one we                                 don't do any cash warming which is kind                                 of bad it means that there is like a                                 small latency spike for merchants and                                 users whenever we switch the index into                                 production but that latency spike since                                 our query rate is high enough goes down                                 pretty quickly so maybe a couple                                 thousand queries hit a bad get a bad                                 query at times out but that's something                                 and our future plans to improve on so                                 now speaking of our future plans so the                                 biggest problem that we have is my                                 sequel query optimization going from                                 this normalized format to a denormalized                                 format in elasticsearch is really hard                                 because                                 like like you have to have a bunch of                                 different associations with your queries                                 like for our orders table for example                                 there's close to                                                         be loaded up just to build one document                                 and we have a maximum query time of                                    seconds in our core application so if                                 one of those queries times out then that                                 job has to retry and hopefully it                                 doesn't fail the next time we do have                                 some resiliency against those timeouts                                 by decreasing the batch size that we                                 request from elasticsearch and then my                                 sequel query so when a query these order                                 table and                                                              table if it fails with the batch size of                                 a thousand then we'll try again with                                     and maybe it'll work then but it's tough                                 and then the second aspect of that is                                 like pushing our products teams and                                 educating them to think about the impact                                 of these complex queries because we're                                 not the ones writing these queries as                                 well we don't know when someone                                 necessarily adds a new index or adds a                                 new field and so it's part of our                                 efforts who educate product developers                                 on how to write efficient my sequel                                 queries and that helps with relaxation                                 as well the other thing is we want to                                 add top level index concurrency so like                                 I said we were index with one job per                                 shop there's shops that have well so                                 they're shops that are legitimate which                                 have for example like this shop right                                 here has                                                         elasticsearch and that's                                            orders for example but there's also                                 shops like if you've ever been scrolling                                 on facebook and you see like the like                                 get your last name on a t-shirt ads on                                 facebook                                 those are Shopify stores sometimes and                                 those shops make a permutation of every                                 single last name possible and they store                                 all of those documents on Shopify and                                 they get into elasticsearch so one shop                                 could have five billion products that                                 are just t-shirts in rejecting those                                 sucks so like here in this time series                                 diagram you can see this is the worker                                 jobs and most of them complete within                                 the first like couple hours of where in                                 the three index starts but then there's                                 always three or four shops that take up                                 to like                                                          complete so our long-term vision with                                 this is to essentially split each of                                 these shops into multiple worker jobs                                 and that way we can have like a hundred                                 workers Rhian dexing with the shops that                                 are a problem                                 so yeah so pretty much what we've                                 covered is first of all lots of searches                                 Shopify and how are you running up for                                 developers and our infrastructure                                 there's trouble like poor infrastructure                                 what reindex thing is for us and how                                 developers can add new mappings top                                 mappings and analyzers and such how a                                 great index at Shopify then after a                                 developer does that and our future plans                                 and that's everything so they still have                                 a time for a couple questions so if you                                 have any please raise your hand I see                                 you thank you for your awesome                                 presentation those three index                                 re-indexing jobs sometimes fail for                                 reason not known to a developer how long                                 do you keep the old indexes you know the                                 index dot zero for recovery or failover                                 yeah so when we're indexing the old                                 index is still live in production so if                                 that reindex fails then there's really                                 no impact                                 we just restart Julie the new index do                                 whatever we need to do to fix it once we                                 switch the old index into production and                                 we stop indexing in real time into the                                 old index so we can't go back to the old                                 index so once you've switched it that's                                 it but before we do switch we verify                                 that everything's good we can send like                                 shadow traffic to it but yeah we have                                 had situations where we switched the                                 index before we knew it was verified to                                 be properly indexed and that was an                                 incident thank you                                 anybody else no perfect thank you very                                 much for your talk is amazing                                 [Applause]
YouTube URL: https://www.youtube.com/watch?v=nPU-MtIkNlQ


